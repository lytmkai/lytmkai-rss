<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 21 Jan 2026 06:05:24 GMT</lastBuildDate><ttl>5</ttl><item><title>飞猪搭建系统演进：从人工运营到多Agent协同 搭投生产</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;飞猪搭建体系介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可视化页面搭建技术在业界已相当成熟，几乎每个前端团队都会建设一套页面搭建系统，它显著提升研发效率与交付速度，已成为业务快速迭代与规模化生产的关键基础能力。飞猪移动端页面搭建最早诞生于营销大促场景，不同于常规的业务开发，营销大促业务具有以下特点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;迭代速度快：飞猪大促项目频次高且节奏紧凑，传统瀑布式研发模式难以适配业务快速迭代诉求。需沉淀一套面向活动场景的低代码平台，将页面公共能力和通用组件进行标准化、平台化，以降低重复开发与跨团队协作成本，支撑快速迭代与高频发布，在有限周期内兼顾交付效率与稳定性；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面数量多：同期活动通常涵盖多个会场、频道、主题页及域外落地页等，上百个活动页面并行推进，且页面结构差异较大，高频、紧急的营销节奏叠加海量页面需求，使研发交付压力显著增加；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面变阵频繁：一场完整的大促活动通常要经历「预热、预售、现货」等多个阶段，不同阶段的核心目标与主推商品 / 权益会递进变化，运营工作人员需要实时盯盘，根据流量、转化与库存等数据频繁调整页面内容与商品坑位布局且要求快速生效；&lt;/p&gt;&lt;p&gt;基于上述特性，飞猪移动端页面搭建体系应运而生，我们围绕可视化搭建、数据配置与动态渲染三大核心能力，打造了一套面向高频迭代与规模化交付的页面搭建体系。该体系从初代寄生于阿里集团搭建平台下的一个服务站点起步，先后经历了数据投放服务自建、平台能力自建、搭建服务与渲染底座自研。最终形成了当前将数据投放与页面搭建深度融合的一体化“搭投”体系，能够灵活支撑营销大促、日常频道、互动玩法、机酒汽等行业场景、商业化以及小程序等多场景业务需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建体系遵循 PMT 模型规范，核心概念包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Page（页面）：用户可访问的完整页面载体，由页面容器聚合多个业务模块完成运行时渲染，页面容器提供通用基础能力，包括：模板引擎渲染、协议数据的转换与分发、组件依赖与运行时注入、以及埋点与日志等公共服务；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Module（模块）：可复用的业务组件单元，封装组件的展示与业务逻辑并定义数据绑定规则，支持独立配置、按需组合编排，快速拼装成不同页面结构；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Tag（资源位）：模块内用于承载内容投放的最小单元，用来挂载与管理投放数据，一对多关联具体数据投放配置，并支持个性化规则定投；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e9df347677d2499dc2009e48327ff04.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建平台&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;现状与可优化空间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从寄生站点到自研体系的全面回迁，标志着飞猪搭建体系逐渐成熟，但是依赖传统人工选搭投的模式没有得到本质化的改变。下一阶段的重点，是在既有能力之上进一步释放效率红利，围绕“搭建自动化、配置智能化、保障体系化”持续优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运营侧提效：页面搭建链路仍有较大提效潜力，当前运营在模块选择、配置理解、跨平台联动配置上投入时间较多，且在日常使用中对咨询支持的需求仍然较旺。与此同时，运营团队更新节奏快，新人从熟悉到独立产出仍需要一定学习成本，存在进一步降低门槛、增强引导与工具化的空间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设计侧降本增效：页面交付对设计资源的依赖仍然较重，尤其在大促等峰值场景，设计需求集中、节奏紧，容易成为影响整体交付效率的关键路径。日常场景采用计件和众包模式也带来一定的成本与协同开销，因此在设计素材智能生成等方面有进一步优化空间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;产研侧提效：答疑提效：尽管已有答疑平台与知识库沉淀，但是在自主排查方面仍可继续增强，高频场景可进一步沉淀标准化诊断与一键解决方案，减少人工介入；技术保障提效：大促会场通常需要多轮预演，上线后也需要持续巡检以应对业务调整带来的体验波动。随着平台迭代和营销活动频次提升，可通过自动化预检和巡检等方案持续降人工保障投入；研发提效：搭建模块遵循特定规范与约束，新人从上手开发到独立交付仍需一定学习成本；同时由于容器和渲染等核心链路的黑盒特性，仍需平台负责人员提供一定协同支持与保障。可引入 AI Coding 辅助研发，并完善 AI 自助诊断与排障工具，提升代码研发与问题排查效率，降低对平台协同支持的依赖；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 化升级的目标&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今正是 AI 蓬勃发展的时代，问答推理、物料生产、流程编排等方案已经成熟，当下的命题应当是思考搭建 + AI 如何深度结合，打造智能化搭建系统，从人工运营向 AI 辅助运营转型，最终实现无人值守自动化运营。为解决选搭投当前的问题正式启动 AI 智能搭投升级专项：深度融合图文素材生产系统、大模型调度编排等能力，将 AI 模式注入招商选品、页面搭建、数据投放等核心系统，构建从需求理解 → 智能编排 → 自动执行的搭投新范式，为未来全域智能运营奠定技术基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提升运营配置效率：打通“招选搭投”体系 AI 基础能力，通过推荐模板页面、智能搭建页面、智能助理运维等能力辅助业务快速完成页面搭投，覆盖飞猪页面搭建全场景、提升运营配置效率、减少页面配置时间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;减少设计投入成本：全面推广 AI 文生图和创意合图能力，实现计件众包模式向业务自助产图转型，减少设计费用实现流程提效；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;减少人工答疑成本：AI 赋能答疑场景，提升答疑助手拦截率，常见配置问题自动定位并引导解决，缩短问题解决耗时；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提升模块研发效率：模块研发全面拥抱 AI 模式，完善搭建 IDE Rules + MCP 生态，探索 D2C 在搭建模块开发应用场景，提升模块开发效率；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;产品结构设计&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能选搭投体系 = AI 搭建 + AI 投放 + AI 素材 + AI 助理 + AI 答疑 + AI Coding&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/9596ea9d0ce02a0fc1a9e751967ce524.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;产品结构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;技术实现细节&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能搭建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模板检索&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了保证 AI 生产的页面尽可能贴合飞猪营销页面规范且保证转化效率，以及让运营人员有体感预期，我们采用模板预览的形式作为平台入口，基于用户描述推荐场景相似且业务效果较好的模板，用户可进行预览并确认后进入下一步生产流程。此外我们对页面模板进行了分类，不同的页面类型会走到不同的 Agent 完成生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/95332b8521a4a258094f7ea64dbf036b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;模板预览&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么这么多页面模板从哪里来呢，总不能人工维护吧？答案是基于历史页面作为模板即可，毕竟运营人工搭的页面一定是符合预期效果的。通过跑定时任务批量对历史页面进行回溯，再交给多模态 LLM 分析页面内容打标，作为检索关键词进行向量化并落库，检索时进行相似度计算排序即可，实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面回溯：处理埋点日志找到页面 UV 最大的日期，基于该日期进行页面结构和投放数据 mock 穿越处理，并忽略页面下线重定向逻辑，即可回溯到该页面历史线上投放效果；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面快照：基于无头浏览器对页面进行截图，生成页面截图快照；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内容打标：基于多模态 LLM 进行页面内容分析，生成页面内容维度标识，包括：页面介绍、关键词、行业标、主题标、质量评分、效率评分等，作为相似度计算的依据；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;向量持久化：基于 multimodal-embedding-v1 等多模态向量模型将快照截图和内容标识转换成浮点向量，存到关系型数据库即可（如果数据量较大推荐使用向量数据库）；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;检索召回：实时检索时先进行简单的 SQL 检索过滤，再将用户关键词进行向量化，与数据库中的向量字段进行余弦相似度计算，最后排序返回即可；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd9dd57f5ddc9a1964954806a5fc6955.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;向量检索链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;页面生产&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在页面生成链路，我们梳理了飞猪现存搭建页面类型，大致可分为“搭建页”、“频道页”、“图文页”、“文本页”四类，不同页面类型有着各自的应用场景，需要针对性设计不同的技术方案：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b8fc57dbebae74784333556f3c090dd8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终生产的页面效果如下，完全达到线上可投放标准：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;搭建页&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统搭建页主要通过各行业通用模块搭建而成，不同搭建模块配置项各异且繁琐，新人理解学习成本非常高，即便是运营老司机走完配置流程也需要花不少时间，另外在配置过程中还需要填写大量图片素材依赖设计团队产图，页面搭建过程并不丝滑，一整套页面搭建下来往往需要花费大半天甚至几天时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此我们设计了智能搭建系统，基于用户选择的页面模板复制页面和模块，LLM 推理用户需求总结归纳为页面搭建方案，批量调用智能投放链路（详见后文）完成模块配置，再打通后续排期发布和页面发布工程链路。整套流程只需要 3 min 即可完成，用户描述需求后全程无感，并且支持退出页面后台异步生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/7866d2e35925e36b2ca9d373022abcf8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;搭建页面生产效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体走大模型编排链路，链路如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd9dd57f5ddc9a1964954806a5fc6955.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;搭建页生产链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各流程节点实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e7/e7afd05a4bd82e85cbb9b05b663588e1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图文页&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统搭建模式难以有效支持图文页面场景，难点在于图文页偏设计化、页面结构较复杂，一直以来我们都依赖设计出视觉稿再切图投放，该模式设计计件费用昂贵并且流程较长，对于短平快的营销场景来说是属于历史糟粕亟需革命。我们尝试了以下方案，结果总是差强人意：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/18f41b8189282c089586e2a098ea298a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们总是在讨论 AI Coding 如何对程序员提效，那么能否直接绕过 “中间商赚差价”（拿起剪刀剪自己辫子），让运营工作人员直接基于 AI 生产页面呢？答案是可以的，抛开复杂的页面交互逻辑，如果仅是产出静态图文页面，完全可行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们设计了一套 Agent 模拟 “前端页面开发” 流程，由 LLM 担任产品、PM、设计、开发等多角色，协同完成图文页面开发任务，最终产物为页面 HTML 代码。再将 HTML 代码渲染至左侧面板提供预览能力，支持文本、图片等 DOM 元素可视化编辑以及选中元素后 AI 微调等能力供运营人员二次调整。页面调整完成后会按页面片段进行图层切分，基于 html2canvas 库将 HTML 片段代码转换成 PNG 图片，导入图片模块并借助 LLM 完成自动圈选热区，最终生成可点击图文页面，或者直接 HTML 代码注入模块实时渲染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd8ceef920132f7f26f89f6e482517ad.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图文页生产效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体采用 Manager–Worker 架构设计智能体，拆分分析器、调度器、设计师、工程师等多个角色协同完成长图文生产任务，单 Agent 内部实现完整的 ReAct 范式确保输出最优，使用任务队列模式存储执行进程和上下文，由 AI 自主完成监督调度。在设计 Agent 时需要严格约定出入参数据格式以便调度器准确调度执行，另外还需要设计纠错机制和重试机制保障 Agent 节点在意外抖动时整体链路能够稳定运行。图片页生产链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/68/684db72e34d22f332d1a1078b57bf1c5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心 Agent 实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/abc34476e860b5d5effcdbfb3b685c2d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能投放&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建体系遵循 PMT 规范，其中资源位数据投放也是搭建链路中重要的一环，我们自建了数据投放系统，支持静态数据源（模块配置）、标准数据源（商品宝贝、酒店日历房）、插件数据源（服务端接口）三类数据类型，涵盖了几乎所有场景的数据内容投放。在搭建 AI 化升级过程中我们对数据投放链路同步进行了升级，衍生出了智能投放链路。智能投放目前支持了图片素材、模块配置、商品选品、二方平台四类场景一句话快速投放，并且支持全类目定投策略智能关联能力，如：时间周期、人群、设备等。我们还对智能投放原子化能力进行封装以支持页面级批量调用，通过 LLM 分析页面主题归纳页面搭建方案，各模块基于搭建方案自主生产投放数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能投放技术链路如下图，首先 LLM 会根据用户描述、页面和模块信息进行推理，生成符合页面主题的模块推荐配置，引导用户快速选择。用户进一步描述后，LLM 针对不同物料类型生成所需的内容素材，包括：配置项（JSON）、文案、图片等，并对素材内容整合转换成模块配置入参，若语义识别到用户有定投诉求会自动进行配置，最后调工程链路完成排期发布和钉钉消息通知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/988c1321516b6e635ed666f8a4972413.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;智能投放链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能投放各节点详细方案和效果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能助理&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在页面搭建过程中，运营人员除了搭建模块和数据投放外，通常还需要配置页面容器、插件，以及后续投放过程中的渠道加参和转码转链等操作。这类配置内容比较分散，甚至跨多个平台，对于新人运营人员有一定学习过程。因此我们结合 AI 能力推出了「智能助理」，涵盖：页面修改、转码转链、热点分析等多种能力，支持语义识别用户的诉求自动调用相应的工具，辅助用户提高运营效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术链路上比较简单，通过 LLM 理解用户的操作意图，转换成工具约定的出入参并调用底层平台能力完成对应的操作，功能实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/75bc75038e89f8f622e754bf02862aa0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能素材&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外我们还提供了原子化素材创意生成能力，帮助有匠心的运营人员快速微调和优化页面素材内容：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 答疑&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建直面一线运营人员，我们没有专业的技术支持人员，答疑工作只能由产研人员“兼职”。每天面对大量的答疑咨询工单，让本就繁忙的研发工作雪上加霜。在 LLM 普及后我们也是第一时间接入答疑场景，提供平台和群答疑两种交互模式，日常常见问题全部交由 AI 处理，技术人员只需处理疑难杂症即可。将答疑工作量减至原来的 1/6，拦截率达到 91.4%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 答疑技术方案比较简单，纯调 LLM 没什么好说的，核心需要丰富的知识库来提高答疑准确率。我们扒了近几年来飞猪搭建相关的技术 &amp;amp; 产品文档，根据常见问题进行分类，修订为搭投提示知识库。有了知识库还需要对 RAG 进行优化，主要通过文档分段打标实现，我们通过三方接口召回了集团答疑工具近年来所有人工答疑记录，作为语料支持对现有知识库进行打标，文档缺失部分将该 Case 作为 FAQ 进行补充。基于存量知识库答疑总是具有局限性，随着飞猪搭建体系不断地迭代发展，所以还需要让 AI 自我学习最新知识，我们将 AI 对话和人工答疑对话落库，跑定时任务召回对话并进行向量化落库。后续实时检索时历史对话也会作为知识库的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a0cfd6bbd758495a4840ffef74018400.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;AI 答疑 RAG 链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI Coding&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;搭建模块开发：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI Coding 方面我们没有选择造 “重复且低质量的轮子”，而是在 Qoder、Cursor 等成熟 IDE 基础上，通过修订 Rules 方式增强 AI 编码能力。我们编写了 8 类搭建模块开发特殊规则，并接入飞猪代码仓库知识库、Ftech、Figma 等 MCP 服务实现组件库代码片段召回、PRD 理解、D2C 视觉稿转码等能力。有助于模块开发提效，对于新人入门非常友好，整体编码提效 80%+，实测模块功能实现和业务逻辑近乎完美只需要微调样式，规则目录如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0857f208d280df51caada65922d2728b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;schema 编写助手&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建模块配置表单遵循类 JSON Schema 规范，我们开发了 Schema 编写助手，辅助开发快速生成表单配置，支持 mock 数据和 schema 配置互转，减少新人学习成本、提高模块开发效率、减少研发答疑成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/de/de2b26a4d5dee44fb56cde7e91dc0870.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;schema 助手&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总 &amp;nbsp; &amp;nbsp;结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过对选品、搭建、投放、素材等平台全面整合，我们对飞猪搭建体系进行全面的 AI 化升级，面向运营、设计、产品、研发等全角色构建了一体化的智能搭建平台。在产品形态上，智能搭建体系已经形成智能创建、智能投放、智能素材、智能助理、AI 答疑、AI Coding 六大支柱，为未来全域智能运营奠定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发一套 AI 体系远比想象中困难，它不像写代码那样有明确、可复现的确定性结果，几乎每天都在对着“人工智障” 挠头。最棘手的问题之一就是大模型的幻觉：自始至终都在困扰着我们，哪怕只是一个很细微的偏差，在多轮任务的链式执行中也会被不断放大，最终产出就可能变得不可用。在不讨论训练 / 微调模型这类高阶手段的前提下，我认为提升 Agent 准确率无外乎以下几种手段：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构设计：Agent 开发需要有 “把不确定性关进笼子” 的架构设计，实践中可以结合具体场景参考网上的经典设计案例，把任务拆解为可控步骤并引入观测、校验、重试、回滚等机制，将大模型的自由生成约束在可验证和纠偏的闭环中以降低幻觉；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;工程实现：多数场景下，工程代码在执行性能与稳定性上更具优势，对于链路中规则清晰、可验证的环节，优先用工程代码实现，能有效降低不确定性；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Prompt 调优：Prompt 是一门学问，调好了能让大模型稳定地发挥能力，总结来说就是结构化、多约束、评估标准、分层书写。网上有许多教程和黑科技，不妨先快速学习一轮，实践中灵活尝试运用，磨刀不误砍柴工；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上下文管理：对于多 Agent 体系中，上下文管理尤为重要，一股脑往里塞必然导致 “上下文污染” 越聊越偏。因此需要设计合理的 memory 结构，为上下文标注来源和边界，只使用可信内容、临时内容及时销毁；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型升级：终极大招，可能你调了半天发现还没换个模型见效快，那就等待 AI 科技的发展进步吧。但是别忘了，基建架构是不可逆的，设计好了就是锦上添花，设计烂了就是雪上加霜；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一阶段的关键在于把 AI 搭建从 “提效 Copilot” 升级为 “可控、可评估、可闭环的生产系统”，通过约束式架构、自动诊断调优等手段不断提升 AI 的自我思考能力并减少幻觉，推广到日常活动、频道运营、大促活动等方方面面，支撑飞猪更高频、更复杂、多场景的全域智能运营。以下是建设过程中的一些思考以及后续规划：&lt;/p&gt;</description><link>https://www.infoq.cn/article/T96C1HNWw5IEiwmMj7It</link><guid isPermaLink="false">https://www.infoq.cn/article/T96C1HNWw5IEiwmMj7It</guid><pubDate>Wed, 21 Jan 2026 05:51:20 GMT</pubDate><author>飞猪技术 丁兆杰</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>零售进入 Agent 时代：Google 联合 Walmart、Shopify 推出 UCP</title><description>&lt;p&gt;Google 正式发布&amp;nbsp;&lt;a href=&quot;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&quot;&gt;Universal Commerce Protocol&lt;/a&gt;&quot;（UCP，通用商业协议），这是一项开放标准，旨在支持“代理式商业”，也就是由 AI 驱动的购物代理可完成从商品发现、下单结算到售后管理的全流程任务。UCP 同时兼顾零售商与消费者需求，在整个购物旅程中始终以“客户关系”为核心，从最初的商品发现到购买决策乃至购买之后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;UCP 在&lt;a href=&quot;https://nrfbigshow.nrf.com/&quot;&gt;美国全国零售联合会&lt;/a&gt;&quot;（National Retail Federation，NRF）年度大会上正式公布。该协议为 AI 代理与商业生态中的后台系统建立了一种安全、标准化的连接方式。企业可以通过 UCP 对外暴露自身能力，并在此基础上扩展诸如折扣等功能；AI 代理则可以通过企业资料动态发现可用服务与支付选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在支付设计上，UCP 将支付工具与支付处理方进行解耦，支持多个支付服务提供商。通信层面，协议支持标准 API、&lt;a href=&quot;https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/&quot;&gt;Agent2Agent&lt;/a&gt;&quot;&amp;nbsp;以及&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_Context_Protocol&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;&amp;nbsp;绑定。Google 还提供了示例实现，包括一个&amp;nbsp;&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/samples/blob/main/rest/python/server/README.md&quot;&gt;Python 服务器&lt;/a&gt;&quot;以及包含商品数据的&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/python-sdk&quot;&gt;软件开发工具包&lt;/a&gt;&quot;，用于展示 AI 代理如何发现商业能力并执行结算流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;American Express 在 LinkedIn 上&lt;a href=&quot;https://www.linkedin.com/posts/american-express_innovation-agenticcommerce-activity-7416134344714715136-6dbm?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;发文&lt;/a&gt;&quot;，强调了该协议在简化商业流程方面的潜力：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Google 推出的 Universal Commerce Protocol（UCP）是一项面向代理式商业的全新开放标准，想减少支离破碎的购物体验，帮零售商与消费者建立更顺畅的连接。UCP 很快将为 Google 搜索的 AI Mode 以及 Gemini 应用中的全新结算体验提供支持。像这样的开放标准，对于构建更安全、更可信的商业体系至关重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 Google 在一篇深入解析的&lt;a href=&quot;http://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/&quot;&gt;技术博客&lt;/a&gt;&quot;中所介绍的，UCP 定义了一系列核心商业能力，包括商品发现、购物车管理、结算流程以及售后工作流。AI 代理可通过查询企业资料来识别可用服务并协商支持的功能，从而减少定制化集成的需求。这种方式既能让企业继续掌控价格、库存和履约逻辑，又能让 AI 代理实现更高程度的自主运行。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/53/59/530774d2061823652c198576a014f059.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;UCP 高层架构示意图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在安全架构方面，UCP 通过凭证提供方对支付和身份信息进行令牌化处理，而具体的交易处理则由支付服务提供商完成。这种分离设计使 AI 代理在无需接触原始支付信息或个人数据的情况下即可完成交易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，UCP 具备传输层无关性，既支持标准 API 交互，也支持面向代理的绑定形式，适用于对话式界面、AI 助手和自动化工作流。目前，UCP 的早期实现已出现在 AI 驱动的搜索与助理平台中，符合条件的零售商可以在不跳转至外部网站的情况下，直接向用户提供结算体验。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/7d/b4/7d59865678aa463b7252d18a6ccd19b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;基于 AI 代理的购买流程演示&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;UCP 由 Google 与 Shopify、Etsy、Wayfair、Target、Walmart 共同开发，并获得了来自商业生态中 20 多家合作伙伴的支持与背书，其中包括&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Adyen&quot;&gt;Adyen&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/American_Express&quot;&gt;American Express&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Best_Buy&quot;&gt;Best Buy&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Flipkart&quot;&gt;Flipkart&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Macy%27s&quot;&gt;Macy’s&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Mastercard&quot;&gt;Mastercard&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Stripe,_Inc.&quot;&gt;Stripe&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Home_Depot&quot;&gt;The Home Depot&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Visa_Inc.&quot;&gt;Visa&lt;/a&gt;&quot;&amp;nbsp;以及&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Zalando&quot;&gt;Zalando&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在未来规划方面，UCP 的路线图着眼于构建一个覆盖全球、超越单笔交易的统一商业标准。相关计划包括多商品结算、购物车管理、会员与忠诚度计划、订单后流程，以及个性化的交叉销售与追加销售能力，同时继续确保核心商业逻辑掌握在企业自身手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，UCP 的早期版本已支持美国部分符合条件的零售商在 Google 产品界面内直接完成结算。接下来，该协议还将拓展至印度、印度尼西亚以及拉丁美洲等市场。Google 与合作伙伴也正在广泛&lt;a href=&quot;http://github.com/Universal-Commerce-Protocol/ucp?tab=contributing-ov-file&quot;&gt;征求反馈&lt;/a&gt;&quot;，以不断完善这一协议，共同塑造可互操作、由 AI 驱动的未来商业形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-agentic-commerce-ucp/&quot;&gt;https://www.infoq.com/news/2026/01/google-agentic-commerce-ucp/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相关报道：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9&quot;&gt;“商业版 HTTP”来了：谷歌 CEO 劈柴官宣 UCP，Agent 直接下单，倒逼淘宝京东“拆家式重构”？&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Rqjcx2J5ZbOSjlNJ9BUz</link><guid isPermaLink="false">https://www.infoq.cn/article/Rqjcx2J5ZbOSjlNJ9BUz</guid><pubDate>Wed, 21 Jan 2026 03:23:42 GMT</pubDate><author>作者：Leela Kumili</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>Agent 不再各自为战：GitLab Duo 构建可编排的研发智能体系</title><description>&lt;p&gt;&lt;a href=&quot;https://about.gitlab.com/releases/2026/01/15/gitlab-18-8-released/#gitlab-duo-agent-platform-now-generally-available&quot;&gt;GitLab 18.8&lt;/a&gt;&quot;&amp;nbsp;带来多项新功能，包括 GitLab Duo Planner Agent、GitLab Duo Security Analyst Agent、自动忽略不相关漏洞等。随着本次发布，用于帮助组织统一编排 AI 代理的 GitLab Duo Agent Platform 正式达到全面可用（General Availability，GA）状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;GitLab 表示，GitLab Duo Agent Platform 能够帮助团队在软件开发的各个阶段协同 AI 代理，从规划、构建到安全防护和最终交付。GitLab 认为，如果 AI 只停留在写代码阶段，价值依然有限。Duo Agent Platform 的思路，是让 AI 参与到整个研发流程中，帮助团队应对代码评审积压、安全漏洞、合规检查以及后续修复等现实问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现这一目标，GitLab Duo Agent Platform 将代理式聊天（agentic chat）、面向具体任务的代理、工作流自动化以及企业级管控能力整合在一起，使组织能够在整个软件开发生命周期中部署和管理 AI。&lt;/p&gt;&lt;p&gt;该平台提供了一个集中的 AI Catalog，团队可以在其中发现、管理并在组织内部共享各类代理和流程。内置的基础代理（如 Planner、Security Analyst 和 Data Analyst）可在关键决策节点处理结构化工作；同时，可定制的流程能够在开发工作流中自动执行多步骤的代理任务，覆盖从 Issue 到 Merge Request、CI/CD 迁移、流水线故障排查以及代码评审等多个场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者可以使用 Planner Agent 来创建、更新和分析 GitLab 中的工作项，例如进行待办事项分析、基于 RICE 或 MoSCoW 等框架进行优先级排序，并识别哪些问题需要人工直接介入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Security Analyst Agent 将漏洞管理从仪表盘和脚本中“解放”出来，工程师只需在 GitLab Duo Agentic Chat 中进行对话，就能完成漏洞分流、评估以及修复指导等工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过漏洞管理策略，安全团队可以自动忽略那些并不适用于自身组织的漏洞，从而减少噪音，让开发者专注于真正的安全风险。被自动忽略的漏洞会在 Merge Request 中被明确标注，并在漏洞报告中记录，以满足审计需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了上述由 AI 驱动的新能力之外，GitLab 18.8 还包含多项其他改进，例如升级后的 GitLab Runner、多容器扫描、集中式凭据管理 API 等。更多细节可参考 GitLab 官方发布公告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要指出的是，GitLab Duo Agent Platform 并非市场上唯一的代理编排平台，其竞争对手还包括&amp;nbsp;&lt;a href=&quot;https://www.infoq.com/news/2024/05/github-copilot-workspace-preview/&quot;&gt;Copilot Workspace&lt;/a&gt;&quot;、&lt;a href=&quot;https://cloud.google.com/gemini-enterprise?hl=en&quot;&gt;Google Gemini Enterprise&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.microsoft.com/en-us/microsoft-agent-365&quot;&gt;Microsoft Agent 365&lt;/a&gt;&quot;&amp;nbsp;等产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/gitlab-18-8-duo-agent-platform/&quot;&gt;https://www.infoq.com/news/2026/01/gitlab-18-8-duo-agent-platform/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7PsJIm4r5EKqUasq3xYt</link><guid isPermaLink="false">https://www.infoq.cn/article/7PsJIm4r5EKqUasq3xYt</guid><pubDate>Wed, 21 Jan 2026 03:16:40 GMT</pubDate><author>Sergio De Simone</author><category>AI&amp;大模型</category></item><item><title>Agent Skills 落地实战：拒绝“裸奔”，构建确定性与灵活性共存的混合架构</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;摘要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 Anthropic 开源 skills 仓库，&quot;Code Interpreter&quot;（代码解释器）模式成为 Agent 开发的热门方向。许多开发者试图采取激进路线：赋予 LLM 联网和 Python 执行权限，让其现场编写代码来解决一切问题。但在构建企业级“智能文档分析 Agent”的实践中，我们发现这种“全托管”模式在稳定性、安全性和可控性上存在巨大隐患。本文将分享我们如何摒弃激进路线，采用 Java (确定性 ETL) + DSL 封装式 Skills + 实时渲染 的混合架构，在保留 LLM 灵活性的同时，确保系统的工业级稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一、 背景：当文档分析遇到“复杂生成”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我们的“文档处理 Agent”项目中，基础的问答功能（RAG）已经解决得很好。但随着用户需求升级，我们面临了新的挑战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这是 2024 和 2025 年的两份经营数据报表，请对比 DAU 和营收的同比增长率，并生成一个 Excel 表格给我。另外，把总结报告导出为 PDF。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类需求包含两个特征：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;逻辑计算：需要精确算术（LLM 弱项）。文件 IO：需要生成物理文件（LLM 无法直接做到）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;引入 Skills（让 LLM 调用 Python 代码）似乎是唯一解。但在具体落地时，我们走了一段弯路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;二、 弯路：激进的“纯 Skills”路线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;起初，我们参考了开源社区做法，采用了 完全的 Code Interpreter 模式。我们将 requests、pandas、reportlab 等库的权限全部开放给 LLM，并在 Prompt 中告诉它：“你是一个 Python 专家，请自己写代码解决所有问题。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“裸奔”模式在生产环境中遭遇了三次暴击：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;输入端不可控：LLM 对非结构化数据（如无后缀 URL、加密 PDF）的处理极其脆弱，经常陷入报错死循环。输出端崩坏：让 LLM 从零绘制 PDF/Word 是灾难。经常出现中文乱码、表格对不齐、使用了过期的库 API 等问题。安全黑洞：数据流完全在沙箱内闭环，Java 主程序失去了对内容的控制权，无法拦截敏感词或违规数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;三、 变革：Java 主控 + DSL Skills 的混合架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决上述问题，我们重构了架构。核心思想是：收回 LLM 的“底层操作权”，只保留其“逻辑调度权”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们制定了新的架构分工：Java 负责确定性的数据流转与安检，LLM 负责意图理解与代码组装，Python 沙箱 负责在受控环境下执行具体计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.1 架构设计概览&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将系统重新划分为四个逻辑层级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ETL 层 (Java)：负责下载、MIME 识别、OCR、敏感词检测。这是“确定性管道”。Brain 层 (LLM)：负责阅读纯文本，进行逻辑推理，并生成调用代码。Skills 层 (Python Sandbox)：提供高度封装的 SDK（DSL），而非裸库。Delivery 层 (Java)：负责将 Markdown/HTML 实时渲染为 PDF/Word。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1f/1f672bd3d0df517c2c226761992e53f9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.2 输入侧：回归 Java 流水线 (ETL)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们不再让 LLM 去下载和解析文件。所有输入文件，先经过 Java 的 DocPipeline。利用 Apache Tika 进行精准解析，并立即进行敏感词检测和文本截断。这一步保证了喂给 LLM 的数据是干净、安全、标准化的纯文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.3 中间层：DSL 封装模式 (The Wrapper Pattern)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我们对 Skills 实践最大的改进。我们禁止 LLM 直接写 import pandas 进行底层操作，而是预置了一套高度封装的 DSL。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Python 端封装 (excel_tool.py)：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;import pandas as pd
import os

def create_excel(data_list, filename=&quot;report.xlsx&quot;, output_dir=&quot;/workspace&quot;):
    try:
        df = pd.DataFrame(data_list)
        save_path = os.path.join(output_dir, filename)

        # 【封装价值体现】自动处理格式、列宽、引擎兼容性，屏蔽 LLM 的幻觉风险
        with pd.ExcelWriter(save_path, engine=&#39;openpyxl&#39;) as writer:
            df.to_excel(writer, index=False, sheet_name=&#39;Sheet1&#39;)
            
            # 自动调整列宽 (LLM 很难写对的工程细节)
            worksheet = writer.sheets[&#39;Sheet1&#39;]
            for idx, col in enumerate(df.columns):
                max_len = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
                worksheet.column_dimensions[chr(65 + idx)].width = min(max_len, 50)
            
        return save_path
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skill 说明书 (SKILL.md)：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在 Prompt 中通过“接口契约”强行约束 LLM 的行为，明确了何时该写代码，何时该纯输出文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;markdown&quot;&gt;# File Generation Skill (Standardized)

你拥有生成专业格式文件（Excel, Word, PDF）的能力。
沙箱中已预装了封装好的 `excel_tool` 库。

**核心决策树**：
1. 如果是 **统计数据/表格** -&amp;gt; 必须生成 **Excel** -&amp;gt; **写 Python 代码**。
2. 如果是 **分析报告/文档** -&amp;gt; 必须生成 **Word/PDF** -&amp;gt; **禁止写代码**，走渲染路径。

---

### 场景 1：生成 Excel (.xlsx)
**规则**：禁止使用 `pandas` 底层 API，必须调用封装函数。
**数据结构**：必须是【字典列表】，每个字典代表一行。

**Python 调用示例**：
```python
import excel_tool

# 1. 准备数据 (从文档中提取)
data = [
    {&#39;年份&#39;: &#39;2024&#39;, &#39;DAU&#39;: 1000, &#39;营收&#39;: &#39;500万&#39;},
    {&#39;年份&#39;: &#39;2025&#39;, &#39;DAU&#39;: 1500, &#39;营收&#39;: &#39;800万&#39;}
]

# 2. 调用封装函数 (自动处理样式、列宽)
excel_tool.create_excel(data, filename=&#39;analysis.xlsx&#39;)
```

---

### 场景 2：生成 Word / PDF (.docx / .pdf)
**规则**：**严禁编写 Python 代码**（如 `reportlab` 或 `python-docx`）。
**执行动作**：
1. 请直接输出内容丰富、排版精美的 **Markdown** 文本。
2. 在 Markdown 的**最后一行**，务必添加对应的动作标签，系统会自动将其渲染为文件。

**输出示例**：
# 2024 年度经营分析报告

## 一、 数据概览
本季度营收同比增长 20%...

| 指标 | Q1 | Q2 |
| :--- | :--- | :--- |
| DAU | 100w | 120w |

... (此处省略 2000 字内容) ...

&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;
&lt;/action:convert|pdf&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.4 输出侧：渲染与交付的分离&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于不同类型的文件，我们采取了截然不同的交付策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Excel（强结构化）：走 Skills 路线。LLM 组装数据 -&amp;gt; 调用 excel_tool -&amp;gt; 沙箱生成物理文件。Word/PDF（富文本）：走 渲染路线。严禁 LLM 写代码生成。LLM 只输出高质量的 Markdown 并在末尾打上 &amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt; 标签。Java 后端拦截该标签，利用 OpenHTMLtoPDF 或 Pandoc 将 Markdown 实时转换 为精美的 PDF/Word。&lt;/action:convert|pdf&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;四、 硬核代码实现 (Spring AI)&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是我们在 Spring AI 体系下实现这套混合架构的关键逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.1 动态技能注入 (SkillManager)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们实现了一个 SkillManager，支持按需加载技能。为了提升性能，我们设计了 Session 级的“防抖机制”，确保同一个会话中只需上传一次 Python 脚本，避免重复 IO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Service
public class SkillManager {
    // 缓存技能脚本: 技能名 -&amp;gt; { 文件路径 -&amp;gt; 内容 }
    private final Map&lt;string, map&lt;string,=&quot;&quot; string=&quot;&quot;&gt;&amp;gt; skillScripts = new ConcurrentHashMap&amp;lt;&amp;gt;();
    // 防止重复注入的防抖 Set
    private final Set&lt;string&gt; injectedSessions = ConcurrentHashMap.newKeySet();

    /**
     * 核心逻辑：根据需要的技能列表，动态注入脚本到沙箱
     */
    public void injectToSandbox(String sessionId, List&lt;string&gt; neededSkills) {
        // 1. 防抖检查：如果该 Session 已注入，直接跳过，避免重复 IO
        if (injectedSessions.contains(sessionId)) return;

        // 2. 注入 Python 包结构 (__init__.py)
        sandboxService.uploadFile(sessionId, &quot;/workspace/skills/__init__.py&quot;, &quot;&quot;);

        // 3. 批量上传该技能所需的 DSL 脚本
        for (String skillName : neededSkills) {
            Map&lt;string, string=&quot;&quot;&gt; scripts = skillScripts.get(skillName);
            if (scripts != null) {
                scripts.forEach((path, content) -&amp;gt; 
                    sandboxService.uploadFile(sessionId, path, content)
                );
            }
        }
        injectedSessions.add(sessionId);
    }
    
    // ... 省略加载 Resource 的代码 ...
}
&lt;/string,&gt;&lt;/string&gt;&lt;/string&gt;&lt;/string,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.2 业务调度与意图分流 (Handler)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;串联 Java ETL、LLM 推理和最终的交付分流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Service
public class DocumentAnalysisRequestHandler {

    public Flowable&lt;response&gt; processStreamingRequest(Request req) {
        // 1. 【Java ETL】确定性解析与安检
        // 无论 URL 还是文件，先转为纯文本，并做敏感词过滤
        List&lt;parseresult&gt; parsedDocs = etlPipeline.process(req.getUrls());
        
        // 2. 【技能注入】
        List&lt;string&gt; neededSkills = List.of(&quot;file_generation&quot;);
        skillManager.injectToSandbox(req.getSessionId(), neededSkills);

        // 3. 【LLM 执行】Context Stuffing
        String prompt = buildPrompt(parsedDocs, skillManager.getPrompts(neededSkills));
        
        // 调用 LLM，挂载 ToolContext 以实现多租户隔离
        Flowable&lt;agentoutput&gt; agentFlow = chatClient.prompt()
                .system(prompt)
                .user(req.getUserInstruction())
                .toolContext(Map.of(&quot;projectId&quot;, req.getSessionId())) 
                .stream()
                .content();

        // 4. 【结果分流】
        return agentFlow
                .toList() // 收集完整回复
                .flatMap(this::handlePostGenerationAction);
    }

    /**
     * 核心分流逻辑：决定是返回沙箱文件(Excel) 还是 调用Java渲染(PDF)
     */
    private Single&lt;agentoutput&gt; handlePostGenerationAction(List&lt;string&gt; rawChunks) {
        String text = String.join(&quot;&quot;, rawChunks);

        // 分支 A：检测到 Python 生成了 Excel (Skills 产物)
        // 格式：[FILE_GENERATED: /workspace/report.xlsx]
        if (FILE_GENERATED_PATTERN.matcher(text).find()) {
            String path = extractPath(text);
            return Single.just(new AgentOutput(path, OutputType.FILE));
        }

        // 分支 B：检测到转换指令 (渲染产物)
        // 格式：&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;
        if (text.contains(&quot;&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;&quot;)) {
            // Java 侧实时渲染：Markdown -&amp;gt; PDF
            // 优势：完美控制字体和样式，避免 Python 生成乱码
            String pdfPath = docConverterService.convertAndSave(text, &quot;pdf&quot;);
            return Single.just(new AgentOutput(pdfPath, OutputType.FILE));
        }

        // 分支 C：普通文本
        return Single.just(new AgentOutput(text, OutputType.TEXT));
    }
}
&lt;/action:convert|pdf&gt;&lt;/action:convert|pdf&gt;&lt;/string&gt;&lt;/agentoutput&gt;&lt;/agentoutput&gt;&lt;/string&gt;&lt;/parseresult&gt;&lt;/response&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbfaaffa67b67e9643771a9f5640c283.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.3 拦截与交付 (SandboxTools)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Tool 执行层做最后一道防线：输出内容的二次安检。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Component
public class SandboxTools {

    @Tool(name = &quot;execute_command&quot;, description = &quot;在沙箱中执行 Shell 命令&quot;)
    public String executeCommand(ExecuteCommandRequest req, ToolContext context) {
        String projectId = (String) context.getContext().get(&quot;projectId&quot;);
        
        try {
            // 1. 执行 Python 脚本
            Map&lt;string, object=&quot;&quot;&gt; result = sandboxMcpService.executeCommand(projectId, req.command());
            String stdout = (String) result.get(&quot;stdout&quot;);

            // 2. 【关键】输出侧安检
            // 防止 LLM 通过代码计算出违规内容，绕过输入侧检查
            if (banwordService.hasBanWords(stdout)) {
                log.warn(&quot;Banword detected in sandbox output!&quot;);
                throw new BanwordException(&quot;敏感内容阻断&quot;);
            }

            // 3. 超长截断 (防止 LLM 上下文爆炸)
            if (stdout.length() &amp;gt; MAX_TEXT_LENGTH) {
                return stdout.substring(0, MAX_TEXT_LENGTH) + &quot;\n[SYSTEM: TRUNCATED]&quot;;
            }

            return stdout;
        } catch (Exception e) {
            return &quot;Execution Error: &quot; + e.getMessage();
        }
    }
}
&lt;/string,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;五、 总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skills 技术让 LLM 拥有了“手”，但这双手必须戴上“手套”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这次架构演进，我们得出的核心经验是：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不要高估 LLM 的 Coding 能力：它是一个优秀的逻辑推理引擎，但在工程细节（排版、库依赖、环境配置）上非常糟糕。DSL 封装是必须的。不要丢掉 Java 的确定性：解析、下载、格式转换、安全检查，这些传统代码擅长的领域，不要交给概率性的 LLM 去做。架构分层：Input: Java (Standardization &amp;amp; Security)Thinking: LLM (Reasoning)Action: Python (Calculation via DSL)Output: Java (Rendering &amp;amp; Delivery)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种混合架构，既保留了 Agent 处理复杂动态需求的能力（如自定义计算涨跌幅），又守住了企业级应用对稳定性与合规性的底线。&lt;/p&gt;</description><link>https://www.infoq.cn/article/MUo1faBPQqOwxVDWA9vB</link><guid isPermaLink="false">https://www.infoq.cn/article/MUo1faBPQqOwxVDWA9vB</guid><pubDate>Wed, 21 Jan 2026 02:42:14 GMT</pubDate><author>仇智慧</author><category>生成式 AI</category></item><item><title>Pinterest的Moka：Kubernetes如何重写大数据处理规则</title><description>&lt;p&gt;数字公告板提供商Pinterest发布了一篇文章，&lt;a href=&quot;https://medium.com/pinterest-engineering/next-gen-data-processing-at-massive-scale-at-pinterest-with-moka-part-2-of-2-d0210ded34e0&quot;&gt;解释了其新平台Moka在大规模数据处理方面的未来蓝图&lt;/a&gt;&quot;。该公司正在将核心工作负载从老化的Hadoop基础设施迁移到基于Kubernetes的系统上，该系统运行在亚马逊EKS上，以Apache Spark作为主要引擎，并即将支持其他框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一个包含两篇文章的博客系列中，Soam Acharya、Rainie Li、William Tom和Ang Zhang描述了Pinterest大数据平台团队如何考虑下一代大规模数据处理平台的替代方案，因为现有的基于Hadoop的系统（内部称为Monarch）的局限性变得越来越明显。他们将Moka作为搜索的结果，以及基于EKS的云原生数据处理平台，该平台现在运行的生产负载达到了Pinterest的规模。&lt;a href=&quot;https://www.infoq.com/news/2025/07/pinterest-spark-kubernetes/&quot;&gt;该系列的第一部分关注整体设计和应用层&lt;/a&gt;&quot;。相比之下，第二部分转向作者所说的“Moka的基础设施重点方面，包括经验和未来方向”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0a/0adbf9dae170c8d0b8fd96d9f0c2d79f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章从实际角度描述了向Kubernetes的转变。它展示了一个全行业的转变，即大型技术公司现在将Kubernetes视为数据的控制平面，而不仅仅是无状态的服务平台。在大数据社区日益增长的受欢迎程度和越来越多的采用的鼓励下，团队探索了基于Kubernetes的系统，作为Hadoop 2.x最有可能的替代品。任何候选平台都必须满足可扩展性、安全性、成本以及托管多个处理引擎的精确标准。Moka是如何在不放弃现有Spark投资的情况下现代化Hadoop时代的数据平台的一个例子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二篇文章的核心主题是如何在Kubernetes上以非常大的规模运行Spark。作者解释了他们如何围绕Moka添加日志、指标和作业历史服务，以便工程师可以在不了解底层集群拓扑的情况下调试和调整作业。他们使用Fluent Bit对日志集合进行标准化，并使用OpenTelemetry和Prometheus兼容的端点发布统一指标。这为基础设施和应用程序团队提供了系统健康的一致视图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pinterest还投资于通过基础设施即代码的方式使平台可重复使用。在文章中，团队概述了他们如何使用Terraform和Helm创建EKS集群、配置网络和安全以及部署支持组件，如Spark历史服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pinterest的工程师还讨论了处理不同的硬件架构。他们描述了他们如何构建多架构镜像，以便他们的数据工作负载在Intel和基于ARM的实例上运行良好，包括AWS Graviton，并将此与集群规模的成本和效率目标联系起来。InfoQ编辑Eran Stiller在LinkedIn&lt;a href=&quot;https://www.linkedin.com/posts/estiller_from-hadoop-to-kubernetes-pinterests-scalable-activity-7356213210661744640-17Ee/&quot;&gt;上对该项目中的总结指出&lt;/a&gt;&quot;，Moka“提供了容器级别的隔离、ARM支持、YuniKorn调度，并通过整合工作负载和跨实例类型的自动扩展实现了显著的成本节省”。这些细节将工作置于云用户寻求在不牺牲性能的情况下削减基础设施成本的更大趋势之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于处理引擎的更广泛的行业对话为Pinterest的故事增添了细微差别。在另一篇&lt;a href=&quot;https://www.linkedin.com/posts/soamacharya_next-gen-data-processing-at-massive-scale-activity-7350931757694640128-vCg0/&quot;&gt;LinkedIn&lt;/a&gt;&quot;帖子中，Acharya写道：“虽然Spark是我们的主要主力，但Moka的成功意味着Pinterest的其他用例也在效仿：Flink Batch已经投入生产，Apache Ray紧随其后，Flink Streaming也将在今年晚些时候推出”。通过对Spark和Flink技术的深入探讨，我们可以了解到这一点的重要性。强调Spark仍然非常适合大型批处理和交互式分析工作负载，而Flink是“为实时、有状态的流处理而构建的”，具有严格的逐事件处理。团队将Moka呈现为一个灵活的基础，可以根据特定工作负载的需求添加不同的引擎，而不是一个只支持spark的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;外部观察者从Pinterest案例中吸取了教训。&lt;a href=&quot;https://ml.blaze.email/archive/ml-engineering-newsletter-5558/&quot;&gt;ML工程师&lt;/a&gt;&quot;通讯将Moka文章描述为“在Kubernetes上部署EKS集群、Fluent Bit日志、OTEL指标管道、镜像管理和Spark的自定义Moka UI”的例子，将其与其他现代数据基础设施案例研究并列。这些反应表明，Moka被视为一类云原生数据系统的参考架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，团队确实将他们的迁移工作呈现为一个正在进行的旅程，而不是一个已经完成的项目。在博客和进一步的&lt;a href=&quot;https://www.linkedin.com/posts/pinterest-engineering_next-gen-data-processing-at-massive-scale-activity-7373833079498371072-EREu/&quot;&gt;LinkedIn&lt;/a&gt;&quot;帖子中，Pinterest作者讨论了“经验和未来的方向”，并描述了早期概念验证如何导致随着对新堆栈的信心增长而逐步远离Hadoop的迁移。Acharya指出，“最好的问题出现在规模上”，构建平台涉及“解决难题”，因为团队转移了实际工作负载。对于其他组织来说，这种经验可能是最重要的教训。复制围绕Kubernetes、EKS和Spark的技术选择相对简单，但从遗留系统中解耦并投资于可观测性、自动化和多引擎支持的过程可能是未来真正的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/pinterest-kubernetes-bigdata/&quot;&gt;https://www.infoq.com/news/2026/01/pinterest-kubernetes-bigdata/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/DksL1xE1iKPIIzqOorEw</link><guid isPermaLink="false">https://www.infoq.cn/article/DksL1xE1iKPIIzqOorEw</guid><pubDate>Wed, 21 Jan 2026 01:44:32 GMT</pubDate><author>Tim Anderson</author><category>云计算</category><category>大数据</category></item><item><title>模力工场 029 周 AI 应用榜：AI 生图文字不再“开盲盒”，GLM-Image 凭精准登顶榜首！</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260120infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场作为官方生态合作伙伴，诚邀您共赴产业前沿盛会——「逐梦 AI ·天使筑基」2026 中关村早期投资论坛暨 AI 新场景产业创新大会。本次大会汇聚政策、资本与产业领袖，深度聚焦机器人、智能体、大模型应用等前沿赛道，共同把脉 AI 趋势、破解落地难题，为您提供决策的一手洞察。1月28日，北京中关村，期待与您共筑未来！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/b0/56/b06d552a0853ba4148d32160df9fb556.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;029 周榜单总介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260120infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 029 周 AI 应用榜来袭！本周共有 23 款应用上架，我们从榜单中精选出十款代表性应用与大家分享。本期榜单应用多为近期热门或美国 CES 参展应用，整体呈现“软硬结合、多领域并进”的特点，涵盖大模型应用、智能硬件、生活方式工具及 AI 基础设施等多个方向。从中可以看出，当前 AI 应用正朝着更实用、更集成、更富交互感的方向演进，硬件创新与场景化服务正成为推动 AI 走向普及的关键动力。以下为本周精选的十款应用简介：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/glmimage?utm_source=20260120infoQ&quot;&gt;GLM-Image&lt;/a&gt;&quot;（智谱 AI）: 图像设计、AI Infra 类，开源图像生成模型&lt;a href=&quot;https://agicamp.com/products/qwenx?utm_source=20260120infoQ&quot;&gt;千问App&lt;/a&gt;&quot;: AI 搜索问答、生活方式类, 阿里最强模型官方 AI 助手&lt;a href=&quot;https://agicamp.com/products/rayneo?utm_source=20260120infoQ&quot;&gt;雷鸟 AI 眼镜&lt;/a&gt;&quot;（RayNeo）: AI 硬件类，想象万千，终于一见&lt;a href=&quot;https://agicamp.com/products/immo?utm_source=20260120infoQ&quot;&gt;影目 GO3&lt;/a&gt;&quot; （IMMO）: AI 硬件类，AI 眼镜美学标杆&lt;a href=&quot;https://agicamp.com/products/Lynxring?utm_source=20260120infoQ&quot;&gt;Lynx Ring&lt;/a&gt;&quot;（云康宝）: AI 硬件类，小巧智能戒指，24 小时健康监测随身管理&lt;a href=&quot;https://agicamp.com/products/Boujour?utm_source=20260120infoQ&quot;&gt;Bonjour 数字名片&lt;/a&gt;&quot;: 生活方式类，Bonjour！创意工作者的 Portfolio&lt;a href=&quot;https://agicamp.com/products/agibot?utm_source=20260120infoQ&quot;&gt;智元机器人 AgiBot A2&lt;/a&gt;&quot;: AI 硬件类，业内首个规模化商用的全尺寸人形机器人&lt;a href=&quot;https://agicamp.com/products/loona?utm_source=20260120infoQ&quot;&gt;Loona（可以科技）&lt;/a&gt;&quot;: AI 硬件类，具备情感反馈的家庭 AI 宠物陪伴机器人&lt;a href=&quot;https://agicamp.com/products/xh-lanyun?utm_source=20260120infoQ&quot;&gt;蓝耘星河&lt;/a&gt;&quot;: AI Infra、新媒体创作、营销增长类，蓝耘星河以智能，驱动增长&lt;a href=&quot;https://agicamp.com/products/Tunee&quot;&gt;Tunee&lt;/a&gt;&quot;: AI Agent、音频语音、视频多媒体类，Tunee！The smartest AI music agent.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周必试应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/glmimage?utm_source=20260120infoQ&quot;&gt;GLM-Image&lt;/a&gt;&quot;（智谱 AI）&lt;/p&gt;&lt;p&gt;关键词：开源图像生成模型 ｜ 复杂视觉文本生成 ｜ 长文本渲染&lt;/p&gt;&lt;p&gt;模力小A推荐：GLM-Image 在中文长文本准确性与小字脚注生成上表现突出，尤其适合法律文书、产品说明等对文字保真度要求极高的场景。此外，其价格仅为 Nano Banana Pro 的一半左右，性价比显著。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上榜冷门但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/qwenx?utm_source=20260120infoQ&quot;&gt;千问App&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：阿里官方出品｜多场景智能问答｜搜索增强｜生活助手&lt;/p&gt;&lt;p&gt;模力小A推荐：如果说之前的千问还是一位“聊天伙伴”，那么现在的它，已经进化成了能真正帮你“办事”的智能管家。随着1月15日新版本的发布，千问 App 全面接入了淘宝、支付宝、高德等阿里核心生态，这意味着你可以直接通过和千问对话完成点外卖、买机票、订酒店等一系列操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本周的 AI 趋势呈现出清晰的双线演进：软件正变得更深、更实用，而硬件则在变得更轻、更自然。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件：从“能说会道”到“能干实事”&lt;/p&gt;&lt;p&gt;近期两个标志性进展值得关注。其一，GLM-Image 登顶 Hugging Face 榜单，证明了国产模型能在专业场景（如法律文书、产品说明）中精准生成文本和图像，同时还具备显著的成本优势，让专业级 AI 工具变得触手可及。其二，千问 App 全面接入阿里生态，意味着 AI 已从单纯的“问答对话”进阶到“办事调度”阶段——用户可以通过自然对话直接完成点外卖、订机票等操作。AI 正从一个聊天对象，转变为串联现实服务的智能中枢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硬件：从“技术秀场”到“场景适配”&lt;/p&gt;&lt;p&gt;从 CES 的趋势来看，AI 硬件近期正在褪去“极客玩具”的标签，转向更务实的设计哲学：不刻意刷存在感，但需要时总在身边。&lt;/p&gt;&lt;p&gt;入口更轻了：新一代 AR 眼镜（如雷鸟、影目）不再追求取代手机，而是通过更轻巧的设计，专注做好“信息提示”“即时导航”这类“抬头即用”的场景，成为生活中的“第二块屏幕”。类似地，像 Loona DeskMate 这样的产品，让用户闲置的旧手机成为桌面机器人的“面孔”，以几乎零成本的方式，把熟悉的设备变成了桌面上可互动、可陪伴的 AI 伙伴。陪伴更久了：以智能戒指为代表的健康设备，正变得像首饰一样无感佩戴。竞争的关键不再是“能测多少项”，而是能否让用户愿意长期佩戴，从而获得持续、有价值的健康数据。同样，人形机器人（如智元 AgiBot A2）也迈入了新阶段：能量产了。接下来的核心问题，是它能在工厂、商场等具体场景中解决什么实际工作，创造什么经济价值。表达更活了：AI 也开始赋能个人形象展示。像 Bonjour 数字名片这样的工具，让个人主页从静态的“电子名片”变成了可动态展示作品、风格乃至个性的“互动橱窗”，帮助用户在社交与职场中更生动地呈现自己。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体而言，当前 AI 的发展更加注重与真实场景、既有习惯的衔接。无论是软件的能力延伸，还是硬件的形态演进，都体现出同一种思路：在用户需要时提供恰到好处的支持，而非刻意强调技术本身的存在。或许只有当技术彻底融入行为日常，才是其真正成熟的标志。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后再介绍一下模力工场的上榜机制和加入榜单的参与方式，欢迎大家继续积极参与提交 AI 应用：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模力工场 AI 应用榜并非依靠“点赞刷榜”，而是参考以下权重维度：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;评论数（核心指标，代表社区真实反馈）收藏与点赞（次级指标）推荐人贡献（注册推荐人可直接为好应用打 Call）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;加入榜单的参与方式：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你是开发者：上传你的 AI 应用，描述使用场景与核心亮点；如果你是推荐人：发现好工具，发布推荐理由；如果你是用户：关注榜单，评论互动，影响榜单权重，贡献真实声音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One More Thing，对于所有在模力工场上发布的 AI 应用，极客邦科技会借助旗下各品牌资源进行传播，短时间内触达千万级技术决策者与开发者、AI 用户：&lt;/p&gt;&lt;p&gt;InfoQ 全媒体矩阵AI 前线全媒体矩阵极客时间全媒体矩阵TGO 鲲鹏会全媒体矩阵霍太稳视频号&lt;/p&gt;</description><link>https://www.infoq.cn/article/1CO7VkWwPjB94w95xxfF</link><guid isPermaLink="false">https://www.infoq.cn/article/1CO7VkWwPjB94w95xxfF</guid><pubDate>Tue, 20 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>MAKE IT SNOW ｜ 2025-2026 Data+AI 年度时刻</title><description>&lt;p&gt;&lt;strong&gt;嘉宾介绍：&lt;/strong&gt;&lt;br&gt;
杨    扬   Snowflake 亚太及日本地区解决方案工程副总裁&lt;br&gt;
郭多娇   Snowflake 中国区市场总经理&lt;br&gt;
王一鹏   InfoQ  极客传媒总经理、总编辑&lt;br&gt;
郭    炜   白鲸开源 CEO&lt;br&gt;
史少锋    Datastrato VP of Engineering&lt;br&gt;
李   飞    数势科技 AI 负责人&lt;br&gt;
朱亦非    罗氏中国 Snowflake 数据平台技术负责人&lt;br&gt;
陈砚琳    西门子数据分析师&lt;br&gt;
高   杰    蔚来汽车人工智能研发负责人 &amp;amp; 高级总监&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;内容简介：&lt;/strong&gt;&lt;br&gt;
InfoQ 携手 Snowflake，邀请数位来自智能制造、智慧医疗、智驾等行业的全球头部企业的数智化专家和大数据专家围炉对谈，深度复盘 2025 年三大认知突破，围绕 Data 与 AI 战略发起“年度十问”，共同见证企业数智化变革的关键时刻。&lt;/p&gt;
</description><link>https://www.infoq.cn/article/uqAQNcO0Ct5oJxoC1ARK</link><guid isPermaLink="false">https://www.infoq.cn/article/uqAQNcO0Ct5oJxoC1ARK</guid><pubDate>Tue, 20 Jan 2026 11:11:39 GMT</pubDate><author>王玮</author><category>企业动态</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>刚刚，基于Grok的X推荐算法开源！专家：ROI 过低，其它平台不一定跟</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;时隔近三年，马斯克再次开源X推荐算法&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚，X 工程团队在X上发帖宣布，正式开源X推荐算法，据介绍，这个开源库包含为 X 上的“为你推荐”信息流提供支持的核心推荐系统，它将网络内内容（来自用户关注的帐户）与网络外内容（通过基于机器学习的检索发现）相结合，并使用基于 Grok 的 Transformer 模型对所有内容进行排名，也就是说，该算法采用了与 Grok 相同的Transformer 架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开源地址：&lt;a href=&quot;https://x.com/XEng/status/2013471689087086804&quot;&gt;https://x.com/XEng/status/2013471689087086804&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e05f1688362e07e6ea6ad9cc07a62290.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;X 的推荐算法负责生成用户在主界面看到的“为你推荐”（For You Feed）内容。它从两个主要来源获取候选帖子：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你关注的账号（In-Network / Thunder）平台上发现的其他帖子（Out-of-Network / Phoenix）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些候选内容随后被统一处理、过滤然后按相关性排序。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，算法核心架构与运行逻辑是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;算法先从两类来源抓取候选内容：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关注内的内容：来自你主动关注的账号发布的帖子。非关注内容：由系统在整个内容库中检索出的、可能你感兴趣的帖子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一阶段的目标是“把可能相关的帖子找出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;系统自动去除低质量、重复、违规或不合适的内容。例如：&lt;/p&gt;&lt;p&gt;已屏蔽账号的内容与用户明确不感兴趣的主题非法、过时或无效帖子&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样确保最终排序时只处理有价值的候选内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次开源的算法的核心是系统使用一个 Grok-based Transformer 模型（类似大型语言模型/深度学习网络）对每条候选帖子进行评分。Transformer 模型根据用户的历史行为（点赞、回复、转发、点击等）预测每种行为的概率。最后，将这些行为概率加权组合成一个综合得分，得分越高的帖子越有可能被推荐给用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一设计把传统手工提取特征的做法基本废除，改用端到端的学习方式预测用户兴趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1724e34ba728f55ce4ad205423b0ce46.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不是马斯克第一次开源X推荐算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早在2023年3月31日，正如马斯克收购Twitter 时承诺的那样，他已将 Twitter 部分源代码正式开源，其中包括在用户时间线中推荐推文的算法。开源当天，该项目在 GitHub 已收获 10k+ 颗Star。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，马斯克在 Twitter 上表示此次发布的是“大部分推荐算法”，其余的算法也将陆续开放。他还提到，希望“独立的第三方能够以合理的准确性确定 Twitter 可能向用户展示的内容”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在关于算法发布的 Space 讨论中，他说此次开源计划是想让 Twitter 成为“互联网上最透明的系统”，并让它像最知名也最成功的开源项目 Linux 一样健壮。“总体目标，就是让继续支持 Twitter 的用户们最大程度享受这里。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/85/85d48494461692b0c3a13e7225fdd9e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今距离马斯克初次开源X算法，过去了近三年的时间。而作为技术圈的超级KOL，马斯克早已为此次开源做足了的宣传。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月11日，马斯克在X上发帖称，将于 7 天内将新的 X 算法（包括用于确定向用户推荐哪些自然搜索内容和广告内容的所有代码）开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此流程将每 4 周重复一次，并附有详细的开发者说明，以帮助用户了解发生了哪些变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今天，他的承诺再次兑现了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9eb159196c1721b87fc4e3bee3768ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克为什么要开源？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当埃隆·马斯克再次提到“开源”时，外界的第一反应并非技术理想主义，而是现实压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去一年里，X因其内容分发机制屡次陷入争议。该平台被广泛批评在算法层面偏袒和助长右翼观点，这种倾向并非零星个案，而被认为具有系统性特征。去年发布的一份研究报告就指出，X 的推荐系统在政治内容传播上出现了明显的新偏见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，一些极端案例进一步放大了外界的质疑。去年，一段涉及美国右翼活动人士查理·柯克遇刺的未经审查视频在 X 平台迅速传播，引发舆论震动。批评者认为，这不仅暴露了平台审核机制的失效，也再次凸显了算法在“放大什么、不放大什么”上的隐性权力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这样的背景下，马斯克突然强调算法透明性，很难被简单解读为一次纯粹的技术决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d9/d9e7cb72307acd0f513610a4cdfbfefd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友怎么看？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;X推荐算法开源后，在X平台，有用户对推荐算法机制做了以下5点总结：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;1. 回复你的评论。算法对“回复+作者回应”的权重是点赞的75倍。不回复评论会严重影响曝光率。2. 链接会降低曝光率。应该把链接放在个人简介或置顶帖里，千万不要放在帖子正文中。3. 观看时长至关重要。如果他们滑动屏幕略过，你就不会吸引他们。视频/帖子之所以能获得高关注，是因为它们能让用户停下来。4. 坚守你的领域。“模拟集群”是真实存在的。如果你偏离了你的细分领域（加密货币、科技等），你将无法获得任何分销渠道。5. 屏蔽/默不作声会大幅降低你的分数。要有争议性，但不要令人讨厌。&amp;nbsp;简而言之：与你的受众沟通，建立关系，让用户留在应用内。其实很简单。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d0/d025a7d0c966f45b1a2850e960f470d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也有网友发现，虽然架构是开源的，但还有些内容仍未开源。该网友表示，此次发布本质上是一个框架，没有引擎。具体少了啥？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;缺少权重参数 - 代码确认“积极行为加分”和“消极行为扣分”，但与 2023 年版本不同的是，具体的数值被删除了。隐藏模型权重 - 不包含模型本身的内部参数和计算。未公开的训练数据 - 对于训练模型的数据、用户行为的采样方式，以及如何构建“好”样本与“坏”样本，我们一无所知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于普通X用户而言，X的算法开源并不会造成太大影响。但更高的透明度可以解释为什么有些帖子能获得曝光而另一些则无人问津，并使研究人员能够研究平台如何对内容进行排名。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么推荐系统是必争之地？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在大多数技术讨论中，推荐系统往往被视为后台工程的一部分，低调、复杂，却很少站在聚光灯下。但如果真正拆解互联网巨头的商业运转方式，会发现推荐系统并不是边缘模块，而是支撑整个商业模式的“基础设施级存在”。正因如此，它可以被称为互联网行业的“沉默巨兽”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公开数据已经反复印证了这一点。亚马逊曾披露，其平台约 35% 的购买行为直接来自推荐系统；Netflix 更为激进，约 80% 的观看时长由推荐算法驱动；YouTube 的情况同样类似，大约 70% 的观看来自推荐系统，尤其是信息流（feed）。至于 Meta，虽然从未给出明确比例，但其技术团队曾提到，公司内部计算集群中约 80% 的算力周期都用于服务推荐相关任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些数字意味着什么？如果将推荐系统从这些产品中移除，几乎等同于抽掉地基。就拿 Meta 来说，广告投放、用户停留时长、商业转化，几乎都建立在推荐系统之上。推荐系统不仅决定用户“看什么”，更直接决定平台“如何赚钱”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，正是这样一个决定生死的系统，长期面临着工程复杂度极高的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统推荐系统架构中，很难用一个统一模型覆盖所有场景。现实中的生产系统往往高度碎片化。以 Meta、LinkedIn、Netflix 这类公司为例，一个完整的推荐链路背后，通常同时运行着 30 个甚至更多专用模型：召回模型、粗排模型、精排模型、重排模型，各自针对不同目标函数和业务指标进行优化。每个模型背后，往往对应一个甚至多个团队，负责特征工程、训练、调参、上线与持续迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种模式的代价是显而易见的：工程复杂、维护成本高、跨任务协同困难。一旦有人提出“是否可以用一个模型解决多个推荐问题”，对整个系统而言，意味着复杂度的数量级下降。这正是行业长期渴望却难以实现的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大型语言模型的出现，给推荐系统提供了一条新的可能路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 已经在实践中证明，它可以成为极其强大的通用模型：在不同任务之间迁移能力强，随着数据规模和算力的扩展，性能还能持续提升。相比之下，传统推荐模型往往是“任务定制型”的，很难在多个场景之间共享能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，单一大模型带来的不仅是工程简化，还包括“交叉学习”的潜力。当同一个模型同时处理多个推荐任务时，不同任务之间的信号可以相互补充，随着数据规模增长，模型更容易整体进化。这正是推荐系统长期渴望、却很难通过传统方式实现的特性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 改变了什么？其实是改变了从特征工程到理解能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从方法论层面看，LLM 对推荐系统最大的改变，发生在“特征工程”这一核心环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统推荐系统中，工程师需要先人为构造大量信号：用户点击历史、停留时长、相似用户偏好、内容标签等，然后明确告诉模型“请基于这些特征做判断”。模型本身并不理解这些信号的语义，只是在数值空间中学习映射关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而引入语言模型后，这一流程被高度抽象。你不再需要逐条指定“看这个信号、忽略那个信号”，而是可以直接向模型描述问题本身：这是一个用户，这是一个内容；这个用户过去喜欢过类似内容，其他用户也对这个内容有正反馈——现在请判断，这条内容是否应该推荐给这个用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语言模型本身已经具备理解能力，它可以自行判断哪些信息是重要信号，如何综合这些信号做出决策。在某种意义上，它不只是执行推荐规则，而是在“理解推荐这件事”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种能力的来源，在于 LLM 在训练阶段接触过海量、多样化的数据，使其更容易捕捉细微但重要的模式。相比之下，传统推荐系统必须依赖工程师显式枚举这些模式，一旦遗漏，模型就无法感知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从后端视角看，这种变化并不陌生。就像你向 GPT 提问，它会基于上下文信息生成回答；同样地，当你问它“我是否会对这条内容感兴趣”，它也可以基于已有信息做出判断。某种程度上，语言模型本身已经天然具备“推荐”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;专家解读：工业界可参考，对学术价值不大&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果 X 的方向真是“让 Grok 成为算法本身”，那么这次开源事件的意义就不止是透明度提升，更像是把一场大模型化推荐的系统级改造公开摆到台前，接受开发者与行业的持续检视与解读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借此机会，我们邀请到了搜推广资深算法专家，生成式推荐模型OnePiece作者，《业务驱动的推荐系统：方法与实践》作者傅聪，为大家解读这次开源事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从代码层面看，X 这套推荐系统中，大模型是否是已经进入核心决策环节？这与传统“LLM + 规则 / 特征管道”的推荐系统相比，最大的结构性变化是什么？是否只是替换了部分模块？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：从系统整体设计层面看，开源的代码依然遵从 recall -&amp;gt; rank这样的多阶段漏斗筛选架构。新的post推送会从数亿 候选集合中 以传统的 双塔 向量召回，合并排序、去重等等环节，最后送给用户。grok没有参与中间过程，只是给post做排序的模型采用了类似grok 的模型架构，但远小于grok的参数量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最大的结构变化在于他们用了一种纯transformer（类grok）的模型结构去做排序，其它差异不大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从能力边界看，该如何看待“每日处理上亿条内容、并进行实时多模态理解”这一目标所带来的系统挑战？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：需要极其充足的GPU算力以及高并发的处理引擎，尤其是视频内容，其token消耗量巨大，因此计算量巨大。此外，模型还需要一个可以高速访问的大型文件系统，保证大量视频可以暂存、传递给Grok模型。而实际上x并没有真的让grok来做这个事情，应该是处于成本考虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：传统推荐系统采用轻量级启发式算法，成本效益高，而Grok方法需要大量计算资源，那么您怎么看待成本和用户体验提升之间的收益比？在算力、成本和基础设施约束下，这种方式是否注定只属于极少数平台？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：Grok消耗的算力是数千倍于传统的推荐系统的，这部分成本往往不能被平台的收益覆盖。尤其是X这样的平台，其收入核心来源是广告。只有做到延迟、体验都能对标原有系统，其广告收入才可以持平。但因为投入成本过高，这个ROI过低，目前来看只X自己也没有真的以这种规模使用grok。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：如果 Grok 真要“把帖子都读一遍、把视频都看一遍”再来做匹配，这是不是相当于把推荐系统推到了更强的“内容级监控”？平台不只是记你点过什么，还能在语义层面猜到你可能会被什么吸引，是否会带来新的以前没有的问题？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：Grok读过并不一定会记忆。很多数据并不一定会被Grok用来训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：另外，传统推荐系统的信息茧房问题，语义理解方式是否能解决？是否更“中立”？（此前的争议有一部分在于认为X平台偏向马斯克个人账号和一些党派言论）。从系统机制上看，它最可能在哪些环节反而更容易固化偏好、放大偏差？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：大语言模型有它自己的bias，以大语言模型为核心的推荐系统会根据它的语言偏好构建新的信息茧房。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从开源意义看，在推荐系统这种高度复杂、长期被视为“黑箱”的领域，这种“持续、周期性开源”代码的方式，实现起来的难度在哪里？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：难度在于只开源代码，不开源所有配套的系统和训练数据，就无法复现它的效果。这种开源，对学术研究价值不大，对工业交流有一定参考意义。但目前其架构来看，可参考的新东西不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：您如何看待这次开源的影响？如果 Grok 这套思路跑通，这次开源是否会迫使其他内容平台跟进，从而引发推荐系统的一轮“范式迁移”？在这种趋势下，行业会不会弱化对行为数据（包括历史数据）的依赖，甚至调整数据收集与画像方式，进而重塑整个推荐系统生态？对广告行为的影响会是什么样的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：即使Grok跑通，其它平台也不一定会跟进。第一其他平台没有属于自己的Grok，第二，其它大部分平台不会在这里投入这么多算力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;行业也不会弱化对用户行为和画像的依赖，经验证明，用户历史行为才是实现个性化的数据根基，缺少这部分信息输入的推荐系统很难千人千面，而容易做成千篇一律。从开源代码看，ranking模型依然在使用用户行为历史进行预测，这一点也符合预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;嘉宾简介：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪，搜推广资深算法专家、生成式推荐模型OnePiece作者，《业务驱动的推荐系统：方法与实践》作者，《生成式推荐系统算法与实践》作者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/xai-org/x-algorithm&quot;&gt;https://github.com/xai-org/x-algorithm&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/XEng/status/2013471689087086804&quot;&gt;https://x.com/XEng/status/2013471689087086804&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/BlockFlow_News/status/2013510113873813781&quot;&gt;https://x.com/BlockFlow_News/status/2013510113873813781&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2lb8A2IuImbvpMI1tR7D</link><guid isPermaLink="false">https://www.infoq.cn/article/2lb8A2IuImbvpMI1tR7D</guid><pubDate>Tue, 20 Jan 2026 10:30:00 GMT</pubDate><author>Tina,李冬梅</author><category>生成式 AI</category></item><item><title>当前关于 Vibe Engineering 的所有认知都会在 1 个月内严重过时</title><description>&lt;p&gt;过去几周，我对于 Vibe Engineering 的实践有了更多的体会, 今天再次总结一下。其实也能看出来我避免使用 Vibe Coding 这个词，是因为当下的重点已经不再是代码，而是一些更高维度的东西。另外，本文的 AI 含量我会尽量控制在 5% 内，可以放心阅读😄。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前我提到的我开始的 TiDB Postgres 重写项目已经不再在是个玩具。在前几天出差的路上, 因为长途飞行没有网络, 我仔细 review 了一下这个项目的代码, 虽然一些地方略有瑕疵, 但是总体质量已经很高, 我认为已经是接近生产水平的 rust 代码，和以前我理解中的早期原型的定义很不一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;顺便提一句, 我认为这个项目从一开始就选择 rust 是一个无比正确的决定, rust 的严谨性让 AI 能写出更接近 bug free 的 infra code (对比我另一个项目 agfs 的 shell 和它自带的脚本语言 ascript，由于这项目使用 python，项目变大后，可维护性就大大降低，但此时重写已经很困难，只能捏着鼻子慢慢重构)，所以现在已经是 2026 年了， 如果你要再启动一个新的 backend infra 项目, rust 应该是你的第一选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;验证差不多后，我也邀请了几位我团队内的几个顶尖的 vibe coder 加入项目, 看看 100% 的 AI Native 研发模式能在多快把这个项目推进到何种程度，无论如何都很想看看，应该会很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面说说自己最近的一些感受。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当前关于 Vibe Engineering 的所有的认知都会在 1 个月内严重过时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并非危言耸听，哪怕我正在写的这篇文章，如果你是 2026 年 2 月看到，那么很遗憾，本文聊到的东西很可能已经过时，这个领域发展的太快，很多今天的 SOTA 也许下个月就过时了。而且很有意思，过去很多对 Vibe Coding 嗤之以鼻的大佬，例如 DHH，Linus，Antirez 等，在 2025.12 月开始纷纷改口，我觉得这是相当正常的，去年 12 月开始，AI 编程工具和头部的模型突然有一个跳跃式的进步，突然对于复杂任务和大型项目的理解，以及写出代码的正确率有了极大的提升。这进步大概来自于两个方面：&lt;/p&gt;&lt;p&gt;一方面头部模型在长上下文（&amp;gt;256K) 的支持，尤其是关键信息的召回率提升惊人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如上面是 GPT-5.2 在长上下文的召回表现和 GPT-5.1 对比很明显，要知道对于 Agent Coding 的场景来说，通常是多轮次推理 + 长上下文（因为要放更多的代码和中间推理结果）才能更好的有大局观，大局观的正确是对于复杂项目起到决定性因素。在这种场景下，你可以做一个简单的计算，一个模型（类似 GPT-5.1) 每轮的召回率 50%，大概 3 轮后，正确的召回率就会降低到 12.5%, 而 GPT-5.2 仍然能保持 70% 以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外一个进步是主流的 Vibe Coding 工具的 Context Engineer 实践日益成熟，例如 Claude Code / Codex / OpenCode。从用户体验到最佳实践，肉眼可见的越来越好，例如对于 Bash 的使用，Subagent 等，这方面越来越多的资深 Engineer 的重度使用和经验分享会对这些工具的进化提供数据飞轮，尤其是 AI 也在深度的开发这些工具，迭代速度只会更快。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实这个进步也并不是去年 12 月那个时间点的突然什么黑科技爆发，其实前几个月一直在进步，不过还不能长时间离开人工干预，更像是那个时间点，主流 Coding Agent 的质量超过了一个临界点：100% 的无人工干预下完成长时间的 Agentic Loop 成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Hire the best (model)，否则就是在浪费生命&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面所有提到的进步，我个人感觉只反映在了最顶尖的闭源头部模型中。我听到很多朋友和我反馈到：“我感觉 AI 编程还是很傻啊？并没有你提到那么聪明”，我首先会反问，你是不是只是用着 $20 一个月那种入门模型？如果是的话，那先去用一阵 $200 以上的 Pro Max 档次的，也许有惊喜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我个人认为，目前主流的模型，即使并非头部那档，作为 chatbot 处理大多数普通人的短上下文的日常工作是完全足够的，哪怕是 GPT-4 在和你讲人生道理的时候也已经足够把你说得一愣一愣了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为人来说，我们的直觉或者是一些简单的 CRUD Demo 已经无法评估这些模型之间的智商差距了。但是在复杂的项目的开发中，这个差距是极端明显的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据我个人的实践来说，当下能用来进行大型 Infra 项目（数据库，操作系统，编译器等）开发的模型大概就两个：GPT-5.2 (xhigh) + Opus 4.5，还有半个算是 Gemini 3 Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大概上个月我主要用着 opencode + oh-my-opencode + Opus 4.5 但是最近两周转向到了 codex + gpt-5.2 的组合，下面分析一下这几个模型的一些脾气和调性，仅仅是个人感受，而且是在后端 Infra 软件开发这个领域，仅供参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Opus 4.5 的风格是速度很快，是个话唠，由于 Sonnet 4 有严重 reward hacking 问题，例如是在解决不了 bug 的时候会偷偷的构造作弊的测试然后糊弄过去，所以导致很长一段时间我都不太敢用 Sonnet 系列模型干复杂的事情，但是这点在 Opus 4.5 中解决得很好，即使在模型冥思苦各种尝试想都搞不定的情况下也没有选择作弊，让我放心不少，但是 Opus 的问题是 reasoning 和做 investigation 的时间太少，动手太快，以至于发现不对的时候，又返回头确认假设和研究，这样的特性催生了像 ralph-loop 这样的奇技淫巧。比方说，同样的一个 prompt 在 Claude Code 结束后又通过 stop hook 重新调用，再完整走一遍流程，不断地逼近最终的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，GPT-5.2 更像是一个更加小心谨慎、话不多的角色。我最开始用 Codex 的体验其实不算太好，因为我一直觉得它有点太慢了。主要是因为我习惯用它的 xhigh 深度思考模式，在真正开始写代码之前，它会花很长时间去浏览项目里的各种文件和文档，做很多准备工作。可能也是因为 Codex 的客户端不会告诉你它的计划和大概需要多久，所以就显得过程特别长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有时候一些复杂的任务，它前期的调查可能就要花上一到两个小时。但是经过长时间思考后它完成的效果通常是更好的，尤其是在一个项目的大体框架已经稳定，Codex 考虑得更周全，最终也体现出更少的 bug 和更好的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于第三个顶级模型，也就是 Gemini 3 Pro。虽然我也知道它的多模态能力非常吸引人，但就复杂任务的 Coding 场景而言，至少从我个人的体验来看，它的表现并没有 Opus 4.5 和 GPT-5.2 那么强。不过它确实针对一些快速的前端项目 Demo 和原型制作做了一些优化，再加上它的 Playground 模式，让你在需要一些炫酷的小 Demo 或前端项目时能更快实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实一个比较反直觉的事情是，过去我们经常说 Vibe Coding 只能搞一些比较简单的事情，比如上面那些小 Demo 或 CRUD 项目，你会看到网上各种各样的 KOL 其实都在做这种小原型，反而大家觉得对于一些像后端这种核心的基础设施代码，当前 AI 还是搞不定的。我以前也这么想，但从去年 12 月份开始，这个结论可能需要修正了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里面的原因是，其实这类基础设施的代码通常是由顶级工程师长期精雕细琢而成，它们有清晰的抽象、良好的测试，甚至代码本身经过多轮重构后也相当精炼。所以当 AI 具备足够的上下文空间 + 更好的推理能力 + 更成熟的 Agentic Loop + 高效的工具调用时，这类 Infra 代码的开发和维护反而是能最有效地利用这些顶尖大模型的智商的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际的工作中，我经常会让多个 Agent 互相协作，或者使用一些复杂的工作流来把它们编排在一起，并不会让一个模型来完成所有的事情。后面我会再分享一些我自己实践中的具体例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;人在什么时候进入？扮演什么角色？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面提到了，这些顶级模型再配合主流的 Vibe Coding 工具，基本上已经能超越大多数资深工程师的水平了。这不仅体现在能写出更少 bug 的代码，也体现在在 review 中能发现更多人类工程师可能看不到的问题，毕竟 AI 真的会一行一行仔细看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以人在这个过程中扮演什么样的角色，哪些阶段只有人才能做？根据我自己的实践来说，第一当然是提出需求，毕竟只有你才知道你想要啥，这很显然，但是有时确实也挺难的，毕竟人很难从一开始就准确描述自己想要什么，这时候我会用一个偷懒的办法：让 AI 来角色扮演，比方说，我在开发 PostgreSQL 版本的 TiDB 时，我就让 AI 假设自己是一个资深的 Postgres 用户，从开发者的视角告诉我有哪些特性是非常重要、一定要实现而且 ROI 比较高的，让它列出 N 个这样的功能点，然后 AI 就会根据它的理解生成一个需求列表，接下来你再和 AI 对这些需求逐个打磨，这其实是一个高效冷启动的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二是在需求提出后，现在的 Coding Agent 大多都会和你有一个规划阶段（Planning），会反复确认你的需求。在这个过程中其实有一些技巧，比如不要给 AI 太具体的方案，而是让 AI 来生成方案，你只需要关注最终你想要的结果；提前告诉 AI 有哪些基础设施和环境的问题，让它少走弯路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我通常会在提出需求的第一阶段就要求 Agent 做的一些关键动作。比如无论接下来做什么，都要把计划和 todo 列表放在一个 work.md 或 todo.md 这类文件里。还有，每完成一个阶段的工作，就把上一阶段的经验教训更新到 agents.md 里。第三点是当一个计划完成并且代码合并后，把这个工作的设计文档添加到项目的知识库中（.codex/knowledge）。这些都是我会在一开始提需求时就放进去的内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个阶段就是漫长的调查、研究和分析的阶段。这个阶段其实基本上不需要人做什么事情，而且 Agent 的效率比人高得多，你只需要等着就好。唯一需要注意的就是在 Research 的过程中，我通常会告诉模型它拥有无限的预算和时间，尽可能充分地进行调研。另外，如果你的模型有推理深度的参数的话，我建议在这个阶段把它们全部调到 xhigh 的级别。虽然这会让过程变慢，但在这个阶段多烧一些 token、做好更好的规划、了解更多上下文，对后续的实现阶段会更有帮助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实现阶段没什么特别好说的，反正我现在基本不会一行行去看 AI 的代码。我觉得在实现阶段唯一要注意的就是，要么你就让 AI 完全去做，要么你就完全自己做，千万别混着来，我目前是倾向于完全零人工干预的模式效果更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四个阶段人就变得非常重要了，那就是测试和验收结果的阶段。其实在我个人和 AI 开发项目的过程中，我 90% 的时间和精力都花在了这个阶段：也就是如何评估 AI 的工作成果，我觉得在 Vibe Coding 时：There&#39;s a test, there&#39;s a feature，你只要知道如何评估和测试你要的东西，AI 就一定能把东西给你做出来。另外值得注意的是，AI 在实现过程中会自动帮你添加很多单元测试，但说实话，这些单元测试在微观层面基本都能通过，毕竟 AI 写这种局部代码时已经很难出 bug。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 AI 不擅长的是集成测试、端到端测试。比如在开发一个 SQL 数据库时，哪怕每个细节的单元测试都没问题，但整合到一起时集成测试可能会出错。所以我在完成大目标前，我一定会先和 AI 一起做一个方便的集成测试框架，并提前准备好测试的基础设施，收集和生成一些现成集成测试的用例，尽量一键能运行那种，这样在开发阶段就能事半功倍，而且关于如何使用这些测试的基础设施的信息，我都会在正式开始前就固化在 agents.md 里，这样就不用每次沟通的时候都再告诉它该怎么测试了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于测试从哪来的问题，我自己的经验是你可以让 AI 帮你生成，但一定要告诉它一些生成的逻辑，标准和目的，另外就是千万不要把生成测试的 Context 和实际进行开发工作的 Agent 的 Context 混在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第五个阶段是重构和拆分。我发现当前的 Coding Agent 在面对单一模块复杂度超过大约 5 万行代码之后，就开始很难在 1-shot 里把问题一次性解决掉（但反过来这也意味着，只要任务复杂度控制在这个阈值之下，在一个足够好的 first prompt 驱动下，很多事情确实可以做到 1-shot AC），Agent 通常不会主动去做项目结构和模块边界的治理，你要它把功能做出来，它恨不得把所有东西都写进几个几万行的大文件里，短期看似很快，长期就是债务爆炸。我自己在这个阶段的做法通常是先停下来，用自己的经验进行模块拆分，然后在新的架构下进行 1～2 轮的重构，之后又可以高并发度的进行开发了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多 Agent 协同编程的一些实践&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面提到我现在使用 Coding Agent 的时候，通常不会只用一个，我自己的工作流会尽量让多个 Coding Agent 同时工作。这也是为什么有时候在一些项目上会花掉好几千美金，因为你必须把并发跑起来。当然，并发和吞吐是一方面，但另一方面我觉得让不同的 Agent 在不共享上下文的前提下互相 Review 工作，其实能显著提高质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就像在管理研发团队时，你不会让同一个人既当运动员又当裁判。相当于 Agent A 写的代码交给 Agent B 来 Review，往往能发现一些 A 看不到的问题。通过这样的循环往复，你就会更有信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，我在实际工作中现在用得比较好的一个工作流是这样的：首先让 GPT-5.2 在 Codex 下生成多个功能的设计文档，做出详细的设计和规划，第一阶段把这些规划文档都保存下来。然后在第二阶段，依然用 Codex 根据这些需求文档一个一个去实现功能。在实现的过程中，就像我前面提到的那样，记录 To-Do、经验教训，并在接近完成的时候，在代码通过测试并准备提交之前停下，把当前的工作区交给另一个 ClaudeCode 或 OpenCode，在不提供上下文的情况下，让 ClaudeCode 来 Review 当前还未提交的代码，根据设计提出修改建议。然后再把这些建议发回给 Codex，让 Codex 来评论这些建议，如果有道理就修改代码。改完之后，再让 ClaudeCode (Opus 4.5) 那边再次 Review，直到双方都觉得代码已经写得很不错了，再提交到 Git 上，标记这个任务完成，更新知识库，然后进入下一个功能的开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外在一个大型项目中我会同时开多个 Agent (in different Tmux) 并行开发多个功能，但我尽量让它们负责完全不同的模块。比如一个 Agent 修改内核代码，另一个 Agent 做前端界面，这样就能分开进行，如果你需要在一份代码上做一些彼此不太相关的工作时，可以利用 git 的 worktree 让多个 Agent 在不同的 git 分支上各自工作，这样也能快速提升吞吐量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;未来的软件公司和组织形态&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的软件公司会是什么形态呢？反正从我自己的实践和与一些朋友的交流来看，至少在当下，团队中用 Coding Agent 的 token 的消耗呈现出一个非常符合二八定律的分布，也就是说，最头部的用 AI 用得最好的工程师，他们消耗的 token 可能比剩下 80% 的工程师加起来还要多，而且 Coding Agent 对于不同工程师产出（质量，吞吐）的增益是不一样的，这个方差非常大，也就是对于用的最好的一群人，他们的增幅可能是 10x，但是普通人可能也就是 10%，而且唯一的瓶颈是人工的 code review 和一些无法被自动化的线上运维工作（我觉得也很快了）而且这样的特点能够让这些头部的工程师在 AI 的协助下可以无边界的工作，也就是会有越来越多的 one-man army 出现，只是目前我认为和 token 消耗是正相关的，你能花掉多少 token，大致代表你能做得多好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外我发现一个很有趣的现象，同样是 10x 的工程师，他们各自的 Vibe Coding 工作流和最佳实践其实并不相同。也就意味着，两个顶尖的 Vibe Coder 是很难在一个项目中（的同一个模块）协作。这种工作方式更像是头狼带着一群狼群（Agents），在一片自己的领地里面耕耘，但是同一片领地里很难容纳两匹头狼，会造成 1+1 &amp;lt; 2。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的组织形态下，我觉得传统意义上的“团队协作方式”会被重新定义。过去我们强调的是多人在同一个代码库、同一个模块里高频协作，通过评审、讨论、同步来达成共识；但在 Vibe Engineering 这种模式下，更有效的方式反而可能是强解耦的并行。管理者要做的是把问题切分成足够清晰、边界明确的“领地”，让每一个头部工程师带着自己的 Agent 群，在各自的领域里做到极致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从管理的角度看，这其实是一个挺大的挑战。因为你不能再用统一流程、统一节奏去约束所有人。对顶尖的 Vibe Coder 来说，过多的流程和同步反而会显著拉低效率，甚至抵消 AI 带来的增益。管理者更像是在做“资源调度”和“冲突隔离”：确保不同头狼之间尽量少互相干扰，同时在必要的时候，能够通过清晰的接口、契约和测试来完成协作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为上面的种种，AI-Native 的研发组织其实很难自底向上从一个非 AI-Native 的组织中生长出来，因为大多数开发者面对变革的时候的第一反应其实并不是拥抱，而是回避和抵触，但是时代的进步不会因为个人的意志转移，只有主动拥抱和被动拥抱的区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大概就写到这里吧，总的来说，在这样一个大环境下，对个人而言意味着一场深刻的转变，就像我之前在朋友圈里提到的，我身边最好的工程师们有一些已经陷入了或多或少的存在主义危机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但是作为具体的 Builder 的我来说是兴奋的，因为造物，在当下，门槛变低了许多，如果你能从造物中能获得成就感和找到人生的意义，那恭喜你，你活在一个最好的时代。但反过来，作为一个抽象的 “人” 来说，我又是悲观的，人类是否准备好面对这样的工具？以及这样工具带来的对于社会和整个人类文明的冲击？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我不知道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/k05gRzGFhz4QerCz4ARl</link><guid isPermaLink="false">https://www.infoq.cn/article/k05gRzGFhz4QerCz4ARl</guid><pubDate>Tue, 20 Jan 2026 10:13:12 GMT</pubDate><author>黄东旭</author><category>生成式 AI</category><category>数据库</category></item><item><title>OpenAI 广告续命遭全网骂，用户要跑路Gemini！需烧400 亿，18个月破产预警</title><description>&lt;p&gt;近日，OpenAI在其官方网站及官方社交媒体公告中表示，公司计划在“未来几周内”开始在ChatGPT对话界面中测试广告投放，这些广告将首先面向美国地区的免费版用户以及新推出的低价订阅层级“ChatGPT Go”用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;广告内容的展示形式预计主要是在ChatGPT生成的回答底部以清晰标注的独立模块形式出现，与AI生成内容严格区分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/1681e00cb156e94baedbc5de08e78cfa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI强调，广告不会影响ChatGPT的回答逻辑，也不会向广告商分享用户对话内容。付费订阅用户（如Plus、Pro、Business 及 Enterprise层级）仍将享受无广告体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据官方发布内容及多家外媒消息，OpenAI此举是为了进一步拓展营收来源，以缓解高昂的研发与基础设施支出压力，同时扩大服务的可持续性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司管理层表示，即便公司业务规模庞大，依靠订阅收入仍难覆盖巨额算力成本，而广告收入是补充营收的一种必要尝试。OpenAI同时承诺，广告不会改变AI应答过程，并且将在敏感话题如健康、政治等领域避免投放广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI此举引发了社区热议，但批评声音居多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Hacker News上，有用户表示，由于他们加了广告，很多用户已经转向了Gemini，所以长远来看这种行为是得不偿失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“OpenAI 广告的一个问题是，用户已经开始转向 Gemeni，而 Gemeni 并不投放广告。&amp;nbsp;ChatGPT 大多数情况下也比 Gemeni 差（或许如此），而且没有像 Gemeni 那样严格的速率限制。因此，他们已经开始流失用户，并且产品体验也比竞争对手更糟糕。&amp;nbsp;OpenAI 当然能从广告中赚到一些钱，但这能弥补他们巨额的亏损吗？我觉得不太可能。他们真的需要像微软那样，被一位财力雄厚的“金主”收购，才能玩转这种游戏。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/ab04c6eea33ee463df39c99f4c6de4b5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有用户表示即使他们加入了广告，也不会向谷歌和Facebook 那样赚大钱，只是赚一些小钱罢了。该用户评价道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“就我所了解的广告市场而言，像谷歌和Facebook这样的公司之所以能赚得盆满钵满，主要是因为它们在广告市场的垂直整合中占据了绝对优势。而 OpenAI 整合广告的方式在我看来，似乎只是想分得蛋糕里最小的一块——一个投放广告的地方——这意味着，我估计他们的用户收入更接近于报纸网站，而不是最大的社交媒体网站，或者更接近于推特或 Tumblr 这类从未实现过巨额盈利的公司。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c6/c6bd1d9ddfc915c46dc857f7023a2a78.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e8/e80fac779975b727b1b89c12981bac1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明知加广告会被骂，OpenAI为什么还要这么做？那就要从OpenAI的财务状况说起。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;营收增长10倍，但算力投入扩大9.5倍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI的财务状况与算力投入呈现出高度协同的增长态势，过去三年，二者均实现了累计十倍左右的扩张，印证了“算力决定营收上限”的核心逻辑。这种强关联不仅是业务发展的结果，更成为OpenAI规划未来投入、平衡供需关系的重要依据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在OpenAI最新一期博客中，公司首席财务官 Sarah Friar 透露了 OpenAI 的财务细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从算力投入来看，OpenAI的扩张速度堪称惊人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年，其算力规模为0.2 吉瓦（GW）；2024年，迅速提升至0.6 吉瓦；2025年，进一步增至约1.9 吉瓦，三年累计扩大约9.5倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为保障未来算力供应，Sarah Friar 称 OpenAI已与微软、英伟达、AMD、甲骨文等企业签署数千亿美元的合作协议，同时从单一供应商转向多云、多芯片的多元化布局，在高端训练任务中采用最新硬件，在大规模推理场景中使用低成本基础设施，平衡效率与开支。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，算力投入存在显著的时间差，当前的投入需提前规划至2028～2030年的需求，这也意味着OpenAI需要稳定的长期收入来覆盖前置成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;营收方面，OpenAI同步实现了三倍速年度增长，与算力扩张节奏高度匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年，其收入达到20亿美元，2024年增至60亿美元，2025年预计突破200亿美元，三年累计增长约十倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种增长并非依赖单一业务，而是构建了多元化的收入结构：一是订阅收入，涵盖个人用户的ChatGPT Go、Plus、Pro档位及企业订阅服务，满足不同层级用户需求；二是API服务收入，为开发者和企业提供模型调用能力，支出与交付成果直接挂钩；三是广告与电商收入，依托免费用户流量开辟新增长曲线；未来还将探索授权许可、知识产权合作、结果导向定价等模式，进一步丰富收入来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从运营效率看，OpenAI的算力投入与营收的强相关性，验证了其商业模式的可行性。但当前仍面临算力缺口的核心挑战——由于算力不足，诸多潜在产品与功能无法落地，尚未充分释放价格弹性杠杆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也解释了为何OpenAI持续加码算力投资，同时通过广告等业务拓宽收入渠道，本质上是为了打破算力瓶颈，释放更多商业价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;加入广告，也会坚守三大原则&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从成本端看，算力是OpenAI发展的核心命脉，且需求近乎无限。但Sarah Friar在博客中表示，即便在模型中加入广告，也会“死守”三大底线。他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“大家普遍会疑惑，广告会对产品本身和公司运营产生怎样的影响？要回答这个问题，我们可以从当前的用户结构说起：如今，我们消费端平台上 95% 的用户都在使用免费版本。这恰恰契合了我们的使命 —— 研发通用人工智能是为了造福全人类，而非仅仅服务于有能力付费的群体。因此，保障用户的访问权至关重要。&amp;nbsp;从广告业务的角度出发，我认为有三点原则必须坚守。第一，我们要让所有人都清楚：模型给出的永远是它能提供的最佳答案，而非付费推广的结果。很多其他平台正是在这一点上栽了跟头，导致用户无法判断看到的内容是付费广告还是真实的最优推荐。而我们的核心准则就是，模型始终以提供最优答案为导向。&amp;nbsp;第二，广告本身可以具备很高的实用价值。我们会明确标注广告内容，让用户一目了然。举个例子，如果用户搜索 “圣地亚哥周末短途旅行”，那么一条爱彼迎的广告可能会非常有帮助。用户甚至可能愿意在 ChatGPT 的对话场景中，与广告商展开深度交流 —— 前提是他们清楚这是广告环节。这正是我们需要创新的方向，要打造出与平台生态深度融合的广告形式，而非简单地把传统的横幅广告生搬硬套过来。&amp;nbsp;第三，也是最后一点，我们必须保留无广告的服务层级，让用户拥有选择权和控制权。同时，我们对用户数据的保护始终保持高度谨慎。此前推出医疗健康功能时，我们就明确告知用户，相关数据会被隔离存储，不会用于模型训练。信任是 OpenAI 的立足之本，即便在广告业务上，我们也会坚守这些原则。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sarah Friar还表示，其实不只是OpenAI，未来，消费者很可能会订阅多款人工智能服务，就像现在大多数人都会订阅不止一个流媒体平台一样，这一消费行为模式可以作为很好的参考。不同的人会根据自身需求做出不同选择，包括免费选项 —— 毕竟也有广告支持的免费流媒体服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且即便是同一项服务，也会同时提供付费版和免费版两种选择，未来的市场格局会呈现出丰富的多样性。不过，用户切换不同平台时也会面临一个问题 —— 迁移成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sarah Friar还表示模型的记忆功能也是值得探讨的问题。他还进一步表示OpenAI不会垄断整个市场：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“未来的模型是会实现跨平台统一记忆，还是会分平台独立记忆？其实即便是基于同一个模型，不同服务商也会推出各具特色的服务，在功能取舍上各有侧重。哪怕是依托 OpenAI 模型的服务，也有很多不同的开发者在提供差异化产品，这也是我所理解的 ‘多平台使用’ 的含义。当然，我并不认为 OpenAI 会垄断整个市场。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为维持算力与营收的同步增长，OpenAI需要持续投入数千亿美元用于基础设施建设与合作伙伴拓展，而单一的订阅制模式难以支撑如此庞大的资金需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;广告业务的引入，能够借助免费用户的流量规模，开辟新的收入来源，为算力投入提供稳定的资金补充，形成“算力支撑业务、业务反哺算力”的循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，广告业务的布局也与OpenAI的长期战略相契合。在ChatGPT月活用户突破8亿且仍有巨大增长空间的背景下，广告成为连接免费用户与商业价值的桥梁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据业内消息，OpenAI预计2026年通过广告获得数十亿美元级收入，未来将逐步放大这一收入来源，与订阅、API服务等形成互补，降低单一模式的经营风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI缺钱了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;既有巨额的算力成本支出，又有逐年翻倍的营收进账，那 OpenAI到底是不是真的缺钱了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，《纽约时报》的一位专栏作家却做出了一个明确的预测：OpenAI 将在 18 个月内因其在人工智能领域的投入而破产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该作家表示，根据去年的一份外部报告，OpenAI 预计在 2025 年将烧掉 80亿美元，到 2028 年将烧掉 400 亿美元。鉴于该公司据报道预计到 2030 年实现盈利，不难计算出其中的利害关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Altman的风险投资计划在数据中心领域投入1.4万亿&amp;nbsp;美元。正如外交关系委员会经济学家Mallaby所指出的，即便OpenAI重新考虑那些受盲目乐观影响的承诺，并“用其估值过高的股票为其他投资买单”，仍然存在巨大的资金缺口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mallaby并非唯一持此观点的人，贝恩公司去年发布的报告显示，即便在最乐观的预期下，该行业也至少存在8000亿美元的资金缺口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a05b0161bf40898d79a714388b440d0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位金融专家巧妙地分析了这种情况，他概括地指出，问题的关键不在于终端用户人工智能是否会在技术上得到普及，而在于开发人工智能在中长期内是否具有经济意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;分析师指出，理论上，投资者应该“弥合一项伟大技术出现与最终盈利之间的差距”，但实际上，许多人工智能公司烧钱的速度似乎远远超过了其盈利能力。Mallaby指出，鉴于微软或Meta等“传统”公司在人工智能出现之前就已经拥有盈利业务，并且（实际上）有能力等待必要的时期，直到人工智能最终带来收益，因此，这些新来者的处境比它们要糟糕得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据他所说，大多数人都在使用免费服务，一旦他们常用的AI模型添加了广告或使用限制，他们就会毫不犹豫地转向竞争对手。目前各种任务都有无数的选择，也证实了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，他认为这对人工智能提供商来说只是暂时的难题，随着智能人工智能越来越深入人们的日常生活，转换将会变得更加困难，因为AI模型最终应该能够掌握你所有的购物偏好、愿望和情感特征——甚至可能比你本人做得更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mallaby确实赞扬了 OpenAI 首席执行官 Sam Altman 的“吸金能力”，他成功筹集了400亿美元的投资，超过了历史上任何一轮私募融资的规模——甚至超过了沙特阿美300亿美元的融资额。不同之处在于，沙特阿美和其他一些上市企业拥有成熟的商业模式和盈利能力，而OpenAI目前这两点都不具备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能金融这条衔尾蛇看起来确实像是要吞噬自己的尾巴，但也有人认为它只会失去较新的部分。如果人工智能市场失去一个或多个开创者，那将颇具讽刺意味。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances&quot;&gt;https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46663341&quot;&gt;https://news.ycombinator.com/item?id=46663341&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xSxkCYr7zMyCVYQBYf1F</link><guid isPermaLink="false">https://www.infoq.cn/article/xSxkCYr7zMyCVYQBYf1F</guid><pubDate>Tue, 20 Jan 2026 10:03:57 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>聚焦算力市场痛点，嘉唐算力供应链平台重磅发布</title><description>&lt;p&gt;在近期举行的第五届AIGC开发者大会上，上海嘉唐科技发布了名为“算力供应链服务平台”的全栈式解决方案。该平台以“生态共建，供需协同”为理念，围绕算力交易、金融配套、资产管理及算电协同等维度展开设计，旨在应对当前算力行业存在的价格透明度低、流程不规范、服务缺乏标准化及供需匹配效率不高等问题，致力于为构建全国算力服务统一市场提供技术支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，算力作为数字经济发展的重要基础设施，已成为衡量新质生产力的关键指标之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据统计，近五年来我国算力产业规模年均增速超过30%，但与此同时，行业仍面临资源结构性失衡、整合程度不足等制约高质量发展的挑战。为此，市场上陆续出现多种服务模式探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉唐科技此次推出的平台整合了撮合与直营等模式，尝试在供需对接、资源保障、产业链协同及能耗优化等方面提供系统性支持，其中算电协同方案通过引入绿电直供等方式，尝试推动算力行业能耗成本优化与绿色化转型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从平台架构来看，其采用“1+3+N”的设计思路，即一个综合服务底座，涵盖算力交易、资产管理、金融服务三大核心模块，并计划拓展至多个行业应用场景。该架构试图在资源整合、智能调度与服务标准化等方面做出探索，与行业主管部门推动的算力互联互通方向具有一定的契合性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生态合作方面，多家来自能源、金融、科技等领域的企业参与了此次发布仪式，并表达了在资源共享与产业协同方面的合作意向。行业分析指出，此类跨领域协作有助于将企业单体优势扩展为产业链整体效能，对推动AI技术在不同行业的落地应用可能形成一定支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内观察显示，随着算力在经济社会各领域渗透不断加深，构建开放、高效、协同的算力供应链体系逐渐成为行业共同关注的议题。相关平台的出现，反映了市场主体在整合算力资源、提升服务能效方面的尝试，其长期成效仍有赖于技术可靠性、模式可持续性及行业协同机制的进一步完善。在算力市场竞争日趋全球化、绿色化的背景下，此类探索也为推动产业高质量发展提供了可供观察的案例。&lt;/p&gt;</description><link>https://www.infoq.cn/article/vbx7cX5Mvva7szH6Fl53</link><guid isPermaLink="false">https://www.infoq.cn/article/vbx7cX5Mvva7szH6Fl53</guid><pubDate>Tue, 20 Jan 2026 09:59:30 GMT</pubDate><author>李冬梅</author><category>芯片&amp;算力</category></item><item><title>智元首席科学家罗剑岚：2026年挖出真实场景的Scaling law，全系机器人将上线“新系统”</title><description>&lt;p&gt;“通用性不再是主要瓶颈，部署中的任务集熟练度和可靠性才是决定机器人能否真正落地的关键。”在近期的一场采访中，智元机器人合伙人、首席科学家罗剑岚称，2026 年是机器人从会做很多事但每个事做得不太好走向把事情做好并落地的关键节点，要求学习范式从静态离线训练升级为部署学习再部署的整套数据闭环系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示，正是基于这个判断，智元机器人具身研究中心提出了 SOP（Scalable Online Post-training），一套面向真实世界部署的在线后训练系统。SOP 的核心目标是，让机器人在真实世界中实现分布式、持续的在线学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据罗剑岚透露，智元今后会在所有机器人上应用SOP。今年，智元计划部署比现在大几个数量级的机器人，真正找到机器人真实场景部署和真实场景落地的 Scaling law。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要在真实世界中大规模运行，通用机器人必须同时满足两个看似矛盾的要求：在复杂多变的环境中保持稳定性与可靠性；在处理差异巨大的任务时，仍具备良好的泛化能力。现有 VLA 预训练模型已经提供了强大的通用性，但真实世界的部署受困于更高的任务专精度要求以及离线数据采集方式的边际效益递减，往往需要通过后训练获得更高的任务成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，当前主流的 VLA 后训练方法仍受离线、单机、串行采集等因素制约，难以支撑高效、持续的真实世界学习。这些限制并非源自具体算法，而是来自学习范式本身。智元方面介绍，SOP 改变的不仅是训练范式，更是机器人系统的生命周期。如果说 VLA 让机器人第一次具备了通用理解与行动能力，那么 SOP 所做的是让众多机器人的经验共同驱动智能的快速成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“SOP目前不是完全开源的，但不排除未来开放的合作形式。”罗剑岚表示，智元从成立之初就坚持走生态开放的路线，希望跟更多厂商一起共建SOP，把SOP的闭环真正接入到业务流程里。SOP不是封闭系统，而是一种新的持续学习、在线学习、协同进化的方式，任意的后训练算法和模型都可以接进来，智元会开放一些SOP的关键模块和接口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从长远来讲，智元的目标是构建一个开放的机器人在线学习生态，不同的机器人本体都可以接入，让数据共享上传到云端一个大脑，数据回传回来并不断进化，给大家使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;SOP：分布式在线后训练框架&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SOP采用Actor–Learner异步架构，本身是一套通用的框架，可以即插即用的使用任意后训练算法，让VLA从在线经验数据中获益。智元选取HG-DAgger（交互式模仿学习）与RECAP（离线强化学习）作为代表性算法，将其接入SOP框架以进化为分布式在线训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，他们将 VLA 后训练从“离线、单机、顺序”重构为“在线、集群、并行”，形成一个低延迟的闭环系统：多机器人并行执行 → 云端集中在线更新 → 模型参数即时回流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/165b264a617472a960966c8d46b445b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP 架构设计图&lt;/p&gt;&lt;p&gt;SOP的关键优势包括：&lt;/p&gt;&lt;p&gt;•&amp;nbsp;高效状态空间探索。分布式多机器人并行探索，显著提升状态–动作覆盖率，避免单机在线学习的局限。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;缓解分布偏移。所有机器人始终基于低延迟的最新策略进行推理采集，提升在线训练的稳定性与一致性。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;在提升性能的同时保留泛化能力。传统的单机在线训练往往会使模型退化为只擅长单一任务的“专家”， SOP 通过空间上的并行而非时间上的串行，在提升任务性能的同时保留 VLA 的通用能力，避免退化为单任务专家。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;实验评估：性能、效率与 Scaling Law&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际效果方面，智元围绕三个方面对 SOP 进行了系统性评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是 SOP 能为预训练 VLA 带来的影响。实验结果说明，在各类测试场景下，结合SOP的后训练方法均得到了显著的性能提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比预训练模型，结合SOP的HG-Dagger方法在物品繁杂的商超场景中实现了33%&amp;nbsp;的综合性能提升。对于灵巧操作任务（叠衣服和纸盒装配），SOP 的引入不仅提升了任务的成功率，结合在线经验学习到的错误恢复能力还能明显提升策略操作的吞吐量。结合SOP的HG-Dagger方法让叠衣服的相比HG-Dagger吞吐量跃升114%。SOP让多任务通才的性能普遍提升至近乎完美，不同任务的成功率均提升至94%以上，纸盒装配更是达到98%的成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/33aa742594ed935cadfafc5c7304bf70.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了进一步测试真机SOP训练后VLA模型是否达到专家级性能，他们让SOP训练的VLA模型进行了长达36小时的连续操作，模型展现出了惊人的稳定性和鲁棒性，能够有效应对真实世界中出现的各种疑难杂症。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，智元使用了三种机器人队伍数量（单机、双机、四机配置），在同样的数据传送总量的基础上，进行了比较。实 验结果表明，在相同的总训练时间下，更多数量的机器人带来了更高的性能表现。在总训练时间为3小时的限制下，四机进行学习的最终成功率达到了92.5%，比单机高出12%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们认为，多机采集可以有效阻止模型过拟合到单机的特定特征上。同时，SOP 还将硬件的扩展转化为了学习时长的大幅缩短，四机器人集群相比单机能够将模型达到目标性能的训练速度增至2.4倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/7968e86516261f7dd57bb0a3763068e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP 学习效率提升&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，他们探究了 SOP 和预训练数据之间的关系，把总量为160小时的多任务预训练数据分为了三组：20小时，80小时和160小时，分别训练一组初始模型后再进行 SOP。接着发现，预训练的规模决定了基座模型和后训练提升的轨迹。SOP 能为所有初始模型带来稳定的提升，且最终性能与VLA预训练质量正相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，对比80小时和160小时实验效果，在解决特定失败情况时，在轨策略经验带来了非常显著的边际效果。SOP 在三小时的在轨经验下就获得了约30%的性能提升，而80小时额外人类专家数据只带来了4%的提升。这说明在预训练出现边际效应递减的情况下，SOP 能够高效突破VLA性能瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca3462b2b121f4f3c33ffa0e53ddaa2e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP在不同预训练数据规模下的对比&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，智元将机器人队伍放到了预训练模型没有见到的真实新环境下执行任务，并使用SOP进行在线训练。当机器人被置于不同的环境时，即便是同样的任务，起初成功率和吞吐量如预期般下降，但在 SOP 介入仅仅几个小时后，机器人的性能便显著回升，能够鲁棒地执行相对复杂的实际任务。&lt;/p&gt;</description><link>https://www.infoq.cn/article/wJjaPXC8rLTxLTeOjLZ0</link><guid isPermaLink="false">https://www.infoq.cn/article/wJjaPXC8rLTxLTeOjLZ0</guid><pubDate>Tue, 20 Jan 2026 09:55:02 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>微软为MCP服务器发布了Azure函数支持</title><description>&lt;p&gt;微软已将其模型上下文协议（MCP）对 &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview&quot;&gt;Azure Functions&lt;/a&gt;&quot;的支持提升至一般可用性，标志着向标准化、身份安全的代理式工作流程的转变。通过集成原生OBO认证和流式HTTP传输，本次更新旨在解决历史上阻碍AI智能体访问敏感下游企业数据的“安全痛点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP扩展于&lt;a href=&quot;https://techcommunity.microsoft.com/blog/appsonazureblog/build-ai-agent-tools-using-remote-mcp-with-azure-functions/4401059&quot;&gt;2025&lt;/a&gt;&quot;年4月进入公开预览，现支持&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-dotnet&quot;&gt;.NET&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-java&quot;&gt;Java&lt;/a&gt;&quot;、JavaScript、&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-python&quot;&gt;Python&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-typescript&quot;&gt;TypeScript&lt;/a&gt;&quot;，而新的自托管选项允许开发者在不修改代码的情况下部署现有的基于MCP SDK的服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由Anthropic开发的模型上下文协议（&lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;）提供了一个标准化的接口，使AI智能体能够访问外部工具、数据源和系统。自2024年11月推出以来，包括OpenAI、谷歌DeepMind和微软在内的主要AI平台已采用该协议，到2025年4月，&lt;a href=&quot;https://www.mcpevals.io/blog/mcp-statistics&quot;&gt;服务器下载量从大约10万次增长到超过800万次。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，正如Mirantis的Randy Bias&lt;a href=&quot;https://www.mirantis.com/blog/securing-model-context-protocol-for-mass-enterprise-adoption/&quot;&gt;所指出的那样&lt;/a&gt;&quot;：“安全和合规团队不能允许运行在开发人员笔记本电脑上的未经审查的‘影子代理’访问电子医疗记录或客户个人身份信息等关键数据系统”——这推动了对具有内置治理的托管平台的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一般可用的MCP扩展引入了几个为生产部署设计的功能。对流式HTTP传输协议的支持取代了旧的服务器发送事件（SSE）方法，微软建议除非客户端特别需要SSE，否则使用新的传输。该扩展暴露了两个端点：/runtime/webhooks/mcp用于流式http和/runtime/webhooks/mcp/sse用于遗留的SSE连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Java开发人员，Maven构建插件（版本1.40.0）提供了构建时对MCP工具注释的解析和验证，自动生成正确的扩展配置。根据微软的说法，这种构建时分析可以防止运行时反射在Java应用程序中引入的冷启动时间增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;内置的认证和授权实现了&lt;a href=&quot;https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization&quot;&gt;MCP授权协议&lt;/a&gt;&quot;要求，包括发出401挑战和托管受保护资源元数据文档。开发者可以为服务器认证配置Microsoft Entra或其他OAuth提供商。该功能还支持代表用户（&lt;a href=&quot;https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-on-behalf-of-flow&quot;&gt;OBO&lt;/a&gt;&quot;）认证，使工具能够使用用户的身份而不是服务账户访问下游服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首席软件工程师Den Delimarsky在2025年4月分享了关于使用Azure Functions和API管理实现安全的MCP服务器的&lt;a href=&quot;https://den.dev/blog/remote-mcp-server/&quot;&gt;见解&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;开发者面临的一个主要痛点是实现与认证和授权相关的任何内容。如果你没有安全专业知识，这本质上是痛苦且有风险的。你可能会错误地配置一些东西，最终将所有数据暴露给不能看到它们的人。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sitecore的云架构师Victor Karabedyants&lt;a href=&quot;https://www.linkedin.com/pulse/mcp-servers-azure-functions-moving-your-ai-tools-from-karabedyants-uvxbc/&quot;&gt;详细说明&lt;/a&gt;&quot;了实践中的认证流程。当客户端连接到远程MCP服务器时，Azure Functions会以包含受保护资源元数据路径的401响应拒绝初始匿名请求。客户端读取此元数据，触发Microsoft Entra ID登录流程，获得OAuth令牌，并用令牌重试请求。“你的Python或Node脚本永远不会看到认证逻辑，”Karabedyants解释说。“平台负责处理繁重的工作。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Java开发者，Maven Build Plugin（版本1.40.0）在构建时提供MCP工具注释的解析和验证，自动生成正确的扩展配置。据微软称，这种构建时分析可以防止Java应用程序中运行时反射引入的冷启动时间增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/self-hosted-mcp-servers?pivots=programming-language-csharp&quot;&gt;新的自托管MCP服务器&lt;/a&gt;&quot;功能目前处于公开预览阶段，允许团队将使用官方&lt;a href=&quot;https://github.com/modelcontextprotocol/servers&quot;&gt;SDK&lt;/a&gt;&quot;构建的MCP服务器部署到Azure Functions作为自定义处理程序；轻量级Web服务器代理请求到开发者的现有进程。微软将此描述为“提升和转移”方法，只需要一个host.json配置文件来定义Functions应该如何运行服务器。该功能目前支持使用Python、TypeScript、C#或Java SDK实现的流式http传输的无状态服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5db15c5de3126e46d94a031df01b619b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（来源：&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/self-hosted-mcp-servers?pivots=programming-language-csharp&quot;&gt;Microsoft Learn&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软的高级云倡导者Yohan Lasorsa在开发者社区&lt;a href=&quot;https://developer.microsoft.com/blog/host-your-node-js-mcp-server-on-azure-functions-in-3-simple-steps&quot;&gt;博客文章中&lt;/a&gt;&quot;强调了自托管方法的简单性：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Azure Functions上托管MCP服务器，可以让你兼得两者的优点：无服务器基础设施的简单性和官方Anthropic SDK的强大功能。只需一个简单的配置步骤，你就可以将现有的Node.js MCP服务器部署到一个生产就绪、自动扩展的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gaurav Rawat在Medium上一篇关于生产部署模式的详细&lt;a href=&quot;https://blog.dataengineerthings.org/how-to-architect-deploy-and-operate-production-grade-model-context-protocol-mcp-servers-with-ce83d64bb015&quot;&gt;文章&lt;/a&gt;&quot;中，强调了在大规模运行MCP服务器时的几个运维考虑因素。他指出，对于P95延迟超过1秒、错误率超过2%以及SSE连接频繁掉线等监控指标，需要在生产环境中立即进行调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rawat还记录了实践者应该意识到的当前限制：在与Azure AI Foundry集成时，嵌套数组和复杂类型必须序列化为逗号分隔的字符串，并且由于UI基础的批准在自动化部署中不持久，因此需要使用require_approval=&quot;never&quot;进行程序化工具批准以用于生产工作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Azure Functions 提供了多种托管计划，以满足不同的MCP服务器需求。&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/flex-consumption-plan&quot;&gt;Flex&lt;/a&gt;&quot;消费计划根据需求自动扩展，采用按执行付费的计费模式和零规模经济。当MCP工具闲置时，成本降至零，同时保持快速的唤醒时间。&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-premium-plan&quot;&gt;Premium&lt;/a&gt;&quot;计划支持“始终就绪”的实例，这些实例保持预初始化状态，消除了冷启动延迟，这对于初始化延迟可能导致SSE连接超时和代理响应时间差的关键时刻工具至关重要。Rawat建议为关键的24/7工具设置两到三个始终就绪的实例，以确保故障转移能力。开发人员还可以使用专用计划来满足需要可预测性能或与虚拟网络集成的工作负载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软已经发布了多种语言的快速入门模板，涵盖这两种托管方法。MCP扩展快速入门覆盖了&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-dotnet&quot;&gt;C# (.NET)&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-python&quot;&gt;Python&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-node&quot;&gt;TypeScript (Node.js)&lt;/a&gt;&quot;，Java快速入门即将推出。该平台直接与&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-mcp-tutorial?tabs=mcp-extension&amp;amp;pivots=programming-language-python#configure-azure-ai-foundry-agent-to-use-your-tools&quot;&gt;Azure AI Foundry&lt;/a&gt;&quot;集成，允许智能体在无需额外配置层的情况下发现和调用MCP工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/azure-functions-mcp-support/&quot;&gt;https://www.infoq.com/news/2026/01/azure-functions-mcp-support/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HY40dxuIKS2uTIJmUb96</link><guid isPermaLink="false">https://www.infoq.cn/article/HY40dxuIKS2uTIJmUb96</guid><pubDate>Tue, 20 Jan 2026 08:24:00 GMT</pubDate><author>Tim Anderson</author><category>微软</category><category>AI&amp;大模型</category></item><item><title>大模型低价趋势延续！智象未来姚霆：B端、C端界限模糊，API不够、逼出 “按结果付费”</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。我们采访了智象未来联合创始人姚霆，他指出在多模态领域，深度 Scaling up 模型能力提升收益放缓，而广度 Scaling up 会带来更多惊喜，多模态能力也在重塑大模型推理过程。另外，2025 年的模型价格战倒逼厂商三大加速：研发新模型抢占短暂的版本优势、提升推理速度、升级高性价比架构降本。他认为，低价趋势 2026 年将延续，核心原因是市场远未饱和。结合公司情况，姚霆表示模型商业模式从卖 API、积分制转向“按结果付费”。下面是详细对话内容，以飨读者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态大模型的 Scaling up&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：Scaling up 是否仍是最佳路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;对于多模态大模型而言，Scaling up 有深度和广度。深度 scaling up 就是类似于单一多模态任务的纯粹模型参数 scaling up 过程，我们会发现这种 scaling up 下模型能力提升收益放缓，并不是指数级的增长，与之搭配的还需要高质量数据和架构的“Scaling up”，而且盲目扩增模型参数也会对推理 cost 带来极大地负担，所以我们在深度 scaling up 过程中除了模型性能之外更多地会去考虑训练和推理的 cost，期望达到极致的性能 - 效率平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而广度 scaling up 指的是从垂域场景和商业化落地的视角下去看 scaling up，即不同多模态任务之间的 scaling up，我们发现这种广度上的 scaling up 会带来更大的惊喜，例如在联合架构中去实现多模态理解和生成任务的统一，以及视频生成和音频生成任务的统一，衍生出类似音画同步的特色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：MoE 架构为什么会成为 2025 年的主流架构？其在参数效率与推理成本间的平衡能力，是否彻底改变了大模型的开发与部署逻辑？非 MoE 路线的企业如何构建差异化竞争力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;稀疏 MoE 架构的一大优势是较高的推理效率，尽管其模型参数量很大，但在推理过程中只有部分参数被激活，这样既保持了高参数量带来的模型学习能力，也在部署推理过程中表现出较高的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而对于非 MoE 架构，也就是参数稠密型的模型，虽然推理的性价比会比 MoE 架构低，但是对于垂域任务，稠密型模型由于总参数量更小，部署更加灵活，也可以体现出较好的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态大模型的代表性发展&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年多模态能力取得了哪些飞跃性发展？Nano Banana Pro 代表的图片生成模型、OpenAI Sora、Google Veo 3 代表的视频生成模型，分别做了哪些优化得到了不错的效果？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2025 年多模态大模型能力有几个代表性的发展：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;音画同步生成，让视频从默片时代进入了有声时代；主体参考的一致性，实现了从片段化到连贯叙事的转变，AI 漫剧因此迎来了井喷的爆发；运镜表达、表情演绎，让视频生成更具备影视表达，从“形似”到“神似”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Veo 3 就在音画同步上做的很出彩，而 Nano Banana Pro 则将主体参考一致性发挥到新的高度，因为都是闭源模型，所以只能猜测在技术上不会局限于单一的 DiT 架构，例如借助多模态推理和生成的统一（VLLM+DiT）实现更精准的多模态内容编辑，而将更多不同模态的 token（文本、图像、视频、语音等）融入到统一的架构中则能端到端实现类似音画同步的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：多模态能力是否会重塑推理？跨模态推理是否也成为必答题？预计推理能力的突破方向在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2025 年&amp;nbsp;多模态能力已经在重塑大模型推理过程，从 DeepSeek OCR 中使用图片来进行长文本压缩，到 Nano Banana 中直接生成解题过程的图片，多模态能力已经成为大模型推理能力中不可或缺的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多模态数据往往能提供比纯文本数据更稠密、直观和具备逻辑关联的信息。目前多模态数据越来越多的引入，对于大模型结构、训练方法以及数据三方面都会带来新的挑战。其中，大模型结构要尽可能支持原生多模态的输入或者输出，对于模型的参数量上提出了更高的要求；训练方法上需要去平衡各种不同的任务，保证模型在不同任务上都达到一定的收敛程度；数据上则对数据的广度和精度上又有了进一步的要求，广度上需要尽可能涵盖需要的多模态推理任务，同时高质量精品数据可以在训练后期提升推理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：从语言模型到多模态模型，再到世界模型，这个演进的本质是什么？您认为世界模型未来发展趋势如何？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;从语言模型到多模态模型，再到世界模型，演进的本质是“大模型对真实世界的建模能力升级”：语言模型是“理解人类符号”，多模态理解模型是“感知世界表象”，多模态生成模型则是“模拟世界表象”，而世界模型是“掌握物理规律和因果关系并与之交互”，这也是通往 AGI 的必经之路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，世界模型未来必将会在理解物理世界空间结构的同时，提升对物理规律和因果关系的刻画能力，而且通过与物理真实世界的交互实现从感知到决策的闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“低价趋势肯定会延续”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年模型价格战最关键的影响是什么？价格战倒逼厂商做了哪些架构演进？低价趋势在 2026 年是否会继续延续？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;主要还是倒逼模型厂商去持续加速，一是加速研发新模型形成短暂的版本优势，二是加速模型的推理时间，时间就是金钱，三是加速模型架构的升级，引入性价比更高的架构设计来降低成本。低价趋势肯定会延续，因为市场还远没有饱和。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年在 B 端和 C 端，都有哪些创新的商业模式出来吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;创新的商业模式是很难的，所以我觉得更多是一些特色吧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;B 端和 C 端的界限越来越模糊，总体来说都是内容的生成者，真正的海量 C 端其实是内容的消费者，所以可以把两个端一起谈，商业模式的创新就是从售卖 API 提升到了售卖结果，以前 B、C 两端都是积分制，本质就是价值折算的积分，但是我们在不断探索按照结果来付费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在移动端，我们也在突破过去 web 端复杂的积分逻辑对应的不同的会员等级，pro、ultra 等等，我们只会把功能区分为会员功能和非会员功能，然后按需充值即可，不会再纠结额度来觉得是否续费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在您看来，2026 年大模型竞赛的核心是什么？您认为下一次“大模型代际飞跃”可能来自哪条技术路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2026 年&amp;nbsp;大模型竞赛的核心，会从“技术能力”转向“价值落地能力”，类似于比拼“行业收入规模”和“客户留存率”。谁能更快将技术转化为行业实效，谁就能占据先机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一次“代际飞跃”很可能来自两个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一是新颖的用户交互体验，随着基础原子能力目前逐渐饱和，2025 年 Agent 相关的应用出现了爆发式的增长，而 Agent 爆发的背后实际上代表了用户在认可大模型能力的同时又对于 AI 应用的交互体验提出了更高的要求，让大模型从单一的原子能力向完整解决方案提供者演变，一旦在用户交互方式、交互体验上跨越式提升，就会带来新的机遇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;二是专业级能力的大众化，目前大模型能力对于专业从业者来说已经达到一个很惊艳的程度，但是对于大众来说还是存在一些使用上的“困难”，这种困难可能来自于高昂的推理成本，编写专业级 prompt 的入门难度，以及缺乏大模型使用经验以及思维，而下一次飞跃可能就来自于如何拉近大模型对于大众的隔阂，出现真正的全民级 AI 应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;&amp;nbsp;“模型和商业化一直会是两个最大挑战”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：根据您的观察，科技公司 2025 年面临的压力如何？对此采取了什么样的应对措施？员工们的状态如何？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;对我们这样的模型研发的公司来说，模型和商业化一直会是两个最大的挑战，这两个挑战汇集在一起就是对于底层模型架构的突破变成必选项，模型公司不能像过去那样不断的优化数据和推理来解决用户的问题，而是要在架构上做出突破，敢为人先。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非常开心的是我们的员工状态始终保持战斗状态，因为我们不要 80 -&amp;gt;85，而是要 120 分的创新和颠覆，同时模型团队也和业务团队有了更多的协同，这种协同对于模型团队的能力落地起到非常重要的作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：经过一年竞赛，国内前沿 AI 水平取得了怎样的成绩？是否赶上了硅谷科技公司？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;在多模态大模型这个赛道，我觉得国内外是百花齐放，例如我们在 2025 年 4 月的图像模型 HiDream-I1 开源打响了国内多模态生成式大模型登顶国际竞技场的第一枪，同时大家也开始重视了多模态生成式大模型的竞技场，这些过去只有硅谷科技公司的模型名单里开始快速出现国内的各家模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您认为，2026 年的技术赛点可能是什么？您会重点关注哪些行业和技术？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;技术赛点从多模态模型架构上来说我觉得还有比较长的路，但是在应用上我觉得技术的赛点是多模态 agent 的成熟落地。2025 年上半年的 Manus，下半年持续火热的 vibe &amp;nbsp;coding 都是大语言模型的应用落地的典型案例，多模态模型看似比大语言模型更解决用户，但是生图生视频场景还没有出现真正技术应用上完全解决用户痛点的 agent，所以我们也会更关注多模态 agent 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fijwq6waNUKJhmbUGMqc</link><guid isPermaLink="false">https://www.infoq.cn/article/fijwq6waNUKJhmbUGMqc</guid><pubDate>Tue, 20 Jan 2026 07:18:07 GMT</pubDate><author>蔡芳芳,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>产业级 Agent 如何破局？百度吴健民：通用模型难“通吃”，垂直场景才是出路</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。我们采访了百度智能云平台产品事业部算法架构师、千帆策略部负责人吴健民，他指出，Agentic 模型训练最大卡点不是模型，是真实环境复刻，外部接口、数据库、登录依赖等真实链路的稳定访问，技术实现门槛极高。在当前，通用全能的 Agentic 模型现阶段不可能实现，业务场景、工具、环境差异过大，通用模型泛化性有限，针对垂直场景的模型定制和持续学习或是破局关键。在多模态模型发展方面，吴健民指出，视觉生成主流为 模型框架从 Diffusion Model 发展到 Flow Matching，效果、稳定性碾压前代方案，视觉理解模型仍以 ViT Encoder 嫁接语言模型的主流方案，模型能力迭代的主要聚焦在垂直方向的数据合成。虽然工业和学术界有很多尝试，当前未真正实现多模态理解和生成的统一建模，目前分开独立优化效果依旧优于融合建模。下面是详细对话内容，以飨读者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“没有模型可以支持所有 Agent 场景”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：如何让大模型更好支持 Agent 应用？技术有哪些瓶颈？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：目前我们的研发目标，是让模型能够在各类 垂直 Agent 场景中更好地发挥作用。其中，最核心、发展也最快的场景是 Coding Agent，包括通用编程以及面向网页开发或特定垂直领域的 Agent 应用。现阶段，我们的工作重点之一就是更具体地聚焦在网页开发相关的 Agent 能力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，有一个重要的问题需要回答：SOTA 的通用模型是否能在各种垂直 Agent 场景下都能达到工业级的效果。就目前来看，具备这种能力的通用模型还没有出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因在于，不同 垂直 Agent 所处的场景设定、可使用的工具集合以及运行环境差异极大，而当前的通用模型尚不足以在如此多样的场景中实现稳定泛化。因此，围绕具体应用场景定制模型，反而更容易形成优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，不同场景对效果的评估标准也存在显著差异，即 Reward 的定义并不通用。如果一个场景能够清晰地定义 Reward，并且该 Reward 判断能够高效自动地完成，那么针对这一场景通过强化学习在通用基座模型上定制训练的 Agentic 模型，往往可以显著超过现有通用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个难点在于环境的复杂性。以代码场景为例，其运行环境不仅涉及代码本身，还包括外部接口调用、工具使用、数据库依赖，以及登录、扫码等一系列真实应用中的外部依赖。在训练过程中，这些依赖都必须能够被高并发、稳定地访问，这对技术实现提出了很高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个挑战在于强化学习系统本身。当前业内已形成共识，即要实现模型在特定场景中的持续迭代，必须依赖一套在该场景下运行顺畅、具备高效率和高吞吐能力的强化学习系统。﻿由于强化学习系统本身的架构复杂性，也出现了不少 RLaaS 的平台产品，把算法复杂性封装在平台内，业务仅需要聚焦在业务场景定义，Reward 评估方案制定和迭代。这也是百度千帆平台 26 年的重点业务方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：那现在有没有比较通用、效果较好的强化学习框架？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：目前开源社区中已有不少强化学习框架，例如 OpenRLHF、TRL 以及 VeRL 等，它们基本覆盖了强化学习流程中的主要环节。但在工业级应用中，这些框架仍然不够成熟，﻿特别是涉及多轮工具调用的 Agentic 场景，往往需要进行深度定制和打磨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;打磨方向主要在两个方面：首先是模型规模支持，严肃应用往往依赖参数量较大的 SOTA 模型，例如百度文心或 DeepSeek 开源的模型，强化框架能否高效支撑这类大模型至关重要；其次是 Agent 训练能力，早期的强化学习多集中于单步任务，例如数学推理，而代码类、客服、DeepReasearch 等 Agent 更依赖多轮工具调用的复杂交互，这就要求强化训练框架能够配合一整套稳定、高效的脚手架系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，工业级 Agentic 模型的 研发对整体技术栈的要求极高，包括沙盒环境以及高性能、高并发的调度运行能力；若涉及联网搜索，还需要稳定的高并发搜索 API 支持。因此，具备云计算或搜索基础能力的团队往往更具优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：要在基座模型上增强 Agentic 能力，需要哪些技术支持？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这一问题的核心仍然在于强化学习如何在基座模型之上更好地服务于具体场景。强化训练的本质并不是创造全新的能力，而是激发和稳定模型在特定场景中的既有能力。因此，首要前提是基座模型本身在目标场景上具备优势。这种优势通常来源于预训练阶段的数据分布。例如，搜索相关数据占比更高的模型，在代码类 Agent 场景中往往更具潜力，不同场景基座模型的选择，通常观察基座模型在对应场景的 Pass@k 指标，即推理多次能得到正确答案的比例。Pass@k 指标高的模型，有更大空间通过强化学习训练激发并稳定模型在对应场景的表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个关键依赖是训练效率。强化学习的过程本质上更接近一种搜索机制：模型通过大量尝试生成不同路径，Reward 对每次尝试进行优劣评估，并将表现较好的路径通过强化训练反馈到模型参数中。在这一过程中，生成尝试路径（Rollout）通常占据 80%—90% 的时间成本。因此，是否能够以高吞吐方式高效完成 Rollout，是强化训练成败的关键。这个过程的关键是“训推一体”的技术，实现训推计算资源的高效利用以及训练精度差异的对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：另外，现在强化学习的 scaling 在业内似乎未形成共识？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：的确不像预训练 scaling 一样普遍的共识。过去，强化训练通常只占总体训练很小的一部分，被视为对预训练模型的微调，给预训练模型的蛋糕上放一个樱桃。而现在，强化训练的样本规模已经可以扩展到百万级，系统性地提升了模型推理和复杂问题解决能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要实现大规模多场景的强化训练，前提是结果评估能够准确自动完成，且最好能有稠密的评估奖励反馈。在代码或数学等评估相对确定的场景中，这一点相对容易实现，模型在代码和数学解题方向能力也得到显著提升。但在通用问答或复杂垂直场景中，由于缺乏统一、自动化的评估方案，规模扩展变得困难。这也是模型尚未在更通用场景实现泛化的重要原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管如此，业内普遍认为强化训练依然具有显著的 scaling 效果，﻿问题的焦点转化到可泛化到评估奖励方案设计上。从依赖人工反馈的小规模 RHF，到基于规则甚至更通用奖励方案的 RLVR 强化训练，随着规模扩大，模型效果确实在持续提升，这一点在实际应用中也得到了验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：通用 Agent 与专用 Agent 之间的能力差距，该如何弥补？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：当前主要存在两种思路。一种是追求在所有方向上都表现出超过人类的全能模型或 Agent，这本质上指向 AGI。业内对实现 AGI 需要的时间判断差异很大，而我们认为这一目标仍然相当遥远。另一种更现实的路径，是在特定专业场景中不断提升模型和 Agent 能力，能够在局部任务上超过人类水平，这在相当长一段时间内仍将是主流方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们负责研发的全球领先的可商用自我演化超级智能体百度伐谋，为可以准确定义评估验证方案的 NP-hard 问题，提供高效的最优解演化方案，实现超过人类水平的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：长上下文能力对 Agent 的支持非常重要，应当如何建设？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：模型支持的上下文长度与 Agent 能力之间存在直接关系。上下文决定了模型能够记忆和理解的信息规模，而在复杂任务中，Agent 需要不断与环境交互，每一次反馈都会进入上下文，成为下一步决策的依据。因此，交互轮次越多，对模型长上下文理解能力的要求就越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，业界也在探索通过 Agent 脚手架本身“放大记忆”的方案。类似人类并不会记住所有信息，而是通过笔记、字典或工具进行辅助，Agent 也可以通过工具使用来弥补上下文长度的限制。例如，在审核数百页合同的场景中，即便无法一次性将全文放入上下文，Agent 仍可以借助工具调用逐页查看、回溯关联内容，从而完成整体审核任务。从这个角度看，通过工具增强记忆能力，也是实现长上下文处理的一种有效路径，体现了 Agent 开发中 Progressive Disclosure 的原则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在一些偏注意力机制的底层架构方面，业内是否做了调整？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这个涉及模型网络结构本身的问题了。无论通过何种工具把上下文扩展得更长，模型本身的上下文理解能力始终存在上限。比如目前常见的 128K 或 256K 甚至 1M 上下文，长上下文能力的关键是模型能否准确理解高效处理，这依赖高效的注意力机制设计和实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型利用上下文，在生成下一个 token 时，一个重要的观察是：并非全部上文 token 都对预估当前 token 同等重要，真正起作用的往往只是其中一小部分。基于这一特性，注意力机制可以采用稀疏化策略，不必对全部 128K 的 token 做同等精细的计算，可以采用比如 DeepSeek DSA 方案，先租略进行一次快速扫描，再对相关性高的部分 token 进行精细注意力计算。另一个思路是把上文 token 进行分块，先筛选相关的块，再对相关块内 token 进行精细注意力计算。结合两个方案的优势，也是一个实现的思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年 MoE 架构被广泛采用，是否意味着更强模型的整体方向已经基本确定？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：MoE 架构被广泛应用到搜索、推荐等不同预估场景。大模型提到的 MoE，实际上是稀疏 MoE。其实从去年年初开始，这项技术就在业内受到较多关注。它要解决的核心问题仍然是 Scaling Law：随着模型参数规模不断增大，训练和推理成本也在持续上升，是否能在保持参数规模扩展的同时，控制实际训推计算的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoE 给出的答案是肯定的。通过这种方式，可以在继续增大模型总参数的同时，让训练和推理所实际使用的参数规模保持次线性增长。具体而言，在 Transformer 架构中，MoE 将原本的全连接层拆分为多个对等的小模块，即“专家”，在每次前向推理只激活其中一部分，从而显著降低计算成本。稀疏 MoE 已逐渐成为业内的主流选择，稀疏比耶做到了 5% 甚至更低的水平，成为推动模型规模继续扩展的一种现实可行方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态模型架构层逐渐收敛&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：从单一模态发展到多模态并引入 Agent，在底层架构上发生了哪些变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：一个最显著的变化，是在原有语言模型基础上引入视觉能力，这也是从去年开始 VLM 大量出现的主要方向。实际工作中，核心仍然在语言模型本身：通常是在语言模型训练到一定阶段后，引入视觉编码器，并用图文对其数据与语言模型联合训练，对齐文本和视觉 token，使模型能够理解视觉信号。这种 “桥接”或“嫁接”的方案，逐步成为当前的主流方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在多模态领域，一个长期目标是希望视觉模型也能像语言模型一样有很好的﻿Scaling Law，但这一问题至今仍未解决。视觉信号本身的信息密度比较低，它更像是自然世界的直接映射，并不一定承载明确的知识结构。﻿相比而言，互联网上存在的海量文本数据，是人类产生的对世界知识的总结压缩，信息密度很高。这使得仅依赖视觉输入进行大规模训练，难以达到语言模型那样的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，现有方案高度依赖图文对齐数据，即为图片配备高质量、细粒度的文本描述，通过充分对齐文本与图片，来提升模型的理解能力。但这类数据难以规模化获取，不易全面覆盖实际的图片分布，目前行业可用的规模大致在 3–5T token，量级上存在明显差距，也限制了多模态模型的进一步 scale。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年文生图、图生图模型更新频繁，突破点主要在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这属于视觉生成方向。从 Sora 开始，这一领域受到了广泛关注，也出现了不少高质量的开源项目，支持生成效果不断提升。但像 Sora 2 或 Nano Banan 等业内 SOTA 的生成模型，其具体实现细节并未完全公开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从算法角度看，视觉生成方案本身仍在快速演进，从早期的 Stable Diffusion 到当前的 Flow Matching，建模方法和训练效率都得到了显著优化。不过，从能力定位上看，视觉生成模型更偏向专精模型，主要解决“生成”的问题，也有观点认为，生成模型可能进一步发展为所谓的“世界模型”，即在理解物理规律的基础上生成符合现实约束的内容，进而通向 AGI 的实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2026 方向：生成与理解的统一建模&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在此基础上，未来一段时间，尤其是 2026 年，大家主要会沿着哪些方向继续演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：一个非常重要的方向，多模态生成与理解的统一建模。很多公司都在尝试通过统一的多模态建模方式，让生成能力和理解能力形成协同效应，而不再是彼此割裂。这意味着模型既不是单纯为生成而设计，也不是只服务于理解任务。外界对 GPT-5 等模型也曾寄予类似期待，尽管目前看相关路径尚未完全跑通，但可以确定的是，这一方向仍在持续探索之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在专家视角下，生成与理解真正实现统一，应当达到什么样的效果？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：最终评价标准仍然是结果导向。如果通过统一训练得到的模型，在生成和理解两个维度上的表现，都优于分别独立训练的模型，那么这种统一才是有意义的。举例来说，如果一个生成 - 理解统一模型在生成质量上能够超过当前生成领域的 SOTA 模型，那么就可以认为内生的理解能力确实提升了生成效果。但就目前来看，分开针对生成和理解进行优化，独立效果仍然更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：也就是说，目前融合后的效果还不如单独优化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：是的，至少在现阶段仍是如此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：但很多团队似乎还是在把各种能力揉合进一个模型里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：确实存在这种趋势，但并非所有团队都选择同一条路径。不同团队对通用人工智能实现方式的理解并不一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一种思路是将多种能力融合到单一模型中，希望模型像人一样具备听、说、读、写等多种模态能力，这是一种全模态模型的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一种思路则是强调模型学会使用工具。人类智能的显著提升，本质上源于工具使用能力的不断演进，从最原始的简单工具到今天的计算机系统，工具极大放大了人的能力。Agent 的发展，本质上正是沿着“工具使用”这一路径展开的，不同理解会带来不同的技术路线和实现方式，当前没有看到哪条路一定能走通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年“世界模型”这个概念被频繁提及，从语言模型到动态模型再到世界模型，这条演进逻辑是怎样的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：“世界模型”这一说法本身就存在多种理解。最早在 Sora 第一代发布时，其自称为世界模型，核心目标是通过建模来理解物理世界的运行规律，尤其是借助视觉输入，让模型学习空间关系和物理约束，例如生成的视频必须符合基本物理常识。这一路线随后发展得很快，重点在于提升模型的空间感知推理和物理一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但也存在另一种理解路径。例如 Meta 前段时间发布的 CWM 模型，强调的是代码能力和工具调用能力，同样定义为世界模型。在这种视角下，只要模型能够高效使用现实世界中的各种工具，就可以被视为对“世界”的一种建模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Agentic 模型是今年必答题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：展望明年，大模型能力提升的核心突破点可能来自哪些技术路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：明年的变化大概率会延续 2025 年已经显现的趋势。2025 年一个非常明显的方向是 Agentic Model，即模型具备稳定、准确的工具调用能力。代码场景已经率先验证了这一点，明年这一能力很可能扩展到更多应用场景，模型将不再只调用编程相关工具，而是能够使用更广泛的现实世界 API，这是一个较为明确的发展趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：那面对复杂环境，大模型将如何应对？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：通用场景的环境通常非常复杂，模型需要对接的 API 接口、数据库、人际交互界面等系统差异较大。针对后者，目前较为可行的方案，仍然是让模型在特定场景的 Agent 脚手架中学会熟练使用该场景所涉及的工具。尽管应用场景很多，但每个场景对应的工具集合通常是相对有限的。模型通过场景反馈不断优化工具使用方式，就可以逐步适应复杂环境。代码 Agent 场景正是一个典型例子，模型通常只需要掌握十几种工具调用方式，随着打磨程度提升，其在该场景下的表现也会持续改善。&lt;/p&gt;</description><link>https://www.infoq.cn/article/RFVtzWIOoQhubp6cz0yK</link><guid isPermaLink="false">https://www.infoq.cn/article/RFVtzWIOoQhubp6cz0yK</guid><pubDate>Tue, 20 Jan 2026 07:10:19 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Mistral发布OCR 3，提升了手写及结构化文档识别的准确率</title><description>&lt;p&gt;Mistral近日发布了其光学字符识别（optical character recognition，OCR）模型的最新版本，&lt;a href=&quot;https://mistral.ai/news/mistral-ocr-3&quot;&gt;Mistral OCR 3&lt;/a&gt;&quot;，该版本专注于在多种文档类型上实现更高的精度，包括手写笔记、表单、低质量扫描件以及结构复杂的表格。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据Mistral表示，OCR 3相较于前一代产品是一次重大的飞跃。在基于真实客户文档工作流的内部评估中，新模型在整体表现上以74%的胜率超越了Mistral OCR 2，尤其在表单、手写内容和含大量表格的文档上优势更为显著。这些基准测试采用模糊匹配（fuzzy-match）指标与人工标注的真实结果进行比对，旨在反映实际业务场景，而非理想化的合成数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba88c8ec331199980f9e1cbdf0583891.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：Mistral博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度看，Mistral OCR 3不仅能够提取文本，还能识别并保留嵌入的图像，同时完整保留原始文档的结构信息。它的输出格式为Markdown，其中表格通过HTML标签（如rowspan和colspan）重建，使下游系统不仅能获取纯文本，还能保留布局语义。这一特性使其非常适合需要结构化JSON、可搜索档案的管道，或集成到智能体（agentic）和检索增强系统（RAG）中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在以往通常依赖人工复核的场景中，该模型也展现出显著的进步。它能够高效处理手写内容，包括连笔的笔记和批注。在表单解析方面，对标签、复选框及混合输入项的识别更加准确。此外，OCR 3对扫描档案中常见的倾斜、压缩伪影、低分辨率以及背景噪点等问题具备更强的健壮性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期用户特别强调了其性能提升和多语言方面的支持能力。ICT安全负责人兼AI安全专家Patrick Jacobs&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407434928260390914?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407434928260390914%2C7407449619036524544%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287407449619036524544%2Curn%3Ali%3Aactivity%3A7407434928260390914%29&quot;&gt;评论&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在速度方面，真的令人印象深刻，而且它处理荷兰语毫无压力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;得益于准确率的大幅提升，Mistral OCR 3的生产部署正在快速扩展。Techseria创始人兼首席顾问Niraj Bhatt&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407434928260390914?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407434928260390914%2C7407454745524678656%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287407454745524678656%2Curn%3Ali%3Aactivity%3A7407434928260390914%29&quot;&gt;分享了&lt;/a&gt;&quot;其实际应用的变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们一直在生产环境中使用Mistral OCR处理销售和采购发票，实现ERP系统的零人工数据录入。现在v3在表单和手写内容上准确率提升了74%，终于让我们能够将覆盖范围扩展到送货单、水电账单以及过去只能靠人工处理的遗留档案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在定价方面，Mistral OCR 3的标准费率为每1000页2美元；若使用Batch API，成本可降至每1000页1美元，使其成为许多企业级OCR系统的高性价比替代方案。开发者可通过API直接集成模型（标识符为mistral-ocr-2512），非技术用户则可通过拖放式的Document AI Playground界面轻松使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于具有严格数据治理要求的组织，Mistral提供了私有化的部署选项，确保OCR工作负载完全运行在客户可控的基础设施内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，Mistral OCR 3已经可以使用了，并完全向后兼容OCR 2。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/mistral-ocr3/&quot;&gt;Mistral Releases OCR 3 With Improved Accuracy on Handwritten and Structured Documents&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/DX8LLPkyH06INOoAjxxa</link><guid isPermaLink="false">https://www.infoq.cn/article/DX8LLPkyH06INOoAjxxa</guid><pubDate>Tue, 20 Jan 2026 07:06:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>2026 年预测：智能体将推动集中化战略，带来新的工作方式 ｜ 技术趋势</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 2025 年稳步发展的基础上，2026 年将成为智能体 AI 在企业中实现真正落地的关键之年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回顾 2025 年初，行业曾普遍预测智能体 AI 将迎来爆发式增长与颠覆性普及。尽管技术进步显著且持续加速，但这一年的更深层意义在于，它重塑了我们对技术可行性的理解。各类组织已超越简单的聊天机器人应用场景，开始积极探索能够自主规划、执行任务并持续迭代的智能体系统。如今，核心智能体能力显著提升，已可胜任一年前仍难以处理的复杂多步骤任务。随着市场的迅速扩张，投资与创新正形成叠加效应，持续推动着该领域的发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为制定本年度的 Snowflake 数据与人工智能预测报告，我与十余位 Snowflake 的领导者共同梳理了对未来一年的&lt;a href=&quot;https://www.snowflake.com/en/lp/snowflake-ai-data-predictions/&quot;&gt;行业展望&lt;/a&gt;&quot;。报告的核心观点是：智能体将在企业级应用中取得实质性突破。以下摘录本年度报告中的部分预测要点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上下文窗口与记忆能力将成为提升智能体性能的关键：未来一年，上下文窗口与记忆能力的重大改进将使智能体能够基于更宏观的情境理解，以更高的自主性应对复杂挑战。Snowflake 工程与支持高级副总裁 Vivek Raghunathan 指出：“这是一种更趋近于人类的能力——能够记住更广泛的情境信息以解决当前问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工作者需精通人与AI的协作与沟通：人类仍将处于决策闭环之中，部分原因是驱动决策的数据并非全部对 AI 开放。Snowflake 产品副总裁 Chris Child 强调，AI 能对其掌握的数据进行深度分析，但人类直觉仍不可或缺。他表示：“AI 模型将深入理解您的数据，但您仍需学会何时存疑、何时在行动前进行深度追问。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据战略将决定AI就绪度与最终成效：Snowflake 首席信息官 Mike Blandina 指出：“当 AI 提供准确答案时，还必须确保私有或专有数据不被泄露。用户是否拥有查看此答案的权限？您的营销聊天机器人是否在泄露员工的社保号或客户的信用卡信息？这并非 AI 本身的问题，而是关乎如何治理与保护数据。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到 2026 年末，核心问题将不再是人工智能能做什么，而是人与人工智能如何协同工作。换言之，重点将转向角色如何演变、决策权如何分配，以及领导者在自主性日益增强的环境中如何建立信任与明确责任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;十年前，首席数据与分析官（CDO）的职责主要聚焦于数据治理。但随着智能体化人工智能的到来，这一角色已扩展至统筹企业内智能体的协同运作。首席数据与分析官需负责保障智能体所依赖数据的质量与合规性，设计智能体嵌入的工作流程，并对这些系统在现实场景中的表现承担最终责任。这使得首席数据与分析官的职能更接近真正的“人工智能首席运营官”——其职责横跨工程技术、合规监管、安全防御、运营维护及产品团队，确保人工智能运行模型具备稳定性、可信度以及与业务目标的高度一致性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到 2026 年，企业面临的挑战将不再局限于将智能体简单部署至生产环境。管理者需要围绕智能体建立起系统化的管理体系，这意味着必须构建可靠的验证框架、厘清人机协同的职责边界，并实现全链路的可观测性，确保每个智能体的行为皆可审计、可解释、可信任。这一趋势将催生正式的 AI 质量控制职能，通过持续监测与评估，保障智能体行为始终与商业意图保持一致。对于注重可靠性的企业而言，这已成为必然的演进方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实现此类管控体系，依赖于坚实且集中的数据基础与治理架构。在早期实验阶段行之有效的联邦模型虽有助于提升开发效率，但随着智能体系统的扩展，必须确保跨工作流的高度一致性：统一的语义规范、严格的权限管理以及不容妥协的安全保障，已成为系统规模化运作的必要条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着企业推进流程与决策权限的重构，建立贯穿组织全局的反馈闭环至关重要。此类闭环可协助团队优化规则边界、改进模型行为，并确保责任机制始终保持清晰。短期来看，智能体系统将最适用于边界明确、结构化程度高且风险可控的工作流程。随着数据成熟度、治理体系以及组织适配能力的持续提升，智能体将逐步进入更复杂的决策链路，获得更高自主权，并产生更具战略价值的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能体AI并非替代人类工作，而是重塑工作模式，开拓新的机遇维度与规模化潜力。若需深入了解更多前沿趋势，敬请参阅&lt;a href=&quot;https://www.snowflake.com/en/lp/snowflake-ai-data-predictions/&quot;&gt;《Snowflake 数据与 AI 预测报告（2026）》&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/data-ai-predictions-2026/&quot;&gt;https://www.snowflake.com/en/blog/data-ai-predictions-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cTKd3cN0LK4Fx90mPArR</link><guid isPermaLink="false">https://www.infoq.cn/article/cTKd3cN0LK4Fx90mPArR</guid><pubDate>Tue, 20 Jan 2026 06:48:44 GMT</pubDate><author>Anahita Tafvizi</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>Django发布6.0版本，提供内置的后台任务和原生CSP支持</title><description>&lt;p&gt;&lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;&quot;是广受欢迎的Python Web框架，最近&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/releases/6.0/&quot;&gt;发布了Django 6.0版本&lt;/a&gt;&quot;，带来了专注于开发者需求的新特性、安全增强以及性能改进，旨在现代化Web应用开发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Django 6.0引入了几项重要特性，包括内置的后台任务框架、原生的内容安全策略（Content Security Policy，CSP）支持、基于组件开发的模板局部文件（partials），并采用了Python的现代化邮件API。此版本&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/releases/6.0/#python-compatibility&quot;&gt;同时支持&lt;/a&gt;&quot;Python 3.12、3.13和3.14，但不再支持Python 3.10 和3.11。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Django 6.0提供了内置的任务框架，允许在HTTP请求-响应周期之外运行代码，无需依赖像&lt;a href=&quot;https://docs.celeryq.dev/en/v5.5.3/django/index.html&quot;&gt;Celery&lt;/a&gt;&quot;这样的第三方库。这使得开发者可以将发送邮件或数据处理等工作卸载到后台worker执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;定义完成之后，任务可以通过配置好的后端进行排队。Django负责任务的创建和排队，不过执行仍需外部的基础设施来管理。社区对这一特性的反应非常积极，有&lt;a href=&quot;https://news.ycombinator.com/item?id=45674209&quot;&gt;Hacker News&lt;/a&gt;&quot;用户评论说：“我喜欢Django，新的任务框架看起来很棒，有望取代Celery。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://www.reddit.com/r/django/comments/1pd92pc/django_60_released/&quot;&gt;Reddit&lt;/a&gt;&quot;上，一位开发者这样写到：“我对内置的后台任务最为兴奋，期待对其进行测试。”不过，也有一些用户&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1pdtbw9/django_6_new_features_2025_full_breakdown_with/&quot;&gt;批评了&lt;/a&gt;&quot;该框架的默认配置过于简单：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;很失望新后台任务没有像最初的django-task那样提供一个基本的数据库后端和worker。这对许多只需要发送一些邮件和运行一些定时任务的基础应用程序来说是个遗憾。&amp;nbsp;这本来可以满足大量基础应用的需求，比如，你可能只需要它们发送一些邮件、运行一些定时任务（cron jobs）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Django 6.0还引入了对&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/topics/security/#security-csp&quot;&gt;内容安全策略（CSP）&lt;/a&gt;&quot;的内置支持，使得让web应用免受跨站脚本和其他内容注入攻击变得更加容易。CSP策略可通过&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/ref/middleware/#django.middleware.csp.ContentSecurityPolicyMiddleware&quot;&gt;ContentSecurityPolicyMiddleware&lt;/a&gt;&quot;实现，并通过Python字典和Django提供的常量配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模板局部文件是另一大增强，允许开发者在模板文件中封装并重用已命名的片段。新的&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/ref/templates/builtins/#std-templatetag-partialdef&quot;&gt;partialdef&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/ref/templates/builtins/#std-templatetag-partial&quot;&gt;partial&lt;/a&gt;&quot;标签使得模板更加模块化，而不需要将组件分割到单独的文件中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还包括对异步支持的改进，比如，&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/ref/paginator/#django.core.paginator.AsyncPaginator&quot;&gt;AsyncPaginator&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/ref/paginator/#django.core.paginator.AsyncPage&quot;&gt;AsyncPage&lt;/a&gt;&quot;类，扩展了跨数据库后端的支持功能，例如，现在除了PostgreSQL外也可使用的StringAgg，以及带有新几何函数和查询的增强GIS功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于从早期版本迁移过来的开发者，Django提供了&lt;a href=&quot;https://docs.djangoproject.com/en/6.0/howto/upgrade-version/&quot;&gt;详细的升级指南&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Django是一个由Django软件基金会开发和维护的开源Web框架，强调快速开发、简洁设计及实用解决方案来构建Web应用。它支持传统的服务器渲染应用和现代API驱动架构，被广泛应用于众多高流量网站，并且横跨多个行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/django-6-release/&quot;&gt;Django Releases Version 6.0 with Built-In Background Tasks and Native CSP Support&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/C0L5QEafugEByCEWWk68</link><guid isPermaLink="false">https://www.infoq.cn/article/C0L5QEafugEByCEWWk68</guid><pubDate>Tue, 20 Jan 2026 03:32:10 GMT</pubDate><author>作者：Daniel Curtis</author><category>框架</category><category>编程语言</category></item><item><title>ClickHouse 完成由 Dragoneer 领投的 4 亿美元 D 轮融资，加速其在分析与 AI 基础设施领域的扩张</title><description>&lt;p&gt;公司收购 Langfuse，正式进军 LLM 可观测性 (LLM observability) 领域，并推出原生 Postgres 服务，以统一事务型与分析型工作负载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;旧金山 — 2026 年 1 月 16 日 — 实时分析、数据仓库、可观测性 (observability) 以及 AI/ML 领域的领导者 ClickHouse 今日宣布完成 D 轮融资，融资金额达 4 亿美元。本轮由 Dragoneer Investment Group 领投，Bessemer Venture Partners、GIC、Index Ventures、Khosla Ventures、Lightspeed Venture Partners、T. Rowe Price Associates, Inc. 管理的账户，以及 WCM Investment Management 共同参与。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次融资正值 ClickHouse 持续且加速增长之际。目前，公司通过全托管服务 ClickHouse Cloud 已服务超过 3,000 家客户，年度经常性收入 (ARR) 同比增长超过 250%。在过去三个月中，Capital One、Lovable、Decagon、Polymarket 和 Airwallex 等客户开始采用该平台或扩大了现有部署。这些新客户加入了 ClickHouse 已建立的客户群体，其中包括 Meta、Cursor、Sony 和 Tesla 等 AI 创新者及全球知名品牌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“ClickHouse 的初衷就是为最严苛的数据工作负载提供卓越的性能和成本效率，而今天的增长势头正是这一战略的最好证明，”ClickHouse 首席执行官 Aaron Katz 表示。“面向未来，我们正在支持统一的事务型与分析型工作负载，让开发者能够在坚实的技术基础之上构建各种由 AI 驱动的应用。同时，我们也在拓展产品能力，引入 LLM 可观测性，帮助 AI 应用构建者在进入生产阶段时，更好地评估 AI 输出的质量和行为。新的资金支持，加上持续的产品执行力，使我们有能力在 AI 时代打造领先的数据与 LLM 可观测性平台。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad66614c2ecefc0a236b255e8d998003.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;对大规模数据基础设施与 AI 的高度确信投资&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dragoneer 成立于 2012 年，由 Marc Stad 创立，采用高度精选、以研究为核心的方法，专注于与少数具有品类定义意义的公司建立长期合作关系。过去十年中，该公司投资了多家领先的数据平台以及多家基础性的 AI 公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 系统逐步从实验走向生产，对底层数据基础设施提出了更高要求。AI 驱动的应用会产生远高于以往的查询量，对延迟更加敏感，同时还需要持续的评估能力和可观测性。在这样的背景下，真正的价值正越来越集中到那些能够支撑大规模、数据密集型生产工作负载的基础设施平台之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“每一次重大的平台变革，最终都会回馈那些最贴近生产环境的基础设施公司，”Dragoneer Investment Group 合伙人 Christian Jensen 表示。“当模型能力不断提升，真正的瓶颈就转移到了数据基础设施上。ClickHouse 的突出之处在于，它能够在大规模 AI 系统运行时，提供所必需的性能、效率和可靠性。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在严谨的评估过程中，Dragoneer 认为 ClickHouse 已成为现代数据技术栈中具有品类定义意义的领导者。该平台广泛支持关键任务级的实时工作负载，深度嵌入于始终在线、面向客户以及 AI 驱动的系统之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ClickHouse 的增长不仅来自对现有系统的替代，更来自对全新工作负载的支持。通过在大规模场景下实现高性价比的实时分析，ClickHouse 让许多过去因延迟或成本受限而无法落地的应用场景成为可能。与主要服务内部分析团队的许多数据基础设施平台不同，ClickHouse 经常直接嵌入到面向终端用户的产品中，在这些场景下，性能和可靠性会直接影响用户体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们寻找的是在系统绝不能停机时依然值得客户信赖的平台，而 ClickHouse 一直展现出这样的能力。”Jensen 补充道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;LLM 可观测性：ClickHouse 通过收购 Langfuse 进入该市场&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ClickHouse 正式宣布收购开源 LLM 可观测性平台 Langfuse。与关注系统健康和性能指标的传统可观测性不同，LLM 可观测性关注的是如何确保非确定性、日益复杂的 AI 系统能够输出准确、安全且符合用户意图的结果。随着 AI 系统不断深入生产工作流，LLM 可观测性已成为构建和运营 AI 应用团队不可或缺的一环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Langfuse 开源项目增长迅速，截至 2025 年底，已获得超过 2 万个 GitHub Star，每月 SDK 安装量超过 2,600 万次。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们之所以在 ClickHouse 之上构建 Langfuse，是因为 LLM 可观测性和评估本质上就是一个数据问题，”Langfuse 首席执行官 Marc Klingen 表示。“如今作为一个团队，我们能够提供更加紧密的一体化体验：更快的数据摄取、更深入的评估能力，以及从生产问题到可量化改进之间更短的闭环。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/73/735db94ae7281565b635da4f11227762.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Langfuse 联合创始人 Clemens Rawert、Marc Klingen、Max Deichmann&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;原生 Postgres 服务：ClickHouse 面向 AI 构建者推出统一数据技术栈&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ClickHouse 同时宣布推出一个与自身平台深度集成的企业级 Postgres 服务。为了支撑既需要事务处理又需要分析能力的现代实时 AI 应用，ClickHouse 打造了一套统一的数据技术栈，其中包括由 NVMe 存储支撑、具备原生 CDC 能力的高性能可扩展 Postgres。用户只需几次点击，就能将事务数据同步至 ClickHouse，从而解锁最高可达 100 倍的分析性能提升。借助由原生 Postgres 扩展提供支持的统一查询层，开发者可以构建横跨事务与分析的应用，而无需维护多个独立系统。该服务由 ClickHouse 与开源云公司 Ubicloud 联合打造，Ubicloud 团队在 Citus Data、Heroku 和 Microsoft 拥有丰富的产品与工程经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Postgres 与 ClickHouse 在架构上天然互补，是 AI 应用不可或缺的组成部分。通过合作，我们为团队交付了一套真正的一体化技术栈，让生产级 Postgres 负责事务处理，让 ClickHouse 专注分析，并作为一个整体协同运行，”Ubicloud 联合首席执行官兼联合创始人 Umur Cubukcu 表示。“我们非常高兴能在 Ubicloud 与 ClickHouse 携手合作，这正是开源生态系统成功的方式：由值得信赖的团队打造一流产品，并共同成长。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f5bb4b688dd0b3f1f7a5891e10d401a8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Ubicloud 联合创始人 Umur、Ozgun 和 Daniel&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;持续的全球扩张与产品动能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成融资并收购 Langfuse 的同时，ClickHouse 也在持续扩展其全球布局和生态体系。过去一年中，公司通过与 Japan Cloud 的合作进入日本市场，并宣布与 Microsoft Azure 围绕 OneLake 建立合作关系。ClickHouse 还在旧金山、纽约、阿姆斯特丹、悉尼和班加罗尔举办了多场用户活动，吸引了超过 1,000 名参与者，演讲嘉宾来自 OpenAI、Tesla、Capital One、Ramp 和 Canva 等公司，并连续第二年举办了 AWS re:Invent Chainsmokers 客户活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一系列近期产品进展进一步强化了 ClickHouse 在分析、AI 与可观测性交汇领域的地位。公司在数据湖支持方面持续投入，新增了对 Apache Iceberg、Delta Lake 以及主流数据目录的兼容性。同时，平台扩展了全文搜索能力，这对于包括 AI 可观测性在内的各类可观测性场景正变得愈发关键。此外，ClickHouse 还引入了轻量级更新机制，以支持需求更高、负载更复杂的 AI 驱动型应用。根据近期基准测试结果，ClickHouse 持续提供行业领先的性价比，在性能与成本比上超越主流云数据仓库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助 D 轮融资、对 Langfuse 的收购以及原生 Postgres 服务的推出，ClickHouse 已做好加速增长的准备，并将进一步巩固其作为统一数据平台与 AI 可观测性平台的战略地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解更多：&lt;/p&gt;&lt;p&gt;在此注册，体验 ClickHouse 的原生 Postgres 服务（&lt;a href=&quot;https://clickhouse.com/cloud/postgres&quot;&gt;https://clickhouse.com/cloud/postgres&lt;/a&gt;&quot;）。在 ClickHouse 博客上了解有关收购 Langfuse 的更多内容（&lt;a href=&quot;https://clickhouse.com/blog/clickhouse-acquires-langfuse-open-source-llm-observability&quot;&gt;https://clickhouse.com/blog/clickhouse-acquires-langfuse-open-source-llm-observability&lt;/a&gt;&quot;）。通过 LinkedIn（&lt;a href=&quot;https://www.linkedin.cn/incareer/company/clickhouseinc/&quot;&gt;https://www.linkedin.cn/incareer/company/clickhouseinc/&lt;/a&gt;&quot;）&amp;nbsp;和 X&amp;nbsp;（&lt;a href=&quot;https://x.com/ClickHouseDB&quot;&gt;https://x.com/ClickHouseDB&lt;/a&gt;&quot;）关注 ClickHouse，获取最新动态与公告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 ClickHouse：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ClickHouse 是一个快速的开源列式数据库管理系统，专为大规模实时数据处理与分析而设计。ClickHouse Cloud 以高性能为核心，提供卓越的查询速度与并发能力，非常适合需要从海量数据中即时获取洞察的应用。随着 AI 智能体 (AI Agent) 越来越多地嵌入软件系统，并生成频率更高、复杂度更大的查询请求，ClickHouse 提供了一个高吞吐、低延迟的引擎，专门用于应对这一挑战。ClickHouse 受到 Sony、Tesla、Memorial Sloan Kettering、Lyft 和 Instacart 等领先企业的信任，帮助团队通过一个可扩展、高效且现代化的数据平台释放数据价值并做出更明智的决策。欲了解更多信息，请访问 clickhouse.com。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 Dragoneer Investment Group：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dragoneer 是一家以增长为导向的投资机构，资产管理规模超过 300 亿美元。该机构与在公有和私有市场中打造品类定义型公司的创始人及管理团队长期合作。迄今为止，已有 50 多家 Dragoneer 投资的公司成功上市。其投资组合包括 Airbnb、Amwins、Atlassian、Databricks、Datadog、Meta、Nubank、OpenAI、Revolut、ServiceNow、Snowflake、Spotify 和 Uber。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 Langfuse：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Langfuse 是一个用于构建、测试和监控 LLM 应用及 AI 智能体的开源平台。团队使用 Langfuse 来追踪和调试智能体工作流、运行评估，并持续衡量和改进生产环境中 AI 输出的质量。Langfuse 既提供托管云服务，也支持在生产规模下自托管。作为增长最快的 LLM 工程平台之一，Langfuse 拥有 20,470 个 GitHub Star、每月超过 2,600 万次 SDK 安装量以及 600 多万次 Docker 拉取，并受到《财富》50 强中 19 家公司和《财富》500 强中 63 家公司的信任。欲了解更多信息，请访问 langfuse.com。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 Ubicloud：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ubicloud 正在打造开源版的 AWS，在裸金属和公有云之上交付核心云服务。Ubicloud 由打造分布式 PostgreSQL 的 Citus Data 团队创立 (该公司已被 Microsoft 收购)。其旗舰数据库产品 Ubicloud PostgreSQL 提供企业级托管 Postgres 体验，并具备行业领先的性价比。Ubicloud 在 AI、计算、PostgreSQL 和 Kubernetes 等领域提供的服务每周支撑超过 100 万台虚拟机运行，可帮助客户将云成本降低多达 70%。Ubicloud 获得了 Y Combinator 及其他知名硅谷投资机构的支持。欲了解更多信息，请在 X 上关注 Ubicloud @ubicloudHQ，或访问 ubicloud.com。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;/END/&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/2f/c1/2f4562446223b287a1a6f7652cc6ebc1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;征稿启示&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面向社区长期正文，文章内容包括但不限于关于 ClickHouse 的技术研究、项目实践和创新做法等。建议行文风格干货输出 &amp;amp;图文并茂。质量合格的文章将会发布在本公众号，优秀者也有机会推荐到 ClickHouse 官网。请将文章稿件的 WORD 版本发邮件至：Tracy.Wang@clickhouse.com。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/24/56/24a8afc1dea55e22e169095ff4323656.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/01/e6/01548053babc1150cea8ff0f4c9yybe6.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dzXRw5ZoyLFbg7jjZvQL</link><guid isPermaLink="false">https://www.infoq.cn/article/dzXRw5ZoyLFbg7jjZvQL</guid><pubDate>Tue, 20 Jan 2026 02:20:51 GMT</pubDate><author>ClickHouse</author><category>DataOps</category></item><item><title>选择技术领导之路</title><description>&lt;p&gt;技术领导者面临的挑战超越了个人贡献者的范畴：需要与业务部门就投资决策达成共识，处理系统层面的问题，开展人才培养，并持续跟进不断演变的代码库。在哥本哈根 Goto 大会上，Patrick Kua 在演讲“&lt;a href=&quot;https://gotocph.com/2025/sessions/3638/level-up-choosing-the-technical-leadership-path&quot;&gt;选择技术领导之路&lt;/a&gt;&quot;”中指出，技术对齐至关重要（包括统一代码风格、实现模式和标准规范），可以避免意外之外的复杂性。领导力可以通过技能实践、优化团队协作以及树立榜样来提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kua 说，技术领导者面临的挑战与个人贡献者不同。他举了一些例子，包括：必须与业务/产品人员就技术投资（如技术债务、平台工作）达成一致，必须处理或与其他团队合作处理更广泛的系统层面的事务（如基础设施、运营等），或为团队成员提供指导。他补充说，与此同时，技术领导者还要努力跟上不断变化的代码库，对于涉及 GenAI 的工具集更是如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术对齐至关重要，正如 Kua 所解释的那样：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;技术上不对齐，每个人都会不断接触同一个代码库，用对他们来说最简单的方式添加功能，但通常无法确保代码库的一致性。随着时间的推移，意料之外的复杂性不断累积，比如有五个不同的库执行相同的任务，或发送电子邮件或推送通知功能有七种不同的实现方式，未来当有人想要做哪个方面的更改时，其工作会变得困难许多。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kua 认为，只要在技术上适当对齐，团队就能持续对同一代码库进行修改，因为系统中的每个部分都将具有相似性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kua 说，为了实现技术对齐，技术领导者需要帮助团队就代码风格达成一致。下一个挑战是就实现模式达成一致。这意味着为共有的功能（如电子邮件发送、用户通知）和共有的任务（如日志记录、异常处理、网络重试等）创建公认的标准。一旦达成一致，就应该在团队维基中把这些标准模式记录下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了衡量工程师之间的对齐情况，可以让每个人都描述一下他们具体如何定义“好”代码，Kua 建议：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果他们写下的东西类似，那么他们就对齐了。如果他们写下的东西差别很大，那么他们之间就是错位的，这时就是展示技术领导力并进行进一步对齐的机会。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有很多资源可以用来提升领导技能。Kua 建议将更广泛的领导技能分解为具体的技能，如技能训练、职业传承、沟通、调解、影响力等。他说，即使一个人不是正式的领导者，每天也有机会在工作场所练习这些技能。选择一项技能，然后找到了解它的方法，如使用 AI 工具、YouTube、在线课程、HR/People 团队提供的课程和书籍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后，最重要的是小处着手，找到应用你所掌握的知识的方法，Kua 说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，工程师可以每天练习提高他们的沟通技巧，因为大多数人在团队中工作，需要与团队成员以及团队外的业务利益相关者进行沟通。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个重要的方法是寻找每个人都在抱怨但没有人处理的问题。这些问题是展示领导力的机会，处理它们可以改善团队中每个人的工作环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kua 说，成为最好的自己，并树立榜样。即使你不是正式的领导者，当人们尊重你并将你视为榜样时，你就是在实践领导力，即使没有头衔。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kua 说，不要将转换到技术领导者的角色视为晋升。你将从事许多与个人贡献者角色截然不同的工作，因此需要运用许多以前从未用过的技能。他总结道，好消息是只要你学习并实践，就能掌握这些技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 就技术领导力的问题采访了 &lt;a href=&quot;https://www.linkedin.com/in/patkua/&quot;&gt;Pat Kua&lt;/a&gt;&quot; 。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：是什么让人们决定成为技术领导者？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Patrick Kua：很多工程师希望他们能有更大的影响力，成为正式技术领导者有一个很大的好处是，你通常会被邀请参加关于未来规划的会议。在这些会议上，卓越的技术领导者可以带来新的见解，他们对现有系统的局限和改进机会有一个很好的理解，并能提出更简单的解决方案来实现相同的业务成果。然而，许多技术领导者不喜欢的一个方面是，这意味着要把更多时间花在会议上……&amp;nbsp;在许多情况下，人们被要求领导一个团队， 是因为他们已经投入了大量的时间来优化工作环境，改善了全体成员的工作体验，而且已经有人认可了他们展现技术领导力的能力。他们可能是团队寻求建议的首选对象，可能是与产品或营销部门沟通最顺畅的人。当管理层注意到他们具备诸多关键技能时，便会邀请其正式担任技术领导职务。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：优秀的技术领导力应该是什么样子？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Kua：区分正式和非正式技术领导角色是有帮助的。正式技术领导者负责确保团队拥有足够的技术领导力。实现这一点的其中一个方法是培养一个环境，让每个人都能舒适地站出来展示技术领导力。当你做得好时，每个人都能展示非正式的技术领导力。&amp;nbsp;之所以存在正式领导者是因为并非所有团队都自然而然地健康或高效。我相信，每名技术人员的记忆中都有这样一个团队，两名工程师不断地争论应该采取哪种方法，并希望有人能介入进来帮助团队做出决策。一个理想的世界是不需要正式领导者的，但团队很少能生活在完美的世界中。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/technical-leadership-path/&quot;&gt;https://www.infoq.com/news/2026/01/technical-leadership-path/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/mUdZKSvKPr1uCW3527qR</link><guid isPermaLink="false">https://www.infoq.cn/article/mUdZKSvKPr1uCW3527qR</guid><pubDate>Tue, 20 Jan 2026 02:16:27 GMT</pubDate><author>Ben Linders</author><category>管理/文化</category></item><item><title>非科班出身、辍学生逆袭AI巨头，Claude Code创始人：不关注对手，那只会让我们迷失</title><description>&lt;p&gt;从 Meta 离职、加入 Anthropic，这个决定在今天看来并不意外，但在当时却并非顺水推舟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对 Claude Code创建者 Boris Cherny 来说，这并不是一次普通的职业跳槽，而是一种价值判断：当大模型从“工具”逐步演化为具备自主生成与推理能力的系统，工程师究竟应该站在什么位置？是把它当作效率插件，还是把它视为一种需要被认真约束、引导和共同演化的新型技术力量？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，Boris Cherny 做客一档名为《The Peterman Pod》的访谈栏目，主持人Ryan Peterman与Boris围绕这一系列问题展开了长达一个半小时的深度对话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;访谈的前半部分，Boris回溯了自己从 Meta 转向 Anthropic 的动机。他并未从公司前景或个人发展谈起，而是从第一次使用 ChatGPT 的震撼说起。在他看来，大语言模型并不只是“更聪明的软件”，而更像一种尚处在早期阶段的“新生命形态”——它的能力增长呈指数级，影响范围远超工程本身，最终会重塑社会运行方式。正因为如此，他选择加入一个将安全、对齐与长期风险置于核心位置的研究机构，而不是继续在以产品速度和规模为优先目标的大厂体系内工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种价值取向，也直接塑造了 Anthropic 内部截然不同的工程文化。在鲍里斯的描述中，Anthropic 依然保留着创业公司式的“常识感”：工程决策不需要层层说服，安全不被视为拖慢产品的负担，而是与模型能力同步演进的前提条件。随着模型能力提升，风险不再只是内容失误或选举操纵，而是逐步逼近生物安全、社会系统性破坏等更高等级的问题。这并非科幻设想，而是工程师当下必须正视的现实边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此背景下，Claude Code 的诞生与演进，成为这次访谈的另一条主线。Boris坦言，Claude Code 在相当长时间里并不是一款“好用的产品”。它之所以最终跑出来，并非因为早期体验领先，而是因为团队在一开始就选择“为未来六个月后的模型能力而设计”，而不是围绕当下模型的短板打补丁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着 Sonnet 和 Opus 4 等模型上线，Claude Code 从辅助工具迅速跃迁为主力生产方式，在 Anthropic 内部，大量代码已经由模型生成，工程师的角色也随之发生变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这并不意味着“氛围编码”可以无条件替代人类判断。Boris反复强调，模型生成的代码与人写的代码必须接受同一套质量标准：不合格就不合并。不同任务对应不同协作方式——原型、临时代码可以交给模型快速推进，而核心逻辑仍需要工程师逐行审视。这不是“人被 AI 取代”，而是人类工程师与模型之间形成一种新的协同分工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，Claude Code 的使用场景正在溢出传统的软件工程边界。数据科学家、分析师、甚至销售团队，都在把它当作通用的工作执行工具，连接数据库、业务系统和数据源完成实际任务。这种扩散并非最初的产品设计目标，却揭示了一个趋势：当“写代码”本身变得门槛更低，软件能力正在被重新分配到更多角色手中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;贯穿整场对话的，是一个清晰却并不轻松的判断：软件工程正在经历一次结构性重写。工程师不再只是代码的直接生产者，而正在成为“智能体系统”的设计者、管理者和最后的责任人。Claude Code 只是一个具体案例，但它所揭示的，是一个更大的变化——当模型能力以指数级提升，工程文化、工作方式乃至风险边界，都必须随之重构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下为访谈实录，经由 InfoQ 翻译及整理：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Claude Code创建者，职业生涯起步于 Facebook&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：我想从你晋升为 Meta 高级工程师开始讲起你的故事。你晋升的那些项目背后有什么故事？当时你在哪里？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：如果我没记错的话，这个项目叫做“群组聊天”，目的是为了让 Messenger 和 Facebook 之间的联系更加紧密。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 Meta 参与的最初几个项目都与 Messenger 和 Facebook 有关。第一个项目是扎克伯格提出的将 Messenger 聊天记录和 Facebook 群组同步的想法。当时有几个项目旨在拉近 Messenger 和 Facebook 之间的距离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最初的动机是，我们感觉公共空间社交产品正在消失，而人们的注意力更多地转移到聊天和更随意的实时空间。我们尝试了几个产品版本，最终“聊天和群组”版本取得了成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得当时应该是第三个或第四个项目。那时我在 Facebook Groups 部门，主要负责 Messenger 的相关工作，但 Messenger 的组织架构和我们离得很远。这个想法是当时的 PM Steve 提出的。我听了之后觉得，好啊，太棒了！就这么办！我就开始着手开发。很快有了进展，于是我申请了更多工程师，有三位工程师加入了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们获得了一些数据科学和设计方面的支持。项目最初在网页端启动，后来也稍微拓展到了移动端。我们验证了在 Facebook 群组内进行聊天的想法，并证明了这类产品是可行的。当然，也有很多方面最终都失败了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以现在的产品标准来看，当时的体验简直糟透了。那时候，大家都在用 Web 开发，各种各样的 bug 都完全可以接受。但现在，视觉效果和质量标准要高得多。产品不断发展壮大，而我们团队很小，所以每个人都得包揽所有工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得当时我们没有用户研究员，所以我会在午餐时间去食堂。我们会推出一个新功能，然后向食堂工作人员展示，问他们能不能找到打开聊天窗口的方法。有时候他们能找到，有时候找不到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一项观察性用户研究。你可以观察人们在特定情况下如何完成任务，而无需过多提示，从而了解他们在哪些方面遇到困难，以及最终取得了哪些成果。我教会了团队如何进行这项研究，很快我们就会利用午餐时间去食堂，询问食堂工作人员（作为用户的代表）这种方法是否合理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：有趣的是，你当时所处的早期 Facebook 文化允许工程师们在代码之外做很多事情。例如，你当时在做用户体验研究。我记得在你的经历中，你也做过一些设计工作，并且指导过其他人进行设计。我认为这在 Facebook 的企业文化中是一个非常有趣且独特的事情。对吗？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我觉得这一点非常重要。直到今天，在我所在的 Claude 团队中，我们仍然非常重视通才型人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我喜欢和通才共事。如果你是一位既会写代码又能做产品开发、设计，并且具备产品意识的工程师，那么你肯定想和用户交流。我非常喜欢和这样的工程师一起工作。现在我们所有职位都是这样招聘的：我们的产品经理会写代码，我们的数据科学家会写代码，我们的用户研究员也会写一点代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我非常喜欢这些通才。我的成长经历也是如此。从 18 岁创办第一家公司开始，我就得事事亲力亲为。在加入 Facebook 之前，我也一直在一些规模较小的公司工作，那里也一样，什么都得做。在大公司里，你可能会被安排在某个特定领域，但这其实只是个形式。工程技能的范围很广，除了编写代码，完成整个流程还有很多其他方面需要考虑。当时能在一家真正重视这种能力的公司工作，感觉真的很棒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得在那半年结束时，我得到了晋升，然后我觉得在那半年结束后，所有的工程师也都得到了晋升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ryan：在那些早期产品中，存在着你多次提到的“潜在需求”概念，这正是许多产品方向的推动力。你能解释一下“潜在需求”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：潜在需求是产品设计中最重要的原则。纵观 Facebook 的成功产品，每一款都蕴含着潜在需求的元素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，Marketplace 的诞生源于一项观察：当时 Facebook 群组中 40% 的帖子都是关于买卖物品的。Facebook 群组最初并非为商业用途而设计，但人们却用它来做这件事。你设计的产品要具有一定的可扩展性和易用性（即使被“滥用”）。然后，你分析数据，看看用户是如何“滥用”的，并以此为基础开发新的产品功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;先是出现了 Facebook 群组，然后是买卖群组。买卖群组之所以发展迅速，是因为人们本来就想在 Facebook 群组里进行商业活动。接下来是 Marketplace，它只是人们这种意图的自然延伸。Facebook Dating 的发展也与之类似。观察发现，大约 60% 的个人资料浏览量来自互不相识的异性用户。这种传统的互相“窥探”行为证明了这种方法的有效性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品设计的核心原则是：你永远无法强迫人们去做他们原本不会做的事情。但你可以找到他们的潜在意图，并引导他们更好地利用这种意图，从而更轻松地实现目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“跨部门工作简直是一场噩梦”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你的叙述中，你提到你曾跨部门工作，因为你负责弥合 Messenger 和 Groups 工程团队之间的鸿沟。你说存在一些明显的文化差异，这很困难。对于在文化差异很大的组织之间工作，你有什么建议吗？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我的天哪，说困难都太轻描淡写了。简直是一场噩梦。当时 Facebook 的目标是尽快推出优秀的产品。而 Messenger 则完全专注于可靠性和性能，他们只关心这些。这完全是截然相反的价值观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不仅仅是文化差异或工程师之间的问题。那个团队的工程师对我们抱有戒心，因为我们的工作可能会影响他们的绩效指标。他们的组织目标是稳扎稳打，在不影响核心指标的前提下稳步推进产品发布；而我们的组织目标是快速发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目标完全不同。他们关注的是服务级别协议规定的正常运行时间，而我们只关注日活跃用户数和用户参与度。这些文化价值观根深蒂固，不仅体现在人们的言谈中，更体现在组织架构、目标设定等各个环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个项目最终能部分成功，克服了价值观差异，关键在于：如果你想让价值观截然不同的团队成功合作，就必须找到某种共同的目标、共同的兴趣或共同的信念。让他们能够一起验证某个假设，并且如果验证成功，对双方都有益处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Facebook 的“聊天和群组”功能很酷，但很多功能在 Messenger 上实现时却不太理想，原因就在于此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：既然你现在知道了这些，你会如何改变现状？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我可能会去找高层（比如扎克伯格），然后说，如果你真的对这件事很认真，我们应该把 Messenger 并入 Facebook 的组织架构下。我觉得这件事后来确实以某种形式发生了。Messenger 最初在公司里，后来搬了出去，然后又搬了进来，之后又搬了出去。公司这么大，这种情况难免发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为，从根本上来说，这类事情要想成功，不能只靠普通经理去协调。可能需要更高层的介入，调整组织架构，让它们更具合作性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，我看到你做了很多非常有趣的副业项目，我很好奇这些项目会产生怎样的蝴蝶效应。例如，在你加入 Meta 之前，你曾参与过 Undux 的开发，这是一个 React 的状态管理框架。我很好奇这段经历对你的职业生涯产生了怎样的影响？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：对我来说，支线任务非常重要。我招聘工程师时，这绝对是我会考虑的因素之一。我想要那些有副业项目的人，比如周末可以做一些有趣的事情。甚至包括那些对制作康普茶充满热情的人。你需要的是那些对工作以外的事物充满好奇心和兴趣的人。这些人全面发展，我喜欢和他们一起工作。我的很多成长都来自于参与这些副业项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，像 Undux 这样的工具，源于我觉得当时用 React 管理状态太复杂了。当时最先进的技术是 Flux，后来又出现了 Redux，但我完全搞不懂 Redux。我自认为只是个水平一般的产品开发工程师，不是那种技术高超的系统工程师。Redux 的概念，比如 reducer，更新一个小小的状态都需要非常复杂的流程，我理解不了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我做了一个更简单的版本，效果不错。当时我在一家非营利组织做志愿者，他们开始使用这个版本，他们的工程师也很喜欢。加入 Facebook 后，我发现很多人在使用 Redux 时遇到困难，公司内部有一个 Redux 用户群，里面有很多问题，和我当初问的都一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你作为工程师遇到问题时，有时可能只有你一个人遇到；但很多时候，其他人也会遇到同样的问题。培养一种“直觉”，能够预判其他人可能也遇到的类似问题，这非常重要。我遇到的这个问题显然是其他人也遇到的，我在用户支持帖子里也看到了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在公司内部推出了 Undux。它还不错；虽然算不上很棒的产品，但比 Redux 简单。在 Facebook 的时候，我不知道该如何推广它，所以我就发帖宣传了一下。结果有几个人开始用了。我记得通知团队的 Jeff Case 是这项功能的早期积极使用者，我们因此熬了好几个通宵，调试一些棘手的通知相关 bug。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了推广这项功能，我写了个小脚本，抓取了提交问题的用户群体，并按团队进行了统计。我通过聊天工具联系了每个团队的技术负责人和经理，并为每个团队安排了一场技术讲座。在几周的时间里，我大概做了二三十场，甚至四五十场技术讲座。我记得当时骑着自行车在 Meta 园区里四处演讲，感觉特别棒，因为大家都很投入，也很兴奋有人关心这个问题并想解决它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;曾经，Undux 是 Facebook 最流行的状态管理框架之一。但很快它就被 Recoil 和其他更现代的方案取代了。如今，Relay 之类的框架又开始流行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：这种副业项目会出现在你的绩效考核中吗？或者对你有什么帮助？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我觉得它应该写在我的绩效考核里了。按 Meta 的标准来说，这算是锦上添花。它本身并不能让你直接晋升到下一个级别。那段时间我还有很多其他的支线任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，我突然对 TypeScript 非常着迷。这是我之前在一家公司用过的。当时相关的资源不多，所以我开始写一本关于它的书，因为总得有人做这件事。这门语言太棒了，设计非常出色，包含了很多当时其他语言不具备的理念，像条件类型、字面量类型、映射类型之类的，简直太疯狂了。即使是最资深的 Haskell 程序员也会惊叹，但之前却没有人系统性地写过这些内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我当时完全沉迷其中，写了这本书，几乎耗尽了我整整一年的业余时间。我不推荐别人也这样做，但深入研究的过程真的很有趣。当时我还在旧金山创办了世界上最大的 TypeScript 聚会。能见到 Node.js 的创始人 Ryan Dahl 以及其他这些著名的 JavaScript 大咖，真是太棒了。这让我意识到，他们也只是普通人；每个人都能创造出很酷的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：后来你在 Meta 工作期间，或者甚至在 Anthropic 工作期间，最终有没有大量使用 TypeScript 或达到那种技术深度？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是啊，说起来挺有意思的；我以前其实对编程语言本身一点兴趣都没有。大概十年前，我骑摩托车的时候出了一场很严重的车祸，双臂都骨折了，身上绑着两个吊带。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：我的天哪。你是怎么写出代码的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：那才是最难的部分。我整整一个月都不能写代码，而且我的手现在还有点疼。我写不了 JavaScript（按键多），所以我不得不学习其他按键次数更少的语言。我一开始学的是 CoffeeScript，因为它的括号比较少。这门语言现在应该已经没什么人用了。我也是通过 CoffeeScript 接触到 Haskell 和函数式编程的。你可以用更少的击键次数完成同样的事情，这正是我当时的动力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在加入 Facebook 之前，我在一家对冲基金工作，我的同事 Rick 对 Scala 非常着迷。他带我入门，也让我接触到了函数式编程。如果让我推荐一本对我的工程师生涯影响最大的技术书籍，我会毫不犹豫地推荐《Scala函数式编程》。你可能永远不会每天都用 Scala，但它教你思考编程问题的方式，与大多数人在学校或实践中接触的方式截然不同，这彻底改变了我的编码方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，Scala 和 Haskell、CoffeeScript 一样，是少数几种改变我思维的语言。第一步是 CoffeeScript，然后是 Scala，再然后是 TypeScript。这改变了我的思维方式，因为现在我编码时会用类型来思考。代码中最重要的就是类型签名，这比代码本身更重要。做好这一点能写出非常简洁、健壮的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即使在 Facebook，我主要用 Flow 和 Hack，后来在 Instagram 用 Python，这种思维方式也很有帮助。在 Anthropic，我主要用 TypeScript 和 Python，所以这一点仍然非常重要。更重要的教训就是要学会用类型来思考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，你提到你刚入职时级别偏低，只是个中级工程师，尽管你经验丰富。你说现在回想起来，当时级别低反而是件好事。我很好奇你当时是怎么想的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：在大公司里，对于项目影响和人员影响方面都有很多期望，具体标准因公司而异。很多事情要么关乎项目的影响，要么就是为了完成一堆任务，而这一切都非常耗时。刚开始的时候等级不够，反而给了我探索的空间，让我可以纯粹为了创造而创造，没有太多绩效压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：没错。我很好奇这是否也有助于积累势头。如果你以中级级别加入，然后表现出色，所有人都会说“Boris太棒了”。这很不可思议。而如果你以更高预期级别加入，表现平平，情况就完全不同了。当你一加入就让所有人眼前一亮时，会产生一种奇妙的效果，你会给人留下非常深刻的第一印象。我认为这有助于建立良好的声誉，从而在未来获得更多的信任、更多的项目等等。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，我觉得完全正确。这对于任何公司来说都是很好的建议。很多时候，工程师跳槽的时候会非常积极地争取更高级别，比如“我想去另一家公司，我想升一级”之类的。这样做有很多弊端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：接下来我想问一下是什么让你在 Meta 晋升为资深工程师（E6）。我很好奇你当时的职位，以及是什么推动你晋升到这个更高层级的领导岗位。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：当时的情况是，Facebook推出了群组聊天功能，并且有一个团队负责开发。在加入Facebook之前，我写过很多JavaScript，但在Facebook内部，我之前从未真正写过JavaScript，因为当时主要用PHP。我真的很想写JavaScript。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们当时专门为Facebook群组开发了一个网页界面。很多人使用网页版而不是手机版，因为群组管理员在电脑上用键盘操作起来更方便。但当时这个网站真的很糟糕，它是一个静态网站，完全用PHP编写，只是在不同的地方零散地注入了一些JavaScript代码，结果导致了各种不一致的状态和问题，用户体验很差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我当时想用JavaScript重写整个界面，但遭到了公司内部的强烈反对，主要原因是我们已有的基础设施还没准备好。幸运的是，与此同时，Comet项目启动了，该项目旨在重写facebook.com的桌面版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时有很多核心成员在做这个项目，我真的很想参与。我主动联系他们，询问我能如何帮忙，并提议先在Facebook群组里进行测试和探索。我几乎是没问任何人就直接开始做了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，我跟Facebook群组部门的领导们说：“嘿，Comet项目马上就要全面推行了。这会是一项艰巨的工作。但我们可以提前做好准备，为所有其他团队树立一个迁移标准，并与其他团队建立联系。”我仍然遇到了阻力，比如他们说“你不能安排20个工程师来做这件事”。经过多次讨论和讨价还价，我们最终组建了大约12名工程师的团队，因为这是一个相当大的迁移项目，大约需要一年时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“群组”是Facebook所有产品中规模最大的一个，这有点出乎意料。这次迁移很成功。除了与这个基础架构团队建立联系和友谊之外，有趣的是，我们因此有机会影响Comet项目的发展方向。对于基础设施项目而言，产品团队通常无法左右项目方向，他们更像是客户。但在这个项目中，由于我们早期深度参与，我们创建了许多抽象概念，后来被其他基于Comet进行开发的团队所使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举一个具体的例子，比如“中继变更”。你需要发送API请求，并且需要某种状态一致性。之前存在一个bug：假设有一个按钮，每次按下它都会发送一个POST请求。为了良好的用户体验，你希望在按下按钮后，按钮的状态立刻切换，这需要使用乐观更新。当网络请求返回时，你还需要更新本地缓存以确保一致性。但如果你连续快速点击按钮，响应可能会乱序到达，最终得到的状态可能与用户界面上显示的状态不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我写了一个系统来排队处理这些变更，这样虽然保证了一致性，但牺牲了一点即时性。在当时这是一个合理的权衡，这个方案最终被广泛采用。我就是在这个过程中认识了Joseph和Relay团队里负责数据存储的其他人。这段经历真的很有趣。每当我看到工程师深入钻研，努力弄明白复杂系统到底发生了什么时，我都很欣赏。产品工程师并不意味着你不能构建基础设施，基础设施工程师也不意味着你不能和用户交流。对技术栈的其他部分保持好奇心就好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成功搞定大项目后，才有更多话语权&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：当然。你抢先一步进行Comet迁移和大规模的JavaScript重写，正如你提到的，这实际上让你拥有了更大的影响力和话语权。你所说的机会，是指构建这些对所有使用新平台的人都至关重要的基础产品架构吗？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，这就是一个例子。另一个例子是，Comet的质量比之前的版本高得多，因为它是一个单页Web应用，所以感觉更加流畅和完善。但我们当时还没有完全弄清楚“产品层面的质量”究竟意味着什么。我写了很多笔记试图定义它，也做了很多演讲，向其他团队的成员讲解我们对质量的理解，并就此展开讨论，共同塑造标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：您提到迁移到Comet需要大量人手。我很想知道，如果现在有了Claude Code、Codex等新的AI编码工具，情况会是怎样的。以您现在对Claude Code的了解，如果您负责评估这项工作，您认为现在需要多少工程师才能完成原本需要12名工程师花费一年才能完成的工作？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：为了迁移Facebook群组，最初是12位工程师，但最后可能动用了20到30位工程师，持续了大约两年时间，最终变成了一个相当大的项目。现在来看，可能只需要5位工程师，耗时六个月左右就够了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：也就是说，只需要四分之一的时间，以及不到一半的工程师？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，因为现在你可以让AI并行工作。你让它运行几个小时，它就能生成一个可用的PR。你甚至可以给它装上Puppeteer之类的工具，让它能看到UI并进行视觉调整。大概就是这样。现在，从编码的角度来看，环境已经截然不同了，因为模型更新迭代太快了。如果你三个月或六个月后再问我这个问题，我的答案肯定会完全不同。六个月后，答案可能就变成了：这实际上只需要一个工程师就能完成。现在的变化实在太快了，很难做出准确的估算，也很难预测它们未来会如何演变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，你曾提到过一件事，也许是半开玩笑地说，那时你学会了在向副总裁评审提案时，总是提出三个选项。因为80%的情况下他们只会选择中间的那个。你这么做的想法是什么？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：这话虽然有点讽刺，但或许当时在Meta公司那种特定环境下有点道理。那些远离一线具体工作的决策者，他们想知道你是否认真研究过各种方案和权衡利弊，是否真的做了充分的工作。但同时，他们也想以某种方式参与决策。提供一个“折中”选项，对他们来说比较容易理解和接受。我这么说确实带有讽刺意味，因为并非所有领导者都这样。很多领导者会亲自深入参与决策，他们或多或少信任自己的团队。管理风格有很多种。我记得当时我们有一位技术水平可能不那么深入细节的领导，而这种方法算是帮助她进行决策的一种沟通方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，你与高层管理人员的接触最为密切。你提到你曾向一位高级总监汇报工作，并且参与了很多重要的项目规划讨论。我很好奇，向这样一位资深人士汇报工作，对你后续的成长产生了哪些影响？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，我觉得这很大程度上取决于工程师个人和公司文化。比如，我现在在Anthropic，我觉得在Anthropic，你向哪个层级汇报并不重要。公司里一些最资深的员工也是向部门经理汇报的。很多部门经理本身就是前首席技术官或非常资深的人士，所以实际上这并不构成障碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为向高级别汇报带来的压力，在某种程度上是Meta公司特有的文化现象。我觉得这里存在两种情况。一是，在Meta，你需要非常主动地找到自己的发展方向和上升路径。有些机会你可以自己发现，有些则需要你的经理或技术领导帮你识别和引荐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Meta的PSC（绩效评估）流程出了名的严苛，你必须不断地强调和证明你的影响力。而“职责范围”是造成这种严苛体验的最大因素。如果你有足够大的职责范围，并且执行得当，就能产生显著的影响力。这就是秘诀。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得在Meta，另一个特点是大家都没有很花哨的头衔。即使是最资深的工程师，头衔也只是“软件工程师”，我非常喜欢这一点。贝尔实验室的技术人员也是如此，Anthropic也是如此，但我们在这里更进一步：所有人的头衔都是“技术人员”。不管你是工程师、项目经理还是设计师，头衔都一样。我其实很喜欢这样，因为这样一来，你就可以跳出自己被默认设定的职责范围，去做那些你认为正确和必要的事情，而不用过分在意别人期望你做什么。我认为这种文化正是促成这种灵活性的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：我看到了不设复杂头衔的很多好处。但我也能想到一种情况，也许这只适用于大公司：当你联系公司里的某个人，说“嘿，我想参与这个合作”，如果你的头衔是“总监”之类的，这就能让他们更容易理解你的层级、影响力和协作方式。如果你是设计师或其他职位，在Anthropic规模扩大了一些的现在，你感受到这一点了吗？也许因为大家现在都认识你，所以你没怎么感受到。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，这绝对是一个潜在的缺点。但我认为优点大于缺点。那就是你必须努力争取，用实力和贡献赢得尊重。我觉得这条原则无论在哪家公司都适用。仅仅因为你以前做过一件很棒的事，并不意味着你在新的环境里就理应自动获得权力和尊重。当然，每个人都应该得到基本尊重；但这不意味着你在一家新公司、一个新项目中就理应拥有决策权。即使是那些一开始就拥有“经理”头衔的人，也需要努力争取团队的信任。在某种程度上，拥有经理头衔反而可能让赢得这种信任变得更难。作为独立贡献者，无论如何你都必须用行动证明自己。我认为，正是因为没有那些预设的、等级森严的头衔，才使得这一切变得稍微容易一些，更注重实际贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;如何从技术人转型为高管&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，你逐渐转型为技术主管或高级技术主管，我记得你讲过一些为数百名工程师制定工作计划的故事。你是怎么做到的？如果工作量如此庞大，而你又只有一个人，你是如何向领导层提出如此大规模的工作计划请求的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的，那段时间真是太疯狂了。我当时和蒂娜·萨奇曼共事很多，她现在在微软，但当时是我的经理。然后是我的上级伊芙。当时公司决定对Facebook群组投入更多资源。我刚加入的时候，群组部门大概只有150到200人，等我离开去Instagram的时候，好像已经有600到800人了。扎克伯格一直觉得Facebook应用的核心应该是社群，他希望我们加快速度，把这个想法变成现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为高管，最重要的就是让合适的人负责决策，然后给他们提供资源。在Meta，资源主要就是工程师。我们向扎克伯格推介了这个名为“社区即新组织”的内部项目。他为此投入了一大批人手，我们只需要弄清楚这些人具体要做什么。对他来说，如果事情很重要，就得投入大量人力。事后看来，我会采取不同的做法，大幅减少初期投入的人员，因为真正重要的是解决用户的问题，打造出色的产品，这必须自下而上地进行，需要随着新产品线找到市场契合点而逐步推进，不能一口气做完所有事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但当时，我们必须先把所有事情都规划好。有好几个星期，我都要写一份庞大的规划文档，内容大致是：“好，我们要安排30个工程师负责A项目。这里有三个技术方案，我们选方案二。下一个B项目，我们要安排20个工程师。这里有三个方案，我们选方案一。”就这样一遍又一遍地重复，才能确保这个庞大的项目计划不是完全异想天开。我们进行了一些基础的技术范围界定，大致估算了每个子项目需要的工程师人数。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中有些事情很有意思。我记得我们曾经尝试合并Facebook群组和主页的数据模型。那真是一次非常棘手的迁移。要彻底实现这一点，需要很多年时间，可能还需要数百名工程师，因为必须跨数据模型、产品层、完整性系统和广告系统进行操作。当时，Yosef Carver刚刚加入；我记得他之前在Profile或Events部门工作，这两个部门与Groups部门合作推进这项工作。他当时正在研究这个问题，但还在犹豫不决，无法就数据模型做出最终决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我召集了一群人，说：“好了，公司所有相关的技术主管都来了，我们今天花三个小时，像玩游戏一样，来讨论一下架构设计。”我把大家分成两队，好像是蓝队和绿队（记不清了）。我们给每个人布置了同一个问题：如何合并这些数据模型，并给出了具体的要求。每个人都有三个小时的时间在白板上设计方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最酷的是，一开始我们都觉得这个问题棘手，不知道该怎么做。但最后，我们两队得到的两个方案竟然有80%的相似度！我们可以采取的行动变得非常明确，而那20%的差异也清楚地揭示了风险所在。我们可以通过一些技术性操作来预先承担一部分风险，但我们也清楚地知道可以立即开始执行哪些部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：是啊，我觉得这个形式很有意思：就像一个技术设计竞赛，所有资深工程师都参与其中，分成几个小组，在不同房间里同步进行设计。我以前从没听说过这种形式。你当初在公司内部提出这个设计竞赛的想法时，大家是觉得很兴奋，还是觉得这有点异想天开？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是有点非常规。这种事，你只能硬着头皮去做，靠行动推动。我直接跟所有相关的人说：“嘿，我们要这么做了”，然后就把会议日期记在了每个人的日历上。感觉挺有意思的；作为工程师，你肯定也想参与这种创造性的解决问题的过程。但有时候你需要漫长的过程来达成共识，有时候你则需要快速行动来打破僵局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种情况下，由于技术方向不明朗，采取行动至关重要。但同时，我自己也不知道确切的最佳路径，所以我们必须召集所有关键人员，通过这种高强度、高协作的方式快速达成共识。作为技术领导者，你总是需要在“推动共识”和“果断行动”这两件事之间寻求平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：有了那次为数百名工程师规划项目的经历，你对那些需要快速进行项目范围界定的技术主管有什么建议吗？有什么对你来说行之有效的方法或原则？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我觉得我看到的最大问题是人们花费的时间太长，而且过于纠结细节。细节总是无穷无尽的。最好从宏观层面入手。大多数技术范围界定都可以在30分钟内完成一个非常粗略的版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你不了解所涉及的系统，现在你甚至可以让AI来帮忙。你只需要在代码库中运行一个查询，或者直接问AI：“要实现X功能，会涉及代码库中的哪些系统和模块？”它实际上可以帮你快速梳理出依赖关系。这真是个不可思议的改变。我以前做这些事情的时候，绝对想不到人工智能现在能帮我做到这些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但回到一般性原则，我过去的建议是：设定严格的时间限制。用于初步范围界定的会议最多花30分钟。如果需要深入研究代码，最多几个小时。一定要联系专家，并列出所有需要咨询的专家名单。和他们所有人交流。但不要只是泛泛地征求他们的意见。给他们一个具体的假设或初步设计方案，这样他们才能真正给你有建设性的反馈，你才能以此为依据进行迭代和完善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：继续你的职业经历。你晋升到高级职位的关键，似乎和Facebook的“公共群组”项目有关。我很想了解这背后的故事，以及其中发生了什么有趣的事？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：是的。“公共群组”项目源于我们想让Facebook群组更开放。我们当时想做一个看似简单、但实际非常复杂的改动：让用户无需加入，就能查看和评论公开群组的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这听起来像改一行代码，但实现起来异常困难。因为它触及了核心的数据模型问题：在数据库里，一个发表评论但未“加入”的用户，到底算不算“群组成员”？这引发了我们内部激烈的技术讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去的“加入”需要管理员批准，是一种信任投票。现在对于公开群组，行为变成了“关注”。那么，“关注”和“加入”在数据层面应该是同一回事吗？当时公司里一位资深的元老级工程师鲍勃，强烈认为这必须是两件不同的事，并要求进行大规模的数据迁移来区分它们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这还引发了连锁反应：如果任何人都能评论，垃圾信息怎么办？我通过一个简单的蒙特卡罗模拟，展示了潜在的垃圾信息风险，最终成功说服了公司的诚信团队介入，帮助调整评论排名算法来应对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了完成数据迁移等所有工作，我们组建了一个大团队。当时我和其他几位资深工程师同级，都需要我来协调指导，这让我一度感到有些“冒名顶替”。但最终，正是因为我推翻了之前鲍勃关于数据迁移的决定，才促成了我后来的晋升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我经过审核发现，最初的“成员”字段其实完全可以同时表示“关注者”和“群组成员”，强制区分只会让后续所有开发变得复杂。我敦促负责的工程师撤销了那次迁移。这个正确的技术判断，反而让鲍勃更认可我作为技术领导的能力，因为我能基于事实对资深人士的决定提出异议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你和高级技术主管之间存在严重技术分歧，但结果反而加强了关系。对于如何处理这类分歧而不损害关系，你有什么建议？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;要敢于挑战权威&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我认为核心是赢得信任。在你拥有足够的技术信任之前，很难去挑战权威。一开始，可以更多地倾听、执行，表现出尊重和合作意愿。同时，你必须通过实际行动积累扎实的技术判断力。在赢得信任后，你基于事实和专业提出的异议，才更容易被接受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ryan：关于“冒名顶替综合症”，以及领导那些和你同样优秀的工程师，你有什么建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：别想太多。其实没人真正知道自己在每个新层面上具体该做什么，大家都在摸索。这种感觉会随着时间慢慢消失。某种程度上，始终保持一点点“冒名顶替”感是健康的，那说明你还在努力突破舒适区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：在你职业生涯的这个阶段，你更像技术主管，编码减少。你曾提到，在Meta，有时其他职能（如产品经理）支持不足，你认为这是让工程师更多关注产品、甚至参与产品管理的机会。你如何决定何时该自己顶上，而不是反复要求更多支持？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：关键在于理解利弊和背景。你需要从决策者的角度思考：他们关心什么？手头有哪些项目？做这件事的代价是什么？是否能帮他们成功？有些组织可能确实资源紧张，或认为你的项目优先级不够。有些组织则可能资源充沛。了解你所在的环境和决策者的处境，才能判断是应该自己主动补位，还是能够争取到资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你将自己的成功很大程度上归功于“副业项目”。你对工程师如何寻找这样的机会有什么建议？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我的很多想法来源于日常工作中那些重复、繁琐的部分。工程师的超能力就是自动化。我养成了一个习惯：在代码审查中，如果我多次评论同一类问题，我就会写一条规则来自动化检查它。很快，我几乎自动化了所有代码审查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些“副业”往往就是去改进那些拖慢日常开发速度的基础设施或工具。这不仅能解放自己，也能惠及整个团队。一个核心原则是：如果你遇到了某个问题，很可能其他人也遇到了。为自己打造解决方案，往往就是为很多人打造解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为何从 Meta 转向 Anthropic&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你从 Meta 离职加入 Anthropic，当时是怎么做出这个决定的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我第一次用到 ChatGPT，是它刚推出不久的时候。当时我在日本，一个小地方，几乎没有人能和我讨论技术。我每天刷 Hacker News，用到 ChatGPT 后的第一反应是：这东西太不可思议了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在回头看我们已经习以为常，但当时的大模型给我的感觉，更像是一种“新生命”。它不仅是一项技术，而是一种可以被培育、被引导的存在。我本人很喜欢科幻小说，尤其是硬科幻，所以当我意识到这类模型意味着什么时，心里只有一个想法：我一定要参与其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我去了解哪些实验室在做这件事，也和一些在不同研究机构工作的朋友聊过。第一次和 Anthropic 的创始团队吃饭时，我随口提到了一本科幻小说，结果发现桌上几乎所有人都读过，还能继续讨论别的作品。那一刻我意识到，这是一个真正认真思考“这些技术会把世界带向哪里”的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能正在改变社会，而且是从工程领域开始，逐步扩散到各个层面。我也很清楚，这项技术存在很多潜在风险，甚至可能走向非常危险的方向。对我来说，加入 Anthropic 几乎是顺理成章的选择——如果我能做点什么，那就是站在一个真正把“安全”和“长期影响”当回事的地方。&lt;/p&gt;&lt;p&gt;在 Meta，安全往往被当成一种负担，是和产品对立的事情。但在 Anthropic 完全不同。我们在安全和对齐研究上投入了大量算力和人力，甚至因为不确定模型是否足够安全而推迟发布。随着模型能力提升，风险也在快速上升，这已经不是科幻，而是现实问题。我很庆幸自己能参与其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：加入 Anthropic 之后，你感受到的工程文化和以前最大的不同是什么？&amp;nbsp;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Boris：有两点特别明显。第一，公司虽然已经不小了，但仍然保留着创业公司的“常识感”。很多事情不需要复杂的流程去推动，大家天然会做正确的决定。这是大公司最容易丢失的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，对我个人来说，最重要的是使命感。它让我每天都愿意来上班，甚至周末也愿意写代码，不是因为 deadline，而是因为我想这么做。我在 Facebook 群组项目里感受过类似的氛围，但在 Anthropic，这种感觉更强烈，也更一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Claude Code 为什么能跑出来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你是 Claude Code 的核心推动者之一。当初市面上也有不少类似工具，Claude Code 真正不同的地方在哪里？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：一开始，大家对“AI 编程”的理解非常狭窄，基本等同于自动补全。即便有一些智能体的概念，也更多是问答工具，而不是能真正参与编程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;坦白说，在很长一段时间里，Claude Code 本身也并不好用。即便在内部，我可能只用它写了不到 10% 的代码。关键在于，我的主管一直提醒我：不要按“现在的模型能力”来设计产品，而要按“六个月后的模型能力”来设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来 Sonnet 和 Opus 4 发布后，一切发生了变化。短短几个月，我自己有一半以上的代码都交给 Claude Code 来写。现在在 Anthropic，很多团队 80% 到 90% 的代码都是用 Claude Code 生成的。公司规模虽然扩大了，但工程师的整体生产力反而显著提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：有人担心模型生成大量代码，但质量不稳定、问题隐蔽，你怎么看？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：AI 编程和任何工具一样，是需要学习如何使用的。我们内部的原则很简单：不管代码是人写的还是模型生成的，标准完全一致。代码不好，就不合并。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不同场景需要不同方式。原型、临时代码，可以更“感觉驱动”；核心代码，就必须逐行推敲。大多数时候，我是和模型一起写代码：先制定计划，再让模型生成，再不断修改、清理。某些关键部分，我仍然坚持手写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的模型编码能力当然还不完美，但已经是“史上最差的一代”了。一年前还只是自动补全，现在已经是完全不同的世界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：除了写代码，你觉得 Claude Code 还能用在哪些地方？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：我们公司里的数据科学家、分析师，甚至销售团队都在用它。有人用它写 SQL、跑分析、搭数据管道；销售团队把它接入 Salesforce，直接干活。这完全超出了我们最初的设计预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你怎么看 Claude Code 和 Codex、OpenAI 之间的竞争？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：说实话，我几乎不看其他产品。过度关注竞争对手，很容易让团队迷失方向。我们只专注一件事：解决我们自己、Anthropic 研究人员以及用户真正遇到的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你不是计算机科班出身，这对你有影响吗？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：几乎没有。我学的是经济学，后来辍学创业。编程是一项高度实践性的技能，最重要的是动手做、做出产品。理论当然有价值，但对我个人来说，从来不是关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你效率很高，有什么秘诀？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：现在我的答案只有一个：学会用 Claude Code，而且是同时用多个。你不再是亲自写每一行代码，而是在做统筹、调度和判断。这就是工程的未来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Ryan：你觉得工程师会不会越来越像管理者？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Boris：某种程度上是的。但我依然很享受安静地写代码。现在我每天早上都会启动几个 Claude Code 代理，让它们先跑起来，等我到电脑前再检查、合并或修改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这听起来很疯狂，但它确实有效。甚至一些多年不写代码的管理者，现在也能重新参与进来。我们熟悉的“写代码”这件事，正在发生根本性的变化，而且正在变得对更多人开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AmdLVWMdjOk&quot;&gt;https://www.youtube.com/watch?v=AmdLVWMdjOk&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5gxv5efXhF6fpzXsgcJy</link><guid isPermaLink="false">https://www.infoq.cn/article/5gxv5efXhF6fpzXsgcJy</guid><pubDate>Mon, 19 Jan 2026 11:00:00 GMT</pubDate><author>李冬梅</author><category>AI 工程化</category></item><item><title>Agent 不是渐进升级，而是要“换代”了：Cursor 工程负责人放话未来三到六个月，行业将迎来大变局</title><description>&lt;p&gt;整理 | 华卫、Tina&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，编码 Agent 的变化速度，已经快到让人很难用“功能升级”来形容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果把时间拨回到一年前，Agent 还主要停留在代码补全、对话式改几行代码的阶段；而今天，在 Cursor 内部，工程师已经开始同时运行多个 Agent 并行“甩活儿”，让它们在代码库中自主修改、调试、复盘，再由人类在最后阶段集中审核结果。开发者不再盯着 Agent 的每一步操作，而是开始习惯“等它跑完再看答案”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在最近一次访谈中，Cursor 工程负责人 Jason Ginsberg 给出了一个明确判断：这不是渐进式优化，而是一场正在发生的“换代”。更重要的是，他把这场变化的时间窗口，压缩到了未来三到六个月——在他看来，Agent 将不只是“更聪明”，而是会真正接管更长周期、更复杂的工程任务，整个行业的工作方式也将随之重塑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，我们在不改变原意的基础上进行了翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一年多时间，编码Agent“翻天覆地”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Harrison Chase：Jason，你能跟大家简单介绍一下自己吗？也给大家讲讲 Cursor 是什么吧。&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：好的。我目前在做一款 AI 编程工具，已经在 Cursor 工作了六个月，担任该产品的工程负责人。不过说实话，我日常的大部分时间还是在写代码和做设计工作。在加入 Cursor 之前，我在 Notion 负责 Notion Mail 相关工作。几年前，我创办了一家名为 Skiff 的公司，后来这家公司被 Notion 收购了。所以，我一直都在从事产品开发相关的工作，而且主要聚焦在生产力工具领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：非常棒。我有很多话题想和你探讨。要不我先抛砖引玉，问问你对编码Agent的发展历程，以及这些年来人机交互模式演变的看法吧。你们可以说是这个领域的先行者之一，我认为编码Agent的发展经历了几个阶段的转变：从最初的代码自动补全，到集成在集成开发环境（IDE）中的对话式交互，再到如今出现的各类终端工具，以及基于云端的异步Agent。我很想听听你的看法，你觉得这样概括其用户体验的演变历程是否准确？或者你们团队是如何看待这一发展过程的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我认为编码Agent的发展确实可以用 “翻天覆地” 来形容，而且这些变革基本上都是在一年多一点的时间里发生的。正如你所说，Cursor 最早开启了代码自动补全的先河，这种模式主要是在逐行的层面上提供辅助，适用范围也基本局限在单个文件内。而此后，几乎每隔几个月，我们就不得不提升产品的抽象层级，这其实是一个极具挑战性的产品设计难题。显然，Agent的出现让开发者能够在多个文件之间灵活切换，并且可以放心地让Agent自主完成代码修改工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去两个月左右的时间里，我发现行业又出现了新的转变：开发者现在已经能够做到从项目启动到结束全程信任Agent，并且会对整个代码库中多个文件的内容进行批量审核。因此，我们不得不对产品的整体布局进行大幅重新设计，将核心从逐行的代码差异对比，转向更偏向代码审查的模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望未来的产品开发方向，我们的工作重心其实会更多地放在多Agent协同运行上。我们需要实现的是，能够快速验证这些Agent是否在正常运行，并且可以让它们并行工作，同时避免受到当前单一对话模式下各种选项和选择的束缚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：推动这些变革的核心因素是什么？仅仅是因为大模型的性能变得越来越好，还是有其他更多的影响因素？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我认为大模型性能的提升是一个很关键的因素，这让开发者能够更加信任Agent编写的代码质量。要知道，以前大家必须对Agent生成的代码进行非常全面细致的审查。&lt;/p&gt;&lt;p&gt;同时，现在也有了更完善的代码审查工具。比如我们有 BugBot，市场上其实还有很多类似的工具，它们都能够自动检查代码中存在的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，我觉得从行业文化层面来看，开发者们对Agent工具的接受度和使用信心也在不断增强，甚至可以说已经 “上瘾” 于这类工具带来的便捷。而且，一旦习惯了完全依赖Agent进行编码的工作模式，再切换回传统的编码方式其实是很困难的。所以现在，我们能看到越来越多的开发者已经将Agent辅助编程作为默认的工作方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最顶尖工程师的干活秘诀：全靠Agent？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：你观察到大家使用 Cursor 的方式都有哪些不同？或者你自己平时是怎么使用 Cursor 的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：其实在我们公司内部，工程师们使用 Cursor 的方式就五花八门。甚至团队里有几位工程师，他们完全不使用 Cursor 的Agent功能，比如负责安全和基础设施的同事。所以，确实有一部分用户非常依赖代码自动补全功能，日常使用中大部分操作都是基于补全功能完成的。但令人意外的是，我发现团队里一些最顶尖的工程师，我们称他们为 “核心用户”，他们做任何工作都会完全依赖Agent，甚至会同时运行多个Agent并行处理任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于我个人的使用习惯，我并不会去设计那些复杂繁琐的提示词，也没有什么所谓的 “Agent使用秘籍”。我写的提示词往往都很简短，甚至还会带有拼写错误。我会针对手头不同的工作任务，或者同一个问题的不同模块，同时启动多个Agent，然后等待它们返回结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前我用得最多的是我们今天刚刚发布的一个新功能：调试模式。这个模式下，Agent能够通过生成日志来进行自我评估，之后开发者复现相关操作步骤，Agent就会通过查看日志判断问题是否得到解决。这个功能非常实用，因为它相当于通过投入算力去不断尝试解决问题，最终攻克那些手动排查起来极为棘手的难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：调试模式具体是什么样的？为什么需要专门设置这样一个模式？难道不能自动完成调试吗？直接给Agent下达调试指令不也可以吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：其实我也认同你的这个想法。所以在开发调试模式的时候，我们内部确实有过不少争论。主要原因在于，Cursor 目前已经有很多功能模式了，如规划模式、询问模式等等，这些模式其实不太容易被用户发现。我们一直认为，这些模式都很实用，理想的状态应该是，Agent能够根据用户的操作场景，自动匹配并启用最合适的模式，无需用户手动切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现阶段调试模式之所以需要手动开启，是因为它的交互方式比较特殊。在运行过程中，Agent会暂停当前的工作，向用户提问以获取反馈。如果用户不熟悉这种交互逻辑，可能会觉得比较困扰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：Agent具体会询问哪些问题，又需要用户提供什么样的反馈呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我举个例子吧。假设我正在开发一个前端应用，遇到了一个很让人头疼的问题：菜单总是在左上角弹出。这时候我会对Agent说：“这个菜单需要锚定到按钮的位置。” 随后，Agent会启动服务器，并在整个代码库中添加大量日志，同时提出一系列可能导致该问题的假设，如 “可能是某个定位参数设置错误”、“可能是事件绑定逻辑有问题” 等。之后，Agent会提示我：“麻烦你点击这个按钮，打开菜单，看看问题是否解决。” 如果我反馈问题依然存在，Agent就会查看生成的日志，然后分析判断：“这个假设成立，那两个假设不成立”。通常这样反复两三次之后，Agent往往就能找出并解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你觉得人类还需要手动操作多久？就不能让Agent自主完成点击、测试这类操作吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：一两个月内，毕竟这个行业的发展速度实在太快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：刚才你提到了Agent的多种不同模式，比如规划模式、解释模式、调试模式等等。这些模式在实际应用中到底意味着什么？难道只是为Agent设置不同的提示词这么简单吗？还是说背后有更复杂的逻辑？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：很多时候，确实就是修改一下系统层面的提示词。不过在某些情况下，我们也需要对用户界面进行相应的调整。比如规划模式现在也加入了交互提问功能，运行过程中会主动打断用户操作，寻求反馈。用户有时也可以自行设置参数，如调整Agent打断的频率等。再比如询问模式，它不只是依赖特定的系统提示词，还会限制Agent调用某些与文件编辑相关的工具，以此来保证功能的稳定性和可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：回到之前的话题，关于大家使用 Cursor 的不同方式，你觉得未来使用编码Agent或者说 Cursor，存在所谓的 “最佳方式” 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得并没有什么 “最佳方式”，具体的使用方法很大程度上取决于工程师的个人工作习惯以及他们所处理的具体工作内容。目前行业里，既有异步运行Agent的应用场景，也有开发者深度参与、实时交互的模式，就像一边编程、一边像画画一样实时调整代码或者进行可视化的编辑操作。不过我经常在推特上看到一些所谓的 “Agent使用技巧”，其实对此我是有点持保留态度的。很多人会说 “这才是使用Agent的最佳方式”，但在我看来，这些技巧往往是凭空杜撰的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们团队内部其实并不会使用那些冗长复杂的提示词，也不会采用多阶段规划的策略。大多数时候，我们都是快速迭代，如果Agent运行的结果不理想，就直接终止进程，重新启动Agent。通常这种方式的效率是最高的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;自然 “唠嗑”是Cursor最终交互模式？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：如果让你预测一下一年后的情况，你认为开发者在 IDE、终端以及其他形态的载体上使用 Cursor 的时间占比会是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：当然，我肯定会带有一定的主观偏向性。但我认为，终端工具并不会成为用户的首选。我觉得，真正驱动行业发展的是用户对Agent的信任度不断提升，他们更希望等到Agent完成所有工作后再查看最终的修改结果，然后决定是否采纳，同时也愿意让Agent运行更长的时间，以实现更智能的处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 IDE 之所以至关重要，是因为它是为整个软件开发周期量身打造的工具。从项目的构思规划，到运行代码修改、查看代码内容、清晰对比代码差异、提交代码合并请求，再到在浏览器中预览效果所有这些环节，都可以无缝集成在 IDE 的模块化功能之中。这一点其实很容易被忽视，毕竟 IDE 的这些功能是经过了数十年的发展才逐步完善起来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，当前行业的一个明显趋势是，产品层面的设计变得越来越重要。现在 Cursor 用户使用频率最高的功能，如规划模式，其实都需要可视化编辑器的支持，用户需要能够在编辑器中添加注释，并进行实时交互。一旦脱离了按钮、弹窗和菜单这些可视化交互元素，用户与工具的交互难度会大大增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，我觉得未来并非所有操作都必须局限在笔记本电脑的 IDE 中完成。这种模式并不会被完全取代，具体的使用场景会根据实际需求灵活变化，适用的场景也会更加广泛。用户在更多场景下，都能够使用到 Cursor 这样的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：未来会有更多场景都能用上像 Cursor 这样的工具。你们应该有对应的官网吧？用户可以直接在网页上进行交互操作，是这个思路吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：对，我们确实有官网。这么做的原因是用户可以通过手机等设备随时随地访问。我觉得在不远的将来，用户完全可以戴着 AirPods，开启语音模式，和Agent实时沟通、碰撞想法，让Agent不断优化方案。等用户到了办公室，打开笔记本电脑，就已经有一堆代码修改记录或者演示视频等着审核了，到时候只需要简单确认通过或者驳回就行。如果某些细节还需要微调，再把项目下载到本地修改就好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我认为 Cursor 真正的优势，在于围绕Agent交互打造的整套设计和用户体验体系。你之前在 Notion 工作过，我记得即便是在生成式AI普及之前，Notion 的设计和用户体验就已经广受认可了。当然，他们在生成式AI时代也很好地完成了转型。从一家在生成式AI普及前就拥有出色设计积淀且顺利完成转型的公司，再到如今专注Agent相关工作，你觉得Agent的出现给产品设计和用户体验带来了哪些变化？现在的工作模式和之前有相似之处吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得总体来说，我们产品的大部分设计其实并不是AI专属的。要知道，产品可用的交互组件和用户体验模式就那么多，市面上的应用本质上也都是基于一些传统的模式搭建的，如收件箱、仪表盘、聊天界面，这些都是很成熟的设计。所以我们的工作核心，更多是把这些现有的设计模式进行合理组合，然后在产品中恰当地呈现出来。这一点和 Notion 的产品理念是相通的，同时也是 Cursor 和集成开发环境（IDE）的核心特质：极高的模块化程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为用户，你会发现每个人的 IDE 界面布局都可以千差万别。你可以自定义面板布局，把任意组件拖放到任意位置，和坐在你旁边的同事设置出完全不同的界面。我认为这种模块化设计对产品的适应性至关重要，毕竟如我之前所说，Agent的能力发展日新月异，用户对产品的需求和期待几乎每隔几周就会发生变化。几个月前我们推出 Cursor 2.0 的时候，并没有把原来的产品推倒重来，只是把各个功能模块重新组合，调整为侧边栏收件箱式的管理布局，同时优化了聊天界面的信息密度而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：听你这么说，很多组件的底层逻辑其实是相通的。那有没有出现新的组件？或者某些组件的优先级发生了变化？毕竟这些组件最初都是为 “人类与软件交互”“人类通过软件协作” 的场景设计的，现在加入了Agent这个新角色。这其中有没有产生什么新的变化？还是说其实本质上没有太大不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我认为底层的设计逻辑和核心要素其实没有变，关键变化在于谁在主导界面交互。而在这个核心框架下，其实可以演变出无数种交互形式。就拿交互的抽象层级来说，一年前大家使用Agent的时候，都恨不得盯着它的每一步操作，全程 “盯梢”。但现在Agent的操作步骤变得无比繁杂，用户根本看不过来。所以我们需要优化信息呈现方式：如何对操作步骤进行分组？如何提炼关键信息？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当用户足够信任Agent的操作后，我们就需要把重点放在文件的实际修改内容上，并且为这些修改添加更详细的注释说明。当然，我们也可以进一步提升交互的灵活度，比如聊天对象不再局限于单个Agent，而是可以同时和多个Agent对话。这就需要一套更智能的后台交互逻辑来支撑 ，系统要能识别用户在和哪个子Agent对话，并且协调这些Agent完成对应的修改。未来这种交互的抽象层级还会不断提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你觉得交互的抽象层级最高能达到什么程度？我知道预测未来很难，但还是想听听你的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得未来，我们现在看到的各种操作选项，如选择模型、选择功能模式、选择运行环境这些都会逐渐消失。最终的交互模式会变得像和真人对话一样自然。但这并不意味着任何人都能随便写代码，在那个阶段，这个工具依然是为专业工程师服务的。因为你还是需要具备专业的行业术语知识，清楚自己想要修改的内容是什么。做产品的人，要明确自己想要的工作流程和功能需求；做基础设施的人，要足够了解代码库，知道什么样的架构和系统设计最适合当前要开发的项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我想强调的是，随着抽象层级的提升，我们并不会摒弃现有的功能。用户依然可以随时深入底层，查看细节、调整参数。只是产品的默认交互方式会不断优化升级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Cursor内部工作揭秘：少审代码、高频反馈&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：你之前提到了人类在Agent工作流程中的角色，比如查看代码差异、进行代码审查。你觉得AI会给代码审查工作带来哪些改变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：首先，就我们产品团队的工作模式来说，现在人工审查的比重已经大幅降低了。我们有一个叫 BugBot 的工具，它会自动检测代码问题，并且自主完成修复，还会在持续集成（CI）流程中不断迭代优化。这个工具的表现非常出色，也让我们对AI审查的代码质量更有信心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是信息的语义化分组。用户查看代码差异时，可以清晰地看到Agent做了哪些修改。我们甚至可以展示Agent的原始指令，更理想的状态是，Agent能够像人类一样，在处理大型代码合并请求时，为每一处修改附上注释，说明这么做的原因。我觉得这虽然算不上颠覆性的变革，但确实能给代码审查工作带来显著的优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：出于好奇，我想问一下，Cursor 的工程师用 Cursor 写代码，用 BugBot 审查代码，那他们还需要和其他工程师沟通协作吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：哈哈，这个问题很有意思。如果你以工程师的身份加入 Cursor，会立刻发现一个现象：所有人都在深度使用自家产品。我记得我入职第一周的时候，修改了一个快捷键设置。那个快捷键是 Alt+Shift+Command+J，非常冷门，我当时觉得选这个键肯定没人会注意到。结果刚改完不到半分钟，就有三个同事在 Slack 上发来消息：“你改的这个快捷键直接打乱了我的工作流程！到底怎么回事？”几乎任何产品改动，都会立刻收到同事们的强烈反馈。我觉得这其实是一件好事，大家就是在这种高频的反馈和交流中，快速推进产品迭代的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：从组织管理的角度，你们有没有采取什么措施来鼓励或者引导这种高频反馈的协作模式？毕竟大量的反馈涌进来，有时候也会让人应接不暇。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：在我创办自己的公司之前，工程师们也会用邮件沟通，但用得并不多。大家甚至会说：“邮件只用来收垃圾邮件和购物通知，可别用它来发长篇大论的工作内容。”而在Agent这个赛道工作，其实完全不需要依赖邮件这种低效的沟通方式。我们团队的所有人都全身心投入工作，毕竟这是一个竞争非常激烈的领域，大家都对产品开发充满热情，会自然而然地用各种即时沟通工具协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，我在规划产品功能时，会遵循一个核心原则：我能开发什么功能，让自己的日常工作更轻松？ 具体来说，就是思考 “做什么能帮我明天更高效地完成工作，不用再处理那些烦人的报错和问题”。这个原则指导着我们的大部分工作。毕竟这种功能开发出来之后，我们自己能立刻受益，比如修复了一个烦人的漏洞，以后上班就不用再被这个问题困扰了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;迭代狂飙背后，核心功能竟来自员工 “自嗨”？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：你觉得你们的产品路线图，有多大比例是由 “让自己工作更轻松” 这个需求驱动的？又有多大比例是来自外部用户的需求？这个比例随着公司发展有变化吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：这个比例确实随着公司规模的扩大在变化。现在我们也会制定月度的产品路线图和目标，但说实话，我们很多核心功能都来自自下而上的创新。比如 Cursor 的Agent功能，这可以说是大家提到 Cursor 时最先想到的核心功能。这个功能是我们团队的一个人开发的，最开始所有人都不看好这个想法，但他很快做出了原型。大家试用之后都惊叹：“哇，这东西居然真的能用！”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我之前提到的调试模式也是如此。感恩节假期的时候我闲着没事，就开发了这个自己很需要的功能，现在这个功能也即将上线。这些功能的开发初衷，都是为了解决团队内部的需求。我们判断一个功能是否具备发布条件，一个重要的衡量标准就是内部的使用率和认可度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你们的产品迭代速度快得惊人，是怎么保持这种高效的开发节奏的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：说实话，我们的工作流程其实非常精简，没有太多繁琐的制度。公司里虽然有几间会议室，也有一两位产品经理，但我们很少通过撰写文档或者开对齐会议来推进工作，大部分的讨论和决策都是在代码层面完成的。而这一切能够实现的核心原因，是我们对人才的极高要求。今年年初的时候，公司总共也就 20 人左右。之所以团队规模增长缓慢，就是因为我们的招聘门槛高到近乎苛刻。我们会反复评估：这个人很优秀，但他能成为团队里最顶尖的那批人吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为团队里的每个人都足够出色，所以我们可以放心地把任务交给任何一个人。团队成员的主观能动性都极强，从提出想法、设计用户体验，到在推特上回复用户的支持请求、和企业客户沟通需求，再到最终将功能落地，整个流程都能独立完成。所以说，我们能保持这样的速度，归根结底还是人的因素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你们是如何规划产品路线图的？你刚才提到了以月为单位的规划周期，这是目前的常规规划时长吗？有没有更长期的规划？另外，行业技术迭代的速度实在太快了，你们是如何平衡 “跟进现有技术浪潮” 和 “实现技术跨越式发展” 这两者的？会不会主动预判技术趋势，提前布局未来方向？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我们确实会投入不少精力去思考未来，比如预判未来三个月可能实现的技术突破，然后主动押注相关方向，团队里有相当一部分人都在做这类前瞻性的工作。我们制定的月度路线图更多是围绕核心产品功能展开，聚焦于用户的实际需求以及那些能优化日常使用体验的功能。而那些需要投入两个月时间重构底层逻辑的重大项目，则会纳入更长期的规划范畴。&lt;/p&gt;&lt;p&gt;此外，我们的应变能力其实非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时候我们会提前拿到新模型的测试版本，试用之后如果发现它在某些方面表现特别出色，团队成员往往会主动利用周末时间加班，争取在新模型正式发布前就完成相关功能的开发。很多重要功能其实几天之内就能搭建完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：说到模型，你们发布了自研的 Composer 模型。开发这个模型的初衷是什么？目前用户的使用情况如何？这个模型有没有改变大家使用 Cursor 的习惯？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我们发现，工程师使用我们产品时的编码场景，需要有专门适配的模型来支撑。Composer 模型就是针对这类场景打造的，它定位非常明确，具备速度快、质量高、逻辑智能三大特点，尤其适合 “人机实时协作” 场景。我自己做前端开发时就经常用它，因为我需要频繁做出细微的交互设计决策，这就要求Agent能在几秒内给出反馈。Composer 就像一个高效的协作伙伴，能快速响应需求、碰撞想法，和那些适用于长周期异步任务的模型形成了很好的互补。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：Cursor 的Agent相关研发工作是全员参与，还是有专门的团队负责？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我们确实有专门的团队负责Agent的性能优化，他们主要聚焦于工具链、调度框架的搭建以及效果评估。但正如我之前所说，我们的团队架构并不僵化，没有严格限制大家的工作范围。比如核心产品团队的工程师在开发规划模式时，如果需要对Agent进行调整，就会和Agent团队密切协作。而且在开发过程中，我们依然会深度使用自家产品进行测试，团队成员会分享使用感受，以此来评估功能的实际效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：无论是Agent团队的成员，还是其他团队中擅长Agent研发的工程师，他们身上有没有什么共同特质？他们的专业背景或者个人能力有没有什么特别之处？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得他们大多是偏产品方向的人才，而不是传统意义上的机器学习或算法研究专家。这些人经常在不同团队之间轮岗，因为Agent研发需要对用户的最终使用体验有很强的直觉，同时还要能准确解读团队的反馈意见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：上周你们和 OpenAI 合作发布了一篇博客，内容是针对 OpenAI 的新模型优化 Cursor 的Agent调度框架。我在推特上经常看到大家讨论 “Agent调度框架” 这个概念。你们是如何看待模型的底层支撑架构的？这类架构是否需要和特定模型深度绑定？比如 Composer 模型和 CodeLlama 模型，对应的架构会不会有很大差异？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我其实没有深度参与这方面的工作，但据我了解，我们的核心目标是打造高度灵活的架构。毕竟我们需要不断尝试新技术、新功能模式，所以架构必须能够随着模型能力的升级快速适配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：很有道理。毕竟整个行业都在飞速变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开放问答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提问者 1：刚才提到了新增的可视化浏览器功能，我发现有些工具比如 Lovable 也有类似的功能。请问这个功能是朝着 “沉浸式可视化编码” 的方向发展吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得它并不是为沉浸式可视化编码设计的。就像我之前说的，这个功能最初是我为自己开发的，我本身就是一名做产品的工程师，它的核心用户群体其实是专业工程师和设计师。大家在开发应用时，肯定都遇到过这种情况：精心设计的界面，最后却变成了大家都看腻了的紫黄渐变配色。这个功能就是为了让大家能够精准把控细节，比如把内边距调整到精确的像素值。它为用户提供了一套更直观的 “视觉化操作语言”，比纯文本指令的精度更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且就算不使用侧边栏，你也可以直接点击页面元素，随时输入提示词下达指令。借助这个功能，你可以在几秒内同时启动六个Agent。如果开启热重载功能，你的网站会实时呈现修改效果，用起来其实还挺有意思的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 2：我特别喜欢你们的浏览器Agent，一直在用。但我发现一个小瑕疵：我想持续迭代优化设计方案，可Agent总是会中断我的工作，直接提交代码合并请求。未来有没有可能实现不间断的持续迭代？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：当然可以。未来的发展方向就是让Agent具备自主评估能力，根据需求长时间持续运行、循环迭代。现在的调试模式还需要人工点击按钮来确认日志信息，但这只是过渡方案。理想的状态是，Agent能够自主完成评估、迭代，直到彻底解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 3：我不知道你是否深度参与Agent相关的研发工作，但我注意到 Cursor 的内存管理功能做得很好。它可以根据工程师个人、部门乃至整个公司的偏好、规则和流程，自主管理相关信息。我们都知道，信息和上下文对Agent来说至关重要。请问你们有没有计划进一步拓展和升级这个功能？尤其是在长上下文处理方面，你们有什么思路？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我们正在进行大量的实验和探索。目前已经落地了规则管理、内存记忆、技能库等多个功能模块。现阶段，我们主要在研究高效的信息摘要技术。另外，借助我们的自研模型，我们也在探索让模型自主识别对话或代码中反复出现的关键信息。当然，跨组织的信息共享功能也很值得探索。不过这里有个需要注意的点，相关规则和信息可能会随着模型的迭代而过时。所以我们必须确保用户能够轻松更新这些内容，避免被过时的规则束缚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 4：关于你们发布的 Composer 模型，我认识一些开发者，他们基于 Gemini 模型微调了一个医疗领域的专用模型。但他们发现，这个微调后的模型效果还不如直接用原生 Gemini 模型做单次提示词调用。他们分析的原因是，微调模型需要持续维护，要跟上 Gemini 等基础模型的更新节奏。请问你们是如何制定策略，确保 Composer 模型不会落伍的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：你说的是 Composer 模型，对吧？我们会持续对它进行迭代优化，它并不是一个静态的模型。我们的核心关注点，是在速度和智能之间找到最佳平衡点，满足 Cursor 用户在大部分场景下的需求。不过在长上下文处理这类特定领域，我们确实还有提升空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 5：我自己是产品经理，一直在用 Cursor 做原型开发，甚至在团队里还客串设计师，用它替代 Figma。我很好奇，有没有用户是在使用 Cursor 之前，从未安装过任何集成开发环境（IDE）的？这类用户会不会成为你们未来重点关注的群体？毕竟现在的编码Agent已经足够强大，很多工作都能在上面完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：坦白说，我们目前并没有把这类用户作为核心关注点。当然，我们认同工具的使用门槛确实需要不断降低，而且 Cursor 的易用性也在持续提升，比如新增的浏览器工具对设计师就很友好。但我们的核心目标，其实是赋能顶尖工程师。我们一直在思考：如何让世界上最优秀的工程师变得更加强大？在这个过程中，我们开发的工具自然会惠及更多人群。不过在产品优化方面，我们确实还有很多工作要做，如优化新手引导和环境配置流程。毕竟设计师和产品经理在配置 GitHub 等工具时，经常会遇到困难。我们希望通过优化这些环节，吸引更多用户尝试 Cursor。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 6：我一直在尝试用 Cursor 做智能合约的验证矩阵构建和试运行逻辑测试。请问在深度质量检测和安全加固方面，有没有什么不太为人知的实用工作流可以推荐？或者刚才提到的调试工具能不能派上用场？我对智能合约的质量检测特别感兴趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：说实话，我们正在尝试让Agent自主完成测试工作，不过这项功能目前还没有完全发布。对于从事质量检测工作的人员来说，我强烈推荐试试我们刚发布的调试模式。这个功能定位问题的逻辑非常清晰，几乎可以说是确定性的，用起来会很有帮助。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问者 7：您认为未来两到四个月，Cursor 面临的最大机遇是什么？会不会是语音Agent？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Ginsberg：我觉得机遇不在于语音Agent。用户现阶段最核心的需求，其实是让Agent变得更智能、运行时间更长、能处理的任务更多。现在的很多Agent，本质上只是在 “读取代码”，并不能真正判断修改后的代码是否有效。未来的发展空间非常大，我们可以投入更多算力，让Agent承担更多人类目前负责的校验工作。我觉得未来三到六个月，整个行业都会迎来巨大的变革，非常值得期待。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dKSGK-fPFyU&quot;&gt;https://www.youtube.com/watch?v=dKSGK-fPFyU&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t2GqLirT9xsmYmKLW22C</link><guid isPermaLink="false">https://www.infoq.cn/article/t2GqLirT9xsmYmKLW22C</guid><pubDate>Mon, 19 Jan 2026 10:32:18 GMT</pubDate><author>华卫,Tina</author><category>AI&amp;大模型</category></item><item><title>最烦做演讲！黄仁勋曝英伟达养了61个CEO、从不炒犯错员工：CEO是最脆弱群体</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“世界上不会再出现第二个我这样的 CEO 了。”近日，英伟达联合创始人兼首席执行官黄仁勋（Jensen Huang）在一场私人访谈中这样说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据称，这场深度对话已经酝酿了三十年，将黄仁勋鲜为人知的一面展现在大众眼前。主持人Jodi Shelton与黄仁勋的职业交集始于三十余年前，彼时，图形处理器（GPU）尚未掀起席卷全球的AI革命。从加速计算的源头到生成式AI的前景，这场对话堪比一堂远见大师课。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在访谈中，黄仁勋表示，从某种意义上说，英伟达其实有 61 位 “CEO”。过去这些年，包括他在内，很多人都犯过严重的错误，但在英伟达，从来没有人因为犯错而被解雇。“我们打造了一个足够安全的环境。”他还透露，CEO 这个职位，远比人们想象的要脆弱得多。“实际上，我们可能是公司里最脆弱的一群人。不过对我来说，承认这种脆弱，并不是什么难事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，他提到，在很多方面，自己都算是一个 “不情愿的 CEO”。“公开演讲简直让我怕得要死。比起待在公司外面抛头露面，我更喜欢扎根在公司内部；比起发表演讲，我更喜欢安静做事；我甚至一点都不喜欢做主题演讲，但为了公司，我必须去做这些事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，黄仁勋称，英伟达的成功，绝不是靠产量取胜。“虽然是英伟达发明了 GPU，但从产量来看，我们其实是全球最小的 GPU 制造商。很多不知名的厂商，GPU 产量都比我们高。”而“没有终极目标” 这一点，对英伟达的发展真的起到了至关重要的作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于五年后的世界，黄仁勋断言，英伟达和整个行业在AI领域的投入，必将彻底改变计算机的运作模式，未来的计算机，将从 “由人类编程” 进化为 “在人类引导下自主学习编程”。并且，100% 的工作岗位都会发生变化，但不会有 50% 的岗位消失。未来的趋势不会是就业岗位减少，反而是大家会变得比现在更忙碌。并且，那些现在没有工作的人，很可能会因为AI获得谋生的手段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;网友们纷纷就此次访谈对黄仁勋评价道，“我从未见过他如此坦诚直率，真是不可思议。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是详细对话内容，我们在不改变原意的基础上进行了翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“走了整整33年才看到成果”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：大众其实特别好奇像你这样的人，毕竟你们正在定义科技的未来，而科技的未来就是整个世界的未来。所以我们想做的，是挖掘你成功光环背后的个人经历以及支撑你走到今天的价值观。你对这个定位怎么看？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：说实话，不太喜欢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：真的不喜欢吗？可你现在是名人啊，大家都想了解名人的故事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我从不觉得自己是名人，也根本不是什么名人。我只是恰好执掌着一家举足轻重的企业，是这家堪称史上最成功的科技公司之一的 CEO。很早以前，我们就做了一些正确的决策。回溯到 1993 年，我们就立志要重塑计算行业，而且对于计算机的架构，我们有着自己独到的见解。在很长一段时间里，这个观点都不被看好，甚至颇具争议。要知道，当时整个行业的焦点都在微处理器和 CPU 上。说起来，我和你就是在那个时期认识的。我们早在 1993 年底或者 1994 年就相识了，对吧？从那时起，英伟达就在做我们现在依然在做的事：重塑计算。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：没错，我记得很清楚。那时候的硅谷，正处在 CPU 为王、摩尔定律大行其道、个人电脑革命如火如荼的年代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是啊。而且我们早期的客户，全都是 PC 芯片组领域的初创公司。这些企业可以说是半导体行业辉煌版图的奠基者，像Cirrus Logic、S3 Graphics、Western Digital、Trident Microsystems，你还记得这些名字吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：当然记得。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：这些公司，称得上是英伟达的 “前辈”。而现在，我们依然在这条路上前行，致力于打造一种全新的计算模式。这条路，我们走了整整 33 年才看到成果。我只是恰好成为了这家公司的 CEO，仅此而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：可能对你来说，这一切是水到渠成，但对整个世界而言，英伟达的崛起堪称横空出世。大概从 2023 年 11 月起，整个世界的科技格局都因你们而改变。你是怎么看待这次转型的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：要知道，想要创造未来，就必须在未来到来之前，先置身于未来之中。坦诚地说，从我们发明 CUDA 技术、推出相关产品的那一刻起，就已经踏上了通往未来的道路。英伟达最让我骄傲的一点是：我们不仅擅长技术发明，更擅长把技术转化为产品推向市场。世界上有太多的公司、科研人员和发明家，他们确实创造出了先进的技术，但最后往往只能感慨 “这个技术我早就做出来了”、“这个想法我早就有了”。每次听到这种话，我都觉得很惋惜。这些优秀的发明家，遗憾的是没能遇上同样优秀的产品创新者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所谓产品创新者，就是能把一项技术发明转化为一款能推向市场的成熟产品的人。而这还不够，你还得为产品制定精准的市场策略，甚至需要亲手培育出一个全新的市场，让市场能够接纳你研发的产品和制定的策略。英伟达就是这样一家公司，我们具备技术发明、产品创新、策略制定、生态构建乃至市场培育的全链条能力，而且我们已经多次成功做到了这一点。所以对我来说，这种 “身处未来” 的状态，已经持续了很长时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：确实如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：很久以前，我们有一个战略，现在已经不怎么提了，叫 “CUDA 无处不在”。很多人都听过我当年四处推广 CUDA 的故事，跑遍各大高校、初创企业和成熟企业。有时候，台下听众加起来也就三个人，但我还是会掏出笔记本电脑，为他们演示 CUDA，告诉他们这项技术将如何改变世界。我走访了无数科研机构和实验室，参加了数不清的行业会议，推广 CUDA 的次数，估计比世界上任何人都多。长久以来，我一直沉浸在这样的 “未来图景” 里，讲的故事多了，甚至会产生一种 “未来已经到来” 的错觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：确实有这种感觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：所以现在看到这一切成为现实，我依然满心欢喜。而且在我看来，这一切其实并不意外，因为支撑英伟达发展的是计算机科学领域最根本的底层逻辑，不是靠一时的直觉，也不是凭主观的喜好。从很多方面来说，如今的成果是一种必然。但我想说的是，当你把一件事物的速度提升一千倍，或者规模扩大一千倍、体积缩小一千倍时，无论这件事物原本是什么，都会发生质的飞跃。而这种质变最终带来的结果，往往是超乎想象的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们早就预见到深度学习技术有着巨大的扩展潜力，这也是我们举全公司之力押注这一领域的原因。我们知道，AlexNet（深度卷积神经网络）绝不会是深度学习的终点，这种技术架构天生具备极强的可扩展性，再加上全球海量的数据资源，深度学习的爆发是水到渠成的事。不过我当时也清楚，有一项技术会成为我们前进路上的障碍，那就是无监督学习，或者说自监督学习，也就是让计算机摆脱人工标注数据的束缚，实现自主学习。因为人工标注数据的效率，迟早会成为技术发展的瓶颈。而当无监督学习技术取得突破的那一刻，我就知道，我们的时代来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在不久前的投资者路演上，还有人跟我说，我当时就明确跟他们提过这场 “质变”。如果你去回看当时的财报电话会议，就会发现每当谈到对世界至关重要的技术话题时，我都会把这一点讲得非常透彻。在每一场投资者路演，在每一个我演讲的场合，我都会强调这个观点。如今，无监督学习技术确实取得了重大突破，深度学习的规模效应也彻底释放出来，我们才算真正驶入了发展的快车道。但即便如此，这项技术如今能解决的问题，依然让我感到惊喜。我们早就预料到技术会发生质变、计算平台会迎来变革，但我们没想到，变革的成果如此丰硕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在能够解读蛋白质的 “语言”、细胞的 “语言”、量子的 “语言”，能够读懂世间万物的各种表征形式。过去我们用来描述信息的方式，如今正在被彻底重塑。从几何图形、纹理材质到如今的 3D 高斯和 3D 点云，信息的呈现形式日新月异。这种感觉就好像人类突然变得无比聪慧，连英语这种语言体系都随之改变了。我们不再沿用过去的词汇、语法和句式，因为我们的智慧已经进化到了一个全新维度，能够用一种全新的方式进行交流。或许未来人类的交流方式会变成简单的 “嘀嘀嗒嗒” 的信号声。这让我想起了电影《降临》里的场景，人类突然开始用抽象的图形进行沟通，仅仅通过图形就能传递海量的信息，实现更深层次、更高效率的交流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最不可思议的是，我们现在解决的很多问题在过去是完全无法想象的，而且解决问题的速度也远超以往。过去我们常说摩尔定律，而现在英伟达的发展速度完全可以用 “英伟达定律” 来形容，比过去快了整整一千倍。未来十年必将是波澜壮阔的十年，光是想想就让人无比兴奋。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：要做到你所做的这些事，要能够预见未来，并且坚信未来一定会到来，需要何等强大的自信啊。就像你之前说的，我们 1994 年就认识了，这么多年来，你一直都是这个样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是啊，我记得很清楚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：那时候我才二十几岁。你应该比我大一点吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：当时我差不多 29 岁，快 30 岁了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;英伟达有61位“CEO”，从没有人因犯错而被解雇&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：我还记得我们第一次见面的场景，当时我是为了给杂志写稿采访你。我问你：“黄仁勋，硅谷人才流动频繁，很多人来了又走，你会担心这个问题吗？” 毕竟当时很多 CEO 都在抱怨这件事。而那时你才 29 岁或 30 岁，你是这么回答我的：“英伟达既不是教堂，也不是监狱。想来的人可以来，想走的人也可以走。” 我当时听完特别震撼，心里想着：“这个人到底是谁啊？” 年纪轻轻，却有着如此的自信和智慧。我还听过一个类似的故事，张忠谋（ Morris Chang）第一次见到你的时候，你当场就说：“我会成为你最大的客户，至少也是最大的客户之一。” 他当时的反应是：“哇，这小伙子可真有魄力。” 所以我很好奇，你这么年轻的时候，这份自信是从哪里来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：哈哈，你要知道，什么都懂其实也挺痛苦的，我开玩笑的。对了，张忠谋要是知道英伟达现在是台积电最大的客户，一定会很开心的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：那是肯定的，他肯定会为你感到骄傲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我也为他感到骄傲。要知道，在个人电脑革命时期，英伟达就曾是台积电最大的客户。如今，我们再次成为了他们最大的客户，对此我感到非常欣慰。言归正传，我觉得一个人必须坚信自己所相信的东西。而且这份信念，不能建立在道听途说之上，不能因为别人说了什么，你就去相信什么。你必须认真思考，梳理出自己相信这件事的逻辑，并且把这些逻辑拆解成可靠的底层原则。之后，你还需要定期检验这些原则，确保你所秉持的信念、所付诸的行动，都是建立在坚实的基础之上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果这个基础不够稳固，或者因为某些原因发生了变化，那就说明它可能并非真正的底层原则 ，也许它并没有锚定在物理规律或客观事实之上。一旦出现这种情况，你就要重新评估，然后及时调整方向。我一直都是这样做的。而且，如果你真心相信一件事，就应该付诸行动去实现它。我从 1993 年起就坚信我们正在做的事情，直到今天，这份信念依然没有改变。正因为坚信不疑，所以我才会不断地推演，不断地在脑海里进行逻辑梳理。我会持续复盘过去的决策，也会不断预判未来的趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就像昨天我们开了那么多场会议，每场会议上，我都会重新梳理我们一路走来的逻辑。你会发现，过去的那些假设，有些是正确的，但也有些是错误的。正是因为我们足够灵活，能够根据实际情况及时调整方向，才最终走到了今天。所以，时常回头复盘、重新推演过往的决策，是一件很有意义的事，它能帮你更好地锻炼向前推演的能力。正因为我一直坚持这样做，所以我始终活在自己认定的真相里。直到现在，我依然觉得自己只是英伟达的一名员工。我非常在乎这家公司，但公司里有很多人都和我一样，对这家公司倾注了深厚的感情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一家治理完善的公司里，CEO 的角色定位是很明确的。CEO 需要向董事会汇报工作，而董事会则要对股东负责。如果 CEO 的工作表现没有达到董事会的预期，不管董事会有 12 位、13 位还是 15 位成员，他就会被解雇。所以说，CEO 其实也是公司这个组织里的一名员工。这就是为什么我说，英伟达既不是教堂，不是想来就能来；也不是监狱，不是想走都走不了。这种心态能让你始终保持脚踏实地，保持谦逊，保持锐意进取的状态，因为你必须每天都努力，才能对得起自己的这份工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时候会有人问我：“黄仁勋，你热爱自己的工作吗？” 我会告诉他们，我并非每天都热爱这份工作，但我每天都会全力以赴去做好它。我觉得，这种态度源于两个方面：第一，我坚信自己是这份工作的最佳人选；第二，我必须每天都努力，才能配得上 “最佳人选” 这个身份。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：在大家眼里，你就是英伟达的代名词，英伟达就是你。这么多年下来，你已经和这家公司深度绑定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我应该是英伟达内部被拍照最多的人吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：没错。不过，要是将来换了新的 CEO，这个人真的能接好你的班吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：世界上不会再出现第二个我这样的 CEO 了。原因很简单，我是被这家公司一步步培养起来的。刚创立英伟达的时候，我对怎么当 CEO、怎么做战略规划、怎么打造产品、怎么开创一个全新的行业，一窍不通。我只知道怎么融资，却不懂怎么和股东沟通，不了解股东、政策制定者、各国领导人以及企业管理者的想法，也不知道该如何把握员工的心态、如何打造企业文化，甚至连 “企业文化” 这个词到底意味着什么，我都无法准确界定，让我制定公司战略那更是天方夜谭。这就是我第一天接手工作时的真实状态。而在过去的 33 年里，我在这些领域都一步步做到了得心应手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说这个世界上有谁能称得上是 “企业战略宗师” 或者 “行业开创者”，那这个人大概就是我这样一个小个子。我把自己的整个职业生涯都投入到学习这些能力上，而且我本身就是个好学生。除此之外，我对这份工作的投入程度和深厚感情，是很难通过招聘来复制的。在我心里，英伟达就像我的孩子一样，我对它倾注了全部的心血。我的家人也陪着我一起，为这家公司的成长付出努力。这种对公司的特殊情感，是很难被替代的。毕竟 33 年来，我见证了英伟达的每一次成功、每一次失败、每一次挫折，亲历了它做过的所有明智决策，也目睹了它犯下的各种错误。这种对公司的深刻理解和情感联结，不是随便招一个能力出众的人就能替代的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过从另一方面来说，英伟达的管理团队架构其实早就做好了准备。我现在有将近 60 位直接下属，他们中的每一个人，放到其他公司都能胜任世界级 CEO 的职位。我总是当着他们的面推演各种决策逻辑，我的每一个决定，都是在他们的注视下做出的，我会把背后的思考过程原原本本地讲给他们听。公司的每一次成功、每一次挫折、每一个挑战、每一场困境，我都会和他们一起复盘。所以从某种意义上说，英伟达其实有 61 位 “CEO”。他们每个人都对这家公司饱含深情，很多人已经在这里奋斗了 33 年。我认为，英伟达的成长模式是独一无二的，这也造就了它无可比拟的韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：显然，你搭建的这套管理架构在行业内已经成了一段传奇，所有人都在谈论你这近 60 位直接下属。要让这样的架构顺畅运转，这些人肯定都得是万里挑一的顶尖人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：没错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：他们不光要头脑聪明，毕竟硅谷从来不缺聪明人，更得是适配英伟达的顶尖人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：确实如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：那你能不能跟我说说，你是怎么筛选和培养这些人才的？另外，我记得你有个原则，找不到合适的人，就宁可让职位空着。我想到了Colette Kress的例子，你当时面试了 22 位首席财务官候选人，最终才选定了她。现在她在华尔街已经是一位传奇人物了。你当初是怎么选中她的？你选拔这类核心人才的标准是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：在我看来，宁让职位空着，也不能让不合适的人占着位置，所以我从来不会急于招人。就算 CEO 的位置暂时空缺，或者某个副总裁职位没人接任，公司的运转也不会停滞。只要你坚信这一点，坚信 “空位胜于错配”，你就有足够的时间去寻找那个真正合适的人。这个合适的人选，需要满足很多条件，其中很重要的一点，就是你得发自内心地欣赏他、认可他。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得Colette Kress入职第一周的时候，就问过我：“黄仁勋，你希望我在首席财务官这个岗位上干多久？” 我告诉她：“只要我们还活着，只要死亡不将我们分开，你就一直干下去。” 因为任何其他答案都是没有意义的。这份工作没有所谓的 “截止日期”，唯一的终点，就是当她觉得英伟达不再适合自己的时候。这个原则不仅适用于Colette，也适用于我那 60 位直接下属。我愿意为了等待合适的人，让职位空很久。而在这个过程中，公司依然会稳步向前。无论这个空缺的职位对应着什么使命、什么工作，大家都会主动顶上。退一步说，就算没人接手，我也会尽全力扛起这份责任，保证公司正常运转。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是我的用人哲学，永远不要让不合适的人占据岗位，耐心等待那个对的人出现。经常有人问我，什么样的员工才算优秀员工，什么样的管理者才算卓越管理者。说来奇怪，我其实没有标准答案。因为能走到我面前的人，都足够聪明、足够能干。你随便找一个首席财务官，我敢保证他绝对胜任本职工作。其他岗位的候选人也是如此。在我看来，英伟达之所以能创造奇迹，关键不在于单个人的能力有多强，而在于团队成员之间的 “化学反应”。更重要的是，这源于我们的企业品格。这种品格，才是一家伟大公司的核心竞争力。市面上有很多公司都在做芯片，虽然是英伟达发明了 GPU，但从产量来看，我们其实是全球最小的 GPU 制造商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这话听起来可能有点不可思议，但事实就是如此，很多不知名的厂商，GPU 产量都比我们高。很明显，英伟达的成功，绝不是靠产量取胜。我认为，真正的秘诀在于我们独特的企业文化和企业品格、团队在逆境中凝聚在一起的力量。在外人看来，我们似乎总是一帆风顺，但其实研发 Grace Blackwell 芯片的过程，差点拖垮了整个公司。但我们硬是咬牙扛了过来。这个项目的复杂度和规模都是前所未有的，外界对我们的期望也高得离谱。我们最终不仅达标，甚至超出了所有人的预期，而支撑我们做到这一点的，100% 是企业品格。这不是靠智商，也不是靠勤奋就能实现的，毕竟这个世界上，聪明又努力的人太多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种企业品格，是没法通过面试来筛选的。但我始终相信一件事：几乎任何人进入英伟达之后，都会被这种品格所感染、所塑造。这就是我们公司最神奇的地方：我们能够承受挫折，能够直面各种艰巨的挑战，并且一次次从困境中突围。很少有公司的团队能做到这一点。通常来说，当公司遭遇重大挑战后，总会有人因为心存不满离开，或者因为被当成 “背锅侠” 而被解雇。在团队合作中，出了问题总要有人承担责任，这是毋庸置疑的，就像一场球赛输了，我们必须清楚是谁失误丢了球。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在英伟达，我们打造了一个足够安全的环境。过去这些年，包括我在内，很多人都犯过严重的错误，这些失误大家都看在眼里，但从来没有人因为犯错而被解雇。久而久之，英伟达就形成了自己独有的文化和特质。这种文化的核心，就是包容、宽恕，以及从错误中学习。对我来说，有两件事至关重要：只要团队里的每一个人，都为了共同的目标拼尽了全力，这就足够了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;敢叫板 20 岁新锐的黄仁勋，也有至暗时刻？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：刚才聊到 “痛苦与磨砺” 的理念，你可以再深入谈谈吗？我最近听Andy Karp 在播客里说，“人生的二十几岁，要么用来享乐，要么用来打拼事业”。你认同这个观点吗？当然，不是每个人都能成为帕兰提尔或者英伟达的 CEO，但对年轻人来说，想要在事业上有所成就，到底需要付出什么？你想给年轻人传递怎样的职业与成功之道？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：Andy&amp;nbsp;很睿智，总能说出一些深刻的人生哲理。不过我对这类说法，倒没那么执念。我一直很佩服张忠谋先生，他一直工作到 80 多岁，思维依然敏锐得像一把刀。如果要在维基百科里查 “大器晚成” 这个词，配图说不定就是他。能在人生最具创造力的阶段，持续奋斗 50 年，这难道不是一件幸事吗？我自己也倾向于这种人生轨迹。对我而言，投身于有价值的事业，远比用后 20 年的时间环游世界更有意义，当然，环游世界本身也没什么不好，只是我现在就已经在满世界奔波了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不得不承认，二十几岁的我，确实更聪明、专注力更强、思维速度也更快。但那个年纪的人，往往缺少一样至关重要的东西，阅历沉淀出的智慧、处理复杂问题的分寸感、制定长远战略的眼光，以及长线思维的能力。这些能力，光靠读书是学不来的。现在的年轻人可以刷短视频，通过共情去感受别人的经历，算是一种间接的经验积累，这种模仿式学习确实有价值。但还有一样东西，是无法通过旁观习得的，那就是坚韧的意志，是直面痛苦与挫折时，懂得如何应对的底气；是熬过精神内耗、挺过煎熬时刻、战胜内心恐惧的勇气。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;经营公司的过程中，恐惧是真实存在的。我们的决策，关乎数万人的生计。当公司发展不顺时，一个感受不到恐惧、焦虑和脆弱的领导者，反而是不合格的。如果对结果毫不在意，那未免也太冷漠了。而这些真切的感受和应对的能力，只有亲身经历过，才能真正掌握。所以我觉得，两种人生选择没有绝对的对错。年轻时打拼，确实精力充沛，可以熬夜加班，可以付出十倍的努力，更容易早早取得成功。但我现在身上拥有的东西，是三十岁时的我完全不具备的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今的我，思维速度虽然慢了，但依靠智慧和经验积累的思维模型，能更快地找到正确答案。就算和二十岁的年轻人同台竞争，我也有信心不输给他。他们未必能胜过现在的我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：那我们来聊点更私人的话题吧。能不能说说你的童年？哪些高光或至暗的经历，对你如今的性格特质产生了直接影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我从来不觉得自己是天赋异禀的人，智商也算不上出众。小时候入学需要参加考试，我当时的成绩确实很不错，那会儿的考试还是全国性的。我记得母亲总是逢人就说，我是个非常聪明的孩子。不管这话是不是真的，她反复的肯定，无形中给了我一种压力，我必须变得足够聪明。这件事让我意识到，无论是为人父母还是做管理，给身边的人或者整个公司设定一个超出常理的高目标，往往能激发他们的潜能，让他们迎难而上。当然，也有人会被这样的目标吓退，但对我而言，这种激励起到了积极的作用。这是我第一个想到的童年片段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一件事，是关于我母亲的。当年我们学习英语的时候，她其实根本不懂英文，而且我觉得她可能连高中都没毕业。但这丝毫没有妨碍她每天教我们学英语。你可能会觉得不可思议，一个完全不懂英语的人，怎么教孩子学英语？她的方法很简单：买一本韦伯斯特词典，照着单词的拼写规律，写下英文单词，再标注上中文释义，把纸对折做成单词卡，然后逼着我们背下来。我们的发音准不准确，她其实也无从判断。但这件事，让我学到了一个道理：一个人只要有足够坚定的意志，就算暂时不知道该怎么做，也不该停下脚步。很多事情，其实并没有想象中那么难。小时候的这段记忆，我一直记到现在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一段经历，是我们搬到肯塔基州之后。我当时是学校里年纪最小的孩子，就读的奥尼塔浸会学院坐落在山顶。每天上学，我都得走下山坡，穿过一条河，再走过一片广阔的田野，才能到达那所小小的学校。那是 1973 年，我是整个镇上第一个出现的中国孩子。镇上的那些孩子都很野，每次我过吊桥的时候，他们都会找我的麻烦。那座吊桥的桥面是木板铺的，有些木板已经缺失了，桥下的河水很深。而那些孩子，就守在桥的另一头等我。那时候我才 9 岁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：天哪，才 9 岁。眼前是一条河，一座破吊桥，桥对面还有等着找麻烦的孩子，这简直太糟糕了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是啊，但我每天都得走这条路去上学。这大概就是童年时期的 “痛苦与磨砺” 吧。每天早上都是这样。下午放学回家后，我还有任务：打扫卫生间。那时候家里的每个孩子都有分工，我哥哥当时 11 岁，他的活儿是去烟草农场干活，而我的工作就是打扫卫生间，每天都要做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：你觉得当年那些找你麻烦的孩子，知道你现在的成就吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：奥尼塔浸会学院的校长最近还发邮件给我呢。他们每年都会给我寄圣诞礼物，知道我喜欢吃肯塔基风味的香肠肉汁配饼干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：这个爱好是在肯塔基养成的吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：没错。我记得我 45 岁生日的时候，家人带我回了一趟母校。当年食堂里做饭的阿姨们居然还健在，特意回来给我做了一顿饭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：天哪，这也太暖心了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：真的特别感动。她们给我做了正宗的肯塔基香肠肉汁配饼干，味道还是小时候的样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：你的父母见证了你的成功吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：当然，他们现在身体还很好，特别为我骄傲。他们对我的事情了如指掌，我父亲会读所有和我相关的报道。要是看到有人说我的坏话，他还会生气。我总劝他别什么都看，不然天天都得生气，别理会那些负面新闻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：挺有意思的。现在功成名就了，你会怀念那些还没这么受关注的日子吗？会想念那些平凡的小事吗？比如你很爱车，现在却没什么机会开车了吧？我记得你是我认识的人里，第一个也是唯一一个拥有柯尼赛格跑车的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：克里斯蒂安・冯・柯尼赛格真是个天才设计师，那辆车太棒了。启动的时候，引擎声和蝙蝠侠的座驾一模一样。而且启动它得按七个步骤，因为动力实在太强劲了，不能随便让别人碰。不过我现在已经没有那辆车了，也确实很少开车了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：会想念开车的感觉吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：有一点吧。我现在还是会关注新车，比如新款的法拉利，每次看到都觉得很惊艳，这些车真的是工程学的杰作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：确实很厉害。我去过法拉利的工厂，亲眼看到一辆车从工业器械一步步变成顶级消费品，现在甚至成了艺术品，这个过程太震撼了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;黄仁勋眼中五年后的世界&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：如果五年后我们再坐在这里，你觉得到那时的世界会是什么样子？哪些变化会让我们最惊讶？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：如果我们回归底层逻辑，再结合现实的实用性和技术落地的规律来判断，有几件事是可以预见的。首先，英伟达和整个行业在AI领域的投入，必将彻底改变计算机的运作模式 ，未来的计算机，将从 “由人类编程” 进化为 “在人类引导下自主学习编程”。过去我们是手把手教计算机学日语，未来我们只需要告诉它 “去学日语” 就够了。未来的计算机，将能够处理比现在大十亿倍的问题规模。这个变化的影响之大，我们现在甚至无法完全想象，因为提出解决方案是一回事，而能否构想出需要解决的问题，就是另一回事了。很多问题之所以无法被解决，往往是因为我们连如何定义和描述它们都做不到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来，无论是数字生物学、物理科学、量子物理还是材料科学的复杂难题，都会变得容易攻克。就算是交通拥堵这种日常问题，也能得到极大改善。就拿智能电网来说，现在的电网存在大量能源浪费，AI会精准计算出所需的能源量，实现按需分配，从根本上避免过度供应造成的损耗。AI解决这些日常难题的能力，会让人惊叹不已。到那时，每一个科学领域都会被重塑，当下所有的难题都会被技术赋能、迎刃而解。工具的速度提升了，难题自然就显得 “渺小” 了。举个例子，如果飞机的速度能达到 10 马赫，整个世界就会变得 “小” 很多， 喷气式飞机的出现，其实已经让世界变小了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达制造的计算机也是如此，极致的运算速度让所有问题都变得更容易被解决。就像 OpenAI 的研究人员曾经说的：“为什么不把整个互联网的数据都喂给计算机呢？” 因为在算力爆发之后，全球互联网的数据量，突然就显得微不足道了。现在我们看互联网数据，也会觉得体量很小，原因就在这里。这种心态，未来会渗透到几乎所有的科学领域。过去人们会说 “这是个无解的难题”，未来大家只会觉得 “这事儿很简单”。五年后，每一位科学家、工程师、企业家和创新者，都会抱着这样的心态。曾经的难题变得简单，我们就能解决更多的问题。这是第一个必然结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个结果，就是企业的生产效率会实现质的飞跃。今天的难题变成明天的小事，供应链管理会变得无比顺畅，浪费现象基本消失；计算机的设计流程也会简化，我们可以尝试更多的方案。这并不是说我们会每年推出更多的产品，我们还是保持一年一款的节奏，但每一款产品都会经过更多次的迭代优化，最终呈现的成品会比现在好得多。这样一来，公司的效率会更高，利润会更丰厚，所有企业都会变得更赚钱，整个社会的财富也会随之增长。但还有一个值得深思的点：当所有我们能想到的问题都变得可以解决时，我们就会去探索更多新的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，未来的趋势不会是就业岗位减少，反而是大家会变得比现在更忙碌。因为以前那些被认为 “不可能完成” 的任务，现在都摆上了台面；那些因为成本太高而无法开展的实验，现在都可以去尝试，AI还会帮我们推进这些实验。只要我们有足够的想象力，所有搁置的难题，都会找到解决的路径。我可以做一个思想实验。现在我工作时，身边围绕着 60 位顶尖人才，而他们每个人又带着数千名精英。这些人在各自的领域里，能力都远超于我，对我来说，他们就像是 “领域内的人工超级智能”。但和他们合作，我完全没有障碍。现在我使用的 OpenAI、Gemini、Grok、Perplexity、Anthropic 这些AI工具，在很多方面也已经比我聪明了，但我每天都在和它们高效协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过有一个很有意思的变化：以前我给团队布置一个问题，需要等两三天才能得到反馈和答案，这段时间里我可以思考下一步的计划 ，因为我的决策需要基于这些中间结果。但如果这些答案能在一秒钟内就反馈给我，会发生什么？我的工作节奏会变得无比紧凑，因为我会成为所有事情的关键节点。刚得到一个答案，立刻就要推导下一步，马上启动新的实验。你不觉得吗？现在信息技术的提速，已经让我们变得更忙碌了。信息、知识和答案的获取速度越来越快，我们作为决策节点，自然会比以往更忙。我觉得未来很多人都会有这种感受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后一点，对于那些没能赶上之前科技浪潮的人来说，AI会填平技术鸿沟。我特别喜欢 “氛围编程” 这个概念，现在任何人都可以成为软件程序员，借助 AI 写出的代码，甚至比很多专业程序员的作品还要好。我很欣赏 Cursor 这家公司的成果，前几天还见到了 Lovable 的 CEO，他是个很厉害的人，他们的公司在瑞典。AI会帮助那些在自己的领域很有天赋，但不懂如何用技术放大自身能力的人实现能力的跃迁。Lovable 的 CEO 就跟我说过，很多人用他们开发的软件创办了小公司，现在每年能赚 2300 万美元。这太不可思议了。这些人终于能融入全球经济体系，不再被技术门槛挡住去路，这一切都是AI的功劳。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;五年后的世界，大家会拥有更有价值的工作，经济效率会大幅提升，GDP 有望实现增长，劳动力短缺的问题会得到缓解，通货膨胀也会回落。更多的科学领域会被开拓，更多的难题会被解决。当然，也有一些悲观的论调，认为AI会让一半的人失去工作。但我觉得，更可能发生的情况是：100% 的工作岗位都会发生变化，但不会有 50% 的岗位消失。而且，那些现在没有工作的人，很可能会因为AI获得谋生的手段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我们的技术会发生翻天覆地的变化，但这些技术层面的革新，反而不是最有意思的。五年后，计算机还是计算机，只是应用变得更智能了，本质上还是软件。我们依然会做电商，只是可能不用自己逛网站了，会有智能代理帮我们购物，但商品还是来自亚马逊这些平台。很多事情，其实都会保持原样。最后我还有一个小小的愿望或者说期待：希望我们在机器人和人形机器人领域的研究能结出硕果，希望未来每个人都能拥有属于自己的 R2-D2 和 C-3PO，它们可爱又贴心。就像在 GTC 大会上，我每次都会邀请迪士尼的机器人上台，那些机器人真的太萌了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么不让每个人都拥有一个呢？我还希望迪士尼能把这些机器人做成周边商品，它们真的值得。我的宠物猫莫莫和库玛，也需要这样的 “宠物玩伴” 不是吗？我真心希望这个愿望能实现。现在有很多孤独的人，已经有不少人联系过我，希望能拥有可以在家陪伴自己的机器人，尤其是那些独居的老人。机器人能给他们带来陪伴和帮助，而且它们本身又那么可爱，这绝对是我们技术发展带来的意外之喜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：如果以后有机器人帮我们做饭、打扫卫生，你还会像现在这样，饶有兴致地看着别人做饭吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：当然会。原因很简单，我现在完全有能力不用自己做饭，但我还是会选择下厨。我们完全可以雇很多佣人，但我们没有这么做。我和洛里一直都是两个人自己过日子。昨晚她做了墨西哥辣椒肉酱，味道棒极了，全程都是她一个人忙活的。以后我们大概率还会保持这样的生活。对我来说，最幸福的时刻，就是孩子们回家来，我们一起下厨做饭，喝喝小酒，这就是最完美的时光。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：一家人在厨房里忙活，这种亲密感真的太美好了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是啊，人生的幸福莫过于此。我们打拼奋斗，不就是为了这样的时刻吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“不爱演讲的黄仁勋”：CEO是公司里最脆弱的一群人&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：当一切尘埃落定，你希望后人如何记住你？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先，能被人记住，本身就是一件很幸运的事。我很庆幸，凭借英伟达的成就，凭借我们打造的事业，凭借我们在全球最重要的科技产业：人类最核心的工具 “计算机” 领域留下的印记，英伟达很可能会在我离开这个世界很久之后，依然对这个世界有着重要的意义。我很庆幸自己能和克里斯、柯蒂斯一起创立这家公司，很庆幸自己能一路学习成长，没有成为拖垮公司的那个短板，反而常常是推动公司走下去的一份力量。我们打造的这家企业，对整个世界都有着深远的影响，而不只是局限于某个行业或某个群体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;能做到这一点的人，在这个世界上并不多。我很庆幸自己作为创始人，能亲身参与并见证这一切，见证英伟达成长为如今的模样，见证它对全球各行各业产生实实在在的影响。公司里有很多已经工作了 33 年的老员工，他们的人生因为英伟达变得更加丰盈；现在甚至已经有第二代、第三代员工加入我们。我们在全球各地建立了自己的团队，我很荣幸能和这些员工并肩作战，分享他们一路走来的绝望与喜悦、希望与悲伤。这样的经历，并不是每个人都能拥有的。我为我们在中国的团队感到骄傲，为我们在印度的员工们由衷赞叹，也为欧洲、加拿大的团队感到欣慰。我们在加拿大的团队正在不断壮大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还希望有朝一日，英伟达能把业务拓展到南半球，让更多地区的人们，也能享受到我们今天所拥有的技术成果。昨天我还和人聊起我们在非洲开展的工作，聊到我们应该在拉美和东南亚投入更多精力。我真的为我们公司带来的这些影响感到自豪。所以，人们会怎么记住我？或许，他们会记得我是英伟达的创始人之一，是这家公司的缔造者之一。或许，还会记得我是个好人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：这是毋庸置疑的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：他们或许还会觉得，我是个风趣幽默的人，不喜欢端着架子。其实在很多方面，我都算是一个 “不情愿的 CEO”。比起待在公司外面抛头露面，我更喜欢扎根在公司内部；比起发表演讲，我更喜欢安静做事；我甚至一点都不喜欢做主题演讲，但为了公司，我必须去做这些事。我确实是个不太情愿的 CEO，但我绝对是个满腔热忱的英伟达建设者。只要是为了公司发展必须做的事，我都会全力以赴。说了这么多，其实我也不知道，人们最终会如何记住我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：我觉得，看到好人获得成功，总是一件令人开心的事。这么多年来，看着你一路打拼，经历起起落落，最终收获成功，我真的由衷地为你高兴。你这一路走来，见过了形形色色的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是啊，真的见过了太多人。我想提醒所有的 CEO，没有人能单枪匹马地成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：确实如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我们虽然是 CEO，但这个位置总需要有人来坐。如果不是早年大家对我的提携与帮助，比如你一直不遗余力地宣传英伟达还有张忠谋奖带来的认可，这些都对我意义重大。张忠谋奖大概是我人生中获得的第一个真正有分量的奖项，直到今天，它对我来说依然意义非凡。这个奖项以他的名字命名，而且他还亲自参与了评选，这份认可真的让我铭记于心。还有那些和我们合作的企业，他们的慷慨相助，我也一直记在心里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实 CEO 这个角色，很多时候都需要寻求帮助。我已经记不清有多少次，我是这样开启一段对话的：“我需要你的帮助。” 很多时候，我是真的需要帮助，而且对方往往是唯一能帮到我的人。一路走来，很多人都慷慨地伸出援手，分享他们的知识，教我做事的方法，帮我解决棘手的难题。这或许才是 CEO 这个角色带给我的真正启示，这个职位远比人们想象的要脆弱得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：而且还是一个很孤独的职位，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：确实可能会感到孤独。但我想说，这种孤独更多是存在于我们的内心世界。当你试图解决一些棘手的难题时，往往需要长时间独自思考，自己跟自己对话。公司发展的每一次转型、每一次跨越，每一次我推动公司自我革新的时刻，我都不知道自己独自思考了多少个小时。在那些时刻，你会真切地感受到孤独。但我们也要明白，其实有很多人都希望我们能成功。就像你之前说的，你很乐意看到我成功，我知道你是真心希望我好，而我也同样希望你能越来越好。从这个角度来说，我们其实并不孤单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以说，CEO 这个职业，是一份充满脆弱感的工作。你无法单打独斗完成任何事，很多时候都需要依赖别人的帮助与善意。或许在外界看来，我们是强大的领导者，但实际上，我们可能是公司里最脆弱的一群人。我经常说，我是公司里唯一一个离开别人的帮助就寸步难行的人。我想，大多数 CEO 应该都是如此。这或许就是这份职业带给我们的感悟：CEO 们，远比他们愿意承认的要更加脆弱。不过对我来说，承认这种脆弱，并不是什么难事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“没有终极目标” ，才成就了英伟达？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jodi Shelton：接下来我们用快问快答收尾。你见过的最聪明的人是谁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：这个问题我没法回答。我知道大家心里对 “聪明” 的定义，就是智商高、会解决问题、技术能力强。但在我看来，这种能力早已经成了一种 “通用品”。而且我们很快就能证明，AI处理这类问题是最轻松的，不是吗？举个例子，以前大家都觉得软件编程是最考验智商的工作，结果呢？AI最先攻克的领域之一就是编程。所以说，“聪明” 的定义，其实和大多数人想的完全不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，从长远来讲，真正的 “聪明”，是那种兼具技术洞察力与人文同理心的能力，是能够洞察弦外之音、预判未知风险、看透表象背后本质的能力。那些能 “见人所未见” 的人，才是真正的聪明人，他们的价值是无可估量的。这种人能凭借数据、分析、底层逻辑、人生阅历、智慧经验，再加上对他人的感知，敏锐地捕捉到潜在的风险，在问题发生之前就提前规避。我觉得这才是 “聪明”，而且拥有这种能力的人，说不定在学术能力评估测试（SAT）里的分数惨不忍睹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：外界对你有什么误解？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：这些问题都好犀利啊。首先，我都不知道外界对我有什么印象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：比如，大家觉得你喜欢抛头露面，觉得你是个很棒的演讲者，所以肯定很享受做演讲的过程。但你之前已经说了，事实并非如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：对，完全相反。公开演讲简直让我怕得要死。不是说站在台上的那一刻害怕，而是现在，想到两周后在华盛顿举办的 GTC 大会，我就焦虑得不行。不，应该说，我已经焦虑一个月了。这种事总是让我心神不宁，脑子里时时刻刻都想着，压力特别大。公司内部的会议演讲也让我紧张到极致。因为台下坐的都是对我而言最重要的人，从某种程度上说，这是我做过的最重要的演讲。但这种演讲根本没法准备，我要讲的所有内容，其实都能在网上的某个视频里找到，他们完全可以自己去看。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我很讨厌把那些内容重复一遍讲给他们听，因为你绝不会回家对着家人做一场 GTC 主题演讲，对吧？我也不想那样做。演讲内容必须是真诚的、独一无二的、对听众有价值的、有意义的，能给他们带来改变。毕竟我还在领导这家公司，我希望通过演讲达成一定的目标。所以我必须拿出全新的内容，但不到演讲结束的那一刻，我永远不知道最终效果会怎么样。大家都觉得财报发布周我会很紧张，但说实话，我一点感觉都没有。真正让我紧张的，是公司的内部会议演讲。所以外界的这个印象，真的大错特错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：你最受不了的事是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：在关键时刻，有人不认真听我说话、不理解我的问题，还胡乱回答。尤其是在我们处理非常棘手、非常困难的问题时，我们需要的是事实，是真相。这个时候我提出问题，如果有人答非所问，我会立刻火冒三丈。我实在无法理解，为什么有人意识不到这场会议的重要性？我们正在为一件至关重要的事努力，我们需要尽快找到真相、解决问题。我到现在都想不通这一点，这种情况每次都会激怒我。谁要是想惹我生气，这招百试百灵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：这下我们知道怎么让黄仁勋发火了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：最后一个问题，是最近有人问我的，我特别喜欢这个问题。如果让你回到 20 岁，你是想回到自己当年的那个年代，还是活在当下的 20 岁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我会毫不犹豫地回到自己的那个年代。因为我觉得，我们那一代人的 20 岁，比现在年轻人的 20 岁更快乐。我总觉得，每个人都应该拥有一段 “懵懂无知” 的时光，不必从第一天起就背负着全世界的重担。我坚信这一点，没人能说服我。有时候，“无知” 也是一种快乐，甚至是一种超能力。如果当初我知道创立英伟达是一件 “不可能完成的任务”，那今天的英伟达根本就不会存在。事实就是，创立英伟达这件事，本来就是天方夜谭。但当时的我什么都不懂，所以没人能说服我放弃。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，乐观的人都这样，你永远没法说服他们 “这件事做不成”。他们就是这么 “无知”，对现实的艰难视而不见，所以才会充满乐观。这难道是坏事吗？现在的年轻人，过早地接触到了太多信息，变得越来越愤世嫉俗。他们并不是天生就这么消极，而是因为看到的东西太多太杂了。其实大可不必如此。人需要培养内心的乐观精神，需要在心里留存一份善意，学会只看到世界美好的一面。我们得锻炼这种能力。我们那一代人，有更多这样的机会。我们 20 岁的时候，就是这样的，乐观得像超人一样，觉得凡事皆有可能。所以，我肯定会选择回到自己的 20 岁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jodi Shelton：真是个完美的收尾。无知是福啊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：没错，无知是福，无知也是一种超能力。任何一个想要开启新征程的人，如果不是因为这份 “无知”，他们一早就会因为觉得事情太难而放弃了。我真的很庆幸，自己当年虽然也算勤奋、也算有一些能力，但那份 “无知” 帮了我大忙。我那时候做任何事都抱着一种心态：“这能有多难？” 结果后来才发现，简直难到超乎想象。你根本没法想象。你看看我今天建立的这一切，如果当初我就知道前路会有这么多艰辛、这么多挫折、这么多失望，把这些困难全都摆在我面前，我绝对不会去做的，绝对不会。所以说，“无知” 真的是一种超能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一种超能力，就是“没有终极目标”。英伟达就没有什么终极目标。总有人问我：“黄仁勋，你的计划是什么？” 我们没有计划，“活下去” 就是我们的计划。我们对未来的世界有憧憬，我们会畅想技术会如何改变世界，但我们 100% 的计划，就是让公司一直运营下去。以前也有人问我，现在也经常有人问：“黄仁勋，你的人生目标是什么？” 我没有什么人生目标，就是想一直工作，一直有事可做，能和一群优秀的人一起做有意义的事。这就是我的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以说，从很多方面来讲，“没有终极目标” 这一点，对英伟达的发展真的起到了至关重要的作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8FOdAc_i_tM&quot;&gt;https://www.youtube.com/watch?v=8FOdAc_i_tM&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/BexlqSeAfD1svmVOXpc1</link><guid isPermaLink="false">https://www.infoq.cn/article/BexlqSeAfD1svmVOXpc1</guid><pubDate>Mon, 19 Jan 2026 10:26:06 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>二十年，重新出发！第 20 届 D2 技术大会「AI 新」议题全球征集正式开启</title><description>&lt;p&gt;二十年，是一个坐标。从 Web 2.0 的萌芽，到移动互联网的爆发，再到云原生时代的重塑，D2 技术大会伴随开发者走过了整整二十载风雨。&lt;/p&gt;&lt;p&gt;今天，我们站在了一个更加宏大的分水岭。AI 不再是遥远的科幻逻辑，它正以一种近乎“重构”的姿态，系统性地改写终端技术的底层范式：从代码生成的协作，到架构设计的逻辑，再到交互体验的边界。&lt;/p&gt;&lt;p&gt;第 20 届 D2 技术大会，年度主题定为——「AI 新」。&lt;/p&gt;&lt;p&gt;它既是我们的时代判断，也是我们的集体宣言。它是 AI 驱动的创新，也是终端人对技术边界追逐的热爱之新。&lt;/p&gt;&lt;p&gt;此刻，我们正式向全球开发者、架构师、技术领袖及创新实践者发出邀请：来 D2，分享你对 AI 时代终端技术的独到见解，共同定义下一个二十年的生产力！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;七大核心专场，期待你的真知灼见&lt;/p&gt;&lt;p&gt;我们渴望真实工程中的突破，珍视深度思考后的落地，让技术回归解决问题的初衷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;01 AI Coding：从写代码开始，重构工程本身&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是本届 D2 的主干专场。AI 正在从“辅助助手”升级为“协作伙伴”。&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;AI Agent 编程工具的研发与设计&lt;/p&gt;&lt;p&gt;侧重 Agent 型 AI 编程工具在本地与远程形态下的架构与产品设计。征集议题包括 IDE 深度集成、上下文采集与记忆管理、代码库索引检索、任务规划与工具调用、执行沙箱与权限控制、审计与回放、可观测性、成本/延迟优化与多模型策略等。重点关注可靠性与可控性：减少误改、支持规范化交付与团队协作。&lt;/p&gt;&lt;p&gt;AI-Native 开发实践&lt;/p&gt;&lt;p&gt;聚焦真实项目中 AI 编程的可复用方法。征集包含 Spec 驱动开发（结构化需求/验收标准/契约/测试）、AI 编程 Workflow 探索（从需求到 PR/发布的流水线）、以及团队级 AI 驱动研发实践（流程改造、提示/模板沉淀、质量门禁、效率与质量度量、失败复盘）。重点是“怎么做得稳、做得快”。&lt;/p&gt;&lt;p&gt;AI Coding 前沿研究与技术趋势&lt;/p&gt;&lt;p&gt;关注下一代 AI Coding 的关键技术与趋势。征集议题包括长上下文与复杂依赖、代码语义理解与程序分析结合、自动化评测与基准、对齐与安全、多智能体协作、可靠性与可解释性增强等。重点探讨研究如何走向工程落地与可验证的效果提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;02 AI 创新体验：当交互正在被重写&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;终端是 AI 被感知的最前线。交互范式的巨变已经发生。&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;UI 范式重塑&lt;/p&gt;&lt;p&gt;探讨从 GUI 向 LUI 或 AUI 的代际演进。聚焦 Agent 驱动下的意图识别、动态 UI 生成及个性化界面即时构建。征集议题包括主动交互设计、多 Agent 协作下的用户反馈回路、以及如何利用 AI 简化复杂业务流的操作门槛。&lt;/p&gt;&lt;p&gt;空间智能体验&lt;/p&gt;&lt;p&gt;聚焦多模态感知与空间计算的深度融合。涵盖视觉、语音、触觉在 3D/XR 环境下的集成交互，以及 AI 驱动的实时场景理解与数据可视化。重点探讨如何利用空间智能让数字世界更符合自然认知，实现高沉浸感的智能反馈。。&lt;/p&gt;&lt;p&gt;具身交互探索&lt;/p&gt;&lt;p&gt;关注 AI 进入物理世界后的交互挑战，从 AI Wearables、AI PC 到机器人具身智能。探讨硬件约束下的自然语言处理、人机交互（HRI）实践及环境感知反馈。重点关注如何通过端侧智能赋予硬件产品生命力，解决真实场景下的交互痛点，探索用户真正愿意买单的终端新价值点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;03 AI 语言 &amp;amp; 框架：模型时代，语言与框架如何进化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当 AI 成为“默认能力”，底层技术如何适配？&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;语言与编译器演进&lt;/p&gt;&lt;p&gt;探讨编程语言如何适配“人机共写”新常态。征集议题涵盖 LLM 友好型语法设计、智能化类型系统、AI 辅助的编译优化与静态分析等。重点研究如何通过语言特性的进化，提升 AI 生成代码的质量、安全性与复杂逻辑表达力。&lt;/p&gt;&lt;p&gt;Agent 框架重构&lt;/p&gt;&lt;p&gt;当 Agent 成为系统编排者，探讨传统框架的抽象层重塑。征集议题涵盖声明式意图驱动的框架设计、元数据驱动的界面自动生成、以及为 AI 重新设计的组件模型。重点关注框架如何提供更高级别的抽象，以支持多 Agent 在复杂业务逻辑中的无缝协作、状态同步与逻辑自治。&lt;/p&gt;&lt;p&gt;智能运行时与内核&lt;/p&gt;&lt;p&gt;推动 AI 从工具层下沉为系统的核心能力。聚焦内置 AI 推理能力的运行时引擎、模型与容器/内核的深度集成，以及 AI 驱动的动态资源调度策略。重点探讨端云协同背景下，如何模糊开发与运行、模型与逻辑的边界，实现具备自适应、自进化能力的智能运行基座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;04 AI 智能测试：质量与效率，不再只能二选一&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;测试不再是滞后的环节，而是 AI 介入最深、收益最显性的战场。&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;用例生成与自愈&lt;/p&gt;&lt;p&gt;探讨利用 LLM 实现测试全生命周期的自动化。征集议题包括基于语义理解的单元/集成测试生成、复杂业务场景下的测试数据合成，以及 UI 自动化脚本的自愈（Self-healing）机制。&lt;/p&gt;&lt;p&gt;风险洞察与优化&lt;/p&gt;&lt;p&gt;聚焦利用 AI 提升质量保障的精准度与效率。征集议题涵盖基于变更分析的智能回归测试缩减、线上异常的实时检测与根因定位，以及多维度的质量风险预测模型。探讨如何利用算法在海量代码变更中快速锁定高风险区域，解决快速迭代与质量稳定性之间的核心矛盾。&lt;/p&gt;&lt;p&gt;治理与角色演进&lt;/p&gt;&lt;p&gt;关注 AI 引入后测试流程与组织效能的系统性重构。核心议题包括 AI 测试工具的 ROI 分析、人机协同模式下的 QA 职责重定义，以及在规模化工程中构建“默认内置 AI”的质量防线。探讨如何通过技术赋能，打破质量与效率的零和博弈，重塑技术团队的质量文化与评价体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;05 AI 智能生产：从工具走向生产系统&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注 AI 在真实业务落地时的“最后一公里”。&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;业务深度嵌入&lt;/p&gt;&lt;p&gt;探讨 AI 如何从外部辅助工具进化为业务逻辑的核心。寻找在复杂业务场景中的落地架构案例，关注如何处理模型输出的不确定性以交付“确定性”结果。重点探讨 AI 对传统业务流程的深度重构，在提升用户价值的同时，确保生产系统的稳定性、安全性与商业收益。&lt;/p&gt;&lt;p&gt;规模化生产交付&lt;/p&gt;&lt;p&gt;聚焦 AI 从原型验证（PoC）走向规模化交付的工程拐点。征集议题涵盖支持大规模 AI 应用的工程底座、端到端 AI 生产平台的演进、以及 FinOps 成本分析与合规治理。探讨如何构建标准化的平台能力，支撑 AI 跨团队、跨业务的高效迁移与持续稳定运行，实现技术普惠。&lt;/p&gt;&lt;p&gt;全链路协同提效&lt;/p&gt;&lt;p&gt;关注覆盖需求、设计、交付及运维的 AI 全链路闭环。核心议题包括新一代人机协作下的流程重塑、领域专用 Agent 的生产环境编排，以及科学的效能度量方法。探讨如何通过技术与组织的双重演进，实现软件生产体系的跨越式提效，将 AI 潜能真正转化为规模化的实际业务产能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;06 终端技术：重构 AI 时代的性能底座&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;底层基础设施如何承载高算力与高响应需求？&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;架构适配与演进&lt;/p&gt;&lt;p&gt;探讨终端架构如何重构以深度兼容 AI 能力，重点研究如何调整传统的软件拓扑结构，以支持 AI 在终端侧的无缝集成、高效编排与复杂的应用状态管理，提升端侧智能的响应实时性。&lt;/p&gt;&lt;p&gt;运行时与性能优化&lt;/p&gt;&lt;p&gt;聚焦通过底层技术突破 AI 运行的性能瓶颈。征集议题涵盖面向 AI 指令集优化的编译器技术、异构算力的极致加速实践，以及轻量化端侧容器演进。探讨如何通过运行时与系统内核的深度协同，在有限的硬件资源限制下，实现极致的推理速度与能效比。&lt;/p&gt;&lt;p&gt;端侧工程与协同&lt;/p&gt;&lt;p&gt;核心议题包括模型量化、蒸馏与剪枝的终端实战、端云协同推理架构，以及隐私安全约束下的端侧学习。探讨如何构建高效的端云配比方案，在保障响应速度与数据隐私的同时，实现计算成本与用户体验的帕累托最优。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;07 一人公司：技术人的个体放大器&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是最具时代情绪的专场。AI 正在让“超级个体”成为可能。&lt;/p&gt;&lt;p&gt;征集方向：&lt;/p&gt;&lt;p&gt;全栈生产力飞跃&lt;/p&gt;&lt;p&gt;探讨 AI 如何打破专业壁垒，实现“一个人就是一支团队”。分享利用 AI 协同完成从需求定义、全栈开发、交互设计到市场增长的全链路实践。&lt;/p&gt;&lt;p&gt;商业闭环与实战&lt;/p&gt;&lt;p&gt;聚焦超级个体的商业化落地与可持续经营之道。征集独立开发者的 AI 实战案例，涵盖极致成本控制下的产品生存策略、AI 辅助的商业决策与自动化运营。探讨在 AI 时代，个体如何构建轻量化、高利润的商业模式，并成功应对从单兵作战到规模化营收的真实挑战。&lt;/p&gt;&lt;p&gt;职业路径重构&lt;/p&gt;&lt;p&gt;探讨从“专项开发者”向“产品主理人”转型的思维重构、AI 时代的个人品牌经营，以及个体长期竞争力的构建。研究在组织边界日益模糊的未来，技术人如何利用 AI 工具集寻找更具自主性的创作路径，定义下一代极简且高效的职业范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;顶尖出品人矩阵：为议题深度护航&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa61443638c019387c39c770e16936f2.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;本届 D2 各专场由行业资深专家领衔，他们不仅是评审者，更是议题的“合伙人”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们寻找的不仅是一个演讲者&lt;/p&gt;&lt;p&gt;更是一个在 AI 工程深水区挣扎过、思考过、最终破局的见证者&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;隐风| 淘天集团-用户&amp;amp;内容终端技术负责人云谦| 蚂蚁集团-高级前端技术专家悟石| 淘宝闪购-消费者端技术负责人渚薰| 前淘宝互动游戏专家偏右| 蚂蚁集团-支付宝体验技术前端平台负责人张磊| 字节跳动 Web Infra 技术负责人泠乐| 淘天集团-淘宝终端质量负责人茹炳晟| CCF TF 研发效能 SIG 主席 / 复旦大学 CodeWisdom 成员达峰| 蚂蚁集团-平台体验技术部负责人穆宸| AliExpress-终端技术负责人 / D2 负责人永霸| 淘天集团-交易终端技术负责人崔红保| DCloud CTO / uni-app 跨平台框架负责人秦粤| 阿里云-数据库高级前端专家梓骞&amp;nbsp;| 启智云图 CEO / Lovrabet 产品创始人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;出品人寄语：“在 D2，我们致力于将前沿的 AI 实践提炼为系统化的技术范式。我们期待与你一同锚定 AI 时代的工程坐标，让每一份实战洞察都汇聚成定义未来的行业基准。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🌟&amp;nbsp;为什么来到 D2 舞台&lt;/p&gt;&lt;p&gt;顶尖技术影响力：D2 是国内终端技术的风向标，线下规模 2000+，线上覆盖数十万专业开发者。二十周年里程碑：参与第 20 届这一极具纪念意义的盛会，与业内最具创新精神的技术人同频共振。常态化社区联动：优质内容将同步至稀土掘金、InfoQ、AI 产品榜等联合承办方平台，获得持续的行业曝光与认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🗓️&amp;nbsp;议题提交指南&lt;/p&gt;&lt;p&gt;截止时间：&amp;nbsp;2026年1月23日（请关注官网最新动态）议题要求：内容具有前瞻性、实战性或深度思考；拒绝纯广告，强调技术细节与真实的踩坑经验&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e113f9df884fa02d84448c5d0828beaf.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;扫码提交议题&lt;/p&gt;&lt;p&gt;二十年是一个里程碑，更是重新出发的起点。在「AI 新」的浪潮中，让我们一起，用 AI 驱动创新，用终端之心热爱创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;*本文由极客时间企业版代发&lt;/p&gt;</description><link>https://www.infoq.cn/article/RJ1OxNTaCwuh6Y12O8dc</link><guid isPermaLink="false">https://www.infoq.cn/article/RJ1OxNTaCwuh6Y12O8dc</guid><pubDate>Mon, 19 Jan 2026 08:12:09 GMT</pubDate><author>极客时间企业版</author><category>AI&amp;大模型</category></item><item><title>Cursor推出动态上下文发现功能，提升了Token的使用效率</title><description>&lt;p&gt;Cursor推出了一种新方法，用于减少发送给大语言模型（LLM）的请求上下文的大小。这种方法名为&lt;a href=&quot;https://cursor.com/blog/dynamic-context-discovery&quot;&gt;动态上下文发现（Dynamic Context Discovery）&lt;/a&gt;&quot;，它摒弃了以往在请求开始时就包含大量静态上下文的做法，转而让智能体（agent）按需动态检索所需信息。这种方式不仅显著减少了token消耗，也避免了将可能令人困惑或无关的细节混入上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了实现动态上下文发现，Cursor采用了五种不同的技术。这些技术有一个共同特点，即以文件作为LLM工具的主要接口，使内容能够由智能体动态存储和获取，而不是一次性塞满有限的上下文窗口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;随着编码智能体能力的快速提升，文件已成为一种简单而强大的基础原语（primitive）。相比引入另一种尚无法完全适应未来需求的抽象层，使用文件是一种更安全、更务实的选择。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor使用的第一项技术是将大规模输出（比如，shell命令或其他工具的输出）写入文件，确保关键信息不会因上下文截断而丢失。随后，智能体可根据需要使用tail等命令读取文件末尾的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，针对上下文过长时被摘要压缩而导致信息丢失的问题，Cursor会将完整的交互历史保存到文件中，使智能体能在后续需要时检索缺失的细节。同样，&lt;a href=&quot;https://cursor.com/docs/context/skills#agent-skills&quot;&gt;领域特定的能力&lt;/a&gt;&quot;被存放在文件中，智能体可通过&lt;a href=&quot;https://cursor.com/blog/semsearch&quot;&gt;Cursor内置的语义搜索工具&lt;/a&gt;&quot;动态发现相关文件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于MCP 工具（Model Context Protocol 工具），传统做法是在请求初始阶段就加载所有MCP服务器提供的工具描述，而Cursor修改为仅传递工具名称。当任务实际需要某个工具时，智能体才会动态拉取其完整定义。这一策略大幅降低了token总量：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;智能体现在只接收少量的静态上下文（包括工具名称列表），并在任务需要时主动查询具体工具。在一项A/B测试中，对于调用了MCP工具的运行实例，该策略平均减少了46.9%的总token使用量（结果具有统计显著性，但方差较大，这取决于所安装MCP服务器的数量）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ec1873f6d16ffc8ddc77bd79169d9beb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，这种方法还带来一个额外的优势，那就是智能体可以监控每个MCP工具的状态。例如，比如某个MCP服务器需要重新认证，智能体可以及时通知用户，而不是完全忽略该问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，所有终端会话的输出会同步到文件系统。这使得智能体能更轻松地回答用户关于命令失败原因的问题。同时，通过将输出存入文件，智能体可使用grep等工具仅提取相关的信息，进一步压缩上下文规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X上，用户@glitchy指出，&lt;a href=&quot;https://x.com/Glitchymagic/status/2008645395485020480?s=20&quot;&gt;虽然减少token是重要目标，但是尚不清楚这种动态机制是否会增加延迟&lt;/a&gt;&quot;。@NoBanksNearby则认为，动态上下文发现“&lt;a href=&quot;https://x.com/NoBanksNearby/status/2008644561674137888?s=20&quot;&gt;在同时运行多个MCP服务器时，对开发效率提升巨大&lt;/a&gt;&quot;”。&lt;a href=&quot;https://x.com/casinokrisa/status/2008862311336047058?s=20&quot;&gt;@casinokrisa&lt;/a&gt;&quot;也对此表示赞同：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;token数量几乎减少了一半，既降低了成本，又加快了响应速度，尤其是在多服务器场景下。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，@anayatkhan09&lt;a href=&quot;https://x.com/anayatkhan09/status/2008790121966170181?s=20&quot;&gt;提出了可能的优化方向&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;下一步应该是向用户开放动态上下文策略，让我们能针对不同代码仓库调整优化的激进程度，而不是对所有工具一视同仁。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据Cursor官方表示，动态上下文发现功能将在未来几周内向所有用户开放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cursor-dynamic-context-discovery/&quot;&gt;AI-Powered Code Editor Cursor Introduces Dynamic Context Discovery to Improve Token-Efficiency&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WJL8IKHd99G4zrEyTO99</link><guid isPermaLink="false">https://www.infoq.cn/article/WJL8IKHd99G4zrEyTO99</guid><pubDate>Mon, 19 Jan 2026 08:00:00 GMT</pubDate><author>Sergio De Simone</author><category>AI&amp;大模型</category></item><item><title>Agoda是如何将多个数据管道统一为单一事实来源的</title><description>&lt;p&gt;Agoda近日分享了他们如何&lt;a href=&quot;https://medium.com/agoda-engineering/how-agoda-enhanced-the-uptime-and-consistency-of-financial-metrics-ef7d54c4e4f0&quot;&gt;将多个独立的数据管道整合为一个基于Apache Spark的集中式平台&lt;/a&gt;&quot;，以消除财务数据中的不一致性的。该公司构建了一个多层质量保障框架，结合自动化校验、基于机器学习的异常检测以及与上游团队签订的数据契约（data contracts），确保用于财务报表和战略规划的财务指标准确无误，同时每天处理数百万笔预订交易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一问题源于一个典型的企业架构模式，Agoda的数据工程、商业智能（BI）和数据分析团队各自开发了独立的财务数据管道，并使用不同的逻辑和定义。尽管这种做法在初期提供了简单性和清晰的责任边界，却导致了重复计算和全公司范围内指标不一致的问题。正如Agoda工程团队的&lt;a href=&quot;https://www.linkedin.com/in/warot-jongboondee-87032ab9/&quot;&gt;Warot Jongboondee&lt;/a&gt;&quot;所解释的那样，这些差异“可能对Agoda的财务报表产生实质性的影响”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/79a40041814672a3081a9fd57fb07f2b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;独立的财务数据管道 (&lt;a href=&quot;https://medium.com/agoda-engineering/how-agoda-enhanced-the-uptime-and-consistency-of-financial-metrics-ef7d54c4e4f0&quot;&gt;图片来源&lt;/a&gt;&quot;)&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这一挑战，Agoda推出了名为Financial Unified Data Pipeline（FINUDP）的统一财务数据管道，作为销售、成本、收入和利润率等关键财务数据的单一事实来源（single source of truth）。该系统基于&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&quot;构建，每小时向下游团队提供更新，用于对账和财务规划。整合过程耗费了大量的精力：协调产品、财务和工程等多个利益相关方就统一的数据定义达成共识耗费了很长的时间；初始版本的运行时间长达五小时，后通过查询优化和基础设施调整，最终缩短至约30分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/7447722c9d9d79a59fbcc418d53870a0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;财务统一数据管道（FINUDP）的架构(&lt;a href=&quot;https://medium.com/agoda-engineering/how-agoda-enhanced-the-uptime-and-consistency-of-financial-metrics-ef7d54c4e4f0&quot;&gt;图片来源&lt;/a&gt;&quot;)&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agoda的质量保障框架采用了多重防御机制。自动化校验会检查数据表中的空值、数值范围约束和数据完整性。一旦关键业务规则校验失败，管道会自动暂停，以防处理可能错误的数据。团队使用&lt;a href=&quot;https://quilliup.com/&quot;&gt;Quilliup&lt;/a&gt;&quot;来比对源表与目标表。与上游团队的数据契约（Data Contracts）会明确约定数据格式、内容和质量要求，任何违反契约的行为会立即触发告警。机器学习模型会持续监控数据模式，识别潜在异常。三级告警系统确保通过邮件、Slack通知以及内部工具实现快速响应，如果数据更新延迟，系统会自动升级至Agoda的7×24小时网络运营中心（Network Operations Center，NOC）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一做法契合了行业的整体趋势。根据最新的行业调研，&lt;a href=&quot;https://www.precisely.com/data-integrity/2025-planning-insights-data-quality-remains-the-top-data-integrity-challenges/&quot;&gt;64%的组织将数据质量问题视为最大挑战&lt;/a&gt;&quot;。Gartner指出，&lt;a href=&quot;https://atlan.com/data-contracts/&quot;&gt;数据契约&lt;/a&gt;&quot;正成为“&lt;a href=&quot;https://www.gartner.com/en/documents/5929107&quot;&gt;管理、交付和治理数据产品的一种日益流行的方式&lt;/a&gt;&quot;”。这类生产者与消费者之间的正式协议，明确定义了数据模式（schema）和质量标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，集中化也带来了明确的权衡取舍（trade-offs）包括，开发速度下降，因为任何变更现在都需要对整个管道进行测试。数据依赖，管道必须等待所有上游数据集就绪后才能启动。详尽的文档编写和广泛的干系人共识拖慢了落地进度，却建立了跨团队的信任。Jongboondee表示，集中化“要求在每个环节都进行更紧密的协作和审慎的变更管理”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，该系统已经实现了95.6%的可用性，并朝着99.5%的目标迈进。所有变更均需经过影子测试（shadow testing），也就是，在合并请求中，新旧版本的查询会并行运行，并自动比对结果。此外，还有一个与生产环境完全一致的专用staging环境，允许团队在正式发布前进行充分的验证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FINUDP项目表明，当企业处理大规模关键业务数据时，正逐步从零散的、事后补救式的质量检查，转向架构层面强制执行的、端到端的可靠性体系。这种体系优先保障数据的一致性与可审计性，而非单纯的开发速度，这一转变在财务数据日益支撑报表生成、机器学习模型训练和监管合规流程的今天，显得尤为关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/agoda-unified-data-pipeline/&quot;&gt;How Agoda Unified Multiple Data Pipelines Into a Single Source of Truth&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/FCovAhOpFvEryKZFFxy9</link><guid isPermaLink="false">https://www.infoq.cn/article/FCovAhOpFvEryKZFFxy9</guid><pubDate>Mon, 19 Jan 2026 07:26:00 GMT</pubDate><author>Eran Stiller</author><category>大数据</category><category>机器学习/深度学习</category></item><item><title>从 Greenplum 到 Doris：集群缩减 2/3、年省数百万，度小满构建超大规模数据分析平台技术实践</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;度小满引入 Apache Doris 替换原有 Greenplum，实现整体查询效率提升 82%，与此同时，集群缩减 2/3、年省数百万的巨大效益。本文将分享度小满如何基于 Doris 从0 到 1 构建超大规模数据分析平台，并围绕平滑迁移、异地多活容灾等方面，分享实践经验。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自度小满 Doris 数据库负责人汤斯在 Doris Summit 2025 中的演讲，并以演讲者第一视角进行叙述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;度小满金融（原百度金融）作为一家覆盖现代财富管理、支付、金融科技等多板块的科技公司，数据的分析处理对其极为重要，已经深度融入业务生命周期的每个环节，是进行风险控制、商业决策、用户体验优化及运营提效的基石。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着业务高速发展，度小满原有基于 Greenplum 搭建的 OLAP 平台，逐渐暴露出三大痛点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规模与稳定性瓶颈：存储已接近饱和，扩容至百余台已接近硬件规模的承载上限，如果继续扩容，将面临更严重的稳定性挑战。性能与体验不佳：Greenplum SQL 查询执行速度慢，且经常出现 “计算时间远小于排队时间” 的情况，严重影响业务分析效率。缺失技术支持：当前使用的 Greenplum 6 版本技术架构已显得陈旧，并且 2024 年 Greenplum 宣布将停止开源，后续的技术支持与迭代升级将无法保障。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了应对这些痛点，度小满金融迫切寻找更为高效、稳定且具备现代化技术架构的数据处理解决方案，以支持其未来的业务发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Apache  Doris：高吞吐、快查询&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对日益增长的业务体量与复杂多变的分析需求，选用一个高效、可靠的数据库系统，已成为支撑业务稳健发展与快速创新的关键。Apache Doris 以其出色的性能表现与高度灵活的架构，成为众多场景下的优选方案。为深入验证其在海量数据与复杂分析场景中的能力，我们展开了一系列性能测试，关键结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;查询性能：在 1TB TPC-DS 标准测试集中， &lt;a href=&quot;https://doris.apache.org/&quot;&gt;Apache Doris&lt;/a&gt;&quot; 的查询速度约是 Greenplum 6 的 20-30 倍。导入性能：在基于 Flink 写入的 TPS 测试中，基于单分片导入，压测最大 TPS 为：5000W/s。JSON 数据处理：针对新推出的 Variant JSON 数据类型，测试显示：存储 2-3 万 Key 时，其空间占用仅为普通 JSON 的 1/10 甚至更低，查询效率则提升至 10 倍以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综上可知，Apache Doris 在写入吞吐、响应速度及存储效率上表现卓越，有力证明了其应对大规模、实时化、半结构化数据分析挑战的坚实技术基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;基于 Apache Doris 的大规模数据分析平台&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在上述详实的选型调研之后，我们决定采用 Apache Doris 替代原有 Greenplum 集群，构建超大规模数据分析平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/728fb136c52d15fb124f6f7853979806.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为验证 Apache Doris 在真实业务场景中的表现，我们先进行了小范围试点，部署了少量 Doris 集群，并先行接入几个关键业务方。试点期间，系统在性能、稳定性和易用性方面获得高度评价。基于这一积极反馈，我们稳步扩展 Doris 集群规模，最终在效率与成本上实现大幅提升：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体效率：端到端分析任务耗时从 274 秒降至 47 秒，效率提升 82%，任务超时查杀比例从 1.3%骤降至 0.11%，降幅达 91%，彻底解决高峰期排队问题实现 0 排队，使分析师的工作不再因拥堵而中断，体验和生产力均有极大提升。集群成本：在同等资源成本下， Doris 仅以  1/3 的集群数量即可提供与 Greenplum 同等的服务能力，存储性能提升 200%。截至目前，已完成 百余台原 Greenplum 服务器的清退工作，以更少的硬件资源支撑了更高的计算与存储需求，实现年度硬件成本节约数百万元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从 0-1 数据平台建设经验&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们基于 Apache Doris 成功替换了 Greenplum，完成了从 0-1 的数据平台重构，覆盖架构设计、数据流转与业务协同的系统性工程。以下将围绕快速平滑迁移、异地多活容灾与全链路生态集成三个核心环节，展开具体实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 快速迁移&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为保障业务连续性与数据安全，我们开发了自动化迁移工具 SqlGlot，将大规模数据从原有 GP 集群迁移至 Doris 集群。整个过程历经半年，累计迁移 PB 级规模数据，全程业务无感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;表结构迁移：在表结构迁移阶段，团队从 GP 系统中导出表结构及相关元数据，借助 SqlGlot 工具实现字段映射与语法适配，并在此基础上完成分区构建与分桶策略设计，确保每个分桶数据量控制在 1G～3G 的合理范围内。该流程最终成功转换超过 20,000 张表，并保障了所有表的分区与分桶结构符合业务与性能要求。表数据迁移：我们通过分布式导出将 GP 数据并行迁移至 Doris 机器，并基于 Doris 官方推荐的 Stream Load 进行并发控制，以文件流式加载的方式高效导入数据至 Doris 集群。整个过程累计完成 PB  级规模数据迁移，稳定支持了 5000+ 次数据同步任务。SQL 迁移：为解决因业务规模庞大、场景复杂而导致的官方工具语法支持不全的问题，我们基于 SqlGlot 并结合正则匹配能力，将 PostgreSQL SQL 高效转换为 Doris SQL。整个迁移流程包括“转换成功 → 执行成功 → 数据一致” ，累计完成约 47 万个 SQL 的转换，实现 95% 的执行成功率 与 92% 的数据一致率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 异地双机房灾备&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为保障数据安全并实现集群高可用，我们基于 Apache Doris 构建了异地双机房灾备架构，确保数据与服务具备跨机房容灾与双活能力。核心设计如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d5dbbf320edfc51d1d189eb7faf7b584.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将所有 Doris 集群节点均匀部署于 A 与 B 两个异地机房，通过设置 tag.location 属性明确节点所属机房。用户账号按机房绑定，访问请求通过轮询机制自动分配，实现负载均衡（例如首次请求路由至 A 机房，第二次则路由至 B 机房）。建表时通过配置 location 参数，确保每张表在双机房各保留 2 个副本，从而达成数据异地双活与故障自动切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关键配置示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设置节点机房标签&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;alter system modify backend ”BE1:9050&quot; set (&quot;tag.location&quot; = &quot;group_a&quot;);
alter system modify backend ”BE2:9050&quot; set (&quot;tag.location&quot; = &quot;group_b&quot;);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;建表时指定双机房副本分布&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;CREATE TABLE ubevent 
(ts DATETIME, uid INT, ...) 
DUPLICATE KEY(ts) 
DISTRIBUTED BY HASH(uid) BUCKETS 10
PROPERTIES (&quot;replication_allocation&quot; = &quot;tag.location.group_b: 2, tag.location.group_a: 2&quot;);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 生态整合&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为构建高效、稳定、易用的数据平台，我们还围绕 Apache Doris 进行系统性生态整合：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;计算引擎无缝集成：通过 Doris 官方提供的 Spark Connector 与 Flink Connector，实现了与现有 Spark、Flink 计算引擎的高效对接，保障了数据流水线稳定运行。运维体系化与自动化：集成 Prometheus、Grafana 及 Doris Manager，构建了覆盖监控、告警、管理与调优的自动化运维体系，全面提升集群稳定性与运维效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;优化经验&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为进一步提升数据平台的效率及资源利用率，在实际落地过程中，围绕集群、负载、存储等多维度总结了以下优化经验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 集群隔离&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前我们有多个 Doris 集群，为合理承接不同业务方的接入需求，我们主要依据业务成本与稳定性要求两大维度进行评估与路由。通常而言，稳定性越高，对应成本也越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新建集群时，稳定性最优，但相应成本也最高。为在成本与稳定性之间取得平衡，我们大多场景是基于 Workload Group 资源硬隔离方案，对 CPU 与内存进行资源组级别的隔离，有效减少不同业务负载间的资源竞争。若业务对稳定性的要求超出共享集群所能提供的范围，则仍需要通过新建独立集群来满足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 存储压力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Apache Doris 的落地与运维过程中，我们曾面临因业务快速增长带来的高达 80%-90% 的磁盘存储压力。针对这一问题，进行了一系列优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;控制表生命周期：部分业务或因对动态分区相关语法不熟悉，未主动采用该策略。为此，集成动态分区的参数配置，简化了开发难度，并提供统一注册入口，业务开发人员仅需选择是否开启、保留天数即可。修改压缩格式：将默认压缩算法从 LZ4 切换为 ZSTD。实测表明，存储空间平均节省约 50%，虽带来约 20%～30% 的 CPU 与内存负载上升，但整体 ROI 仍然较高。存储指标监控告警：为预防因误操作或异常行为导致的存储激增，建立了针对“人员”与“表”双维度的监控体系。环比分析业务人员数据占用趋势及单表每日增长量，可自动识别异常（如单日增长飙升至日常 10 倍），并及时触发告警及通知。Hive 与 Doris 打通：在基于 Kerberos 认证的 Hive 环境中，对 Doris Hive Catalog 功能进行了二次开发，实现跨系统的直接数据访问，无需依赖 Flink 等同步工具，简化了架构并提升了数据使用效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 负载均衡&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为确保系统在负载高峰期的稳定运行，特别是应对异常 SQL 与大查询带来的资源压力，应对措施如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;双机房负载均衡：基于已有的异地双机房架构，通过轮询机制实现业务流量在 A 与 B 机房之间的自动分发：首个 SQL 请求路由至 A，次个请求则导向 B，以此循环，确保双机房负载均衡，避免单点资源过载。SQL 参数限制：通过 enable_query_memory_overcommit = false、exec_mem_limit = 256 * 1024 * 1024 * 1024 等参数将最大占用内存限制为 256G，避免集群被打满，后续计划降至 60G。Workload 资源队列动态调整：基于任务类型划分资源队列，配置 CPU 的软隔离和内存的硬隔离，并支持错峰调度。比如：例行任务通常在夜间执行，为其创建专门资源队列，数据分析等公共任务大多在白天执行，将配置更大的资源队列，随着白天/夜间需求的变化动态调整资源。此外，依据各队列负载设定并行度与并发数，控制任务排队时长。异常 SQL 拦截：实时识别与拦截异常 SQL，避免其影响 BE 节点稳定性。初期使用 Doris 内置正则规则进行拦截，但规则复杂导致 CPU 开销上升。为此，我们将拦截逻辑外移至平台层执行，以避免正则匹配及超大 JOIN 导致的 CPU 负载过高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;04 集群稳定性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着集群规模不断扩大，保障 FE、BE 节点稳定性成为运维工作的核心挑战，为此，我们构建了以下保障体系：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分层触达+全维度覆盖：根据不同指标优先级设置通知电话、短信、飞书提醒，P0 监控准确率 ≥80%；自动异常处理：为 FE 和 BE 的宕机重启设置了自动化处理方案，在识别到服务卡住时，系统会自动重启进程。此外，对于磁盘掉线，将自动下线故障盘并触发副本补齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们同时采用对战分析、火焰图和日志查看等方法进行详细记录，以便后续调优。此外，编写了 SOP 手册，涵盖不同场景的应对措施，并进行了异常处理演练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结束语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截至目前，我们已搭建 3 个基于 Doris 2.1.10 版本的线上集群，其中最大规模的集群达万 core 级别、上百 TB 内存和 PB 级磁盘。目前仍在扩容中，计划在年底前新增百余台 CN 节点和数十台 Mix 节点。未来，我们将重点关注并探索以下能力：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存算分离：重点关注 Doris 3.X 版本的存储分离架构，推动落地实践。湖仓一体：全面打通数据湖与数据仓库，目前已小规模试点 Paimon；此外，针对数据外置场景，计划通过异步物化视图提升查询性能。智能物化视图探索：引入语义建模与 AI 智能分析，降低研发与业务沟通门槛，并对智能推荐与模板化方案进行探索与实践。&lt;/p&gt;</description><link>https://www.infoq.cn/article/kflvtWGPeXN40M3OPzff</link><guid isPermaLink="false">https://www.infoq.cn/article/kflvtWGPeXN40M3OPzff</guid><pubDate>Mon, 19 Jan 2026 07:04:20 GMT</pubDate><author>SelectDB</author><category>数据库</category></item><item><title>解决移动分析碎片化困局：Uber的平台引领之道</title><description>&lt;p&gt;为了标准化iOS和Android平台的事件工具，Uber工程团队&lt;a href=&quot;https://www.uber.com/blog/how-uber-standardized-mobile-analytics/&quot;&gt;重新设计&lt;/a&gt;&quot;了其移动分析架构，解决了所有权分散、语义不一致和跨平台数据不可靠的问题，目标是简化工程工作，提高数据质量，并为骑手和司机应用的产品和数据团队提供可靠的洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Uber工程师的说法，移动分析对于决策、功能采用和衡量用户体验至关重要。随着应用程序和团队的增长，工具变得分散。功能团队独立定义并发出事件，共享UI组件常常缺乏分析钩子，类似的交互在不同的团队中有不同的记录方式。其结果是，超过40%的移动事件属于自定义或临时事件，这不仅增加了分析复杂度，还降低了聚合指标的可信度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了应对这些挑战，工程师将核心分析职责从功能级代码转移到了共享基础设施。他们与产品、设计和数据科学团队合作，定义了点击、展示和滚动等标准事件类型。这些事件基于共享模式通过代码生成，在UI组件层进行监控，通过集中式报告层输出，由后端服务进行数据增强，并通过Uber的分析管道进行消费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/57/57752b438115618e1209bd6e40755fb7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Uber移动分析系统架构（图片来源：&lt;a href=&quot;https://www.uber.com/blog/how-uber-standardized-mobile-analytics/&quot;&gt;Uber博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一项关键决策是将分析逻辑嵌入到平台级UI组件中。工程师引入了分析构建器，用于管理事件生命周期、元数据附件和事件发出逻辑，使功能团队可以开展标准化的分析工作而无需编写自定义工具。他们对包含100个展示记录组件的示例应用做了性能测试，结果显示，CPU使用率或帧率没有退化，这是在性能敏感设备上推广该工具的先决条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6e/6e7e6bc5e3ff85d7b89f1160a0a08a1c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ImpressionAnalyticsBuilder类事件生成的数据流图（图片来源：&lt;a href=&quot;https://www.uber.com/blog/how-uber-standardized-mobile-analytics/&quot;&gt;Uber博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该平台还实现了常见的元数据收集。应用级元数据（如接送地点或餐厅UUID）会自动记录，而事件类元数据（包括列表索引、行标识符、滚动方向和视图位置）则由AnalyticsBuilder捕获。界面通过Thrift模型实现了标准化，可以确保容器视图、按钮和滑块的日志记录保持一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/76/76e670c370d6f7564e3dacf0d9bab52f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分析元数据金字塔概览（图片来源：&lt;a href=&quot;https://www.uber.com/blog/how-uber-standardized-mobile-analytics/&quot;&gt;Uber博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了验证平台有效性，工程师通过新旧API对两个功能进行了dual-emitted分析。查询结果表明，跨平台事件量、元数据及界面是匹配的，而像滚动开始/停止计数和视图位置等语义也保持了一致。试点应用揭示了平台和记录方法的差异，并突出了列表增强的好处——将多个行事件合并为单个标准化事件，简化了查询并提高了可测试性。功能团队还采用了可见性检查机制，减少了自定义实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;试点应用之后，Uber分析团队进行了旧事件到标准化API的迁移，使得产品团队可以专注于他们的路线图。在需要支持的地方，他们创建了自动化脚本，扫描iOS和Android代码，评估高优先级事件，并生成适合迁移的列表。平台团队还添加了一个linter，目的是拦截使用非标准API新建的点击或展示事件，防止它们进一步漂移。根据工程师的反馈，跨平台一致性得到提升，元数据和语义保持了统一，工具代码量减少，展示计数更可靠，并实现了可扩展的开箱即用UI交互覆盖功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望未来，Uber工程师正在通过组件化增强分析功能，为按钮和列表等UI元素分配唯一ID，以便标准化事件命名和元数据，进一步减少开发人员的工作量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-mobile-analytics-platform/&quot;&gt;https://www.infoq.com/news/2026/01/uber-mobile-analytics-platform/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AIuL3zoje0zRZX7CgB2r</link><guid isPermaLink="false">https://www.infoq.cn/article/AIuL3zoje0zRZX7CgB2r</guid><pubDate>Mon, 19 Jan 2026 06:02:00 GMT</pubDate><author>作者：Leela Kumili</author><category>架构</category><category>安全</category></item><item><title>阿里云 CIO 全面深度解析：企业 AI 大模型落地实践「 RIDE 方法论」</title><description>&lt;p&gt;据麦肯锡发布的《The state of AI in 2025》全球调研报告揭示， 88% 的企业已在至少一个业务职能中常规使用 AI（如 IT、营销、知识管理），但  62% 仍处于实验或试点阶段，仅有少量实现企业级的规模化部署。我们可以理解为，当下企业的 AI 落地正呈现“高采用、低价值”的典型特征，多数企业卡在试点到规模化之间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/84bd4dfab1e1871019a3cc5867d459b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;麦肯锡《The state of AI in 2025》报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 应用进入深水区，竞争的核心已经转向规模化的落地能力，而非技术本身。这也指向一个重要问题：当下的 CIO 群体，想真正实践 AI 大模型在企业的有效落地，实现规模化价值，要化解过程中的诸多坑点与难点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自阿里云智能集团副总裁、 CIO 蒋林泉在 AICon 2025 年 8 月所分享的 “阿里云大模型应用落地实践之路”，并完整呈现他对企业AI落地的经典方法论“RIDE”和数字人案例。文中，通过规模化上线的 28 个数字人的成功实践经验，分享从组织共识挑战、业务机会识别，到 AI 指标衡量，再到产品工程落地的体系化思考，以蒋林泉的第一视角，解析企业 AI  真实落地的系统路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第一视角观察&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我自担任阿里云 CIO 三年以来，第一次对外发表演讲。此次分享浓缩我过去三年在阿里云带领团队推进数字化与智能化进程中沉淀的案例与经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在担任 CIO 之前，我主要负责阿里云飞天核心系统的产品和研发工作，当时对外的演讲内容更多围绕飞天和阿里云的产品，角色也更偏向于“乙方”的产研身份。而今天，以阿里云 CIO 的身份首次对外演讲，更多是站在“应用开发者”的角度，分享如何在企业内部场景中推进数字化和智能化落地的一些实践与体会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b2533ae16f00775108b486dba8de950.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;阿里云智能集团副总裁、 CIO 蒋林泉&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两三年，我带领团队致力于推动 AI 大模型在企业各类场景中的落地应用，在这个过程中有很多感触。想先谈一下，在这个阶段里的一些观察和思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今时代，我们常常会思考一个问题：一个人或者一个组织发展得这么好，到底是时代的原因，还是自己努力的原因？其实最主要还是时代的原因。我们能够发展到今天，很大程度上是因为坐在了一个很好的“电梯”。比如搭上了中国这个电梯，中国互联网的电梯，以及我所在的阿里巴巴这个平台的电梯。平台本身发展得很好，在上面自然也发展得很好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，在电梯里做俯卧撑，还是在平地上做俯卧撑？两者达到的高度是不一样的。个人努力固然重要，但更重要的是平台。我认为，在这个时代，AI 就是那个最大的电梯。无论是组织还是个人，有没有搭上 AI 这趟电梯，将直接决定在未来能够达到的高度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8c8ae65217f6c22f5268e44ab58a999.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;ARK INVEST 报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据 ARK INVEST 以往的一份调研报告预测，到 2030 年，算力性能相较于现在将增长 1000 倍。这是什么概念？在 AI 时代之前，我们常常讲摩尔定律，技术性能大约每 18 个月翻一番。而在 AI 时代，技术发展的速度被极大地加快了。如果不能及时搭上 AI 这趟高速电梯，大概率会落后于时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这样的认识，我们发现，无论是企业还是个人，都开始逐渐意识到 AI 的重要性。意识到这一点后，许多企业，包括 CEO 和业务部门，开始变得焦虑起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就涉及到，这一轮科技革命与以往的科技革命最大的不同之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个信息技术产业中，无论是 PC 互联网还是移动互联网时代，技术在企业中的应用过程是一个渐进的过程，非常循序渐进。那个时候，企业的 CEO 看到业界的炒作、厂商的炒作，都比较冷静，可以慢慢来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，这一次的情况却截然不同。我觉得这是第一次，企业 CEO 和业务部门比 IT 团队、比供应商还“上头”。因此，我们可以说，现在企业内部最大的矛盾，就是业务部门在社交媒体、PR 渠道里看到的 AI，往往呈现出一些“炸裂”、“梦幻”的效果，而 IT 部门或者说 CIO，在实际生产力上的发展却是不均衡、不充分的。这种矛盾体现得非常突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阿里巴巴集团内部，以及我与业界几十位 CIO 交流的过程中，观察到，在企业内部，这种现象大量存在。企业中会涌现出很多 Idea，做出很多 Demo，上线很多技术平台，一个团队里，恨不得要搭好几套 Dify 平台，各种智能体平台都在搭建。但是，在这些过程中，还是技术主导比较突出，更多是拿着平台去做 Demo，业务方的参与往往比较浅层。这类现象在企业里是比较过剩的，可以说整个企业都充斥着类似的情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/78d6cdd7990881aa70079cdefc1c6083.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，我在企业中普遍观察到，很多方面的投入都严重不足：是否真正深入到业务本身去做价值识别，或去正确地定义产品，以及如何开展知识工程（注意，这里我们不再仅仅是传统的软件工程，而是知识工程），还有我们强调的业务专家知识动员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们认为，如果要在企业里真正用好 AI，并且产生实际的业务结果，就要做非常大的投入。恰好，在这个领域，我们做了很多探索和实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;阿里云企业大模型应用实践落地&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，想向大家展示阿里云内部企业 AI 大模型业务落地的全景图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这张图中，我们可以看到很多“数字人”，无论是在阿里云的官方网站、CRM（客户关系管理系统）、业务支撑系统，还是在内容管理系统、人事管理系统中，这些数字人都已经广泛地落地应用，并在原来的业务中发挥真实的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过程中，我们已经落地了大约 28 个数字人项目，从中挑几个有代表性的例子来分享，让大家更有体感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ed/edf0d08a52ae28b3e83b7faee353744c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI 翻译数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家都知道，翻译是大模型非常擅长的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在阿里云，我们遇到过很大的挑战。作为一家公共云服务提供商，为客户提供服务时，文档的作用至关重要（ ToB 的服务非常依赖文档）。阿里云拥有 300 多个产品，十几万篇文档，涉及上亿文字。其中有一个非常大的痛点在于“出海”，我们要出海到日本、美国、欧洲、印尼，还有土耳其，而我们的开发者要高度依赖文档，来操作云计算服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题在于，我们缺乏既懂本地语言，又懂云计算的人才，技术类的翻译必须同时具备这两方面的能力。但即使有足够的资金，也很难招聘到这样的人才。过去，我们只能选择“忍”，仅翻译了英文文档，以及部分日文文档，而其他语言的翻译工作基本停滞不前，这也导致海外开发者的反馈不佳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一轮 AI 技术突破之前，我们尝试过用传统 NLP 来做翻译，但效果根本不行。到了 ChatGPT 3.5 版本，我们发现自然语言处理技术，仍然无法满足我们的需求。而到了 ChatGPT 4 版本，我们再次尝试发现，翻译质量终于能和那些“既懂技术又懂本地语言”的专业译者打平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，当时也做了计算（时间在一年多前），每篇文档的翻译成本，仅为当初专业技术翻译团队的 1/200。从那时起，我们开始大量使用大模型进行翻译工作，到现在，我们已经完成了印尼语的全部翻译工作。这意味着，解决了原本靠资金也无法解决的组织问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用专业的评分来看，过去，用懂本地语言、懂技术的专业翻译团队来翻译，评分大约为 4.12 分（满分 5 分）。现在，我们用 AI 来翻译，评分能够达到 4.68 分。在海外市场，我们发现海外网站的用户体验以及 NPS（净推荐值）都得到了显著提升。因此，这不仅仅是一个成本问题，更是通过 AI 解决了过去无法解决的难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;技术文档验证数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才提到，阿里云有十几万篇文档，覆盖三百多个产品。其中，有一半是操作指南和解决方案，客户需要完全依照这些文档进行操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里一个很大的问题是：传统IT产品可能是半年或一年一个版本，文档和产品可以同步开发。但我们是互联网模式的 IT 系统，我们的情况是，线上功能不停迭代，功能的迭代和我们文档的一致性，就要实时保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原来，也是依赖外包团队进行文档验证和测试，由于“带宽”限制，只能解决中文文档的验证问题。每六个月会把所有文档“跑”一遍，去验证它们和线上功能是否一致，经常会发现有很多版本不一致的问题。但这个过程本身就有很大问题：首先一轮验证就需要六个月时间，当第一个月验证并修复好的内容，到第六个月，验证可能又变得不一致了。原来，我们一直没能把这个问题解决，导致客户经常会遇到功能与文档不符而操作不下去的问题，这就要求我们提供最新内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在我们是怎么做的呢？用 AI 来模拟这个过程：它会左边打开技术文档，右边操作浏览器里同步打开阿里云网站，然后严格按照文档里的步骤进行操作。过程中，AI一旦卡住或无法继续，就大概率意味着文档和实际功能不匹配。虽然少数情况是云产品控制台本身的问题，但绝大部分的确是文档与功能不一致。当AI发现不一致时，它会立刻把不一致的“单拎”出来，并自动创建一个Aone需求单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们后续还有一个“文档修复数字人”，它会“接手”这个Aone需求单，分析实际情况与文档描述的差异，并做修复。然后，它会把这个修复好的文档，给到我们technical writer做确认，确认后就能上线了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这之后，过去需要六个月才能完成一轮的验证工作，现在只要一个星期。同时，我们现在也把这套验证机制应用到日文、英文以及其他语种上，确保国际站的功能和文档也能保持一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去靠人工验证时，一致率到底是多少？验证质量好不好？覆盖度够不够？这些其实都是一个“unknown”的状态。而现在，一切都变得清晰、可量化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;网站 AI 助理数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个案例是网站AI助理。阿里云有几百万客户，那我们的自服务模式是怎样的呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看一组数字：每天大约 97% 的客户访问阿里云，都是通过自助操作，只有 3% 的客户会选择“提工单”。而在这3%的客户中，百分之七八十的任务也还是由自己解决的，只有极少数最终会变成需要人工介入的工单。所以，我们的客户绝大部分是自服务的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便如此，由于我们的客户基数太大，这“漏”进来的一小部分工单，依然需要我们服务团队投入大量人力去处理。在这些工单里，有一半都属于“咨询工单”。什么是咨询工单呢？就是客户遇到问题直接提问，我们的小二在后台查文档、翻知识库（Knowledge Base），找到答案再回复给他。这类工单纯粹是信息问答，不涉及操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类工单主要有几个问题：第一是一半的工单服务成本很高，第二是个时效问题。我们统计过，过去一个咨询工单的平均关闭时间，绝大部分要到5个小时左右。也就是说，一个客户平均想要解决这种咨询问题，需要花费大量时间才能解决。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网站 AI 助理上线后，大量的咨询问题已经由 AI 直接回答了，而平均响应时间是 10 秒左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们正在和服务团队合作，与服务团队共同承担全年工单降量，我们一起努力，希望通过 AI 在网站自服务的深入应用和渗透来实质拓展服务带宽，更重要的是，能够一起提升阿里云的客户服务体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智能电销辅助数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才讲的是服务，探讨了如何帮助客户解决咨询工单和自助诊断的问题，把服务体验提升了。现在来看另一个场景：销售。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云要服务上百万的企业，无法对每一家企业都用直销的方式去覆盖。因此，我们有很大一块业务是面向中小企业（SMB），通过电话销售来帮助我们客户实现售前咨询，以及售前购买的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;电话销售小二的日常工作，主要分为话前、话中、话后三个环节。话前： 小二需要做计划，规划当天要打哪些电话、了解客户的商机、准备话术，并排好优先级。需要这样一个准备过程，才能保证一天的工作有序高效；话中： 就是与客户的实际沟通；话后： 需要复盘，记录通话小结，整理哪些需要follow-up，哪些需要申请折扣。需要处理的问题都要记下来，这样才能闭环到后续的业务处理，形成一个完整闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，我们在这三个环节都提供了 AI 数字人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话前”，由AI来完成通话计划，包括怎么打，话术是什么。过去小二自己排计划要花半个多小时，现在一上班，计划就已经生成好了，可以直接开工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话中”，我们提供了一个智能辅助提醒。当小二与客户通话时，系统会根据对话内容，在工作台右侧实时提醒他如何回答，比如客户在说他想要这个，建议你这么回答。目前已经在辅助小二去解答客户非常复杂的一些云计算咨询问题。目前话中提示小二的采纳率已经达到了50%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话后”，像通话复盘、撰写小记、follow up，包括后续的通话质检，这些工作都交给了一个自动化的AI数字人来完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们的小二可以从繁杂的事务性工作中解放出来，集中精力在真正的销售沟通上，大幅拓宽了我们销售的服务带宽。同时，AI 的智能计划、实时辅助和后续复盘，也极大地提升了我们服务客户的质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智能质检数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 应用到电话质检之前，这几乎是一个原理上无解的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原来我们大规模的外呼电话作业过程，是非常难被知晓的。比如中间过程是否按照公司的作业规范进行？与客户的沟通是否足够礼貌？更有时候，有的外呼人员可能会把客户引导到私下公司去联系、去成交。但原来，我们是很难去做这个电话质检的，因为这是语音作业，很难管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，我们用 AI 把所有的电话语音全部智能化，从而识别里面所有的这些问题，再通过统一的质检标准，就能够得到一个规模化的质检。于是，这个AI质检能够大规模地提升我们的服务质量与效率，覆盖全量业务场景，关键还能控制我们的业务风险（避免产生额外的风险）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，这件事我们原来几乎是无法搞定的，因为原来是靠抽样，也就是人工抽样去听那些电话录音，如果抽样抽到了问题，再去一个个处罚，但效率是非常非常低的。它的抽样完整性、抽样覆盖度都几乎是没法被使用的（覆盖度仅有2%），不同质检员的判断差异也很大，对人力的消耗也很厉害。所以，现在通过AI质检数字人，能够让覆盖度提升到100%，质检的准确率也远高从前，带来的最终效果是非常好的，这使得整个服务质量能够规模化地提升上去。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智能外呼数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才我们讲到AI如何辅助做事，这里则是一个能直接进行智能外呼的数字人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;众所了解，云计算本身是非常复杂的，如何招聘到足够多的外呼坐席人员，让他们既具备相关技能，又熟悉云计算知识，同时还能够耐心地每天坐在工位打一天的电话，这对我们来说是一个巨大的痛点。因为招聘和能力培养难度很大，人员流动率非常高，这使得无论是销售服务，还是电话服务的质量，都存在明显的短板。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本质来看，这是一个短线影响业务增长，长线影响服务满意度与企业品牌塑造的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在前期已经有一定的知识积累，包括语音、多模态等方面的经验，因此，我们通过 AI 的方式直接引入智能外呼。它直接上场，与我们的客户沟通，挖掘销售商机，交付给服务团队去做主动的服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，在潜在客户的线索清洗、免费试用、转生产、以及产品即将到期的续费提醒等主动外呼场景中，这个数字人已经上线运行了。目前，我们还在开发场景包括产品到期的主动关怀、NPS 调研等，上线后，预计可以拓展出“能交付结果的”上百个 HC 的服务带宽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数字 AI 客服的外呼，还有些不一样的特征。首先，它可以灵活、快速地按需扩容，而且，它的声音可以做得更甜美，也可以做得更有情商。更重要的是，在技术的不断加持下，这个AI小二解决问题的能力，可能已经超过了原来人类员工的平均水平，而且还在不停地提升。目前，我们的智能外呼数字人可以像“金牌销售”一样工作，非常接近真人体验。未来会有更多的想象空间，让它能够更好地服务阿里云客户，提升我们的服务质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;直销辅助数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享了很多电销案例，这里谈谈“直销”场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阿里云的直销业务中，我们面临着一个核心挑战：销售如何变得更加专业和高效，促进公司业绩增长？在实际工作中，我们的销售团队遇到了两大业务痛点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一个痛点：云计算销售要求高、招聘难、培养成本高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云计算销售不仅需要具备良好的客户拓展能力，还需要深入理解云计算技术与行业应用场景。复合型人才稀缺，招聘难度大、周期长，新人从入门到胜任，需要经历数月的培训与实战积累，培养成本居高不下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个痛点：销售运营专业服务带宽不足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;销售运营、数据BI、财务、法务等运营中台的服务带宽，无法充分支撑前线销售需求，难以及时响应每一位销售人员的专业支持诉求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这些问题，我们将整个销售流程分为“拜访前”和“拜访后”两个关键环节，在每个环节都提供AI数字人的全方位支持。核心围绕销售作业的有效性展开，让直销过程实现“在线化”，全面提升销售过程的辅助效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;拜访前：销售“一键”获取客户“谈参”，了解客户用云信息、技术类型、解决方案、竞对情况等全面画像。过去，销售自己从各渠道去查询要花1个多小时，现在，10分钟就能查询到，而且信息质量更优、内容更全面，有效促进了与客户key person的高质量拜访。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;拜访后：我们提供AI对拜访过程的全方位复盘，包括商机要点是什么，客户对阿里云品牌表现出的情感倾向是什么，建议后续怎么推进客户成单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI软硬结合的优势，我们让直销的销售过程实现“在线化”，高质量拜访小记达到100%全面覆盖，新销售也能通过高质量在线信息资产快速学习，上手周期缩短50%，大幅降低新人培养成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式，相当于拓展数百位专业销售运营为销售团队“贴身辅助”，销售人员得以从繁琐的流程性工作中解放出来，能够更专业、更高效地服务客户，大幅提升了销售有效性，有力促进了公司业绩增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;合同风险审核数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ToB 业务的一大特征，是有大量的政企和大客户，他们通常不会使用我们的标准合同。这些合同金额巨大，需要进行严格的风险审核，涵盖财税法、风控、信控等多个方面的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去，要完成这样的风险审核，我们需要专业的法务、财务等领域的精英人士，他们大多来自国际四大会计师事务所。然而，鉴于我们业务规模庞大，不可能招聘到足够多的精英来从事这项工作。因此，我们在合同风险审核方面遇到了巨大瓶颈，审核时间过长，最长甚至可达 5 个月，平均也需要两周到一个月。这极大地拖累了业务效率，包括服务大客户的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们培养了一大批“数字人”，包括财务数字人、信控数字人和法务数字人。并且，把这些数字人送到合同撰写端，让他们在销售和客户沟通、合同拟定的瞬间，就能够实时识别潜在风险并提示谈判方案，而不是等到审核端后才发现问题，再回过头去处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在合同审核端，我们通过审核标准数字化、专家经验数字化，用统一的标准执行，极大提升了准确率。而AI也正是实现知识工作线下流程线上化的体现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI技术，我们不断拓展中后台的服务带宽，解决商业拓展流程中的效率瓶颈。后续，我们也期望它在风险拦截上的能力，能够持续提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;员工服务数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么特别提到员工服务数字人？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为大型企业里，HR 系统有一个显著特征，就是非常分散。比如请假、体检、福利、在职证明等，各式各样的流程和服务都散落在不同的系统里。与此同时，各类政策也同样分散，包括公司内部的福利政策、外部的人才政策等等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;员工在需要获取这些信息或使用这些系统时，会遇到两个难点：第一，这些服务是低频使用的；第二，它们分散在不同地方，获取难度非常大。由于是低频服务，无法配备一个庞大的服务团队来支持，所以 HR 团队的负担很重，而员工的服务体验也不足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们将这些低频、分散的服务全部整合到一个智能体中，通过钉钉平台打造了一个“云小宝”（数字人），为员工提供统一的智能服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们发现，通过引入智能体，折算下来相当于节省或新增了几十名员工在为大家服务。更重要的是，员工的体验得到了极大提升，比如，我们服务员工的响应时长已经从平均7.2分钟缩减到5秒。再比如，员工只需要用自然语言输入，如“下周一请假”、“国庆前后两天请假”或“为父亲预约体检”，系统就能迅速响应并完成操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;面试智能辅助数字人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个场景，我们聊聊招聘。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们对外招聘，核心是描述我们需要什么样的人。从这个角度出发，前置是OKR，我们通过AI分析每个部门日常在做什么，目标是什么，根据日常目标和事情，去看清楚招聘的JD（职位描述）是不是合理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再者，从JD开始，根据岗位要求，再结合当前的候选人简历信息，在面试的时候就会生成面试计划。面试时，结合岗位要求，面试官应该问哪些问题？根据最佳实践，怎么去考察候选人？这些专业问题在面试前，已经帮面试官提供好。面试中，通过对话过程，发现应该追问哪些问题，以及面试后，怎么总结面试过程中候选人是不是qualified这个岗位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI，我们可以更结构化、体系化地来做这件事，使得面试过程管理，面试质量，以及对面试人评价的客观性，都得到很大的提升。这也彻底改变了原来仅仅通过电话形式的面试，因为它的过程是一个黑盒逻辑，而“黑盒”最大的问题是无法提升过程的质量，包括保持长期的、闭环的有效性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对一家公司来说，招聘是件非常严肃的事情，我们经常讲，如果招错一个人，会导致后面的事情是非常糟糕的。所以本质上来讲，面试智能辅助数字人，提升了我们整个组织在招聘进人方面的有效性。这不只是效率问题，而是能够规模化促使我们在面试过程中的专业性、面试评价的专业性得到质的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;28个数字人全面上岗，真正产生业务价值&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3c3e51ab6dca7d2a3779dedeac199a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们有二十几个场景实现了数字人的智能化服务，这里只是挑选了10个来举例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些数字人应用背后的评估衡量，有一个共同逻辑：&lt;/p&gt;&lt;p&gt;一是折算拓展了多少人力；&lt;/p&gt;&lt;p&gt;二是业务效率提升了多少；&lt;/p&gt;&lt;p&gt;三是业务效果提升了多少。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们非常注重这一结构，因为每一个数字人上线落地，都必须衡量其对原来业务是否真正拓展了服务带宽 ，并且，是否比原来人工操作的效率和效果更好，这是非常关键的，与外界所谓的众多智能体最大的区别，就在于此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些智能体最终都是在对应的岗位上实际工作的。在我们的 HR 系统中，这些数字人被分配到对应的业务部门，向对应的业务团队汇报工作，与我们从外部招聘的员工没有任何区别。所以，它们必须在对应的岗位和业务团队中，发挥超过一定人数的实际任务执行作用，才能真正融入团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我们的钉钉系统以及内部工作系统中，这些数字人与普通员工一样，拥有工号和头像。唯一的区别在于，它们的工号以“AI”开头，如 AI001、AI002，目前我们已经有大概 28 个智能体上线，后续还有更多智能体在排队等待上线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，在过去两年，带领团队推进业务落地的过程中，我也深刻体会到，真正将技术应用于业务并取得成效，没有那么简单。特别是，真正在业务中产生价值和仅仅做出一个 Demo 之间，是天壤之别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，想和大家进一步分享，我们在这一过程中遇到的困难，以及总结出的一些解决方法，希望能对大家有所帮助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型 E2E 落地坑点与解法 —— RIDE&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家可能听过红杉提出的一个概念叫&amp;nbsp;RaaS，即“结果即服务”。这一概念的核心在于，如果仅仅提供工具和产品，让企业自行落地是不够的。所以，我们特别重视真正上线，并产生业务结果的项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我作为 CIO 所带的团队，在企业内部为业务部门提供的，就是这种&amp;nbsp;“以交付结果为导向的服务”。在推进 RaaS 的过程中，也总结出一套方法论，叫&amp;nbsp;RIDE。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e4/e49d732c30605558b4469bd17b2a70da.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RIDE 包括四个关键步骤：Reorganize（重组组织与生产关系）、Identify（识别业务痛点与 AI 机会）、Define（定义指标与运营体系）、和 Execute（推进数据建设与工程落地）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是&amp;nbsp;Reorganize。在 AI 时代，新的生产力下，原来的生产关系是非常不适应新生产力的发展，这种不适应会在每个毛孔里面表现出来，然后阻碍 AI 的发展和落地，所以要求我们要重新调整生产关系。第二，是&amp;nbsp;Identify。也就是我们需要精准地识别出企业中哪些问题适合用 AI 来解决，这要求我们首先明确问题的定义，然后结合 AI 的能力和业务需求，确定哪些问题可以通过 AI 得到有效的解决。然后是&amp;nbsp;Define。在明确了问题和 AI 的能力之后，我们需要精准地定义产品及其运营指标，进行准确的指标跟踪。最后才是&amp;nbsp;Execute。执行阶段是一个金字塔结构，上面是业务目标，下面是工程数据和评测，中间是工程应用算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这套我们称之为 RIDE 的方法论，并非在做 AI 转型的第一天就有了，而是在二十多个智能体真正有效落地业务的过程中，我们发现，如果不遵循方法论中的这些步骤，项目很可能会失败。遵循这些步骤，虽然不能保证 100% 的成功，但至少可以提高成功的概率。这是一套用两年时间、用血泪经验总结出来的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Reorganize ｜重组组织与生产关系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;书同文、车同轨 ：AI时代的通识教育&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们首先从&amp;nbsp;Reorganize&amp;nbsp;开始讲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在落地第一年，我发现了一个问题：无论是业务团队还是我们自己的团队，对大模型的能力边界、发展程度、具体原理等基本概念的理解都存在差异，甚至在我自己的团队，产品经理、算法、工程团队内部都无法拉齐概念认知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们发起了一个行动，叫&amp;nbsp;“书同文、车同轨”。&amp;nbsp;我们要求全员参加&amp;nbsp;AI 大模型的认证培训。最主要的原因，是要解决大家在基本功和认知逻辑上的差异。我称之为&amp;nbsp;“AI 时代的通识教育”，相当于要在团队里重新走一遍“高中的教育”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一培训分为两类：大模型ACA认证（面向非技术人员）、 大模型ACP认证（面向技术人员），因为我们不仅需要技术人员之间能够对齐话语，也希望非技术人员和技术人员对齐话语。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种通识教育对于团队的协作至关重要，首先在我们 CIO 线内部已经完成了全员的认证，后面，我们的业务方，也就是我们的财务、人力、销售、中后台等都在做&amp;nbsp;全员认证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，整个阿里巴巴集团都在用这个方法来做 AI 转型的基础教育，重新建立大家的基础认知。不然就会出现这种情况：大家都在谈论同一个概念，但其实理解的内容和现实完全不同。如果没有做过深入工作，很难体会到那种无力感，一旦通过通识教育统一认知，沟通效率就会显著提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/32/3210b1f4ab3dd2665737f14423be6824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云大模型ACA认证：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://edu.aliyun.com/certification/aca13?spm=a2cwt.28380597.J_1564692210.17.28813487dUqGKW&quot;&gt;https://edu.aliyun.com/certification/aca13?spm=a2cwt.28380597.J_1564692210.17.28813487dUqGKW&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;阿里云大模型ACP认证：&lt;a href=&quot;https://edu.aliyun.com/certification/acp26?spm=a2cwt.28380597.J_1564692210.18.28813487dUqGKW&quot;&gt;https://edu.aliyun.com/certification/acp26?spm=a2cwt.28380597.J_1564692210.18.28813487dUqGKW&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;「企业免费体验」大模型认证：&lt;a href=&quot;https://edu.aliyun.com/learning/topic/llm-free-trial&quot;&gt;https://edu.aliyun.com/learning/topic/llm-free-trial&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样的基础上，又设计了两个比赛。一个是产研提效比赛，一个是业务提效比赛。和其他大赛最大的不一样，我们的比赛是真正以 E2E 为衡量标准的。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如产研比赛，我们要看的，是原来 E2E 同样粒度的一个需求，需要多少“人月”完成，而现在能减少到多少人月。而不是看代码采用率，因为代码采用率很容易“灌水”，而且它往往只能补全那些最容易写的代码，最难的代码可不容易补全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在业务 E2E 方面，我们的比赛就是要真正进入业务场景，帮助业务去拓展，而且效果和效率都要超过原来。所以，这两件事非常重要，第一，是做“书同文，车同轨”的通识教育，因为 AI 时代的知识在不断发生巨变，每个月都在变，现实的实践知识和原来的基础知识都有大量的不同；第二，是“以赛促练”，整个组织通过正确目标下的比赛，大家会发现短板，发现相互之间可以学习的地方，就能够激发组织不断地去创新、去提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;数字员工 ：业务方与IT方 联合培养&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说说我们的数字员工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有一个非常关键的安排：我们的这些数字人最后都是汇报给业务部门的。这不仅关乎形式，更重要的是心理。我们不能让业务部门觉得，AI 技术会威胁到他们的工作，而是要让他们明白，AI 技术是来帮助提效的。如果这个关系没处理好，就会遇到无数的暗礁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/88e14865fc7aa21a7640472b6bec9423.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，我们把自己定位为数字人供应商，业务部门是 AI 先进组织，业务部门可以雇佣我们的数字员工，并与我们一起联合培养。&amp;nbsp;这样，业务部门会更愿意接受 AI 技术，减少阻力。所以这是第一点，我们把自己退到一个外包供应商的位置上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二点，我们还发现，AI 数字员工是不能扛责任的，也不能给它打“3.25”（低绩效）。这意味着，数字员工在系统里执行任务出了问题，谁来承担的问题。我们将 AI 数字员工汇报到业务部门，属于业务部门的人（让他们放心），并一起参与 AI 员工的培养过程，同时数字员工也会受到正式员工的监督，来承担相应业务领域的责任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;定标准 ：AI要与人比，不与“神”比&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我们经常听到一句话：ToC 还好，但 ToB 的大模型有幻觉，做不到 100% 正确。但实践经验告诉我们，其实人也有幻觉，而且人的幻觉还很大。如果认真看，在很多任务里，人其实也是不靠谱的，也经常会失败，只是企业没发现而已。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们强调的一点是：如果 AI 项目和业务部门真正达成了共识，并且通过培养逐步磨合，就必须认真回头来看，AI 的要求标准到底是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要求 100% 正确，其实就是把 AI 拿来和“神”比。但如果是和原来人做事的效果和准确率去对比，那就是和“人”比。所以，追求比人做得更好、更准，才是真正有意义的对标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那怎么避免和“神”比呢？回到前面所说，解决生产关系的问题，处理好内部业务的逻辑、目标和关系，这样才能真正实现 AI 和人比，而不是和“神”比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fc/fc59a744ddd066e6b79446f598c0a184.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个&amp;nbsp;Reorganize&amp;nbsp;的过程中，我们还发现，要把数字人汇报到业务部门，对 HR 部门来说，这就等同一个“正式员工”。注意，我们是真的把它当作正式员工来看待的，用它能否产出真正的业务结果来度量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们在内部与 CPO（HR 负责人）沟通时，讨论过：怎么去度量 AI 数字人是否真的发挥了一个正式员工的效果？最后，我们确定了一个方向：AI 数字人必须有一个目标，就是在原有具体的业务流程里，接管一个重复且有价值的任务，并且能够折算出“相当于拓展出多少人力”，这就是唯一的目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但要&amp;nbsp;真正让数字员工上线、上岗，必须满足两个标准条件：&amp;nbsp;一是数字人执行原来任务的效率，一定要比原来提升一定百分比，一定要比原来执行任务的人效率高；二是数字人执行任务的效果，同样，也要比原来提升一定百分比。只有当数字人做到效率高、效果好时，才能“正式上岗”，进入业务部门工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Identify ｜识别业务痛点与 AI 机会&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从三个特征，挖掘AI机会&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才讲的是&amp;nbsp;Reorganize，如果不解决 Reorganize 的组织问题就会不断遇到暗礁，甚至没法往前走。但解决了组织的问题后，业务部门会说，好，我们来联合培养数字员工。那从哪里开始呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以第一件事就是业务机会的识别（Identify）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2496e59b238364c5d2bc83c77059b1e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这轮 AI 革命的核心其实是 LLM（large language model），所以，我们在内部有一个逻辑：所有以 language 为中心的工作，都将被大模型深刻影响。比如电销、客服、招聘、OKR、文档、翻译、合同审核，还有研发类的 C language、Java language、SQL language 等，这轮以 language 为中心的工作受影响最大。所以第一个特征是&amp;nbsp;Language 类&amp;nbsp;工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个特征是被重复执行、规模化执行。因为 AI 是自动化的，越大规模、越重复的任务，AI 越有机会去做。第三个特征是，如果本身缺人，甚至有人投诉效率低，那这个地方就是个大的机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三个特征，是我们与业务部门一起来 Identify，识别哪些业务是可以着手的。这也是我们在内部形成共识后，如何去识别机会、定义机会的关键点。因为只有把问题定义清楚了，后面做事才会顺畅。如果解决错了问题，那投入就白白浪费了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;数字员工，以“单任务”为核心换算&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我们刚才讲到，数字员工要在对应的任务里拓展目标，也就是拓展对应岗位的人力，实际面对各种场景具体怎么处理，又怎么核算？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f33e3f605ada391bc70ba68a0fa1f467.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的经验是，首先，有些“单任务岗位”，比如技术翻译，我们是按字收费的，那么，AI 翻译一个字多少钱，就可以直接线性替换了。一个人一天的产能可能是翻译 2 万字，那我们就差不多折算成 “2 万字的产能”等同于“一个人”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果是“多任务岗位”，比如产品经理，他一会儿做 PRD，一会儿分析工单，一会儿画 Demo，一会儿又去客户那里访谈。这种多任务岗位，我们发现往往有些任务是重复的、繁琐的，也不是高价值的。为了提效，非常适合将这些低价值任务，拆分成一个个“单任务岗位”，如工单分析岗位、产品原型设计岗位等，让数字员工去做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，原岗位上的人就从繁琐工作中卸载出来，可以聚焦在更高价值的主线工作上，他们的幸福感也会爆棚。在换算方面，最终也都是”以单任务岗位为核心进行 HC 换算”，逻辑清晰明了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式原先主要是由外包承接，但受制于外包员工管理难度大、成本构成多、招聘周期长、稳定性低、用工风险高、能力上限低（薪资因素）等诸多原因，多方面都受到约束，无法大面积展开。当我们有了数字员工之后，自然解锁了这些约束，&amp;nbsp;这件事就变得更加切实可行。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Define ｜AI 的产品度量与运营度量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;准确率是AI产品核心&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过了&amp;nbsp;Identify&amp;nbsp;这一步，下面就是&amp;nbsp;Define。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个时代和以前做产品有很大不同。我们前面提到的一些产品大多都类似，比如都有交互、体验。在这个流程里，其实和上一轮移动互联网的产品没有区别。但&amp;nbsp;AI 产品有一个特别关键的点，就是“准确率”。&amp;nbsp;当然，除了准确率之外，还有响应时效性和安全合规等非功能性指标，比如在电销过程中，和客户实时对话，延迟必须非常低，不然客户会觉得交流效率不高，像机器人说话一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9b45111fb0badee9130ac4bc4cf4cb6f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，实时性和准确性非常关键。如果准确性不够好，客户根本无法使用，也根本不可能真正上岗。所以，准确率是 AI 项目的第一核心指标，整个项目组都必须盯住它，这也是产品定义中最核心的部分，必须重新去&amp;nbsp;Redefine。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;运营与产品指标「协同度量」，才不掉坑&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，运营指标同样至关重要。如果只有产品指标和准确率指标，那大概率会掉到“坑里”。即使是在对内的业务项目里，原来移动互联网那些基本功也不能丢，比如：&lt;/p&gt;&lt;p&gt;DAU（每日活跃用户数）；用户提问数；渗透率，即目标客户的覆盖率；留存率（最关键）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果同一个客户今天用了，下周还愿意继续用，说明这个 AI 智能体真正帮他解决了问题。如果客户只用了一次就不再回来，那么无论前面的产品指标再漂亮，都没有意义，那可能就是定义错了问题。运营指标就是用来兜底的，如果不紧盯这些指标，很容易让产品、工程和算法团队陷入“自嗨”。什么叫自嗨？就是他们说“我的指标很好”，结果客户根本不用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子，在阿里云官网的 AI 助理中，我们就设定了这样的度量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如下图所示，左图展示了准确度的度量指标，时间线大约覆盖从去年到今年的一年时间。蓝色区域代表表现良好的部分（精准解决了客户的咨询问题和任务），黄色区域为中等水平（虽解决了任务，但伴有大量无关信息），红色区域则是表现差劲的部分（回答与客户问题完全不相关）。中间图展示了 DAU 和客户问题数，右图则是留存率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/69/69aea369bbf9588752408a3a661b13a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们的留存率实际上已经达到了一个相当高的水平（PPT中并未刷新数字）。从图中可以清晰看到，随着准确度的持续提升，DAU 和留存率也在稳步上升。但是反之，如果 DAU 和留存率始终停滞不前甚至下滑，即使你的工程和算法团队声称准确率很高，那无疑是自欺欺人的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际上，很多工程算法团队成员，可能并未意识到上述这一点。之所以能明确指出，是因为在左图的准确度指标上，我也曾经被多次误导，但这也并非团队有意为之。在如今的信息环境中，随便搜索公众号就能发现大量类似“用这一招，你的准确率能提升到 95%”的文章，但这些文章往往存在误导性，它背后都有一个前提条件，即在某个狭窄的小场景下，准确率能够达到 95%，然而在面对海量问题时，这一指标却难以提升（这一点稍后会详细分享）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Execute ｜推进数据建设与工程落地&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;掌握「产品研发工程金字塔」&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;定义好了产品和运营指标（Define），往下走才是执行（Exectute）阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Exectute&amp;nbsp;阶段的关键在于：一定要用产品和业务目标来拉动。因为在牵引拉动的过程中，才能充分动员领域知识专家的参与和评测。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果没有知识专家的深度参与和强大的评测能力，大模型的应用上限是很难提升的，这是第一点。第二，如果项目目标缺乏价值，或者没有真正的痛点，那么会发现得不到资源的“祝福”。也就是说，一方面难以获得其他团队的配合，另一方面自身团队的价值感也难以维持，这将直接影响项目的推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b32b199df39f232cd6c8a4b1f77f6b2d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个执行逻辑中，金字塔最下面是工程的数据与评测，我把这个放成最大的一块底座，因为这是基石——业务数据、业务 API 以及评测能力是大模型应用的基础，对这一部分的投入必须充足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一基础上，才是工程应用算法、预训练（Pre-training）、RAG 以及微调等等，这些在媒体报道里面出现的技术热词，并非不重要，但这些只是&amp;nbsp;“必要条件”。我观察到，大多数产研团队在这部分（工程 - 应用与算法）投入了 80% 至 90% 的时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但想强调的是：这些只是必要条件，仅靠这些无法解决企业 E2E 落地的问题。哪怕你在必要条件上投入再多，再加 10 倍的努力，也无法实现真正的 E2E 落地。因此，必须设法补齐真正实现 E2E 落地所需的充分条件。&amp;nbsp;如果无法做到这一点，项目成功的希望将十分渺茫。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;常见的LLM AI应用范式：翻译、Agent&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在与业务团队沟通以及处理各种复杂问题的过程中，我们总结出了几种常见的模式：&amp;nbsp;首先是基础设施层面，涉及知识和数据的构建；中间是编排和调度，无论是大家熟悉的工作流编排，还是智能体自主规划编排，或是两者的结合；最上面是对客的产品与运营。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里，重点讲述图中深蓝色部分的两种模式：第一个是翻译模式，第二个是 Agent 模式，我认为主要分为这两种典型的应用模式。其中，翻译模式最容易取得成效，因为它相对简单；而智能体模式则较为复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7d/7d19b513a0cc83ac769f6db55472de5a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;翻译模式：关键在“蛋糕坯”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;先谈谈翻译模式。&amp;nbsp;在公司内部，我们将所有翻译类模式统称为 AI 领域的“低垂果实”，这类模式相对容易实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一轮的大型语言模型背后的算法是 Transformer。Transformer 最早是 Google 为了翻译任务而开发，在不停做翻译的过程中衍生出了 Transformer 算法。随后，预训练模型如 BERT 也大量应用于翻译领域。所以，大模型的原理 Transformer 特别擅长做翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;翻译又可以分为狭义翻译和广义翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;狭义翻译指的是中译英、英译中等语言之间的转换。而广义翻译则涵盖更广泛的形式，比如：自然语音转成文本，再转成语音；自然语言转成 SQL 语言；自然语言转成 Java 语言；甚至让一篇论文用自然语言“翻译”成中学生能听懂的表述，这些都属于广义翻译范畴。无论是狭义翻译还是广义翻译，Transformer 都特别擅长，因此这是最容易出结果的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a03ff9f85ee03396d4af733c98d5a773.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这里有一个坑：&amp;nbsp;虽然（图中）左边的翻译能力已经具备，但如果右边原有的系统还没准备好（not ready），就会出现问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Chat BI 来说，为什么 Chat BI 在企业里没什么成功的案例呢？其实很大一部分原因在于，Chat BI 的逻辑无外乎就是：用自然语言翻译成 SQL，然后在后台的数据库或大数据系统里执行，再把执行结果取出来，再翻译成自然语言返回给人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Chat BI 的实质，就是自然语言 → SQL → 执行结果 → 自然语言，这本质上还是一种翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我们会发现一个很有意思的问题：很多企业说要上 Chat BI，但如果原本数据库和里面的业务逻辑、数据口径积累不足，甚至连人都写不出对应的 SQL 来，那自然语言也一样翻译不出来。因为后台本身没有可执行的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我认为，企业里绝大部分在 Chat BI 上踩的坑，都来自于一开始就想做一个过于“宽”的东西。但是做了这个翻译之后，如果原来的系统 API 没准备好，数据没准备好，甚至连原来的人都无法执行这些操作，那自然语言翻译也没法落地。这就是最大的误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此我们在内部的逻辑是：要先 Identify 原系统具备哪些能力。比如，如果你原来的 ODPS、数据库和数据中台本身已经有 BI 和运营，能够在某个领域里不断取数、用 SQL 分析数据，而且业务场景也很丰富，那么，那些高频的 SQL 语句才是真正值得作为翻译目标的部分，而不是盲目地去做一个 Chat BI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以很关键的是要分成两个部分 ：一部分是翻译，一部分是原来系统的语言处理能力。我习惯这么来形容：原来的系统就是“蛋糕坯”，大模型翻译就是上面的“樱桃”。如果你现有的蛋糕坯是 ready 的，我放一个樱桃上去，你就可以吃樱桃蛋糕了。但是如果原来的蛋糕坯都没有，你让我做一个樱桃蛋糕，是做不出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里非常重要的一点是：要能够识别出原来的蛋糕坯是不是 ready ，然后在上面放上你的樱桃，而不是直接拿一个樱桃就装作是樱桃蛋糕。这个地方往往就是个误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;翻译模式是“低垂的果实”，容易做，但里面其实有非常多的坑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 模式：关键在意图与知识空间&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说&amp;nbsp;Agent 应用模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家可以注意这样一句话：所有的 Agent 应用模式都是始于用户意图，终于意图满足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你不是从用户意图出发，最后又不是以是否满足客户意图来作为度量标准，去看待你的智能体，那一定会失败，没有任何成功的可能性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我发现团队，甚至整个业界，最容易出现的问题。因此我们引出了一个方法，这是我在内部做智能体时，一定要去践行的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一件事情，要找到这个领域的“意图空间”。&amp;nbsp;当一个客户在智能体里和你交互时，他一定是带着意图的。那么这些意图都有哪些？比如客服场景里，客户会提出各种咨询问题，这些问题本质上就是一个空间、集合。所以，第一步就是要搞清楚这个集合的&amp;nbsp;边界和完整性。如果你不知道它的完整性，就无法去度量。只有在建立了完整的意图空间之后，才能继续往下做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，第一件事要建立意图空间。然后，当清楚地知道了意图空间，就要基于这个意图空间来准备&amp;nbsp;知识工程。也就是说，你的知识、文档是否完备？API 和结构化数据是否具备？能否真正满足客户的这些意图？我认为这是最基础的必要条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ec59798c52a3434dbee1855bf4222dd3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再者，有了知识、意图空间，接下来才能带着意图去做评测。&amp;nbsp;因为既知道用户的意图，也掌握了知识，这样才能真正开展工作。如果意图不清楚、知识不具备，其实就是“空转”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的经验是：在客服场景里构建意图空间，从原来就在满足意图的领域出发，从&amp;nbsp;工单&amp;nbsp;里去分析和构建意图空间。有了意图空间之后，就可以对意图进行分类。分类完成后，再根据不同类别去检查和补全知识，做好知识工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，当&amp;nbsp;意图空间&amp;nbsp;和&amp;nbsp;知识空间&amp;nbsp;都建立好了，才有可能开展评测，也才知道如何去度量你的 Agent。只有具备了度量能力，才有可能进一步做工程和算法迭代，这个是原理决定的。这也是我们在内部做智能体的一个必修课。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里，简单总结一下两个模式： 翻译模式是樱桃，一定要先找到原来的蛋糕坯在哪里，再把樱桃放上去。如果蛋糕坯不 ready，只放个樱桃一定会失败。而 Agent 模式的关键则是：始于用户意图，终于意图满足。&amp;nbsp;这是一系列完整的逻辑方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 落地要点：意图空间、品味&amp;amp;评测&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我们就展开讲这个稍微复杂一些的 Agent 模式，看看在业务体系里实现 E2E 落地的一些关键要点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b387cad2e57fd680552b6106c750368.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，对意图空间的投入进行 ROI 评估。做一个 Agent，它的 ROI 高不高？这取决于意图空间的大小。如果工程所需的知识量庞大，意图也非常多、非常宽，那么所需要的投资就会非常大。意图空间越大，为满足这些意图所需要的知识、工程和迭代的投入也就越大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以有一个非常清晰的结论：第一件事情，就是要控制意图空间的规模。如果不控制规模，会导致失败，因为后续的投入很难支撑。这里要记住一句话：如何去控制一个智能体的意图空间？如果没有控制好，或者不清晰，那么 ROI 根本算不出来。而一个算不出 ROI 的项目，成功的可能性将大打折扣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，&amp;nbsp;我们经常讲，最近大家肯定也听说过，在 AI 领域里经常提到一个词叫“品味”。AI 时代里，品味非常重要。&amp;nbsp;那么品味来源于哪里？我自己猜测，要追溯到 1995 年乔布斯（Jobs）的一次采访。当时记者说：听说你比较粗暴、独裁，你怎么知道你的决定就是对的？乔布斯想了大约 10 秒，回答道：“归根结底，最后是品味决定的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;品味和这一轮 AI 的关键问题——评测——高度相关。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e5d44b291935bf9a2538dfc67f28ffe.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一轮和上一轮 AI 革命最大的区别在哪里？&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上一轮深度学习主要是计算机视觉。那时候的数据评测怎么做？一张图给猫、狗、交通灯、汽车、人等等打圈，数据打标就是这么来的。所以评测时，只需要看分类对不对（猫有没有被错分成狗？对了就好）。ImageNet 就是这样做的，李飞飞当年找了很多外包团队来做标注，这种标注工作很适合外包，找普通人就能做。原因很简单，猫狗识别不难，就算是一些专家领域，比如故障识别、次品检测，标注也相对容易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这一轮情况完全不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型的输入是小作文，输出也是小作文。在专业领域尤其如此，很难直接度量。这就是为什么要强调品味——因为没有标准答案。我们都是经历过高考的。高考作文有没有标准答案？没有。开放题，比如写一篇中心思想总结，有没有标准答案？也没有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型的评测正是如此。所以，这一轮大模型最关键的区别在于：度量数据、评测没有标准答案。既然这是没有标准答案的，意味着成本最高，也就成为落地的瓶颈。&amp;nbsp;如何解决这个瓶颈？只能重投入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 落地要点：如何做好「评测」&amp;nbsp;&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这里讲的“品味”，就是如何做评测的问题。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/954a3b39d3b94c4660a5d04eb8f29e4a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们怎么去评测？评测是一件非常重的事情，这包括业务效果的评测能力，也包括评测本身的工程化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，在人工评测中，我们如何去解决分类的标准问题？什么是“好”，什么是“中”，什么是“差”？如何能够确保，评测对真实业务意图的覆盖度是足够的？如果覆盖度足够好，标准也足够清晰，我们又如何通过工程化的方式，对系统的迭代和变动进行自动化评测？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于人工评测和度量，很多时候就像写一篇小作文，它是非标的，是没有标准答案的东西。相反，为什么现在编程发展很快？因为数学和编程都有标准答案，可以被编辑器校验，但是纯文本是没有标准答案的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，评测这项工作非常耗时，也很容易成为整个项目的瓶颈，是需要极大加强的。如果不去加强，那么整个项目的基石就可能动摇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在评测的过程中有一个非常重要的点，叫&amp;nbsp;E2E 归因。因为在智能体的过程中会有非常多的环节，在这么多工作流和智能体的编排逻辑中，如果一个意图没有被满足，我们必须要有能力确定这个 Bad case 的问题到底出在哪个环节。当每一个 Badcase 都应该归因到工程里的具体环节，才能对具体的原因进行聚类和改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b4959fc8d9887488b8ad3f85288eebeb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果从产品宏观功能体系来看，体系的最底层，必须要有两样东西：第一，是业务评测；第二，是全链路的归因分析能力。我把这两项放在最底层，就是因为它们太重要了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图这是个大概率的经验总结，也就是说，如果具备度量能力，会发现&amp;nbsp;大部分问题都出现在数据层面，出现在非结构化、结构化数据 API。如果基本能力不具备，这就是智能体失败的主要原因。部分问题可能出现在知识预处理、意图识别、上下文检索，以及后续的意图识别总结等环节。数据极为重要，但没有评测也就谈不上数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b260a7e00c9eec4a1b1814491691a22.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;引出一个经常被讨论的问题：是否需要引入模型训练？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的观点非常明确：必须在白盒方式下使用基模 API，注重评测和数据，并进行 E2E 归因迭代。只有当数据质量和评测能力具备时，才能引入训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因很简单，如果数据和评测能力不 Ready，投入在训练上的每一分钱都是浪费。如果数据不够好，那就是“garbage in, garbage out”。这些问题，都不是训练本身能够帮助解决的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，训练的周期长、成本高、迭代速度慢，如果没有能力评估训练结果的好坏，也没有足够的数据进行训练，这种投入是不明智的。因此，只有在必须使用训练，且基模无法解决问题时，我们才会引入预训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/af4dcf4c9bf5a8fb2b7d24b1f94954ce.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;写在最后：AI+云的「大电梯」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，为大家回顾一下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在阿里云内部推进 AI 转型，本质上是需要为业务提供 Result as a Service（RaaS）。我们也是当前时点为数不多的，能够真正大规模实现 E2E 落地，给业务交付结果的实践团队。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而我们实现 Result as a Service 的方法叫&amp;nbsp;RIDE，RIDE 分别代表 Reorganize、Identify、Define 和 Execute。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要特别注意的是，在必要条件上再努力，也解决不了充分条件的问题，所以这个 RIDE 方法论的核心是在提醒大家：只有把落地所需要的充分条件补齐，才能真正开展 AI 企业有效落地的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a0118e1b86a2b596ba7940d8b986889a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;呼应最开始讲的“电梯”，想表达的是，冰山之上，我带着团队一直在做业务的数字化转型，之所以能够实现，是因为冰山之下，有强大的阿里云作为底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是涵盖通义千问在内各种模型服务的 MaaS 百炼，还是 PAI，ODPS，数据库等 PAAS 服务、或是底层 IaaS 比如 ECS、灵骏、存储、网络服务，都是我们依赖的企业应用的有力支撑武器。而且，这些能力的成本在不断下降，功能也在持续拓展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，当企业选择了一个强大的技术底座，随着技术水平的增长和成本的下降，企业的数字化转型也就能够搭上一部更好的“电梯”。我自己认为，阿里云就是这样一部“大电梯”，企业上云后，这部电梯持续为企业实现数字化转型，提供源源不断的上升动力。&lt;/p&gt;</description><link>https://www.infoq.cn/article/BKMR19Rj75XwLR1ysM4B</link><guid isPermaLink="false">https://www.infoq.cn/article/BKMR19Rj75XwLR1ysM4B</guid><pubDate>Mon, 19 Jan 2026 03:23:04 GMT</pubDate><author>籍云</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>微软介绍了TypeScript 7的更新</title><description>&lt;p&gt;微软近日分享了&lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt;&quot;&amp;nbsp;7（代号为Corsa项目）的最新进展，披露了对TypeScript编译器的一次根本性重构。该更新&lt;a href=&quot;https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/&quot;&gt;发布于2025年12月&lt;/a&gt;&quot;，详细介绍了团队将TypeScript编译器用Go语言重写的宏伟计划，他们承诺构建速度最高可提升10倍，并显著降低内存的占用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款名为tsgo的全新原生编译器充分利用了Go语言的性能优势，带来了大幅度的速度提升。据TypeScript团队表示，与旧版本相比，完整构建速度最高可提升10倍，并具备高效的多项目并行处理能力。为编辑器功能（如代码补全、跳转定义、重构等）提供支持的原生语言服务目前已基本稳定，可供日常使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户现在就可以试用这一预览版：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;npm install -g @typescript/native-preview&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TypeScript 7最重要的变化之一是默认启用&lt;a href=&quot;https://github.com/microsoft/TypeScript/issues/62333&quot;&gt;严格模式（strict mode）&lt;/a&gt;&quot;，这是一项与以往版本不兼容的破坏性变更。这一转变体现了团队对类型安全的坚定承诺，也符合行业最佳实践，但可能要求从旧版本升级的项目进行相应调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;选择Go作为实现语言在开发者社区引发了广泛讨论。团队在一份&lt;a href=&quot;https://github.com/microsoft/typescript-go/discussions/411#discussioncomment-12464988&quot;&gt;详尽的FAQ&lt;/a&gt;&quot;中解释说，Go提供了自动垃圾回收机制，同时又是目前最贴近“原生优先”理念的语言。此外，现有TypeScript代码库采用高度函数式的编程风格，几乎不使用类，因此Go的函数与数据结构范式比面向对象语言更为契合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=43332830&quot;&gt;Hacker News&lt;/a&gt;&quot;上，开发者们对性能提升表现出了极大的热情。一位用户评论说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇，这太震撼了！10倍的速度提升对我们这类大型TypeScript项目将是颠覆性的。我一直在等待这样的改进，我们团队的项目在CI上的类型检查耗时极长，并严重拖慢了IDE的响应速度。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，也有开发者对依赖TypeScript编译器API的工具迁移路径表示担忧：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;……对于我们这些工具作者来说，这个原生编译器将如何分发？我猜会通过WebAssembly（WASM）？编译器API是否兼容？比如转换器（transforms）、抽象语法树（AST）、LanguageService、Program、SourceFile、Checker等等？&amp;nbsp;我非常担心工具生态的迁移可能会异常困难。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些开发者已经上手尝试。Reddit上有&lt;a href=&quot;https://www.reddit.com/r/typescript/comments/1pcgmrj/comment/ns0frwz/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;用户&lt;/a&gt;&quot;称其类型检查时间减少了75%。&lt;a href=&quot;https://www.reddit.com/r/webdev/comments/1pcqzn3/progress_on_typescript_7_december_2025/&quot;&gt;还有人&lt;/a&gt;&quot;对默认开启严格模式表示欢迎：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;默认启用严格模式真是太棒了。我们以前经常在项目中工作到一半才发现严格模式没启用，结果要修复一大堆问题，非常令人头疼。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于重度依赖编译器的开发工具而言，TypeScript 7的原生实现使其与其它以原生语言编写的高性能JavaScript工具站在了同一赛道。例如，用Go编写的&lt;a href=&quot;https://esbuild.github.io/&quot;&gt;esbuild&lt;/a&gt;&quot;，以及用Rust编写的SWC和oxc，均已证明原生实现能带来显著的性能优势。TypeScript团队此次转型不仅验证了这一架构方向的正确性，同时也确保了与TypeScript语言规范的完全兼容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TypeScript是由微软开发和维护的一种强类型编程语言，它在JavaScript基础上增加了静态类型定义。自2012年发布以来，TypeScript可编译为纯JavaScript，运行于任何支持JavaScript的环境，包括浏览器、Node.js及其他JavaScript运行时。通过其类型系统，开发者能在编译阶段而非运行时捕获错误；借助智能代码补全、重构等特性，IDE支持也得到了显著增强，同时，显式的类型契约使大型代码库更易于维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/typescript-7-progress/&quot;&gt;Microsoft Share Update on TypeScript 7&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ev8UP654Oux2DreuC6T0</link><guid isPermaLink="false">https://www.infoq.cn/article/ev8UP654Oux2DreuC6T0</guid><pubDate>Mon, 19 Jan 2026 02:42:41 GMT</pubDate><author>作者：Daniel Curtis</author><category>微软</category><category>编程语言</category></item></channel></rss>