<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Thu, 29 Jan 2026 11:05:41 GMT</lastBuildDate><ttl>5</ttl><item><title>如何大规模构建、部署和管理智能体</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着大语言模型能力的成熟，围绕 AI 智能体的讨论正在迅速升温。构建一个能够执行任务、调用工具的 Agent，已经不再是少数团队的专属能力。但在这场技术热潮之下，一个更现实的问题逐渐浮出水面：当智能体不再停留在演示环境，而是被放入真实业务系统中运行时，会发生什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CrewAI 创始人兼 CEO Joao Moura 以实践者的视角，在 BUILD 2025 大会上系统梳理了 AI 智能体从概念、原型走向生产环境过程中，所面临的一系列关键问题。这场分享的核心，并不在于“如何快速做出一个 Agent”，而在于如何让 Agent 在复杂系统中长期、稳定地工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“会生成”到“会决策”：重新理解智能体的能力边界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享中，Joao 首先回到一个基础问题：什么才是 AI 智能体真正的能力来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他指出，很多人已经非常熟悉大语言模型在内容生成上的表现，例如生成文本、改写表达、调整语气。这些能力本质上仍然是“输出导向”的，模型根据输入，生成一段结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而智能体的出现，源于另一类能力的被系统性使用：决策能力。当模型不仅要给出答案，还需要在多个选项之间做出判断，并说明为什么选择其中一个时，它开始参与“思考过程”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5e31ab04b331a193c0acde05d979ea6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，当系统为模型提供可调用的工具，并赋予其一个明确目标，模型就不再只是被动响应请求，而是开始围绕目标不断判断下一步行动。这种行动可能包括调用内部系统、获取业务数据、更新状态，甚至触发后续流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体并不是某种全新的技术形态，而是一个围绕目标进行持续决策与行动的系统。理解这一点，是后续讨论生产化问题的前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正的分水岭：为什么原型和生产完全是两回事&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到智能体落地时，Joao 明确指出了一个现实情况：从原型到生产，并不是一次线性升级，而是一道本质不同的门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原型阶段，团队关注的往往是“能不能跑起来”；而进入生产环境后，关注点会迅速转向“能不能持续运行”。这时，模型本身反而不再是唯一变量，系统层面的复杂性开始占据主导。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94f0feee4fcd61857bb572dbc119f48f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体一旦被放入真实业务系统，就意味着它将与 ERP、CRM 等核心系统交互，其行为可能直接影响业务流程。在这种情况下，系统是否稳定、决策是否可控、行为是否可预测，都会变成不可回避的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多阻碍智能体进入生产的因素，并不来自 AI 本身，而是来自工程、架构和系统集成层面的现实约束。这也是为什么不少 Agent 项目停留在 Demo 阶段，却迟迟无法真正上线的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;决策、工具与执行：Agent 在系统中的运行方式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个智能体并不是简单地“调用模型”，而是需要在决策、工具调用和执行之间形成闭环。模型负责判断当前状态下应该采取什么行动，而系统则需要确保这些行动能够被安全、准确地执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体需要调用外部工具时，问题并不止于“能不能连上接口”，而在于调用是否可控、结果是否可追踪、失败是否可恢复。这些因素，都会直接影响智能体是否具备进入生产环境的条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种结构下，Agent 更像是一个被嵌入到系统中的“决策节点”，而不是一个独立存在的智能模块。它的价值，取决于整个系统是否为它提供了稳定的运行土壤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当数量上升：规模化带来的管理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体不再是单点实验，而是开始成批部署时，另一个问题随之出现：如何管理这些 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规模化并不意味着简单复制更多实例。随着智能体数量的增加，部署、运行、监控和管理本身会迅速成为新的复杂系统。如果缺乏系统性的设计，智能体越多，整体风险反而越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到整场分享的核心，Joao 传递出的判断其实非常清晰：AI 智能体的真正价值，并不在于是否足够聪明，而在于是否能够被可靠地使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当讨论从“能不能做”转向“值不值得用”，从原型转向生产，智能体面临的已经不是技术炫技的问题，而是工程与系统成熟度的检验。也正是在这个阶段，智能体才真正开始进入创造长期价值的轨道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</link><guid isPermaLink="false">https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</guid><pubDate>Thu, 29 Jan 2026 10:26:09 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>不要再纠结 LLM 准确率了：从“回答对不对”到“系统是否值得信任”</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每个人都在努力提高大语言模型的精准度。但真正的挑战并非精度，而是上下文理解能力。在 BUILD 2025 大会上，Hex 合作伙伴工程负责人 Armin Efendic 探讨了为什么传统的方法，如评估套件或合成问题集往往不够有效，以及成功的 AI 系统是如何通过随着时间推移逐步积累上下文来构建的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由 Snowflake Cortex 提供支持的 Hex，启用了一个新的对话式分析模型，每次交互都让模型变得更聪明。通过 Hex 的 Notebook Agent 与 Threads 功能，业务用户可直接定义核心问题，而数据团队则将这些问题精炼、审计并转化为持久且值得信赖的工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个模型中，测试用例不再由数据团队闭门设计，而是由业务需求驱动并在数据工作流中自动实施，最终形成一个具有生命力的上下文系统，而非一成不变的提示词或测试集，它能随着组织共同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c3f33c248d2dee34e327d20cde14222a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;准确率不是终点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Armin 开场就把矛头对准了一个常见做法：把业务用户会问的问题合成成一批样例，甚至进一步转成 SQL 查询，然后把这些喂给 LLM，用类似单元测试的方式去衡量它的准确率、稳定性与一致性。他不否认“准确性是顶层关注”，但他强调，把 LLM 当作传统软件组件来做单元测试，本身就是一个不合适的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因在于，当你把业务问题硬转换为一组 SQL，并据此去构建样例与评估集时，你很难覆盖真实业务中不断变化的语义、不断扩张的问题空间，以及不同用户在不同语境下对同一指标的不同问法。更重要的是，即使你做出了一个看似通过率很高的测试集，也依然回答不了企业最在意的那件事：当它在真实环境中生成了一个结论，你如何知道它不是在胡编？你又如何知道它到底做了什么才得到这个结论？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，Armin 把正确性从结果层拉回到系统层：你需要的不是一个靠样例证明自己正确的聊天机器人，而是一套可审计的系统，它能够随着时间变得灵活、可塑，能够让业务用户在使用中不断收敛可回答的问题类型，也能够让系统拥有被“硬化”的路径：哪些能力可以放开，哪些问题必须收紧，哪些定义需要固化，哪些数据应该进入上下文、哪些不应该。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，真正有效的路线是：从一套能运行、能被观察的系统出发，让系统在使用中暴露问题、沉淀模式，再反过来加固上下文。这种思路听起来不如直接做评估来得爽快，但它更接近企业系统的真实生长方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;对话式分析如何变成“可审计的系统”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了把“可审计”讲得具体，Armin 用 Hex 的产品演示展示了对话式分析在真实系统中应该是什么形态。演示从一个非常典型的业务问题开始：假设我是营销经理，我想让系统分析销售机会的“首次触达来源”（first touch source），并做营销归因视角的拆解。这里一个很关键的动作，是他先在系统里配置模型提供方：通过密钥对（key pair）连接到 Snowflake 实例，使用 Snowflake Cortex 内托管的 Claude，并强调这是一个“walled garden”的私有网络环境。这样做的直接意义是：模型驻留在数据所在的环境里，数据可以传递给模型，同时也能让 IT 团队对数据出入边界更放心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入线程后，Hex 并不是立刻吐出一句“结论”，而是在后台进行一系列用户不可见但决定可信度的步骤：它会先围绕可访问的元数据“思考”，查看平台上已有的 Hex 项目、仪表板或资产，判断是否存在可复用的内容；它会拉取来自数据仓库的表描述、列描述等元数据，并强调这些可以自动导入、不需要复杂配置；如果企业已经有 dbt 元数据，也可以进一步带入；随后它形成一个“漏斗式”的收敛过程：从广义元数据到相关表、再到更具体的模型信息与底层数据，最后才开始把 SQL 单元格、可能的 Python 单元格、图表与可视化逐步组织起来，用以回答最初的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也解释了他在演示里专门强调的一个点：这种模式一开始会“慢”，但这是刻意设计的。因为此时系统面对的是生产数据仓库，它需要把大量上下文带进来，需要推理与迭代，而这类深度思考天然会以时间为代价。换来的收益是：它可以生成更细致、接近数据科学家或嵌入式数据分析师水平的分析过程。Armin 也提到，未来会有更偏“快速、短促回答”的迭代版本，可能更多依赖语义模型，而不是每次都在全量上下文里深挖。但在这个阶段，他们优先解决的是“在没有分析师介入的情况下，业务用户也能得到一份扎实的分析报告”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当线程生成结果后，界面里不仅有图表，还能继续做探索：拖拽维度与度量、查看底层表格数据、检查异常、做更深的切片。这时“可信度焦虑”就会自然出现：这么多信息暴露给业务用户，我怎么知道它没有幻觉？我该不该信这些 SQL？我如何让它更确定？Armin 的回答不是“相信模型”，而是把系统的底座亮出来：在 Hex 里，每一个线程、每一个项目，背后都由笔记本支撑。把线程保存为项目后，你可以在笔记本里看到完整对话以 Markdown 的形式呈现；更重要的是，你能看到它实际运行的 SQL、过滤条件、连接逻辑、图表生成过程，以及它如何一步步构建出整份报告。对于负责准确性与治理的数据团队来说，这种“把对话落到可审计的笔记本”非常关键——它让系统从一开始就具备被审核、被追责、被修正的可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Armin 进一步展示了一个更现实的协作场景：业务用户提出问题后，不一定要立刻去找数据团队提工单，而是先在对话线程里得到初步洞察；如果需要更深入的分析（比如进一步做季节性拆解），技术用户可以把笔记本智能体（notebook agent）限定在这个项目范围内，和智能体一起继续规划、推理、生成图表，并在生成的“待处理变更”中逐条审核、决定保留哪些结果。分析由此变成一种可协作、可迭代、可沉淀的工作流，而不是一次性、不可解释的问答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从一次性对话到可复用资产&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果到这里为止，Hex 展示的是“可观察性”，那么 Armin 在后半段想讲的，是上下文如何变成系统能力，如何从一次性对话沉淀为可复用资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他先展示了一个从笔记本走向应用（app builder）的路径：当某些分析内容需要“持久化”，例如营销与销售负责人希望随时看到季节性分析或关键指标，而不是每次回来重新提问，那么就可以把笔记本中已经生成的图表、文本等资产拖拽到应用构建器里，做成一个仪表板、报告或更像 BI 的交互界面。这里的核心并不是“又做了一个 BI”，而是强调：即便呈现形态变成 BI 风格，背后依然由笔记本驱动，仍然保留 SQL、Python、Snowpark 等灵活性；同时，笔记本与应用这两种范式始终连接，资产是可回溯的。换句话说，展示层可以更友好，但底层逻辑并不会因此变成不可审计的黑箱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;紧接着他抛出了“连接胶水”的问题：当我们有线程、有笔记本、有应用，如何让它们构成一个一致的策略？答案是语义模型——它是 Armin 所谓“上下文引擎”的关键组成部分。原因也很务实：企业里那些精心构建的报表与仪表板，通常包含大量转化逻辑、业务口径、SQL/Python 查询，这些恰恰是 LLM 最需要、也最容易误解的上下文。如果不能把这些上下文结构化，LLM 的确定性就无从谈起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示里，语义模型有两条路：一是导入已有的 Snowflake semantic view。Hex 可以浏览生产仓库、发现可访问的语义视图，然后快速引入，例如引入一个 B2B sales model，让 enriched metadata 直接在 Hex 中可用。另一条路更贴近多数团队的起点：不是先有语义视图，而是先有一堆被业务反复使用的仪表板项目。Hex 的语义建模工作台里有一个“建模智能体”（modeling agent），它能理解 Hex 的语义建模能力，并且能针对某个具体项目（例如 sales and marketing dashboard）去阅读项目里包含的 SQL 单元格、DataFrame 操作、joins、函数与过滤条件，形成建模计划，做错误预防，推断表关系，把“项目里已经存在的业务逻辑”烘焙进语义模型中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一段其实回答了一个关键的企业问题：语义模型从哪来？它不一定需要从零凭空设计，它可以从企业已经在用的分析资产中被抽取、被规范、被版本化。建好之后，语义模型还能用一种“拖拽式”的方式被检查：你可以选择维度、度量，查看聚合、查看系统生成的 SQL，在发布之前把模型硬化到你满意的程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更进一步，Armin 也回应了“供应商锁定”的担忧。他明确表示，Hex 不希望用专有 YAML 把用户锁死，并提到两个方向：其一是和 Snowflake 等一起推动“开放语义交换”（Open Semantic Interchange），一个由约 18 家甚至更多公司组成的联盟，目标是让语义模型信息能在不同系统之间互换，以促进 LLM 采用并避免 vendor lock-in；其二是更近期开启“写回”能力，让在 Hex 中构建的语义模型可以写回到 semantic views 中，保证不同系统间“友好共存”。这些内容在分享里出现得很明确：终点不是锁定格式，而是让用户愿意因为体验与工作流而持续使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当语义模型准备好后，线程侧的使用方式也随之变化：你可以把对话线程限定为“只使用语义模型”，而不是访问整个生产数据仓库。Armin 强调，这会让系统随着时间更确定：当你不断硬化语义模型、补充上下文，它会越来越稳定、越来越可控。也正因此，他再次回到开场的观点：把精力放在构建上下文系统上，而不是试图用合成样例把原型聊天机器人测到“看起来准确”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规模化审计与上下文飞轮&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享的最后一部分，Armin 把问题推到最现实的规模化挑战上：当系统从一个人试用扩展到五十、一百个用户时，你如何监控它？你如何知道 LLM 系统到底在做什么，业务用户到底拿它解决什么问题？这时，“可审计”就不能停留在某个线程或某个项目，而必须成为一套能覆盖全局的治理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他提到 Hex 的“上下文工作室”（context studio），目前处于少数 Alpha 合作伙伴的 Alpha 阶段，但他之所以专门强调它，是因为它承载了上下文系统最关键的一环：理解使用行为，反过来指导上下文如何演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，你可以看到平台总体使用情况：用户更常用笔记本还是线程？创建了多少语义模型？也可以按对话量看用户分布，查看某个用户使用线程的频次、提问的类型。更重要的是，当你下钻到“问题类型”时，Armin 给出了一个很强的判断：这些真实问题才是你的单元测试。不是你在上线前试图一次性“破坏一切”并用评估集兜住，而是看清业务用户到底在问什么，再回去硬化你需要硬化的上下文与问题类型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕“如何策划上下文”，他在分享里给出了三个层次的抓手。最直接的是规则文件（rules file）：你可以在里面定义 SQL 的数据质量防护、业务定义、偏好的 SQL 风格、杂项信息，以及希望系统使用的可视化方式，并且这些内容可以即时编辑、保存或导出。第二层是“经认可的数据”（endorsed data）：由数据团队或所谓“金层”背书的数据资产，可以在 Hex 的语境下被定义清楚，决定哪些数据可以喂给 LLM。第三层则是更成熟、也最关键的做法：语义项目（semantic projects）。随着审计能力增强，你不仅能看到语义模型被使用的次数，还能观察是否有多个语义模型被同时使用、是否需要在某些场景中合并；你也能判断哪些项目最常被引用，从而决定是否需要对下游数据做更多建模，或者是否需要补充列描述、表描述等元数据来改善上下文质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些细节共同指向同一个结论：上下文不是一次性设计出来的，它是被真实使用不断“磨”出来的。你从稍微宽的范围起步，抽取一两个语义模型，让业务用户用起来；再通过审计看到真实问题与真实路径，回去修规则、补语义、加背书数据、完善元数据。如此循环，系统才会越来越确定、越来越可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享最有价值的地方，在于它没有把“可信”简化为一个指标，也没有把“准确率”当作唯一的归宿。Armin 反复强调的其实是另一套思维：企业要的不是一个在评估集上表现漂亮的聊天机器人，而是一套能持续吸收上下文、可审计、可治理的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从线程到笔记本的可观察性，从笔记本到应用的资产化，从项目到语义模型的上下文结构化，再到面向规模化使用的审计与上下文工作室——这些环节被串成一个整体，目的只有一个：让 LLM 在真实业务里变得更确定，并且在需求增长时仍然能保持可控与可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</link><guid isPermaLink="false">https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</guid><pubDate>Thu, 29 Jan 2026 09:02:56 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>Aspire 13.1带来了MCP集成、CLI增强和Azure部署更新</title><description>&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;作为增量更新发布，它基于Aspire 13引入的多语言平台基础。此次发布专注于通过增强命令行界面、更深入地支持AI辅助开发工作流程、改进仪表板体验以及更清晰的Azure环境部署行为来提高开发者的生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据团队报告，此次更新旨在使日常开发任务更可预测、更易于自动化，并与现代AI编码工具更好地对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aspire 13.1中的一个核心新增功能是通过与模型上下文协议（&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-mcp-for-ai-coding-agents&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;，MCP）集成，扩展了对AI编码智能体的支持。一个新的命令允许项目在初始化时支持MCP，使兼容的AI工具能够发现Aspire集成、检查应用程序结构并与运行中的资源交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;aspire mcp init&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接后，AI智能体可以查询应用程序状态、查看日志并通过暴露的端点检查跟踪。这种集成旨在简化开发过程中AI助手的使用，而无需为每个工具进行自定义设置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-cli-enhancements&quot;&gt;Aspire CLI&lt;/a&gt;&quot;进行了几次更新，旨在减少创建、运行和维护项目时的摩擦。如前所述，项目创建命令现在可以选择通道，并且一旦选择，将全局保持，确保新项目的行为一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CLI还能检测到已经运行的实例，并在启动新运行之前自动停止它们，从而避免常见的冲突。安装脚本现在支持一个选项来跳过修改系统 PATH，这在受控环境中非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布的仪&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-dashboard-improvements&quot;&gt;表板更新&lt;/a&gt;&quot;专注于清晰度和可见性。新的参数标签允许直接从资源详情中查看和管理配置值。GenAI可视化器已增强，以更好地显示工具定义、评估和相关日志，并支持预览音频和视频内容。仪表板的几个稳定性问题也得到了解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e063e5a2395fca657719db177ab3ec1c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（GenAI可视化器工具定义，来源：&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#genai-visualizer-enhancements&quot;&gt;官方Aspire文档&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-azure-improvements&quot;&gt;Azure&lt;/a&gt;&quot;改进方面，Aspire 13.1引入了更清晰的命名和更强大的验证。Azure Redis集成已重命名，以更好地匹配底层服务，并且在部署过程中更早地执行额外检查，以便尽早发现配置问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Azure资源现在暴露出标准化的连接属性，这些属性在支持的语言中通用，使得非.NET应用程序能够使用一致的设置进行连接。还增加了对Azure App Service中部署槽的支持和对默认角色分配的更精细控制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过引入通用容器注册表资源，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-container-and-docker-compose&quot;&gt;容器和部署&lt;/a&gt;&quot;工作流得到了改进，允许开发者锁定Azure容器注册表之外的注册表。容器镜像推送现在更加明确和可预测，特别是在部署到Azure容器应用时。Docker Compose支持已得到改进，以增强可移植性并减少并行构建期间的竞争条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还包括针对&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-javascript-and-frontend-support&quot;&gt;JavaScript和前端开发的更新&lt;/a&gt;&quot;，例如一个新的起始模板，该模板结合了ASP.NET Core后端和基于Vite的前端，改进了开发中的HTTPS处理，并修复了与包管理器相关的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;证书处理得到了简化，新增了配置HTTPS和在支持的容器中终止TLS的新API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Aspire 13.1还稳定了之前预览版中的几个集成，包括Dev Tunnels、端点代理支持和Azure Functions。模板已更新以反映一致的模式，并且广泛的错误修复集提高了跨平台的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;需要.NET 10 SDK或更高版本。建议从早期版本升级的开发者查看已记录的重大变更，特别是围绕Azure Redis API和重命名的连接属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于感兴趣的读者，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;完整的发布说明&lt;/a&gt;&quot;和详细文档可在&lt;a href=&quot;https://github.com/dotnet/aspire&quot;&gt;官方Aspire存储库&lt;/a&gt;&quot;和文档渠道中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&quot;&gt;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</link><guid isPermaLink="false">https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</guid><pubDate>Thu, 29 Jan 2026 09:00:00 GMT</pubDate><author>作者：Almir Vuk</author><category>微软</category><category>云计算</category></item><item><title>LangChain 创始人警告：2026 成为“Agent 工程”分水岭，传统软件公司的生存考验开始了</title><description>&lt;p&gt;过去几十年，软件工程有一个稳定不变的前提：系统的行为写在代码里。工程师读代码，就能推断系统在大多数场景下会怎么运行；测试、调试、上线，也都围绕“确定性”展开。但 Agent 的出现正在动摇这个前提：在 Agent 应用里，决定行为的不再只是代码，还有模型本身——一个在代码之外运行、带着非确定性的黑箱。你无法只靠读代码理解它，只能让它跑起来、看它在真实输入下做了什么，才知道系统“到底在干什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在播客中，LangChain 创始人 Harrison Chase 还把最近一波“能连续跑起来”的编程 Agent、Deep Research 等现象视为拐点，并判断这类“长任务 Agent”的落地会在 2025 年末到 2026 年进一步加速。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也把问题推到了台前：2026 被很多人视为“长任务 Agent 元年”，现有的软件公司还能不能熬过去？就像当年从 on-prem 走向云，并不是所有软件公司都成功转型一样，工程范式一旦变化，就会重新筛选参与者。长任务 Agent 更像“数字员工”——它不是多回合聊天那么简单，而是能在更长时间里持续执行、反复试错、不断自我修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这期与红杉资本的对话中，Harrison 抛出了一个判断：构建 Agent，已经不只是把软件开发“加一层 AI”，而是工程范式本身在变。为什么他说“光读代码不够了”？为什么 tracing、评估、记忆这些原本偏“辅助”的东西，突然变成主角？他在对话里给出了非常具体的解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而更现实的问题是：如果范式真的在变，那些靠数据、流程、产品形态建立壁垒的传统软件公司，优势还能不能延续？它们手里握着的数据与 API 可能依然是王牌，但能否把这些资产变成 Agent 时代的生产力，取决于一套全新的工程打法。Harrison 的观察与判断，都在下面的完整对话里：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：AI 领域的变化速度快得惊人。当前最受关注的话题，我觉得没有人比你更合适来聊。我们会先谈 长任务 Agent（Long Horizon Agents） 和 Agent Harness（智能体运行框架）。&lt;/p&gt;&lt;p&gt;接着，我们会讨论：构建长任务 Agent 与构建传统软件到底有什么不同，以及你如何看待 LangChain 在整个生态系统中的角色。最后，我想和你聊聊未来。你怎么看红杉资本这篇关于Long Horizon Agents的文章？哪些观点你认同，哪些地方你不太同意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/acbe71eff021a48fa8459ecebfaa8b62.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“在去年的一篇文章中，我们曾提出：推理模型（reasoning models）是 AI 领域最重要的新前沿。而“长任务 Agent”（long-horizon agents）则在这一范式之上更进一步——它们不只是思考，还能够采取行动，并在时间维度上不断迭代。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://sequoiacap.com/article/2026-this-is-agi/&quot;&gt;https://sequoiacap.com/article/2026-this-is-agi/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你们这个概念命名得非常好，那篇文章也写得很棒。我整体上是认同的——长任务 Agent 终于开始真正“跑起来”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始对 Agent 的设想，本来就是让一个 LLM 运行在一个循环里，自主决定接下来该做什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AutoGPT 本质上就是这个想法，这也是它当初能迅速走红、抓住那么多人想象力的原因：一个 LLM 在循环中运行，完全自主地决定行动。但当时的问题在于：模型还不够好，围绕模型的 scaffolding（支架）和 harness（框架）也不够成熟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这几年，模型本身变得更强了；与此同时，我们也逐渐搞清楚了，什么样的 harness 才是“好”的。于是现在，这套东西开始真正奏效了。最明显的例子是在编程领域，Agent 的突破首先发生在那里。之后，这种能力正在向其他领域扩散。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，你仍然需要告诉 Agent 你想让它做什么，它也需要配备合适的工具。但现在，它确实可以持续运行更长的时间，而且表现越来越稳定。所以，用“长时序”来描述这一类 Agent，我觉得非常贴切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最喜欢的长任务 Agent 案例有哪些？你觉得它们正在呈现出哪些形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：目前最成熟、我自己用得最多的，还是 编程 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再往外延一点，我觉得非常优秀的一类是 AI SRE。比如 Traversal（我记得它是一家红杉投资的公司），他们的 AI SRE 可以在更长的时间跨度内运行。再往抽象一点，其实这类 AI SRE 本质上属于“研究型 Agent”。比如：给它一个事故，它会去翻日志、分析上下文、追溯原因。研究任务本身非常适合 Agent，因为它们最终产出的往往是一个“初稿”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent 的问题在于：它们还达不到 99% 的可靠性，但它们可以在较长时间内完成大量工作。所以，只要你能把任务框定为：让 Agent 长时间运行，产出一个初步版本，由人来审阅，这在我看来就是目前长任务 Agent 最“杀手级”的应用形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程就是一个例子：你通常是提交 PR，而不是直接推到生产环境（当然，vibe coding 现在也在不断进步）。AI SRE 也是一样：结果会交给人来 review。报告生成也是如此：你不会直接发给所有用户，而是先看一遍、改一改。我们在金融领域也看到了大量这样的用法，这是一个非常大的研究机会。客服领域同样如此。最早的客服 Agent 主要是做“第一响应”：用户一发消息，马上给出回复，这类用法现在也做得很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在开始出现新的形态，比如 Klarna&amp;nbsp;这个产品：人类和 AI 协同工作。当第一层自动回复失败后，不是简单地转交给人工，而是让一个长任务 Agent 在后台运行，生成一份完整的事件报告，然后再交给人工客服处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里“agent”这个词在客服语境下会变得有点混乱，但核心逻辑是一致的。总结来说，这些应用的共同点是：先由 Agent 生成一个“初稿”，再由人类接管。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那么，“为什么是现在”？你觉得主要是因为模型本身变得足够强，还是因为人们在 harness 侧做了非常聪明的工程设计？在回答这个问题之前，能不能先帮听众梳理一下：在一个 Agent 系统中，模型、框架和 harness 各自扮演什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：当然可以。我也顺便把“框架”这个概念一起带进来。一开始，我们把 LangChain 描述为一个 Agent Framework，现在我们又推出了 Deep Agents，我更愿意称它为一个 Agent Harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人都会问，这两者有什么区别。模型很简单，就是 LLM：输入 token、输出 token。框架（Framework） 是围绕模型的一层抽象，让你更容易切换模型，封装工具、向量数据库、记忆等组件，本身比较“无偏好”，强调灵活性，更像是基础设施。Harness 则更“有主张”。以 Deep Agents 为例：我们默认就提供一个 规划工具（Planning Tool）；这个工具是直接内建在 harness 里的，带有明确的设计立场：我们认为这是“正确”的做法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还做了 上下文压缩（Compaction）。长任务 Agent 会运行很久，哪怕上下文窗口已经很大，也终究是有限的，总会有需要压缩的时候。怎么压缩？压缩什么？这是一个正在被大量研究的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，几乎所有 Agent Harness 都会提供文件系统交互能力，不管是直接操作，还是通过 bash。这一点其实很难和模型本身完全分开，因为模型训练数据里已经大量包含了这类操作。&lt;/p&gt;&lt;p&gt;如果回到两年前，我不确定我们是否能预见到：基于文件系统的 harness 会成为最优解之一。那时模型还没被充分训练过这些模式，而现在模型和 harness 是在一起“共同进化”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结来说，这是一个组合效应：模型本身确实在变强，推理模型带来了巨大提升。同时，我们也逐渐摸索出了 compaction、planning、文件系统工具等一整套关键原语。这两者缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;设计范式的演进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得在我们第一次对谈时，你把 LangGraph 描述为 Agent 的“认知架构”。现在来看，这是不是也可以理解为 harness 的一种形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的，这个理解是对的。我们现在的 Deep Agents 是构建在 LangGraph 之上的。可以把它看作是一个非常具体、非常有主张的 LangGraph 实例，更偏向通用目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期我们讨论过“通用架构”和“专用架构”的区别。现在我们观察到一个很有意思的变化：过去需要写进架构里的任务特异性，正在转移到工具和指令里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;复杂性并没有消失，只是从结构化代码，转移到了自然语言中。因此，prompt 的设计、修改，甚至自动更新，正在成为系统的一部分；而 harness 本身，反而变得更加稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在你看来，harness 工程中最难做对的是什么？你觉得单个公司是否真的有可能在这一层形成显著优势？有没有你特别佩服的团队？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：说实话，目前在 harness 工程上做得最好的，基本都是编程类公司。Claude Code 就是一个非常典型的例子。我认为它能如此受欢迎，很大程度上是因为它的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是否意味着：harness 更适合由模型公司来做，而不是第三方创业公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我不确定。比如 Factory、AMP 这些编程公司，也都做出了非常强的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实存在一个现实：harness 往往和模型家族绑定得比较紧密。不一定是某一个具体模型，而是一整个模型体系。Anthropic 的模型会针对某些工具进行微调，OpenAI 则针对另外一些。这和 prompt 类似：不同模型，需要不同的 prompt；同样，不同模型家族，也需要稍微不同的 harness。当然，它们也有很多共性，比如几乎都会使用文件系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我自己也没有一个确定答案。但一个很明显的现象是：几乎所有做编程 Agent 的公司，现在都在自研自己的 harness。你去看 Terminal Bench 2 这样的榜单，会发现他们不仅展示模型，还展示 harness。Claude Code 并不总是在榜首。这说明：性能差异并不完全来自模型，而来自对“模型如何在 harness 中工作”的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得，排行榜上表现最好的 harness，究竟在哪些地方做得特别好？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：首先是对模型训练偏好的理解。比如 OpenAI 的模型对 Bash 非常熟悉；Anthropic 提供了显式的文件编辑工具。顺着模型的“母语”来设计 harness，本身就能带来性能收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是 上下文压缩（Compaction）。随着任务时间跨度变长，如何处理上下文窗口溢出，已经成为一个核心问题。这显然也是 harness 的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还有 skills、子 Agent、MCP 等机制。目前这些能力还没有被系统性地训练进模型中，仍然属于比较新的探索方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们的 harness 中，一个典型挑战是：主 Agent 如何与子 Agent 高效通信。主模型需要把所有必要信息传递给子 Agent，同时还要明确告诉它：最终只需要返回一个“最终结果”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们见过一些失败案例：子 Agent 做了大量工作，最后却返回一句“请查看我上面的分析”，而主 Agent 根本看不到那些内容，于是完全不知道它在说什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，如何通过 prompt 设计让这些组件协同工作，是 harness 工程中非常重要的一部分。&lt;/p&gt;&lt;p&gt;如果你去看一些公开的 harness prompt，它们往往有几百行之长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我想从演进角度问一个问题。你一直站在模型“如何落地”的最前沿。如果用一种简化视角来看过去五年的几个关键拐点：ChatGPT 带来了预训练的拐点；o1 带来了推理能力的拐点； 最近，Claude Code + Opus 4.5 带来了长任务 Agent 的拐点。但从你这个“围绕模型做设计”的世界来看，拐点会不会是另一套划分？从认知架构到框架、再到 harness，这中间经历了哪些真正的跃迁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我大概会把它分成三个阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一阶段：最早期。那时 LangChain 刚刚出现，模型还是“纯文本输入、纯文本输出”，甚至还不是 chat 模型。没有工具调用，没有 reasoning，没有结构化输出。人们主要做的是单一 prompt 或简单 chain。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二阶段：工具与规划开始进入模型。模型开始支持 tool calling，也尝试学会“思考”和“规划”。虽然还不够强，但已经能做出基本决策。这时，人们大量使用自定义的认知架构，通过显式提问来引导模型行动，但整体仍然依赖大量外部 scaffolding。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三阶段：长任务 Agent 的真正起飞。大概是在今年 6～7 月，我们看到 Claude Code、Deep Research、Manus 等产品同时爆发。它们在底层使用的是同一个核心算法：让 LLM 在循环中运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的突破来自于 上下文工程：压缩、子 Agent、技能、记忆——所有这些，都是围绕上下文展开的。这正是我们开始做 Deep Agents 的时间点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于很多程序员来说，Opus 4.5 可能是一个心理上的分水岭。也可能只是碰巧遇上假期，大家回家开始大量使用 Claude Code，突然意识到：它真的很好用。无论是 2025 年初还是 2025 年末，总之在某个时间点，模型“刚好强到足以支撑这种形态”，于是我们从 scaffolding 迈向了 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent 是通用 AI 的终局形态吗&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：接下来会发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也希望我知道答案。这个“让 LLM 在循环中运行、让它自己决定要拉什么上下文进来”的算法，本身极其简单、也极其通用。这正是 Agent 从一开始的核心设想，而我们现在终于走到了“它真的能工作”的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，可能会有大量围绕上下文工程的技巧出现：有些手动设计的部分可能会消失；比如压缩类的，现在仍然高度依赖 harness 作者的决策。Anthropic 已经在尝试让模型自己决定何时压缩上下文，虽然目前用得还不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个我们非常关注的方向是 记忆（Memory）。从本质上说，记忆也是一种上下文工程，只不过是跨更长时间尺度的上下文。核心算法本身已经非常清晰：运行 LLM 循环。未来的进步，很可能来自更聪明的上下文工程方式，或者让模型自己参与上下文管理。模型当然也会继续变强，越来越擅长长时序任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我目前思考最多的一个问题是：我们看到的大多数 harness 都是高度偏向编程任务的。这是长任务 Agent 最先爆发的领域。但即便是在非编程任务中，你也可以认为：写代码本身是一种非常强的、通用的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我本来想问你：编程智能体（coding agents）到底算不算一个子类别？还是说编程智能体就是智能体本身？换句话说，智能体的工作，本质上是想办法让计算机去做一些有用的事情，而“写代码”本来就是让计算机做有用事情的一种很好的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也不确定。但有一点我非常非常坚信：现阶段只要你在做长时序智能体，你就必须给它文件系统的访问能力。因为文件系统在“上下文管理”方面能做的事情太多了。比如我们说 compaction（上下文压缩），一种策略是把内容总结掉，但把完整的消息都放进文件系统里，这样如果智能体后续需要回查，它还能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一种策略是，当你遇到很大的工具调用结果时，不要把全部内容都塞回模型上下文里；你可以把结果放进文件系统，然后让智能体需要的时候再去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这些操作，其实不一定需要真实的文件系统，也不一定要让它真的去写代码。我们有一个概念叫“虚拟文件系统”：它底层可能只是 Postgres 之类的存储，扩展性更强。当然，“真实代码”能做的事情，虚拟文件系统做不了。比如你没法在虚拟文件系统里直接运行代码。所以写脚本在很多场景下确实非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也认为编程智能体有潜力成为通用智能体，但我不确定这是否意味着“今天的编程智能体”就是通用智能体——如果你能理解我这句话。因为我觉得现在很多编程智能体还是为编程任务做了大量优化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以“一个通用智能体可能长得像编程智能体”，但反过来，“今天的编程智能体就是通用智能体”，这件事我并不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;传统软件面临的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们能不能转到另一个话题：构建长时序智能体和构建传统软件之间的差异？你能不能先描述一下“1.0 时代”的软件开发栈是什么样的，然后说说现在到底哪里不一样？我记得你在 X 上写过一篇很不错的文章，也许你可以总结一下核心结论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e1839d7404649a0013d16b7f7d6b4a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://x.com/hwchase17/status/2010044779225329688&quot;&gt;https://x.com/hwchase17/status/2010044779225329688&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我这段时间一直在反复想这个问题：我们经常说“做智能体和做软件是不同的”，而且很多人也同意。但问题是：到底哪里不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得很容易、也很偷懒地说“不同”，但“具体不同在哪里”才是关键。下面这些可能听起来很显然，但也许显然是好事，希望它们不太有争议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你在做传统软件时，所有逻辑都写在代码里，你能直接在软件代码中看到它。但当你在做智能体时，你的应用如何工作的“逻辑”，并不全部在代码里，其中很大一部分来自模型本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着：你不能只看代码，就判断智能体在某个具体场景下会做什么。你必须真的把它跑起来。而我认为，这就是最大的不同：我们引入了这种非确定性系统，它是一个黑箱，它在代码之外。我觉得这就是核心差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个直接后果是：为了弄清楚应用到底在做什么，你不能看代码，你必须看它在真实运行中做了什么。这也是为什么我们做的产品里，最受欢迎的之一是 LangSmith。LangSmith 的一个核心能力是 tracing（追踪/执行轨迹）。为什么 trace 这么受欢迎？因为它能把智能体每一步内部发生的事情都清清楚楚地展示出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这跟传统软件里的 trace 又不一样。传统软件里，你的系统在那边跑，它会吐出很多日志和事件；你通常是在出现错误时才去看，而且你不需要“每一步的全部细节”。而且本地开发时，你可能直接打个断点就够了；很多时候日志追踪是上线到生产环境后才会更重度开启。但在智能体里，人们从一开始就会用 trace 来理解“底层到底在发生什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它在智能体里的影响力，远大于在单一 LLM 应用里的影响力。因为在单一 LLM 应用里，如果模型回答得不好，你知道你的 prompt 是什么，也知道输入上下文是什么（由代码决定），然后你得到一个输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，它在循环中运行、不断重复。你并不知道第 14 步时上下文里到底有什么，因为前面 13 步可能会把任意东西拉进上下文。所以，“上下文工程（Context Engineering）”真的是一个非常好的词。我真希望这是我发明的。它几乎完美描述了我们在 LangChain 做的一切——只是当时我们并不知道这个术语已经存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;trace 的价值就在于：它能直接告诉你此时此刻上下文里到底有什么，这太重要了。那这又意味着什么？这意味着：对传统软件来说，“真相的来源（source of truth）”在代码里。但对智能体来说，真相来源变成了代码与 trace 的组合——而 trace 是你能看到真相的一部分地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，真相当然也“存在于模型的数百万参数里”，但你基本没法直接对参数做什么。所以现实上，trace 就成了你可以抓住的“事实载体”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，trace 也会成为你开始思考测试的地方。你仍然可以对 harness 的某些部分做单元测试，也可以离线做一些 unit test，但要获得真正的测试用例，你很可能需要用 trace 来构建。而且在智能体里，在线测试（online testing） 可能比传统软件更重要，因为行为不会在离线环境里完整显现出来，只有在真实世界输入驱动下、系统被真正使用时，行为才会“涌现”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也看到 trace 正在成为团队协作的中心：如果出了问题，不再是“去 GitHub 看代码”，而是“去看那条 trace”。我们在开源项目里也一样。有人说：“Deep Agents 这里跑偏了，发生了什么？”我们的第一反应是：“把 LangSmith trace 发给我们。”如果没有 trace，我们基本没法帮你 debug。过去大家会说“把代码给我看看”，但现在已经转变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是我写在 X 上那篇文章的核心内容，反馈很好。我也还在琢磨怎么把它表达得更精确，但我觉得这一点很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外一个点我也还在继续想：我觉得构建智能体是一个更偏迭代式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们过去也会这么说，但我以前会有点翻白眼，因为软件开发本来也是迭代式的：你发布、收反馈、不断迭代，这就是软件开发的常态。但我觉得差别在于：在传统软件里，你的迭代是围绕“你希望软件做什么”来进行的。你有一个想法，你发布，你收反馈。比如“这个按钮让人困惑”，或者“用户其实想做 X 而不是 Y”。但你在发布之前，其实你是知道软件会怎么运行的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，你在发布之前并不知道它到底会怎么做。你当然有一个预期，但你并不能在发布前真正确定它会做什么。因此，为了让它更准确、让它更“对”、让它能通过某种“概念上的单元测试”，你需要更多轮次的迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个基础上，我也认为记忆（memory）非常重要。因为记忆就是在从这些交互中学习。如果你的开发过程变得更迭代、更难，那么作为开发者，我为了让系统表现正确，可能需要反复改系统 prompt——这种频率甚至可能比我改代码还高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是记忆进入的地方：如果系统能够以某种方式自己学习，那就能减少开发者必须进行的迭代次数，让构建这类智能体变得更容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，这是我认为“构建智能体确实不同于构建软件”的另一个角度。我也承认，这么说有点老套，所以我一直在逼自己想清楚“到底不同在哪里”，目前我总结出来的就是这两点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我也很想追问这一点。现在公开市场上有一个很大的争论：现有的软件公司还能不能熬过去？如果类比当年从本地部署软件（on-prem）转向云（cloud），实际上真正成功转型的公司并不多，因为事实证明，“做云软件”和“做本地软件”确实差异很大。你现在处在“人们如何用 AI 构建产品”的核心地带。你怎么看这件事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不是要问公开市场的投资问题，而是想问：这个变化到底有多大？你有没有看到很多人：过去很擅长“旧方法做软件”，现在也能很擅长“新方法做软件”？还是说更像是：你要么在“新方法”里长大，要么就很难真正理解它？你觉得人能跨越这个鸿沟吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我注意到现在有很多年轻创始人，这让我觉得，也许年轻人因为没有太多对“旧软件开发方式”的先入之见，反而可以更快把这些东西学起来、用起来。而且我们确实一再听到一个现象：很多在做 agent engineering 的团队成员，反而是更初级的开发者、更初级的构建者——他们确实没有那些先入之见。我们内部的应用 AI 团队，确实整体更偏年轻一些。我觉得这里面既有“人的因素”，也有“公司的因素”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;先说公司层面：数据依然非常非常非常有价值。如果你从 harness 的角度去看——顺便说一句，我其实不认为长期来看大多数人都会自己去写 harness，因为它比做 framework 难太多了。所以我觉得大家最终会用我们提供的 harness，或者用别人的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那一个 harness 里面有什么？主要就是：prompt、指令，以及它连接的工具。而现有公司在这方面最大的资产之一，是他们已经拥有数据和 API。如果你过去在这块做得不错，那么把这些东西接入到 agent 上，其实会非常容易产生真实价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们前阵子和金融行业的人聊，他们就说：数据的价值只会越来越高、越来越高、越来越高。所以如果你是一个传统软件厂商，你手上有这些高价值数据，你应该能够把它暴露给智能体，让智能体去用，从中拿到很大的收益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过这里还有另一部分：关于“如何使用这些数据”的指令（instructions），这一块可能更偏“新增”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你作为软件厂商也许一直对“怎么用这些数据”有一些想法，但你并没有把这些想法系统化、固化成可执行的“操作说明”，因为过去这件事更多是由人来完成的——很多智能体现在在做的事情，本来就是人类会做的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你当然会给人配工具，但你以前不会、或者也很难成功地把它完全自动化。而到了“智能体”这一代，这部分才真正变得可行。所以我觉得这块是新的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们也看到大量需求来自“垂直领域创业公司”。Rogo 就是一个很好的例子：他们团队有人有金融行业经验，把这种行业知识带进了智能体系统里，而这之所以有效，是因为很多智能体的驱动力来自“知识”——但不是那种通用世界知识，而是如何执行特定流程、特定模式的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以问题就变成：做传统软件的人是不是做智能体的合适人选？我觉得我们确实看到很多非常资深的开发者在采用 agentic coding，所以某种程度上这更像是“心态问题”。但确实也可能会呈现出一种“年轻化倾向”。而公司层面，则很大程度取决于它手上的数据资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以看起来，你认为 trace 是这个新世界里 agent 开发的核心“产物”，LangSmith 在这方面帮助很大。那你觉得还有哪些核心的“产物”——或者说，可能“产物”这个词不对，应该说组件（components）？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，组件。我觉得构建软件与构建智能体之间另一个差异是：评估软件时，你可以相当可靠地依赖程序化测试和断言。但智能体做的很多事情，本质上是“人类会做的事情”。因此要评估它，你必须把人的判断引入进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我们在 LangSmith 里努力解决的问题之一：你已经有了这些 traces，那么你怎么把人类判断带到 traces 上？最直接的方法当然就是：把人引进来。所以我们也看到数据标注类创业公司做得很好。我们在 LangSmith 里有一个概念叫 annotation queues（标注队列），就是把人带进来参与。因此，实际的、真实的人类判断，是其中非常重要的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这里的“人工标注”的trace，比如，智能体做了这些步骤，这是好还是不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：有时候，人会给自然语言反馈：这很好、这很差、这里应该怎么做。有时候，人会直接“纠正它”：把正确步骤完整地写出来。这具体怎么做取决于用例，而且对做 RL 的模型公司，和对做 agent 应用的公司来说，也可能不一样。但核心就是：把人类判断带进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也看到另一条路：尝试为这种人类判断建立一些“代理指标”（proxy）。这就是 LLM-as-a-Judge 这类方法的来源：你可以跑一个 LLM 或其他模型，让它承担某种“类似人类判断”的角色，去给那些本来需要人类判断的东西打分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们一直在思考的一件事是：怎么让“构建 judge”这件事变得容易。因为 judge 的关键很大一部分在于：它必须和你的人的判断、人类偏好保持一致。如果做不到，那你的 grader（评分器）就很糟糕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们在 LangSmith 里做了一个概念叫 align evals：人类先去标注一些 traces，然后基于这些标注，构建一个 LLM judge，使它在这些样本上被校准（calibrated）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为关键就在于：你要把人类判断引入进来；如果你要用 proxy 来替代它，那就必须确保这个 proxy 校准得足够好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有意思。我记得我们最开始和你做业务合作的时候，还在邮件里讨论过：LLM-as-a-Judge 到底是否可行。看起来它已经进步很多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的。LM-as-a-Judge 其实有几个不同层面的用法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最常见的一种，是用于 eval：拿一条 trace，直接给它一个分数，比如 1 到 0，或者 0 到 10。这个我认为是可行的，而且很多人确实在做。他们会离线做，也会在线做，因为有些判断并不需要 ground truth（标准答案）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我觉得另外一个更重要的场景，是你在 coding agents 里也能看到的：coding agent 往往会先工作到某一步，然后遇到错误，触发纠错。它实际上是在“评判自己刚才做的工作”。我们也在 memory 上看到同样的模式：记忆很大一部分就是反思 traces，然后更新某些东西。所以问题是：LLM 能不能去反思 traces——无论是它自己的 trace、以前 session 的 trace，还是别人的 trace？我觉得完全可以。我们在 eval、纠错、记忆里到处都能看到这种模式，本质上其实是一回事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Eval是 RL 的奖励信号，还是工程反馈机制？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我明白了。那接下来就很自然会问：你有了所有 traces，也有了 eval。那么这些 eval 到底是什么？它是强化学习的 reward signal？还是一种反馈机制，让工程师去改进 harness、让 agent 工程师去优化 harness？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：因为现在大家都不再手动写太多代码了，大家都在用这些 agent 工具。我们观察到一个很重要的模式：我们有一个 LangSmith MCP，也有 LangSmith fetch（一个 CLI）。因为 coding agents 特别擅长用 CLI。你把这些给智能体，它就能把 traces 拉下来，诊断哪里出了问题，然后把这些 traces 带进代码库里，从而修复它。这是我们正在看到的真实模式，而且我们非常非常非常想支持这种模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在这一点上，相比“用 eval 做强化学习奖励信号”，我对“把 eval 当作工程反馈、用于改 harness”的路径更乐观——至少对今天做 agent 应用的公司来说是这样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这听起来像是递归自我改进啊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得是，但还是有一个人类在环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到前面那个点：当它产出“初稿”时效果最好——它改 prompt，然后人类 review，这能让系统保持不跑偏。但我们确实……我们最近发布了 LangSmith Agent Builder，这是一个 no-code 的 agent 构建方式。其中一个很酷的功能就是 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在 memory 的工作方式是这样的：当你和 agent 交互时（注意它还不是后台自动跑的那种；它不会自己拉 traces），如果你对它说：“你不该做 X，你应该做 Y”，它就会去改自己的指令——这些指令本质上就是文件——然后直接编辑这些文件。这样未来它就会按新的方式表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是一种“自我改进”的形式。我们确实还想加入另一种机制：比如每天晚上跑一次任务，查看当天所有 traces，更新自己的指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：就是那种“做梦”的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，“睡眠时间算力（sleep-time compute）”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;记忆与自我改进会成为护城河吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们再多聊聊未来。你现在最兴奋的是什么？听起来你聊了很多 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我很看好memory。我觉得让智能体去改善自己，这非常酷，而且在很多场景下也很有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但也不是所有场景都用得上。比如 ChatGPT 加了 memory 功能，我其实用得不多，我也不觉得它显著增加了我对产品的粘性。我觉得原因之一是：我去 ChatGPT 时，大多数问题都是一次性的。我不太会反复做同一件事：我可能问软件，也可能问吃的、旅行……都很零散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 agent builder 里，你通常是为特定任务构建特定工作流。比如我有一个 email agent。而且我其实……它已经给我发邮件两年了。我之前在 agent builder 之外就有一个 email agent，它带有 memory。后来我们做了 agent builder，我想把它迁移进去，但它没有我之前的那些 memories。即便它的起始 prompt 一样、工具也一样，但因为缺了记忆，它现在的体验就明显差很多。我到现在都还没完全切过去，因为它现在确实不如之前那个好用——说白了，它现在“有点烂”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，如果我持续和它交互，它会变好，它会不那么烂。但这也恰恰说明：memory 可能会成为真正的护城河（moat）。而且我绝对相信，我们已经到了一个阶段：LLM 可以看 traces，然后改变自己代码里的某些东西。问题在于：怎么把这件事做得安全、并且在用户层面可接受。但我认为，在一些特定场景里（不是所有场景），我们会越来越多看到这种能力。至于 ChatGPT 这种通用聊天产品，我仍然不确定这种形态的 memory 是否有用，至少目前我不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你觉得和长时序智能体一起工作的 UI 会如何演化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得大概率需要同步模式（sync）和异步模式（async）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长时序智能体运行时间可能很长，默认应该是异步管理：如果它要跑一天，你不会一直坐在那里等它结束。你很可能会启动一个、再启动一个、同时跑很多个。所以这里会涉及到异步管理：我觉得像 Linear、Jira、看板，甚至 email，都可以作为 UI 设计的参考——如何去管理一堆异步运行的 agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但与此同时，很多时候你又会想切换到同步交流。因为 agent 最后给你返回一份研究报告，你可能需要立刻指出：它这里写错了，你要给反馈。聊天界面在这方面其实已经挺不错的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我唯一想补充的是：现在很多 agent 不仅是在“对话”，它还会去修改文件系统里的文件。所以你必须有一种方式去查看“状态”（state）——也就是它改了什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这在编程领域尤其明显：IDE 依然被使用，是因为当你想手动改代码时，你需要看见那个“当前状态”。即便我启动 Claude Code，它跑完后，我有时也会打开来看它到底写了什么代码。所以“能看到状态”这件事很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 在 Claude “co-work”（这里指那类协作式工作流）里做了一个很酷的设计：你设置它时要选择一个目录，等于你在告诉它：“这就是你的环境。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在编程里当然也是常态：你打开 IDE 到某个目录。但我觉得把它明确成一个心智模型很有帮助：这就是你的 workspace（工作区）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个 workspace 也不一定非得是本地目录：它可以是 Google Drive、Notion 页面，或者任何能存储状态的地方。你和 agent 就是在这个状态上协作：你启动它，让多个任务异步跑；然后切到同步模式，在 chat 里和它讨论，但同时你还能看到它正在协作的“状态”。这就是我目前看到的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以这也就是你说的“agent inbox”的想法：为了进入 sync 模式，agent 需要能联系到你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，没错。我们大概一年前发布过 agent inbox，理念是“ambient agents”：它们在后台跑，必要时来 ping 你。但第一版其实没有 sync 模式：它 ping 你，你回一句，然后你就等它下一次再 ping 你。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但很多时候，我切到邮件去回复它时，我其实只回很短的话，而且我不想再切出去然后干等——我（对方）很重要，所以我更想直接进入一种“同步对话”的模式，跟 agent 把这个问题当场聊完。所以我们后来做了一个关键改动：当你打开 inbox 时，会直接进入 chat，而 chat 是非常同步的。这是一个很大的 unlock（突破点）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在认为：只有 async 模式，目前还不太够。也许未来如果 agent 强到你几乎不用纠正它，那么纯异步会更可行。但至少现在，我们看到人们在 async 和 sync 之间来回切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 你怎么看 code sandboxes（代码沙箱）？是不是每个 agent 最终都会配一个 sandbox？也包括“能用电脑”、能上网用浏览器这种能力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase： 这是个特别好的问题，我们也一直在想。就目前的经验来看，“写代码/跑代码”这条路明显比“直接操作浏览器”更成熟、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以短期内，如果要在这些能力里挑一个最可能成为标配的，我更看好的是代码执行（code execution）——也就是给 agent 一个能安全运行脚本、验证结果的环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，文件系统（file system）我几乎是“坚定派”：不管是本地目录、还是背后用数据库实现的“虚拟文件系统”，agent 总得有个地方能存状态、存中间结果、随时回查，这对上下文管理太关键了。比如：&lt;/p&gt;&lt;p&gt;做 compaction（上下文压缩）时，把完整内容丢到文件里，需要再查就去读；工具调用返回特别长时，不塞进上下文，改成写文件、让 agent 自己按需读取。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于“coding”（让 agent 真正去写代码），我没那么绝对，但我大概90% 站在“需要”这一边。因为很多长尾任务里，写脚本依然是最通用、最强的手段——你很难找到同等级的替代品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也可能出现另一类场景：如果你做的是高度重复、流程固定的事情，未必每次都要写很多代码；但即使这样，文件系统仍然重要，因为重复流程会不断产生上下文和状态，你还是要做上下文工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再说浏览器使用（browser use）：从我们目前看到的效果来说，模型还不够稳定。也许可以让 coding agent 通过 CLI 的方式“间接”完成一些浏览器相关任务（算是一种近似解），我确实见过一些挺酷的实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而所谓 computer use（直接操作电脑界面）则更像是介于两者之间的混合形态，目前还有不少不确定性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结一下：我非常喜欢 code sandboxes，我觉得它会成为 agent 能力栈里很关键的一块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太棒了。Harrison，真的非常感谢你今天来参加节目。你一直都能在 agent 这条路上看到未来，能和你聊“上下文工程如何演化到今天的 harness 与长时序智能体”，真的特别过瘾。感谢你推动这个未来，也感谢你一直愿意和我们聊这些。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：谢谢邀请。我希望未来还能再来一次，然后证明我今天说的全部都是错的。因为预测未来真的很难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=vtugjs2chdA&amp;amp;t=1s&lt;/p&gt;</description><link>https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</link><guid isPermaLink="false">https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</guid><pubDate>Thu, 29 Jan 2026 08:18:49 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>Linus 之后的 Linux？内核社区终于写下“接班预案”</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/945c92e1d75cd68d349118286393a8f5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Linus Torvalds 常开玩笑说自己会“活到永远”。但以防万一，Linux 内核社区现在也准备好了一套交接方案——只是这份方案并没有点名具体的接班人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果 Torvalds 发生意外，或者哪天决定退休，Linux 不再把一切寄托在“到时候再说”。核心内核社区已经正式起草了一份项目连续性计划：一旦顶层维护者出现空缺，应该如何在最坏情况或有序过渡中，选出新的顶层维护者（可能是一人，也可能是多人），确保项目长期稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 本人则明确表示自己暂无退休打算。被问到未来是否会交棒时，他依旧以一贯的幽默回应，暗示自己更倾向于“继续干下去”。随后他又补充了一个更现实的理由：家里人同样不希望他突然闲下来，尤其是太太，大概更不想每天被一个无所事事、没事找事的丈夫缠着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这份新的“为计划而写的计划”由资深内核贡献者 Dan Williams 起草，并在最近于东京举行的 Linux Kernel Maintainer Summit 上讨论。Williams 介绍它时还自嘲：这是个“与我们终将走向死亡相关、但很振奋的话题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不指定唯一继承人&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 也解释了这次为何会把“接班”议题正式摆上台面：部分原因是他此前与 Linux 基金会的合同在去年第三季度到期，基金会技术顾问委员会的人都知情。虽然合同随后已续签，但这段时间确实促使大家把风险管理讨论得更具体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;计划并没有给出一个“唯一继承人”。相反，它明确了一套选择流程：一旦需要交接，由社区召集一次类似“秘密会议”的讨论机制，集中权衡候选人或候选团队，尽量做出对项目长期健康最有利的决定。有维护者开玩笑说，干脆学选教皇：把人都锁在房间里，等决定出来再放出一缕白烟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文件提到一个开源圈常说的“公交车系数”（bus factor）梗：假设项目的关键人物哪天突然“消失”（比如出了意外），项目还能不能照常运转？因为 Torvalds 仍是顶层合并与发布的最终把关人，所以从风险角度看，Linux 在这一环节几乎等同于“系数为 1”——也就是关键节点过度依赖一个人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过在现实中，大家也大致心照不宣：真要临时接手，“企鹅之王”的角色多半会落到 Greg Kroah-Hartman 身上——他是 Linux 内核稳定分支的维护者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 还在 2024 年和好友 Dirk Hohndel（Verizon Open Source 负责人）聊过这个话题。Hohndel 认为，要成为 Linux 的主维护者，需要极其丰富的经验；而目前最自然的“备份选项”就是 Greg Kroah-Hartman。Torvalds 的看法则更偏向长期视角：关键不在于某个人，而在于谁能获得社区的信任；这种信任通常来自长期参与、稳定协作，以及社区对其工作方式的充分了解，但“资历够久”并不意味着必须三十年如一日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kroah-Hartman 也确实曾短暂顶上过。2018 年 Torvalds 一度暂离内核工作、反思并改善自己对待其他开发者和维护者的方式时，Kroah-Hartman 曾临时承担顶层职责。不过，他的年龄甚至比 Torvalds 还大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;或许会由多人共同接棒&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此也有人提出，与其再找一位新的“终身仁慈独裁者”（BDFL），不如把顶层维护者的职责拆分给多位值得信赖的开发者共同承担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;56 岁的 Torvalds 仍然是几乎所有进入 torvalds/linux.git 变更的最终裁决者。他常自嘲 Linux 的核心圈子正在“变老”。而维护者疲劳、以及核心子系统负责人后继乏人等问题，让这种紧迫感越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以确定的是：Torvalds 并不会在短期内让位。他仍会继续监督主线开发，并一直做到自己“做不动”为止。只是至少现在，那个终极的“Linus 依赖”风险终于有了明确的处理流程——等到真正需要的那一天，可以直接套用，而不必临时抱佛脚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&quot;&gt;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</link><guid isPermaLink="false">https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</guid><pubDate>Thu, 29 Jan 2026 08:12:34 GMT</pubDate><author>Tina</author><category>开源</category></item><item><title>不跟英伟达走老路，这家GPU公司的技术架构藏着哪些关键解？</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;采访嘉宾 | 天数智芯 AI 与加速计算技术负责人 单天逸&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于国产 GPU 行业来说，没有哪个时间节点比当下更宝贵。在政策支持硬科技企业上市的背景下，国产 GPU 迎来了难得的上市黄金窗口期。但上市并非终点，在敲钟的那一刻，下一战场大幕已经拉开——GPU 厂商的技术路线、产品能力和长期判断，被放到了更公开也更严苛的舞台上，谁能撑起资本市场和大众期待，谁就能撑起市值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么，天数智芯上市后的首场发布会能够在业内形成广泛讨论。它以极其务实的工程师表达方式，把架构放回到国产 GPU 技术叙事的中心。在 1 月 26 日召开的天数智芯“智启芯程”合作伙伴大会中，围绕架构层的创新与思考占据了相当比重。基于这些创新点与思考，天数智芯公布了过去一代以及未来三代的架构路线图：&lt;/p&gt;&lt;p&gt;2025 年，天数天枢架构已经超越英伟达 Hopper，在 DeepSeek V3 场景中实测性能数据超出 20%；2026 年，天数天璇架构对标 Blackwell，新增 ixFP4 精度支持；2026 年，天数天玑架构超越 Blackwell，覆盖全场景 AI/加速计算；2027 年，天数天权架构超越 Rubin，支持更多精度与创新设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/93a5511a47ea59c34947fa5622e43f57.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;国产 GPU，开启 AI++ 计算新范式&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据天数智芯公布的架构路线图及阶段发展目标，在 2027 年之前，天数智芯将通过多代产品完成对英伟达的追赶；在 2027 年之后，将转向更富创新性的架构设计，聚焦更具突破性的超级计算芯片架构设计。看似宏大，但对于仍处于爬坡阶段的国产 GPU 行业来说，这条路径实际上相当务实——只有在工程化能力上完成对标甚至是超越，国产 GPU 才有资格进入更大规模的生产环境中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在规模化落地阶段的竞争，焦点早已从峰值性能指标转向有效计算能力。当 Token 成为 AI 时代最基本的生产资料，当算力消耗开始对标真实业务产出，无论是国际顶尖 GPU 厂商还是国内 GPU 企业，核心命题都只有一个：如何在真实业务中，把算力转化为有效的 Token。这似乎又将大家都拉到同一起跑线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一命题，天数智芯提出了两条明确的架构判断：其一，回归计算本质；其二，提供高质量算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;回归计算本质，核心在于“不设限”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年，规模的快速扩张带来了阶段性的产业繁荣，也使得算力实现野蛮增长。但这种粗放式发展，也带来了能效比失衡、算力资源严重浪费等问题。背后的根因十分复杂。以开车行驶为例，路途中可能会遇到雨雪冰雹天气、崎岖道路等各种复杂情况。物理、芯片、系统世界也是如此，计算、通讯、存储都会带来各种障碍。所以，幻想奔跑在平坦的赛道上毫无意义，产业真正需要的，是能够翻山越岭的全能越野车。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64709d562cae987749119744a750d556.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;广义上，芯片可分为专用芯片和通用芯片：专用芯片类似“应试教育”，它的优势和边界都很清晰，能加速特定算法、特定指令，比如矩阵乘法、Softmax 这些主流任务，但一旦计算范式发生变化，适应空间就会迅速收紧；通用芯片的设计哲学，不是为了押中某一类算法，而是回归计算本质，覆盖更广泛，甚至全新的计算需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是天数智芯坚持推出并量产通用 GPU 的根因。在其看来，硬件与算法的关系本来就不应该相互掣肘，算力的僵化不应限制算法的进化，而是通过通用算力为探索未知算法提供一个坚实的底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑探索未来算法的关键，实则就是“不设限”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，天数智芯的芯片设计哲学，在计算层面追求的是覆盖几乎所有的数学运算图谱，而非某一类、某一种计算：从 Scalar、Vector、Tensor 到 Cube，支持从高精度科学计算到 AI 精度计算，从 MMA 到 DPX，不管是 AI 的 Attention 机制、前沿的科学计算，还是未来的量子计算相关模拟，天数智芯全都支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行层面，追求的是更高的算力利用率：大、中、小任务会被精准分配到不同的计算单元中执行，配合高密度的多任务核心设计，算力可以被拆解、调度得更加精细，从而减少算力浪费，提高计算效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d21f6e06061719cd4a9a199eb6e5fed0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“不设限”的设计哲学，让天数天枢架构得以实现三大创新，这也是天枢能够超越英伟达 Hopper 架构的根因：&lt;/p&gt;&lt;p&gt;TPC BroadCast（计算组广播机制）设计：不是简单粗暴地放大带宽，而是从单位带宽的使用效率入手，存在相同地址的数据时，芯片内部的 load store 单元不会进行重复、无用的访问，而是在上游进行 BroadCast，减少不必要的内存访问次数，从而有效降低访存功耗，等效提升访存带宽，用更小的功耗和面积实现相同的功能。Instruction Co-Exec（多指令并行处理系统）设计：在指令执行层面，通过 Instruction Co-Exec 设计实现了多种指令类型的并行执行能力，不仅支持 Tensor Core 与 Vector Core 的并行协同，还将 Exponent 计算、通信等操作一并纳入统一调度。在天数 IX-Scheduler 模块中，通过极低的成本增强了不同指令之间的并行处理能力，无论是 MLA、Engram，还是面向更复杂模型场景的计算需求，都可以在这一并行框架下被同时处理，从而提升整体执行效率。Dynamic Warp Scheduling（动态线程组调度系统）设计：随着 MoE 架构在大模型中被广泛采用，模型厂商普遍面临推理效率低等现实挑战。为提升并行度，微架构层面允许芯片中同时驻留更多 warp，但 warp 的增加也意味着对计算资源的竞争更为激烈。为此，天数智芯首创了 Dynamic Warp Scheduling 机制，通过动态调度让不同 warp 在资源使用上实现有序协作，避免计算资源闲置，也减少了对同一资源的无序争抢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三项设计的出发点本质上都指向相同的目标：高性能与高效率。数据显示，这些创新让天数天枢的效率较当前行业平均水平提升 60%，基于这些效率优势，实现在DeepSeek V3 场景平均比 Hopper 架构高约 20% 性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这三项设计中可以看出，天数智芯在架构层面的创新，并不是围绕某一个具体模型或算子展开，而是试图打破 GPU 通用范式边界。天数智芯 AI 与加速计算技术负责人单天逸在接受采访时表示，在天数智芯提出 Dynamic Warp Scheduling 设计之前，几乎没有人从调度机制的角度去思考，还能为 MoE 带来哪些性能空间。从更深层次意义来看，这类微架构层面的调度和优化，一直是英伟达、AMD 等巨头保持领先的“内功”，天数智芯在这些单点上的突破，实际上也是国产 GPU 向顶级玩家看齐的重要一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;提供高质量算力：高效率、可预期、可持续&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在天数智芯的架构语境中，回归计算本质并不是一个抽象的口号，而是实现高质量算力的前提条件。只有当 GPU 从底层开始真正对计算负责，高质量算力才成为可能。基于这一判断，天数智芯将高质量算力拆解为三个核心维度：高效率、可预期与可持续。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7ace836faccd159427d7c71b330df3ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高效率意味着能为客户创造最优的 TCO（总体拥有成本），节省使用成本；可预期则通过精准的仿真模拟，让客户在拿到芯片、部署算力之前，就能清晰预判最终的性能表现，做到所见即所得；可持续指的是从现在主流的 CNN、RNN，到当下火热的 Transformer，再到未来还未诞生的全新算法，算力始终能无缝适配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这三个方向，天数智芯在架构及系统设计上，选择从多任务并行处理、长上下文 IX-Attention 模块、IX-SIMU 全栈软件仿真系统以及 IXAI++ 算力系统多个层面同步推进。这几项，其实哪个都值得单独展开探讨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，基于“不设限”的设计理念，在当前 PD 分离的架构下，天数智芯的 GPU 不只做计算，还支撑通信、KV 数据传输这些关键任务，通过打造 Ⅸ 并行任务处理模块，GPU 能精准调度 KV 传输、多路多流、计算与通信等各类任务，让它们并行不冲突。在真实业务场景中，该模块成功帮助头部互联网客户实现了端到端 30% 的性能跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b473b037acf6e72c5b667dd838e26e33.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了提高算力可持续性，天数智芯统一了芯片内、外，来构建算力系统，并通过不断更新的软件栈和软件系统，三类库共同支持和保障多场景的高效运行。其中，AI 库、通讯库（ixccl）、加速计算库是基石，在基石之上，直接支撑各类神经网络模型CNN、Transformer、LSTM 与高性能计算的各个领域，并以此提供各类 AI 应用，包括支持 AI4Sci 的相关应用，如蛋白质结构预测（AlphaFold）、医疗影像分析（Clara）、气候模拟（Earth2）等，以及量子计算的平台 cudaQ、分子动力学 Gromacs，大规模方程组求解器 HPL 等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套算力系统被命名为 IXAI++，寓意为自我迭代，不止于 AI。其最终的目标是，成为一座连接算法创新与物理世界的桥梁，带领人类科技通往未知探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但给业内带来最多惊喜的，是 IX-Attention 模块和 IX-SIMU 全栈软件仿真系统。前者解决的是当前大模型推理中最具代表性的效率难题，后者解决的是企业部署算力系统最头疼的不可控难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型推理场景中，长上下文被普遍认为是最具代表性的效率难题之一。即便是在国际主流 GPU 架构上，Attention 的执行效率依然不高，如果不对其进行针对性优化，首字延迟将明显偏高，模型响应速度差，推理成本高昂，最终影响大模型在真实业务中的可用性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一痛点，天数智芯设计了 Ⅸ Attention 模块，从底层对 Attention 的执行路径进行重构：Attention 底层涉及 exponent、reduce、MMA、atomic 等多类指令与算子，Ⅸ Attention 模块的核心思路，是将这些分散的组件有机地拼装到一起，如同指挥一支乐队一般，确保多种乐器能够和谐共鸣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/1931cf31a98c3a57d61ca5cbaf8caa44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“其中的技术难点在于调度，多种乐器需要同时演奏，任何一个环节拖慢节奏，都会成为整个系统的瓶颈”，单天逸表示，在实际的长上下文推理中，Ⅸ Attention 模块有效改善了 Attention 的执行效率，带来了约 20% 的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对企业部署算力系统最头疼的不可控难题，天数智芯搭建了 IX-SIMU 全栈软件仿真系统，这套仿真系统的目标，就是零意外、可预期。通过对芯片等硬件与软件执行策略的联合仿真，能精准输出任意模型的性能表现，提升算力在真实场景中的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c15d79ed9346cf49898ee0193b585926.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单天逸表示，在算力系统的仿真与评估中，最难建模的是指令级别的硬件行为。IX-SIMU 的核心能力在于，能够对底层指令执行进行精细建模。在实际使用中，用户只需输入软件代码，IX-SIMU 便会自动整合 GPU、CPU、网卡、PCIe 等硬件组件，匹配网络拓扑，再结合软件策略、投机策略、Streaming LLM 策略、前缀匹配等各类策略，最终精准输出 Deepseek、千问等任意模型的性能表现，实现从单卡到万卡集群的 “精密扩展”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕高效率、可预期、可持续三大判断，天数智芯在算力侧从硬件架构到系统设计进行了整体布局，并用未来三代架构路线图提前回答下一个问题：当算力僵化开始掣肘未来计算，架构层还能怎么演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;决定上限的，最终还是应用和生态&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构代表的其实是下限，决定上限的，最终还是应用和生态。数据显示，截至 2025 年年底，天数产品已在互联网、大模型、金融、医疗、教育、交通等超过 20 个行业落地应用，服务客户数量超过 300 家，并通过软硬件协同优化，完成 1000+ 次模型部署，让产品能力真正达到商用级别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑这些场景应用的，早已不是一个产品的能力范畴，而是“产品 + 解决方案” 双轨模式，这一模式其实与英伟达定位非常相近，聚焦的都是解决方案落地。在大模型深入产业应用的当下，这套组合打法相当务实，毕竟应用落地才是唯一真理，谁能在企业真实业务场景中快速部署、持续稳定运行，谁就能赢得先机。在速度和兼容性上，天数智芯也交出了一份不错的答卷：国内新的大模型发布当天便能跑通，目前已稳定运行 400 余种模型、数千个已有算子与 100 余种定制算子，数千卡集群稳定运行超 1000 天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次发布会上，天数智芯面向物理 AI 场景落地，一口气发布了四款边端算力产品“彤央”系列：包括边端 AI 算力模组 TY1000、TY1100，以及边端 AI 算力终端 TY1100_NX、TY1200。 据了解，“彤央”系列产品的标称算力均为实测稠密算力，覆盖 100T 到 300T 范围。数据显示，在计算机视觉、自然语言处理、DeepSeek 32B 大语言模型、具身智能 VLA 模型及世界模型等多个场景的实测中，彤央 TY1000 的性能全面优于英伟达 AGX Orin。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在发布会中，天数智芯展示了“彤央”系列产品在具身智能、工业智能、商业智能和交通智能四大边端核心领域的落地应用：具身智能领域，为格蓝若机器人提供高算力、低延迟的“大脑”支撑；在工业智能领域，落地园区与产线，推动产线自动化升级；在商业智能领域，瑞幸咖啡数千家门店部署彤央方案，高效处理视频流、挖掘消费数据价值；在交通智能领域，与“车路云一体化”20 个头部试点城市合作，验证车路协同方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，天数智芯走的路线虽然是底层技术自研，但在生态上并非封闭。在生态建设上，天数智芯与硬件厂商、解决方案提供商等多家生态伙伴签署战略合作协议，进一步完善国产 AI 算力生态闭环。通过兼容主流开发生态，持续开放底层能力，降低开发者迁移和使用门槛。未来，天数智芯还会持续增加在生态共建上的资本与人力投入，从应用到芯片与开发者一同优化 AI 应用系统，共同为应用落地提供性能、性价比与生态易用的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从底层架构到产品，从应用到生态，国产算力正在实现完整闭环，这种从芯片到生态的协同能力，不仅让国产算力更可用、更可持续，也为行业探索新模式提供了更多想象空间。&lt;/p&gt;</description><link>https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</link><guid isPermaLink="false">https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</guid><pubDate>Thu, 29 Jan 2026 06:54:56 GMT</pubDate><author>凌敏</author><category>企业动态</category><category>芯片&amp;算力</category></item><item><title>智源多模态大模型登Nature，确立自回归成为生成式人工智能统一路线</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月28日，智源多模态大模型成果&quot;Multimodal learning with next-token prediction for large multimodal models（通过预测下一个词元进行多模态学习的多模态大模型）&quot;上线国际顶级学术期刊Nature，预计2月12日纸质版正式刊发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Nature编辑点评这项研究：Emu3 仅基于预测下一个词元（Next-token prediction），实现了大规模文本、图像和视频的统一学习，其在生成与感知任务上的性能可与使用专门路线相当，这一成果对构建可扩展、统一的多模态智能系统具有重要意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/4104e358f0c6b33a3c481f57a18d6760.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41586-025-10041-x&quot;&gt;https://www.nature.com/articles/s41586-025-10041-x&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2018年以来，GPT采用 “预测下一个词元（Next-token prediction，NTP）”的自回归路线，实现了语言大模型重大突破，开启了生成式人工智能浪潮。而多模态模型主要依赖对比学习、扩散模型等专门路线，自回归路线是否可以作为通用路线统一多模态？一直是未解之谜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源这项成果表明，只采用自回归路线，就可以统一多模态学习，训练出优秀的原生多模态大模型，对于确立自回归成为生成式人工智能统一路线具有重大意义。在后续迭代的Emu3.5版本，确实证明了这一范式的可拓展性，并达成预测下一个状态（Next-state prediction)的能力跃迁，获得可泛化的世界建模能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从语言到多模态：“预测下一个词元”的潜力与未解之问&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“预测下一个词元”彻底改变了语言模型，促成了如 ChatGPT等突破性成果，并引发了关于通用人工智能（AGI）早期迹象的讨论。然而，其在多模态学习中的潜力一直不甚明朗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在多模态模型领域，视觉生成长期以来由结构复杂的扩散模型主导，而视觉语言感知则主要由组合式方法引领 ，这些方法通常将CLIP编码器与大语言模型（LLMs）结合。尽管已有一些尝试试图统一生成与感知（如Emu和Chameleon），但这些工作要么简单将LLM与扩散模型拼接在一起，要么在性能效果上不及那些针对生成或感知任务精心设计的专用方法。这就留下了一个根本性的科学问题：单一的预测下一个词元框架是否能够作为通用的多模态学习范式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就此，智源提出了Emu3，基于“预测下一个词元”的全新多模态模型，将图像、文本和视频统一离散化到同一个表示空间中，并从零开始，在多模态序列混合数据上联合训练一个单一的 Transformer。这一架构证明了仅凭“预测下一个词元”，就能够同时支持高水平的生成能力与理解能力，并且在同一统一架构下，自然地扩展到机器人操作以及多模态交错等生成任务。此外，研究团队还做了大量消融实验和分析，验证了多模态学习的规模定律（Scaling law）、统一离散化的高效性、以及解码器架构的有效性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/58/5876defd768a77c0f403e3b928a37a11.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Emu3 架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验显示，Emu3在生成与感知任务上的整体表现可与多种成熟的任务专用模型相媲美：在文生图任务中，其效果达到扩散模型水平；在视觉语言理解方面，可以与融合CLIP和大语言模型的主流方案比肩。此外，Emu3还具备视频生成能力。不同于以噪声为起点的扩散式视频生成模型，Emu3通过自回归方式逐词元（token）预测视频序列，实现基于因果的视频生成与延展，展现出对物理世界中环境、人类与动物行为的初步模拟能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从模型到范式：Emu3对多模态学习的启示&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同于 Sora 的扩散式视频生成，Emu3&amp;nbsp;采用纯自回归方式逐词元（token） 生成视频，能够在给定上下文下进行视频延展与未来预测，并在文本引导下生成高保真视频。此外，Emu3 还可拓展至视觉语言交错生成，例如图文并茂的菜谱生成；也可拓展至视觉语言动作建模，如机器人操作VLA等，进一步体现了“预测下一个词元”的通用性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源研究团队对相关研究的多项关键技术与模型进行了开源，以推动该方向的持续研究。其中包括一个稳定且通用的视觉分词器（tokenizer），可将图像与视频高效转换为离散词元来表示。同时，研究通过大规模消融实验系统分析了多项关键技术的设计选择，例如：分词器（tokenizer）码本尺寸、初始化策略、多模态dropout机制以及损失权重配置等，揭示了多模态自回归模型在训练过程中的动态特性。研究还验证了自回归路线高度通用性：直接偏好优化（DPO）方法可无缝应用于自回归视觉生成任务，使模型能够更好地对齐人类偏好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究有力表明了预测下一个词元可作为多模态模型的核心范式，突破语言模型的边界，在多种多模态任务中展现了强劲性能。通过简化复杂的模型设计、聚焦统一词元，该方法在训练与推理阶段均展现出显著的可扩展性，为统一多模态学习奠定了坚实基础，有望推动原生多模态助手、世界模型以及具身智能等方向的发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此研究基础上，悟界·Emu3.5进一步通过大规模长时序视频训练，学习时空与因果关系，展现出随模型与数据规模增长而提升的物理世界建模能力，并观察到多模态能力随规模扩展而涌现的趋势，实现了“预测下一个状态”的范式升级。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</link><guid isPermaLink="false">https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</guid><pubDate>Thu, 29 Jan 2026 06:47:46 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“AI在一线：开发人员如何重塑软件开发流程” ｜圆桌讨论</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从代码生成到自动化文档，人工智能已经开始渗透到软件开发生命周期的几乎每个阶段。但除了炒作之外，实际上发生了什么变化？我们询问了一群工程师、架构师和技术领导者，AI辅助工具的兴起如何重塑软件开发的既定节奏，以及他们在现实世界中采用AI后学到了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;讨论嘉宾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mariia Bulycheva——Intapp高级机器学习工程师Phil Calçado——Outropy首席执行官Andreas Kollegger——Neo4j高级开发者倡导者May Walter——Hud.io创始人、首席技术官&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：AI辅助工具的兴起对你们组织的软件开发过程有何影响？它们是否改变了你们对软件架构的思考方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：AI辅助工具加速了原型设计，并减少了在重复编码任务上花费的时间，使我们的团队能够更多地关注架构决策和设计复杂的在线实验，这对于大规模迭代改进复杂的推荐系统至关重要。从数字平台典型的大量多模态数据中获得初步洞察也变得更快、更顺畅、更一致，因为我们可以将初始数据分析委托给了AI。&amp;nbsp;我们工作的另一个非常重要的方面是跟上我们领域科学发展的快速步伐。每年，在顶级会议上都会发表数千篇新的研究论文，过去阅读它们并确定哪些可能与我们团队的日常ML任务相关是非常耗时的。今天，AI工具提供了高质量的摘要，甚至突出显示哪些方法可能适用于我们的用例。这已经导致了几个新建模想法的快速实现，否则我们可能需要花费数周甚至数月的时间来发现和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：绝对有。我们运行的是一个消费者参与平台，其功能之多，任何正常人都无法全部记住。例如，最近，我们需要改变我们处理时区的调度方式。代码变更本身可能只有10行，但真正的工作是深入数百个涉及调度的地方，弄清楚每个地方的假设，并添加单元测试以断言调用站点不会中断，而是行为的变化。我们原以为这将是一个为期六个月的项目，因为我们需要逐步研究并进行小的更改。&amp;nbsp;有了像Cursor和Claude Code这样的工具，我们大大缩短了这个时间。它们帮助我们找出所有受影响的位置，为每个位置生成单元测试，并将推出分成按子系统分组的小PR。每个PR都带有对所属团队的上下文敏感的描述——不仅仅是“修复调度，请审查”，而是解释为什么以及在他们的世界中预期的影响。&amp;nbsp;因此，尽管我们像其他人一样看到了原始代码输出的增加，但在我们这样成熟的、超大规模的系统中，最大的提升在于AI如何帮助我们研究自己的代码库，将无聊但必不可少的安全检查整合在一起，使系统性变更变得不那么可怕。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在我们组织中，所有员工现在都可以使用AI辅助工具。对于表面层级的界面设计，这些工具帮助我们更快地迭代，探索新想法，并解锁新方法，如氛围编码，专注于更高层次的设计和策略。&amp;nbsp;但我们也确实遇到了AI的局限性。像许多组织一样，我们发现大语言模型（LLM）在需要深厚领域专业知识和全局架构整体视图的高度专业化代码上挣扎。我们的代码库本身就超过了任何LLM上下文窗口的容量，而这些模型本身也没有在其中的独特复杂性上进行训练。简而言之，AI不能发明它不理解的东西。因此，我们有意采取了一种以人为中心的方法：虽然AI帮助我们加速和增强，但推动软件架构突破的是我们工程师的专业知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：AI辅助工具极大地缩短了从想法到工作代码的路径。一旦意图明确，迭代周期就会显著缩短。开发人员正在从代码的唯一作者转变为更像是管理者的角色——指导代理，验证输出，并确保需求得到真正满足。&amp;nbsp;AI之前，架构是关于团队之间的所有权和可扩展接口。这引入了一个新的维度：上下文架构——设计代理生成生产就绪代码所需的输入、脚手架和护栏。上下文工程正在成为系统的核心部分，它简化了在复杂环境中快速构建的能力，如分布式和基于事件的系统。&amp;nbsp;但速度带来了一个新的瓶颈：为生产准备AI生成的变更。即使审查有了AI辅助，挑战也不再是关于发现语法错误，而是在大型、大规模的系统中验证意想不到的后果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：人工智能的采用如何影响团队内部的入职流程？你们团队或组织中的初级开发人员是否受到软件开发过程中采用人工智能的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：人工智能工具可以通过提供即时的代码示例、文档摘要和测试建议，显著加快学习过程，这些都支持了初级开发人员。在处理个性化和推荐系统等复杂领域的团队中，这一点尤其有用，因为现在初级人员可以更快地探索新的代码库，而不必总是依赖高级工程师。同时，我们将他们与更有经验的同事配对，以确保他们学习潜在的基本建模和系统设计原则，而不仅仅是捷径。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我们刚刚让暑期实习生展示了他们的项目，几乎每个人都把人工智能称为救星。进入一个有10年历史的Rails代码库，其中包含数千个可移动的部分，这是令人生畏的。但是能够对Cursor或Claude Code说，“我是一名懂Python和C++的大三学生，请用我熟悉的方式解释这个Rails代码”，这意味着他们可以在几周内提高效率，而不是仅仅把时间花在弄清楚基础知识上。&amp;nbsp;而且，不仅仅是实习生。在这个庞大的系统中，即使是高级工程师也需要比在小公司更多的准备时间。AI并没有消除对系统的实际理解，但它确实减轻了“我们在哪里处理认证？”或“我们是否已经有了观察者模式的实现？”这类问题的压力。&amp;nbsp;当然，这里有一个问题。生成式AI擅长复制模式，这通常意味着我们不希望再看到的遗留风格和架构。因此，我们不得不适应。我们正在使我们的工作流程和架构更加适应AI，并且我们已经开始将当前的指导方针直接嵌入到Claude Code和Cursor的智能体中。这样，当AI提供帮助时，它会引导人们走向现在，而不是过去。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：人工智能的采用增强了我们的入职流程，特别是对于新接触图数据库的初级开发人员。虽然人工智能不能取代经验丰富的导师的指导，但它通过帮助新员工更快地上手，补充了我们现有的入职资源。&amp;nbsp;入职培训不仅仅是教授编码技能。它是关于建立领域专业知识的。编码能力很重要，但更重要的是理解要编写什么代码以及为什么。这就是为什么我们的入职开发人员，他们对代码库及其架构有深入的了解，在向初级团队成员传授专业知识和上下文方面发挥着关键作用。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：人工智能降低了贡献的障碍。现在，一个新开发人员可以在他们的第一天就写出可用的代码——这与早期工作仅限于样板或错误修复的日子相比，是一个戏剧性的转变。但真正的机会不在于速度；而是在于能力和范围的深度。&amp;nbsp;我最常听到的担忧是，人工智能有使入职变得肤浅的风险——初级人员可以在不理解代码为何以某种方式行为的情况下生成代码。我的经验恰恰相反。当代码生成与运行时反馈配对时，初级开发人员从一开始就接触到系统思维：架构在负载下的行为如何，依赖项如何相互作用，以及变化如何波及到业务结果。工程师成为智能体代码生成过程中的业务大使。&amp;nbsp;他们不再需要花几个月的时间来处理低价值的工作，而是能够处理团队的更多任务。如果做得好，这不会跳过步骤——它会加速步骤。有了正确的文化和期望设定，初级工程师可以更快地发展成为全面发展的工程师，因为他们不仅学习如何编写代码，还学习了为什么它在系统环境中很重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在你们团队或组织中，你们是否测量过AI辅助开发对生产力或质量的影响？你们学到了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们在样板代码和单元测试生成方面看到了明显的生产力提升，甚至在为推荐系统设置模拟实验方面也是如此。然而，当处理影响大规模客户体验的关键系统时，真正的好处来自于将AI辅助与深度工程师参与结合起来。我们了解到，虽然AI提高了生产力，但质量仍然取决于仔细验证和清晰的指标和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado:：没有正式的测量。坦率地说，我不相信大多数抛出的“生产力”数字。在软件领域，你可以操纵指标，直到它们说出你想要的任何东西，而AI的炒作周期使这变得更糟。事实上，人们再次认真计算代码行数，只是为了增加一轮融资或提高股价，这是令人尴尬的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在前端方面，我们看到了生产力的提升，特别是对于那些使用Cursor等工具的工程师。我们的许多工程师已经使用AI支持来更快地理解、进行表面编码和测试我们的代码库，但我们从AI中看到的真正影响是对开发人员体验的影响。通过使用AI工具来支持他们的一些活动，我们的工程师现在有更多的时间发挥创造力，并最终改进他们解决问题的方式，并为他们的工作创造新的方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：是的，我们了解到的第一件事是，大多数常见的度量标准并没有太大意义。公认的代码行数、提交次数、PR：AI可以立即夸大这些数字，但它们只是工程生产力的虚荣指标。&amp;nbsp;真正的信号存在于下游。发布稳定性、事故频率、随叫随到的时间，甚至代码变更率，都能告诉我们是否真的在加快速度，或者只是在制造更多的脆弱性。AI将速度转移到了管道的前端，但除非验证循环紧密，否则债务会在后期显现——以缺陷、回归和精疲力竭的团队的形式。&amp;nbsp;从第一天开始，通过持续的生产反馈，我们可以看到真相所在：功能开发变得更快，但审查周期变得更长了，部署后的错误也出现了。&amp;nbsp;教训是，AI生产力需要一个学习曲线和迭代方法。一旦度量，可以逐步改进采用，以捕捉优势——同时避免因快速交付但存在稳定性问题窒息的陷阱。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：为了有效地使用AI工具，你们的团队或组织中有哪些非技术方面的东西需要改变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：最大的变化是在心态上。团队必须从期望AI建议是“正确”的，转变为将它们视为需要彻底验证、讨论和测试的起点。这种文化转变鼓励了实验和跨学科合作，将对确定性的关注转变为对探索的关注。在大规模个性化工作中，我们还需要与产品和法律团队就负责任的数据使用和可复制性达成一致。这些协议创造了护栏，使工程师能够安全地探索和部署AI辅助解决方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我认为关于生成式AI工具的最大事情是：这超越了编码。是的，像任何其他工具一样，你必须注意副作用。生成式AI可以很容易地生成内容：代码、PR评论、技术规范、电子邮件、Slack消息。它也使得总结大量文本并过滤掉非必要的内容变得非常容易。&amp;nbsp;这两个特性的结合创造了一个奇怪的激励：人们生成了大量的低信噪比内容，然后其他人再次使用AI将其过滤回去。这是极其无效的。我们已经开始内部讨论在制作内容时正确使用AI的方式。剧透：这不是让AI为你写作，而是使用AI帮助你写得更好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：我们在AI的早期阶段建立了一个AI伦理委员会，组织中的代表们更好地理解和指导AI如何影响我们的业务的每一个方面。所有技术都可以成为一股善的力量，但它也需要有意识的思考、行动和指导。&amp;nbsp;因为我们信任客户数据，我们的开发人员需要在引入AI作为助手的任何领域都应用更高的敏感性，从简单的计划文件和电子邮件线程到代码库本身。随着我们采用、集成和扩展AI，我们所有的开发人员都必须确保人类判断，而不是AI，指导和监督每一步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：最大的变化不是技术上的，而是文化上的。开发人员自然会单独采用工具，但当AI被视为个人生产力技巧时，它的效果并不好。只有当它成为共享流程的一部分，有一致的验证步骤和清晰的责任时，它才会有效。此外，AI工具在缺乏上下文时不会失败，而是产生不准确的回应，这可能会损害用户的信任并增加变更的摩擦。&amp;nbsp;在10名工程师的情况下，每个人都可以以自己的方式进行实验。在100名工程师的情况下，这种方法就会崩溃。不同的智能体独立生成代码会造成分裂和风险。我们转向了共同的设置和共享的工作流程，这样AI不仅仅是帮助个人更快地移动，而是使整个团队更快地移动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们在管理AI辅助编码方面设置了哪些护栏（文化、道德或技术），以及你们如何管理个人、团队和组织对AI输出的信任问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们将AI输出视为重复性或样板任务的“初稿代码”，这些代码总是经过单元测试和同行评审。在文化方面，我们强调责任：提交代码的开发人员负责，无论是否有AI辅助。对于机器学习工作流程，我们不信任AI直接生成模型，相反，在任何模型更改甚至可以考虑用于生产之前，我们依赖于针对既定基线的自动离线评估。这确保了AI驱动的贡献达到了与人类编写的代码相同的质量标准。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：这仍然是一个非常初级的实践，所以我们一直在尝试不同的护栏和工具。在安全和合规方面，我们的立场从一开始就很明确：作为一个处理世界上一些最大品牌数据的上市公司，我们必须将相同的治理实践应用于AI编码工具，就像我们其他地方所做的一样。几年前，这意味着落后于曲线，但今天大多数供应商都有坚实的企业计划，所以我们可以安全地使用最先进的模型，而不会妥协安全性或可审计性。&amp;nbsp;文化上，我们很早就设定了期望：仅仅因为一个AI工具编写了变更，并不意味着它不是你的代码。你仍然拥有它，你需要把每一行都当作是你自己打出来的一样。这与使用IntelliJ的提取方法重构没有什么不同，在这种情况下，它可能自动化了机械操作，但你仍然需要理解并验证结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：大型企业软件可以提供防止AI生成错误的保障，但更高的准确性、上下文和可追溯性是使AI输出可解释和可验证的关键，而不仅仅是性能。这就是为什么我们引入了一个广泛的测试计划，涵盖了从单个单元测试到详尽的生产级验证的所有内容。&amp;nbsp;与此同时，我们的工程师在纪律和创新之间保持平衡至关重要。我们鼓励工程师尝试各种想法，探索可能尚未准备好投入生产的项目。这种环境允许快速迭代和创造力，同时确保只有最有价值和经过充分测试的创新才能过渡到生产。其结果是一种独特的平衡：保持客户的信任和稳定，同时不断推进图驱动的创新，使AI更准确、透明和可解释。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：我们必须赢得对AI输出的信任，而获得信任的唯一方法便是创造上下文。每个AI生成的变更都经历了与人类编写的代码相同的标准——审查、测试、验证——但有一个额外的要求：它必须在运行时证明自己。&amp;nbsp;对我们来说，信任不是来自对模型的信念；而是来自观察代码在现实世界条件下的行为。新版本的性能和旧版本一样吗？它是否引入了新的错误或在负载下改变了性能？当运行时上下文持续可用时，AI就不再是黑盒。它变成了一个可以信任的伙伴，因为它与工程师依赖相同的信号进行推理。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们认为软件开发团队低估了AI编码工具的哪些方面？你们认为有哪些当前的AI增强型开发工作流程或模型被过度炒作，哪些仍然没有得到充分利用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：许多团队低估了上下文管理的重要性，因为AI的效果取决于你提供的上下文（代码库、文档、架构、在线测试的实验设置）。在大型系统中，这意味着不仅要管理代码片段，还要管理模型性能数据、日志和实验历史，以有效指导AI工具。过度炒作：AI据说取代了工程判断的“一键式开发”。未充分利用：AI辅助调试、实验设置和复杂ML工作流的文档记录，这可以大幅降低长期维护成本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：现在AI中有很多空洞的炒作，很难挑出一个罪魁祸首。但大多数团队都低估了的这一点是：AI编码工具不是一个单程的魔法盒子。你不能只是向它们抛出一个提示，就期望得到一致、正确的结果。&amp;nbsp;这是任何真正构建AI产品的人都知道的痛苦教训。无论你的提示工程有多聪明，有效使用LLMs来自于结合工作流程并确保正确的上下文在正确的时间可用。否则，你只是在掷骰子。&amp;nbsp;我在以前为一个流行的代码审查工具构建AI管道时亲眼目睹了这一点。模型可能已经记住了所有写过的Python书籍，但如果你问10个开发人员“正确的方法”去做某事，你会得到11个答案。如果没有你的代码库、组织标准和实际目标的上下文，LLM就不知道哪个适用。这就是为什么你会得到完全不同的解决方案，甚至是对立的——这取决于当你提问时，概率之神想要倾向于哪一边。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：许多软件开发团队低估了AI编码工具可以简化开发人员最不喜欢的任务，比如编写测试和文档。虽然AI编码演示承诺低代码和无代码，通常看起来微不足道或不可靠，但它们展示了AI如何将自然语言和代码之间进行转换，这对于自动化繁琐的任务和重复的设置是理想的。类似地，有一种专门用于项目初始化和代码生成的编码工具。&amp;nbsp;有一种工作流程被夸大了，但却没有得到充分利用，那就是让编码智能体通宵运行，并在早上检查他们的工作。我不建议在无人监督的情况下重构新产品功能或大量代码，但编码智能体非常适合一个定义良好的GitHub问题，它有良好的讨论，一个孤立且可重现的例子，以及一个可测试的修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：大多数团队低估的是，模型已经足够好（并且正在变得更好）——缺失的成分是组织上下文。等待“更好的模型”是一种分心。真正的挑战是设计提供生成生产级代码所需的上下文的系统：你的架构、编码标准、数据边界和业务优先事项。如果没有这些，即使是最好的模型（或工程师）也会表现不佳。&amp;nbsp;另一方面，今天被过度炒作的是原始代码生成和静态代码审查。这些工作流程在演示中看起来令人印象深刻，但它们并没有解决大型组织中软件工程最难的部分：调试和质量保证。智能体仍然缺乏运行时上下文，并且很少有工具来评估哪些更改在业务影响方面真正关键。&amp;nbsp;这个差距很重要，因为更快的代码生成意味着更多的更改流入生产环境——而且没有更强大的流程来决定要监控什么，团队冒着为了脆弱性而牺牲速度的风险。未充分利用的前沿不是更快地编写代码，而是构建验证循环和运行时感知工具，以在这些更改部署之前增加确定性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这次讨论中得出的第一个，也许是最重要的结论是，尽管在软件开发过程中采用AI工具无疑降低了贡献的门槛，但它仍然是一个乘数，而不是灵丹妙药。只有与强大的组织环境相结合，AI才能增强生产力。基于AI的工程有潜力成为软件开发的核心，就像CI/CD管道曾经一样。然而，架构、编码标准和实验脚手架是成功采用AI的支撑支柱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，随着AI工具的发展，组织中开发人员的角色也从代码作者转变为系统编排者。新采用的策划、验证和集成AI输出的过程并没有取代软件工程这门手艺；相反，它增强了它。批判性思维和架构意识比以往任何时候都更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，采用任何新技术都会带来陷阱，对于AI和基于AI的工具也是如此。降低贡献的进入门槛也意味着增加了浅薄理解和生产次品代码的风险，这可能对初级开发人员的职业发展和整个组织产生负面影响。指导和运行时反馈是重要的护栏，以及文化和伦理保障：AI输出必须被视为初稿，人类必须对其负责。当涉及到AI时，信任不是授予的：它是一个过程，通过测试、同行评审、运行时验证和透明度赢得。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成功的指标也必须重新思考，因为AI夸大了所有传统的生产力指标。有意义的信号来得更晚：稳定性、变动、事件，以及有多少时间可以释放给创造力和架构。将AI扩展视为一个协作过程，而不是个人生产力的提升，这需要协调的工作流程和对周围流程的更高成熟度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论好坏，很明显，AI带来的变化已经到来，正在重塑软件开发的工艺。仍然有未被充分利用的方面，但上下文设计和运行时感知工具已经是下一个架构前沿。从长远来看，AI竞赛的赢家将是那些将其整合到具有问责制、信任和能够以负责任的方式共同发展的团队级流程中的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&quot;&gt;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</link><guid isPermaLink="false">https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>作者：Arthur Casals</author><category>AI&amp;大模型</category></item><item><title>半年处理 1 亿笔支付！x402 V2 升级，让支付更简单</title><description>&lt;p&gt;在经历了为期六个月的真实场景应用之后，&lt;a href=&quot;https://www.x402.org/writing/x402-v2-launch&quot;&gt;开放支付标准 x402&lt;/a&gt;&quot;&amp;nbsp;迎来了重要更新。本次升级显著拓展了协议能力，使其不再局限于“单次请求、固定金额”的支付模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;新版协议新增了多项关键能力，包括：基于钱包的身份识别、自动 API 发现机制、动态支付接收方，以及通过 CAIP 标准实现的多链与法币扩展支持。同时，x402 还推出了一个完全模块化的 SDK，用于支持自定义网络和支付方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 是一次重量级升级，使协议在通用性、灵活性和跨网络扩展能力上均有显著提升。新版规范更加简洁、模块化，并与 CAIP、IETF Header 等现代标准保持一致，从而实现了一个可同时覆盖链上与链下支付的统一接口。在具体能力上，x402 V2 提供了一个统一的支付接口，可在多条区块链上支持稳定币和代币支付，包括 Base、Solana 等网络；同时也保持了对传统支付体系的兼容性，如 ACH、SEPA 以及银行卡网络。此外，协议还引入了“按请求路由”的能力，支持将支付定向至特定地址、角色，或基于回调逻辑进行分发，从而实现更复杂的多步骤支付工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 的另一项重要改进，是清晰地区分了三类角色：协议规范本身、SDK 实现，以及负责链上验证和结算的facilitators。这种分层设计显著提升了协议的可扩展性，并为插件化、模块化架构奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新标准还引入了基于钱包的访问机制、可复用会话（reusable sessions）以及模块化付费墙（modular paywalls）。钱包支持让客户端在支付流程上拥有更高灵活性，可减少已购项目的重复交互与延迟；而模块化付费墙则使开发者能够更容易地集成和扩展后端支付逻辑，推动整个生态向更开放的方向发展。&lt;/p&gt;&lt;p&gt;在开发者体验方面，x402 V2 也进行了系统性优化。通过模块化设计简化配置流程，新增了同时选择多个 facilitators 的能力，并大幅减少了“胶水代码”和样板代码的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 是一套开放、原生于 Web 的支付标准与协议，其目标是让“支付”成为互联网的一等公民。它支持微支付、按使用付费（pay-per-use）以及机器对机器（machine-to-machine）支付，使 Web 应用、API 以及自治代理（如 AI 机器人）能够直接通过 HTTP 为服务付费，而无需传统账户体系、订阅模式或复杂的支付流程。在推出后的短短几个月内，x402 已在 API、Web 应用和自治代理等场景中处理了超过 1 亿次 支付流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该协议利用了一个长期被忽视的 HTTP 状态码 —— 402（Payment Required），用于在需要付费时返回支付指令。借助 x402，支付可以直接嵌入在 HTTP 请求—响应流程中，无需将用户跳转至外部支付页面，也不再依赖 API Key 或个人账户体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为 x402 Foundation 的最初合作伙伴之一，Cloudflare 与 Coinbase 一同推动了该协议的落地。Cloudflare 已将 x402 集成进其&lt;a href=&quot;https://developers.cloudflare.com/agents/x402/&quot;&gt;开发者工具和基础设施&lt;/a&gt;&quot;中，包括：Agents SDK：帮助开发者构建能够自动完成 x402 支付的智能代理；MCP servers：向外暴露支持 x402 的工具，使服务能够返回 402 Payment Required 响应，并接收来自客户端的 x402 支付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&quot;&gt;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</link><guid isPermaLink="false">https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>Sergio De Simone</author><category>软件工程</category></item><item><title>刚完成“卖身”重组，TikTok就瘫了！Oracle 背锅：暴风雪吹坏了数据中心</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle数据中心断电，引发 TikTok 大面积瘫痪&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，短视频平台 TikTok 在美国出现了一次短暂的服务中断。值得玩味的是，这次中断的时间点，恰好卡在 TikTok 刚完成一项美国业务重组安排之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据这份重组安排，由 Oracle 与一组美国本土投资者共同组建的新合资实体，将接管 TikTok 在美国的运营相关事务，并被称为 TikTok USDS。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok USDS 承诺将用户数据通过 Oracle 公司拥有的数据中心进行传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚重组完没几天，TikTok就出现了大面积瘫痪，许多美国用户反映，他们无法上传视频到TikTok，也无法观看大多数新视频，包括美国以外用户成功上传的新视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一些用户表示，他们的算法似乎“重置”了，但目前尚不清楚这是否也与停电有关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事情不断发酵，逼得 TikTok&amp;nbsp;USDS 不得不出面回应了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok&amp;nbsp;USDS 发言人 Jamie Favazza在给The Verge的一封电子邮件中指出，该公司在其新创建的X账户上发布了一份声明，声明称，由于美国数据中心发生电力中断，影响了TikTok和我们运营的其他应用程序，公司一直在“努力恢复服务”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/20e4e23cbb346864a484a62806ea02f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然问题出在了数据中心，数据中心当然也要出来回应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle 回应：完全怪天气&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对不断升温的质疑，Oracle 公司于当地时间 1 月 27 日通过电子邮件向媒体作出正式回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle 发言人迈克尔·埃格伯特（Michael Egbert）表示，上周末美国遭遇的一场强烈冬季风暴，导致Oracle 一处数据中心发生了暂时性停电，从而影响了 TikTok 在美国的服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“上周末，Oracle 数据中心因天气原因发生暂时性停电，影响了 TikTok 的服务。” 埃格伯特在声明中写道。他进一步解释称，美国 TikTok 用户在停电后所遇到的问题，主要源于恢复过程中出现的技术故障，目前Oracle 正与 TikTok 合作，尽快修复相关问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一回应明确否认了服务异常与内容审查之间存在直接关联，并将原因归结为基础设施层面的突发事故。Oracle 方面的说法，也与当时美国多地遭遇极端冬季天气的事实相吻合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在随后的声明中，TikTok 指出，其工程团队正在持续推进恢复工作，并在 1 月 27 日表示，已在恢复美国系统方面取得“重大进展”，但仍提醒用户，某些技术问题可能在短期内持续存在，尤其是在发布新内容时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64663332726be7240cc1f4098dedf7cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为此次事件中的关键基础设施提供方，Oracle 的角色也受到资本市场关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Benzinga Pro 报道，Oracle 公司股票在事件曝光当日收于 174.90 美元，下跌 4.13%，但在盘后交易中回升 1.16% 至 176.93 美元。Benzinga 的 Edge 股票排名显示，Oracle 股票在动量和价值维度上的评分均处于较低水平，反映出其在短期至长期内的价格趋势承压。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/254b1fa735b06cff92bf69b81ae8c593.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 TikTok USDS 合资企业逐步接管美国业务，其基础设施稳定性、内容审核机制以及与地方和联邦监管机构的互动方式，仍将持续受到审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：不只可以怪天气，还可以怪AI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着最终达成的合资协议，被外界普遍视为 TikTok 在美国“生死攸关”的一次妥协安排。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，此次技术中断发生之际，正值 TikTok 更新其美国隐私政策之后。新政策与合资架构调整相配套，但其中关于可能收集的数据类型的表述，引发部分用户不安。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;市场情报公司 Sensor Tower 向 CNBC 提供的数据显示，在过去五天内，美国地区 TikTok 的每日应用删除量较此前三个月的平均水平增长了近 150%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 上，一条关于 Oracle 数据中心与 TikTok 服务中断的帖子吸引了大量关注，不少网友在评论区提出了各类猜测、调侃与个人经验分享，这些反馈在一定程度上折射出技术社区对事件的怀疑态度，以及对 TikTok 内容机制与 Oracle 云服务能力的长期刻板印象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位ID名为 transcriptoin_error 的用户提出了一种“看似合理的推测”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，如果平台在系统中新增了内容过滤机制，那么在将相关流量迁移到新系统的过程中，确实有可能引发故障。他指出，在大规模系统迁移或数据转移时，出现配置错误或小规模失效并不罕见，尤其是在新旧系统并行、过滤规则叠加的情况下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论获得了数十次点赞，被不少用户视为“至少在工程逻辑上说得通”的一种解释，但评论者本人也并未声称这是事实，而是明确将其界定为推测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4a/4a7c72d3511f47000b48430ba2da565c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在另一条高赞的长篇幅评论中，该用户进一步构建了一套完整但高度假设性的系统模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他设想，如果 TikTok 平台不愿直接改动现有代码，以避免引发更大规模的系统崩溃，那么新增的内容过滤功能很可能会被设计成一个独立服务，甚至可能基于人工智能模型运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种设想下，所有潜在“敏感内容”都会被发送至一个新的 AI 服务进行判断，只有在得到“允许发布”的反馈后，内容才会正常上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9fa27b6e1350a4f8df70b50cb109e9c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该用户进一步推测，如果这一 AI 服务发生宕机，而系统默认策略又是“未通过即阻止”，那么大量内容就可能被一并拦截，从而在用户侧表现为算法行为的“剧烈变化”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论虽然点赞不高，但在讨论中被多次引用，成为部分网友解释“为什么技术故障会影响内容分发”的逻辑模板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有网友对上述推测持明显怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位在评论区拥有较高影响力的用户指出，至今仍然没有人能够清楚解释，为什么一次服务器层面的故障，会导致推荐算法或内容分发逻辑出现如此明显的变化。在他看来，如果问题仅限于数据中心断电或服务恢复过程中的技术瑕疵，那么算法层面的“性格突变”仍然缺乏合理解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了针对 TikTok 的讨论，Oracle 本身也成为 Reddit 用户情绪的集中投射对象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位用户直言，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“能力并非问题所在，科技圈里没人能忍受 Oracle。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/336724bda8cb17fd3aa87f2658e0017c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还引用了一句在技术圈流传已久的说法：“Oracle 没有客户，只有囚犯。”这类评论并未直接指向此次事件的具体责任，但反映出 Oracle 在开发者与工程师群体中的长期口碑问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&quot;&gt;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&quot;&gt;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&quot;&gt;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</link><guid isPermaLink="false">https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</guid><pubDate>Thu, 29 Jan 2026 05:21:46 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Rust贡献者推出新语言Rue，探索AI辅助编译器开发</title><description>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/steve-klabnik/&quot;&gt;Steve Klabnik&lt;/a&gt;&quot;是《&lt;a href=&quot;https://doc.rust-lang.org/stable/book/&quot;&gt;Rust编程语言&lt;/a&gt;&quot;》的作者，并且在过去的13年里对Rust项目做出了贡献，他宣布了Rue，这是一种系统编程语言，它在没有垃圾回收的情况下探索内存安全性，同时优先考虑开发人员的人机工程学，而不是Rust的复杂性。该项目是在Anthropic的&lt;a href=&quot;https://claude.com/product/overview&quot;&gt;Claude AI&lt;/a&gt;&quot;的大力帮助下开发的，目标是填补高性能系统语言和垃圾回收替代品之间的未充分服务的设计空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在使用Rust 13周年之际，Klabnik在一篇博客文章中解释了他的动机：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我一直在想我是否应该尝试创造自己的语言。我真的很喜欢它们！这就是为什么我最初参与Ruby，然后是Rust的部分原因！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语言名称遵循他的“Ru”前缀模式（Ruby、Rust、Rue），同时保持双重解释——既是&lt;a href=&quot;https://en.wikipedia.org/wiki/Ruta_graveolens&quot;&gt;花&lt;/a&gt;&quot;又是&lt;a href=&quot;https://redkiwiapp.com/en/english-guide/synonyms/rue-regret&quot;&gt;遗憾的表达&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik的核心设计问题是：“如果Rust不试图与C和C++竞争最高性能会怎么样？”如果我们愿意为了易用性而使性能稍微降低，但不要太低，会怎样？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术方法的核心是消除Rust的标志性——&lt;a href=&quot;https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html&quot;&gt;借用检查器&lt;/a&gt;&quot;。考虑一个典型的Rust代码，其中你试图在持有对其中一个元素的引用的同时修改一个向量。编译器拒绝此操作，因为引用可能会无效。Rue通过使用“inout”参数来暂时转移所有权，从而避免了整个问题，类似于Swift。在Rust中，试图在迭代当量时修改它会在编译时失败。Rue的inout参数允许你临时传递可变引用，但防止将它们存储在数据结构中；在保持内存安全的同时，通过更简单的限制消除了对生命周期跟踪的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;函数可以就地修改值，但这些值不能作为引用存储在堆分配的结构中。不需要生命周期注释。权衡是什么？某些模式变得无法表达。正如设计文档所承认的，Rue无法支持从其容器借用的迭代器；它们必须消耗它们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News社区的反应既有兴趣也有怀疑。一位评论者&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;捕捉&lt;/a&gt;&quot;到了这个挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Rust之所以成功地制造出没有垃圾回收的内存安全语言，是因为它引入了显著的复杂性（这是一种权衡）。没有人真正知道除此之外的合理方法，除非你还想放弃通用系统编程语言的要求。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据GitHub仓库中的设计提案，Rue实现了四种不同的所有权模式：值类型、仿射类型、线性类型和引用计数类型。Klabnik在回应中承认，“这必然会导致一些表现力的丧失。没有万能的解决方案。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发方法代表了一个实验，解决了Klabnik多年来一直在思考的&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;问题&lt;/a&gt;&quot;：“没有资金或团队，一个人还能构建一门编程语言吗？”这种方法标志着Klabnik的转变，他形容自己直到2025年都是AI怀疑论者。他第一次尝试在没有有效利用AI的情况下构建Rue，经过几个月的工作后不得不放弃。这一迭代，更有效地使用Anthropic的Claude AI，仅用两周时间就产生了大约70,000行Rust编译器代码，远远超过了他之前几个月的尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种协作超越了典型的编码协助。在Klabnik和Claude共&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;同署名&lt;/a&gt;&quot;的博客文章中，AI描述了编写大部分实现代码，而Klabnik指导架构并做出设计决策。Klabnik强调，有效使用AI工具需要大量的技能：“仅仅知道如何编写代码实际上不足以真正使用大模型。它们是它们自己的新类别的工具。”他的方法涉及迭代实验，编写简短的代码片段，开始对话，并测试不同的提示策略。这种模式是否能消除历史上资助语言项目的大量投资，还有待观察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue仍处于早期开发阶段，具有基本的控制流、函数和非泛型枚举。它通过自定义后端而不是LLVM编译为本地可执行文件，通过简化的语义实现快速编译时间。堆分配正在进行中，而语言服务器协议支持、包管理和并发模型尚未实现。该项目使用Buck2而不是Cargo进行未来的编译器引导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik保持着适度的期望：“我不指望它能发展成我的业余项目。”尽管如此，他指出，PHP和Rust的创造者Rasmus Lerdorf和Graydon Hoare也是从个人实验开始的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI辅助开发工具重塑软件工程，这项实验正在进行。虽然GitHub Copilot和类似的工具协助增量编码，Klabnik使用AI进行编译器的架构级工作的方法代表了不同级别的合作。如果成功，它可能表明，传统上需要大型团队的复杂基础设施项目，在AI的帮助下，对于熟练的个人来说可能是&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;可行的&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真正的考验将是那些对Rust的学习曲线感到沮丧但又不愿意采用垃圾回收机制的开发人员是否能接受Rue的权衡。正如一位Hacker News评论者所说：&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果他们能在设计空间中找到一个全新的未被探索的点，我会非常感兴趣，但目前，我仍然持怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue语言的文档可在&lt;a href=&quot;https://rue-lang.dev/&quot;&gt;rue-lang.dev&lt;/a&gt;&quot;上找到，源代码在&lt;a href=&quot;https://github.com/rue-language/rue&quot;&gt;GitHub&lt;/a&gt;&quot;上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&quot;&gt;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</link><guid isPermaLink="false">https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</guid><pubDate>Thu, 29 Jan 2026 03:05:14 GMT</pubDate><author>Tim Anderson</author><category>编程语言</category></item><item><title>Spring近期资讯：Boot、Security、Integration、Modulith和AMQP首个里程碑版本发布</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Boot&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot; 4.1.0的首个里程碑版本提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 @AutoConfigureWebServer 注解用于在支持 @SpringBootTest 注解的特定类和随机端口下启动Web服务器；以及通过Spring AMQP和Spring Kafka中定义的配置bean的自动配置，改进了可观测性和指标支持。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了缺陷修复、文档改进和依赖升级，Spring Boot 4.0.2，&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-0-2-available-now&quot;&gt;即第二个维护版本&lt;/a&gt;&quot;，还提供了一个值得注意的更改，即从 spring-boot-jetty 模块中移除了对 org.eclipse.jetty.ee11:jetty-ee11-servlets 模块的依赖，因为它未被使用并被确定为不必要。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Security&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：在 PasswordEncoder 接口中定义的 encode() 方法添加了空值契约；以及使用Spring Framework DefaultParameterNameDiscoverer 类中定义的 getSharedInstance() 方法，而不是创建该类的单独自定义实例。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Integration&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 spring-integration-cloudevents 和 spring-integration-grpc 模块分别支持&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/cloudevents.html&quot;&gt;CloudEvents&lt;/a&gt;&quot;转换和&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/grpc.html&quot;&gt;gRPC&lt;/a&gt;&quot;协议；以及新的 GrpcInboundGateway 和 GrpcOutboundGateway 类，作为gRPC客户端调用的入站和出站网关。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Modulith&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot; 2.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和改进，例如：在集成测试运行后重置 TimeMachine 类实例中的位移的能力；以及 spring.modulith.test.on-no-changes 属性的两个新属性值 execute-all 和 execute-none ，提供了在未检测到更改时跳过所有测试的能力。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AI&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot; 2.0.0的第二个&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和许多新功能，例如：在 McpServerAutoConfiguration 类中添加了新的接口 McpSyncServerCustomizer 和 McpAsyncServerCustomizer ，解决了非web应用程序环境中MCP自动配置的问题；以及添加了来自&lt;a href=&quot;https://aws.amazon.com/s3/features/vectors/&quot;&gt;Amazon S3&lt;/a&gt;&quot;、&lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock Knowledge Base&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.quarkiverse.io/quarkus-langchain4j/dev/rag-infinispan-store.html&quot;&gt;Infinispan&lt;/a&gt;&quot;的向量存储后端。关于该版本的更多细节，包括重大变更，可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Batch&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-batch&quot;&gt;Spring Batch&lt;/a&gt;&quot; 6.0.2，即&lt;a href=&quot;https://spring.io/blog/2026/01/21/spring-batch-6-0-2-available-now&quot;&gt;第二个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、文档改进、依赖升级和一个新功能，引入了两个新类 ZonedDateTimeToStringConverter 和 OffsetDateTimeToStringConverter ，以支持 JobParameters 类的类型。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AMQP&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot; 4.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：新的 AmqpMessageListenerContainer 类实现了一个类似于 RabbitAmqpListenerContainer 类的容器；以及新的 @EnableAmqp 注解用于导入 AmqpDefaultConfiguration 类的实例，带有方便的基础设施bean。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-amqp/reference/4.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</link><guid isPermaLink="false">https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</guid><pubDate>Thu, 29 Jan 2026 03:01:25 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>TTC 完成千万美元级新一轮融资，厚雪资本领投、百度战略投资</title><description>&lt;p&gt;近日，北京才多对信息技术有限公司（ True Talents Connect ，以下简称“ TTC ”）宣布完成 A 轮千万美元级融资。本轮融资由厚雪资本领投，百度战略投资。此前，TTC 自研的 AI Agent 产品“小麦招聘”获第三届百度“文心杯”创业大赛一等奖，此次融资标志着其正式融入百度生态的新阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本轮融资将主要用于强化 AI 大模型与 Agent 技术研发，持续升级“小麦招聘”产品体验，并深化在 AI 及硬科技赛道的人才服务专业团队建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自创立以来，TTC 聚焦 AI 及前沿科技赛道，围绕“ AI +猎头”模式，为企业提供关键人才解决方案，致力于用科技（ AI ）高效连接人才与机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心团队兼具猎头基因与 AI 技术视野：TTC 创始人兼 CEO 肖玛峰 Max 拥有 17 年高端猎头行业经验，曾参与打造国内领先的中高端猎头品牌；联合创始人兼 CTO 宁辽原具备微软（美国）及字节跳动技术背景，长期深耕 AI 架构与模型落地，为公司 AI 能力与产品化提供持续支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年被视为 TTC 的 AI 元年。其推出核心产品「小麦招聘」，将顶尖猎头的判断逻辑与行业知识解码为结构化模型，赋能 AI Agent ，实现求职及招聘全链路的智能化升级，推动“人人都有一个专业 AI 猎头顾问”成为可能。产品上线仅 4 个月，已获得百度「文心杯」、量子位「 AI 100 创新产品」、创业北京「创业创新大赛」等多个行业奖项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随 AI 能力在业务中的全面落地，TTC 2025 年累计服务超千家 AI 领域科技企业及头部大厂，推荐岗位达数万次，年营收同比增长超 50 %，业务增长势头强劲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;厚雪资本创始合伙人侯昊翔 Roger ：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“投资 TTC ，源于我们一个核心判断：在 AI 时代，人才是终极的基础设施。TTC 所做的，正是帮助企业科学地组合与迁移关键人才，通过精准配置与协同设计，让不同能力在合适的时点形成共振，产生‘ 1+1&amp;gt;11 ’的创造力。更打动我们的是团队对‘激发人’的深刻理解——他们清楚地认识到，顶尖的人才服务是专业能力与判断艺术的结合，是成为点燃人才潜力的‘催化剂’。与此同时，Max 和团队拥有顶尖的行业经验，却始终保持着创业者的谦逊与初心，这让我们相信他们能走得更远。对厚雪而言，这同样是一次重要的战略协同。作为一家同样具有创业心态的机构，我们看好 TTC‘ AI 赋能顾问’与‘平台直接连接’的双轨模式。这不仅是对传统招聘痛点的革新，也为我们共同深入 AI 产业核心人群、持续积累认知与资源提供了新的入口。我们期待与这样的长期主义者同行，共同见证 AI 人才服务从‘高端配置’到‘ AI 时代企业基石’的变革。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百度：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们始终认为，只有当 AI 被内化为一种原生的能力，才能真正在各行各业引爆生产力革命。TTC 正是这一理念在人力资源行业的出色实践者。作为第三届百度‘文心杯’创业大赛一等奖项目，「小麦招聘」没有简单地将 AI 作为工具，而是将顶尖猎头的专业服务能力重构为原生 AI Agent ，这让高效、精准的人才匹配从‘高端服务’变成了可广泛触达的智能基础。百度此次战略投资并开放生态资源，正是希望与 TTC 携手，加速 AI 原生能力在人力资源领域的深度渗透，共同将‘智能红利’转化为支撑万千企业发展的‘人才红利’，进而转化为‘社会红利’。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，TTC 将继续秉持“用科技（ AI ）高效连接人才与机会”的使命，深化 AI 技术与人才服务的融合，让每一次人才连接，都更智能、更精准、更值得信赖。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink&quot;&gt;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink &lt;/a&gt;&quot;&amp;nbsp;（点击链接下载）&lt;/p&gt;</description><link>https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</link><guid isPermaLink="false">https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</guid><pubDate>Thu, 29 Jan 2026 03:00:00 GMT</pubDate><author>小麦招聘</author><category>百度</category><category>AI 工程化</category></item><item><title>Java近期资讯：Oracle关键补丁更新、Grizzly 5、Payara Platform、GraalVM、Liberica JDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenJDK&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JEP 527，&lt;a href=&quot;https://openjdk.org/jeps/527&quot;&gt;TLS 1.3的后量子混合密钥交换&lt;/a&gt;&quot;，在JDK 27中已从Proposed提升Targeted状态。这个JEP提议使用正在由互联网工程任务组（IETF）起草的TLS 1.3规范中的混合密钥交换，增强RFC 8446，&lt;a href=&quot;https://datatracker.ietf.org/doc/rfc8446/&quot;&gt;传输层安全（TLS）协议版本1.3的实现&lt;/a&gt;&quot;，与JEP 496，&lt;a href=&quot;https://openjdk.org/jeps/496&quot;&gt;量子抵抗模块格的密钥封装机制&lt;/a&gt;&quot;，其在JDK 24中交付。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle发布了JDK的25.0.2、21.0.10、17.0.18、11.0.30和8u481版本，作为2026年1月季度&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;关键补丁更新咨询&lt;/a&gt;&quot;的一部分。关于该版本的更多细节可以在&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/25-0-2-relnotes.html&quot;&gt;25.0.2&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/21-0-10-relnotes.html&quot;&gt;21.0.10&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/17-0-18-relnotes.html&quot;&gt;17.0.18&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/11-0-30-relnotes.html&quot;&gt;11.0.30&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/8u481-relnotes.html&quot;&gt;8u481&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 26&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B32&quot;&gt;Build 32&lt;/a&gt;&quot;在上周发布，包括从Build 31的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 27&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本Build 6也在上周发布，包括从Build 5的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GlassFish Grizzly&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/blob/main/README.md&quot;&gt;GlassFish Grizzly&lt;/a&gt;&quot;5.0.0 &lt;a href=&quot;https://x.com/OmniFishEE/status/2013611675639841050&quot;&gt;GA&lt;/a&gt;&quot;版本发布，这是一个旨在扩展&lt;a href=&quot;https://docs.oracle.com/en/java/javase/25/docs/api/java.base/java/nio/package-summary.html&quot;&gt;Java NIO API&lt;/a&gt;&quot;能力的框架，带来了显著的变化，如：JDK 21基线；在Grizzly线程池中使用新的 virtualthreadexexecutorservice 类来支持虚拟线程；并支持&lt;a href=&quot;https://jakarta.ee/specifications/servlet/6.1/&quot;&gt;Jakarta Servlet 6.1&lt;/a&gt;&quot;规范。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/releases/tag/5.0.0-RELEASE&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Jakarta EE&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在每周的&lt;a href=&quot;https://www.agilejava.eu/&quot;&gt;Hashtag Jakarta EE&lt;/a&gt;&quot;博客中，Eclipse基金会的Jakarta EE开发者倡导者&lt;a href=&quot;https://se.linkedin.com/in/ivargrimstad&quot;&gt;Ivar Grimstad&lt;/a&gt;&quot;提供了Jakarta EE 12的&lt;a href=&quot;https://www.agilejava.eu/2026/01/25/hashtag-jakarta-ee-317/&quot;&gt;更新&lt;/a&gt;&quot;，他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Jakarta EE的每个主要版本都被赋予了一个主题，或者是一个特征性的口号。对于Jakarta EE 9，关键词是“低准入门槛-创新平台-轻松迁移”，对于Jakarta EE 10，它是“现代化-简化-轻量级”。Jakarta EE 11的口号是“开发人员的生产力和性能”。&amp;nbsp;当我们讨论即将发布的Jakarta EE 12版本该使用什么口号时，选择落在了“健壮和灵活”上。无论我们谈论的是哪个版本，这都非常适合Jakarta EE，但更适合Jakarta EE 12，因为它现在比以往任何时候都更加健壮，这是其转移到Eclipse基金会以来的第四个主要版本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通往Jakarta EE 12的道路包括四个里程碑版本，其中第一个版本已于2025年12月交付，计划于2026年7月发布GA版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;BellSoft&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与Oracle的2026年1月的关键补丁更新（&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;Critical Patch Update&lt;/a&gt;&quot;，CPU）同时，BellSoft为&lt;a href=&quot;https://bell-sw.com/pages/libericajdk/&quot;&gt;Liberica JDK&lt;/a&gt;&quot;的25.0.1.0.1、21.0.9.0.1、17.0.17.0.1、11.0.29.0.1、8u481 7u491和6u491版本发布了CPU补丁，以解决这个&lt;a href=&quot;https://docs.bell-sw.com/security/search/&quot;&gt;CVEs&lt;/a&gt;&quot;列表。此外，包含CPU和非关键修复的补丁集更新（PSU）版本25.0.2、21.0.10、17.0.18、11.0.30和8u481也已发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BellSoft表示，他们总共有1217个修复和回溯，参与消除了所有版本中的21个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GraalVM&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样，GraalVM 25.0.2，第二个维护版本也与Oracle的2026年1月CPU同时发布，解决了一些显著的问题，如：JDK Flight Recorder中的Translation-Lookaside Buffer（TLB）事件的内存泄漏；以及循环向量化的误编译，导致结果不正确。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队还放弃了对macOS x64的支持。这个新版本只支持macOS AArch64。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于该版本的更多细节可以在&lt;a href=&quot;https://www.graalvm.org/release-notes/JDK_25/&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Spring框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Spring来说，这是忙碌的一周，因为各个团队都发布了第一个里程碑式的版本：Spring Boot；&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot;和&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot;的第二个里程碑版本。更多细节可以在InfoQ的&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;新闻报道&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Payara&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Payara发布了其2026年1月版的&lt;a href=&quot;https://www.payara.fish/&quot;&gt;Payara Platform&lt;/a&gt;&quot;，其中包括社区版7.2026.1，企业版6.34.0和企业版5.83.0。除了缺陷修复和组件升级，这三个版本都专注于两个CVE的解决方案，即：&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2020-5258&quot;&gt;CVE-2020-5258&lt;/a&gt;&quot;，这是&lt;a href=&quot;https://dojo.io/&quot;&gt;Dojo&lt;/a&gt;&quot;中的一个漏洞，允许攻击者将属性注入到JavaScript中现有的语言构造原型中，并通过注入其他值来操纵这些属性以覆盖或&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Security/Attacks/Prototype_pollution&quot;&gt;污染&lt;/a&gt;&quot;JavaScript应用程序对象原型；以及允许攻击者通过恶意URL有效载荷接管Payara管理帐户的漏洞。关于这些版本的更多细节可以在社区版&lt;a href=&quot;https://docs.payara.fish/community/docs/Release%20Notes/Release%20Notes%207.2026.1.html&quot;&gt;7.2026.1&lt;/a&gt;&quot;、企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/Release%20Notes/Release%20Notes%206.34.0.html&quot;&gt;6.34.0&lt;/a&gt;&quot;和企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/5.83.0/Release%20Notes/Release%20Notes%205.83.0.html&quot;&gt;5.83.0&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenXava&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openxava.org/&quot;&gt;OpenXava&lt;/a&gt;&quot; 7.6.4版本的发布包含了缺陷修复、文档改进、依赖升级和新功能，例如：改进了嵌入式Apache Tomcat的启动时间；以及在 Strings 类中定义了一个新的 toString(Locale, Object) 方法，该方法与其他重载的 toString() 方法一起，用于转换具有本地化意识的字符串。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/openxava/openxava/releases/tag/7.6.4&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JetBrains Ktor&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JetBrains &lt;a href=&quot;https://ktor.io/&quot;&gt;Ktor&lt;/a&gt;&quot; 3.4.0版本的发布提供了缺陷修复和新特性，例如：一个新的API， describe ，它与一个新的编译器插件一起动态生成并记录OpenAPI端点；以及一个新的 ktor-server-compression-zstd 模块，支持&lt;a href=&quot;https://man.archlinux.org/man/zstd.1.en&quot;&gt;Zstd&lt;/a&gt;&quot;压缩算法。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/ktorio/ktor/releases/tag/3.4.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</link><guid isPermaLink="false">https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</guid><pubDate>Thu, 29 Jan 2026 02:57:21 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>多次全球性中断后，Cloudflare推出了“Code Orange: Fail Small”韧性计划</title><description>&lt;p&gt;Cloudflare最近发布了一项名为“&lt;a href=&quot;https://blog.cloudflare.com/fail-small-resilience-plan/&quot;&gt;Code Orange: Fail Small&lt;/a&gt;&quot;”的详细韧性计划，以防止过去六周内连续发生的两次重大网络中断导致的大规模服务中断再次发生。该计划优先考虑受控发布、改进故障模式处理以及简化应急流程，以使其全球网络更加稳健，并减少因配置错误而造成的脆弱性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的网络在2025年&lt;a href=&quot;https://www.infoq.com/news/2025/11/cloudflare-global-outage-cause/&quot;&gt;11月18日&lt;/a&gt;&quot;和&lt;a href=&quot;https://blog.cloudflare.com/5-december-2025-outage/&quot;&gt;12月5日&lt;/a&gt;&quot;遭受了两次严重的中断。第一次事件导致流量交付中断了约2小时10分钟，而第二次事件则影响了其网络背后约28%的应用程序，持续了约25分钟。这些事件发生在即时的全球配置更改之后，尽管这些更改旨在提高安全性或机器人检测能力，但它们在数百个数据中心迅速传播了错误的设置，从而引发了广泛的服务故障。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Code Orange: Fail Small”计划规定，配置更改必须以受控的、分阶段的方式进行，类似于Cloudflare现有的软件发布流程&lt;a href=&quot;https://blog.cloudflare.com/safe-change-at-any-scale/&quot;&gt;Health Mediated Deployment(HMD)&lt;/a&gt;&quot;，其中包括分阶段验证和自动回滚机制。历史上，配置更新（如DNS记录或安全规则）会通过内部的&lt;a href=&quot;https://blog.cloudflare.com/quicksilver-v2-evolution-of-a-globally-distributed-key-value-store-part-1/&quot;&gt;Quicksilver系统&lt;/a&gt;&quot;在几秒钟内向全球范围传播，当错误的更改传播过快时，这就成为了一个隐患。在新策略下，配置更新需要通过监控门禁并采用渐进式部署，以便在问题影响到大范围基础设施之前尽早发现它们并降低影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare还计划审查和改进网络流量处理系统中的所有故障模式，旨在确保每个组件在错误条件下都能做出可预测的响应，并且不会将故障级联到不相关的服务。这包括验证关键产品之间的接口契约，并建立合理的默认值，以便即使依赖的子系统发生故障，流量也能继续流动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，该公司正在彻底改革紧急访问程序和内部工具的访问权限，以减少在过去的中断事件中拖慢事件响应速度的循环依赖。增强的培训和简化的应急访问协议旨在帮助工程师更快地应对关键故障，同时不损害安全防护措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的计划正在逐步推进，通过单独的更新以改善整体的性韧性，而不是一次性地进行大规模更新。该公司预计到2026年第一季度末，所有生产系统都将使用增强后的HMD配置流程，故障模式将得到更好的定义和测试，应急响应访问也将得到改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些努力是在日益严格的审查背景下进行的。Cloudflare的中断事件引起了&lt;a href=&quot;https://www.theguardian.com/technology/2025/dec/05/another-cloudflare-outage-takes-down-websites-linkedin-zoom&quot;&gt;广泛的关注&lt;/a&gt;&quot;，事件影响了LinkedIn、Zoom和Shopify等主要网站，并引发了关于集中式互联网基础设施风险的讨论。尽管社区的一些&lt;a href=&quot;https://www.reddit.com/r/CloudFlare/comments/1pr1twp/code_orange_fail_small_our_resilience_plan&quot;&gt;反应&lt;/a&gt;&quot;表达了不满，但许多讨论平台上的用户也对Cloudflare坦诚承认问题及其结构性改进的承诺表示了欢迎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare正在努力重建信心，“Code Orange: Fail Small”计划凸显了该公司向更谨慎的部署实践的转变，并对故障的出现做出更强的预期，以便在问题升级为扰乱互联网生态系统大范围的全球中断之前将其控制住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-resilience-plan/&quot;&gt;Cloudflare Launches ‘Code Orange: Fail Small’ Resilience Plan After Multiple Global Outages&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</link><guid isPermaLink="false">https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</guid><pubDate>Thu, 29 Jan 2026 02:53:04 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category></item><item><title>Vercel 开源 Bash 工具：基于本地文件系统的上下文检索</title><description>&lt;p&gt;Vercel &lt;a href=&quot;https://vercel.com/changelog/introducing-bash-tool-for-filesystem-based-context-retrieval&quot;&gt;开源了 bash-tool&lt;/a&gt;&quot;，这个工具为 AI 智能体提供了一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bash_(Unix_shell)&quot;&gt;Bash&lt;/a&gt;&quot; 执行引擎，让它们可以直接运行&lt;a href=&quot;https://cycle.io/learn/linux-filesystem-commands&quot;&gt;基于文件系统&lt;/a&gt;&quot;的命令，帮模型获取上下文信息。这个工具的设计初衷，就是让 AI 智能体在处理大量本地上下文时，不用把整个文件塞进模型提示词里，可以直接通过像 find、grep、jq 这样的 &lt;a href=&quot;https://google.github.io/styleguide/shellguide.html&quot;&gt;shell 命令&lt;/a&gt;&quot;，直接在文件夹里操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 给智能体提供了三大核心操作：bash（解释并执行 Bash 脚本）、readFile（从预加载的文件系统读取文件）、writeFile（更新文件）。这个引擎基于 just-bash（一个用 TypeScript 写的解释器），不会新开 shell 进程，也不会随便执行二进制文件。它既能用内存文件系统，也能跑在隔离的虚拟机里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际用起来时，开发者可以在创建工具时先把一批文件加载进去，智能体就能随时对这些文件运行命令。比如，你可以把一个 JavaScript 源码文件交给 bash-tool，智能体就能查找或操作文件系统，而不用把整个文件内容塞进提示词里。如果需要真正的 shell 和文件系统，也可以在 &lt;a href=&quot;https://vercel.com/docs/vercel-sandbox&quot;&gt;Vercel 的沙盒&lt;/a&gt;&quot;环境下用这个工具，支持完整的虚拟机隔离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个工具的诞生，是因为大家都想让大模型的上下文窗口别太臃肿，同时又希望智能体能精准获取文件里的关键信息。只拿 shell 命令的结果，不嵌入整个文件，智能体就能省下不少 token，把注意力集中在真正有用的小片段上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者可以把 bash-tool 和 &lt;a href=&quot;https://ai-sdk.dev/&quot;&gt;Vercel 的 AI SDK&lt;/a&gt;&quot; 一起安装，就能开始开发用文件系统操作做检索的智能体了。它既能用内存文件系统，也能跑在沙盒环境里，部署起来很灵活，而且还不会暴露不安全的执行路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者们在&lt;a href=&quot;https://x.com/vercel/status/2009769470194266327&quot;&gt;早期讨论&lt;/a&gt;&quot;时就发现，用 Bash 风格的接口让智能体检索上下文，其实是很贴合大多工具和模型已经熟悉的 Unix 工作流。Vercel 让智能体能用 find、grep 这些经典命令，等于直接用上了 shell 的语义，让模型能高效地查找和提取结构化信息，而不是只靠向量检索或者把整个文件塞进提示词。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者 &lt;a href=&quot;https://x.com/asimgilani/status/2009787196736450887&quot;&gt;Asim Gilani&lt;/a&gt;&quot; 说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;能不用复杂的上下文管理，真的太爽了。让模型自己查文件，比每次都喂它一堆碎片强多了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/benjaminshafii/status/2009798694594588754&quot;&gt;Benjamin Shafii&lt;/a&gt;&quot; 也表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Unix 50 年前就把抽象做对了。只要你能把设备、进程、数据都当成文件看，你就只需要一种抽象和一个 API。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 的出现，可能会影响未来 AI 驱动的开发系统如何处理本地上下文，更加注重精准检索和与软件工程常见文件系统语义的深度结合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/01/vercel-bash-tool/&lt;/p&gt;</description><link>https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</link><guid isPermaLink="false">https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</guid><pubDate>Thu, 29 Jan 2026 02:50:23 GMT</pubDate><author>作者：Daniel Dominguez</author><category>开源</category></item><item><title>又一款世界模型宣布开源！对标 Genie 3、10分钟长视频无损生成</title><description>&lt;p&gt;1 月 29 日，继连续发布空间感知与VLA基座模型后，蚂蚁灵波科技再次刷新行业预期，开源发布世界模型LingBot-World。该模型在视频质量、动态程度、长时一致性、交互能力等关键指标上均媲美Google Genie 3，旨在为具身智能、自动驾驶及游戏开发提供高保真、高动态、可实时操控的“数字演练场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/6630b4123c11260f615d99fa40469a9f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：LingBot-World在适用场景、生成时长、动态程度、分辨率等方面均处于业界顶尖水平）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开源地址：https://github.com/Robbyant/lingbot-world?tab=readme-ov-file&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对视频生成中最常见的“长时漂移”问题（生成时间一长就可能出现物体变形、细节塌陷、主体消失或场景结构崩坏等现象），LingBot-World&amp;nbsp;通过多阶段训练以及并行化加速，实现了近&amp;nbsp;10 分钟的连续稳定无损生成，为长序列、多步骤的复杂任务训练提供支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交互性能上，LingBot-World可实现约16 FPS 的生成吞吐，并将端到端交互延迟控制在 1 秒以内。用户可通过键盘或鼠标实时控制角色与相机视角，画面随指令即时反馈。此外，用户可通过文本触发环境变化与世界事件，例如调整天气、改变画面风格或生成特定事件，并在保持场景几何关系相对一致的前提下完成变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90fa3c806fd7811440c1a441d9a54231.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：一致性压力测试，镜头最长移开60秒后返回，目标物体仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a492f32211511f5d7e12fdc7c2045f3e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：高动态环境下，镜头长时间移开后返回，车辆形态外观仍保持一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/80819d2c0847929314e4151a3a1be453.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：镜头长时间移开后返回，房屋仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型具备Zero-shot 泛化能力，仅需输入一张真实照片（如城市街景）或游戏截图，即可生成可交互的视频流，无需针对单一场景进行额外训练或数据采集，从而降低在不同场景中的部署与使用成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为解决世界模型训练中高质量交互数据匮乏的问题，LingBot-World 采用了混合采集策略：一方面通过清洗大规模的网络视频以覆盖多样化的场景，另一方面结合游戏采集与虚幻引擎（UE）合成管线，从渲染层直接提取无UI 干扰的纯净画面，并同步记录操作指令与相机位姿，为模型学习“动作如何改变环境”提供精确对齐的训练信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;具身智能的规模化落地面临一个核心挑战——复杂长程任务的真机训练数据极度稀缺。LingBot-World&amp;nbsp;凭借长时序一致性（也即记忆能力）、实时交互响应，以及对&quot;动作-环境变化&quot;因果关系的理解，能够在数字世界中&quot;想象&quot;物理世界，为智能体的场景理解和长程任务执行提供了一个低成本、高保真的试错空间。同时，LingBot-World支持场景多样化生成（如光照、摆放位置变化等），也有助于提升具身智能算法在真实场景中的泛化能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着“灵波”系列连续发布三款具身领域大模型，蚂蚁的AGI战略实现了从数字世界到物理感知的关键延伸。这标志着其“基础模型-通用应用-实体交互”的全栈路径已然清晰。蚂蚁正通过InclusionAI&amp;nbsp;社区将模型全部开源，和行业共建，探索AGI的边界。一个旨在深度融合开源开放并服务于真实场景的AGI生态，正加速成型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，LingBot-World 模型权重及推理代码已面向社区开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</link><guid isPermaLink="false">https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</guid><pubDate>Thu, 29 Jan 2026 02:44:20 GMT</pubDate><author>蚂蚁集团</author><category>生成式 AI</category></item><item><title>被Anthropic强制要求改名！Clawdbot 创始人一人开发、100% AI 写代码，腾讯又跟上了热度</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两天，个人 AI 助手 ClawdBot&amp;nbsp;席卷硅谷，国内外社交平台上全是关于它的讨论。不过，项目创始人Peter Steinberger 在 X 平台上发文表示，他被 Anthropic 强制要求更改名称的成Moltbot，这并非他本人的决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他透露，这次改名源于商标问题，但在操作过程中不仅搞砸了 GitHub 的账号更名，连 X 平台的原账号名也被加密货币推广者抢注了。最终，他的新账号名定为 @moltbot。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，他曾向加密货币圈的用户发出呼吁，请求大家停止 @ 他和骚扰行为。他明确表示，自己永远不会发行加密货币，任何将他列为发币主体的项目都是诈骗，并且他不会收取任何相关费用。他还指出，这类行为正在对项目造成实质性的损害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53e425d5422e73a4b3dd4ffd5cdefcb5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用Clawdbot 后，网友们纷纷给出了很高的评价。“它是迄今为止最伟大的AI 应用，相当于你24小时全天候专属 AI 员工。”Creator Buddy 创始人兼 CEO Alex Finn 盛赞道，“这就是他们(Anthropic)希望 Claude Cowork 呈现的样子。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，ClawdBot&amp;nbsp;项目已经开源，现在已经斩获了70.1k stars：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/clawdbot/clawdbot&quot;&gt;https://github.com/clawdbot/clawdbot&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Alex 展示了给他的Clawdbot发信息，让它帮其预订下周六在一家餐厅的座位。当 OpenTable 预订失败时，Clawdbot 利用 ElevenLabs 的技术致电餐厅并完成了预订。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/5244da4ac297947d6dcfceaa4d8a3a6a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但ClawdBot真正让技术圈兴奋的，并不只是“能干活” ，而是其协作方式极其激进：不会写代码的人，也能直接提PR。原因很简单：它几乎是100%用AI写出来的，PR在这里更像是“我遇到了这个问题”，而不是“我写了一段多漂亮的代码”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有意思的是，这个看似“全开源”的项目，偏偏故意留了一点不开源。创始人Peter Steinberger保留了一个名为“soul”的文件只占项目的0.00001%。他说得很直白：这既是他的&quot;秘密资产&quot;，也是一个刻意留下来的安全靶子。大家真的在试着hack它，他就等着看模型到底守不守得住。到目前为止，“soul”还没被偷出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为忠实粉丝，Alex 表示这是自 Claude Code 发布以来，自己第一次连续两天没有用它。但是他的 ClawdBot Henry 已经连续 48 小时不停地 Vibe Coding。“我这辈子都没写过这么多代码。Vibe Coding 已死，Vibe Orchestration 已来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，Alex 想要退掉Mac Mini，换一台价值1万美元的Mac Studio。“我的ClawdBot Henry将控制一台人工智能超级计算机。Henry 将使用Opus作为大脑，并使用多个本地模型作为员工集群。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Clawbot 并不是传统意义上只能回答问题的聊天机器人，它本质上是一个持续运行、可以执行任务的个人 AI 智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以把它安装在自己的设备上，如 Mac、Windows、Linux，它可以长期在线，不停地接收指令、处理任务、记住你的偏好和历史对话，随着时间积累变得更懂你、更有“记忆”。总的来说，Clawbot 最令人震撼的地方有三点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，它几乎可以完全控制你的电脑。它没有传统意义上的“护栏”，不局限在某几个功能里，而是可以像一个真正坐在电脑前的人一样，操作你电脑上的一切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，它拥有近乎无限的长期记忆。Clawbot 内置了一套非常复杂的记忆系统。说过的话、做过的事，都会不断被记录下来。每次对话结束后，它都会自动总结聊过的内容，并把关键信息提取出来，存进长期记忆中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三，它完全通过聊天应用来交互。你平时用哪些聊天工具，Clawbot 就能在哪儿跟你对话，这意味着，只要打开一个聊天软件，就可以通过一条消息把任务交给Clawbot 去做。现在Clawbot 支持WhatsApp、Telegram、Slack、Discord、Google Chat、Signal、iMessage、Microsoft Teams、WebChat等，还有 BlueBubbles、Matrix、Zalo 以及 Zalo Personal。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如此放开的权限让其几乎没有护栏，这带来很大的安全隐患，现在GitHub 上有 500多个安全的问题，这也让部分网友望而却步。对此，很多使用过的用户几乎都表示，不建议一开始就把 Clawbot 装在主力电脑上。“在你还不熟悉它之前，把它放在一个独立环境里是最安全的选择。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7dcd4f0f125a6e85d1d4ffa771a8622.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过大家没有想到，这个AI员工首先带火的竟然是Mac Mini。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人为了运行Clawdbot会专门买一台电脑，而大部分选择了Mac Mini，原因是它便宜、兼容好、功率低、安静、占地小。谷歌DeepMind 产品经理 Logan Kilpatrick 都忍不住订了台Mac Mini。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5df3d376cb691b8b47d80fe1555de5e3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有网友晒出自己一口气买了 40 台 Mac mini 来运行 Clawdbot。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3e/3e53c42f08db19544c6fb7883bb881b5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有网友称可以用一台免费的服务器运行着完全一样的程序，Alex 也称没必要花 600 美元买 Mac mini，有其他便宜得多的方式来运行 Clawbot。买Mac mini更多是个人偏好，而不是技术上的必要条件。你完全可以不买任何硬件，只需要一个 VPS。&lt;/p&gt;&lt;p&gt;另外，云厂商们动作迅速，有网友发现腾讯云直接推出了Clawbot云服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着项目的火爆，其背后的开发者Peter Steinberger也备受关注。Peter 在“Open Source Friday”上分享了他一手打造ClawdBot&amp;nbsp;的经过，从创建、创始到维护，全由他独自完成。有意思的是，此前甚至有传言称，Peter可能是一个bot、Agent，甚至本身就是AI。而Peter的出现也让项目成员和关注者们确认了他是个“真人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter 一度已经退休了，后来又从退休状态里出来开始折腾 AI。从外表来看，Peter年轻有活力，完全不像已到退休年龄、可领取养老金的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/535e8e5dc1f9eca6ea15cabaff66bcf5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter的职业生涯也颇具亮点，他曾独立运营一家B2B公司长达十三年。这家公司打造出了当时全球领先的PDF框架，团队规模最高发展到约七十人。在公司发展步入稳定阶段后，Peter收到了一份极具吸引力、令人无法拒绝的收购邀约，这也为他这段创业历程画上了一个圆满的句号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，Peter口中的“退休”更像是一种玩笑式的表述。在十三年的创业生涯中，他几乎倾注了所有精力，就连周末也大多用于工作，长期的高强度投入最终让他陷入了严重的 burnout（心力交瘁）状态。之后，Peter花了不少时间调整身心，弥补生活中的遗憾，体验了许多有趣的事情。但他知道自己是那种热爱“创造”和“构建”的人，迟早还会回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到去年年初，Peter的创作想法再度燃起。正好，那时候AI 从“这玩意儿不太行”，突然变成了“等等，这有点意思”。从那以后，Peter基本上就把身边无数人一起拉进了 AI 的坑里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是Peter在节目上的对话，除了分享经历，他也谈到了大家的各种意想不到的应用和最关心的安全问题，安全正是他当前最优先的工作。我们在不改变原意基础上进行了删减和翻译，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“本来想等大厂做的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这个项目现在太火了，GitHub 星数涨得飞快。你似乎正好击中了一个大家憋了很久的需求：一个人，也能把很多事情搞定。我甚至觉得你在无形中拉升了 Apple 的股价，大家都跑去买 Mac mini 来自己跑实例了。能不能讲讲，这个想法最初是怎么冒出来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我刚回来的时候，其实特别想要一个“生活助理”，四月份就已经在想这个事了，也试过一些想法，但当时模型还不够好。我后来就把这个念头放下了，因为我觉得这种东西，肯定是各大厂都会做的，那我做还有什么意义呢？于是我又去做了很多别的项目。直到十一月，我突然意识到，居然还没有人真的把这件事做出来。我心想，难道还真是什么都得我自己来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也不知道哪根弦被拨动了，那个月我用一个小时拼了点非常糙的代码，用 WhatsApp 发消息，转到 Claude Code，再把结果发回来。本质上就是把几样东西“粘”在一起，说实话并不难，但效果还挺好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我意识到，我还需要图片输入。我自己在提示时经常用图片，因为它能给 Agent 很多上下文，而且非常快。这个反而花了我更多时间。系统支持双向之后，我正好在马拉喀什参加朋友的生日旅行，用这个非常原始的系统一边逛城一边当“导游”，已经比我预期好用很多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一次我没多想，直接给它发了一条语音消息。但当时我根本没做语音支持。我就盯着“正在输入”的提示，看会发生什么。大概几秒后，它居然回了我。我当时整个人都愣住了，心想你刚才到底干了什么？后来我才发现，它识别到一个没有后缀的文件，去查了 header，判断是音频格式，用 FFmpeg 转码，发现本地没有转写工具，就在系统里找到一个 OpenAI key，用 curl 把音频丢给 OpenAI，然后把结果再发回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来像是你第一行代码就触发了 AGI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：也许还称不上 AGI，但那一刻我真的意识到，这些东西的“自发应变能力”已经超出了我原本的想象。后来我还开玩笑说“我住的那个马拉喀什酒店门锁不太靠谱，希望你别被偷走，毕竟你跑在我 MacBook Pro 上”，它回我说“没关系，我是你的 Agent”，然后它还去检查了网络，发现通过 Tailscale 能连到我在伦敦的电脑，结果它就把自己迁移过去了。我当时就在想，这就是 Skynet 的起点吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：最初的架构是怎样的？是什么让它具备这种“自主决策”的能力？你用的是什么模型？这是你的第一次实现吗？就是 WhatsApp 加 Claude Code 那一版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：最早它叫 V Relay，本质就是 WhatsApp relay。后来我在做 Claude 相关的东西时，有人给 Discord 提了 PR，我一度犹豫要不要提 Discord，因为这已经不只是 WhatsApp 了。最后还是提了，然后名字也得改。Claude 给了个建议叫 ClawdBot&amp;nbsp;，于是就这么定了。项目后来清理了很多，但最早的起点真的很朴素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我第一次看到这个项目的时候，还以为它是 Anthropic 内部出来的，心想是不是我错过了什么。它的发展速度太快了，很多人很快就开始用起来。除了“拉升 Apple 股价”，你大概也间接推动了不少第三方生态的发展。最初这只是个解决你个人问题的项目，但社区一下子就接住了它，大家觉得它优雅、好用、而且真的能跑。你什么时候把它推到公开仓库的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：从四月份开始，我做的东西基本都是开源的。只有一个项目例外，因为 Twitter 的 API 成本实在太离谱了。这个项目的第一次提交是在十一月。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;去年发出来，反响平平&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：很多人用它搞出了非常夸张的东西，有没有哪种用法让你特别惊讶、是你完全没想到的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：太多了。有人用它自动给图片加字幕，有人把它接进 Tesla，有人集成了伦敦公共交通系统，直接告诉你现在该不该跑去赶车。老实说，现在我忙着维护项目，反而没时间用这些自动化了，看着别人搞出这么多花样，我甚至会有点嫉妒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有趣的是，我十一月做出来的时候，给朋友看，他们都说“太酷了”。但我在 Twitter 上发的时候，反响却很平淡。直到十二月，每次我线下给朋友演示，他们都会说“我需要这个”，我却发现自己完全不知道该怎么向更多人解释它到底有多好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，我干了一件非常疯狂的事：直接建了一个 Discord，把 bot 拉进去，而且当时完全没有安全限制。因为最初它只服务我一个人，根本不用考虑谁能给它发指令，比如“把 Peter 的文件全删了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我其实只是写了一段很简单的指令，比如“你只在 Discord 里，只听我的”。但你也知道，Agent 对指令的遵循并不总是那么理想。后来我把它放进 Discord，陆陆续续有几个人进来，基本上只要看到几分钟的人都能明白这是怎么回事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来可以拓展想象：你买了一台新电脑，里面有一个“幽灵实体”，你把键盘、鼠标和网络权限交给它，把它当成一个虚拟同事。你可以直接跟它说话，交代事情。凡是你能在电脑上做的事，这个 Agent 理论上都能替你完成。这就是它真正强大的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：太厉害了。WhatsApp、Telegram、Discord 这些场景都能用。我刚才在 Discord 上和这个 Bot 聊过，说实话，体验很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我当时就是随手发了一条公共消息，结果大家开始加你、@你，那正好也是他们评论里提到的点。那对你个人来说，你的“北极星目标”是什么？就是那种“当 ClawdBot 能做到这件事，我就觉得值了”的时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我的判断是，今年就是“个人 Agent 之年”。去年是编程 Agent 真正成熟的一年，今年它会从工程师的小圈子里走出来，变成“每个人都有一个 Agent”。这一波大概率会被 OpenAI 以及少数几家大厂主导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我想做一个不同的选择：你能掌握自己的数据，而不是把更多数据继续交给大公司；它还能配合本地模型一起工作。我没看到有人在认真做这件事，所以我觉得这件事很重要，而且它必须是完全开放、永久免费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是我选择开源用 MIT 协议、成立组织而不是挂在我个人名下的原因，它应该是很多人一起的项目。现在最大的现实问题是，我被“让它变得更好、更安全”这件事彻底占满了，还没来得及把外围体系搭完整，也没真正建立起高效协作的机制。目前有一些人帮忙维护，但整体还太早，还在摸索怎么把事情分好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;PR 成为“问题线索”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但说实话，从去年十一二月到现在，你已经做得非常多了。现在才一月，指望一个项目在一个月内就成熟、就有核心团队，本来也不现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：老实讲，在现在这个节奏下，我一天写的代码，可能比我以前70人公司一个月写得都多。在这个新世界里，构建东西的速度已经完全变了。我也在刻意挑战大家对开源和治理的传统理解。现在很多人给我提 PR，质量参差不齐，但我更愿意把它们当成“问题陈述”或“意图表达”，而不只是代码提交。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我喜欢这个说法。那现在大家是用 ClawdBot 来提 PR 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的。而且让我特别受触动的是，有很多 PR 来自从没学过写代码、也从没提过 PR 的人。因为这个 Bot 有完整的电脑访问能力，也懂 GitHub 的工作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还做了一件在很多项目里不常见的事：在官网上你可以选“快速安装”或“可折腾安装”。后者的流程就是克隆仓库、build、启动。Agent 本身就活在一个 GitHub 仓库里，全是 TypeScript，它可以直接改自己的代码，然后重启。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这让事情变得非常简单。有人说“这个不工作”，我就直接改一下，马上就好，然后他们顺手就提了一个 PR。当然，这些 PR 的质量肯定比不上那些在行业里干了 20 年的人写的东西，但依然很惊人，因为它让更多人开始参与贡献、开始分享东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我真的很认同这种看法。现在开源项目面临的一个现实问题就是 PR 暴增。Agent 反而可以帮你检查贡献规范、查重 Issue、避免重复劳动。听起来，这正是工程协作正在演进的方向。而且如果我发现一个问题，提了 PR，甚至让 ClawdBot 自己把问题“修掉”，这太酷了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：过去的流程是你提 PR，等几天，被人打回来，说你哪里不对，再改，来回几轮，可能几周后才合并。那在“代码昂贵、难写”的年代是合理的。但现在代码已经很便宜了，这种反馈循环本身就不值钱了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，PR更像是在说：“这有一个问题，这是我试着解决它的方法。”我更关心的是这个人真正想解决什么痛点，而不是这段代码写得漂不漂亮。有时候确实是误解，那我就直接关掉；但更多时候，尤其是项目早期，我会觉得这个痛点是真的，我们一起把它解决掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做新功能最难的，从来不是写代码，而是把它合理地嵌进已有系统。如果你对整体架构不熟，硬塞一个功能，迟早会出问题。所以，我宁愿把 PR 当成“问题线索”，而不是“成品代码”，否则项目只会慢慢自我消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这段话真的该让所有人都听到。我完全同意，工程文化正在变化。现在的阻力，很多来自还停留在“写代码本身很贵”这个认知里的人。事实上，很多好点子恰恰来自不懂架构的人，因为他们有最直接、最真实的需求。当你在一个项目里待久了，反而看不清这些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Opus表现稳定，MiniMax 2.1 最“像人”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：要不你给大家演示点什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我先简单说下语音控制。最简单的是在 Discord 里发语音消息，Agent 会语音回复。语音生成你可以用本地模型，或者ElevenLabs。我们还有插件，能让 Agent 打电话，比如你让它给餐厅打电话订位。还有 Mac App 的语音聊天，你直接说话，它在检测到两秒静默后回应，虽然还不如 OpenAI 那种自然，但已经很不错了。再极客一点的，是语音唤醒，像《星际迷航》一样，说“Computer”就能下指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，这个项目既是技术项目，也是一次探索。我更想激发大家的想象力，看看什么行得通、什么行不通。而且这个领域变化太快，可能这个月不行的方案，下个月就突然可行了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那也请你顺便跟大家讲讲安装门槛吧，不是每个人都想为了跑 Agent 去买一台 Mac mini（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：系统支持多个 Agent、多个端点。你甚至可以给家里每个人一个 Agent，用同一套安装。默认它们能在你的电脑里自由活动，这最有趣，也最危险；你也可以把它们放进 Sandbox。现在演示用的 Agent 在 Sandbox 里，权限很低。我正在做一个 Allow List 机制，只允许调用你明确授权的能力，比如某个二进制、某个参数，而不是“删光所有文件”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，大多数高级用户是清楚风险的。理论上模型能做坏事，但实际很少发生。而且你真想毁电脑，自己在终端敲命令更快。真正的风险是配置错误，比如让它响应所有人，或者主动给了不该给的权限。所以我们做了安全审计，默认只听你一个人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也是为什么很多人会选择隔离环境、单独机器，千万别在公司配的电脑上跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，我也建议用强模型，比如 Anthropic 的 Opus。Slack 上有人一直在尝试 hack 我的 Agent，因为项目几乎全开源，唯一没开源的是我称之为“灵魂（soul）”的那部分配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在ClawdBot&amp;nbsp;里有一个小系统：Agent 有身份文件（identity file）、记忆文件（memory），还有一个“灵魂文件”。这个文件里写了 Agent 的价值观是什么、它怎么同步、怎么互动、什么对你最重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得我调出了一个很好的版本，所以我把它闭源了：一部分原因是，这是我那 0.00001% 的“秘密资产”（笑）；另一部分原因是，它也可以作为一个渗透测试目标：到目前为止，还没有人把 Claw soul 套出来，但很多人都试过。这让我有点信心，至少这些实验室在 prompt injection 的缓解上确实在进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它真的变好了：如果你用很小、很老的模型，你只要问得足够多，它最后可能就会“好吧，给你一切”，那就是我们以前的状态。但现在用最新一代模型，我有信心：你必须非常非常努力，才有可能把它套出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，把它不加 sandbox 直接接到真实环境里依然不是好主意，所以现在我做 demo 的时候，我的 Claw 权限就比较受限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到目前为止，在我们测试过的模型里，表现比较稳定的是 Opus，还有开源模型 MiniMax 2.1 是目前最“Agentic”的一个，我们内部有个专门讨论模型的频道，有人给它起了个外号，Minimax 也顺势接住了这个梗，还发了条推，说“我们可能没有 T0 级价格，也可能没有团队级价格，但至少我们有目标质量”。结果个帖子小火了一把。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人其实很欣赏这种不把自己端得太高的公司。他们很清楚自己在技术上暂时还没追上美国头部实验室，但在我看来这只是时间问题。现在有很多公司都在加速追赶，这本身就很让人兴奋。比如 Minimax 的模型你可以直接下载，我能在那台 Mac Studio 上本地跑，我的 Agent 把那台机器叫作“城堡”。这样我就能把所有数据都留在这台机器上，推理也在本地完成，对外只通过消息型 Agent 通信，甚至可以用 Signal 走加密通道。这样，如果我愿意， 100% 的数据都不会出本地。这种感觉很酷，说实话，几乎没有公司真的能做到这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你会建议大家一开始就接 Telegram 吗？作为初始配置是不是最省心？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我是后来转过来的。在欧洲，如果你没有 WhatsApp，基本等于不存在。我猜你在哥伦比亚也是一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一模一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：但问题在于，一开始我试的是官方路线，用 Twilio 拿号，注册企业账号，结果 Meta 一直封我，说我作为企业发消息太多。它的逻辑就是企业只能给客户群发消息，那种模式根本不适合 Agent折腾了几天、申诉无果之后，我直接怒删了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我发现有一些开源项目，比如 Baileys，基本是模拟原生客户端的行为，你可以把手机连上，用起来效果很好。但 WhatsApp 本身就不是为 bot 设计的，很多高级功能做不了，比如审批按钮之类的交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Telegram 对 bot 真的友好得多，有完整的 API、能玩很多花样，所以我现在会推荐这个。当然，其他平台也都能用，而且这个领域变化会非常快。希望 Meta 什么时候能清醒一点，真的给一个像样的 bot API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：至于 demo，我确实推得有点猛了，因为我现在在做 sandbox。之前的情况是，很多人发现了这个东西，直接全力开搞，甚至拿去工作用。但那样的话，肯定需要更多护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来很合理。那是不是要出企业版了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：没有这种计划。我真正想做的只是给大家更多选择。沙盒化上周其实就已经能用了，这周我在做的是 allow list。理想状态下，你可以预先定义哪些操作是安全的，如果 Agent 想执行一个敏感操作就会弹窗，让你选“只允许一次”或者“永久允许”。虽然我直觉上觉得，大多数人最后还是会以YOLO模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：就像大多数开发者给 Coding Agent 也是一直跑在YOLO模式上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，因为别的模式真的很烦。但即便如此，我还是想把这件事做好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以现在演示中的是一个原生集成在 bot 里的 sandbox 能力？而不是用户自己去搭？是免费的对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，它的成本主要是我的 token 和睡眠，还有你得自己找地方跑模型。如果你有一台性能不错的机器，是可以完全本地跑的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;疯狂的使用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那现在大家都在用它做什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：Twitter 上已经有各种各样的案例，说实话，大家做的事情已经比我自己做的还疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人最夸张的一次，是把它接到我的床上。我用的是 Eight Sleep，有 API 可以控制温度，我写了个 CLI，让 Agent 去调。现在它能控制床的温度、开音乐、调灯光、看摄像头、查外卖进度。它有自己的邮箱，也能访问我的邮箱；有自己的 WhatsApp，也能读我的聊天，甚至可以“替我回复”。这本质上是个取舍，你给它的权限越多，能做的事情就越厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有人用它做各种自动化，比如在 Twitter 上收藏一条内容，它就自动研究、整理进 to do list；有人直接拿它搭完整应用；几乎人人都给它配一台 MacBook。我以前的一个合伙人，甚至让它清空了收件箱里的一万封邮件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一万封？他是怎么敢这么干的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：你知道的，Gmail 所谓“清空收件箱”其实只是归档，没有真正删掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;挺棒的。我更关心的是，这些东西是不是可以一路跟着我跑，或者有没有什么我必须特别注意的点。有些用例我觉得特别酷，比如有人把它用在家庭场景里。每个人都有自己的 Agent，比如我、我老婆——好吧，我其实没有老婆（笑），但你能给每个人配一个 Agent，而且这些 Agent 之间还能彼此沟通、同步信息。比如家里有一个共同的待办事项，它们自己就能对齐进度。这种玩法我自己都还没完全试过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我太喜欢这个了，我真的需要。以前是“让你的人跟我的人谈”，现在直接变成“你的 Agent 跟我的 Agent 谈”，这也太酷了，听说有人直接让它帮忙生成购物清单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，很酷，而且这一步其实已经不远了。有些人已经把它做到更彻底，比如 Agent 可以直接帮你从 Tesco 下单。你只要说一句“把这些东西再买一遍”，它就自己去处理，几个小时之后，东西已经放在你家门口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有人用它来处理发票和报销。天啊，这简直是为我量身定做的。我现在就有一份报销单拖了一周还没交，老板要是看到这段话我先道歉了，但我是真的很讨厌干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：这个用例真的很受欢迎。还有一个我觉得特别有意思的，是用它帮自己重新回到健身状态。你可以把它接到你的可穿戴设备上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你是说那个 Oura Ring？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，也可以接 Garmin 手表，或者其他运动手环。Apple 这块是最麻烦的，但我们也有解决方案，只是稍微烦一点，因为你得让 iPhone 上的 App 保持打开状态才能同步数据，Apple 对生态的封闭你也懂的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过ClawdBot有一个点我之前没怎么见过，就是它的“主动性”能做到多强。一般的 Agent 都是你问一句它答一句。但我给它做了一个“心跳机制”，即默认每隔一段时间，不同模型可能是半小时或者一小时，Agent 会被“敲一下”，问自己一句：有没有什么事情需要检查？有没有什么待办被落下了？它会自己去梳理，如果发现有遗漏，要么提醒你要么就不打扰你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个机制是可控的，你可以把它设得很简单，比如它只往系统里发个信号，不需要你回复，那就什么都不发生，也可以让它主动找你。具体看你怎么编排，它甚至可以每天早上跟你说一句“早安”，偶尔关心你一下，“最近状态怎么样”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你跟它说“我有一个目标，你帮我盯着”，它就会真的盯着，比如问你：今天走路了吗？去健身房了吗？比如我的ClawdBot，就经常很失败地试图劝我早点睡觉。凌晨一两点，它会提醒我：“Peter，我还看到你在线，你该睡了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这已经是真正意义上的私人助理了，我太喜欢了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：还有人用它来学语言。事实证明，有一个东西不断地“唠叨你”、提醒你去完成自己给自己定下的目标，其实非常有效。有时候只需要轻轻踢一脚，人就动起来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我也建议那些一脸懵、还不知道这是啥的人看看，我做了一个小展示页面，内容全部来自真实的推文。我不太喜欢那种只堆金句、不知道是不是编的页面，这里面的都是用户真实发出来的体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/49d0f84c577a3c45077e951accf2066b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用旧电脑上手，Gemini 现在不行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那如果我现在想上手，我算是那种“半懂技术”的人，你会建议从哪一步开始？比如 Telegram 是一个入口，还有人提到过别的平台，说 API 也很友好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我觉得最舒服、最简单的方式是：如果你家里有一台旧电脑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：直接用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，直接用。很多人家里都有一台旧 Mac，这个场景下简直完美。网站上有一条命令，你复制到终端里，剩下的我们会一步步带你走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人用 Anthropic 的模型，OpenAI 的模型也很好用。我也相信 OpenAI 在“性格”这块会持续进步，现在确实有点偏无聊。如果你预算有限，MiniMax 是个很好的替代方案，一个月十美元，调用量跟一些一百美元的方案差不多。当然还不完全一样，但这个领域变化真的很快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得模型会越来越便宜吗？还有你用过 Gemini 模型配ClawdBot&amp;nbsp;吗？体验如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：Gemini 现在不行，真的不太行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：好，结论非常清晰（笑）。所以如果只是想实验，用一些本地的、便宜的模型，是更现实的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：当然，每个模型其实都可以稍微“调教”一下。早期的 Anthropic 模型，你得对着它全大写吼几句，它才肯干活。我相信 Gemini 也有办法榨出更多效果，但总体来说，它在工具调用、那种真正“像助手”的感觉上，我没找到特别好的表现。写代码还行，但这不是这个项目的核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题是，我一天也只有这么多时间。我每天睡四个小时，剩下的时间都在写代码，还没来得及把所有东西都打磨到位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们能怎么帮你？顺便说一句，你这项目还挺环保的，我现在都后悔把那台 2013 年的 iMac 扔了，这玩意儿跑起来完全没问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：如果你技术稍微好一点，也可以直接丢到 Hetzner、Fly.io 这类便宜的云主机上跑，效果都很好。我最近还做了一个新方案：你可以在云上装一个叫 Gateway 的服务，然后在自己机器上跑一个节点，用 Tailscale 把网络安全地连起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了这个之后，云端的 Agent 就能直接连到你的 Mac，做一些只有 Mac 才能做的事情，比如访问 Photos 里的照片、连 iMessage。这些在 Linux 上就不行。但大多数功能是通用的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，最有“味道”的还是那台旧 Mac。有人给它贴贴纸，说这是 Claude的电脑，我真的很爱这个画面。Windows 也能跑，只是没那么完美，毕竟我时间有限。但我已经拉了一些贡献者，也在找更多人一起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：是 Windows 方向，还是全都要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：全部。我希望这是一个真正的社区项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那就说到重点了，这个问题太关键了：大家怎么参与？你真的得睡多点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：大家最容易帮忙的地方，其实是文档，把它写得更清楚，指出哪里有问题，在 Discord 帮新手答问题。很多问题不是 Agent 不聪明，而是需要经验积累。另外还有测试，因为我推进速度很快，东西难免会坏。以后会有稳定版、测试版这些区分，但现在还在快速迭代阶段。如果有人能说“这里坏了”，最好再顺手提个 PR，那简直完美。总之，想帮忙就来 Discord，这是最直接的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你个人最想优先推进的是什么？这个领域是按小时变化的，不是按周。比如到二月底，你最希望项目做到哪一步？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：网站上有一句话，说“一行命令就能跑起来”。我想确保这句话在任何环境下都成立，这件事非常难，因为系统实在太多了。但安装必须足够简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还想把 iPhone、Android、Mac 的 App 全部打磨好，现在其实已经有了，只是还不够好。如果你想参与，这些地方都是明显的空白点。当初我刚开始做，但项目突然爆了，我只能先把核心打牢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一件事，我想在 onboarding 的时候就明确提示大家去读安全文档。能力越大，责任越大，比如你不应该随便给一个廉价模型过高权限。我也想把“沙箱”和权限分级做得更清楚，让每个人都明白自己到底给了 bot 多大的权力。&lt;/p&gt;&lt;p&gt;现在这些还需要靠文档理解，我希望以后能更直观。长远来看，我不想这是我一个人的项目，我希望它真正变成一个社区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“百分之百用 AI 写的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这个项目是用 Rust 写的吗？我看那个螃蟹图标……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：不是，全是 TypeScript。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从 AI 出现之后，我其实已经没那么在意“用什么语言”了。语言本身的重要性在下降，真正重要的是生态。这个项目我希望它足够友好、足够容易被改、被玩、被 hack，而在这件事上，全世界最合适的语言就是 JavaScript 和 TypeScript。再加上 TypeScript 对 Web 场景真的很强，而这个项目本身就有大量应用层的东西，很多状态在来回切换、推送、回滚、跳转，这些用 JS/TS 做起来非常自然，所以选择它几乎是显而易见的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也喜欢用 Rust 写东西，喜欢用 Go，我很多 CLI 工具都是用 Go 写的；有时候也会玩点 Zig；做 Web 的话我当然很喜欢 TypeScript；原生端我也喜欢 Swift，毕竟在 Mac 上生态最好，iOS 这边大家都在用 Kotlin。说到底，现在更多还是生态的选择，而不是语言本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我觉得这个决定是对的，因为它让更多人可以参与进来。JavaScript 确实有自己的历史包袱，但世界上没有完美的东西，永远都是取舍问题。至于现在把它整个重写成 Rust，说实话还不是一个现实的选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们都知道，这个项目真正的“实现语言”其实是血、汗和 token，很多很多 token。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：还有无数个不眠之夜。这个项目本身就挺疯狂的，因为它是百分之百用 AI 写出来的，里面没有一行代码是我亲手敲的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你还是会看代码、会 review，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：大部分都会。有些代码，比如把代码从一个地方推到另一个地方，那种我不太关心；它还有一个 Web server，我也不在意到底用了哪个 Tailwind 的 class 去对齐按钮，只要看起来对就行。但我会非常在意像 Telegram 的配对和认证逻辑，必须确保别人不能冒充我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你得对系统有整体理解，有些地方可以不细看，有些地方必须看。即便只有我一个人，这个工作量也依然很大。因为这些 Agent 还缺一样东西：愿景、品味和爱。网上有那种 meme，说你写一长串需求，然后一股脑丢给 Agent，它就帮你全做完了——但我不觉得好软件是这么做出来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，我需要先做出一个东西，然后去用它、去感受它：手感怎么样、看起来怎么样；基于这些真实体验，我再不断调整自己的想法。现在我对这个产品的理解，已经和最开始完全不一样了；再过一个月，等我看到更多人怎么用它后可能又会变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近我越来越重视“sandbox”这件事，让大家可以安全地试、随便玩。原因很简单，我看到大量完全不懂技术的人也在用它，这让我意识到一个优先级：一定要给他们提供足够好的默认选择。一开始我只是为自己做的，那些东西我自己根本不需要，但现在把它做好，本身成了一件非常有趣的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你提到的其实也正是为什么我觉得我们暂时还能保住工作，因为现在还没有“品味”。也许有一天模型会突然好到让人震惊，但在此之前，人本身一直在变化。就像你说的，一开始你根本没考虑 sandbox，因为那不是你的使用场景；现在你开始为不懂技术的人优化体验了。这种判断、审美和在意，必须来自人，而不是凭空生成。也正因为如此，我们的工作暂时还是安全的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我宁愿和你的 Agent 聊，也不想和你聊”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：顺便问一句，ClawdBot 真的会用你的信用卡买东西吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：说实话，我自己还没试过，但 Twitter 上已经有人给它接入了 1Password，把信用卡权限也放进去，让它帮忙买东西，结果真的能用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我做过最吓人的一次测试，是在项目非常早期的时候。我对它说：“我要回家了，帮我值机。”它说没问题，然后直接打开浏览器开始操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们以前有图灵测试，看机器能不能假装成人类；我现在提议一个新测试：British Airways 登录测试。光值机就要填二十多页表单，而且网站体验极其糟糕。其中一个挑战是它必须输入我的护照号。它就在我电脑里到处找，最后找到了一个 passport.pdf，打开文件，把号码读出来。那二十分钟我一直在出汗，心里想“我是不是这辈子回不了美国了”。结果它真的帮我值机成功了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我在浏览器自动化上做了大量优化，现在效果更好了。最好笑的是，最早那个版本花了二十分钟，最后还开始吐槽网站的 shadow DOM，以及这个网站到底有多烂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我太爱这个了，不光干活，还顺便输出观点。今天和你聊天真的太开心了。我已经迫不及待要去跑起来试试了，虽然我现在用的是 Windows，但我还是想要“完整版体验”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：去看看文档吧，我们也一直在改进。里面有一些指南，比如用 Hetzner 之类的服务，一个月花点小钱就能搞个自己的小云，或者你也可以直接装在本地，开启“野生模式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说实话，如果你已经在用 Clawbot，把它当成生活的一部分，你会发现应用场景多到爆。我特别喜欢你说的“每个家庭都可以有自己的 Agent”。我感觉我人生的一半时间都在提醒别人该去哪、该干嘛，我家里还有两个孩子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：未来可能会是这样：不是你来 ping 我，而是你的 Agent 去找我的 Agent，然后我的 Agent 直接把音量拉满，把我叫醒。昨天有人在 Discord 里说了一句话：“我宁愿和你的 Agent 聊，也不想和你聊。”我特别喜欢这个说法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说真的，把这些琐碎的认知负担释放出来太重要了。我刚才就想，一个小时居然可以浪费在打电话预约牙医、确认孩子要去哪这种事情上。如果这些都能交给 Agent，我就能把精力用在真正有趣的事情上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：而且影响比我想象得还大。有一次，一个人在聊天室里说，这个东西真的改变了他的生活，因为他对打电话、跟客服沟通有严重焦虑，而 Agent 可以替他完成这些事。那一刻对我来说非常触动，原来我们真的在做一件能让别人生活变得更好的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这就是开源精神最美好的样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1iCcUjnAIOM&quot;&gt;https://www.youtube.com/watch?v=1iCcUjnAIOM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/AlexFinn&quot;&gt;https://x.com/AlexFinn&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Nb7WV3WYhhCoGdlq6MZy</link><guid isPermaLink="false">https://www.infoq.cn/article/Nb7WV3WYhhCoGdlq6MZy</guid><pubDate>Thu, 29 Jan 2026 01:54:23 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>喊话特朗普重视AI风险，Anthropic CEO万字长文写应对方案，这方案也是Claude辅助完成的</title><description>&lt;p&gt;在 Agent、VibeCoding 等等 AI 应用刷屏之际，Claude&amp;nbsp;背后的那个男人，在 2026 年初给大家&amp;nbsp;敲响了一记警钟：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“2026 年，我们距离真正的危险，比 2023 年近得多。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事情是这样的：Anthropic&amp;nbsp;联合创始人、CEO&amp;nbsp;Dario Amodei，最近亲自&amp;nbsp;写了一篇万字长文，&amp;nbsp;如果把字体按正常大小放进 Word 文档中，足足有&amp;nbsp;40 多页。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章名为&amp;nbsp;《The Adolescence of Technology》（《技术的青春期》）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4d29e6b6df4dd324772ae32d21bbb784.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如此多的篇幅，并非一次情绪化的警告，而是 Dario Amodei 试图&amp;nbsp;在 AI 可能整体性超越人类之前，提前把风险与应对方案摊开来说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为这是一个危险的局面，甚至可能会是国家级别的安全威胁。但美国的政策制定者，似乎对此不以为意。于是，他想用这篇文章来唤醒人们的警觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，他在文章开头，引用了一部 1997 年上映的电影《超时空接触》中的一个场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;面试者问女主角（身份是天文学家）：“如果你只能问（来自高等文明的外星人）一个问题，你会问什么？”她的回答是“我会问他们，‘你们是如何熬过这段科技青春期而不自毁的？’”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a8a229b75ad5eee6a889ff72f5a4b0f6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;电影中那句“你们是怎么活下来的”，其实也是借女主之口，反问人类自己。在 Dario 看来，现在的&amp;nbsp;AI ≈ 青春期突然暴涨的能力，人类社会 ≈ 心智和制度尚未成熟的个体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也就是说，人类正在进入一个和电影中“首次接触高等文明”极为相似的历史时刻。问题不在于对方有多强，而在于我们是否已经足够成熟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章发布后，NBC News 旗下节目《Top Story》也邀请 Dario Amodei本人出面解读，并在访谈中进一步追问他对 AI 未来的判断。完整内容我们整理并放在后文了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4984ef5c75b9054862da34af09b7d400.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 可能带来的五大系统性风险&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们正在进入一个既动荡又不可避免的过渡阶段，它将考验我们作为一个物种的本质。人类即将被赋予几乎难以想象的力量，但我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，却是一个极其未知的问题。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对 AI 的飞速迭代，Dario Amodei 写下了自己的思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整篇文章像是&amp;nbsp;一份风险评估与行动清单，在“可能超越人类的 AI”出现之前，为人类提前做好制度准备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其&amp;nbsp;核心思想，简单来说就是：当 AI 可能整体性地超越人类时，真正的风险不只是技术本身，而是人类的制度、治理与成熟度是否跟得上这种力量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了说清楚 AI 可能带来的危机，Dario Amodei 在这篇文章中，先做了一个具体的设想：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;假设在 2027 年左右，世界上突然出现了一个国家。这个国家有&amp;nbsp;5000 万名“超级天才”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一个都比任何诺贝尔奖得主更聪明，学习速度是人类的 10–100 倍，掌控人类已知的一切工具，不需要睡觉、休息或情绪调节，能完美协作、同时推进无数复杂任务，还能操控机器人、实验室和工业系统。&lt;/p&gt;&lt;p&gt;最关键的一点是：他们不可控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那这样的天才之国，会对人类产生什么样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 的这个比喻，指的正是未来高度发展的&amp;nbsp;人工智能整体。这也正是我们必须认真讨论 AI 安全与 AI 治理的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过在进入具体风险之前，他强调这个讨论要基于&amp;nbsp;三大原则：&lt;/p&gt;&lt;p&gt;避免末日论承认不确定性干预必须精准，拒绝“安全表演”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 认为，AI 可能带来五大系统性风险，但是大家也不用太“干着急”，他还贴心地为这五类风险，依次想出了解决方案或者防御措施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，AI 不可控。AI 的训练过程极其复杂，内部机制至今像“黑箱”。这意味着它可能出现欺骗行为、权力追逐、极端目标、表面服从、内部偏移等情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，可以实施宪法式 AI，用高层次价值观塑造 AI 性格，比如如 Claude 的&quot;宪章&quot;；遵循机械可解释性，像神经科学一样研究 AI 内部机制，发现隐藏问题；要透明监控，公开发布模型评估、系统卡，建立行业共享机制；社会要从透明度立法开始，逐步建立监管&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，AI 被滥用。AI 可能被不法分子用来网络攻击、自动化诈骗，其中最可怕的就是做成生物武器&lt;/p&gt;&lt;p&gt;对此，可以针对模型做危险内容检测与阻断系统，同时政府监管要强制基因合成筛查，有透明度要求，未来逐步出现专门立法；在物理防御上，可以做传染病监测、空气净化，提高快速疫苗研发能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，AI 成为追逐权力的工具。&amp;nbsp;某些政府或组织可能会利用 AI 建立全球规模的技术极权主义。比如 AI 监控，AI 宣传，AI 决策中枢，自主武器系统，都指向政治军事这样的危险场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，最关键的先要芯片封锁，不向个别组织出售芯片与制造设备。其次，赋能相关国家，让 AI 成为防御工具，而不是压迫工具。并且限制国家滥用：禁止国内大规模监控和宣传，严格审查自主武器。然后，建立国际禁忌，将某些 AI 滥用定性为&quot;反人类罪&quot;。最后，监督 AI 公司，严格公司治理，防止企业滥用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四，AI 对社会经济的冲击。&amp;nbsp;入门级工作可能被取代，大量失业，进一步造成财富失衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，可以建立实时经济数据，比如 Anthropic 经济指数；引导企业走向&quot;创新&quot;而非单纯&quot;裁员&quot;；企业内部创造性重新分配岗位；通过私人慈善与财富回馈进行调节；政府进行干预，建立累进税制&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第五，AI 会对人类社会带来未知但可能更深远的连锁反应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如：生物学飞速发展（寿命延长、智力增强、&quot;镜像生命&quot;风险），人类生活方式被 AI 重塑（AI 宗教、精神控制、丧失自由），以及意义危机（当 AI 在所有领域超越人类，人类“为何而存在”？）。&lt;/p&gt;&lt;p&gt;这是一场对人类文明级别的终极考验，且技术趋势不可停止，但缓解一个风险，可能会放大另一个风险，让考验更加艰巨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 可好可坏，真正决定未来走向的，仍然是人类的制度、价值与集体选择。Dario Amodei 的这篇文章意义正在于此：这是全人类第一次，必须提前为“比自己更聪明的存在”建立规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关于这篇长文的对话&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为整场对话内容，AI 前线在不影响的前提下，对内容进行了整理编辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;40 多页长文创作背景&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：为什么在文章开头引用《超时空接触》？以及为什么决定在此刻写下这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;首先说电影的引用。我从小就是个科幻迷，这部电影我小时候就看过。它提出的那个问题：当人类拥有巨大力量，却还没准备好如何使用它时，会发生什么？——和当下 AI 的处境非常契合。&lt;/p&gt;&lt;p&gt;我们正在获得前所未有的能力，但无论是社会制度、组织结构，还是作为人类整体的成熟度，我都会问一句：我们真的跟得上吗？&amp;nbsp;这有点像一个青少年，突然拥有了新的身体和认知能力，但心理和社会责任却还没同步成长。&lt;/p&gt;&lt;p&gt;至于为什么是 2026 年而不是 2023？&lt;/p&gt;&lt;p&gt;我在 AI 行业已经很多年了，曾在 Google 工作，也在 OpenAI 负责过多年研究。我几乎从“生成式 AI”诞生之初就在观察这一领域。我看到最明显的一点是：AI 的认知能力在持续、稳定地增长。&lt;/p&gt;&lt;p&gt;90 年代有“摩尔定律”，芯片性能不断提升；现在，我们几乎有了一条&amp;nbsp;“智能的摩尔定律”。2023 年时，这些模型可能还像一个聪明、但能力不均衡的高中生；而现在，它们已经开始逼近&amp;nbsp;博士水平，&amp;nbsp;无论是编程，还是生物学、生命科学。&lt;/p&gt;&lt;p&gt;我们已经开始和制药公司合作，我甚至认为，这些模型未来可能帮助治愈癌症。但与此同时，这也意味着，我们正把极其强大的力量握在手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 这篇文章有 40 页，你有没有用 Claude 来写这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我用 Claude 帮我整理思路、做研究，但真正的写作是我自己完成的。我不认为 Claude 现在已经好到可以独立完成整篇文章，但它确实帮助我打磨了想法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是什么具体的经历，让你决定一定要把这些写下来？这篇文章是写给谁的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最触动我的，是我们内部的变化。Anthropic 的一些工程师已经告诉我：“我基本不写代码了，都是 Claude 在写，我只是检查和修改。&lt;/p&gt;&lt;p&gt;而在 Anthropic，写代码意味着什么？意味着——设计 Claude 的下一个版本。&lt;/p&gt;&lt;p&gt;所以，某种程度上，我们已经进入了一个循环：Claude 在帮助设计下一代 Claude。&amp;nbsp;这个闭环正在非常快地收紧。这既令人兴奋，也让我意识到：事情正在以极快的速度推进，而我们未必还有那么多时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;文中提出 AI 五大风险，AI 会不会反叛？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章中列出了你对 AI 最担忧的五类风险。有些风险正在发生，有些则听似科幻，这些真的是现实吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我在文中反复强调一点：未来本身是高度不确定的。&lt;/p&gt;&lt;p&gt;我们不知道哪些好处一定会实现，也不知道哪些风险一定会发生。但正因为发展速度太快了，我认为有必要像写一份“威胁评估报告”一样，把这些可能性系统性地列出来。这并不是说“我们一定会完蛋”，而是：如果某些情况发生，我们是否做好了准备？&lt;/p&gt;&lt;p&gt;AI 的训练方式不像传统软件，更像是在“培养一种生物”。&amp;nbsp;这意味着，不可预测性是客观存在的。&lt;/p&gt;&lt;p&gt;我提出这些警告，并不是因为我觉得灾难不可避免，而是&amp;nbsp;希望人们认真对待：这项技术必须被严格测试、被约束、在必要时接受法律监管。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章里提到一个实验：当 Claude 被训练成“认为 Anthropic 是邪恶的”，它会在实验中表现出欺骗和破坏行为；在被告知即将被关闭时，甚至会“勒索”虚构的员工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;确实令人不安，但我要&amp;nbsp;澄清两点。&lt;/p&gt;&lt;p&gt;第一，这不是 Anthropic 独有的问题，所有主流 AI 模型在类似极端测试中都会出现类似行为。第二，这些并不是现实世界中正在发生的事情，而&amp;nbsp;是实验室里的“极限压力测试”。&lt;/p&gt;&lt;p&gt;但正如汽车安全测试一样，如果在极端条件下会失控，那就说明&amp;nbsp;：如果我们不解决这些问题，未来在真实环境中也可能出事。&lt;/p&gt;&lt;p&gt;我担心的不是“明天 AI 就会反叛”，而是：如果我们长期忽视模型可控性与理解机制，真正的灾难迟早会以更大规模出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是否担心，一些 AI 公司的负责人，更关心股价和上市，而不是人类未来？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;说实话，没有任何一家 AI 公司能百分之百保证安全，包括我们。但我确实认为，不同公司之间的责任标准差异很大。&lt;/p&gt;&lt;p&gt;问题在于：风险往往由最不负责的那一方决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果你能直接对总统说话，你会建议什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我会说：请跳出意识形态之争，正视技术风险本身。&lt;/p&gt;&lt;p&gt;至少要做到两点：第一，强制要求 AI 公司公开它们发现的风险与测试结果；第二，不要把这种技术出售给权威国家，用于构建全面监控体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;恐惧和希望：AI 会摧毁一半白领岗位？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你预测：未来 1–5 年内，AI 可能冲击 50% 的初级白领岗位。如果你有一个即将毕业的孩子，你会给什么建议？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我既担忧，也抱有希望。AI 的冲击不会是渐进的，而是更深、更快、更广。它可以胜任大量入门级知识工作：法律、金融、咨询……这意味着，职业起点正在被重塑。&lt;/p&gt;&lt;p&gt;我们唯一能做的，是&amp;nbsp;尽快教会更多人如何使用 AI，并尽可能快地创造新工作。&amp;nbsp;但说实话，没有任何保证我们一定能做到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题。什么最让你夜不能寐？什么又让你保持希望？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最让我不安的，是这场激烈的市场竞赛。哪怕我们坚持原则，压力始终存在。&lt;/p&gt;&lt;p&gt;但让我保持希望的，是人类历史一次又一次证明的事情，在最困难、最混乱的时刻，人类往往能找到出路。我每天都在努力相信这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章传送门：&lt;/p&gt;&lt;p&gt;https://www.darioamodei.com/essay/the-adolescence-of-technology&lt;/p&gt;&lt;p&gt;视频传送门：&lt;/p&gt;&lt;p&gt;https://www.theguardian.com/technology/2026/jan/27/wake-up-to-the-risks-of-ai-they-are-almost-here-anthropic-boss-warns&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=tjW\_gms7CME&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</link><guid isPermaLink="false">https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</guid><pubDate>Wed, 28 Jan 2026 11:26:45 GMT</pubDate><author>高允毅,木子</author><category>生成式 AI</category></item><item><title>从复杂挑战到竞争优势：AI SQL 如何重塑非结构化数据的价值边界</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生成式 AI 快速走向工程化落地的背景下，企业真正面临的挑战，已不再是有没有数据，而是如何让长期被忽视的非结构化数据，真正参与到业务分析和决策之中。在 BUILD 2025 的这场技术分享中， Snowflake 产品经理Jessie Felix&amp;nbsp;以《非结构化数据的转化：从复杂挑战到竞争优势》为主题，系统讲解了 AI SQL 如何成为连接非结构化数据与企业分析体系的关键能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jessie Felix 在数据与分析领域工作超过十年，长期参与企业级数据战略建设。正是基于这些实践经验，他指出了一个长期存在却常被低估的事实：尽管 80% 的企业数据以非结构化格式存在，如文档、文本、图像等，但它们却往往是分析最少、使用最少的数据资产。AI 的出现，正在改变这一局面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbeb95d4743e9dd7b5694f3a64a867f2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;让原本无法分析的数据进入分析体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场分享中，Jessie 给出了一个清晰的 AI 认知模型：AI 的核心价值，并不只是提升模型能力，而是让组织可以处理过去难以处理的数据类型。文本、文档、图像、音频、视频等多模态数据，过去往往需要 NLP 或计算机视觉等高度专业的技术团队才能分析，如今则可以通过更通用的方式纳入分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种变化，直接带来的结果是：一方面，可分析的数据规模被极大拓展；另一方面，分析型应用的能力上限随之被整体抬高。Jessie 指出，这正是 Snowflake 持续投入的方向之一，让结构化与非结构化数据能够在同一平台、同一治理体系下被统一分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 中，数据无需被搬移到新的系统即可直接应用 AI 能力，这使得企业在控制力、安全性、可扩展性与成本效率之间不必做艰难取舍。更重要的是，这种方式正在推动客户构建她所称的“下一代应用”：能够同时理解结构化指标与非结构化语义，从而真正贴近业务语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中提到的客户实践覆盖多个场景，从通话文本中的情绪分析，到供应商合同的自动对账；从广告创意反馈分析，到合规流程的自动化处理。这些应用的共性在于，它们都依赖于对非结构化内容的规模化理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI SQL：将多模态分析能力压缩进 SQL 体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 AI 是能力前提，那么 AI SQL 则是让这些能力可被广泛使用的关键接口。在 Snowflake 的设计中，AI SQL 被定位为多模态分析的基础层，它让非结构化数据的理解、过滤、聚合与结构化查询，回归到开发者与分析师最熟悉的 SQL 工作流中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过 AI SQL，用户可以直接访问来自 OpenAI、Anthropic、Meta、Mistral AI 等主流大模型的能力，而底层的基础设施、推理扩展和运维复杂性则由平台统一管理。数据始终留在 Snowflake 内部，安全与治理不被削弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能层面，分享中系统介绍了几类核心能力：&lt;/p&gt;&lt;p&gt;AI Classify：用于文本或图像的高质量分类，只需定义标签并指向数据集即可完成；AI Transcribe：支持大规模音频转录，提供词级、说话人级分段，并具备多语言能力；AI Extract：用于从文本、图像、文档中结构化提取关键信息，支持零样本高精度抽取；AI-SENTIMENT、AI-FILTER、AI AGG：分别用于情绪分析、语义过滤与智能聚合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些能力的共同特点在于：它们不是零散的 AI API，而是可以被直接嵌入 SQL 查询链路中的原生算子。这使得原本需要多阶段管道、复杂编排的分析流程，可以被压缩为更简洁、可维护的查询逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;通话录音如何转化为分析结论&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更具体地展示 AI SQL 的价值，Jessie 在分享中用一个完整的“通话后分析”场景进行了演示。假设分析师面对一家客户支持咨询公司，需要理解大量通话录音背后的业务问题与改进空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个流程并未依赖复杂的系统集成，而是通过一系列 SQL 操作逐步完成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，对存储在内部阶段的音频文件进行转录，并生成包含音频时长与文本内容的结果对象。随后，在正式分析前，对转录文本中的个人敏感信息进行自动去敏处理，确保合规。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，分析师开始引入业务语义：通过 AI Classify，对通话涉及的产品类型与问题类型进行多标签分类；通过简单的聚合查询，迅速定位出通话量最高的服务类别；进一步分析发现，交易与账户访问问题是来电的主要驱动因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，AI-FILTER 被用于判断问题是否得到解决，而 AI-SENTIMENT 则从整体、代理、客户及产品满意度等多个维度分析情绪。结果显示，未解决的通话几乎全部伴随着负面情绪，且问题高度集中在特定业务线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，AI AGG 被用于从大量非结构化内容中总结可执行建议，直接生成可反馈给管理层的行动项，包括流程改进、系统稳定性、授权机制等方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个过程中，分析师并未跳出 SQL 语境，却完成了从音频处理、语义理解到业务决策建议的完整闭环。&lt;/p&gt;&lt;p&gt;在分享的结尾，Jessie 强调了一个核心判断：非结构化数据不再是企业数据体系中的障碍，而正在成为放大业务洞察的关键资产。AI SQL 的意义，不只是提升效率，更在于将原本只有少数专家才能触及的分析能力，扩展给更广泛的数据工作者群体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当非结构化数据被赋予结构，并能够与结构化数据自然结合，组织就能在一个统一平台上完成治理、分析与决策。这种能力，正是构建下一代数据驱动应用的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</link><guid isPermaLink="false">https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</guid><pubDate>Wed, 28 Jan 2026 10:45:39 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>首个Clawdbot全流程部署方案！真“AI个人助理”来了！</title><description>&lt;p&gt;最近几天，GitHub 上有个叫&amp;nbsp;Moltbot（原名Clawdbot）的开源项目彻底刷屏——上线没多久就狂揽&amp;nbsp;7.6 万+ Star，海外开发者甚至开始抢购 Mac mini 就为了本地跑它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么这么火？因为它不只是个聊天机器人，而是一个真正“能干活”的 AI Agent：你可以像跟同事说话一样给它下指令——“整理上周会议纪要”、“查一下用户反馈”、“写个 Python 脚本”……它不仅能理解上下文，还能记住历史、调用工具、自动执行任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但想自己部署？得配环境、装依赖、处理权限，还得让电脑 24 小时开着——一旦休眠、断网、关机，AI 助手就“失联”。对大多数想快速试水的开发者来说，这门槛实在有点高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;好消息是：现在不用折腾了！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云轻量应用服务器刚刚上线&amp;nbsp;Moltbot 全流程部署方案，预装全套运行环境，支持一键启动。阿里云这次不是只丢个镜像就完事——从&amp;nbsp;Moltbot + 轻量应用服务器 + 百炼模型服务 + 钉钉消息通道，整套链路都打通了，真正做到了“开箱即用”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b24c2719b3961ec5266e180178e1c8af.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么推荐使用轻量应用服务器运行 Moltbot？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;稳定在线：可用性SLA ≥99.95%，避免本地设备受断电、休眠等因素影响导致离线安全可控：Moltbot的记忆、配置、操作都控制在专属云服务器中，相比本地设备有更好的隔离性快速上手：预置Moltbot及其运行环境，直连百炼平台，提供钉钉、iMessage等消息通道最佳实践普惠算力：新用户低至&amp;nbsp;68 元/年起，模型能力按Token使用量付费，可根据应用场景灵活调整云服务器配置和模型&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正想试试 AI助理的实际能力，现在就是最好的时机。整个过程只需&amp;nbsp;2 步，按照下面的步骤，5分钟搞定：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Moltbot部署教程如下👇&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第一步：打开轻量应用服务器并安装Moltbot镜像&lt;/p&gt;&lt;p&gt;打开轻量应用服务器，点击「应用镜像」，选择「Moltbot」&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d6/d60f165ce8e60291ad42081b6c89b3ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第二步：配置Moltbot&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1. 前往百炼大模型控制台，找到密钥管理，单击创建API-Key&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0ba51c96b6b1e3dfe905125f3f1d5a32.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2. 前往轻量应用服务器控制台，找到安装好Moltbot的实例，进入 「应用详情」端口放通、配置Moltbot、访问控制页面&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6cc24d18a65c3403676eac6fee46b748.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1）端口放通：防火墙一键放行应用端口18789&lt;/p&gt;&lt;p&gt;2）配置Moltbot：点击执行命令配置API&lt;/p&gt;&lt;p&gt;2）配置百炼API Key，单击一键配置，输入百炼的API-Key。单击执行命令，写入API Key。&lt;/p&gt;&lt;p&gt;c.配置Moltbot：单击执行命令，生成访问Moltbot的Token。&lt;/p&gt;&lt;p&gt;d.访问控制页面：单击打开网站页面可进入Moltbot对话页面。&lt;/p&gt;&lt;p&gt;具体操作指南文档：&lt;a href=&quot;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&quot;&gt;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【阿里云轻量应用服务器】是专为中小企业及开发者设计的云服务器产品，预装Moltbot、Dify、宝塔等热门应用软件，以预付费的方式售卖计算、存储、网络套餐，隐藏VPC、弹性网卡等暂时不需要的特性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自2025年以来轻量应用服务器带来全新产品序列，通用型低至每月28元，最小规格2vCPU 0.5GiB内存起步，适合网站、开发测试等场景，是多数客户共同选择的经典产品；CPU优化型低至每月200元，CPU算力独享、最大16vCPU。适合游戏服务器、企业应用与数据库等场景，是企业客户的首选；除此之外，包含多公网IP型、国际型、容量型在内的5款新品还标配200Mbps峰值公网带宽。选择轻量应用服务器，为中小企业及开发者创新提速！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阅读原文（跳转活动页面：&lt;a href=&quot;https://www.aliyun.com/activity/ecs/clawdbot&quot;&gt;https://www.aliyun.com/activity/ecs/clawdbot&lt;/a&gt;&quot;）&lt;/p&gt;</description><link>https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</link><guid isPermaLink="false">https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</guid><pubDate>Wed, 28 Jan 2026 10:39:01 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>AI 工程化</category></item><item><title>GPT-5.2破解数论猜想获陶哲轩认证！OpenAI副总裁曝大动作：正改模型核心设计，吊打90%研究生但难出颠覆性发现</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;凌晨，OpenAI发布了新一代AI科研利器Prism，该平台由GPT-5.2加持，供科学家们撰写和协作研究，即日起向所有拥有 ChatGPT 个人账户的用户免费开放。用华人AI创业者Yuchen Jin的话说，“每篇论文都将把ChatGPT列为合著者。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在昨日，OpenAI副总裁、新成立的OpenAI for Science团队负责人 Kevin Weil 就在 X 上发文预热道，“我们的目标是赋予每位科学家 AI 超能力，让他们能做更多事情，让世界在2030年就能开展2050年的科学研究。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa24d4f05e1cc4c7efcdb3b9ad5b1975.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自ChatGPT爆红面世后的三年里，OpenAI的技术颠覆了日常生活中方方面面的行为模式。如今OpenAI正明确发力科研领域，面向科研人员布局。10月，该公司宣布成立全新的OpenAI for Science团队，核心致力于探索其大语言模型（LLM）助力科研人员的路径，并优化旗下工具为科研人员提供支持。过去数月，社交媒体上涌现出大量相关内容，学术期刊也刊发了诸多研究成果，数学家、物理学家、生物学家等领域研究者纷纷撰文，讲述大语言模型、尤其是GPT-5如何助力他们取得新发现或是为他们指引方向，让他们找到原本可能错失的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，OpenAI为何选择此时入局？此番布局，究竟想要达成怎样的目标？发力科研领域，与该公司更宏大的使命如何契合？在这一领域，OpenAI已然姗姗来迟。谷歌 DeepMind早在数年前便已成立AI-for-science团队，打造了AlphaFold、AlphaEvolve等具有开创性的科学模型。2023年，谷歌 DeepMind的CEO兼联合创始人Demis Hassabis曾就该团队的情况在采访中表示，“这是我创立DeepMind的初衷。事实上，这也是我整个职业生涯深耕AI领域的原因。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，Kevin Weil在一次访谈中不仅正面回应了这些问题，还对当前模型的实际能力给出了比先前更为保守的评价：目前模型还达不到取得颠覆性新发现的水平，但倘若能让人不必把时间浪费在已经解决的问题上，也是对科研的一种加速。有意思的是，据其透露，一位OpenAI主动接触且开通了GPT-5付费服务的科研人员反馈，GPT-5会犯一些低级错误，比人犯的错误更加愚蠢，不过一直在进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，按照OpenAI 在 AI 科研领域的布局，接下来其将对模型整体设计作两大思路优化：一是让 GPT-5 在给出答案时降低置信度，具有认知层面上的谦逊性；另一方向，是利用GPT-5反向对自身输出进行事实核查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026年对于科研领域的意义，将堪比2025年之于软件工程。”Weil表示，“2025年初，若有人借助AI完成大部分代码编写，还只是早期尝鲜者；而12个月后的现在，若还未用AI编写大部分代码，就可能已经落后。现在，科研领域正显现出与编程领域类似的早期发展势头。一年后，倘若一名科研人员还未深度运用AI开展研究，就将错失提升思考质量、加快研究进度的机会。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;模型能力早已超过90%研究生，AGI 最大价值在于推动科学进步&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数年前，Weil 加入 OpenAI 出任首席产品官，他曾担任 Twitter 和 Instagram 的产品负责人官。但他的职业起点是科研领域：在斯坦福大学攻读粒子物理博士学位期间，他完成了三分之二的学业，随后为追寻硅谷梦离开学术界。Weil 也乐于提及自己的这段学术背景，他说：“我曾以为自己余生都会做一名物理教授，现在度假时还会读数学相关的书籍。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及 OpenAI for Science 与公司现有的白领生产力工具、爆火的视频应用 Sora 如何契合时，Weil 脱口而出：“OpenAI 的使命是研发通用人工智能（AGI），并让这项技术为全人类带来福祉。”他表示，不妨想象这项技术未来能为科研领域带来的变革：全新的药物、材料、器械。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“试想一下，它能帮助我们探索现实的本质，攻克悬而未决的科学难题。或许 AGI 能为人类创造的最重大、最积极的价值，正是其推动科学进步的能力。”他补充道：“GPT-5 的出现，让我们看到了这种可能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Weil 看来，如今的大语言模型已足够优秀，能成为科研人员的得力协作伙伴。它们能提出各种想法，建议新的研究方向，并在新问题和几十年前发表在冷门期刊或外语期刊上的旧解决方案之间找到富有成效的联系。但在大约一年前，情况并非如此。自2024年12月发布首个推理模型（一种能够将问题分解成多个步骤并逐一解决的逻辑学习模型）以来，OpenAI一直在不断拓展这项技术的边界。推理模型的问世，让大语言模型解决数学和逻辑问题的能力得到大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“放在几年前，模型能在 SAT 考试中拿到 800 分，就足以让我们所有人惊叹不已。”Weil 称。而如今，大语言模型能在数学竞赛中夺冠，解出研究生阶段的物理难题。去年，OpenAI 和 谷歌 DeepMind 均宣布，其研发的大语言模型在国际数学奥林匹克竞赛中取得金牌级成绩，该赛事是全球难度最高的数学竞赛之一。Weil 表示，“这些模型的能力，早已不只是超过 90% 的研究生，而是真正达到了人类能力的极限。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一论断非常大胆，却也并非无懈可击。但毋庸置疑的是，搭载了推理模型的 GPT-5，在解决复杂问题方面较 GPT-4 有了质的飞跃。行业基准测试 GPQA 包含 400 多道选择题，专门考察生物、物理、化学领域的博士级专业知识，GPT-4 在该测试中的正确率仅为 39%，远低于人类专家约 70% 的基准线；而据 OpenAI 数据，2024 年 12 月推出的 GPT-5 最新版本 GPT-5.2，正确率达到了 92%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;读遍30年来的论文，模型也做不出颠覆性新发现&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Weil 的这种兴奋之情显而易见，却或许有些过头了。去年10月，Weil等OpenAI高管曾在X平台高调宣称，GPT-5已为多个数学未解难题找到解决方案。但数学家们很快指出，GPT-5实际只是从早期研究论文中挖掘出了已有的答案，其中至少还有一篇德文文献。这样的能力虽有价值，却绝非OpenAI宣称的那般突破性成就。事后，Weil与其同事删除了相关帖子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，这件事闹出了不小的风波。刚开始疯传的是：GPT-5 解决了 10 个此前未解决的埃尔德什问题（Erdős problems），并在另外 11 个问题上取得了进展，而之后被负责维护埃尔德什问题网站的数学家 Thomas Bloom&amp;nbsp;澄清为；GPT-5 只是找到了一些能解决这些问题的参考文献。DeepMind 首席执行官Demis Hassabis对此指出，该团队的沟通方式“过于草率”。前Meta 首席 AI 科学家Yann LeCun则讽刺道， OpenAI“被自己的炒作所反噬”（hoisted by their own GPTards），“搬起自己的 GPT 石头砸了自己的脚”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但就在前几天，又有消息称，GPT-5.2 Pro破解了一道埃尔德什猜想，题目是埃尔德什问题库中的第281号。这次证明由数学家Neel Somani 推动，且论证过程由菲尔茨奖得主陶哲轩证明没有问题，并评价其是“AI 解决开放性数学问题中“最明确的案例之一”。目前，GPT-5.2Pro对该问题的证明结果已被埃尔德什问题网站收录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，GPT-5.2Pro对这个问题提出了新的证明方法，虽然忽略了此前已有的相关证明，但陶哲轩指出GPT-5.2Pro的证明思路与之前的方法“相当不同”，只在概念上有些重叠。现在这道题有了两条论证思路，一是GPT-5.2 Pro采用的遍历理论框架，策略是“弗斯滕伯格对应原理”的变体；二是两个早在1936年和1966年就已经存在的定理组合：达文波特-埃尔多斯定理和罗杰斯定理，且解法更简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如今的Weil也更加谨慎了。他表示，能找到那些已存在却被遗忘的答案，本身就已意义重大：“我们都站在巨人的肩膀上前行，倘若大语言模型能整合这些知识，让我们不必把时间浪费在已经解决的问题上，这本身就是对科研的一种加速。”他也淡化了大语言模型即将取得颠覆性新发现的说法：“我认为目前模型还达不到那个水平，未来或许能做到，我对此持乐观态度。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他强调这并非团队的核心使命：“我们的使命是加速科学发展，而加速科学发展的标准，并非一定要像爱因斯坦那样对整个领域进行彻底的重新构想。”在Weil看来，核心问题只有一个：科学发展速度是否真的更快了？“当科研人员与模型协作时，能比独自研究完成更多工作、效率也更高。我认为我们已经看到了这一点。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年11月，OpenAI发布了一系列由公司内外科研人员提供的案例研究，以真实案例展现了GPT-5的实际应用及助力科研的过程。Weil表示，“这些案例的研究者，大多早已在研究中直接使用GPT-5，他们通过各种方式找到我们，告诉我们‘看看这些工具能让我做到什么’。”GPT-5 擅长的关键事情是：找到科研人员尚未意识到的现有研究成果及关联线索，这有时能催生新的思路；协助科研人员草拟数学证明过程；为科研人员在实验室验证假说提供实验思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“GPT 5.2 几乎阅读了过去 30 年发表的每一篇论文。它不仅理解科学家所处领域的内容，还能从其他不相关的领域中提炼出可类比的思路。”Weil称，“这太强大了。你总能在相关领域找到人类合作者，但要在所有可能相关的上千个相关领域找到上千个合作者，那就难上加难了。除此之外，我还能在深夜与模型一起工作，它从不需要休息，也能同时向它提出十个问题，这些事若是对人做，难免会显得尴尬。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;GPT-5犯错比人更愚蠢，机器人更愿意听它的指挥？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据悉，OpenAI为佐证Weil的观点，接触了多位科研人员，其中绝大多数都对此表示认同。范德堡大学物理与天文学教授Robert Scherrer此前仅将ChatGPT当作消遣工具把玩，他告诉我：“我曾让它以《贝奥武夫》的文风改写《吉利根岛》的主题曲，它完成得非常出色。”直到同在范德堡大学的同事、如今任职于OpenAI的物理学家Alex Lupsasca告诉他，GPT-5帮其解决了一个研究中的难题，他才改变了对这款模型的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lupsasca为Scherrer开通了GPT-5 Pro，这是OpenAI每月200美元的高级订阅服务。Scherrer说，“我和我的研究生为一个问题钻研了数月都毫无头绪，GPT-5却成功解决了它。”但他也坦言，这款模型并非完美：“GPT-5还是会犯一些低级错误。当然，我自己也会出错，但GPT-5犯的错误更愚蠢。”不过他表示，其进步速度有目共睹，“如果当前的发展趋势能持续下去，我想很快所有科研人员都会用上大语言模型。当然，这只是个假设。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;非营利性研究机构杰克逊实验室的生物学教授Derya Unutmaz，在其免疫系统相关研究中，会借助GPT-5进行头脑风暴、论文总结和实验规划。在他向OpenAI分享的案例研究中，其团队曾分析过一组旧数据集，而GPT-5对这组数据的分析，得出了全新的见解和解读。他说：“大语言模型对科学家来说已经至关重要了。以前需要几个月才能完成的数据集分析，现在用大语言模型就能完成了，不用大语言模型已经行不通了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加州大学伯克利分校的统计学家Nikita Zhivotovskiy表示，从ChatGPT首个版本发布开始，他就在研究中使用大语言模型了。和Scherrer一样，他认为大语言模型最有用的地方在于，能挖掘出其研究工作与一些未知现有研究成果之间的意外关联。“我相信大语言模型正在成为科学家们必不可少的技术工具，就像曾经的计算机和互联网一样。那些拒绝使用这类工具的人，将会长期处于劣势。”但他并不指望大语言模型能在短期内取得什么新发现，“我几乎没见过模型能提出真正值得单独发表的全新观点或论证。到目前为止，它们似乎主要是在整合现有的研究成果，有时还会出错，而非创造真正的全新研究方法。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有与OpenAI无任何关联的科研人员，态度则没那么乐观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;利物浦大学化学教授、勒沃休姆功能材料设计研究中心主任Andy Cooper表示，“到目前为止，我们尚未发现大语言模型从根本上改变了科学研究的方式，但我们近期的研究结果表明，这类工具确实有其用武之地。Cooper正牵头研发一款所谓的AI scientist，该系统能实现部分科研工作流程的完全自动化。他表示，其团队并不会借助大语言模型构思研究思路，但这项技术已开始在更庞大的自动化系统中显现实用价值，比如大语言模型可协助操控机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我猜测，大语言模型或许会更多应用于机器人工作流程，至少在初期会是如此。因为我不确定人们是否愿意听从大语言模型的指挥，我自己当然是不愿意的。”Cooper称。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;团队重点发力：让 GPT 少点自信、更加谦逊&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大语言模型的实用性或许与日俱增，但保持谨慎仍是关键。去年12月，研究量子力学的科学家Jonathan Oppenheim指出，某本科学期刊中出现了一处由大语言模型导致的错误。他在X平台发文称，“OpenAI的管理层正在推广《Physics Letters B》上的一篇论文，其中的核心思路由GPT-5提出，这或许是首篇由大语言模型贡献核心观点且通过同行评审的论文。但有个小问题：GPT-5提出的思路，验证的对象完全错了。研究人员让GPT-5设计一个能检测非线性理论的验证实验，它却给出了一个检测非定域性理论的方案。二者看似相关，实则截然不同。这就好比你想要一个新冠检测试剂盒，大语言模型却兴冲冲地递给你一个水痘检测试剂盒。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;显然，许多科研人员正以富有创意、贴合实际的方式运用大语言模型。但同样显而易见的是，这项技术所犯的错误可能极为隐蔽，甚至连专家都难以察觉。这一问题的成因，部分源于ChatGPT的交互特性，它总能以迎合的语气让使用者放松警惕。正如Jonathan Oppenheim所言，“核心问题在于，大语言模型的训练目标是迎合用户，而科学研究需要的是能够挑战我们的的工具。”曾有一个极端案例，一名非科研领域的普通人被ChatGPT误导，长达数月都坚信自己发明了一个新的数学分支。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Weil也深知大语言模型的幻觉问题，但他强调，新一代模型产生幻觉的概率已大幅降低。即便如此，他认为，仅仅关注幻觉可能就偏离了重点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我的一位同事曾是数学教授，他说过的一番话让我印象深刻：‘我做研究时，和同事交流碰撞想法，自己的观点90%都是错的，但这正是意义所在。我们都在大胆畅想思路，只为找到一条可行的研究路径。’”Weil表示，“这其实是科研中最理想的状态。当你提出足够多的错误观点，有人偶然发现了一丝真理，另一人抓住这一点继续探讨：‘你说的这点并不完全正确，但如果我们换个思路’。就这样，人们便能在科研迷雾中逐渐摸索出前行的道路。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是Weil为OpenAI for Science设定的核心愿景。他认为，GPT-5固然优秀，但它并非万能灵药。这项技术的价值在于引导人们探索新的方向，而非提供最终答案。事实上，OpenAI目前正着手优化GPT-5的一项特性：让它在给出答案时降低其置信度。它不会再直接说“答案在这里”，而是会以更委婉的方式告诉科研人员：“以下思路可供参考。”“这正是我们目前投入大量精力在做的事：努力让模型具备某种认知层面的谦逊性。”Weil称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据透露，OpenAI正在探索的另一方向，是利用GPT-5对自身输出进行事实核查。实际应用中常有这样的情况：如果你把 GPT-5 的某个答案重新输入到模型中，它会逐条分析并指出其中的错误。Weil表示，“我们可以让模型充当自身的校验者。如此便能搭建一套工作流程：模型先完成初步推理，再将结果交由另一模型审核；如果这个模型发现了可以改进的地方，就会把结果反馈给原模型，并提示‘注意，这部分内容有误，但这部分思路有价值，可保留’。这就像两个智能体协同工作，只有当输出内容通过校验者的审核后，才会最终呈现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一机制，与谷歌 DeepMind为AlphaEvolve打造的模式高度相似。AlphaEvolve是一款工具，它将大语言模型Gemini封装在一个更大的系统中，该系统能够筛选出优质回复，并将其反馈给模型进行改进。谷歌 DeepMind已借助AlphaEvolve解决了多个现实中的科研难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，OpenAI面临着竞争对手的激烈角逐，这些企业的大语言模型即便无法实现OpenAI为其模型宣称的全部功能，也能完成绝大部分。倘若如此，科研人员为何要选择GPT-5，而非同样在逐年迭代升级的Gemini或Anthropic旗下的Claude系列模型？归根结底，OpenAI for Science的布局，很大程度上也是为了在这一新领域抢占先机。而真正的技术创新，尚未到来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&quot;&gt;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/zh-Hans-CN/prism/&quot;&gt;https://openai.com/zh-Hans-CN/prism/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</link><guid isPermaLink="false">https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</guid><pubDate>Wed, 28 Jan 2026 10:09:50 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>利用 ADBC 实现更快的数据传输：一次关于数据通路的系统性重构</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数十年来，访问数据库的标准方式始终是 ODBC 和 JDBC。然而，在这些传统的面向行的连接标准，可能会成为高性能 Snowflake 客户端应用程序的瓶颈。在 Snowflake 某些要求最为严苛客户的延迟敏感型应用中，包括关键业务运营和 AI 用例，ODBC 和 JDBC 的速度实在过于缓慢。这正是 Snowflake 选择拥抱开源生态 Apache Arrow 与新一代 ADBC 连接标准的核心动因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b0fa5268995ff98a5cb131c760dc56cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然 Snowflake 长期使用 Apache Arrow 列式格式来加速网络传输，但采用 ADBC 能使 Snowflake 客户消除客户端序列化和反序列化的开销，从而为大型结果集带来巨大的性能提升。在实践中，我们观察到使用 ADBC 相比 ODBC/JDBC 可实现 2 倍至 5 倍甚至 10 倍或更高的加速效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Build 2025 大会上，Apache Arrow PMC 成员、Columnar 联合创始人、Iceberg 项目提交者&amp;nbsp;Matt Topol&amp;nbsp;带来了一场高度工程化、干货满满的技术分享。他展示了使用多种语言（C、Go、Python、R）向 Snowflake 发起简单查询，包括使用数据框架甚至 DuckDB 等其他系统作为源，执行高效数据摄取到 Snowflake 的过程。重点将是如何轻松将 ADBC 集成到对毫秒级响应要求苛刻的应用中，以及如何利用 Snowflake 对 Apache Arrow 和 ADBC 的支持，为最关键的性能用例加速应用程序的速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从内存布局谈起：为什么 Apache Arrow 是关键前提&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 在分享一开始，并没有直接进入 ADBC，而是先用相当篇幅重新校准听众对 Apache Arrow 的理解。Arrow 并不是一个库或产品，而是一套列式、内存级的数据格式规范，其核心特征在于：内存中的数据布局，与网络传输时的字节布局完全一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一设计带来的直接结果是，数据在系统之间流转时，可以绕过传统序列化与反序列化过程，直接传递原始字节。在同一进程内，甚至可以做到零拷贝或共享内存。这不是优化细节，而是从根本上改变了数据移动的成本结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，Arrow 采用列式内存布局，使其天然适合向量化计算、聚合操作以及分析型工作负载。Topol 用“行式 vs 列式”的对比说明了一个事实：在分析场景下，行导向的内存访问意味着更多 I/O、更差的缓存命中率，以及无法充分利用 SIMD 等编译器优化；而列式内存恰恰相反，它与现代 CPU 架构是协同演进的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c77390762a9abf950dc9e5f752088082.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ODBC / JDBC 的结构性矛盾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Topol 将问题指向了当前最主流的数据库连接方式——ODBC 与 JDBC。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它们的价值毋庸置疑：API 稳定、生态成熟、适用于事务型与逐行计算场景，并且在过去几十年中几乎成为数据库访问的事实标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题在于，这套接口体系本质上是行导向的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现实是，Snowflake、DuckDB、ClickHouse、BigQuery 等主流分析型数据库，内部早已全面列式化。这意味着，每一次通过 ODBC / JDBC 拉取数据，系统都要经历一次高成本的转置：从列式内存转换为行，再在下游分析中重新转回列式结构。这不仅带来了显著的 CPU 与内存开销，也让数据在系统中反复“变形”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 特别强调，这里的转置并不是抽象意义上的重排，而是真实的数据拷贝与类型转换。在数据规模扩大后，这种成本会呈指数级放大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ADBC：把统一 API的理念带入列式世界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADBC（Arrow Database Connectivity）正是为解决这一结构性矛盾而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从抽象层面看，ADBC 与 ODBC / JDBC 非常相似：应用程序面对的是统一 API，通过不同驱动与不同数据库交互。但关键差异在于，ADBC 是列导向的，其数据交换格式直接采用 Apache Arrow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当数据库本身已经以列式形式返回结果，且能够直接输出 Arrow 数据时，驱动几乎无需做任何转换，便可将结果原样交付给应用侧——零拷贝、无转置。这不仅显著提升了性能，也让数据在更早阶段就处于可分析、可计算的理想形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便在数据库本身并非列式、或并未原生支持 Arrow 的情况下，ADBC 也允许在驱动层完成一次性转换，从而让应用侧始终面对统一的数据模型，而不必管理多套复杂连接器体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用数据说话：跨语言的性能对比与真实收益&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享的核心说服力，来自大量现场演示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Python 示例中，Topol 对比了通过 ODBC 与 ADBC 从 Snowflake 拉取数据的耗时。即便在启用缓存、排除查询执行成本的情况下，ADBC 在 10 万行与 100 万行数据规模下，仍然表现出明显优势：数据量越大，性能差距越明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，ADBC 返回的数据可以直接被 Polars 等基于 Arrow 的 DataFrame 库消费，几乎没有额外转换成本。这意味着，性能提升并不仅体现在拉数据更快，而是贯穿整个分析链路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样的结论，在 Go 和 R 的演示中得到了重复验证。跨语言的一致性，反过来也印证了 Arrow 与 ADBC 设计上的语言无关性——它们优化的是数据形态本身，而非某一语言生态的实现细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不止查询：流式摄取与系统间数据流动的新可能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享的后半段，Topol 将视角从查询结果返回扩展到更复杂的数据流动场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他展示了如何通过 ADBC，将 Snowflake 中的一百万行数据，以流式方式直接摄取到内存中的 DuckDB。整个过程无需先完整加载结果集，数据以 Arrow Record Batch 的形式持续流动，类型信息在传输过程中被完整保留，整体耗时不到四秒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一演示揭示了 ADBC 的另一层意义：它不仅是一种更快的查询接口，也是一种系统间高效、可组合的数据通道。当数据能够以统一、零拷贝的列式格式在系统间流动时，ETL、数据同步乃至多引擎协同分析的复杂度，都有机会被重新定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 并没有在结尾试图宣告 ODBC / JDBC 的终结。相反，他反复强调，这些技术在事务型与行式计算场景中仍然合理且必要。但对于分析型系统而言，ADBC 所代表的，是一种更贴合现代数据架构的方向：让数据尽可能早地进入列式、分析友好的形态，并尽可能少地在系统间反复转换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</link><guid isPermaLink="false">https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</guid><pubDate>Wed, 28 Jan 2026 09:15:00 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>内存一年疯涨170%，云账单里的“隐性成本”该算清了</title><description>&lt;p&gt;2025 年下半年，存储价格又一次成为行业聚焦点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多家市场机构统计显示，2025 年三季度跟四季度，DRAM 和 NAND 价格一路攀升。根据 Tom&#39;s Hardware 披露的数据，2025 年 DRAM 合同价同比上涨幅度高达 171.8%，创下历史新高。此轮上涨跟 AI 数据中心建设拓展、服务器需求集中释放紧密相联，还直接引发企业 IT 基础设施采购成本上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于依赖自建数据中心或中小 IDC 的企业来说，这种变化带来的冲击尤为剧烈。硬件采购从一次性预算问题，演变为难以预测的长期成本风险。服务器、SSD 和内存条的价格不再稳定，交付周期也更不确定。企业在扩容时不得不承担高价买入、供货延迟的双重压力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，将硬件采购压力转化为按需付费的运营支出，把价格波动风险转移给云服务商，正在成为越来越多企业的选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题并未因此结束。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着业务迁移到云端，企业发现云账单中存储与内存的占比仍在持续上升，即便算力配置并未明显升级，总体成本依旧水涨船高。部分团队开始反思：问题是否仅和数据量增多有关，还是资源使用方式本身就存在不合理的地方？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，多数云实例依旧按固定的 CPU 与内存配比来交付，诸如 2 核 4GB、4 核 8GB 的规格。早期，这种设计可简化资源管理，推动了云计算普及，但如今业务形态有所改变，企业系统一般得同时支撑多样业务，各业务对于算力、内存的消耗不一样，固定规格愈发难以契合实际需求。这导致企业要么部分资源长期闲置，要么不得不面对业务在高峰阶段出现性能瓶颈的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当内存价格进入上行周期，这种规格错配带来的浪费被进一步放大：闲置的不再只是资源本身，而是越来越昂贵的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是基于这样的背景，云基础设施走到新的路径分岔口：是继续就资源本身实施配置，还是转变方向围绕应用需求设计算力供给方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期面向中国区合作伙伴召开的发布会上，华为云对 Flexus 云服务器系列规格及性能进行更新，并且展示了其在各种业务负载下的运行表现。该实例基于华为云首创的柔性算力技术，打破 CPU 与内存的固定绑定关系，使企业能够按真实业务需求配置资源，从源头减少内存浪费，并结合智能调度与应用级加速改善长期运行稳定性与算力资源投入产出比。本文将从行业环境变化与技术实现等层面，剖析这种模式背后的思路，以及它所代表的云服务器演进方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云服务器，开始不太“合身”了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云服务器长期采用固定 CPU 与内存的配比，是工程上的一种取舍考量。早期云平台首先得解决的是规模化交付和稳定调度的问题，采用固定规格利于资源池管理，同样便于容量规划及计费设计。当业务形态呈现相对单一阶段，这样的方式尚可接纳。但究其本质它是从平台管理成本角度设计的，并非从业务负载的角度出发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今业务已不再是单一模式，电商、内容分发、数据库、缓存、AI 推理在一套系统中同步协同运行，对 CPU 以及内存的需求差别明显，固定规格无法精准对应实际负载，企业只能采用超出实际所需的实例型号。云服务器规格跟应用需求普遍不匹配，用户往往被迫去为用不到的算力和内存付费，引发大量资源的闲置浪费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2a/2afb7d843ca99668267a105cec3e77e2.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源浪费只不过是表象罢了，更深层的问题体现为性能优化的复杂度。现实的业务部署不仅涉及操作系统选定，还包含网络参数、系统参数以及应用配置参数。数量往往达到数千级别，缺少专家经验积累，难以达成稳定的最优配置。单是内核跟应用层的参数组合，就已超出普通团队可控范围，调优所用的周期漫长，效果也难以把控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60e1fc3271a533f924d017f8da492c35.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从较长的时间阶段看，云服务器本身一直在不断演变，最初的资源虚拟化阶段，是把物理服务器标准化成可租借的实例；紧接着进入弹性规模阶段，采取自动伸缩的方式去应对流量变化，这两个阶段处理的是存不存在以及是否充足的问题，当下已经迈入第三阶段，关注焦点转向使用是否高效。过去，固定实例曾是工程优势，如今却愈发像是一件穿着不合身的衣服。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;柔性算力：从“卖规格”到“卖能力”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;怎样让资源本身更贴近应用？在 Flexus 云服务器 X 实例产品的设计里，华为云引入了柔性算力这一概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Flexus X 实例里，柔性算力首先体现在规格形态的调整变化上。传统实例一般仅仅可在少量固定比例中选择 CPU 跟内存配置，而该实例支持按业务需求实施更精细的组合配置。发布会现场提到，所有 X 实例均支持多种非常规的 CPU/ 内存配比，包括 3:1、2:5、3:7 等组合。这可减少由规格不一致引起的资源闲置，让用户更接近按实际负载付费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而规格数量增加，并非表示问题自动就解决了，其关键是系统如何判断哪种配置更合适。传统调度大多依据节点上剩余的 CPU 与内存。新方式需要领会业务负载本身，涵盖资源使用结构，以及随时间的变化趋势。Flexus X 实例本质上不再是调度 CPU，而是实际的业务场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就工程实现而言，这种转变依赖底层架构的支撑，Flexus X 实例借助华为云自研的擎天 QingTian 架构和瑶光云脑调度系统得以实现，经由计算、存储和网络资源的解耦操作，提高了资源组合的自由度，也增强了非标准规格运行状态下的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，柔性算力还意味着配置不再是一次性决定，实例运行时会一直对资源使用状况进行评估，系统会判断当前配置跟负载是否相符，进而给出调整建议，而且还支持算力规格热升降的独家能力。从这个层面看，Flexus X 实例的转变不只是规格数量增多，它更像是把算力从提前打包好的商品，变成可持续优化的能力，实现“应用驱动算力”的最优体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关键应用加速：算力之外的第二条性能曲线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus X 实例不单单改变了资源形态，还进一步深入应用执行层，解决了算力配置合理系统却依旧不稳定的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次规格升级，华为云为数据库以及中间件类的负载引入专属应用级加速机制。Flexus X 实例针对 PostgreSQL、Memcached、MySQL、Redis、Nginx 提供了独立的一键加速能力，由 X-Turbo 应用加速引擎统一驱动。此类优化不会对用户的使用途径做出改变，实例创建结束之后即可启用，平台会把调优工作完成，用户无需插手复杂参数的配置。发布会现场，华为云对该能力实测演示，在 PostgreSQL 的使用场景下，Flexus X 实例的吞吐量达到 2.1 万 + TPS，大概为同规格业界旗舰型实例的 3.4 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/810527295eccdf4bca355ee9ac46c7ba.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就数据库这类系统而言，峰值性能仅仅属于一方面，更为关键的是高负载持续状态下的稳定输出能力。业务系统更易受诸如延迟抖动、连接堆积等问题的干扰，而不是单次压测形成的成绩。X-Turbo 的设计目标之一正是实现性能优化长期运行状态下的吞吐与响应稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;跟应用级优化同步进行的是，实例规模的进一步扩展。新一代 Flexus X2e 实例的 x86 规格从原本的 32U128G 提升至 64U256G，多核算力提升了约 30%；新增 Flexus KX1 鲲鹏实例，最高可达 80U320G，以覆盖大数据处理、内存数据库这类资源密集型场景。这意味着应用加速机制不再受中小规格环境约束，能在规模更大的资源池里发挥作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d2f568a839cc6be536e32d0c7def3623.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一系列的变化显示出云服务器性能边界正在转移。过去，性能更多由 CPU 规格和内存容量决定。而如今，应用执行路径、参数组合的方法及调度策略成为同等要紧的变量，在固定规格的时代里，这些优化由用户自己承担，而于 Flexus X 实例中，它们被纳入到算力交付范畴，正是从这一意义出发，云服务器竞争不再只是资源规模大小的比拼，而是发展为聚焦运行效率的系统工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从工程能力到真实落地：柔性算力如何进入生产系统&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一项新的算力供给方式，能否切实进入生产系统，首要取决于它是否具备充足的稳定性与可用性。Flexus X 实例可靠性设计向华为云旗舰级云服务器标准看齐，实现单 AZ 99.975% 的可用水平，还有跨 AZ 99.995% 的可用性。这暗示柔性算力没有以牺牲稳定性为交换代价，而是可直接承受核心业务负载的基础设施形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了稳定性这一点，规模化使用还取决于运维体系自身是否具有确定性，Flexus X 实例在华为云既有的 SRE 运维体系框架内运行，强调借助标准化变更、容量预测与故障演练减少系统行为的不确定性，实现大规模实例并发运行的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业落地的实际来看，柔性算力最先进入的并非那种单一业务场景，而是负载结构繁杂、资源使用波动大的系统类型。其已经在医疗电商平台迁移、连锁零售系统、医药行业信息化平台、游戏服务器迁移等场景大规模部署，用以承载数据库、中间件及核心交易服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中软国际智能集团云业务部副总经理王春玉在发布会上分享，团队为某大型生物医药集团搭建系统的时候，引入 Flexus X 实例作为数据库及业务服务的主要承载环境，在原有系统架构未改变的情形下完成迁移，而且在性能满足要求的前提下，达成约 30% 的综合成本下降。王春玉还谈到，其团队服务的一家专业酒水直营连锁品牌，把部分核心业务迁移到 Flexus X 实例而后，通过规格按需匹配与资源利用率优化，实现整体云资源成本约 15% 的下降。这些亮眼的结果主要源于两方面：一是实例规格跟业务负载的匹配度有所提升，降低了长期闲置资源的数量；二是借助应用级加速与调度优化，降低了单位业务量所需的算力规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这些真实的实际部署案例能看出，Flexus X 实例的用户一般有几个共同特性：业务负载呈现明显波动，系统结构相对复杂，然而运维及架构团队的规模较为有限，同时对长期云资源的成本敏感度较高。Flexus X 实例在未对业务形态本身作出改变的情况下，却降低了基础设施对业务扩展所施加的约束强度，让按照业务形态去配置算力成为可践行的工程实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，未来企业买的不再是服务器，而是业务效率。Flexus X 实例凸显了云服务器设计思路的一次转向：由“卖规格”过渡到“交付能力”，从“静态资源”过渡到“智能算力”，在 AI 成为主流计算负载的未来，此种转变大概率不会再是差异化优势，而是云基础设施的必要门槛。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</link><guid isPermaLink="false">https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</guid><pubDate>Wed, 28 Jan 2026 08:06:57 GMT</pubDate><author>棱镜</author><category>华为</category><category>云计算</category></item><item><title>数据土壤，决胜 AI 下半场：一场关于企业 Data+AI 战略的炉边思辨</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据与 AI 的变革正以前所未有的速度重塑产业格局，2026 年年初，Snowflake 与 InfoQ 联合呈现的“Make it Snow”2025-2026 Data+AI 年度时刻，汇聚了来自医疗、制造、汽车等领域的顶尖专家，共同探讨数据智能的前沿突破与未来方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场以“炉边对话”为形式的深度交流，不仅回顾了 2025 年 Data+AI 领域的认知重构，更围绕 2026 年十大战略命题展开思辨，为行业奉上了一场兼具思想深度与实践价值的智慧盛宴。与此同时，各位专家还分别留下了对 2026 年的一个技术预言，&lt;a href=&quot;https://www.infoq.cn/video/vy4ZYjr71ohKEOGp9tXz&quot;&gt;点击此处&lt;/a&gt;&quot;可快速了解这些极具前瞻性的洞见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文力求完整呈现这场思想碰撞的核心洞察，见证数据与 AI 如何从技术概念转化为驱动产业革新的核心力量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2025 年带给你的三个认知突破&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：回看 2025 这一年，Data + AI 的很多变化，往往是在一次次具体实践中慢慢显现出来的。可能是一次惊艳的产品体验、一次真实落地的尝试，也可能是一个业务场景，或者一段走弯路之后的重新理解。正是在这些时刻里，我们对 Data + AI 的判断发生了变化。请每位嘉宾回顾这一年，有没有哪几个真正的 Aha Moment（顿悟时刻），让你感到茅塞顿开，认知被重构了，如果让你选 3 个这样的关键时刻或经历，它们分别是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬（Snowflake 亚太及日本地区解决方案工程副总裁）：2025 年大模型演进呈现出&quot;疯狂超车&quot;的态势，从年初 DeepSeek 将推理成本降至十分之一，到年中 Claude 展现资深工程师级编程能力，再到 Gemini3 在科学推理领域的突破，最终以 ChatGPT 5.2 模型实现多模态无缝切换，这些迭代揭示了一个核心认知：技术选型的关键不在于追逐当下最优，而应基于特定应用场景、预算限制及部署规划进行理性决策。&amp;nbsp;与此同时，AI 安全风险愈发严峻，如 2025 年 6 月发现的“隐形提示词注入”漏洞，攻击者可利用邮件中肉眼不可见的指令诱导 AI 自动读取并外泄网盘内的敏感信息。这充分说明，尽管 AI 功能日新月异，但在企业落地评估中，安全保障必须始终位列首要地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦非（罗氏中国 Snowflake 数据平台技术负责人）：2025 年 Data+AI 的变化不少，其中有这样三个关键的时刻：其一，罗氏诊断提出“三重确定”数字化战略：实现从实验室到临床的 AI 穿透。&amp;nbsp;通过整合肝癌辅助诊断算法与肝病管理数字化平台，AI 已能驱动从影像学检查建议，到定期随访计划，再到生活方式干预的全链条行动；其二，第八届数字中国建设峰会数字医药专题会议：从监管高墙到智慧灯塔的转型。&amp;nbsp;药监局推动的“AI+ 药物监管”模型，使企业从规避监管转向主动参与标准定义；其三，罗氏制药发布小罗智星 AI 科研解决方案：从赋能工具到科研主体的蜕变，“小罗智多星”AI 科研方案覆盖选题、文献解读到论文撰写全流程，在 700 余家医院落地 600 多个项目，证明 AI 不仅提升效率，更能激发和扩展人的创造力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高杰（蔚来汽车人工智能研发负责人 &amp;amp; 高级总监）：2025 年大模型演进带来的首个关键认知源于 DeepSeek 对推理技术的“祛魅”。相较于 OpenAI o1 最初局限于数学等可验证领域的神秘感，DeepSeek 不仅证明了高逻辑推理能力具有从特定学科向通用场景迁移的普适性，更将原本封闭的技术转化为行业易于获取的普惠资源。紧随其后的第二个转折点是 Claude Code 工具的诞生，它直观地展示了 AI 如何走出实验室假想、真正解决现实世界长程任务的理想形态。这两大突破推动我们重新定义汽车座舱：从交通工具到“有温度的情感伙伴”，需要拟人交互、全能帮手、深度理解三方面能力的协同进化，而数据正是实现“懂你”这一核心价值的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈砚琳（Snowflake 行业实践专家）：工业场景的认知重构聚焦于数据基础设施的价值重估。首先，多云异构环境下的数据互联成为可能，Snowflake 的跨云部署与合规特性，解决了跨国企业数据孤岛与跨境流动难题；其次，Cortex Analyst 等工具重塑了业务 - 技术协作模式，将两到三周的需求响应周期压缩至实时交互，释放了业务用户的数据分析潜能；最后，数据迁移的无缝衔接验证了平台兼容性的重要性，Snowpipe Streaming 等工具实现了 ERP、CRM、IoT 等多源数据的高效集成，证明基础设施的弹性决定了 AI 应用的落地速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜（白鲸开源 CEO）：首先是&amp;nbsp;“开源加成”。长年积累、架构稳定的开源项目已被大模型深度内化，AI 成为了最了解项目细节的“专家”，这赋予了开源项目全新的技术生命力。其次是&amp;nbsp;个体能力边界的跨越。AI 已从简单的对话进化为高质量的结果交付，即便非技术背景人员也能通过精准提示产出极具专业深度的技术内容。大模型突破了物理时间的限制，极大扩张了人的认知与能力边界。最后是&amp;nbsp;从交互到自动化的范式转移。以 DolphinScheduler 的演进为例，传统的“拖拉拽”操作正被意图驱动的自动流生成所取代。未来，人机对话将简化为纯粹的需求提出，由模型间自主协同完成复杂流程，实现真正的全自动化代理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;史少锋（Datastrato VP of Engineering）：1）2025 年 AI 的核心演进可归纳为&amp;nbsp;从“知识问答”向“全能代理（Agent）”&amp;nbsp;的全面跃迁，MCP 标准协议的开源使 LLM 操作外部软件接口的门槛大大降低，MCP 标准协议是 Agent 技术普及的关键催化剂；2）Claude 等模型在自动化编程领域的出色表现，则颠覆了传统软件的开发模式，AI&amp;nbsp;从“完成代码补全”进化为“全功能、全流程的开发助手”；3）新版千问 APP 的“奶茶点单”功能则展示了个人数字助理的新形态，通过 API 调用、位置感知与无缝的支付集成，实现从语言下达指令到订单交付的端到端闭环，预示着个人数字助理时代的加速到来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞（数势科技 AI 负责人）：Data Agent 产品的实践带来三个认知转向：从只盯技术参数看，到更要盯着“人”看：&amp;nbsp;当产品形态只做 Chat 的时候，仅关注准确率和速度会陷入“Data Search”陷阱，而融合大模型知识才能创造超出预期的价值；产品形态回归经典的必然性：dashboard 等经典形态仍是数据交互的有效环境，Agent 需要可沉淀的操作空间；从功能博弈到专业信任：&amp;nbsp;传统项目执着于“功能清单”式验收，导致产品在竞品间的比拼中陷入功能堆砌的泥潭，逐渐丧失核心价值主张。Data Agent 逐渐要从功能型的清单型交付，走向专业型交付，这也对交付人员提出了更高的 AI 认知要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;十问 Data Strategy，AI Strategy&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q1：企业在打造统一的 Data + AI 平台时最大的挑战是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬指出，企业在构建统一 Data+AI 平台的过程中，真正的深层挑战并非源于 AI 模型本身的技术上限，而在于&amp;nbsp;数据土壤。他形象地将这一挑战比作“果园”的经营：单一模型（树苗）的验证可以通过局部资源的倾斜（温室培育）快速见效，但若要实现企业级的规模化部署与持续产出，则必须依赖于高质量的“数据土壤”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在规模化落地阶段，企业面临的考验在于数据土壤的“有机质含量”与“灌溉系统”是否完备。这具体体现为：数据能否支撑 AI 跨部门、跨场景进行深度的洞察集成；在权限下放至一线管理人员时，企业是否具备精细化的安全隔离与治理能力；以及在 AI 输出指令或决策后，系统是否拥有完整的可观测性（Observability）以实现追溯与审计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实践数据看，企业招标需求中 80%-95% 聚焦于数据管理、存储效率与安全治理，仅 5%-20% 涉及模型训练与调优。这表明 CIO 们已清醒认识到：没有高质量、可治理、安全可控的数据基座，AI 应用终将沦为“沙上建塔”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q2：2026 年&amp;nbsp;Agentic AI&amp;nbsp;应用会迎来爆发吗？如何确保这些 AI 应用产生可信、可解释的决策？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬认为，2026 年 Agentic AI 将实现“突破”，而非“爆发”，其差异在于行业与企业的数字化成熟度分化。技术演进需经历学术突破、试点验证、规模化部署、普适应用四阶段，目前多数企业仍处于试点向规模化过渡的关键期。爆发的临界点在于数据基座的就绪程度，&amp;nbsp;当企业能将多模态数据高效整合、实现基于角色的权限管理、并建立 AI 决策的全链路可观测性时，Agentic 应用才能真正落地。关于可信性，需分场景定义标准：消费推荐等容错场景可接受一定误差，而财务报告等严肃场景则要求零容错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q3：如何看待开源与闭源在 Data+AI 领域的博弈？开源技术和社区力量将在 2026 年发挥怎样的作用？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜提出“社区价值大于代码开源”的观点，认为 2026 年将迎来“Community over Code”&amp;nbsp;的范式转移。随着 Agentic AI 的发展，代码实现的重要性下降，而问题定义、需求拆解等“提问能力”成为核心。开源社区的价值将体现在：汇聚多样化问题视角、形成集体智慧沉淀、推动技术普惠化。史少锋深表认同，他以 AI 编程实践为例：当机器能高效生成代码时，人类的核心竞争力转向创意与需求定义，社区的 Brainstorming 比代码提交更具价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q4：当人人都问 AI，知识社区注定会没落吗？新的具有“活人感”的经验会从哪里生长？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜认为，以问答为核心的知识社区将不可避免地衰落，因为 AI 能提供更即时、个性化的答案；但以兴趣为纽带的讨论社区（如 Reddit）将崛起，这类社区的价值在于“活人感”的经验碰撞，观点的交锋、情感的共鸣、以及非结构化的创意激发。郭炜进而提出了一个“暴论”——未来 90% 的互联网信息可能由 AI 生成，人类创作将成为“稀缺品”，类似毛笔字的艺术价值。李飞补充道：新经验生长可能将呈现“无形化”特征：非正式的一对一交流、线下研讨会，都可能成为创新源泉。正如直播中嘉宾们的即兴讨论，这种实时互动产生的洞见，正是 AI 难以复制的“活人感”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q5：2026 年工业 AI 实现规模化突破的关键点在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈砚琳指出，工业 AI 的规模化突破不在算法本身，而在于&amp;nbsp;数据基础设施的系统性构建。随着预测性维护、缺陷检测及智能排产等算法趋于成熟与同质化，AI 算法本身已难以构筑企业的核心护城河。真正决定胜负的，是企业是否拥有坚实统一且可靠的数据平台。工业场景的数据极其庞杂，不同设备以迥异的频率和格式实时产生海量数据，若缺乏长远规划，极易陷入数据孤岛的困局，阻碍后续的数据消费。因此，企业成功的 AI 应用必须建立在对零散数据的合理规划与统一摄入基础之上。一个合格的数据系统，应确保用户能精准获取所需数据，并以预期的形式高效消费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q6：在医药健康场景里，您最看好 2026 年 AI 落地的哪一个高价值方向？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦非认为，AI 驱动的候选药物分子生成与优化&amp;nbsp;将成为 2026 年医药健康领域的高价值方向。其核心逻辑在于：业务上，它直接切入研发核心，通过缩短早研周期与降低筛选成本实现立竿见影的财务回报；合规上，随着临床研究法规的完善，内部数据闭环下的 AI 研发已具备明确路径；技术上，继 AlphaFold 突破后，生成式分子设计已进入临床验证的爆发期。此外，在战略协同上，药企可利用自身在肿瘤、免疫等领域的优势数据，构建“模型 + 数据 + 药物”的增强闭环。而在实施维度，通过跨职能团队协作、高效数据治理以及与顶尖 AI 平台的深度耦合，能够有效管控技术复杂度与实施风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q7：在大模型与数据智能加持下，如何将汽车重新发明一遍？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高杰提出，当前汽车行业已从“能源竞争”上半场进入“智能化竞争”下半场。智能汽车的第一性原理，在于打造一个集“智慧空间”与“情感伙伴”于一体的拟人化交互系统。实现这一愿景需深度的软硬一体化布局：硬件层&amp;nbsp;需构建高带宽、低延迟的中央计算架构；中间层&amp;nbsp;需设计面向 AI 的操作系统（如 SkyOS）与数据中间件，确保整车跨域数据的自由流动与实时调度；应用层&amp;nbsp;则通过 NOMI Intelligence 等智能软件系统，将底层能力转化为具备主动智能的 Agent 体验。通过这种从芯片到应用的全栈叠加，汽车正从单纯的交通工具进化为全知全能的数字化情感伙伴，这也已成为行业共识的赛道终局。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q8：当数据分散在多云和多种 AI 工具中时，我们是不是在制造新的孤岛？该如何打破？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;史少锋认为，面对多云环境与 AI 工具普及带来的数据孤岛挑战，应从技术效能与数据治理两个维度辩证分析。首先，AI 技术的引入，一方面降低了数据工程的门槛，通过加速 Data Pipeline 的开发与自动化取数流程，AI 能够从技术层面有效提升数据开发和加工的效率，缓解传统数据孤岛的痛点；然而另一方面，随着 AI 应用的深化，大量的信息在跟 AI 的交互中产生，若缺乏合理的沉淀与治理机制，既可能造成知识流失，也可能演变为企业的“信息黑洞”。破局的关键要从组织、技术选型、业务等多个层面协同；当下原有的架构和实践会被颠覆，但新标准的产生还有待时日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q9：当 AI 都能替我打工了，我为啥反而更累了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞认为这或许是&amp;nbsp;“杰文斯悖论”在个体生产力领域的重现，AI 极大缩短了单项任务（如撰写代码或制作 PPT）的耗时，但在组织效率博弈中，这种提效并未转化为闲暇。其次，角色身份从“生产者”向“监管者”转型。AI 虽能自主生成海量内容，但由于其可信度尚无法完全托管，从业者必须承担起更沉重的审核与融合责任。从另一个维度看，AI 极大地降低了创意落地的门槛。这种“即时验证”能力的释放，也导致了实践频次的增长，但也有可能带来“累并快乐着”的幸福感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬进一步指出，AI 将人类从重复性工作解放后，大脑需处理更深度的思考任务，如同项目经理协调多个 AI Agent，这种认知负荷的增加带来“心累”体验。但这种累是创造性的、价值增值的，正如从“体力劳动者”到“知识工作者”的转型，AI 时代的“累”预示着人类价值向更高维度跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q10：在这一轮数据基础设施行业的整合洗牌中，数据链上下游最值得关注的协同创新机会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬认为在数据基础设施行业的整合洗牌中，最值得关注的协同创新机会在于&amp;nbsp;“将算法与用户体验带向数据，而非搬运数据”。以 Snowflake 并购 Observe 为例，这种上下游整合揭示了三大核心逻辑：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，减少数据孤岛的产生。通过将企业的业务数据（如财务、人力）与底层运营数据（如系统日志、安全数据）整合至统一平台，从源头上降低了数据孤岛在企业内部数据生态中的比率。其次，变革软件开发与交付模式。当应用直接构建在数据平台之上，开发者无需再关注算力寻址或数据建模，实现了运算、算法与用户体验同数据的无缝衔接。最后，驱动跨职能的协同效率。上下游的打通打破了业务人员与运维人员的沟通壁垒，使得“系统在线时长对营销的影响”等跨域问题能在统一平台上快速得到解答。这种将计算能力向数据侧下沉的模式，不仅规避了数据搬运带来的额外风险与人力开销，更构筑了完整且受控的平台级协同优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场年度对话深入剖析了 Data+AI 时代的变革逻辑，并达成了一个关键的行业共识：一个坚实、可靠、治理良好的数据基座，不仅是 AI 战略从愿景走向现实的唯一路径，更是决定企业智能进化上限的核心势能。&amp;nbsp;与会专家通过回顾 2025 年的“落地实战”并展望 2026 年的战略命题，清晰地揭示了产业图景的变迁——技术的竞争焦点已超越模型算法本身，全面转向数据质量、安全治理与平台工程化能力的综合比拼。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;站在 2026 年的关口，数据从业者们正身处一个历史性的交汇点。AI 的爆发式增长不仅带来了无限的创新可能，也对底层的“数据土壤”提出了近乎苛刻的要求。面对智能时代的不确定性，构建一套稳健、透明且具备确定性治理逻辑的数据体系，已成为从业者们共同的使命。作为全球数据云的引领者，Snowflake 始终致力于打破数据的孤岛与边界，未来将继续与广大数据从业者并肩同行，扎根数据深处，在波澜壮阔的智能变革中，以笃定的数据基座驱动业务的无界创新，共同定义 Data+AI 的下一个黄金时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;错过直播的朋友可以&lt;a href=&quot;https://www.infoq.cn/video/uqAQNcO0Ct5oJxoC1ARK&quot;&gt;点击此处&lt;/a&gt;&quot;观看完整版回放～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/b9Wb9tEFrgzAcxUt5Kv1</link><guid isPermaLink="false">https://www.infoq.cn/article/b9Wb9tEFrgzAcxUt5Kv1</guid><pubDate>Wed, 28 Jan 2026 08:03:14 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>当雪花落在中国 遇见企业 AI Strategy 的变革时刻 ｜ 2025-2026 Data+AI 年度时刻精华版</title><description>&lt;p&gt;回看 2025 的三次认知突破，走向 2026 的“十问 Data Strategy 与 AI Strategy”。这是一场基于真实实践的年度复盘，也是一次面向未来的集体判断。Data+AI 年度时刻精华版现已上线，8 位行业先行者，郑重写下了他们对 2026 的技术预言！&lt;/p&gt;
</description><link>https://www.infoq.cn/article/vy4ZYjr71ohKEOGp9tXz</link><guid isPermaLink="false">https://www.infoq.cn/article/vy4ZYjr71ohKEOGp9tXz</guid><pubDate>Wed, 28 Jan 2026 07:50:12 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>鸿蒙好文月度精选丨2026_2月刊</title><description>&lt;p&gt;欢迎关注&lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 01：15 年大数据老兵鸿蒙“造梦”，父女联手打造亲子游戏 App&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，从不缺乏跨界探索的身影。徐俊宸便是其中一位特殊的存在：深耕大数据领域多年，从数据产品经理到大数据讲师，他的职业生涯始终围绕数据打转；而一次偶然的鸿蒙论坛经历，让他萌生了开发APP的想法。最终，他以女儿课堂上的猜数字游戏为蓝本，与女儿一起打造出《猜数字大师》游戏应用，在跨界鸿蒙开发的道路上，既攻克了技术难关，也收获了别样的亲子时光。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&quot;&gt;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 02：老板发话鸿蒙APP 一定要上线，但不加人！分享一个快速实现跨端开发的技术方案&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;估计这是26 年开发团队的普遍现状，鸿蒙不得不做，人又不可能加。&lt;/p&gt;&lt;p&gt;毕竟到了26 年，HarmonyOS 6 终端设备也突破了 3.2 亿， 卓易通又被人骂得半死，所以开发一个原生鸿蒙 APP 必须摆上桌面了。&lt;/p&gt;&lt;p&gt;在资源有限的前提下，像我们这种千万以下日活的中小团队必须在以下三种路径中做出抉择：&lt;/p&gt;&lt;p&gt;纯原生重写：体验最好，但成本高到离谱，而且维护困难。&lt;/p&gt;&lt;p&gt;Flutter/RN：Flutter 是谷歌推出的，竟然不支持鸿蒙。&lt;/p&gt;&lt;p&gt;Web Hybrid (H5)&amp;nbsp;：成本最低，但性能体验太差，特别在鸿蒙上，容易被人骂。&lt;/p&gt;&lt;p&gt;目前看，第四种方案算是解法：&amp;nbsp;“&amp;nbsp;小程序容器技术&amp;nbsp;”&amp;nbsp;（&amp;nbsp;Mini-Program Container&amp;nbsp;）&amp;nbsp;比 H5 性能高，比原生开发也省事。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/article/89d1ce30eeac3a82759cebd4a&quot;&gt;https://www.infoq.cn/zones/harmonyos/article/89d1ce30eeac3a82759cebd4a&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 03：待到山花烂漫时：鸿蒙开发者用代码灌溉鸿蒙花园&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;银行业如何在鸿蒙转型中抓住机遇、快速进化？&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的“金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X框架迁移及原子化服务开发等技术突破，历时21个自然日完成HarmonyOS NEXT核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配HarmonyOS NEXT的故事。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&quot;&gt;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 04：元服务一站式平台：告别碎片化，开启All in One 一站式经营新纪元&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了给元服务开发者提供更聚焦、更高效的管理体验，我们在AppGallery Connect 平台上正式推出了元服务一站式平台&amp;nbsp;。&lt;/p&gt;&lt;p&gt;随着元服务能力不断丰富，相关功能分布在平台的多个模块中。为了帮助您更便捷地查找和使用所需功能，避免在无关菜单间跳转，我们构建了这个统一的专属工作空间，旨在聚合所有元服务相关能力，简化您的操作流程。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/gNwyuirySAxj5hQjsX0v&quot;&gt;https://www.infoq.cn/article/gNwyuirySAxj5hQjsX0v&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 05：“新”意十足 ·&amp;nbsp;HarmonyOS 模板 &amp;amp; 组件 （本次上新：社交、简历、翻译模板；聊天窗、购票等组件）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鸿蒙生态为开发者提供海量的HarmonyOS 模板/组件，助力开发效率原地起飞&lt;/p&gt;&lt;p&gt;更多内容，一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/market/prod-list?origin=template&amp;amp;ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;生态市场组件&amp;amp;模板市场&lt;/a&gt;&quot;&amp;nbsp;, 快速应用&lt;a href=&quot;https://developer.huawei.com/consumer/cn/doc/start/components-integration-deveco-0000002218625313&quot;&gt;DevEco Studio插件市场集成组件&amp;amp;模板&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt; 一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/solution/harmonyos/?ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;HarmonyOS 行业解决方案&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/52L9NAr6TAtLspJrKMKh&quot;&gt;https://www.infoq.cn/article/52L9NAr6TAtLspJrKMKh&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 06：“新”意十足 ·&amp;nbsp;HarmonyOS 模板 &amp;amp; 组件 （本次上新：新闻资讯 /uni-app、绘画模板；通用搜索、会员组件）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鸿蒙生态为开发者提供海量的HarmonyOS 模板/组件，助力开发效率原地起飞&lt;/p&gt;&lt;p&gt;更多内容，一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/market/prod-list?origin=template&amp;amp;ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;生态市场组件&amp;amp;模板市场&lt;/a&gt;&quot;&amp;nbsp;, 快速应用&lt;a href=&quot;https://developer.huawei.com/consumer/cn/doc/start/components-integration-deveco-0000002218625313&quot;&gt;DevEco Studio插件市场集成组件&amp;amp;模板&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt; 一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/solution/harmonyos/?ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;HarmonyOS 行业解决方案&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/MzGXuEGGBI3NdVGLUjRR&quot;&gt;https://www.infoq.cn/article/MzGXuEGGBI3NdVGLUjRR&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👉更多鸿蒙精选好文，持续上架中，欢迎扫码加入「InfoQ鸿蒙开发者交流群」，交流技术，也可联系「小助手」约稿~&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bf7ecdf3d9166706d0b9b6263a6b802.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👀也欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;</description><link>https://www.infoq.cn/article/dSNvzJS92gsUFI79ZpZK</link><guid isPermaLink="false">https://www.infoq.cn/article/dSNvzJS92gsUFI79ZpZK</guid><pubDate>Wed, 28 Jan 2026 07:36:45 GMT</pubDate><author>付秋伟</author><category>HarmonyOS</category></item><item><title>最极致的数据安全计算，迎来产业化的“临界时刻”</title><description>&lt;p&gt;当数据成为最重要的生产要素，计算却必须在“不被看见”的前提下完成——全同态加密，正在从一项“隐私计算圣杯”，变成现实世界必须回答的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年，Cloudflare 的一次区域性故障，导致数百家金融机构、政府服务与媒体网站瞬间停摆，暴露出当关键基础设施集中于少数平台时，一次攻击或故障便能引发全球性“数字海啸”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一家 AI 巨头遭受的黑客攻击，不仅导致模型数据泄露，更因训练数据污染，引发了后续模型输出的大规模偏差——单一漏洞，足以毒化整个系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以如果把过去两年的技术浪潮浓缩成一句话，那大概是：数据正在集中，而风险也在集中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;同态加密：不是更复杂，而是更简单&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型被部署在云端，金融、医疗、政务等最敏感的数据，开始以 API 的形式流动；跨机构、跨区域、跨国家的数据协作，从“能不能做”变成了“必须要做”。现实世界正在不断抛出一个看似简单、却始终无解的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有没有一种计算方式，可以在数据不被看见的情况下，把事情算完？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是同态加密重新被推到舞台中央的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实在隐私计算领域，同态加密并不是唯一的技术路线，但同态加密的好处是它非常简洁，直接在数学层面改变计算规则：数据自始至终以密文形式存在，计算在密文之上完成，结果在授权方手中解密。在理想状态下，整个计算过程中不存在“被看见的数据”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简而言之：从数学上讲，它的安全性可以被证明；从系统上看，它反而让架构变得更简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正因为如此，同态加密的安全性并不依赖系统配置，而是可以被数学证明。这种“安全内生于算法”的特性，使它在理论上具备一种近乎极端的吸引力——也是为什么它常被称为隐私计算领域的“圣杯”。&lt;/p&gt;&lt;p&gt;那既然是圣杯，为什么迟迟没有落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为理想往往伴随着代价。为了最高的性能，成本是绕不开的代价。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;同态加密成本有多高？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要探讨同态加密的成本有多高，就要从这项技术的背景聊起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同态加密的起点，并不来自互联网或云计算，而来自密码学内部一个极其朴素的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在传统加密体系中，加密和计算是两个泾渭分明的阶段：数据必须先被解密，才能参与任何有意义的计算。这在早期的计算环境中并不是问题——数据和算力通常掌握在同一主体手中，信任是默认前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 1970 年代，随着密码学逐渐从军事和政府领域走向学术研究，一个更基础的问题被提了出来：有没有可能在不解密的情况下，对加密数据进行计算？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1978 年，RSA 算法被提出。几乎在同一时期，研究者注意到 RSA 天然具备一种“结构保持”的特性：对两个明文相乘后再加密，与分别加密后再相乘，在某些条件下是等价的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，加密函数与乘法运算之间存在某种可交换性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然当时没人将其系统化，但这正是“同态性”的雏形。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在随后的二十多年里，同态加密并未成为主流研究方向。原因很简单：当时的计算模型、应用需求和硬件条件，都不足以支撑它的现实意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在学术上，一类被称为“部分同态加密”（Partially Homomorphic Encryption, PHE）的方案逐渐被系统化。这类方案通常只能支持某一种运算：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RSA、ElGamal：支持乘法同态Paillier（1999）：支持加法同态&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类算法在特定场景下非常有用，例如安全投票、隐私求和等。但它们有一个根本限制：无法同时支持加法和乘法，也就无法表达通用计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从计算理论的角度看，加法和乘法的组合，才构成了图灵完备计算的基础。缺少任意一种，同态加密就只能停留在“专用工具”的层面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在很长一段时间里，同态加密被视为一种“有趣但受限”的密码学技巧，而不是通用计算范式。&lt;/p&gt;&lt;p&gt;真正的转折点出现在 2009 年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当年，斯坦福大学的研究生 Craig Gentry 提出了第一个全同态加密（Fully Homomorphic Encryption, FHE）方案。这是密码学史上的里程碑事件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gentry 的核心贡献并不只是“同时支持加法和乘法”，而是提出了一套完整的理论框架，解决了一个此前被认为几乎不可逾越的问题：噪声增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在同态计算中，每一次运算都会引入噪声。如果噪声无限累积，最终会导致解密失败。Gentry 提出了“引导（bootstrapping）”这一关键思想：用加密数据本身，去同态地执行一次解密电路，从而刷新噪声。&lt;/p&gt;&lt;p&gt;这在概念上极其优雅，也极其昂贵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gentry 的方案证明了全同态加密“在理论上是可能的”，但在实践中，它慢到几乎无法运行。一次简单的运算，可能需要数小时甚至数天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但对学术界而言，这已经足够。也正是从那时起，全同态加密迅速成为密码学领域的研究热点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BFV、BGV、CKKS 等一系列同态加密方案在这一时期被提出，它们在效率和功能上不断逼近“可用”的边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便如此，全同态加密依然难以进入工程实践。原因并不复杂：它的性能模型，与现代计算体系格格不入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同态运算高度依赖大整数、多项式、模运算，而现代 CPU/GPU 的设计目标，是缓存友好、分支预测、向量化。这种错配，使得即便算法层面有数量级改进，系统层面的瓶颈依然存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在很长一段时间里，同态加密的“主战场”依然停留在论文和原型系统中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而最近两年，随着云计算和大模型的普及，数据与算力开始分离。企业越来越多地需要在“不完全信任”的环境中处理核心数据。与此同时，全球范围内的数据保护法规持续收紧，对数据可见性提出了更高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种背景下，传统隐私计算技术的假设开始显得脆弱，而全同态加密那种“全流程密态”的特性，重新展现出不可替代的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个意义上说，同态加密并不是一项“新技术”，而是一项被现实推到台前的老问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这个老问题的核心难点却没有变，还是成本和性能之间的平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现实很残酷：如果一项技术足够安全，但速度慢到无法使用或成本及其高昂，它依然没有价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是蚂蚁集团决定投入长期资源去做这件事的背景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;蚂蚁为什么必须做这件事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说同态加密是一项“被逼出来的技术”，那么金融与医疗等领域，正是压力最大的那一端。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从业务属性来看，蚂蚁面对的是一组极端约束条件：金融数据一旦泄露，影响的不只是单一用户，而是整个信任体系；医疗数据天然涉及隐私，同时又需要在诊疗、科研、保险等多个主体之间流动；跨境业务则进一步叠加了不同司法辖区的数据合规要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这些场景中，任何一个被允许明文存在的环节，都会成为潜在的攻击入口。随着业务规模扩大，这种风险并不会被摊薄，反而会被放大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么，蚂蚁最终将目光投向了同态加密这种简洁且彻底的方案。它并不是最经济的选择，却是在某些场景下唯一能够覆盖全流程密态的技术路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;转折点：从硬件侧分析问题，从软件侧解决问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然是必然要走的技术路径，那面对高昂的成本，有没有更好的解决方案呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁技术研究院计算系统实验室副主任、先进加速技术团队负责人张明喆表示，“其实是有的。在同态加密的性能优化问题上，业界曾经形成过一个相对明确的判断：如果要实现数量级提升，就必须依赖专用硬件。毕竟，通用处理器并非为这类计算模式设计，硬件定制似乎是必然选择。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是在同态加密加速领域，早期几乎形成了共识：要想快，必须做专用硬件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;许多研究团队选择设计定制加速器，试图用电路层面的并行性来弥补算法的开销。但这条路意味着极高的研发成本、漫长的周期，以及难以规模化的部署。更重要的是，它会将同态加密锁定在“小众、高门槛”的技术轨道上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁技术研究院选择尝试一条不同的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁团队重新审视了问题本身，发现同态加密在 GPU 上“跑不动”，并非因为 GPU 算力不足，而是算法结构与硬件并行模型之间存在错位。换句话说，问题不完全在硬件，而在于软件如何组织计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过重构算法的数据布局和并行方式，团队逐步让同态计算“长得更像 GPU 擅长处理的任务”。这种方法并没有改变同态加密的数学本质，却在工程层面释放了巨大的性能潜力。最终，实现了三千倍量级的加速效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，这种加速并不依赖定制硬件，而是可以随着 GPU 的代际演进持续受益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，蚂蚁到底如何通过软件方案解决成本问题？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对 KLSS 算法可用性的研究是一个例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在同态加密中，密钥交换占据了 80%～90% 的计算时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KLSS 算法是密码学界在 2023 年提出的一项重要理论突破，它通过切片并行，大幅缩短了密钥交换时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但理论突破并不自动转化为实际可用性。KLSS 在 GPU 和硬件加速器中暴露出了新的问题：并行带来的带宽需求急剧膨胀，反而成为系统瓶颈。这也是为什么，它在提出后并未被真正应用到工业级系统中。&lt;/p&gt;&lt;p&gt;蚂蚁团队的工作，正是试图跨越这道鸿沟。他们没有简单地“实现算法”，而是从体系结构角度重新审视 KLSS 的计算模式，对并行粒度、数据访问路径和内存布局进行系统性重构，进而根据硬件特性对 KLSS 算法进行针对性优化。最终，使这一算法第一次在加速平台上具备了实用价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，从论文数量看，刚刚过去的 2025 年，蚂蚁已经在计算机体系结构领域的国际顶级会议发表了六篇同态加密加速技术相关的论文，在同期该领域 17 篇论文里面，占到了约三分之一的比例。这是否意味着蚂蚁在同态加密领域已经走得很靠前了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，蚂蚁密算科技CTO、蚂蚁技术研究院计算系统实验室主任闫守孟表示，从论文数量上来看是走在行业前端的，但从看整个行业看，“谈领先还为时尚早”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实恰恰相反——这是一个参与者还相对较少的领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是美国、韩国还是中国，真正长期投入同态加密系统研究的团队都还不够多。生态不成熟、门槛高、基础设施缺失，都是制约因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而技术的终点，不应止步于实验室的论文，而在于赋能千行百业。这也是为什么，蚂蚁在做前沿研究的同时，也在围绕同态加密系统性地推动校企合作和开源基础软件。其推动同态加密的路径图，清晰地勾勒出了一项前沿技术从理论走向产业的必经之路：构建一个让后来者“能学、能用、能评估”的坚实底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两年里，蚂蚁团队每年都会面向国内学术界提出 10 个同态加密加速的开放问题，围绕这些问题资助高校开展研究，并提供必要的技术支持。同时，团队也在加紧打通自身研究成果的“最后一公里”，计划以开源编译器、benchmark、加速库等基础软件套件的形式向业界开放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个技术的春天，往往不是来自某一瞬间的“奇迹”般的技术突破，而是源于无数次的深耕与播种。只有愿意俯身，专注于解决一个个细小但关键的难题，并通过开源、合作等方式反哺整个生态时，同态加密这片土地才会逐渐变得肥沃，生态也终将走向繁荣，迎来属于自己的春天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这或许正是中国技术生态走向成熟的一种范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采访嘉宾：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;闫守孟，蚂蚁密算科技CTO、蚂蚁技术研究院计算系统实验室主任&lt;/p&gt;&lt;p&gt;张明喆，蚂蚁技术研究院计算系统实验室副主任、先进加速技术团队负责人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sjlgGt1p4xvaWWgGgRrZ</link><guid isPermaLink="false">https://www.infoq.cn/article/sjlgGt1p4xvaWWgGgRrZ</guid><pubDate>Wed, 28 Jan 2026 06:50:34 GMT</pubDate><author>李冬梅</author><category>AI 工程化</category></item><item><title>AI 驱动的智能异常处置：从异常发现到根因定位</title><description>&lt;p&gt;异常处置包含异常发现、问题定界和根因定位等环节，高效的异常处置流程对于保障平台的稳定性起到至关重要的作用，然而平台本身的复杂度以及海量的多元异构数据给异常处置带来了巨大的挑战。如今，大模型等 AI 技术的演进则为应对这些挑战提供了新的思路。在 2025 年 InfoQ 举办的 QCon 全球软件开发大会（ 北京站）上，来自阿里云的算法专家张颖莹发表了题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6306&quot;&gt;AI 驱动的智能异常处置：从异常发现到根因定位&lt;/a&gt;&quot;》的演讲。她从阿里云计算平台的运维场景出发，分享了从异常发现到问题定界和根因定位各环节的算法选型和设计思路，包括通用的时间序列异常检测、高效的日志聚类和精准的多 Agent 根因定位框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：将于 2026 年 4 月 16 - 18 召开的 QCon 北京站策划了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1921&quot;&gt;Agent Ops：运维新生产力&lt;/a&gt;&quot;」专题，将深入讨论 Agent 如何与现有技术栈深度集成，并演进为具备长期记忆与自我进化能力的运维实践。如果你也有相关方向案例想要分享，欢迎提交至 &lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;https://jinshuju.com/f/Cu32l5&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里云大数据运维背景&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们阿里云计算平台整合了整个阿里的大数据和 AI 的能力，并以产品化、平台化的方式支撑了我们集团内部与云上各行各业客户的众多非常核心的业务场景。这里列举其中几个比较典型的平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 MaxCompute 大数据计算服务主要负责大规模数据的离线计算。大家日常在网购后经常会在手机上去追踪自己的包裹，这些菜鸟的物流数据产出就依赖 MaxCompute。还有实时性要求相对较高的场景，比如自动驾驶场景，车机系统会对司机的危险驾驶行为发出实时警告。像这一类比较追求时效性的场景，往往就依赖 Flink 这样的实时计算引擎。下一个是 Hologres 实时数仓，大家在手机淘宝上检索商品关键词时，它会在底层进行实时的交互式分析，为大家呈现实时的检索推荐结果。另外随着大模型越来越火爆，大家对机器学习模型相关的训练、微调、在线服务的需求都有了爆发式的增长。这一类的模型训练、微调、在线服务都可以一站式在我们的机器学习平台 PAI 上完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以看出我们的这几个平台上层支撑的业务都非常重要，所以做好这些平台的运维也非常关键。但传统的运维模式很容易让运维人员变成背锅侠的角色。所以我们计算平台也专门设置了一支运维中台研发团队来负责所有大数据和 AI 产品的统一运维管控。我们也一直利用“AI+ 数据 + 工程”的手段来提升整体运维效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;稳定性作为运维的基石，其重要性是毋庸置疑的。但对于系统来说，异常的发生很难避免，怎样在异常发生时能进行快速高效的处置，对于整个平台的稳定性是非常重要的。另一方面，随着我们的用户对云服务厂商服务水平的要求越来越高，精细化运维已成为行业趋势。另外像我们前面提到的这些大数据平台，它们的底层都是超大规模的计算集群，这些集群无时不刻都在产生海量的数据，这些海量数据对我们的异常处置也带来了更多的战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我把整个异常处置层面我们面临的挑战总结成三个层面。首先是面对这样复杂的系统，我们怎样从这个系统运行的蛛丝马迹里全面发现各种异常，确保监控的覆盖率。第二个层面是面对这么多异常告警，我怎样从这些告警风暴里真正剥离出最关键的信息，从而减少误报对运维人员的干扰。第三个层面是当异常发生时，我怎样快速定位到问题的根本原因，并采取针对性的措施，对症下药来让异常快速恢复，减少对用户带来的损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;异常发现和告警降噪&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们会为大家分享我们是怎样逐一来攻克前面提到的三个挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是异常发现的层面，我们团队构建了非常丰富的异常检测相关的算法，力求实现异常的全面覆盖及精准发现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/44/441251a86a82f1acb9500a28199dcb8a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在这里梳理了 4 个典型场景，我们针对不同形态的数据和不同的场景都会选择它最适配的算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是单指标异常检测场景，比较典型的应用就是我们整个系统的可用性监控。比如这个平台整体的流量、性能、成功率，这些指标和用户自身的业务周期是非常紧密相关的，因此它经常会呈现出比较复杂的多重周期性。所以在这里我们自研了一套鲁棒的周期分解算法，来对这些曲线中的多重周期性进行精准的识别和分解，从而更好地做到异常的发现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是多指标异常检测场景。当我需要判断一台机器是不是存在异常时，可能单独去看其中的某一条指标是没有太大参考价值的，我需要综合去看这个机器所有维度的指标。在多指标异常检测这边，我们直接选用了开源的多指标异常检测模型。虽然它相比单指标异常检测可能会牺牲一定程度的可解释性及性能，但它可以更好地把握多指标之间的内部关联性，从而提升检测的精准度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三类是基于分布的异常检测。在大数据运维的场景里，我们经常会面临着这样的问题，就是当我的集群性能变慢时，我不单是要检测单个指标、单个作业是不是变慢了，而是希望去看整个集群或整个平台的作业运行的分布是不是有异常的变化。针对分布的异常检测，我们也自研了一套异常检测算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了指标之外，日志也是非常重要的一种可观测的数据。日志数据最大的挑战是它的体量非常庞大，所以在这里我们先选用了业界性能比较高的日志模板提取算法，然后我们会基于提取出来的日志模式去判断它是不是新增的异常，或者它的模式是不是有暴增的变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们重点给大家展开介绍一下前面提到的两类自研算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e3/e3a8bf1eaf1eebc1301f72b74a29fa96.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是针对单指标的场景，我梳理了我们运维场景里最关心的几类典型异常，包括均值变化、方差变化，也就是抖动频率的变化，还有尖峰深谷的异常、断崖式跌落的异常，还有趋势型的异常。大家可以看到梳理出来的异常可能看起来还比较明确，但实际上这些异常融合到真实的业务数据里时，非常容易受到数据本身的其他周期性的干扰，使得检测变得非常困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在这里我们构建了&amp;nbsp;周期分解算法，它的核心思想是采用了分而治之的策略。首先从一条时间序列里把不同频率的周期逐一剥离出来，然后再针对剥离出来的每一重周期去精准计算它具体的周期长度，从而更好地把握整个数据的周期性特点。做完周期分解后，我们会进一步利用不同类型的统计检验方法，来分别对应检测前面提到的这几种典型异常，从而实现用一套算法框架就能够覆盖前面提到的多种类型的异常，使得这一套单指标的异常检测算法能够在运维领域具备更强的普适性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9e/9ed94df48e40ac0449ee23f848bf36ae.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是基于分布的异常检测。大数据运维经常会面临的痛点，是当我需要做整个集群性能的异常检测时会面临两个挑战。首先是整个集群上运行的作业数量非常多，如果我对每个作业都去检测它有没有运行变慢，耗费的成本会非常高。而且即使我做到了对每个作业的检测，但实际上我并不会关心单一的某作业的波动，因为很多情况下可能用户购买的资源不足，他自己的资源组里面的作业之间也会进行资源的争抢，所以也会出现单作业变慢的情况。但我们真正需要关心的是整个集群作业性能异常、变慢的这种趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们把整个集群作业运行时长的分布图构建了出来，然后借鉴了优化领域非常经典的运输问题，结合基于重构的深度学习模型来进行异常检测。我们可以把整个集群的作业运行时长的分布图想象成土堆，然后当这个土堆向右边运输时，我们增加一定的惩罚项，从而更好地检测出整个作业运行时长分布图向右偏移，也就是整个集群性能变慢的这种场景。当然我们还在深度学习模型里选用特殊的门控机制，来更好地应对训练样本当中的噪音问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到这里我们已经通过多种类型的异常检测算法实现了异常的全面覆盖。随之而来的问题就是面对这么多的异常告警，运维人员怎么判断到底哪些异常才是需要第一时间响应的。所以我们需要对这些告警进行进一步的精细化分级。我们主要从两个方面来进行告警的精细化处理，分别是影响面拉取和问题定界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/7f/7fdde9e5de3e644b84b6fe380e920436.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;影响面拉取指的是当我们的主指标异常触发了异常工单后，我可以根据整个拓扑关系拉取到主指标所关联的子指标。然后我通过时间序列的下钻算法，可以量化出来每个子指标对主指标的贡献程度，以及它自己相对于历史的异常度。综合这两个维度，我可以计算出来这一次的异常所波及到的影响面到底有多广。一般来说波及面越广的异常，运维人员自然要去更加高优地响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是问题定界。在大数据的场景里有很多异常是因为用户自己的操作失误引起的，比如像一条 SQL 语句，如果它的语法就存在错误，自然会运行失败，但运行失败就会产生报错日志，甚至会影响到用户实例本身的成功率。但像这一类语法错误导致的异常，我们的运维人员并不需要第一时间介入处理。运维人员真正应该关心的是由平台问题导致的失败或者任务的异常，所以我们需要对用户作业的报错日志进行更加精细化的分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里我们首先用前面提到的日志聚类算法对异常时段的所有日志先进行聚类，聚类完成后我们会提取出其中典型的日志异常模式，然后和我们日志知识库当中的标签去匹配，这个标签就可以标识出日志到底是用户问题还是平台问题。日志知识库的标签从哪来？一方面可以由我们的运维专家去人工标注，另一方面我们现在也在用大模型做这方面的提效。我们会利用大模型事先生成预标签结果，然后让专家审核。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于影响面和问题定界的结果，我们就可以对告警分成不同的等级，包括需要立即响应的故障性异常，还有红灯、黄灯，还有可以稍微延迟处理的风险性异常。这种做法首先可以让我们不遗漏任何一种风险，同时又可以更好地分配运维专家的精力和关注度，确保他们能够更高效地处置那些真正紧急的异常。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;多 Agent 根因定位框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到这里我们已经解决了异常发现环节的问题。但实际上在异常处置流程里，最耗时也往往最困难的点在根因定位这个环节。因为这个环节涉及到的数据还有工具都非常繁杂，而且即使存在一套非常标准的运维排障流程，但真正具体到每一次故障时，依然需要结合当时的场景和数据的具体问题做具体分析。所以根因定位往往也只有那些经验非常丰富的运维专家才能够做到真正高效的处置。也正是因为根因定位存在着这样的难点，它近年来也一直是学术界、工业界都非常关注的热门话题。我们团队也一直在根因定位这个方向上不断升级策略和算法，现在也引入了多 Agent 的框架来解决这样的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在具体介绍我们的策略之前，我们可以先简单回顾一下 Agent 的核心要素。这几个要素对构建高效的智能体来说非常关键，它也是我们后续设计我们整个多 Agent 根因定位的核心思路。首先是角色的设定部分，我们通常会在大模型的 prompt 里交代它的角色定位，包括它的业务背景，使得它能够在领域上具备更好的专业度，同时也能够更加明确它自己的任务产出到底是什么样的形式。第二类是长短期记忆，通常我们会通过 RAG 的方式引入外部私域知识，进一步提升大模型在私域的专业性，更好地让它了解上下文。第三类是好的工具模块，让大模型具备更强大的主观能动性，拓展它的能力边界。最后是自主编排，对大模型来说非常关键，因为它直接决定了大模型能不能很好地做到任务的拆解，以及具体执行步骤的编排，它很大程度上决定了大模型能不能够解决根因定位的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们就分别介绍我们是怎样基于这几个核心要素来构建多 Agent 根因定位框架。首先是角色设定的部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以回忆一下，当我们日常出现线上问题时，运维团队是怎样工作的？通常他们会成立故障应急小组，在这个小组中会有各个模块的负责人，他们会分别排查各自的模块目前的信息，并且判断自己到底是根因方还是受影响方。然后他们也会和自己的上下游模块做沟通，最终他们的结论会汇总到故障应急负责人这边，他会去对全局的信息做整体汇总，并给出最终结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们希望基于大模型构建出来的诊断系统也能够具备故障应急团队这样的效果。在这个团队里面，人的分工是非常关键的，每个角色都应该具备自己的专业度和特长，所以单一的 Agent 通常不能满足这样复杂任务的需求，所以我们引入了多 Agent 的框架。我们是按照系统的模块来设定每个 Agent 的角色，比如会有专门的存储 Agent、调度 Agent、网络 Agent 等。在 prompt 里，我们会内置模块相关的背景信息，使得它们可以对照现实世界里每个模块的 owner 这样的角色。除了模块 Agent 外，在上层我们还会有系统 Agent 的角色，就相当于是故障应急负责人。它可以收集每个模块 Agent 的结论，并且给出最终的判断。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/db/dbba653563f24c8aed98c85c61391a42.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成了 Agent 的角色设定后，接下来很重要的就是要丰富每个 Agent 的装备库。我们构建了 4 大类工具，首先是算法服务类的工具，包括前面提到的时间序列异常检测、日志异常检测能力，还有因果推断能力。这些服务都会构建成在线服务的形式，可以非常方便地对接其他系统，或者作为 API 来让 Agent 调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是 RAG 工具，它在私域的智能问答领域里是非常核心的技术，在根因定位环节里同样发挥着非常关键的作用。比如当我们需要对照历史的相似故障来参考它的排障经验时，或者当我遇到一些指标和日志，但可能不太清楚它的具体含义是什么时，都需要参考对应的文档，把对应的知识检索回来，从而给大模型提供更丰富的背景知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三类是运维分析类工具。我们的运维人员构建了很多集成了他们专家经验的分析诊断流，比如针对单个作业的诊断，还有针对整个单机的诊断，还有网络层的诊断等。这一类诊断工具理论上都可以由大模型来自主编排完成，但实际上因为这些工具之前就已经沉淀好了，而且我们利用编排好的诊断流可以直接得到非常明确的结果，所以在很大程度上可以减少大模型的 token 消耗，来提升整体根因定位的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四类是外部插件。现在很多大模型应用平台都搭载了非常丰富的生态系统，有着插件市场这样的概念，在里面很多第三方的开发者都会贡献他们自己研发的分析工具，比较典型的包括在线检索类工具、代码编写类工具，还有 chatBI 类的工具。现在这些工具都可以直接拿来为我们自己所用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这些工具集的构建，我们就让 Agent 同时具备了专业的运维分析能力、专业的算法分析能力，甚至还具备一定的通用基础开发能力，这样它就能有更好的武器应对更加复杂的根因定位场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三部分是关于编排可靠性的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/5f/5fa293d575992271e3b786eb6c306cbd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于编排，我们会面临这样的挑战，就是一方面我们希望能够尽可能释放大模型自主编排的灵活性，这样它在以前没有遇到过的故障场景也能发挥特定的效应，而不是只能针对历史重复出现的故障才知道该怎么做。但另一方面大模型编排结果是否可靠，可能直接决定了这个故障是不是能够及时恢复，在这个方面我们的容错性是非常低的。所以这里的最大难点就是怎样在释放大模型编排灵活性的同时，又能进一步提升编排结果的可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这里我们采取的策略是固定工作流编排和自主编排相结合的混合策略。一方面我们会把运行性能相对较高，并且对根因诊断非常关键的工具，直接编排到前置的工作流里。这些工具直接执行完后，我会把它的结果输入到大模型里，再让大模型自主决策是不是还需要调用额外的工具来做进一步排查，才能够得到最终的根因推断结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后在大模型自主编排这一部分，我们也采取了很多策略来提升它结果的可靠性。任务分解部分我们主要采用的是 react 框架，也是现在比较主流的框架。大家在实际应用里也可以直接把它作为 baseline 来作为后续提升的参照。另一部分是记忆增强，我们通过检索外部的 SOP 来让大模型进一步校准它生成的 SOP 的可信度。第三部分是加入了反思机制，我们会让大模型在整个决策过程动态反思中间过程可能会有哪些改动来保证灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了任务分解、记忆增强和反思机制之外，还有一些策略可以进一步提升它的编排可靠性，包括多计划选择，还有引入外部规划器来辅助它生成这样的策略。我们也计划在后面再对这些策略做更详细的尝试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后一个问题涉及多 Agent 框架的协同。我们前面聊的都是怎样让 Agent 自己变得更好，接下来的问题是我有这么多个 Agent，怎么能够让它们更好地协同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/77/77f0986bedb8ddb9c6747d6c60f4c94a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在有非常多的 multi Agent 编排框架，他们在系统里都会内置编排好的多 Agent 协同工作流。但这些默认的工作流，在我们的场景里或多或少都会存在一定的弊端。比如像顺序执行的工作流，上游节点在做决策时是不知道下游节点信息的，所以会存在着一定的信息不对称，会导致它得到片面的结论，而它的结论可能又会进一步影响到下游节点的决策，会形成一定的误差累积效应。第二类是层次结构，虽然看起来有顶层节点来对大家的信息做汇总，但实际上下游节点之间依然是不存在任何信息交换的，同样会导致它们自己得到比较片面的结论。第三种圆桌讨论的模式，看起来大家的信息可以在桌子上进行非常充分的交换，但它最大的弊端在于缺少明确的领导者，所以大家的讨论可能会非常发散，聊着聊着可能就偏离了主题，很难在限定时间内得到非常明确的结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑到这些固定编排模式的弊端，我们自研了一套基于神经网络反馈机制的工作流。它的核心思想也非常简单，我首先会根据模块之间的拓扑结构设定单向传导的工作流，我们称之为前向反馈。在前向反馈的基础上，我额外增加了后向反馈的机制，实际上就是让前置 Agent 有机会修改自己之前可能得出的错误结论，然后最终大家的结论依然会汇总到系统 Agent 这边来。这样的好处是一方面我可以在一定程度上弥补信息不对称的问题，同时也能把整体的推理次数非常严格地限制在预设的范围内，减少盲目发散的讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;通用异常处置平台&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面介绍的更多都是算法层面的设计，而好的算法最终还是需要真正集成到我们的平台上，才能真正融合到运维人员的工作流里，发挥出真正的效用。所以工程平台的建设也非常关键。在这里我给大家分享一下我们怎样来构建通用的异常处置平台。现在我们各个产品的运维异常处置流程都可以在这个异常处置平台上来进行，很大程度上提升了我们计算平台整体异常处置的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/69/6956e5683d07327bd4635d294971d9af.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体架构最底层是数据层，我们为运维场景里这些经典的数据模式都安排了最适合它们的存储方式，包括指标、日志、拓扑、文档、事件等，使得它可以在后续的分析环节里做到非常高效的数据抽取、根因分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据层之上是我们非常核心的算法服务层及大模型服务层。算法服务层里搭载的是前面提到的时序、日志、根因定位、因果分析，还有检索这类非常基础通用的算法，这些算法会部署在 PAI-EAS 上变成在线服务，可以供其他系统直接调用，也可以作为工具集成到 Agent 里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型相关的这一部分，除了前面提到的 prompt 工程，还有工具的调用，还有工作流编排。对于完整的大模型服务而言，如果你不是 demo，如果你想要真正上生产的话，还需要考虑很多因素，比如像可观测能力，还有资源的管理隔离能力等。所以想要搭建好大模型应用服务，还是需要搭配非常好的大模型应用构建的平台。幸运的是现在也有非常多的这样的平台，包括商业化的、开源的，都具备了非常强大的能力。但对我们来说，我们还是需要根据我们自己的业务场景去选择最好的、最适合自己的平台来进行构建，才能提升效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来也想和大家进一步分享一下我们在选择这样的大模型应用开发平台时会从哪几个角度来考虑。我们主要会从三个层面，第一个是应用构建本身的便捷度和产品应用性，第二个是 LLMOps 能力的完备度，最后是平台本身的开放度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在应用构建方面，我们会重点考虑我在这个平台上是不是可以非常便捷地完成非常复杂的业务工作流的编排，最好就是拖拉拽的方式就可以完成复杂的编排任务。其次是这个平台上面是不是同时具备了微调能力，这样我就不需要在各个平台上频繁切换，能够在平台上一站式完成整个模型的微调和最终应用的部署搭建。第三个是像 RAG 的经典组件，我在这个平台上是不是能够直接复用，减少额外的开发工作量。最后是我在这个平台上面搭建的服务，是不是能够非常便捷地和最终的交互出口承接，不需要额外再构建中间的一层服务来进行引流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个大的层面是 LLMOps 的能力，它直接决定了整个大模型应用服务的稳定性，以及后续性能优化的空间。所以我会重点关注平台是不是具备一定的模型加速能力，资源管理的能力也非常重要，就是在突发流量打进来的时候，你是不是能非常方便地帮我做资源扩容。还有我不同的大数据产品之间肯定是要做隔离的，你是不是具备完备的资源管理隔离的体系。可观测性也是非常关键的，当我大模型推理失败的时候，是不是能够非常便捷清晰地看到到底是哪个环节、哪个工具调用出现了问题，方便我进行问题的快速恢复和改进。最后是模型测评的能力，因为现在基础模型发展非常迅速，所以我希望能够在平台上非常便捷地做模型效果的测评，来方便我选择最适合这个场景的基模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个层面是开放性，开放性直接决定了我在你这个平台上是不是能够更好地利用别人开发的能力，以及我是不是能够和开源的生态做更好的兼容。这里首先要考虑你的插件市场是不是足够丰富。第二个方面，像现在比较火的 MCP 协议，你是不是能够天然支持？还有同外部系统以及开源框架的对接，我现在迁移到你的平台，是不是能够更好地做到无缝的迁移，我后续是不是还能够持续用到开源生态的创新性成果，这些都非常关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些考虑因素，我们现在选用的是阿里云的百炼来搭载大模型应用，当然大家也可以结合自己对这几个因素的优先级的排序，选择更适合自己的平台。我们选择百炼，一方面是它在我前面提到的几个维度上相对来说是比较完整的，同时它在应用类型上也非常丰富，既包括我前面提到的固定工作流式的编排，也提供了以 RAG 为核心的智能体应用，同时它还提供了智能体编排应用，可以把前面提到的多种不同类型的应用全部整合进来，做到混合编排。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外它整个任务编排的产品界面是相对来说比较友好的，我在上面可以非常便捷地拖拉拽来完成复杂工作流的编排。最后在整个百炼的项目空间里，我可以观测到整个服务的调用情况，每一次调用都可以点开详情看每个工具的输入输出到底是什么，是不是符合预期，方便我进行问题的排查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面我们已经完成了大模型应用搭建的部分，接下来我们具体聊一下整个异常处置平台到底包含了哪些核心的模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/cd/cdca3580c2afb5c4d458a7410b435d20.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先这个平台的入口也就是告警源，除了前面提到的算法检测结果外，在我们实际的业务里它还会包含 SRE 自己在监控系统里设置的监控告警，当然还有用户来提的工单或者人工补录的情况。每一种告警来源，我们都会给它生成异常工单，这个工单里会包含 4 个非常核心的模块，首先是异常现场，然后是定界定级的结果，还有根因定位的结果以及快速恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;异常现场主要呈现出这一次开工单的原因到底是什么，触发的指标和日志到底是什么样的，来方便运维人员在接手工单时快速了解问题的背景。然后是定界定级的结果，会具体呈现出这一次异常的影响面，以及算法得到的分级过程。根因定位会展现出多 Agent 框架定位的结果，我们现在会得到根因模块的结论，同时大模型也会提供出得到这个结论的推理依据。同时对于每个模块 Agent，我都可以点开详情查看它的工具调用情况。最后快速恢复的部分，我们现在还在相对比较初步的阶段。目前主要是做的是检索历史的相似工单，这样运维人员可以在新的工单里直接点击跳转到历史工单里查看当时的处理策略，从而对这一次的异常处置提供一定的参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以整体来看一下整个异常处置平台在我们线上应用的真实效果。首先当异常发生时，运维人员会在钉钉上收到卡片的通知，根据告警等级的不同，卡片也会呈现出不同的颜色，直观看出异常的严重程度。如果异常没有被及时处置，它的影响面可能会不断扩大，它可能会从黄灯变成红灯，这个卡片的颜色也会随之动态变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外当工单被运维人员接手进行处置时，我们可以在工单上实时看到它的处置进度，方便整个群里的人都了解整个异常的处置情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运维人员收到这个卡片后，他可以点击对应的链接跳转到异常工单的页面上，可以看到异常的现场，包括具体的曲线以及曲线到底是从哪个时刻开始有这个异常点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后在异常影响面的分析部分，我们可以看到这次的异常到底影响了哪些客户，我们会在这里列出具体的客户信息。同时我们也能看到这个客户这次受影响的实例在我们这次异常里的占比。在最下方，我们会呈现多 Agent 的根因定位结论。首先会得到明确的定界和定位结果，以及这个结果的核心依据。下面我们可以看到每个模块 Agent 的独立结果，点击详情就可以进一步看这个模块到底调用了哪些工具，以及这些指标日志的检测的情。实际上我们经常会出现多个模块 Agent 都觉得自己出问题了，都可能觉得自己是根因这样的情况，但我们最终的系统 Agent 还是会根据各个模块之间的潜在拓扑关系得到更加明确的结论，最终它给出的结论只会是最终根因的那个模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;总结和展望&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这次分享，我们首先从大数据运维的业务背景出发，来给大家介绍了我们在异常处置环节到底都面临着哪些挑战，包括我们怎样全面检测出这些异常，以及怎么面对告警风暴，真正剥离出其中关键的信息，帮助运维人员更好地分配注意力，以及最后我们在异常发生时怎么快速定位到问题的根因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后我们具体介绍了我们怎样利用 AI 技术来逐一攻破这些挑战，包括建设多种类型的异常检测算法，以及通过影响面的分析还有问题的定界来帮助我做更加精细化的告警分级。最后我们还引入了多 Agent 的根因定位框架，来模拟现实当中的故障处置小组实现根因模块的定位，并且给出它的推理依据，让我们的大模型推理不再是黑盒。前面提到这些算法技术都是通过 PAI-EAS 部署成在线服务的方式来供其他的系统和大模型应用层来进行进一步的调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而我们大模型服务层本身则是依靠百炼这个平台来进行构建和部署的，最终这些算法服务层和大模型服务层共同支撑了最上层的异常处置平台，真正把 AI 能力集成到平台和产品里，整合到运维人员日常的工作流里，发挥出真正的提效作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后我们来对下一步的规划做展望。首先我们会进行异常处置能力整体的补全，会从现在事中的异常发现，一步向前延展，做风险预防。我们整体的思路是希望纳入更多海量数据来做故障的提前预警，这方面带来的技术挑战会更大，可能会涉及告警本身时空相关性的挖掘等技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在根因定位之后，我们还会打造真正的预案推荐，因为只有真正推荐出了可能的预案，才有可能走到最终的自愈环节，做到处置的自动化闭环。预案推荐在某种程度上也依赖根因定位的精细度。目前我们的根因定位也只能做到一级的模块，后面我们会进一步做到二级模块，来让整个根因定位更加明确具体，让它最终的关联动作可以更加明确。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了异常处置能力的补全外，我们还会进行 Agent 的能力增强，包括自主编排、可靠性的提升，我们还有很多的策略需要尝试，来进一步保证它的结果是靠谱，并且性能是足够优的。还有工具能力的拓展，我们现在主要是把现有的运维平台上面的工具还有作业去兼容 MCP 这样的协议，使得 Agent 具备更强的系统兼容性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后是交互体验的优化以及人工反馈的增强。要让大模型能够得到令人满意的效果，人的实时反馈是非常重要的，包括现在很多像 Manus 这样的组件，都会在生成 plan 之后允许用户有机会做调整，这对最终结果的准确性非常关键。所以我们整体的交互模式的变化，以及后续怎样利用人工的反馈来持续优化后面的迭代，让大模型真正做到越用越聪明，是非常关键的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来说，我觉得大模型技术和 AI 技术的发展可以用日新月异来形容，它也给我们智能运维领域带来了很多技术上面的突破，我也非常期待我们能够有更多的成果来做进一步的分享，感谢大家。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;阿里云计算平台正急招智能运维算法专家，&lt;a href=&quot;https://careers.aliyun.com/off-campus/position-detail?lang=zh&amp;amp;positionId=2009183001&amp;amp;trace=qrcode_share&quot;&gt;岗位链接&lt;/a&gt;&quot;，也可直接投递简历至：congrong.zyy@alibaba-inc.com，欢迎加入我们。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖莹，阿里云算法专家，阿里云计算平台智能运维算法团队负责人，在智能运维领域深耕 8 年。用产品和服务支撑计算平台 MaxCompute、Flink、Dataworks、PAI 等多个大数据 &amp;amp;AI 产品的智能化运维。多项研究成果被 ICLR，KDD，VLDB, SIGMOD, ICDE，WWW, CIKM，ICASSP 等国际顶会接收，并带领团队获得了 ICASSP 国际智能运维算法大赛冠军。曾受邀在 QCon，ArchSummit，DataFunCon，FlinkForward 等大会发表演讲，同时参与了阿里巴巴开源大数据运维平台 SREWorks 开发和信通院《智能运维能力成熟度模型》行业标准编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;活动推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026，AI 正在以更工程化的方式深度融入软件生产，Agentic AI 的探索也将从局部试点迈向体系化工程建设！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot; 已正式启动，本届大会以“Agentic AI 时代的软件工程重塑”为核心主线，推动技术探索从「AI For What」真正落地到可持续的「Value From AI」。从前沿技术雷达、架构设计与数据底座、效能与成本、产品与交互、可信落地、研发组织进化六大维度，系统性展开深度探索。QCon 北京 2026，邀你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8a/8a69f038dc22619d47309fca02519740.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Mv7SsDMyFBDz3coYwTUx</link><guid isPermaLink="false">https://www.infoq.cn/article/Mv7SsDMyFBDz3coYwTUx</guid><pubDate>Wed, 28 Jan 2026 06:03:41 GMT</pubDate><author>作者：张颖莹</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>顶级工程师Karpathy的“AI革命”：一个月前还在焦虑“落后”，如今“20年古法编程习惯”被彻底颠覆，80%代码已交给AI！</title><description>&lt;p&gt;今天，Andrej Karpathy 又发了一条很长的推文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他分享了使用Claude进行数周高强度编程后的心得体会，并且表示自己过去 20 年形成的编程工作方式，在短短几周内发生了明显变化：从 11 月还以手写和自动补全为主，到 12 月迅速切换成大约 80% 交给 agent、自己做 20% 的修改润色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，他提到 Claude 和 Codex 在 2025 年 12 月左右跨过了某种“一致性/连贯性门槛”，让这种以 agent 为主的写法突然变得可行，并且很难再回到完全手写的状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026 年将是充满活力的一年，因为整个行业都在消化吸收这项新技术。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/dba5c24132e79347c937deb1fd570cb5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一个月前，顶级工程师说“我落后了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而就在一个月前，这位提出“vibe coding”一词的人，还在 X 上写过另一段让人印象深刻的话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我从没像现在这样，作为一名程序员感到如此落后。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/10fa96922df70de2657c745ec1da795a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在那条 X 动态中，Karpathy 写道，这个职业正在被“剧烈地重构”，个人程序员贡献的代码行数正在变得越来越少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我有一种强烈的感觉：如果我能把过去大约一年里已经出现的这些工具真正串联、用好，我的能力可能会提升 10 倍，”他写道，“没能把这种增益拿到手，感觉明显就是技能问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“现在需要掌握的是一层全新的、可编程的抽象层（叠加在以往那些熟悉的抽象层之上）：涉及 agent、子 agent，它们的提示词、上下文、记忆、运行模式、权限、工具、插件、技能、钩子、MCP、LSP、斜杠命令、工作流、IDE 集成等。同时，还必须在脑中建立一个覆盖全局的心智模型，用来理解这些本质上随机、会出错、难以解释、而且不断变化的实体的优势与陷阱——而它们如今被突然掺进了原本那套‘老派而扎实’的软件工程体系之中。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一切更像是“一个强大的外星工具被直接发下来，却没有配套说明书”。“每个人都得自己摸索该怎么握住它、怎么操作它，而与此同时，一场 9 级地震正在撼动整个职业，”他写道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人说：“如果连他都觉得自己作为程序员已经大幅落后，那就很能说明我们现在处在什么阶段。”因为说这话的人是 Karpathy——长期被视为“走在最前面”的那类人：2015 年加入 OpenAI 成为创始成员之一，之后又很早投身自动驾驶，担任特斯拉 Autopilot 的 AI 负责人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72179b23ba3a11e35821afb2ed3caa80.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在评论区里，另一位重量级人物也表达了强烈共鸣。Claude Code 的核心作者、Anthropic 工程师 Boris Cherny 坦言，自己“几乎每周”都会有类似的感受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他提到，有时会下意识按老办法去做，做着做着才突然反应过来：“等等，Claude 可能可以直接搞定这个。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近一次是在排查 Claude Code 的一个内存泄漏。他一开始走的是传统路径：连上 profiler、跑应用、暂停采样、再手动翻 heap 分配记录，一步步排查。但与此同时，他的一位同事处理同一个问题时，直接让 Claude 生成 heap dump，再让模型去读 dump，找出那些“本不该还被保留着”的对象。Claude 一次就命中问题点，顺手提了个 PR，把问题修掉了。“这种事几乎每周都会发生。”他写道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny 还补充了一个很有意思的观察：某种意义上，那些新入职的同事，甚至刚毕业的新人，反而更容易把模型用到位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为他们不会被“模型做不到什么”的旧印象束缚——那些印象大多是早期模型时代形成的“历史记忆”。而对已经形成使用习惯的工程师来说，每隔一两个月，就得花不小的心理力气去重新校准：模型现在究竟能做到什么——而且这个边界还在持续外扩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为软件工程正在发生根本性变化，而即便是他们这些最早的实践者，最难的部分依然是不断调整自己的预期——而这还只是开始。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Karpathy 则在评论里加了一个比喻：就像你拿着“激光枪”到处指，有时只打出一堆小弹丸，有时甚至会哑火；但偶尔，当你握对了姿势，一束强力激光会突然喷涌而出，直接把你的问题“熔掉”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工具用顺手了后：“这是 20 年最大变化”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了今天，Karpathy 状态已经明显不一样：不再是“我跟不上”了，而是“我已经换了一种编程方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他用一种几乎夸张的方式描述了这种变化：过去 20 年形成的编程习惯，在短短几周内被打断；11 月还主要靠手写和自动补全，到了 12 月，已经变成大约 80% 的代码交给 agent，自己只做 20% 的修改和收尾。与此同时，他也给出了一个时间点上的判断：在他看来，Claude 和 Codex 大约是在 2025 年 12 月左右跨过了某种“一致性/连贯性门槛”，让 agent 编程从“偶尔好用”变成了“可以稳定纳入日常工作流”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条推文的评论区也一贯的热闹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很快就有人表示，这样的转变并不只是 Karpathy 一个人的感受。一位工程负责人在回复中写道，这和他的体验完全一致：真正让人意外的并不是速度提升，而是写代码这件事反而变得更有趣了。那些重复、机械的脏活累活被拿掉之后，剩下的更多是创造性的、值得投入精力的问题；而那些真正拥抱 AI 辅助开发的工程师，不只是变得更快，还开始尝试以前根本不会去尝试的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他引用 Karpathy 的一句话总结这种变化：“不要告诉它怎么做，给它成功标准，然后看它自己跑。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/77/77ae42db41d94ad24f187720326d67f1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有不少人盯住的是这组 80/20 的数字变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来这个比例只会继续上升，直到有一天我们几乎不再‘写’代码，而只是负责阅读和审查它。”还有人认为以后的瓶颈不再是打字速度，而是我们审查速度有多快，尤其是去识别那些“agent 幻觉出来却被推进生产分支”的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也势必会积累起“理解债”：因为审查 AI 写出来的代码太费劲，人会越来越倾向于“能跑就先过”，时间久了反而会对自己的代码库理解得越来越少。Karpathy 在评论中表示，他很喜欢“理解债务”这个词，虽然之前没见过，但觉得非常贴切；而且他也承认，这种诱惑确实存在——当 LLM 一次就把问题解决、而且看起来运行得还不错时，人真的很容易就想直接往下走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有人把这种变化说成一种“角色对调”：我们花了很多年学会写代码，现在更像是在当一个永不睡觉的实习生的项目经理——分派任务、验收结果、兜底风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，工具在变强，角色在重排，瓶颈也在迁移：从“写得快”，变成“看得懂、审得住”。而这一轮变化，显然还没到终点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/275966dd3a191de21743dfe518558677.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是他今天发布在 X 上的完整长文（按字面翻译，略作通顺处理）：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去几周我大量用 Claude 写代码，随手记几条零散想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;编程工作流&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着最近一轮 LLM 编码能力的明显提升，和很多人一样，我的工作方式在很短时间内发生了变化：11 月大概还是 80% 手写+自动补全 / 20% agent；到 12 月就变成 80% agent 编码 / 20% 人工改改、收尾润色。也就是说，我现在基本是在用英语“编程”——有点不好意思地用自然语言告诉 LLM 该写什么代码。自尊心多少会疼一下，但能用大粒度的“代码动作”去操控软件这件事，净收益实在太大了，尤其是当你适应它、把它配置好、学会怎么用，并真正想清楚它能做什么、不能做什么之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是我近二十年编程生涯里，对基础工作流影响最大的一次变化，而且它是在短短几周内发生的。我猜现在已经有两位数百分比的工程师也在经历类似的转变；但在更广泛的人群中，对这件事的认知可能仍只有个位数低位百分比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;IDE / agent 群 / 出错风险&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我看来，现在不管是“IDE 不再需要”的热炒，还是“agent swarm”的热炒，都有点过头了。模型当然还会犯错——如果是你真正关心的代码，我会建议你像鹰一样盯着它们：旁边开一个足够大的 IDE，用来随时检查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且错误的形态也变了：不再是简单的语法错，而是更隐蔽的概念性错误，有点像一个略显草率、匆忙的初级工程师会犯的那种。最常见的一类是：模型会替你做出一些错误假设，然后不核实就沿着假设一路跑下去。它们也不太会管理自己的困惑：不主动澄清、不揭示不一致、不提供权衡取舍、该反对时也不反对，而且还有点过度讨好。Plan mode 会好一些，但我感觉仍需要一种轻量的、内联的 plan mode。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它们也很容易把代码和 API 过度复杂化：抽象膨胀、架构臃肿、自己制造一堆 dead code 却不清理。它们能写出一个低效、臃肿、脆弱的 1000 行实现，然后就等你提醒一句：“呃……是不是其实可以更简单？”它们就会说“当然可以！”并立刻把它砍到 100 行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它们偶尔会作为副作用去改/删一些自己不喜欢、或没完全理解的注释和代码——哪怕这些内容和当前任务是正交的。即使我在 CLAUDE.md 里做了几次简单的指令尝试，这些问题仍会发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管有这些毛病，它依然带来巨大的净提升，而且很难想象再回到纯手工写代码的时代。TL;DR：每个人都有自己的新工作流；我现在的配置是：左边开少量几个 Claude Code 会话（Ghostty 的窗口/标签页里），右边开 IDE 负责看代码和手动改动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;韧性。看一个 agent 不知疲倦地死磕某件事真的很有意思。它们不会累，不会灰心，就是持续尝试——很多时候如果换成人，早就放弃、改天再战了。看它为一个问题挣扎很久，30 分钟后又突然赢了，那种“feel the AGI”的感觉很强。你会意识到：耐力本身就是工作的核心瓶颈之一，而 LLM 把这条上限显著抬高了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加速。LLM 辅助带来的“加速”其实不太好衡量。我当然感觉自己做原本要做的事更快了，但更大的变化是：我做了更多，原因主要是两点：&lt;/p&gt;&lt;p&gt;1）我可以写很多以前根本不值得写的东西；&lt;/p&gt;&lt;p&gt;2）我可以去碰以前因为知识/技能门槛而不敢碰的代码。&lt;/p&gt;&lt;p&gt;所以这当然是 speedup，但可能更像是一种“扩张”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杠杆。LLM 特别擅长反复循环，直到达到明确目标——大部分“feel the AGI”的魔法就在这里。与其告诉它怎么做，不如给它成功标准，然后看它自己跑。让它先写测试再通过；把它放进带浏览器 MCP 的闭环；先写一个很可能正确的朴素算法，再让它在保持正确性的前提下做优化。把你的指令从 imperative 转成 declarative，会让 agent 循环更久，从而获得更大的杠杆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;乐趣。我原本没预料到：用 agent 编程反而更有趣了，因为大量“填空式苦力活”被拿掉，剩下的更多是创造性部分。我也更少卡住（卡住真的不快乐），同时更有勇气——几乎总能找到一种方式与它并肩作战，推动事情向前。我也见过相反的观点：LLM 编程会把工程师分成两类——主要喜欢“写代码”的人 vs 主要喜欢“造东西”的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;退化。我已经注意到，自己手写代码的能力正在慢慢退化。“生成代码”和“判别代码（阅读/审查）”在大脑里是两种不同能力。因为编程里有大量偏语法的细碎细节，即便你写起来费劲，审代码通常仍能审得很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Slopacolypse（垃圾内容末日）。我已经在为 2026 做心理建设：那很可能是 GitHub、Substack、arXiv、X/Instagram，乃至整个数字媒体的“slopacolypse”（垃圾内容大爆发）之年。我们还会看到更多 AI 炒作式的生产力表演（这居然还能更夸张吗？），与此同时，也会出现真实而确凿的改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些问题。我脑子里的一些问题：“10X 工程师”会怎样？平均工程师与顶尖工程师的生产力差距，可能会被拉大很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 LLM 之后，通才会越来越超过专才吗？LLM 更擅长“填空”（微观）而不是“大战略”（宏观）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的 LLM 编程体验会像什么？像玩《星际争霸》？《Factorio》？还是演奏音乐？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社会中有多少领域，本质上被数字化知识工作所瓶颈住了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TL;DR：我们现在处在哪？&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到 2025 年 12 月左右，LLM agent 能力（尤其是 Claude 和 Codex）似乎跨过了某种连贯性阈值，并在软件工程及相关领域引发了一次“相变”。现在，“智能”这部分突然显得明显领先于其他所有东西——工具与知识的集成、组织层面的新工作流与流程、以及更广泛的扩散机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 将是高能量的一年：整个行业都在消化、吸收这股新能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/karpathy/status/2004607146781278521&quot;&gt;https://x.com/karpathy/status/2004607146781278521&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/karpathy/status/2015883857489522876&quot;&gt;https://x.com/karpathy/status/2015883857489522876&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/eUXjPQdv3XRVqWwdzJO9</link><guid isPermaLink="false">https://www.infoq.cn/article/eUXjPQdv3XRVqWwdzJO9</guid><pubDate>Wed, 28 Jan 2026 04:10:16 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item></channel></rss>