<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 14 Jan 2026 05:02:17 GMT</lastBuildDate><ttl>5</ttl><item><title>直播预告：新瓶旧酒还是涅槃重生？操作系统的 AI 进化终将走向何方？|《AI 进化论》第八期</title><description>&lt;p&gt;在 AI 与本土化双重浪潮之下，服务器操作系统正迎来历史性变革。由龙蜥社区理事长单位阿里云联合 InfoQ 打造的直播 IP 栏目《AI 进化论：智算时代操作系统的破局之路》，以云、AI、安全等技术与服务器操作系统如何融合演进为主线，聚焦服务器操作系统在智算时代的进化之路，特邀学术权威、行业专家、客户代表围绕原生智能、原生安全、软硬协同等热点议题展开深度对话。截至目前，已直播七期，线上观看人次达 60 万+。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 浪潮引发基础设施革命，服务器操作系统也正在迈入“(Cloud+OS)xAI”多维赋能的全新阶段。从国内外主流 OS 的差异化演进、阿里云 Alibaba Cloud Linux 4 的内核突破与性能跃升，到 “GPU 时代”的内核争议；从 OS-Copilot 的升级赋能，到 RISC-V 异构算力适配的前沿探索，操作系统的 “涅槃重生” 需要跨越哪些技术鸿沟？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《AI 进化论：智算时代操作系统的破局之路》系列直播第八期将于&amp;nbsp;1 月 22 日 14:00&amp;nbsp;开始，特别邀请到，阿里云智能集团总监、龙蜥技术委员会主席杨勇，中国科学院软件研究所高级工程师、RISC-V 行业生态负责人郭松柳，InfoQ 极客传媒策划编辑凌敏三位嘉宾，聚焦从业者核心困惑，结合龙蜥社区理事长单位阿里云 AI 增强套件、百万级服务器运维实践与 RISC-V 适配经验，深度拆解 AI 时代操系统的技术重构与价值重塑！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多直播亮点，可点击下方海报了解，欢迎大家打开微信，扫描二维码预约直播：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/40/40fa9f217d7e517813f953f26de25eae.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;—— 完 ——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</link><guid isPermaLink="false">https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</guid><pubDate>Wed, 14 Jan 2026 03:14:30 GMT</pubDate><author>阿里云</author><category>AI&amp;大模型</category><category>操作系统</category></item><item><title>待到山花烂漫时：鸿蒙开发者 用代码灌溉鸿蒙花园</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用代码浇灌春天，最终必将见证万紫千红的生态盛景。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;她说：“我愿在这群芳争艳的时代，绽放一抹‘吉祥’红！”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的 “金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X 框架迁移及原子化服务开发等技术突破，历时21个自然日完成 HarmonyOS NEXT 核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配HarmonyOS NEXT的故事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自22年加入吉林银行以来，卢妍娆便先后投入到了新一代核心系统建设以及吉林银行手机银行6.0迭代建设。23年年末吉林银行对应用鸿蒙化表示明确认可，认为鸿蒙生态适配不仅仅是吉林银行构建数字金融护城河的战略突破口，更是实现技术自主可控的关键战役，如春潮涌动时抢占滩头的先锋。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们非常期待能在HarmonyOS NEXT这个种满花卉的生态里，迅速绽放并共同成长，掌握一定的话语权。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在“打仗”之前，吉林银行研发团队完成了鸿蒙开发的学习，并于2024年2月与华为达成鸿蒙适配的合作意向。“华为为我们提供了技术上的答疑指导，帮助我们打通开发道路，让后面的开发更加便利。”万事俱备只欠东风，2024年5月底立项申请通过，项目正式启动，基于手机银行6.0功能及性能提升后的框架，6月18日正式上架核心交易功能版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c86c07962f19e5db24e0bbbf7a62a405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;卢妍娆在HDD活动照片&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“HarmonyOS NEXT跟安卓不一样，是个全新的系统，也是全新的体验”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;卢妍娆最初担心，吉林银行App适配鸿蒙的时候会很困难，因为原有的代码架构需要大规模重构。在鸿蒙声明式开发里，UI 是通过声明式语法描述的，需要重新编写大量的 UI 代码。事实上，开发过程真的很艰难吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“遇到技术难题的时候，你可以直接提出问题，鸿蒙的官方技术人员会回复，甚至提供样例代码手把手帮你解决问题。例如，我们开发团队在遇到微信分享无法获取uicontext，自定义弹窗无法展示的问题时，华为团队提供了示例代码解决问题；由于医保缴费框架存在中断逻辑，导致页面存在多次跳转，华为团队根据每次ID的不同，提供样例代码规避了消费者界面多次跳转的问题；开发语音识别功能的时候，我们团队没有足够的经验，华为技术人员提供了语音识别代码Demo以及UI代码，帮助我们快速实现语音识别功能。”卢妍娆回忆道。相比安卓开发中依赖第三方论坛的“投石问路”，鸿蒙的这种开发者帮扶模式更高效更贴心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea809d09e8659a816a8eef3713be9383.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;应用适配鸿蒙生态架构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;HarmonyOS SDK接入：纯净之境，开启开发新篇章&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们的手机银行集成第三方SDK有18个，HarmonyOS SDK替代了部分，不仅协同加速，提升了我们开发的效率，还为我们节省了大量成本。” 卢妍娆跟我介绍她们的应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统SDK在架构设计上往往存在冗余和复杂的问题，在接入时会引入大量不必要的代码和依赖库。而HarmonyOS SDK采用的原子化服务架构，将功能拆解为最小可复用单元，使用起来就像搭建积木一样，我们可以根据需求灵活选择和组合这些原子化服务。这种模块化设计使得代码更加简洁、清晰，如同月光下的水晶棱镜，每一个模块都剔透纯净。以一个简单的天气卡片组件为例，在HarmonyOS SDK中，开发者可以通过简洁的代码实现其功能，非常高效简洁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/49965718a3998fc12793e6a4f47f93bd.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;小组开会研讨方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&quot;HarmonyOS NEXT不是简单的系统升级，而是给开发者重新定义了工具类应用的魔法棒。当设备间的界限消失，我们才能真正聚焦于用户需求本身。&quot;对于吉林银行来说，鸿蒙生态带来的意义不仅仅优先他人一步，更重要的是带来了万物互联的时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;夜幕降临，金融街的灯火次第亮起。在这场由鸿蒙系统掀起的数字化浪潮中，银行正从传统的 &quot;金融服务提供者&quot; 转变为 &quot;智能生态构建者&quot;。当吉林银行以金融级安全纽带编织起千万用户的数字生活场景，既筑牢数字经济时代的安全护城河，又为银行生态的生长埋下战略伏笔；当意图框架读懂用户每一个潜在需求，各个企业正在书写属于自己的全场景智慧篇章。而这，仅仅是鸿蒙星河下的序章。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解鸿蒙开发认证详情，探索鸿蒙开发者联盟丰富资源，点击链接：&lt;a href=&quot;https://developer.huawei.com/consumer/cn/training/certifications/harmonyos?ha_source=51cto&amp;amp;ha_sourceId=70000008&quot;&gt;鸿蒙开发者联盟&lt;/a&gt;&quot;。这里有开发文档、论坛、工具等，快加入，开启鸿蒙开发之旅！&lt;/p&gt;</description><link>https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</link><guid isPermaLink="false">https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</guid><pubDate>Wed, 14 Jan 2026 02:49:38 GMT</pubDate><author>HarmonyOS</author><category>华为</category><category>HarmonyOS</category></item><item><title>规范驱动开发：让架构变得可执行</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第五代抽象&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程历史上的每一次重大转变都是由一种一致的力量驱动的：抽象的兴起。最早一代的软件是用原始机器代码编写的，后来汇编语言引入了可读性和控制层。更高级的语言已经跨越多个不同的范式发展，使得像C、Java和Python这样的通用语言及其衍生品得以发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些语言使得抽象得以进步，其中像内存管理和特定平台的怪癖这样的概念在日常工作流程中被掩盖，并且由开发者代表进行处理。这种级别的可访问性允许更广泛的生态系统发展，因为随着&lt;a href=&quot;https://medium.com/@ajuatahcodingarena/generations-of-programming-languages-bed30d19ea8e&quot;&gt;每一代语言&lt;/a&gt;&quot;的出现，相应的支持工具链也在发展。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4914486123b5441a9001b80e8ff7272b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图1：编程语言的世代&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第五代，即使用自然语言的第五代，长期以来一直是编程语言的目标演变，其中人类用他们的母语交谈，并且以可执行的方式进行解释。这种演变直到最近才真正成为主流。推动这一点的是人工智能（AI）的代际能力，现在已经成熟到可以接收以人为中心的输入，并用你选择的编程语言构建解决方案。几十年的学术研究记录了这一进化过程，&lt;a href=&quot;https://www.dreamsongs.com/RiseOfWorseIsBetter.html&quot;&gt;博客也非正式&lt;/a&gt;&quot;地对此进行了讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些转变中的一个共同主题是开发人员角色的演变。每一次抽象提升都允许开发人员更多地关注意图，而不是机制，我们现在进入了另一个拐点。第五代不仅因为广泛使用生成式AI而加速，而且与行业主导的采用相吻合。这代表了开发者如何从根本上接近他们手艺的新纪元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着前几代的成熟，支持它的工具链也出现了。在前几代中，工具是在一段时期稳定后出现的；然而，随着AI研究和产出的快速发展，工具链现在有望塑造和定义这一代，而不仅仅是支持它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为这些进步的一部分，一套关键的工具出现了，集中在规范驱动开发（SDD）上。这种趋势是由AI辅助代码生成的接受所驱动的，它允许开发人员提升自己的抽象，并表达系统应该做什么，而智能工具则实现了它的实际完成方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变重新定义了我们如何接近系统的架构和设计。现在，团队维护的是活的规范，而不是随着时间的推移而偏离原始意图的静态架构图。这些定义了系统契约、边界、不变量和行为。这些规范在设计上是可执行的；它们可以生成代码、文档、SDK、模拟甚至服务基础设施。AI智能体，通过角色映射的能&lt;a href=&quot;https://github.com/ambient-code/platform/tree/main/agents&quot;&gt;力来播种&lt;/a&gt;&quot;它们的上下文，其中角色的专业知识被捕捉为智能体的可消费输入，现在可以作为解释器、验证器和特定于领域的协作者行使权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在本文中，我们将SDD作为一种架构范例进行研究，详细说明规范如何成为系统的可执行支柱，漂移检测和持续验证如何将架构转变为运行时不变量，AI和代理工具如何重塑生成和管理，以及这种模型如何代表软件抽象长期演变中的下一个主要拐点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SDD架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规范驱动开发（SDD）这个名字可能暗示了一种方法论，类似于测试驱动开发。然而，这种框架低估了它的重要性。更准确的理解是，SDD是一种架构模式，它通过将可执行规范提升到代码本身之上，从而颠倒了传统的事实来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD代表了软件系统架构、治理和演变方式的根本转变。在技术层面上，它引入了一个声明性的、以契约为中心的控制平面，将规范重新定位为系统的主要可执行工件。相比之下，实现代码成为了次要的、生成的架构意图表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统架构依赖于源代码作为规范的控制面。在SDD中，控制面向上移动到规范控制面。这个控制面正式允许我们定义诸如：&lt;/p&gt;&lt;p&gt;接口契约（功能、输入/输出、行为保证）数据模式和不变量（结构、约束、验证规则）事件拓扑（允许的流、排序、传播语义）安全边界（身份、信任区域、策略实施）兼容性规则（包括向后和向前）版本控制语义（演进、降级、迁移）资源和性能约束（延迟、吞吐量、成本）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一变化涵盖了跨行为和治理的经典体系结构表面区域的组合，并具有正确性的时间维度。SDD不是独立地在服务和存储库之间协调这些领域，而是将它们集中到一个单一的权威模型中。这种模式更接近于语言类型系统或编译器：它不执行程序本身，而是定义了什么是可表达的，拒绝什么是无效的，并限制演变以保持随时间的正确性和兼容性。架构不再是咨询性的；它现在变得可实施和可执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管越来越多的工具被冠以SDD的标签，但从根本上说，它不是一个产品、框架或正式语言。相反，它是一个架构构造，以惊人的一致性作为一个五层执行模型重新出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层中，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/828f7550d967b76f786cb020c8143027.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图2：SDD 5层执行模型&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些层次共同构成了一个封闭的、由规范控制的控制系统，其中意图不断塑造执行，而执行不断地验证意图。由此产生的并不是对现有架构的渐进式改进，而是权威、控制和真实性所在位置的根本倒置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在让我们来看看这五个层次。在整个过程中，我们将遵循一个经典的订单管理服务的简化示例，以展示各层之间的进展以及它们是如何相互加强的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是系统行为的权威定义。它捕获了系统的声明性意图，而不是如何实现。这一层通常包含API模型、消息传递契约、领域模式和特定于系统、以策略为中心的约束。从抽象的角度来看，它既是人类可读的，也是机器可执行的，同时作为设计工件和操作控制面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能的样子：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;javascript&quot;&gt;service: Orders
api:   
  POST /orders:
      request:       
        Order:         
          id: uuid         
          quantity: int &amp;gt; 0     
      responses:       
        201: OrderAccepted       
        400: ValidationError  
policies:   
  compatibility: 
  backward-only   
  security:     
    auth: mTLS&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个规范明确声明了我们的期望：&lt;/p&gt;&lt;p&gt;订单必须是正数API不得引入破坏性变更请求必须经过认证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里没有引用任何语言、框架或基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层将声明性系统意图转化为可执行的形式。它作为一个多目标系统编译器，但与发出机器指令的经典编译器不同，这一层发出系统形状和跨语言、框架和平台的可执行运行时界面。在这里，问题空间由规范层声明，工具将其操作形式具体化。典型的输出包括类型模型、契约存根、验证中间件、文档以及一系列集成和一致性测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单示例为例，规范被摄取并发出可执行的系统表面。从概念上讲，这看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;spec.yaml
   → Type models (Java, TypeScript)
   → Request validators   
   → API stubs  
   → Contract tests&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具将声明的意图转化为具体的、可执行的形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;构件层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层包含了生成阶段的具体输出：生成的服务、组件、客户端、数据模型和适配器。关键的是，这些工件不被视为主要资产。相反，它们是可再生的、可丢弃的、可替换的，并且可以持续协调。这颠覆了传统软件架构的一个基本假设：代码不再是系统的记录；规范是。随着代码变得无限可复制和按需生成，新出现的术语环境代码恰如其分地抓住了这种范式转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的订单的形状现在可以用生成的一次性代码实现了。这可以看到类似于类型化模型的输出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;export interface Order {
   id: string;
   quantity: number; 
}
With a validator:
if (order.quantity &amp;lt;= 0) {
    throw new ValidationError(&quot;quantity must be greater than zero&quot;); 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些构件不是真相的来源。如果规范发生变化，它们将被重新生成。如果它们被删除，什么也不会丢失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;验证层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层强制执行意图和执行之间的持续一致性。它由契约测试、模式验证、有效载荷检查、向后兼容性分析和架构漂移检测机制组成。它在结构上扮演了编程语言的类型系统和管理程序对虚拟机所扮演的角色：积极防止架构违规传播到运行时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的生成层创建的工件最终在这里进行管理，其中验证确保运行时不能偏离意图：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;✓ Reject requests with quantity &amp;lt;= 0&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;违规行为在构建时、部署期间以及我们的持续集成系统中被检测到。架构正确性是持续强制执行的，而不是手动审查的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是操作系统本身，由典型的一系列构件组成，例如：&lt;/p&gt;&lt;p&gt;API消息代理和流处理管道函数、方法和等效结构集成服务&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至关重要的是，运行时的形状完全受到上游规范和验证层的约束。因此，运行时行为在架构上是确定的，而不是涌现性的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果我们尝试在我们的订单服务使用负数量，如下所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;POST /orders 
{ 
  &quot;id&quot;: &quot;123&quot;, 
  &quot;quantity&quot;: -1 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们返回了一个400 ValidationError，并不是因为运行时拒绝了请求，而是因为在系统执行任何请求之前，该行为在规范层中声明，由生成层具体化，由构件层实例化，并由验证层持续强制执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构反转&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几十年来，软件架构一直在一个基本上未受挑战的假设下运作，即代码是最终的权威。架构图、设计文档、接口契约和需求规范都是用来指导实现的。然而，运行中的系统总是从最终部署的内容中获得其真相。当出现不匹配时，标准的反应是“更新文档”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD完全颠覆了这种关系。规范成为系统现实的权威定义，实现是持续派生、验证的，并且在必要时重新生成以符合该真实性。这不是一个哲学上的区别；它是软件系统治理的结构性反转。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统的软件交付遵循线性、有损失的管道，如图3所示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/98bba58fc95d62fbe1391268cfc625d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图3：传统的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一步翻译都引入了重新解释、手动适应和隐藏的假设。因此，不能阻止架构漂移；它是在晚期被发现的，通常是通过生产事件、失败的集成、安全审计或合规性违规。当检测到不一致时，它是取证而不是纠正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD从根本上将这种流程重构为一个受控的控制循环：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5a9a1f07925b2e6f8103b15cdc647cf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图4：SDD受控的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个控制循环用积极的架构强制执行取代了延迟发现。漂移检测不会修补运行时行为；它纠正规范权威，并触发系统的受控再生。传统架构假设代码随时间的推移成为事实；SDD通过确保规范保持永久的事实来源，并且运行时持续被迫符合它，从而颠覆了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种架构反转可以概括如下几点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在SDD中，代码不再是真相出现的地方，而成为真相仅仅被实现的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种反转在结构上等同于早期的范式转变，即从人的责任中移除整个类别的正确性约束，并使其在机械上可执行：&lt;/p&gt;&lt;p&gt;从手动内存管理到垃圾收集，内存安全成为运行时不变量从裸机到虚拟机，隔离和资源边界成为平台保证从物理服务器到声明性基础设施，其中配置漂移和拓扑正确性不断得到协调从无类型语言到静态类型系统，在编译时强制执行结构正确性从非正式的接口协议到模式和契约强制执行的API，交互正确性被机械地验证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在每种情况下，正确性都从传统上由人类强制执行转变为由平台结构性强制执行。SDD将这一原则应用于系统边界、架构和行为本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;漂移检测：使架构自我强制执行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦规范成为权威，漂移检测不再是一种测试便利；它成为一种强制性的架构能力。它是将意图转化为不变性的执行机制。在这个模型中，漂移不仅仅是模式不匹配；它是声明的系统意图和观察到的系统行为之间的任何偏差。这种偏差可能是结构性的、行为性的、语义性的、与安全相关的或进化性的。我们在实验中遇到的一些例子包括：&lt;/p&gt;&lt;p&gt;一个API返回了规范中未声明的字段一个服务在重构过程中默默地省略了必需的字段消息负载在没有协调的模式版本控制的情况下不断演变错误处理偏离了合同保证相对于最初的策略意图，安全范围正在退化&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没有漂移检测，SDD就会退回到文档驱动的开发。有了它，系统就变成了自我监管。漂移检测形成了一个闭环反馈控制系统。它不断地比较系统声称要做的事情和它实际做的事情。这与古典测试相比，后者只提供周期性的、基于样本的保证，是一种根本不同的操作姿态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统架构中，意图的偏差会悄无声息地传播，通常数月之后才会以中断、审计失败或安全漏洞的形式显现出来。在SDD系统中，漂移变成了机器默认可以检测到的。规范验证器可以直接嵌入我们的CI管道中，运行时执行层：模式验证、有效负载检查、契约验证和规范差异引擎都成为了一等的架构组件。当输出违反规范时，系统会快速失败，并允许进行航向修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种强制要求在一个固有的多模型未来中变得更加重要。软件系统将越来越多地受到人类驱动开发和机器驱动生成的影响，通常在同一规范表面上并行操作。系统中不再有单一的线性路径。更改可能来自开发者、AI代理、自动化重构工具或政策驱动的生成器。这种进化路径的多样性极大地放大了漂移问题：分歧不再是边缘情况；它是必须持续治理的自然状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总体效果是治理方式的深刻转变。架构不再是设计阶段的产物；它变成了一个持续执行的运行时不变量。规范从被动的参考资料转变为主动的控制表面，漂移检测作为反馈信号，保持系统与意图一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，这并不意味着一个完全自主的系统，机器单方面定义正确性。规范不仅仅是一个机械合同；它是人类对目的、风险容忍度和权衡的表达。漂移检测可以识别系统已经偏离，但它不能单独决定这种偏离是可以接受的、偶然的还是可取的。一些漂移代表缺陷，而其他漂移代表进化。在这个边界上，当自动化执行遇到解释性判断时，人类的角色再次变得至关重要。不是作为失败后被动审查日志的审查者，而是作为治理意义、意图和受控变更的积极参与者。这就是Human-in-the-Loop（人在循环中）不再只是一个安全网，而是一个一等的设计原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人在循环中：在自动化架构中保留意图&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们最初探索这种系统设计模式时，我们以一种天真的“氛围编码”心态来接受生成的变化，最小化阻力，并信任SDD工具链为我们处理边缘情况。那个假设很快就失败了。取而代之的是一个更强大的认识：SDD并没有将人类从循环中移除；它将人类判断重新定位到更高的控制层面。问题不再是人类如何实现系统，而是如何以及在哪里治理系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD并没有消除人类在软件设计中的参与。它重新分配了人类认知的应用领域。传统上，一旦功能实现，开发人员就会花费大量的精力来解决不匹配、调试集成故障、协调分散的服务以及修复更改的意外副作用。随着时间的推移，这被错误地等同于软件工程本身的手艺。实际上，这是维护大型、长期、面向生产的系统的负担。SDD将这个负担转移到机器上，同时故意保留人类对意图、策略和意义的权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引入了一种新型的人机界面。人类仍然是领域语义、风险容忍度、安全范围和系统进化方向的最终守护者。这种权威也扩展到隐含地塑造工程决策的法律、伦理和道德框架中。这些维度不能仅从执行跟踪或行为观察中推断出来。它们存在于机器无法完全拥有的抽象层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，人类将这些约束明确编码到规范层中，机器承担起执行、生成和持续一致性的责任。这反映了我们技术的历史演变：就像我们曾经将手动内存管理交给垃圾收集一样，我们现在正在将结构性执行和机械一致性委托给SDD。取代这种委托的不是盲目的自动化，而是明确的审批边界：&lt;/p&gt;&lt;p&gt;破坏模式更改需要人工批准策略转变需要人工授权AI提出的重构需要人工确认兼容性降级需要人工解释&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，SDD实现了有限的自主性，而不是完全自动化，并且在这些限制内，长期架构意图可以得以保留。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过强制漂移检测和人工意图监督，SDD在人和机器之间建立了新的责任分工。执行变得自动化。意义仍然是人类的。这种分离不是哲学上的；它是架构上的，正是这种分工产生了一类新的基础设施能力。一个的规范原生系统现在必须将执行、演化、验证和治理直接编码到其核心原语中。我们接下来探讨这些能力，以及它们为什么在结构上与经典软件架构中的能力不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规范原生系统的核心能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD不是由单一工具、框架或平台启用的。它源于一组紧密耦合的架构能力，这些能力共同允许规范变得可执行、可强制和可扩展。当这些能力中的任何一个缺失时，SDD就会退回到文档驱动开发或临时代码生成。要从理论进入操作范式，系统必须内化五个核心能力：&lt;/p&gt;&lt;p&gt;规范编写作为一等工程表面正式验证和类型强制确定性生成和组合持续的一致性和漂移执行受控演化和兼容性控制&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们将这种操作规程称为SpecOps，即规范操作。从SpecOps的角度来看，规范被视为一等的、可执行的系统资产，这些能力并没有定义一个产品类别；它们代表了软件意图的控制平面。在规范原生系统中，规范编写不是在实现之前进行的活动；它就是实现活动。因此，系统必须支持多模型规范，其中结构、行为和策略定义共存于统一的模式空间内。这需要可组合的领域建模，使得分层规范成为一种可行的架构策略，而不是文档便利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着规范成为主要的系统构件，它们必须像源代码一样被严格地处理：版本控制、同行评审、分支和受控合并策略都要是强制性的。此时，规范不再是描述性的，而是成为系统本身的可编程模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦规范是可执行的，它还必须是机器可验证的，就像编译器前端或类型系统一样严格。这种执行涵盖了结构验证、语义一致性和领域不变性执行。条件约束、引用完整性和跨规范一致性必须是可证明的。效果不仅仅是提高了正确性，而是从可以表示的所有内容的空间中消除了整个类别的系统故障，就像静态类型限制非法程序一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个范式中，生成不是脚手架的一种形式。它是声明的系统真理的具体化。这需要严格确定性行为。一个生产级别的规范原生系统必须保证输入的确定性：相同的规范总是产生相同的构件。它必须是目标无关的，跨语言、平台和运行时环境产生一致的输出。最关键的是，生成必须是逻辑可逆的。系统必须始终能够回答一个简单但基础的问题：哪个规范状态产生了这种行为？这种决策的谱系可追溯性是将生成从生产力辅助提升到架构权威的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦生成自动化，执行就必然变得连续。运行时系统不能再悄悄地偏离声明的意图。实现不能引入未记录的行为。消费者不能依赖于未定义的属性。因此，架构从设计时断言转变为运行时不变性，由系统本身积极维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD中最困难的能力不是生成或验证，而是不断裂的更改。规范原生系统必须自动将更改分类为添加性、兼容性、破坏性或模糊性，并执行明确的兼容性策略。这引入了受控演化的正式概念：需要并行版本表面、已知的兼容性窗口、受控的弃用曲线和用于破坏性更改的显式批准门。没有这个，SDD在架构上就会变得脆弱。有了它，系统可以在不违反自己的正确性保证的情况下进行演化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这五个能力引入的最深刻的转变不是技术上的；它是结构上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交付的单元不再是服务或代码库。交付的单元变成了规范本身。这将结果与产出重新对齐：声明的是什么，交付的就是什么。这与以氛围驱动、生成性编码方法形成鲜明对比，在这些方法中，偏差是创造力（或幻觉）的涌现属性，而不是设计中受控的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论：存在的工程权衡和挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程中每一次重大的抽象飞跃都带来了非凡的生产力提升，同时引入了全新的系统性风险类别。垃圾收集消除了大量内存错误，同时引入了暂停时间行为和新的故障模式。虚拟机简化了部署，同时增加了业务编排的复杂性。云平台消除了基础设施负担，同时引入了深层操作耦合。规范驱动开发也不例外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将系统的真实来源提升到规范和生成器中，SDD并没有消除复杂性；它只是简单地重新定位了复杂性。下面的权衡定义了我们在大规模采用这种范式时所经历的真实工程成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范成为主要的复杂性表面&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，规范不再是文档构件，而是成为长期存在的可执行基础设施。因此，它们获得了传统上与源代码相关的属性。它们继承了通常与源代码相关的所有属性：技术债、跨团队耦合、兼容性惯性和架构引力。因此，模式工程成为了与数据建模和分布式系统设计同等重要的一级架构学科。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成器信任成为供应链问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，AI代码生成器不再是开发者的便利工具。它们成为系统可信计算基础的结构组件。确定性、可重复性、可审计性、沙箱执行和可验证的出处不再是可选属性；它们是强制性的。代码生成从工具提升为关键基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时执行有实际成本&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD将执行从社会过程转移到技术控制。这种转变是强大的，但不是免费的。运行时契约验证引入了实际的计算开销。在小规模上，这个成本是微不足道的。在大规模上，我们需要考虑系统的目的，无论是高频API、实时流还是对延迟敏感的系统。这成为了一个明确的架构预算项目。正确性成为了计量资源，而不是默认的免费属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;认知转变非同小可&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD用契约优先推理取代了实现优先的思维。这要求工程师采用新的心智模型：&lt;/p&gt;&lt;p&gt;用不变量而不是行为来思考关于兼容性而不是功能的推理用声明式而不是过程式表达意图将模式视为可执行程序&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;历史上每一次抽象的转变都扩大了人类的影响力，同时引入了不熟悉的失败模式，需要多年才能掌握。SDD现在正进入相同的成熟曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;架构权威的价格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然新的范式转变通常令人兴奋，但最终是否采用这一转变归结于平衡所涉及的实际权衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，SDD提供了：&lt;/p&gt;&lt;p&gt;架构确定性持续的正确性执行系统性减少漂移多语言平价可复制的系统边界&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但它以以下代价：&lt;/p&gt;&lt;p&gt;模式复杂性生成器信任要求运行时验证成本长期兼容性负担工程角色的认知转变&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不是避免SDD的理由。这是一个有意识地采用它的理由，要有明确的治理、有纪律的规范实践，以及对其成本的清醒认识。每一次抽象的飞跃都需要新的严谨形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD只是将这种严谨重新定位到它一直属于的地方：系统真理本身的定义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/spec-driven-development/&quot;&gt;https://www.infoq.com/articles/spec-driven-development/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</link><guid isPermaLink="false">https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</guid><pubDate>Wed, 14 Jan 2026 02:11:22 GMT</pubDate><author>Leigh Griffin</author><category>架构</category><category>AI&amp;大模型</category></item><item><title>Java近期资讯：Spring gRPC、Quarkus、Gatherers4j、Keycloak、Grails、Java Operator SDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B29&quot;&gt;Build 30&lt;/a&gt;&quot;在上周发布，其中包括对Build 29的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B29...jdk-26%2B30&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2026%20and%20%22resolved%20in%20build%22%20%3D%20b30%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B4&quot;&gt;Build 4&lt;/a&gt;&quot;也在上周发布，包含了从Build 3的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B3...jdk-27%2B4&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2027%20and%20%22resolved%20in%20build%22%20%3D%20b04%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-grpc&quot;&gt;Spring gRPC&lt;/a&gt;&quot; 1.0.1，&lt;a href=&quot;https://spring.io/blog/2026/01/07/spring-grpc-1&quot;&gt;第一个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、依赖升级和增强功能，例如：与跟踪相关的更详细的错误消息；以及使用Spring Security SecurityContextHolder 类中定义的 getContext() 方法与gRPC特定的Kotlin协程的能力。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/spring-projects/spring-grpc/releases/tag/v1.0.1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Quarkus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Quarkus 3.30.6，&lt;a href=&quot;https://quarkus.io/blog/quarkus-3-30-6-released/&quot;&gt;第六个维护版本&lt;/a&gt;&quot;，带来了显著的变化，例如：解决了在&lt;a href=&quot;https://quarkus.io/extensions/io.quarkus/quarkus-jfr/&quot;&gt;JDK Flight Recorder&lt;/a&gt;&quot; 扩展在发出运行时信息时由于关闭时失败而导致的 NullPointerException ；以及移除了官方&lt;a href=&quot;https://github.com/lz4/lz4-java/blob/master/README.md&quot;&gt;LZ4 Java&lt;/a&gt;&quot;项目（ org.lz4:lz4-java ），转而使用由Oracle的首席技术员工&lt;a href=&quot;https://www.linkedin.com/in/yawkat/&quot;&gt;Jonas Konrad&lt;/a&gt;&quot;维护的分支（ at.yawk.lz4:lz4-java ），因为前者在2025年底停止维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.30.5&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gatherers4j&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://tginsberg.github.io/gatherers4j/&quot;&gt;Gatherers4j&lt;/a&gt;&quot; 0.13.0版本发布了新的中间方法 -uniquelyOccurringBy() ，旨在将流限制为由给定函数测量的唯一发生元素，以及添加到 Gatherers4j 抽象类中以计算 Java Stream&lt;t&gt; 接口的移动和运行中的中位数、最大值和最小值的 movingMedian() 和 movingMedianBy() ， runningMedian() 和 runningMedianBy() ， movingMax() 和 movingMaxBy() ， movingMin() 和 movingMinBy() ， runningMax() 和runningMaxBy() ， runningMin() 和 runningMinBy() 等方法。&lt;/t&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gatherers4j由德意志银行的主管和首席工程师&lt;a href=&quot;https://www.linkedin.com/in/tginsberg/&quot;&gt;Todd Ginsberg&lt;/a&gt;&quot;于2024年7月推出，是一个基于JEP 485，&lt;a href=&quot;https://openjdk.org/jeps/485&quot;&gt;Stream Gatherers&lt;/a&gt;&quot;的中间流库，在JDK 24中交付。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/tginsberg/gatherers4j/releases/tag/v0.13.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Keycloak&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.keycloak.org/&quot;&gt;Keycloak&lt;/a&gt;&quot; 26.5.0&lt;a href=&quot;https://www.keycloak.org/2026/01/keycloak-2650-released&quot;&gt;版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：&lt;a href=&quot;https://www.keycloak.org/securing-apps/jwt-authorization-grant&quot;&gt;JWT授权授予预览版&lt;/a&gt;&quot;，用于OAuth 2.0客户端身份验证和授权授予（&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc7523&quot;&gt;RFC 7523&lt;/a&gt;&quot;）规范的JSON Web令牌（JWT）配置文件的实现，用于使用外部签名的JWT断言请求OAuth 2.0访问令牌；以及OpenTelemetry增强功能，包括将日志导出到OpenTelemetry收集器和使用Quarkus &lt;a href=&quot;https://quarkus.io/guides/telemetry-micrometer-to-opentelemetry&quot;&gt;Micrometer和OpenTelemetry&lt;/a&gt;&quot;扩展导出指标。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Grails&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://grails.apache.org/&quot;&gt;Grails&lt;/a&gt;&quot; 7.0.5，第五个维护版本，提供了缺陷修复和增强功能，例如：添加了缺失的应用程序类名和脚本名参数到 url-mappings-report Grails控制台命令；以及移除了 org.apache.tomcat.embed:tomcat-embed-logging-log4j 模块，因为它自2016年5月以来一直没有维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Java Operator SDK&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://javaoperatorsdk.io/&quot;&gt;Java Operator SDK&lt;/a&gt;&quot; 5.2.2版本发布，这是一个用于与Kubernetes操作符一起工作的工具，带来了显著的变化，例如：在 ExpectationIT 和 PeriodicCleanerExpectationIT 类中添加了 @Sample 注解，以改进集成测试；以及解决了在启动出现错误时线程池不停止的问题。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.2.2&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</link><guid isPermaLink="false">https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</guid><pubDate>Wed, 14 Jan 2026 01:17:20 GMT</pubDate><author>作者：Tim Anderson</author><category>编程语言</category></item><item><title>模力工场 028 周 AI 应用榜：AI “身体”觉醒，从工业前线到情感陪伴</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模力工场新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;将亮相 OceanBase 社区嘉年华！诚邀您加入我们的上海现场展位。作为 OceanBase 合作的创新社区，模力工场将于 1 月 31 日 登陆上海社区嘉年华，并拥有专属展位。这不仅是一次技术交流——我们更希望和您一起，在现场用 AI Coding 展现创造力、在开放麦分享您的项目故事、与行业先锋面对面切磋、在开源市集交换灵感。我们为您预留了专属席位，期待与您共同呈现：当开源精神遇上 AI 创造力，能碰撞出多少令人惊艳的可能。立即报名，锁定与数百位技术同行深度连接的一天！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fb3d72f8356cfc07975e9e5f9d2e177c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;028 周榜单总介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 028 周 AI 应用榜来啦！本周上榜的应用大多来自美国 CES 展及阿里云通义智能硬件展，从优必选的集群物流调度系统到银河通用的零样本抓取机器人，从众擎的拟人步态双足机器人到 Walulu 的情感陪伴毛绒玩具——这些应用共同见证了一场时代风暴：AI 硬件正在集体跨越“工具”属性，进化为真正的“智能体”。它们不再是被动响应指令的机械装置，而是具备了理解环境、自主规划、闭环执行乃至情感交互能力的“数字生命体”。这场从“功能叠加”到“语音助手”再到“智能体化”的范式革命，正同时重塑生产力与生产关系：在工业场景成为可靠的“数字员工”，在消费领域则成为可建立羁绊的“数字伙伴”，标志着人机协同进入了全新的历史阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;: 一款面向创作者与普通用户的 AI 互动式内容生成应用，通过自然语言或轻量交互，快速生成有趣、可分享的内容。&lt;a href=&quot;https://agicamp.com/products/deeprobotics?utm_source=20260112infoQ&quot;&gt;云深处巡检机器人&lt;/a&gt;&quot;: 专注于工业复杂环境的自主巡检解决方案。其四足机器人具备强运动与感知能力，可在无网络支持下独立完成巡检任务并安全返回，已在电力、能源等领域实现落地应用。&lt;a href=&quot;https://agicamp.com/products/ubtrobot?utm_source=20260112infoQ&quot;&gt;优必选（UBTECH）搬运/物流机器人&lt;/a&gt;&quot;: 提供从智能搬运机器人到集群调度系统的软硬件一体化智慧物流方案，帮助企业实现仓储搬运环节的自动化升级与效率提升。&lt;a href=&quot;https://agicamp.com/products/engineai?utm_source=20260112infoQ&quot;&gt;众擎机器人&lt;/a&gt;&quot;: 聚焦高动态双足人形机器人的研发，致力于突破拟人步态与平衡控制技术，为未来机器人在人类环境中的通用移动能力提供底层支撑。&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu 📍成都&lt;/a&gt;&quot;: 一款具备情感交互与离线记忆能力的 AI 智能毛绒玩具，通过多模态交互设计，为用户提供个性化、可长期互动的陪伴体验。&lt;a href=&quot;https://agicamp.com/products/galbot?utm_source=20260112infoQ&quot;&gt;银河通用机器人&lt;/a&gt;&quot;: 研发面向仓储、零售等场景的通用移动操作机器人，具备视觉识别与自主抓取能力，可在动态环境中完成物品拣选、搬运等任务。&lt;a href=&quot;https://agicamp.com/products/spirit?utm_source=20260112infoQ&quot;&gt;千寻智能Spirit AI&lt;/a&gt;&quot;: 从事通用人形机器人系统研发，整合高性能硬件平台与 AI 算法栈，探索机器人在多场景下的感知、决策与执行能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周必试应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：全流程托管｜零门槛动画｜AI 协同创作&lt;/p&gt;&lt;p&gt;模力小A推荐：通过七个 AI 智能体（导演、编剧、美术等）分工协作，将你的文字想法自动转化为包含分镜、角色与场景的动画视频，大幅降低了专业动画内容的制作门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上榜冷门但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：AI 硬件｜可成长陪伴｜离线记忆&lt;/p&gt;&lt;p&gt;模力小A推荐：一款结合了情感计算模型的智能玩具。它能够记住与你的互动，并做出个性化的反应，提供一种注重私密性与持续性的陪伴体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 正在从虚拟世界走向物理世界，为自己寻找真实的“身体”。本周&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;榜单上的应用清晰地展示了这一趋势——AI 不再是停留在软件层面的算法，更是成为驱动各类硬件的“大脑”。这次上榜的八大应用，集中体现了AI 硬件在两大关键赛道的爆发：工业效率革命与情感陪伴需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在工业领域，AI 机器人正从简单的机械臂进化为真正的“智能员工”。云深处的巡检机器狗能够在无网络环境的复杂场景中自主完成巡检任务，实现了从“自动化”到“自主化”的跨越；优必选的智慧物流方案已超越单台设备，提供机器人群调度与仓储管理系统深度集成的整套解决方案；银河通用的物流机器人则实现了“零样本抓取”能力，即使面对全新商品也能准确识别搬运。这些进展表明，工业机器人正从实验室原型走向工程化落地，其核心价值在于可量化的投资回报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在消费领域，情感陪伴型机器人正开辟全新市场。Walulu 的 AI 毛绒宠物通过情感模型与离线记忆技术，创造出能随互动成长的“伙伴关系”，本质是在贩卖情感价值而非功能价值。这反映了 AI 正从解决效率问题，转向满足更深层的心理需求。未来，能否建立稳定、专属的“数字亲密关系”，或将成为此类产品发展的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得关注的是众擎的人形机器人——虽然步态尚显蹒跚，但其对双足行走、自然步态的追求，瞄准的是机器人无缝进入人类环境的终极目标。这种对“通用形态”的前瞻布局，代表着产业在为更广阔的未来场景做技术储备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了实体硬件产品，OiiOii 这款 AI 动画创作应用近期也备受瞩目。其“全流程托管模式”尤为亮眼——平台将传统动画制作中的艺术总监、编剧、分镜师、角色设计师、场景设计师、动画师、音效总监等七个核心角色，分别由七个 AI 智能体担任。这些智能体不仅形象亲切可爱，更如导师般指引用户一步步完成创作。用户只需输入创意想法，并在关键节点进行确认，即可产出完整动画作品。这极大降低了创作门槛，让普通用户也能轻松上手动画制作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综上，AI 硬件已越过“加个语音模块”的简单升级阶段，进入以智能体化为特征的第三阶段。产业不再满足于制造“能联网的工具”，而是致力于创造“能自主行动的数字生命体”。从工业现场到家庭空间，AI 正在改写生产力与生产关系的定义——在工厂成为可靠的数字员工，在生活场景成为温暖的数字伙伴。当 AI 真正获得在物理世界中感知、决策和执行的能力，人机协同或将进入一个前所未有的新纪元。&lt;/p&gt;</description><link>https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</link><guid isPermaLink="false">https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</guid><pubDate>Tue, 13 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>Claude Code创建者的开发工作流程</title><description>&lt;p&gt;Claude Code的创造者Boris Cherny描述了他如何在&lt;a href=&quot;https://x.com/bcherny/status/2007179832300581177?s=20&quot;&gt;Anthropic&lt;/a&gt;&quot;上使用Claude Code，强调了诸如运行并行实例、共享学习成果、自动化提示和严格验证结果等实践，以随着时间的推移提高生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny没有定制Claude Code，因为他发现它开箱即用，非常好用，可以并行运行许多会话，包括在他的&lt;a href=&quot;https://x.com/bcherny/status/2007218725754442106?s=20&quot;&gt;MacBook&lt;/a&gt;&quot;终端本地运行的5个会话和在Anthropic的网站上运行的5-10个会话。为了避免冲突，&lt;a href=&quot;https://x.com/bcherny/status/2007200880081436864?s=20&quot;&gt;每个本地会话使用自己&lt;/a&gt;&quot;的 git checkout ，而不是分支或工作树。他从CLI开始与 &amp;amp; 进行远程会话，并经常使用 -teleport 将它们来回移动。然而，由于意外情况，&lt;a href=&quot;https://x.com/bcherny/status/2007219912411115702?s=20&quot;&gt;这些会话中有10-20%被放弃了。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny更喜欢使用Opus 4.5进行所有编码工作，他重视其比Sonnet更高的质量和可靠性，尽管Sonnet的速度较慢。他还发现Opus更擅长使用工具，并指出其总体上比小模型更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic的每个团队都在git中维护一个 CLAUDE.md 文件，以便Claude可以随着时间的推移而改进，以及最佳实践，如风格约定、设计指南、&lt;a href=&quot;https://x.com/bcherny/status/2007212366094811401?s=20&quot;&gt;PR模板&lt;/a&gt;&quot;等。Cherny经常经常在同事的PR上使用 @.claude 标签，将学习成果添加到 CLAUDE.md 中，确保每个PR的知识都被保存下来。Cherny说，目前，他们的 CLAUDE.md 有2.5k的token。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他的工作流程的一个关键方面是，先制定一个计划，然后迭代完善，再切换到自动编辑：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果我的目标是写一个Pull Request，我会使用Plan模式，然后和Claude来回交流，直到我喜欢它的计划。从那里，我切换到自动接受编辑模式，Claude通常可以一次性完成。一个好的计划真的很重要！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny使用斜杠命令执行提交、PR、简化和验证等日常工作流程来启动子智能体。所有的命令都存储在 .claude/commands/ 中，这也有助于减少对明确提示的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，Claude和我每天使用/commit-push-pr斜杠命令数十次。该命令使用内联bash预先计算git状态和其他一些信息，以使命令快速运行，并避免与模型来回切换。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然Claude的代码通常格式良好，但不一致有时会导致CI失败。为了防止这种情况发生，Cherny运行了一个PostToolUse钩子来清理代码：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;&quot;PostToolUse&quot; : [
     &quot;matcher&quot;: &quot;WritelEdit&quot;,
      &quot;hooks&quot;: [         
        {             
          &quot;type&quot;: &quot;command&quot;,             
          &quot;command&quot;: &quot;bun run format || true&quot;         
        }     
      ]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;出于安全考虑，Cherny几乎从不使用 --dangerously-skip-permissions 。相反，他通过 /permissions 启用在他的环境中安全的常用bash命令。这省去了他在诸如 bun run build:* 、 bun run test:* 、 cc:* 等命令上不必要的许可提示。他使用 --dangerously-skip-permissions 的唯一情况是在沙箱中运行长期任务，以防止Claude重复停止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最重要的技巧是给Claude提供一种通过反馈循环验证其工作的方法，例如运行bash命令、测试套件、或通过浏览器或模拟器测试应用程序。这可以将最终结果的质量提高2-3倍：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Claude会使用Claude Chrome扩展测试我给他的每一个claude.ai/code变更。它打开一个浏览器，测试UI，不断迭代，直到代码正常运行，用户体验很好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总的来说，Cherny解释说，&lt;a href=&quot;https://x.com/bcherny/status/2007290414961963524?s=20&quot;&gt;这种工作流程让他的团队专注于代码审查和指导&lt;/a&gt;&quot;，并指出当工程师阅读PR时，代码已经处于良好的可用状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny的推文在X.com上引发了广泛的讨论，包括一些我们在这里包含的有用澄清，但请务必阅读原文以了解全部细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&quot;&gt;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</link><guid isPermaLink="false">https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</guid><pubDate>Tue, 13 Jan 2026 07:03:00 GMT</pubDate><author>Sergio De Simone</author><category>性能优化</category></item><item><title>亚马逊云科技推出VPC加密控制，在传输过程中实施强制加密</title><description>&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2025/11/aws-vpc-encryption-controls/&quot;&gt;亚马逊云科技（AWS）最近推出了VPC加密控制功能&lt;/a&gt;&quot;，允许客户验证VPC内部和VPC之间的流量是否加密，并在支持的地方要求加密。该功能提供了对未加密流量的可见性，支持使用兼容的基于Nitro的基础设施进行强制执行，并允许排除无法加密流量的资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据云服务提供商称，这项新功能有助于组织在他们的AWS环境中应用一致的加密标准，并展示符合HIPAA、PCI DSS和FedRAMP等监管框架的合规性，这些框架要求全面加密。AWS的首席开发者倡导者&lt;a href=&quot;https://www.linkedin.com/in/sebastienstormacq/&quot;&gt;Sébastien Stormacq&lt;/a&gt;&quot;&lt;a href=&quot;https://aws.amazon.com/blogs/aws/introducing-vpc-encryption-controls-enforce-encryption-in-transit-within-and-across-vpcs-in-a-region/&quot;&gt;解释道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;金融服务、医疗保健、政府和零售等行业的组织在维护云基础设施的加密合规性方面面临着重大的操作复杂性。传统方法需要将多个解决方案拼凑在一起，并管理复杂的公钥基础设施（PKI），同时手动使用电子表格跟踪不同网络路径上的加密。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然社区的反应大多是积极的，但许多人&lt;a href=&quot;https://www.reddit.com/r/aws/comments/1p3jgtg/introducing_vpc_encryption_controls_enforce/&quot;&gt;最初对定价方法&lt;/a&gt;&quot;表示困惑，或者质疑为什么应该为安全控制付费。用户kei_ichi写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这个功能应该默认启用并且免费。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员可以为现有的VPC启用该功能，以监控流量流的加密状态，并识别无意中允许明文流量的VPC资源。云安全顾问和AWS安全英雄&lt;a href=&quot;https://www.linkedin.com/in/jcfarris/&quot;&gt;Chris Farris&lt;/a&gt;&quot;在他的&lt;a href=&quot;https://www.chrisfarris.com/post/reinvent2025/&quot;&gt;re:Invent&lt;/a&gt;&quot;概述中写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;让我们从为什么应该避免这种情况开始——每个非空VPC每月110美元。如果你需要“满足像HIPAA和PCI DSS这样严格的合规标准”和“展示符合加密标准”，这绝对是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC加密控制有两种操作模式：监控和强制执行。激活后，强制执行模式确保所有新资源仅在兼容的Nitro实例上创建，并且在检测到错误的协议或端口时丢弃任何未加密的流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba9147fdbbff2c458bbff4b8e9870f93.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：AWS博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员只有将所有资源迁移到兼容加密的基础架构后，才能启用强制模式。Farris指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你的VPC中有未加密传输的资源，你不能启用强制执行模式。这里的迁移工作将非常巨大，但如果你的审计员要求你手工完成这项工作，这些成本是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这需要首先升级到支持的硬件和通信协议。可以为不支持加密的资源（如互联网或NAT网关）配置特定的排除，因为它们的流量离开了AWS网络。在“理解现代云安全中的VPC加密”的文章中，Anish Kumar&lt;a href=&quot;https://medium.com/@anishkumarait/understanding-vpc-encryption-in-transit-for-modern-cloud-security-0cee62cd6501&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于你的云安全态势，你可以自信并有证据地回答这个问题：“我所有的VPC中的流量都加密了吗？”从合规审计的角度来看，你可以在流量日志和排除列表中展示加密状态。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项新功能目前在AWS的一些区域可用，包括弗吉尼亚北部、爱尔兰、伦敦和新加坡。在3月1日之前，VPC加密控制将免费使用，之后将对每个非空VPC收取固定的小时费，每小时0.15美元起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls/&quot;&gt;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</link><guid isPermaLink="false">https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</guid><pubDate>Tue, 13 Jan 2026 06:11:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>云安全</category></item><item><title>TanStack发布框架无关的AI工具包</title><description>&lt;p&gt;&lt;a href=&quot;https://tanstack.com/&quot;&gt;TanStack&lt;/a&gt;&quot;是广受欢迎的TypeScript库（如&lt;a href=&quot;https://tanstack.com/query/latest&quot;&gt;TanStack Query&lt;/a&gt;&quot;和&lt;a href=&quot;https://tanstack.com/table/latest&quot;&gt;TanStack Table&lt;/a&gt;&quot;）背后的团队，该团队最近发布了&lt;a href=&quot;https://tanstack.com/ai/latest&quot;&gt;TanStack AI&lt;/a&gt;&quot;的alpha版本。这是一个与框架无关的AI工具包，旨在消除供应商锁定，让开发者完全掌控自己的AI技术栈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI引入了跨多个AI供应商的统一接口、多语言服务器支持以及开放式协议架构。该alpha版本提供了对JavaScript/TypeScript、React和Solid的支持，并内置了&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.anthropic.com/&quot;&gt;Anthropic&lt;/a&gt;&quot;、&lt;a href=&quot;https://gemini.google.com/&quot;&gt;Gemini&lt;/a&gt;&quot;和&lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt;&quot;的适配器。此次发布代表了一种全新的AI工具理念，即将自身定位为中立于供应商的基础设施，而非平台服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI的突出特性之一就是其同构（isomorphic）工具系统，允许开发者通过toolDefinition()一次性地定义工具，并通过.server()或.client()方法提供特定环境的实现。这种架构在整个应用中提供类型安全性，同时支持工具在服务器和客户端上下文中执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具模式有两种定义方式：推荐使用&lt;a href=&quot;https://zod.dev/&quot;&gt;Zod&lt;/a&gt;&quot;&amp;nbsp;Schemas，或者使用&lt;a href=&quot;https://json-schema.org/&quot;&gt;JSON Schema&lt;/a&gt;&quot;（适用于已有JSON Schema定义的项目）。该工具包还提供了模型粒度的类型安全性，使开发者能够针对每个模型获得完整的、针对特定供应商选项的类型提示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;客户端库包括原生JavaScript、React和Solid，未来还将支持更多框架。alpha版本还附带了同构的开发工具，可洞察大语言模型（LLM）在服务器端和客户端的行为，使开发者能使用熟悉的模式调试AI工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在开发者社区中获得了积极反响。开发者Stanley Ulili在Better Stack的一篇&lt;a href=&quot;https://betterstack.com/community/guides/ai/tanstack-ai/&quot;&gt;详细指南&lt;/a&gt;&quot;中这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;虽然仍处于alpha阶段，但是它已经展现出了巨大的潜力。它注重清晰的架构、强大的TypeScript支持，并强调融入现有技术栈的自由，而非强制绑定特定框架或供应商。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1peowss/tanstack_ai_alpha_your_ai_your_way/&quot;&gt;Reddit上&lt;/a&gt;&quot;，一些评论者对SDK的使用场景以及这个新库试图解决的问题提出了疑问，这促使TanStack生态系统的创始人Tanner Linsley作出了回应：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;最近，我和TanStack的所有其他维护者都在深入探索AI，我们发现Vercel的解决方案仍有足够的改进空间，因此决定自己打造一个更贴近我们&lt;a href=&quot;https://tanstack.com/tenets&quot;&gt;产品原则&lt;/a&gt;&quot;的方案。&amp;nbsp;到目前为止，这带来了更好的类型安全性、更优的同构模式，坦白说，这也能够让我们自由地朝着自己想要的方向发展，而不必受制于其他团队。&amp;nbsp;竞争是好事，它能推动整体进步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI将自己定位为Vercel AI SDK的直接替代品，后者目前是JavaScript AI工具领域的主导者。与Vercel的做法不同，TanStack AI作为纯粹的开源基础设施，不包含服务层、不收取平台费用，也不存在供应商锁定。团队强调，开发者直接连接到自己选择的AI提供商，无需通过中间商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于这是新库的alpha版本，因此不存在从早期版本迁移的路径。开发者可通过npm安装核心包并开始使用：npm install @tanstack/ai @tanstack/ai-react @tanstack/ai-openai。&lt;a href=&quot;https://tanstack.com/ai/latest/docs/getting-started/quick-start&quot;&gt;快速入门指南&lt;/a&gt;&quot;提供了创建聊天应用的分步说明，而&lt;a href=&quot;https://tanstack.com/ai/latest/docs/guides/tools&quot;&gt;工具指南&lt;/a&gt;&quot;则深入讲解了同构的工具系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI是由TanStack团队开发和维护的开源项目。它延续了该团队在构建框架无关的开发者工具方面的良好声誉，目标是提供真正开放的工具，兼容任何技术栈，而非将开发者捆绑进专有的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/tanstack-ai-sdk/&quot;&gt;TanStack Releases Framework Agnostic AI Toolkit&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</link><guid isPermaLink="false">https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</guid><pubDate>Tue, 13 Jan 2026 02:40:29 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category><category>AI&amp;大模型</category></item><item><title>Anthropic深夜放出王炸！白领饭碗要被AI砸了？网友：不支持Linux，差评</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;在开发者工具 Claude Code 推出之后，Anthropic 团队很快意识到一个出乎预料的现象：开发者并没有把它局限在“写代码”这件事上。相反，Claude Code 被迅速用于整理资料、撰写文档、生成报告、分析数据，甚至承担起类似“数字同事”的角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种使用方式的外溢，最终促使 Anthropic 做出一个更激进的产品判断——如果大模型已经被当作工作伙伴使用，那么是否应该为“所有人”，而不仅仅是开发者，提供一种真正面向日常工作的智能协作形态？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是今天，Anthropic 正式推出了 Cowork。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/2798224b81184116211242f420b586d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 工程师、Claude Code&amp;nbsp;创建者&amp;nbsp;Boris Cherny 在 X 上发帖宣布了该消息。他写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;自 Claude Code 发布以来，我们发现用户将其用于各种非编码工作：例如进行度假研究、制作幻灯片、清理电子邮件、取消订阅、从硬盘恢复婚礼照片、监测植物生长、控制烤箱等等。这些应用场景丰富多样，令人惊喜——原因在于底层 Claude Agent 是最佳代理，而 Opus 4.5 是最佳模型。今天，我们非常激动地推出 Cowork，这是我们让 Claude Code 服务于所有非编码工作的第一步。该产品目前仍处于早期阶段，功能尚不完善，与 Claude Code 最初发布时的状态类似。Cowork 包含许多我们认为使其真正与众不同的创新用户体验和安全功能：内置虚拟机用于隔离、开箱即用的浏览器自动化支持、以及对所有非编码工作的支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa95ea01942eb8bff88234f7195c111d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Cowork 是一款基于 Claude Code 底层架构构建的全新产品，目前以“研究预览版”的形式，率先面向 macOS 平台上的 Claude Max 订阅用户开放。与传统对话式 AI 不同，Cowork 的核心定位并非“聊天”，而是“协作”：它试图让 Claude 从一个被动响应指令的助手，转变为能够理解任务、制定计划、持续执行，并与用户保持协同关系的智能工作体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从“对话助手”到“数字同事”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长期以来，大模型产品的主流交互形态仍然是对话。用户输入问题，模型生成回答；用户提出修改，模型再次响应。这种模式在信息查询、文本生成等场景下行之有效，但在真实工作流中却暴露出明显局限——上下文需要反复提供，文件需要人工整理，输出结果往往还要用户自行转换为可用格式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 试图解决的，正是这一断裂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 模式下，用户可以直接授予 Claude 对本地指定文件夹的访问权限。需要强调的是，这种访问并非“全盘授权”，而是由用户明确选择、逐一控制的结果。Claude 只能看到、读取、编辑或创建那些被允许的文件和目录，而无法触及任何未授权内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦获得权限，Claude 的能力边界就发生了质变。它不再只是基于文本上下文“想象”文件内容，而是可以直接操作真实存在的工作材料。例如，它可以扫描一个杂乱无章的下载文件夹，按照文件类型、时间或用途进行分类和重命名；可以从大量截图中提取关键信息，自动生成一份结构化的费用清单；也可以将零散的会议笔记、草稿和片段，整理成一份逻辑清晰的报告初稿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种能力的本质，并不是简单的“更聪明”，而是 Claude 被嵌入进了用户的实际工作环境之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在产品说明中多次强调，Cowork 的体验更接近“给同事布置任务”，而不是与机器人来回对话。一旦任务被下达，Claude 会自行拆解步骤、规划执行路径，并在执行过程中持续向用户同步进展。用户无需等待任务完成即可插入新的反馈或补充想法，这些指令会被自动排队、并行处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是 Cowork 与普通对话模式最根本的差异之一：它默认假设用户的工作是多线程的，而不是线性的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，“更自主”的能力，意味着更高的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让 AI 进入文件系统，甚至具备修改、创建和删除文件的能力，无疑是一种能力跃迁，同时也是风险跃迁。Anthropic 并未回避这一点，反而在产品介绍中反复提醒用户保持警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是操作层面的风险。如果收到明确指令，Claude 确实可以执行具有破坏性的操作，例如删除本地文件或批量修改内容。一旦指令本身存在歧义，或者模型误解了用户意图，后果可能是不可逆的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在 Cowork 中，Claude 在执行任何“重要操作”之前，都会主动征求用户确认。这种设计并非形式上的“弹窗提示”，而是希望用户在关键节点重新审视任务目标，必要时进行纠正或细化指令。Anthropic 也明确建议，在涉及高风险操作时，用户应提供尽可能清晰、具体的指示，而不是依赖模糊的自然语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一类更复杂、也更具行业共性的风险，是“提示注入”（Prompt Injection）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 的工作过程中，Claude 可能会接触来自互联网的内容，例如网页、文档或第三方信息源。如果这些内容中被恶意嵌入了指令，试图诱导模型偏离原本的任务计划，就可能引发安全问题。Anthropic 表示，他们已经构建了针对提示注入的多层防御机制，但也坦言，“代理安全”——即确保 AI 在现实世界中执行操作时的可控性——仍然是整个行业正在积极探索的前沿问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，Cowork 并不是一个“已经完全成熟”的产品，而更像是一次对未来工作方式的现实实验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 也明确指出，这些风险并非 Cowork 独有，而是所有具备“行动能力”的 AI 工具都会面临的问题。只是对许多用户来说，Cowork 可能是第一次接触到一个超越简单对话、真正能够影响本地环境的 AI，因此更需要建立正确的使用习惯和风险意识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研究预览版背后的产品逻辑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 目前被定义为“研究预览版”，这一定位本身就释放了明确信号：Anthropic 并不认为自己已经找到了最终形态，而是希望通过真实用户的使用反馈，加速产品迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据官方披露，Anthropic 计划在后续版本中引入多项重要改进。其中包括跨设备同步能力，使 Cowork 不再局限于单一终端；以及将其移植到 Windows 平台，从而覆盖更广泛的办公人群。同时，安全机制也将持续强化，尤其是在代理行为可解释性和可控性方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从产品路径上看，Cowork 与 Claude Code 之间存在清晰的继承关系。两者共享相同的底层架构，这意味着 Cowork 在能力上，理论上可以完成 Claude Code 已经证明可行的许多复杂任务。不同之处在于，Cowork 将这些能力重新封装为更偏向非技术用户的交互方式，降低了使用门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 Claude Code 面向的是“愿意为效率付出学习成本”的开发者群体，那么 Cowork 的目标人群显然更加广泛：内容创作者、产品经理、运营人员、行政人员，乃至任何需要与文件、资料和信息打交道的知识工作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在掌握 Cowork 的基本使用方式后，用户还可以进一步扩展 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是连接器。Claude 可以通过用户已有的连接器，访问外部信息源，从而将本地任务与外部数据打通。这使得 Cowork 不再只是一个“本地整理工具”，而是可以承担跨系统的信息整合角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次是新增的一系列技能。这些技能专门用于提升 Claude 在创建文档、演示文稿以及其他常见办公文件时的表现，使其输出更加贴近真实工作场景的格式和标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，如果用户在 Chrome 浏览器中将 Cowork 与 Claude 配对使用，Claude 还可以完成需要访问浏览器的任务。这一步，实际上进一步模糊了“对话 AI”“自动化工具”和“数字员工”之间的界限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从整体设计来看，Cowork 试图减少用户在“提供上下文”和“整理结果”上的认知负担。用户无需手动拼接背景信息，也无需将 Claude 的输出再加工成可用成果。更重要的是，用户不必为了等待 AI 完成某个任务而中断自己的工作节奏——任务可以被连续布置、并行执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在描述这种体验时，用了一个耐人寻味的比喻：这更像是给同事留言，而不是来回沟通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用户：没有Linux版本，差评！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 发布之后，迅速在开发者社区、AI 产品圈以及更广泛的知识工作者群体中引发讨论。与以往单纯围绕模型能力、跑分或价格的争论不同，这一次的焦点明显转向了一个更现实的问题：“AI 是否真的开始成为一个可以被信任、被授权的工作参与者？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上的最新讨论串里，有用户评论指出他们“很期待尝试这个功能”，认为 Anthropic 近来在产品和用户信任构建上做得不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/842f272a3e6e92967e9aabc5c537d87e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;**因为仅限 macOS 和订阅计划，部分用户感到遗憾。**在另一个 Reddit 讨论串中，有用户对 Cowork 的平台限制表达了不满或遗憾，评论集中在“只支持 macOS”这一点上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/8060c0e9f39720be6dc7c63fd4acf667.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，值得注意的是，有些评论虽然不是专门针对 Cowork，但有一些用户还是对 Anthropic 近期产品策略与沟通的不满，对 Cowork 的发布背景和用户关系具有间接关联语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 平台，有长期用户表示，自己已经从忠实支持者变成对 Anthropic 的信任下降甚至不满。该用户指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“作为很早一批用户，我原本极力推荐 Claude，但最近几个月感觉 Anthropic 的产品质量沟通都变差了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://claude.com/blog/cowork-research-preview&quot;&gt;https://claude.com/blog/cowork-research-preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/bcherny/status/2010809450844831752&quot;&gt;https://x.com/bcherny/status/2010809450844831752&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</link><guid isPermaLink="false">https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</guid><pubDate>Tue, 13 Jan 2026 01:30:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>活久见！连Linux之父等“顽固派”大佬，都在用AI编程了</title><description>&lt;p&gt;程序员中的超级“保守派”、Linux 之父Linus Torvalds，现在也用起了 AI 编程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b675a5ab379b540e9e7b3a6cb345baa0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图源：GitHub&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，Linus 在 GitHub 上悄悄上传了一个小项目。项目本身不大，但特别的是，它是他用一款谷歌系 AI 编程助手&amp;nbsp;进行&amp;nbsp;Vibe Coding&amp;nbsp;完成的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个仓库很快就被眼尖的网友挖了出来，目前已经收获了&amp;nbsp;1600+ 颗 Star。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8afd55b114c702572f76a00a2f8f99d7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 缔造的 Linux，与 Windows、macOS 一起，构成了当今计算世界的三大通用操作系统阵营之一。&lt;/p&gt;&lt;p&gt;不过他曾直言：“在过去将近 20 年里，我并没有从事编程工作。”这并不是他远离技术，而是早就从亲手写代码的人，转变成了为整个系统长期演进负责的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种角色下，这位老哥过去对“AI 帮你写代码”这套叙事，一直保持高度警惕甚至是嗤之以鼻——他关注重点的不是代码写得快不快，而是代码在多年之后是否还能被理解、维护和演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，Linus 对 AI 编程的态度可谓是“大转弯”：不仅开始亲自尝试 Vibe Coding，还公开表示自己对这种方式“相当积极”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些事情的冲击力并不在于“AI 又进步了”，而在于连最不吃 AI 编程这一套的人，也开始松动了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;反 AI 编程的“顽固派”们，也开始接受 Vibe Coding 了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生成式 AI 席卷软件行业的当下，有这么一群特殊的 “顽固派”， 他们定义了现代计算机的技术基石，却曾长期对 AI 编程嗤之以鼻，甚至公开泼冷水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 Linux 之父 Linus Torvalds、Java 之父 James Gosling、Redis 之父 antirez（Salvatore Sanfilippo），个个都是编程界的殿堂级人物。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但有意思的是，随着 AI 工具能力的突飞猛进，这群昔日的 “反 AI 先锋”，正以各自的方式重新划定 AI 的边界：有人有限度拥抱，有人批判中认可，还有人干脆彻底转身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Linus 老哥，之前对生成式 AI 一直保持观望的态度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并不否认 AI 的潜力，但极度厌恶围绕 AI 的过度炒作。在一次开源峰会上，他直言当前关于生成式 AI 的讨论“90% 是行销炒作，只有 10% 是现实”，并毫不掩饰自己的反感。正因为讨厌炒作，他选择在相当长一段时间内&amp;nbsp;主动忽略 AI 热潮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 之前一直没有使用各种 AI 编程工具。不过，这并不代表他对新范式抱有敌意。相反，他对&amp;nbsp;Vibe Coding 总体持正面态度，只是并未急于亲自下场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，随着工具逐渐成熟、噪音开始下降，Linus 也终于对 Vibe Coding 上手了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他用上了谷歌的智能体优先开发平台 Antigravity，靠 Vibe Coding 搞定了项目里的 Python 音频采样可视化工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从最初的 “搜索 + 照猫画虎”，到后来直接让 AI 写代码，甚至自定义组件，最终效果比他手写的还要好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对内核社区里 AI 生成补丁泛滥的争议，他的立场很清醒：问题不在于 AI 本身，而在于维护者是否真正理解代码、承担责任。在他眼里，AI 可以当帮手，但不能当甩手掌柜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而&amp;nbsp;Redis 创始人 Salvatore Sanfilippo（网名：antirez）&amp;nbsp;的转变更具戏剧性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位以 “简洁、可预测” 为信仰的系统级程序员，曾固执地坚持一行行手写代码，对自动化工具保持高度警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最近，他公开抛出了一句颠覆自己过往理念的话：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“对于大多数项目而言，除非是为了娱乐，现在自己写代码已经不再明智了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/35/359c6853cabb8f0a7b9e22d03877bc92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让他改口的，是实打实的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用 Claude Code 的过程中，他发现 AI 在极少人工干预的情况下，就能完成原本需要数周的系统级任务：修复 Redis 测试中的并发与时序问题、重写核心库、复现复杂的数据结构改动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更夸张的是，他只提出需求，Claude Code 5 分钟就生成了一个 700 行的纯 C 库，用于 BERT 类嵌入模型推理，性能仅比 PyTorch 慢约 15%；而他耗时数周完成的 Redis Streams 内部改动，AI 根据设计文档，20 分钟便复刻完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他坦言，对抗浪潮没什么意义，不如主动拥抱：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“忽略人工智能对你或你的职业生涯都没有好处。花几周时间仔细研究，而不是五分钟浅尝辄止。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 antirez 强调，这不是编程乐趣的终结，而是转移：“真正有趣的事情，已经从‘如何写代码’，变成了‘要做什么、为什么这样做’。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这位技术极客也没丢掉警惕性。他担忧 AI 技术的集中化风险。少数公司掌握核心能力，可能引发程序员失业、技术权力失衡等问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比前两位，Java 之父 James Gosling&amp;nbsp;的态度要尖锐得多。他多次炮轰，当前的 AI 热潮 “基本上是一场骗局”，AI 已经沦为“自带误导属性的营销术语”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，生成式 AI 编程的本质，不过是对已有代码和模式的重组，根本谈不上真正的创造力。那些看起来惊艳的演示，一旦碰上复杂项目就露馅：“刚开始接触氛围编程，会觉得它特别酷炫。可一旦项目变得稍微复杂一点，氛围编程就会很快耗尽开发者的脑力。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gosling 的核心质疑点很明确：AI 只能复刻见过的代码，但专业软件开发的精髓，在于开拓性的创新 —— 这些内容从来不在现成的代码库里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也没把话说死。他承认 AI 技术背后的数学与统计原理很复杂，也认可它的实用价值，不是取代程序员，而是 “生成没人愿意去写的文档”，或者解释现有代码的功能。说到底，AI 更像一个智能搜索引擎，而非编程大神。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还不忘吐槽一把资本：“科技行业里骗子和炒作者的数量之多，令人难以置信。风险投资者只关心成功获利，而不是开发出真正有用的技术。” 他甚至预言，“绝大多数 AI 投资都会被烧个精光。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说到底，这三位大佬的转变，都不是向 AI “投降”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们认可的，是 AI 在重复劳动上的效率；他们坚守的，是人类程序员不可替代的核心价值，对复杂系统的理解、对工程架构的判断、对长期维护的责任，以及开拓性的创新能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;对 Linux 内核开发，Vibe Coding 还欠火候&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要说明的是，虽然 Linus 现在对 Vibe Coding 的态度很积极，但他也直言称，这种方式&amp;nbsp;并不适用于 Linux 内核开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个重要原因在于，今天的计算机系统早已比他学习编程的年代复杂得多。Linus 曾回忆，当年他接触的一些输入程序，甚至是从计算机杂志上照着敲下来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然他已经很久没有深度参与具体功能编程，长期为整个内核的演进负责。在他的“系统维护者”视角下，稳定性、安全性和可维护性，远比“写得快不快”更重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一点，其实在他最近上传到 GitHub 的那个项目里有所体现：AI 主要写的只是对 Python 可视化工具部分，核心 C 语言部分（音频效果的数字信号处理等）还是他亲自写的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Linus 看来，Vibe Coding 在小项目和探索性场景中确实优势明显：进入门槛低、反馈速度快，能迅速把模糊的想法变成可运行的程序，用来生成样板代码、辅助脚本，或者“先跑起来看看”，都非常合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这种方式的短板同样明显——生成代码往往风格不稳定、抽象边界模糊、依赖隐性假设，短期能用，长期却很难维护。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 Linux 内核，恰恰是一个对“可维护性”极端苛刻的系统：代码需要被不同年代、不同背景的维护者反复阅读、修改和重构，任何一次“看起来省事”的生成式决策，都可能变成未来十年的技术债。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过话说回来，即便不能“全靠 AI 写代码”，“部分交给 AI”本身，就已经在重塑程序员的工作方式。&lt;/p&gt;&lt;p&gt;在另一条时间线上，有些工程师甚至已经开始用 AI 来开发 AI 本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Boris Cherny。作为 Anthropic 工程师、也是&amp;nbsp;Claude Code&amp;nbsp;的创造者，他已经几乎不再以传统方式写代码了，而是把自己打造的 AI 编程工具玩儿出了花：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他让 Claude Code 自己参与开发自己，然后竟在一年内完成了 1096 提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7fda4d0dceb79c4ffd024e6d5db386b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个工具已成为全球最受欢迎的 AI 编程工具之一，去年还给 Boris 带来了超过&amp;nbsp;10 亿美元（约合人民币 70 亿元）&amp;nbsp;的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://github.com/torvalds/AudioNoise&lt;/p&gt;&lt;p&gt;https://www.theregister.com/2025/11/18/linus\_torvalds\_vibe\_coding/&lt;/p&gt;&lt;p&gt;https://antirez.com/news/158&lt;/p&gt;&lt;p&gt;https://www.bnext.com.tw/article/81200/linus-torvalds-gen-ai-bubble&lt;/p&gt;&lt;p&gt;https://x.com/bcherny/status/2009072293826453669&lt;/p&gt;</description><link>https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</link><guid isPermaLink="false">https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</guid><pubDate>Tue, 13 Jan 2026 01:16:08 GMT</pubDate><author>木子,高允毅</author><category>生成式 AI</category></item><item><title>刚刚，DeepSeek 突发梁文峰署名新论文：V4 新架构提前曝光？</title><description>&lt;p&gt;今天凌晨，喜欢闷声做大事的 DeepSeek 再次发布重大技术成果，在其 GitHub 官方仓库开源了新论文与模块 Engram，论文题为 “Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models”，梁文锋再次出现在合著者名单中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1e/1edd54354aa449ddce92b7a3365002b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统的大模型架构相比，该方法提出了一种新的“查—算分离”机制，通过引入可扩展的查找记忆结构，在等参数、等算力条件下显著提升模型在知识调用、推理、代码、数学等任务上的表现。代码与论文全文均已开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram&quot;&gt;https://github.com/deepseek-ai/Engram&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种查和算分离的Engram新方法的整体架构如下图所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba205b29dd447e4968a93381616947d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么需要 Engram？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，我们为什么需要 Engram ？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前主流的大语言模型架构依然基于 Transformer 和 Mixture-of-Experts（MoE）结构。MoE 是目前推进参数规模和能力扩展的关键技术之一，通过动态路由机制，只激活部分参数以降低计算成本，同时在任务容量方面实现大规模扩展。DeepSeek 自家系列模型（如 DeepSeek V2、DeepSeek V3 等）也采用了先进的 MoE 方法进行扩展训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这些传统的 Transformer 架构（无论是 Dense 还是 MoE）中，模型的参数实际上承担着两种截然不同的角色：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实性记忆（Memorization）： 存储海量的知识事实。例如，“法国的首都是哪里？”、“世界最高的山脉是哪座”等。这类信息相对死板，更多依赖于“查表”式的检索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;逻辑推理与计算（Calculation）： 负责复杂的逻辑链条、多步推理和情境理解。例如，“根据这段代码的逻辑推导可能的 Bug”、“解析一段复杂的哲学论证”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前的大语言模型倾向于将这两者混在一起。当你试图让模型记住更多知识时，你不得不增加参数量。而在传统的 Dense 模型中，参数量增加意味着前向传播时的计算量（FLOPs）也会同步激增。MoE 架构虽然通过稀疏激活解决了“算力随参数同步爆炸”的问题，但 DeepSeek 研究发现，MoE 专家在处理“死记硬背”的任务时依然不够高效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;神经网络本质上是连续的数学变换，用高昂的矩阵运算去模拟简单的“查表检索”，本身就是一种极大的浪费。DeepSeek 的 Engram 正是为了打破这一困境——“该查表的查表，该算的算”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Engram 的核心思想与架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;聚焦到问题本身，Engram 方法为什么能解决上述问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Engram”一词源自神经科学，意为“记忆痕迹”，它是一个可扩展、可查找的记忆模块，用于语言模型在推理过程中过去可能已经见过的模式或片段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 的核心技术之一是现代化的哈希 N-Gram 嵌入（Modernized Hashed N-gram Embeddings）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统方式： 模型通过多层自注意力（Self-Attention）和 MLP 层的非线性变换，反复提取输入文本中的特征。Engram 方式： 它对输入的 Token 序列进行 N-Gram（连续 N 个词）切片，并利用哈希算法将这些片段映射到一个巨大的、可学习的查找表（Lookup Table）中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于采用哈希索引，这种查找是确定性且 O(1) 时间复杂度的。这意味着无论模型存储了多少万亿个记忆片段，检索的速度几乎是恒定的，且算力消耗极低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;O (1) 的含义是： 一次查找的耗时是常数级的，与 N-gram 表的规模无关。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也就是说，这种设计本质上将一部分“记忆职责”从深度神经计算中卸载出来（例如序列模式、固定知识段的识别与回填），使得模型既拥有活跃神经通道（例如 Transformer + MoE）处理复杂计算，也有静态记忆通道高效处理固定模式，这就是所谓的 “稀疏性的新轴”（a new axis of sparsity）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说就是 MoE 负责：“计算密集”神经推理与复杂组合功能、Engram 负责：“记忆查找”固定模式以及模式重建，两者协同构成一个更高效的整体架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它还具备条件记忆（Conditional Memory）。与简单的静态查找表不同，Engram 是“条件化”的。它会根据当前上下文的隐向量（Hidden States）来决定提取哪些记忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在架构设计上，Engram 模块位于 Transformer 层的早期阶段。它负责“模式重构（Pattern Reconstruction）”，即在计算层（MoE 或 Dense）开始干活之前，先把相关的背景事实和历史模式检索出来，作为“素材”喂给后续的逻辑层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它与 MoE（Mixture of Experts）的关系是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文特别指出：Engram 提供了一个新的稀疏性轴，与 MoE 的条件计算不同，它通过条件查找提供静态记忆容量。下面图表中从目标、计算方式、优化方向和作用位置四个维度解释了Engram 和 MoE的区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，DeepSeek 将 Engram 与 MoE 结合，形成了一个双系统：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 模块： 负责海量知识点的“存储与快速检索”。MoE 专家： 摆脱了沉重的记忆负担，全身心投入到“逻辑推理与合成”中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种分工极大地优化了参数效率。在 27B 的实验模型中，Engram 模块可以占用大量的参数用于记忆，但在实际推理时，它只消耗极少的计算量（FLOPs）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/258f97af1beff0b2daba80ee30e0a5e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：V4 将采用这种架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit、X和其他平台的相关帖子中，Engram 的技术核心受到了不少用户的肯定和技术肯定。众多网友认为这个模块的特点在于让模型架构处理“记忆模式查找”和“神经计算推理”两块职责分离，从而开启了新的稀疏性方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 平台有用户评论说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&amp;nbsp;“Engram嵌入方法很有意思。大多数模型仅通过MoE进行扩展，但Engram增加了静态记忆作为补充的稀疏性轴，查找复杂度为O(1)。他们发现 MoE 和 Engram 之间存在 U 形缩放规律，这指导着如何在两者之间分配容量。分析表明，这减轻了早期层级静态模式重建的压力，从而保留了用于复杂推理的深度。确定性寻址意味着它们可以将嵌入表卸载到主机内存中，而不会增加太多推理开销。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0f96995f96abcc3dd5d2e6ca94b84029.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，有用户对这种基于 n-gram lookup 的机制表达了直观兴趣，他评论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“即便是在不依赖 GPU 的环境下也能实现这种 O(1) 查找方式，让不少开发者对本地部署这样的大模型功能有了更实际的期待。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b75e18cfe6ca43ea711351ec52468e1f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在部分技术性评论中，有人指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;即从已有技术逻辑来看，在 LLM 中加入静态记忆查找似乎是“顺理成章”的发展方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这类观点反映了一个重要观点：专家群体开始从纯参数扩张思维转向更“智能”的架构设计，包括查表式模块和神经网络的协同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不少高级开发者在讨论中进一步提到，这种设计在理念上类似于对传统 NLP 技术（如 n-gram embedding）的现代化转换，结合了高效寻址机制（deterministic addressing）和神经推理模块，这种组合在纸面上看具有较高的可行性和实用性（这一点正是 Engram 的核心贡献）。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一条社区评论指出，Engram 很可能是 DeepSeek 即将发布的 V4 模型的核心技术基础：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;业内观察者认为 Engram 模块可能会成为 DeepSeek V4 的重要组成部分，并预示 DeepSeek 下一代模型会在记忆和推理协同上实现架构级提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X平台，也有网友表达了同样的猜测，认为V4 也将采用这种架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6ba93d5100cbacfee2b85d806c7ebc87.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友调侃，原本想抄袭下谷歌的技术，但现在要抄袭DeepSeek了，因为它比谷歌更好！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b09f2dec8c0db4892ed0b417422d8754.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友表示，其实Meta之前也有过类似想法，但用到的技术不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9b474fc250b9518bd87e5af00a84076e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/scaling01/status/2010748516788777445&quot;&gt;https://x.com/scaling01/status/2010748516788777445&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</link><guid isPermaLink="false">https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</guid><pubDate>Tue, 13 Jan 2026 00:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>阿里云把“华强北”们推向CES</title><description>&lt;p&gt;今年的&amp;nbsp;CES，中国硬件又一次成为主角。活跃在拉斯维加斯展台上的诸多出海产品，背后依托的是深圳的研发效率与供应链能力，而其智能化核心，则越来越多建立在以&amp;nbsp;Qwen&amp;nbsp;为代表的多模态、全尺寸的大模型基础上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与沙漠赌城的&amp;nbsp;CES&amp;nbsp;同期，在深圳蛇口，阿里云也举办了一场智能硬件展。这场展会面向公众免费开放，选址于本地居民日常散步、观海和看展的滨海文化地标，却意外成为&amp;nbsp;AI&amp;nbsp;硬件从实验室走向真实市场的缩影。1000&amp;nbsp;余款智能硬件在这里集中亮相，其中超过&amp;nbsp;200&amp;nbsp;款与&amp;nbsp;CES&amp;nbsp;同款甚至首发。这里既有来自北京、杭州的创新团队，也有来自义乌、华强北等产业带的制造与渠道力量——他们对技术趋势的嗅觉，向来快过任何市场报告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术验证与市场反馈在同一空间同时发生。在这里你可以听到合作方直接询价“多少钱，做OEM吗，能做多少套”，也可以看到消费者直接下单，把&amp;nbsp;399&amp;nbsp;元的&amp;nbsp;AI&amp;nbsp;玩具带回家。许多普通家庭第一次在这里集中体验到能对话的毛绒玩具、教用户跳舞的镜子、能翻跟头的机器狗，和具备实时提醒能力的&amp;nbsp;AI&amp;nbsp;眼镜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在&amp;nbsp;2024&amp;nbsp;年云栖大会上，阿里云董事长吴泳铭就明确指出，未来&amp;nbsp;AI&amp;nbsp;最大的想象力会来自于物理世界：“我们不能只停留在移动互联网时代去看未来，深层次AI最大的想象力绝对不是在手机屏幕上做一两个超级APP，而是接管数字世界，改变物理世界。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在这轮&amp;nbsp;AI&amp;nbsp;硬件浪潮中，阿里云没有选择去做终端硬件的制造者，而是以软硬一体的融合理念，向产业提供底层模型能力、云基础设施与生态支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据显示，通义大模型的多模态能力已深度赋能超过&amp;nbsp;15&amp;nbsp;万家智能硬件厂商。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d28fc8e8d4409f5e48b4f7b2dea413ff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从雷鸟的&amp;nbsp;AI&amp;nbsp;眼镜、听力熊的儿童&amp;nbsp;AI&amp;nbsp;Pin，到优必选机器人、趣丸科技的生成式&amp;nbsp;AI&amp;nbsp;吉他，这些走进全球家庭的产品背后，都能看到以通义为代表的阿里云基础设施的支撑。而它们从概念到量产、从深圳到世界的惊人速度，也再次印证了深圳这座“硬件硅谷”在研发、供应链与商业化效率上的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;For&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的&amp;nbsp;AI&amp;nbsp;硬件&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;逛完阿里云通义智能硬件展，一个强烈的感受是，这是我经历过为数不多，能让普通人玩得开心、让创业者看到机会、让厂商验证商业模式，同时清晰传递主办方战略意图的展会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云租下深圳海上世界文化艺术中心三层空间，用一种近乎“生活化”的方式，向公众展示：AI能长在玩具里、眼镜上、健身镜中，甚至成为家庭一员的日常存在。向企业展示：你能快速依托阿里云的生态，快速做出能进入全球家庭的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展会围绕两条主线展开：一是呈现阿里云的底层能力，二是展示其赋能下的千款智能硬件成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一楼以“智能中枢”为核心，展示通义大模型的能力：观众上传一张照片，就能生成一段短视频；走过一段互动迷宫，便能直观感受多模态AI如何理解图像、语音和动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能中枢周围环绕着“创造有AI”“生活有AI”“AI实训营”等主题区，OPPO、理想、影石等品牌在此展示手机、智能座舱和AI影像设备，而像趣丸科技的AI吉他、Looki这样的新奇产品，则让人看到AI如何重塑音乐、娱乐等日常互动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/27e1fe905091def2e73df36bc3297948.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;趣丸科技与阿里云合作推出的全球首款生成式AI吉他TemPolor&amp;nbsp;Melo-D，在通义大模型的支持下，重新定义了人与音乐的交互方式，提供了个性化的AI音乐创作体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三楼聚焦陪伴、健康与安防，专设义乌厂商展区；四楼覆盖家居、教育、健身等提效场景，华强北的硬件老板们也把“一米柜台”搬到了现场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea7a53958edc4cb3f43237c85e9cedcd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;通义联合听力熊为青少年定制随身AI对话智能体，打造国内首款儿童&amp;nbsp;“AI&amp;nbsp;Pin”&amp;nbsp;Mooni&amp;nbsp;M1，提供多种角色选择。经过通义千问大模型加持，用户的&amp;nbsp;AI&amp;nbsp;使用时长提升40分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云想让大家知道，AI有能力在所有场景里带来更好的体验。它同时也呈现出一种可能——不管是软件应用还是硬件产品，每个人都可以在这个时代搭建些什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c18195f4e17f32ac1f9330d8a1ccb08f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;阿里云&amp;nbsp;AI&amp;nbsp;实训营的 Agent 硬件搭建小课堂&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于普通人来说，硬件展是一个游戏体验。孩子和AI毛绒玩具对话，年轻人跟着镜子学舞，有人让AI解读运势、推荐香水，还有中学生在阿里云&amp;nbsp;AI&amp;nbsp;实训营中搭建了自己的第一个交互硬件。我们这代人仍然处于有“AI硬件”概念的时期，而对于下一代人来说，可能已经不存在“AI硬件”。当生活总所有一切都有AI，AI之于人，阿里云之于硬件和应用产品，就是水之于人的存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对创业者和企业主而言，展会成了高效的信息源。有用户的直接提问和反馈，也有工程师在展位前递上简历。采购顾问带着非洲、拉美的客户穿梭其间，现场询价、谈订单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e4/e4fd0f58e096c36684adf9311b2e1962.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL、影石、安克创新的案例，更是为想要入局AI硬件和出海的企业打气——依托阿里云全球全栈AI基础设施，大型制造企业可实现研发、服务、出海一体化，新锐品牌也能快速站稳全球舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚在CES获得Best&amp;nbsp;of&amp;nbsp;Innovation奖项的影石，依托&amp;nbsp;Qwen-VL&amp;nbsp;实现视频与图片的分类打标和场景识别，结合&amp;nbsp;Qwen-Plus&amp;nbsp;生成剪辑脚本，赋能全球百万视频创作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60f1d931f1cedfe5b371d8454ec23495.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;安克创新依托阿里云“全球一张网”，实现跨境资源调度与合规部署，核心系统互访提速30%，并将&amp;nbsp;Qwen&amp;nbsp;与&amp;nbsp;Wan&amp;nbsp;深度融入语音助手、多模态交互等产品功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa1a742f457dce5707bd11d956d5ace3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL则基于通义大模型打造了半导体显示专家系统&amp;nbsp;X-Intelligence，支撑其全球研发体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，阿里云把义乌、华强市场这些产品背后的“制造和分发网络”呈现在大家面前。在他们的摊位上，你可以看到很多产品尽管“粗糙”，却仍然有市场。在很多欠发达国家，AI硬件需要的不是精致，而是先以成本最低的方法被用上。很多义乌玩具、小3C产品的批发商，嗅到AI风潮后，已经在深圳有了自己的硬件工厂。华强科技生态园等孵化器，也开始重点招募AI硬件的创业公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如阿里云智能集团通义大模型业务总经理徐栋所说：“这样一个平台（以通义多模态交互开发套件为代表的AI硬件赋能平台）是我们非常重要的业务的选择，我们需要更多贴近阿里云的智能硬件开发伙伴。很多场景是碎片化的，只有做更贴近实际的生产环节、消费环节，每个人对AI硬件的体验才能更深。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，正在成为&amp;nbsp;for&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的日常现实。而阿里云的角色，不是站在台前造产品，而是站在幕后，让创新更快实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阿里云，在&amp;nbsp;AI&amp;nbsp;硬件变革前夜&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件从极客圈层走向大众日常，标志着市场已从“启蒙期”进入“挑剔期”。当用户开始为AI服务付费、并将设备融入日常生活，产品的成败就不再取决于功能数量，而在于能否持续兑现可感知的价值——这要求厂商必须拥有一套覆盖模型、工程、服务与生态的系统性能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，特别是在消费级市场，正经历着一场根本性的转型。从传统的联网设备到如今的“端侧智能体”，AI不再只是硬件的附加功能，而是直接决定产品核心价值的引擎。这一转变的核心标志在于：AI&amp;nbsp;不再作为附加功能嵌入硬件，而是成为产品定义、体验构建与价值交付的底层引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期智能硬件以“连接+控制”为基本范式，其智能化主要体现在远程操作与数据回传；而新一代AI硬件则要求设备具备持续感知、上下文理解、自主决策与协同执行的能力，成为一个能在真实场景中与用户形成闭环互动的“智能体”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一转变正在重塑硬件的设计逻辑、用户的价值预期与厂商的技术路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用对AI硬件的认知早已超越“新奇感”，转而关注端到端体验是否流畅、可靠、有用。更重要的是，用户开始愿意为持续服务付费。例如按月订阅儿童AI陪伴内容，或为高级健身指导功能续费。这催生了“硬件+服务”的新商业模式，但也带来新挑战，如果AI不能提供可感知的显性价值，订阅就难以为继。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术架构也随之重构。端云协同的逻辑发生了变化。之前的端云协同更多指向算力分工，即端上承载不了的算力放在云上，但现在的端云协同是指能力互补。安全、延时、功耗的问题必须在端上解决，而生态打通这些能力可能在云上做。同时，交互方式正走向“无感化”——不是让用户察觉不到AI存在，而是让使用门槛足够低，无需学习就能自然融入原有生活节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，对大多数硬件厂商而言，这场转型并不轻松。模型迭代速度远超硬件研发周期，而一个产品往往需要组合多个模型才能实现完整功能，集成复杂度陡增。与此同时，Agent架构、工具链和工程平台快速演进，传统硬件团队难以跟上软件层的节奏。更棘手的是，许多厂商擅长制造和渠道，却缺乏用户运营、数据闭环和订阅服务能力，难以构建可持续的商业模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对这些系统性挑战，阿里云提供了AI硬件的全链路支持体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在基础设施层面，阿里云面向&amp;nbsp;AI&amp;nbsp;应用场景全面升级计算、存储与网络能力，为高并发、低延迟的智能硬件业务提供稳定底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在模型层面，通义大模型家族（包括&amp;nbsp;Qwen3、Qwen-VL、QwQ&amp;nbsp;等）全面开源，并提供闭源高阶版本，同时接入第三方优质模型，帮助厂商一站式、低成本调用全球先进&amp;nbsp;AI&amp;nbsp;能力。针对多模态交互场景，阿里云还推出专有优化模型，降低端到端语音和视频交互时延。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云的模型能力，已经获得顶尖手机、汽车、具身智能、智能配件品牌的认可和验证：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，全球&amp;nbsp;Top&amp;nbsp;10手机厂商已都在使用阿里云的大模型能力。例如，OPPO利用阿里云人工智能平台&amp;nbsp;PAI&amp;nbsp;对&amp;nbsp;Qwen&amp;nbsp;开源模型进行后训练，以支持其AI多场景应用；荣耀则联合阿里云百炼打造&amp;nbsp;VQA&amp;nbsp;端到端方案，图片细分场景识别率提升近40%，延迟降低30%。荣耀Magic&amp;nbsp;V5&amp;nbsp;接入飞猪旅行、高德地图两个垂直Agent&amp;nbsp;两个月即斩获百万级用户好评。基于“模型+工程+生态”三位一体的战略，阿里云正持续加速手机行业的AI功能创新与规模化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/3f/16/3f3f3ec409b7a511c1dbc3d97b9f5f16.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想汽车基于阿里云MindGPT大模型，整合高德、飞猪、支付宝等生态，实现全球首个“车机AI扫码支付”；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/70/e6/70de20e17376b8709151305c4cbb4fe6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷鸟创新联合阿里云推出行业首个面向智能眼镜的AI大模型，意图识别准确率达98%，搭载该模型的雷鸟眼镜出货量领跑AR行业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/fd/10/fdb4be397f9d2375b7950bb10c61ec10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优必选的萌&amp;nbsp;UU&amp;nbsp;陪伴机器人，搭载通义千问与自研情感智能体“点灵”，且具有长期记忆&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90b730812c7aeda3120052d8ccd77d00.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得注意的是，阿里云此次还推出了全模态智能交互开发套件，将上述能力封装为标准化工具。该套件适配&amp;nbsp;30&amp;nbsp;多款主流&amp;nbsp;ARM、RISC-V&amp;nbsp;和&amp;nbsp;MIPS&amp;nbsp;架构芯片，覆盖市面上绝大多数终端设备。未来，通义大模型还将与玄铁&amp;nbsp;RISC-V&amp;nbsp;实现软硬全链路协同优化，进一步提升在国产芯片上的部署效率与推理性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b84bac627e24c2d372a32ada8ec958ea.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套开发套件不仅提供基础能力，还预置十余款&amp;nbsp;MCP&amp;nbsp;工具和&amp;nbsp;Agent，覆盖生活、工作、娱乐、教育等高频场景。例如，基于出行规划&amp;nbsp;Agent，用户可直接调用路线规划、旅行攻略、本地探索等功能。同时，套件深度集成阿里云百炼平台生态，支持开发者添加社区模板，或通过&amp;nbsp;A2A&amp;nbsp;协议兼容第三方&amp;nbsp;Agent，极大扩展了应用边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99dcc21e3fc0751d1c9fa8ff12bf4c50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是&amp;nbsp;OPPO、理想这样的品牌厂商，还是华强北的创客、义乌的出海团队，甚至“一人公司”，都能借助阿里云的解决方案快速验证想法、打造产品，并参与全球竞争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是阿里云“基础设施先行”的思路，让展会上那些看似天马行空的产品，得以从概念走向量产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有趣的是，阿里云大模型能力的升级节奏，与AI硬件的集中爆发高度同步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2023年8月，阿里云开源Qwen-VL视觉语言模型，首次让中小厂商能免费调用工业级多模态能力；2024年，Qwen-Audio、Qwen2-VL等模型集中发布，补齐了语音、图像与文本融合交互的关键拼图；到2025年初，原生端到端的Qwen3-Omni模型的发布，以及Qwen-Agent，进一步支持硬件端构建任务型智能体。这一连串技术释放，恰好为AI硬件创新提供了可落地的底层支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从2024年下半年起，阅读器、眼镜、耳机、学习机等细分品类迎来AI功能的规模化落地：文石、闪极、AIxFU、听力熊、云希谷等能纷纷接入阿里云大模型能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些产品的共同点，是都受益于通义的“全谱系开源”策略——0.5B到480B的模型全覆盖，文本、语音、视觉、视频能力一应俱全。无论是大型企业，还是华强北的硬件作坊，都能找到适合自己的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是这种低成本接入到快速验证的正向循环，让AI硬件从概念走向规模化落地。阿里云没有造AI硬件产品，却通过持续开源和能力迭代，成为这场硬件浪潮背后最坚实的推手。&lt;/p&gt;</description><link>https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</link><guid isPermaLink="false">https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</guid><pubDate>Mon, 12 Jan 2026 10:42:48 GMT</pubDate><author>陈姚戈,王一鹏</author><category>AI&amp;大模型</category></item><item><title>英伟达发布了跨AI、机器人和自动驾驶的开放模型、数据集和工具</title><description>&lt;p&gt;英伟达（NVIDIA）&lt;a href=&quot;https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/&quot;&gt;发布&lt;/a&gt;&quot;了一套涵盖语言、智能体系统、机器人技术、自动驾驶和生物医学研究的开放模型、数据集和开发工具。此次更新扩展了多个现有的NVIDIA模型家族，并通过GitHub、Hugging Face和NVIDIA的开发者平台提供了相应的训练数据和参考实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在代理式AI领域，NVIDIA扩展了Nemotron模型家族，为语音识别、检索增强生成和安全提供了新的组件。Nemotron Speech包括针对低延迟、实时用例优化的自动语音识别模型。Nemotron RAG引入了用于多模态文档搜索和检索流程的嵌入和重排视觉语言模型。Nemotron Safety增加了用于内容过滤和敏感或个人身份信息检测的更新模型。NVIDIA还发布了用于选定Nemotron模型的数据集和训练代码，包括在公共基准上评估的嵌入模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于机器人技术和物理AI，NVIDIA引入了新的Cosmos世界基础模型，这些模型支持在真实环境中的感知、推理和合成数据生成。Cosmos Reason 2是一个多模态推理模型，旨在增强智能体在物理环境中操作的场景理解。Cosmos Transfer 2.5和Cosmos Predict 2.5专注于在不同环境和条件下生成合成视频数据，支持仿真和数据增强工作流程。基于Cosmos，NVIDIA发布了Isaac GR00T N1.6，这是一个用于人形机器人的开放视觉-语言-动作模型，支持全身控制并将视觉感知与动作规划集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公告的一个组成部分是NVIDIA Alpamayo，一个用于基于推理的自动驾驶的新开放模型家族。Alpamayo结合了感知、规划和可解释性，采用视觉-语言-动作架构，并与仿真工具和大规模驾驶数据集相匹配。NVIDIA还引入了AlpaSim，这是一个用于自动驾驶汽车模型闭环评估的开源仿真框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据NVIDIA汽车部门负责人吴信洲&lt;a href=&quot;https://www.linkedin.com/posts/xinzhouw_level-2-autonomous-driving-in-san-francisco-activity-7414325238630342656-moGb?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAACX5yoEBhsg1xPtc5iaJXHCu_Rv298CmfZA&quot;&gt;表示&lt;/a&gt;&quot;，Alpamayo和相关工具反映了跨研究、模拟、数据工程、安全和集成团队多年的开发努力。吴指出，这项工作涉及广泛的道路测试、使用Cosmos等平台进行持续的大规模模拟，以及与包括梅赛德斯-奔驰在内的汽车合作伙伴的紧密合作，计划在即将推出的量产车辆中进行初步部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;医疗保健和生命科学更新通过新的NVIDIA Clara模型提供。这些包括用于原子级蛋白质设计的La-Proteina，用于合成感知药物设计的ReaSyn v2，用于早期安全和相互作用预测的KERMT，以及用于RNA结构建模的RNAPro。NVIDIA还发布了一个包含45.5万个合成蛋白质结构的数据集，以支持该领域的训练和评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有模型和数据集均在开放许可下发布，可通过GitHub和Hugging Face访问。NVIDIA表示，许多模型还被打包为NIM微服务，以便在从本地推理环境到云基础设施的NVIDIA加速系统上部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/nvidia-open-models/&quot;&gt;https://www.infoq.com/news/2026/01/nvidia-open-models/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</link><guid isPermaLink="false">https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</guid><pubDate>Mon, 12 Jan 2026 09:12:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>英伟达</category><category>AI&amp;大模型</category><category>跨端开发</category></item><item><title>MongoBleed漏洞允许攻击者从MongoDB的堆内存中读取数据</title><description>&lt;p&gt;MongoDB最近修补了&lt;a href=&quot;https://www.mongodb.com/community/forums/t/important-mongodb-patch-available/332977&quot;&gt;CVE-2025-14847&lt;/a&gt;&quot;，这是一个影响多个支持和遗留MongoDB服务器版本的漏洞。根据披露，该漏洞可以被未认证的攻击者以较低的复杂度远程利用，可能导致敏感数据和凭证的外泄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞被称为MongoBleed，以臭名昭著的&lt;a href=&quot;https://en.wikipedia.org/wiki/Heartbleed&quot;&gt;Heartbleed&lt;/a&gt;&quot;命名，CVSS得分为&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2025-14847&quot;&gt;8.7&lt;/a&gt;&quot;，由对zlib压缩网络流量处理不当触发，允许未经身份验证的攻击者泄露未初始化的内存，并可能从受影响的MongoDB服务器窃取敏感数据，如凭证或令牌。根据&lt;a href=&quot;https://www.wiz.io/blog/mongobleed-cve-2025-14847-exploited-in-the-wild-mongodb&quot;&gt;Wiz&lt;/a&gt;&quot;的安全研究人员，该漏洞正在被广泛利用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如MongoDB的声明所述，MongoDB Atlas上的托管实例已经被修补，但是如果自托管MongoDB不更新，仍然存在风险。强烈建议组织立即应用安全补丁，或禁用压缩并限制网络暴露。Merav Bar、Amitai Cohen、Yaara Shriki和Gili Tikochinski解释：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;CVE-2025-14847源于MongoDB服务器基于zlib的网络消息解压缩逻辑中的一个缺陷，该逻辑在认证之前进行了处理。通过发送畸形的压缩网络数据包，未经身份验证的攻击者可以触发服务器错误处理解压缩的消息长度，导致返回给客户端未初始化堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Wiz文章，42%的云环境中至少有一个易受攻击的MongoDB实例，Censys报告称全球大约有87,000台服务器存在潜在的风险。由于该漏洞可以在没有认证或用户交互的情况下被利用，暴露在互联网上的数据库服务器面临特别高的风险。Wiz团队补充道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在代码层面，这个漏洞是由message_compressor_zlib.cpp中的错误长度处理引起的。受影响的逻辑返回了分配的缓冲区大小（output.length()），而不是实际解压缩数据的长度，从而允许过小或畸形的有效载荷暴露相邻的堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞影响了自&lt;a href=&quot;https://github.com/mongodb/mongo/pull/1152&quot;&gt;2017&lt;/a&gt;&quot;年以来发布的所有MongoDB版本。Linkfields Innovations的软件开发人员Gourav Boiri&lt;a href=&quot;https://www.linkedin.com/posts/bgourav2287_cybersecurity-mongodb-infosec-activity-7411163710372835328-DQQh&quot;&gt;评论道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MongoBleed突出了即使是成熟的数据库，当暴露或打补丁时，也可能成为关键的攻击面。预认证内存泄露、主动漏洞攻击和87K+暴露实例——提醒我们，数据库安全就是基础设施安全。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在“&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;简单解释MongoBleed&lt;/a&gt;&quot;”的文章中，&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;Stanislav Kozlovski&lt;/a&gt;&quot;解释了这一漏洞的工作原理，并警告说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;它非常容易被利用——只需要连接到数据库（不需要认证）。截至撰写本文时，它已经被修复，但一些EOL版本（3.6、4.0、4.2）将不会得到修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoSec创始人和实践者&lt;a href=&quot;https://www.linkedin.com/in/ecapuano/&quot;&gt;Eric Capuano&lt;/a&gt;&quot;解释了&lt;a href=&quot;https://blog.ecapuano.com/p/hunting-mongobleed-cve-2025-14847&quot;&gt;如何从日志中检测数据库服务器是否被利用&lt;/a&gt;&quot;。在一个流行的&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1py2c0w/mongobleed_vulnerability_explained_simply/&quot;&gt;Reddit&lt;/a&gt;&quot;帖子中，用户misteryuub争论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;很多人争论说开源代码比闭源代码更安全，或者安全问题会在开源代码中更快被发现。这种级别的漏洞存在是对这个论点的反驳。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kozlovski不同意：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当人们说开源更安全时，他们通常指的是有活跃社区的开源项目。Mongo在2017年似乎没有这个，因为引入这个漏洞的PR没有在公共GitHub上被审查。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mongodb.com/try/download/community&quot;&gt;MongoDB补丁版本&lt;/a&gt;&quot;现在可用于从4.4到8.0的所有支持版本。&lt;a href=&quot;https://www.percona.com/blog/urgent-security-update-patching-mongobleed-cve-2025-14847-in-percona-server-for-mongodb/&quot;&gt;像Percona Server for MongoDB这样的分支也受到上游漏洞的影响&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&quot;&gt;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</link><guid isPermaLink="false">https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</guid><pubDate>Mon, 12 Jan 2026 08:23:00 GMT</pubDate><author>Renato Losio</author><category>云端开发</category><category>数据库</category></item><item><title>亚马逊云科技预览Route 53 Global Resolver，将DNS与区域故障解耦</title><description>&lt;p&gt;亚马逊云科技（AWS）最近宣布公开预览&lt;a href=&quot;http://aws.amazon.com/route53/global-resolver&quot;&gt;Amazon Route 53 Global Resolver&lt;/a&gt;&quot;，这是一项在全球范围内提供安全、可靠的DNS解析的新服务。组织可以使用该服务来解析互联网上的公共域和与Route 53私有托管区域关联的私有域名的DNS查询。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从历史上看，管理混合型DNS带来了巨大的操作开销。在传统的区域设置中，管理员必须手动同步水平分区基础设施，并管理复杂的转发规则。这通常需要维护冗余的VPC解析器端点，并在多个区域中复制安全策略以确保故障转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Route 53 Global Resolver通过消除对单独分DNS转发的需求来解决这些挑战。正如AWS的高级解决方案架构师Esra Kayabali解释的那样：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它通过多种协议提供DNS解析，包括DNS over UDP（Do53）、DNS-over-HTTPS（DoH）和DNS-over-TLS（DoT）。每个部署提供一组通用的IPv4和IPv6任何播IP地址，将查询路由到最近的AWS区域，减少分布式客户端群体的延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b72ba1a3835c40c990f45b0e65569ea9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（来源： &lt;a href=&quot;https://aws.amazon.com/blogs/aws/introducing-amazon-route-53-global-resolver-for-secure-anycast-dns-resolution-preview/&quot;&gt;AWS&lt;/a&gt;&quot;新闻博客文章）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该服务集成了与Route 53 Resolver DNS防火墙等效的安全功能，可以集中实施策略。主要的安全功能包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;托管过滤：管理员使用&lt;a href=&quot;https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/gr-managed-domain-lists.html&quot;&gt;AWS托管域名列表&lt;/a&gt;&quot;来阻止恶意软件和网络钓鱼等威胁，或限制特定网络内容。行为保护：解析器检测并阻止域名生成算法（&lt;a href=&quot;https://www.geeksforgeeks.org/computer-networks/what-is-domain-generation-algorithm/&quot;&gt;Domain Generation Algorithm&lt;/a&gt;&quot;，DGA）模式和DNS隧道尝试。加密传输：支持DoH和DoT保护查询在传输过程中免受未经授权的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了&lt;a href=&quot;https://en.wikipedia.org/wiki/Zero_trust_architecture&quot;&gt;支持零信任架构&lt;/a&gt;&quot;，Global Resolver仅接受经过身份验证的客户端的流量。除了标准的IP/CIDR允许列表外，该服务为DoH和DoT连接引入了基于令牌的身份验证。这提供了细粒度的控制，允许管理员为特定客户端组或单个远程设备分配和撤销令牌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Abhijeet Kulkarni在LinkedIn&lt;a href=&quot;https://www.linkedin.com/posts/cloudwithabhi_route53-aws-route53-activity-7410397751529738240-mamy&quot;&gt;帖子&lt;/a&gt;&quot;中指出，虽然传统的DNS依赖于区域绑定的解析器，其中故障可能会放大中断，但Global Resolver引入了一种根本不同的运维模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过任播将解析移动到边缘，DNS在默认情况下成为全局分布的。Kulkarni强调，这提供了“解析层的故障隔离”，确保在DNS层吸收区域中断，而不是通过网络级联。这有效地将DNS从区域依赖转变为具有弹性的全球系统边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版目前在几个全球区域可用，包括美国东部（弗吉尼亚北部、俄亥俄州）、美国西部（加利福尼亚北部、俄勒冈州）、欧洲（法兰克福、爱尔兰、伦敦）和亚太地区（孟买、新加坡、东京、悉尼）。定价详情可在官方&lt;a href=&quot;https://aws.amazon.com/route53/pricing/&quot;&gt;Route 53&lt;/a&gt;&quot;定价页面上找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/route53-global-resolver-anycast/&quot;&gt;https://www.infoq.com/news/2026/01/route53-global-resolver-anycast/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Jey8ALfAhSOBWZ7El1is</link><guid isPermaLink="false">https://www.infoq.cn/article/Jey8ALfAhSOBWZ7El1is</guid><pubDate>Mon, 12 Jan 2026 07:44:00 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>数据库</category></item><item><title>抨击AI炒作、曝企业需求为先，Anthropic 联创：模型提 0.01 性能就血赚，算力烧钱但值！</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic由7位前OpenAI核心成员创立，他们曾参与GPT-2、GPT-3、Scaling Laws及AI安全研究。Daniela Amodei 就是其中之一，她是Dario Amodei 的妹妹，也曾任&amp;nbsp;OpenAI 的安全与政策副总裁，现在是 Anthropic 联合创始人兼总裁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2021年初疫情期间，Dario 冒雨向Eric Schmidt路演，后者成为Anthropic A轮投资人。与其他大模型公司不同，Anthropic会将大模型使用中的风险公开，比如Claude在极端“生存威胁”情境测试中，多数情况下会选择勒索，类似操作在行业中极少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Amodei 近期在接受 CNBC 采访中，她谈到了如何在支出方面控制成本、如何保障人工智能安全以及 2026 年上市的可能性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei 认为，“AI安全”不是商业负累，而是核心优势，企业客户对安全性的高要求，恰好匹配其创立初衷，这一理念在早期被认为“激进”，如今成为B端竞争壁垒。Anthropic是唯一能同时登陆微软、亚马逊云科技、Google三大云平台的前沿大模型厂商，企业客户需求曾数次超过其算力供给能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，她还提到，Anthropic以“不要相信炒作”为内部价值观，拒绝博关注，通过B端真实价值锚定长期方向，避免被行业泡沫裹挟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 在支出和算法效率方面采取了更为谨慎的态度，而其竞争对手 OpenAI 则承诺投入 1.4 万亿美元用于计算。她提到，即便行业算力投入规模惊人，但“更好的硬件回报极高”，哪怕模型性能仅提升0.01，价值也足够可观；且前沿模型的算力需求仍在指数级增长，需提前大规模布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，我们在不改变原意基础上进行翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 起源：离开OpenAI为什么值得&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这家公司创建初期，当时世界处在什么状态？你们觉得 Anthropic 要特别解决什么问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：Anthropic 就要迎来五周年了。说到最初，其实我和另外六位联合创始人，当时都在 OpenAI 一起工作。我们一起参与过很多不同的项目，从把一些当时规模最大的模型做起来，比如 GPT-2、GPT-3，到很多早期的语言模型工作，后来都成了大模型革命的一部分；还有 scaling laws 相关的研究，以及大量偏技术安全方向的工作，比如可解释性和对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时间点，我们逐渐形成了一个非常清晰的想法：我们想建立一家真正站在 AI 前沿、开发变革性技术的公司，但同时对系统的安全性和可靠性保持一种极其严格、近乎执念式的关注。那时我们觉得，与其在原有框架里继续做，不如自己出来，把这件事从头到尾做到极致。Anthropic 就是在这样的背景下诞生的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果把时间背景说清楚，那是 2020 年的冬天，到 2021 年初。大家都被封在家里，正值疫情高峰。那种感觉很复杂：机会既让人兴奋，又让人害怕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前提到过一个像电影画面的瞬间，2021 年 1 月，在 Dario 的后院，大家都戴着口罩，Eric Schmidt 也在，下着大雨，你们在帐篷下面向他做介绍。你会把它看作公司的起始点吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：是的，那一幕真的很难忘。具体日期我可能记不太准了，但应该是 1 月初的某一天。我们就在 Dario 家后院，正下着雨，我们临时搭了一个帐篷，我们私下都叫它“派对帐篷”，大家就挤在下面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，Eric 成了我们的 A 轮投资人。但当时，其实我们只是刚刚做出“要出来创业”的决定，一切都还非常早期，对公司具体会长成什么样，说实话并没有清晰的答案。我们只有一个特别大的愿景、特别宏大的想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个小插曲：那时候我其实已经怀孕八周了，怀的是我儿子。我觉得在所有联合创始人里，可能只有 Dario 知道这件事，甚至我都不确定他当时是不是已经知道了（笑）。所以那段时间，真的发生了太多事情：口罩、保持社交距离，一切都很混乱。但与此同时，我们又怀抱着一个巨大的梦想，无论在个人层面还是职业层面，那都是一个重大时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那一刻，你们觉得 OpenAI 做错了什么，才让“离开”这件事即使有很大风险也值得？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我们并不是“逃离”什么，更像是在“奔向”某个目标。我的意思是，我们这群联合创始人，彼此认识的时间其实非常久了，不只是 OpenAI 这段经历。比如 Dario、Chris Olah、Tom Brown 之前就在 Google Brain 一起共事；Jared 曾是 Dario 的研究伙伴；Dario 和我是兄妹。我和 Chris 已经认识十三年了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们是一群长期一起工作、在价值观上高度一致的人。我们都深信，人工智能有着极其巨大的正向潜力，但要真正释放这种潜力，必须极其严肃地对待风险问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时间点，我们开始想，如果能从一开始就创办一家把“安全与可靠性”放在一切核心位置的公司，会怎么样？我们内心其实也相信，这样的理念不仅有伦理意义，从商业角度看也同样有价值，甚至会成为一种优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时很多人认为，“安全”和“商业成功”是相互冲突的，但我们反而相信，这两件事是高度相关、彼此强化的。现在回看，这个想法在当时确实听起来很激进、很新，但那正是我们创立 Anthropic 的根本动因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：把“我们非常重视安全”这句话，真正落到可执行的策略上来看，现在最让你担忧的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得在安全层面，大概可以分成两个方面来说。首先是技术安全本身，这里面其实还有大量非常有意思、但尚未被完全发现和解决的技术问题。我认为 Anthropic 一直在努力成为这个领域的引领者，至少是积极推动者。无论是我们在机制可解释性上的研究，还是Constitutional AI，本质上都是在做一件事：想办法把“护栏”直接内建进模型里。我们的技术团队花了非常多时间去琢磨，怎样才能真正从模型内部把这些安全机制做好。但现实是，这件事永远做不完，而且模型变聪明的速度实在太快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是技术对整个社会层面的影响。这一点我们也公开谈过很多次。Anthropic 在这方面相对比较“异类”，我们会发布大量研究，去探讨人工智能可能带来的社会影响。比如我们最近发布过一份报告，讨论 AI 可能带来的经济层面影响，以及对劳动力市场的冲击。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们之所以尽可能透明，是因为我们真的认为，提前面对潜在问题，总比事后补救要好。作为一家公共利益公司（Public Benefit Corporation），我们觉得公开讨论这些问题本身就是我们的责任。当然，我们并不认为 Anthropic 能单独解决所有问题，但我们必须和公民社会、政府以及更多相关方一起讨论：当人工智能开始能够完成大量人类日常工作的那一天，世界会发生什么变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：“激进透明”似乎已经成了你们文化的一部分。你们也发布过研究，显示在面对“生存威胁”的极端情境时，Claude 在绝大多数情况下会选择勒索，而其他模型也有类似表现。你们把这些东西公开出来，几乎就像一条公共安全提示：这是这项技术现在能做到的事情，而这正是我们要解决的问题。那在这些案例之后，当你们进行方向调整时，最紧迫的安全重点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：确实，这种做法在行业里并不常见。很多人都会觉得，一家公司这么公开地谈论自己技术的风险和潜在伤害，是一件很不寻常的事情。我们之所以这么做，有几个原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，作为一家公共利益公司，这本身就是我们使命的一部分。我们确实相信 AI 有巨大的正向潜力，比如我们真心觉得，未来它可能在治愈疾病等领域发挥颠覆性的作用。但要真正实现这些美好愿景，就必须把最棘手、最困难的问题先解决好。从这个角度看，越是坦诚地谈风险，对所有人反而越有利。因为我们的目标不是制造恐慌，而是防止坏事发生，好让这些积极的成果真正落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个原因是，我们相信，更充分的信息和更开放的讨论，通常会带来更好的结果。我们很幸运，处在一个可以第一时间看到风险信号的位置，也有条件把这些信息讲清楚。比如我们可以明确地说：Claude 可能被用于网络攻击，这是一件必须高度警惕的事情。而且如果这种情况发生在我们身上，很可能也会发生在其他前沿模型开发者身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在安全、信任与防护这些领域，很多工作其实是可以、也应该跨公司协作的。把趋势、问题用清晰、易懂的方式公开出来，本身就是降低整体风险的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们经常会做一个反事实思考：如果你是一家上一代的技术公司，比如社交媒体平台，如果可以回到过去，提前知道这些平台后来带来的社会后果，你会不会选择做出不同的设计决策？Anthropic 想做的就是尽量在今天问自己这些问题。我们当然无法预测未来，但至少要问清楚：如果我们已经意识到某些风险的可能性，那我们今天有没有尽最大努力去讨论它、应对它、降低它？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在算力上是不是花太多了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：回看过去半年，整个行业签下的算力合同数量可以说非常惊人。与此同时，Gemini 在模型性能上也明显追近了差距。不少分析师指出，Google 的优势在于它几乎掌控了整个技术栈，从芯片、云业务，到各种可以直接部署技术的产品入口。而 Anthropic 现在也开始自建一方基础设施，在既有云资源承诺之外，又投入五百亿美元，在纽约和德州建设数据中心。这是不是你们赢得 AI 竞赛战略的一部分？要想胜出，就必须自己做基础设施，掌控更多垂直整合的能力吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：这是个很有意思的问题。人工智能领域的一个核心挑战在于，如果你想训练真正处在前沿的大模型，对算力以及相关资本的要求实在是太高了。Anthropic 一直以来的目标，是在这种“必须大量消耗算力”的现实下，尽可能理性、高效地使用我们手头的资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，长期以来，Anthropic 拥有的算力和资本，其实都只是竞争对手的一小部分，但在过去几年中，我们却相当稳定地推出了性能最强、效果最好的模型之一。我认为，这一方面来自团队本身的质量，另一方面也来自我们的价值取向，即用更少的资源，做更多的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，面向未来，算力需求确实会非常巨大。如果我们要随着公司规模扩大，继续站在技术前沿，那毫无疑问，我们也需要更多算力支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：粗略算一笔账，Anthropic 的算力投入大概在一千亿美元量级，而你们的竞争对手 OpenAI，据说已经到了万亿美元级别。从整个行业来看，我们是不是在算力上花得太多了？投入是否已经超过了大模型目前能够可靠变现的能力？还是说，这是服务用户所必需的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：很多被拿出来讨论的数字，其实并不完全可比，因为这些交易的结构本身就差异很大。有些是提前锁定购买权，有些是长期承诺，形式并不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从根本上讲，整个行业押注的是这样一个判断：如果你想在未来几年里拥有训练前沿模型所需的硬件资源，就必须非常早地、非常大规模地提前投入。如果你去问我的一些技术同事，他们会说一件很有意思的事：即便我们是 scaling laws的提出者之一，理论上早就相信“更多算力会带来更好结果”，但实际进展依然一次次超出我们的预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dario 也公开谈过这一点：无论是模型性能还是收入规模，很多指标看起来都呈现出一种指数级的增长。当然，我们内部也常说一句话：指数增长会一直持续，直到某一天不再成立。每一年我们都会怀疑：“不可能再这样增长下去了吧？”但结果是，每一年它都继续成立。所以这确实是个无法确定未来的问题，但至少到目前为止，年复一年的性能提升，看起来仍然相当接近指数曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：过去几个月我们也频繁讨论一种“循环式交易”：模型公司、硬件供应商、云厂商之间，通过股权换芯片、资源互换等方式形成闭环。这种结构中，哪些是健康的飞轮效应？又有哪些地方值得警惕？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我当然不能评论 Anthropic 以外的具体交易，更不可能讨论任何交易细节。但我想说的是，这些交易之间差异其实非常大，并不存在一种统一模式。不同参与方，对于算力和资本的理解方式本来就不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回到 Anthropic 自身，我们一直以来都是用相对更少的资源，去完成更多事情。我们的期望是，未来这些模型提供方，确实能成为你刚才说的那种“飞轮”的一部分。事实上，我们已经在某种程度上看到了这种趋势：Claude 是目前唯一一个同时在微软、亚马逊云科技和 Google 三大云平台上提供的前沿模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其在企业市场，我们会持续创造出大量价值。对 Anthropic 来说，我们一直是以企业需求为优先。而在过去一年左右的时间里，有不少时间段，我们甚至出现过“需求大于供给”的情况，从算力角度来看，Claude 的需求一度超过了我们能提供的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在硬件层面，你们是如何考虑芯片折旧的？是按三到四年的生命周期来规划，还是会把 GPU 用到十年，把整个可用寿命都榨干？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：坦率说，我并不是芯片方面的专家，我的一些同事会更适合回答细节。但从宏观上看，它和大模型的发展其实很相似。每一代新的前沿模型，性能都会好到让高端用户更愿意使用新模型，硬件也是如此。新一代芯片往往在性能、成本效率、能效上都会有所提升，所以，能尽早用上新一代芯片，本身就具有很高的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们是不是正处在一个 AI 泡沫里？我不是说技术不真实，而是支出增长曲线，是否已经跑在了收入增长曲线前面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我会把这个问题拆成两个层面来看：技术层面和商业层面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术层面，我们非常有信心。无论是 Anthropic 内部，还是 Dario 最近的公开表态，我们都认为，从纯技术角度看，进步并没有放缓。未来当然无法预测，但截至目前，模型仍然在以相当稳定、快速的节奏变得更强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在商业层面，这个问题就复杂得多。无论技术多先进，把它真正落地到企业或个人场景中，都需要时间。关键问题在于：企业，尤其是企业，能以多快的速度真正利用这些技术？也许Claude 5、Claude 6，在性能上依然是按同样比例提升的，但在组织内部推广和落地，可能会因为“人”的因素而遇到瓶颈：变革管理很难，采购流程很慢，很多应用场景一开始根本想不到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，真正值得观察的是：技术扩散到经济体系中的速度，是否能持续匹配技术本身的加速速度。这也是我认为最有挑战、也最值得持续关注的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：基于刚才的讨论，你觉得我们现在是不是在模型公司，或者在硬件供应链上投入得有点过头了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：从某种角度看，这个市场其实很小。说“小”听起来有点奇怪，毕竟金额巨大，但真正参与其中的玩家数量并不多。我也不完全确定该如何解读这一点，它有点不寻常。不过到目前为止，我们看到的情况是：更好的硬件，回报非常高，哪怕模型只提升一点点，比如0.01的性能提升，回报同样很可观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Anthropic 的历史中，这一点几乎一直都成立。所以我不太愿意直接用“过度投资”来形容，但我确实认为，这种参与者数量有限的结构值得警惕，一旦链条中的某个环节出了问题，后果会是什么？这是个很重要、也很有意思的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得我们现在大概处在这个周期的什么位置？不管你把它叫作泡沫破裂，还是一次正常的修正，考虑到最近出现的各种乱象和泡沫迹象，这种调整会不会在未来六到十二个月内发生？如果会的话，Anthropic 现在是否已经在为这种下行风险做准备？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：对于 Anthropic 来说，我们一直把自己看作是资本的理性、负责任的管理者。这一点从成立之初就是我们的重点。对我们而言，每一分算力、每一美元投入都非常重要，它们要么意味着我们能训练出更好、更安全的模型，要么意味着我们能服务更多客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我更愿意相信，我们对模型质量的预期、训练所需的算力、推理阶段服务客户所需的算力，以及我们能持续为客户创造的价值，都有一个相对合理的判断。当然，没有人能做到完美预测。但至少从一家负责任企业的角度来说，不管市场环境怎么变化，我希望我们都能处在一个相对稳健的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于整个市场会发生什么，这确实很难一概而论。但就 Anthropic 自身而言，做资本的负责任管理者，始终是我们的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们本身就是个做 to B”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：聊聊 Anthropic 接下来的资本路径吧。收购这条路，考虑到反垄断和你们目前的规模，基本可以排除了。那 IPO 会不会是2026年的一个选项？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：目前我们没有任何可以对外公布的具体计划。正如我之前说的，Anthropic 一直在努力以负责任的方式使用手中的资本。我们也始终在权衡：在哪里、以什么方式获得所需的资本，才是最合适的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Amazon 仍然是你们最大的战略支持方，但你们的股东和合作方阵容也在不断扩大，比如 Google 既是投资方又是云合作伙伴，还有 Microsoft、Nvidia。与此同时，Google 自身也在全力参与模型竞争。当你的合作伙伴本身方向和野心并不完全一致时，你们是如何处理这些关系的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得这恰恰说明了市场对这项技术的强烈需求。Anthropic 的模型能够同时在三大云平台上提供服务，本身就很有意思，甚至包括彼此之间存在竞争关系的云厂商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的直觉是，这些超大规模云厂商都在密切观察自己的客户在业务层面发生了什么。财富五百强企业可能用的是一家云，也可能是两家、三家，但现在几乎所有企业都有一个共同点：他们觉得自己必须要有 AI 解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我们看到的情况是，由于 Anthropic 特别专注企业场景，我们往往正是客户最想用的那个模型。如果企业无法使用 Claude，反而会对他们的业务造成伤害。所以，对我们来说，最重要的一点就是：在客户需要的地方出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些客户会直接用我们的一方服务，但更多客户已经和云厂商建立了长期合作关系，通过云平台接入大模型，对他们来说是非常自然的一条路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从一开始，Anthropic 似乎就不像 OpenAI 那样，急于抢占大众文化层面的心智，而是选择把筹码压在企业客户身上。事实证明，这可能是一个更聪明的选择。现在很多人都在追赶你们，试图在企业市场分一杯羹。你们当初是怎么判断，企业才是最值得投入的市场？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：你给我们的评价有点高了，我不敢说我们一开始就“确定”这条路一定是对的，但我觉得可以从两个方面来看。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，Anthropic 这个组织本身就非常适合做一家 B2B 公司。我们对可靠性、安全性和安全边界的重视，是写进公司基因里的。这也是我们创立 Anthropic 的初衷之一：既要释放 AI 的巨大潜力，也要尽可能降低风险。而事实证明，企业客户非常看重这一点。我从没听过哪家企业客户对我说：“如果 Claude 能多一点幻觉、多生成点有害内容就好了。”从这个角度看，企业对安全性的高要求，反而让 Anthropic 从第一天起就处在一个很有优势的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，是一种更偏经济学层面的判断，当时也可能判断错了。我们认为，这些模型虽然在娱乐层面也很有吸引力，但从长期看，它们更像是帮助人类完成高价值工作的工具。无论是现在 Claude 被大量用于写代码，还是用于总结复杂信息、做金融分析和数据分析，我们在2020年底、2021年初，就已经隐约看到了这样一种未来：模型可以承担大量工作场景中需要高智力投入的任务。而我们认为，这是一个非常大的市场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这两个因素叠加在一起，让我们觉得，把 Anthropic 做成一家以企业为核心的公司，是一条合理的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：企业客户通常既强调安全，也永远希望有更多功能、更强的Agent能力。有没有一些需求，是客户明确提出来了，但因为安全护栏的原因，你们暂时还不愿意提供的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：有意思的是，到目前为止，我们还没有遇到那种特别明确的场景：安全和功能之间形成了正面冲突。更多时候，挑战在于如何确保我们发布的模型始终处在前沿水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实有过这样的情况：我们在内部已经有一个模型准备好了，但在正式发布之前，还需要做更多安全测试。客户并不会直接看到这一点，但这是我们必须坚持的过程。所以如果说安全和产品之间的“交汇点”在哪里，那大概就是：确保我们推向市场的模型，已经在安全性上做到我们能力范围内的最好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“AI 原生”创业公司蓬勃发展&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：谈到规模发展，很多当初的决策其实都需要随着时间不断演化。比如一开始，Anthropic 曾明确表示不会接受来自中东的资金，但在最近一轮融资中，这个立场发生了变化。你们是如何在坚持最初的原则、以及为了在激烈竞争中生存和发展而必须做出调整之间取得平衡的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我认为，在最重要的层面上，Anthropic 随着规模扩大，其实一直坚守着自己的价值观。尤其是我们的 PBC（公益型公司）结构，以及“公共利益公司”作为北极星一样的存在，对我们非常重要。正是因为有这样一个长期愿景，当具体问题出现时，我们总会回到一个核心判断：我们现在做的事情，是否真的在为公共利益服务？是否是在努力让 AI 的转型过程走得更好？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，不同的人对“公共利益”具体意味着什么，理解上可能会有差异，但我们对新员工、候选人、投资人都非常坦诚：这就是 Anthropic，这就是我们的价值观。正因为如此，大多数情况下我们都能比较顺畅地做出判断。只是正如你所说，随着公司规模变大，确实会遇到一些处在灰色地带、更加棘手的情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有一种批评声音认为，把“安全使命”放在如此核心的位置，实际上会形成一种“可防御壁垒”，让最早成立的几家大模型实验室更容易在监管环境下维持竞争优势，而后来进入的初创公司，由于没有经历同样的积累过程，反而更难追赶。你怎么看这种说法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：这点挺有意思的。虽然我现在没有具体数据在手，但我印象中，绝大多数初创公司其实都是云计算用户。真正被算力和资本门槛高度限制的，是“前沿模型”的研发。正如我们之前聊到的，要成为一家前沿模型实验室，成本确实非常高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在 Anthropic，我们看到的是一个正在蓬勃发展的生态系统，我们称之为“AI 原生”创业公司。就像五到十年前大家谈“数字原生”企业一样，现在出现了大量“AI 原生”公司：它们的产品从一开始就是围绕人工智能能力构建的，而其中绝大多数都是构建在云基础设施之上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为，我们对这个生态系统的影响，最终取决于我们是否能够持续打造行业里最优秀、最安全的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但现在“安全”并不是一个特别受欢迎的立场。一个月前你们和 David Sacks 有过一些隔空讨论，Dario 也写了一篇文章回应。面对这样的情况，你们如何避免让外部环境干扰你们真正的技术工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我认为 Anthropic 一直努力把重点放在“政策”而不是“政治”上。我们在很多议题上，其实能够跨越党派找到共识，而这些议题正是美国公众真正关心的事情。比如，如何保持美国在全球 AI 领域的领先地位，又比如如何确保我们开发出来的模型真正对人有益、对孩子有益、对使用它们的成年人也有益。在这些问题上，其实存在着相当多的共识空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，人工智能仍然是一个非常新的领域。正因为如此，我们始终保持开放和好奇，去探索以安全、可靠的方式发展这项技术的最佳路径。我们也一直在学习，这也是为什么我们会大量公开发布研究成果的原因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在还会去思考“有效利他主义”（Effective Altruism）吗？我知道你之前说过，这个标签在你看来已经有些过时了，也不再是公司当前叙事的一部分，但无论是早期招聘还是融资阶段，它确实曾经深深嵌入你们的创始故事里。那它现在在公司内部的文化中还留下了些什么吗？还是说，到 2025 年，这更多只是外界投射到 Anthropic 身上的一种标签？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得“投射”这个词可能更接近。Dario 之前也谈过这个问题。你得回到一个背景：在 AI 非常早期、差不多二十年前的时候，真正认真思考“AI 可能会变得如此强大”的人其实非常少。而恰恰是那一小撮人，往往同时也非常关注风险问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你会看到，早期的 OpenAI，以及后来成为 Anthropic 创始成员的一些人，最初确实是从“风险”这个角度出发的，我们在担心技术可能出什么问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为 Anthropic 最大的不同之处在于：我们同样高度关注技术的“正向价值”和“上行空间”。我们一直在思考，人工智能在医学、生命科学、医疗健康、金融服务，以及整个经济体系中，究竟能带来多大的积极影响。当然，如果我们不能把它做得足够安全，事情也可能会走向非常糟糕的方向，这两点是并存的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“不要相信炒作”，AGI理念过时了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Anthropic 的品牌似乎自带一种“神秘感”，我不太好精准形容，但感觉公司内部的人，几乎把它当成一种信念体系。你会如何描述你所塑造的 Anthropic 员工文化？另外，我也注意到，虽然最近几个月你们变得更公开了一些，但整体来看，你们仍然非常克制，往往在真正准备好之前，很少对外释放信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：你这么说真的很善意，我不知道是不是“神秘感”，但我很感激这样的评价。对我们来说，有一个内部反复强调的价值观，就是“不要相信炒作”。这听起来好像很小，但我觉得它其实回到了我们之前讨论的那些关于经济、商业的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 从来不是为了博关注、抢头条而存在的。我们真正关心的是把事情做好，无论是在模型训练层面，如何以公平、负责的方式训练模型；还是在客户层面，如何每天都真正为客户提供价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在 AI 领域的炒作非常多，而我们是一家以企业客户为核心、B2B 导向的公司，这在某种程度上让我们更加“脚踏实地”。我们的目标很简单：为企业创造真实价值。这项工作往往不那么炫目，但它能帮助我们不被泡沫裹挟，始终记得我们当初为什么要创办这家公司——我们是一家公共利益公司，我们关心的是长期价值。如果没有这个北极星，其实很容易迷失方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Yann LeCun 以及其他一些机器学习领域的“老一代”学者认为，大语言模型并不能通向 AGI，他们转而研究世界模型，认为还需要一些关键突破才能迈向下一阶段。你怎么看？你认为真正解锁 AGI 所需要的突破是什么？未来你们是否需要超越 LLM，才能在行业中保持竞争力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：AGI 这个词本身就挺有意思的。Dario 也说过，很多年前，这个概念是有意义的，它帮助我们思考“什么时候 AI 会和人类一样强”。但有趣的是，按照某些定义，我们其实已经超过了这个标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如说，Claude 写代码肯定比我强，这个门槛不高。但它也已经能在一定程度上，达到甚至接近 Anthropic 许多工程师的水平。要知道，我们雇的可是世界上顶尖的一批工程师，而他们中的不少人都会说，Claude 已经能完成他们相当一部分的工作，或者极大地加速他们的效率。这本身就很疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，另一方面，Claude 依然做不了很多人类能做的事情。所以我觉得，AGI 这个概念本身可能不是“错了”，而是有点过时了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于是否需要新的突破才能继续前进，老实说，我们并不知道。技术发展的路径，本身就是科学与工程的复杂混合体。而我觉得实验室最特别的地方就在于：不同团队会用完全不同的方式去逼近同一个目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至少从目前来看，进展并没有放缓。当然，一切都是“直到它真的放缓为止”。如果让我下注，我会说，能力还会在相当长一段时间内继续提升，我们也应该为这样的世界做好准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你和 Dario 的能力结构差异很大，你在哪些方面补足了他？你是如何帮助他把想法打磨得更锋利的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：能和我的哥哥一起经营 Anthropic，真的是一种“特权”。我感觉我们认识彼此一辈子了，他在我出生前独自生活了四年，挺惨的（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dario 有一种非常罕见的能力，仿佛能“看到未来”。虽然我总说没人真的知道未来，但如果真有这样的人，那大概就是他。从技术视角来看，他对技术走向、对社会和组织意味着什么，有着极其敏锐的直觉，这是一种真正的愿景型领导力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我更偏向实务型。我非常喜欢运营组织，我大部分时间都在和高管团队一起工作，比如搭建团队、招聘负责人、思考客户真正需要什么、如何为企业创造价值、如何构建让公司长期可持续的合作关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得 Dario 和我彼此成就。他会不断把我拉回更宏大的视角，而我则专注于如何打造一家能长期存在、可持续发展、聚集了一群真正想做我们五年前一起立志要做的事情的优秀人才的组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GMXnmaky9FY&quot;&gt;https://www.youtube.com/watch?v=GMXnmaky9FY&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fZkrM56MKzx9s8dNHYZ7</link><guid isPermaLink="false">https://www.infoq.cn/article/fZkrM56MKzx9s8dNHYZ7</guid><pubDate>Mon, 12 Jan 2026 07:02:44 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>“死了么”APP爆火，3人开发成本1500元：不改名；姚顺雨入职腾讯后首发声；微软本月大裁员，至少涉1.1 万人；字节实习生全面涨薪 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;引言：唐杰、杨植麟、林俊旸、姚顺雨聚会：AI发展的共识和差异;“死了么”APP爆火，开发者：用户数翻了50倍，尚不准备改名；消息称微软本月将启动新一轮大裁员，规模达 1.1 万至 2.2 万人；字节实习生全面涨薪，最高涨幅达150%；马斯克：X平台将于七天内开源其算法；消息称约翰・特努斯成库克头号苹果接班人，曾主导 iPhone Air 项目；OpenAI预留500亿美元员工股权激励池；王腾官宣创业：核心成员来自小米、华为，薪资福利基本看齐大厂；京东将推出全年龄段人群AI玩具……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;唐杰、杨植麟、林俊旸、姚顺雨聚会：AI发展的共识和差异&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在近日的AGI-Next 前沿峰会上，唐杰、杨植麟、林俊旸、姚顺雨等行业标杆人物，与张钹院士共同勾勒出大模型发展的新图景，围绕技术突破、行业分化、范式变革与中国 AI 的未来展开了一场思想碰撞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术发展的核心议题上，各位领军者达成了“突破现有瓶颈、迈向多元智能”的共识。智谱创始人唐杰直言，中国开源大模型虽成果斐然，但与美国闭源大模型的差距可能仍在拉大，行业需保持清醒认知。他提出，大模型的下一阶段应借鉴人脑认知过程，重点突破三大能力：多模态“感统”能力，实现视觉、声音、触感等多源信息的统一感知；构建全人类“第四级记忆”，解决模型记忆与持续学习不足的问题；探索反思与自我认知，挖掘大模型自主意识的可能性。2026年，智谱将聚焦架构创新、多模态感统等方向，推动AI进入长任务场景并实现具身智能，同时预判今年将成为AI for Science的爆发年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;月之暗面Kimi创始人杨植麟则从Agentic时代的技术架构切入，强调提升token efficiency与实现long context的双重重要性。他认为，前者能以更少token达到同等效果，后者可突破传统架构局限，支撑复杂Agent任务，二者结合方能实现更高水平的代理智能。更具启发性的是，他提出智能具有“非同质化”属性，未来的技术升级不仅是算力的堆砌，更关乎“品味”——即对AI价值观与形态的深层理解，这种差异性将催生出更多新颖应用场景。面对AGI潜在风险，杨植麟秉持开放态度，认为AGI是提升人类文明上限的关键工具，应在风险可控的前提下持续迭代突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通义Qwen技术负责人林俊旸则将目光投向物理世界，提出打造Multimodal Foundation Agent的愿景。他认为行业发展“殊途同归”，全模态模型与具身推理是核心方向，Agent将从数字世界走向物理世界。林俊旸描绘了具体的落地场景：数字特工可实现GUI操作与API调用，物理特工则能完成斟茶倒水等实体交互动作，这种从虚拟到现实的延伸，为AI应用开辟了广阔空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为压轴嘉宾，张钹院士从旁观者视角给出了深刻洞见。他指出，大模型当前擅长跨领域泛化，但落地应用需实现跨任务泛化，重点解决分布外、长尾场景的泛化难题，具体应推进多模态、具身交互、结构化知识对齐等六大方向。在人机关系上，他大胆质疑“机器必须与人类对齐”的传统认知，认为人类存在固有缺陷，无需让AI完全复刻；而AI治理的核心，不应是约束机器，而是规范研究者与使用者的行为。值得关注的是，张院士一改以往态度，鼓励最优秀的学生投身创业，认为人工智能时代的企业家应承担起将知识、伦理与应用转化为通用工具的使命。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;圆桌对话环节，嘉宾们围绕行业分化、范式变革、Agent战略与中国AI的胜算四大议题展开深度探讨。腾讯首席科学家姚顺雨从跨中美视角指出，To C与To B场景的模型需求已分道扬镳：To C用户对强智能需求有限，To B领域则呈现“智能即生产力”的鲜明特征，模型强弱分化将愈发明显。在范式变革方面，姚顺雨提出自主学习已实际发生，只是尚未形成颠覆性感知；唐杰则预判2026年将出现新范式，单纯依靠扩算力、扩数据的Scaling模式已难以为继，创新是唯一出路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于中国AI的全球竞争力，嘉宾们既正视差距也保持信心。姚顺雨认为中国团队在快速复现与局部优化上具备优势，但缺乏敢于探索未知的“冒险家”；林俊旸坦言美国在算力投入上领先1-2个数量级，中国团队领先概率约为20%，但“穷则思变”可能催生创新机会；唐杰则强调，凭借敢冒险的年轻一代、良好的发展环境与持续深耕的定力，中国AI有望在长期竞争中实现突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;“死了么”APP爆火，开发者：用户数翻了50倍，尚不准备改名&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 年 1 月，郑州月境技术 3 人 95 后团队开发的 8 元付费 APP “死了么” 爆火，苹果付费软件排行榜登顶，用户数较此前翻 50 倍仍在上涨。据悉，该 APP 专为独居人群设计，2 日未签到即自动向紧急联系人发邮件，因名字有传播力、需求旺盛等爆火，团队表示暂不改名，计划上线短信提醒、留言等功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该软件不需注册登录，首次使用只需填写本人姓名与紧急联系人邮箱即可。每天打开应用轻轻一点完成签到，后台自动监测状态。系统有一个异常未签到自动通知的功能，如果用户连续2天没有在应用内签到，系统将于次日自动发送邮件告诉对方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其背后公司名为月境（郑州）技术服务有限公司，2025年3月份才成立，注册资本10万元。创始人之一小郭对媒体介绍，团队有3人，一位是朋友，一位是网友，都是95后。这款APP耗时1个月完成，开发成本约1500元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，“死了么”在2025年中旬上线，不过期间团队未花过多精力打理，在一个月前才做了一次更新。上线后很长一段时间里用户量很少，团队也不擅长营销，直到最近突然爆火，用户数达到之前的50倍，目前热度还在上涨。不过由于用户规模数能直接推导出团队收益，小郭表示，目前不便透露具体用户规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;消息称微软本月将启动新一轮大裁员，规模达 1.1 万至 2.2 万人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 7 日消息，据报道，微软公司计划于 2026 年 1 月启动新一轮裁员。预计全球范围内裁员规模将达到 1.1 万至 2.2 万人，约占其全球约 22 万名员工总数的 5% 至 10%。此次裁员预计将在 1 月第三周实施。有员工透露，微软 Azure 云团队、Xbox 游戏部门以及全球销售部门将是裁员的重点领域。截至目前，微软尚未证实该计划。微软在 2025 年尽管全年营收与利润保持稳健态势，该公司仍通过多轮裁员削减了超过 1.5 万个岗位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，微软正加大对人工智能系统的投入力度。仅在 2026 财年第一季度，其资本支出就高达 349 亿美元（现汇率约合 2441.36 亿元人民币）。该公司预计全年总支出将突破 800 亿美元（现汇率约合 5596.24 亿元人民币），超过 2025 财年水平。这笔资金的大部分将用于数据中心、芯片及人工智能工具的建设与研发。分析师认为，受此战略调整影响，微软正将资金从人力成本转向长期技术资产投资。因此，中层管理人员及传统产品团队将面临更高的裁员风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;字节实习生全面涨薪，最高涨幅达150%&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月5日，有消息称字节跳动实习生全面涨薪，覆盖技术、产品、运营等多个岗位，薪资标准自2026年1月1日起正式生效。其中，技术类实习生日薪调整至500元，较此前上涨25%。产品类岗位从每日200元提升至500元，较此前上涨150%。此外，运营、设计、市场、职能、销售等其他岗位也均有不同程度涨薪，调整后日薪区间涵盖100余元至400余元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;需要注意的是，此次公布的涨薪标准主要适用于北上广深杭等一线城市。同时，具体薪资仍会根据岗位类型、所在业务线等因素有所区别，并非完全统一。通过查询招聘软件发现，目前北京地区的产品实习生日薪已调整为500元，运营、营销类实习生日薪则为350元/天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，字节跳动 2025 年 12 月发布面向全球员工的内部邮件，宣布继续加大人才投入，提高薪酬竞争力、提升期权激励力度。具体包括以下措施：增加奖金（含绩效期权）投入，2025 全年绩效评估周期相比上个周期提升 35%；大幅增加调薪投入，较上个周期提升 1.5 倍；提高所有职级薪酬总包的下限（起薪）和上限（天花板）。该公司表示，此举系为确保员工薪酬竞争力和激励回报在全球各个市场都“领先于头部水平”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克：X平台将于七天内开源其算法&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交媒体平台X创始人埃隆・马斯克于周六表示，该平台将在七天内面向公众开源其新版算法，这一算法包含用于决定向用户推荐哪些帖文及广告的相关代码。“这项举措将每四周推行一次，同时会附上详尽的开发者说明文档，助力大家了解算法的具体更新内容。”身为X平台所有者的马斯克在该平台发布的一则帖子中如此表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;消息称约翰・特努斯成库克头号苹果接班人，曾主导 iPhone Air 项目&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 9 日消息，报道称伴随着现任首席执行官蒂姆・库克年满 65 岁，且其本人有意减轻工作负荷，苹果公司已加速接班人计划，而约翰・特努斯再次被认为是接班热门人选。媒体援引博文介绍，现年 65 岁的库克向高层坦言感到疲惫，希望减轻工作负担。若库克决定卸任 CEO 一职，极有可能转任苹果董事会主席。在众多候选人中，现任硬件工程主管约翰・特努斯尽管行事低调，但已跃升为头号热门人选。特努斯现年 50 岁，这一年龄恰好与库克 2011 年接替乔布斯时的年龄相同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知情人士透露，特努斯之所以脱颖而出，源于其在产品定义与商业利益间“穿针引线”的精准把控力。据内部人士回忆，2018 年前后，苹果为了提升摄影与增强现实（AR）体验，曾考虑在 iPhone 上引入一种微型激光（LiDAR）组件。然而，该组件高达 40 美元的单项成本将严重压缩利润。特努斯当时果断建议：仅在价格更高的 Pro 机型上搭载该组件。他认为，购买 Pro 系列的忠实用户更愿为新技术买单，而普通用户对此并不敏感。这一决策不仅保住了利润，也确立了产品分级策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对外界关于其缺乏创新能力的质疑，Ternus 的支持者指出，他实际上深度参与了近年来多个关键产品的研发。值得注意的是，备受瞩目的 iPhone Air 以及即将面世的折叠屏 iPhone 均由他牵头主导。这些项目显示，Ternus 不仅具备卓越的执行力，在推动产品形态创新方面同样拥有实际战绩。此外在管理风格方面，特努斯被认为与库克高度相似。他于 2001 年加入苹果，以注重细节和深谙庞大的供应链网络著称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI预留500亿美元员工股权激励池&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日消息，据外媒报道，人工智能公司 OpenAI 去年秋季设立了一项规模达约 500 亿美元的员工股票激励池，相当于公司当时估值的约 10% 股份，该估值基于 2025 年 10 月约 5000 亿美元 的公司估值水平。报道指出，此前 OpenAI 已向员工授予约 800 亿美元的已归属股权，本次新增的股票激励池与既有部分合计约占公司总股份的 26%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去一年中，OpenAI 的估值经历了快速增长。2025 年年中公司通过一笔员工股份二级市场交易达到约 5000 亿美元估值，高于前一次由 SoftBank 等领投的 3000 亿美元融资轮。二级股权交易不仅为员工提供了变现渠道，同时也被视为衡量市场对 OpenAI 增长前景信心的一个指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一大规模股权激励池反映了 OpenAI 在全球 AI 竞争中对人才吸引与保留的高度重视。在人工智能研发与产品商业化日益加剧的背景下，顶尖 AI 研究人员和工程师成为市场追逐的稀缺资源，竞争对手包括 Meta、Google 等科技巨头均提供了丰厚的股权激励条件。在行业快速发展与人才争夺日益激烈的背景下，OpenAI 的股权策略旨在通过高比例激励计划锁定核心技术人才，同时支持公司未来产品和业务长期增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;王腾官宣创业：核心成员来自小米、华为，薪资福利基本看齐大厂&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日，王腾在社交平台公布最近情况。王腾称，从小米离开后开始筹备创业，最近新公司已经成立，公司取名为“今日宜休”，目标是通过研发睡眠健康相关的产品，让大家能拥有更好的精力状态。王腾表示，目前已经组了一个初创团队，核心成员主要来自小米、华为等头部科技大厂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王腾还放出招聘广告，重点招聘软硬件产品经理、 健康/AI算法工程师、脑科学睡眠健康专家等岗位。王腾还解释为何选择睡眠健康、精力管理方向：1. 首先睡眠、精力已经成为每个人都关心的健康问题。2. 社会对睡眠的价值理解有待提升。3. 新时代下AI大模型发展迅速，让很多产品的体验能大幅提升。公开信息显示，北京今日宜休科技有限责任公司成立于2026年1月6日，由王腾持股55%并担任法定代表人，注册资本是100万人民币，注册地址是北京市海淀区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3f/3f405b05a6f547704545d3cf7428ce71.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此前报道，去年9月8日，小米发布内容通报，原小米中国区市场部总经理、REDMI品牌总经理王腾因泄密被小米公司辞退。11月份，王腾发文称告别手机行业。他表示前段时间因为自己的问题离开小米，最近也有一些公司发来邀约，但综合竞业限制和个人兴趣的考虑，想跟手机行业说声再见了，愿还在这个行业的朋友们继续加油，期待更精彩的产品出现。王腾还透露11月开始准备尝试些新的赛道，大的方向是科技+健康领域，具体还在筹备中，“迎接新的挑战，正是闯的年纪。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;京东将推出全年龄段人群AI玩具&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日消息，据媒体报道，京东成立“变色龙业务部”，全面承接JoyAI App、JoyInside、数字人等核心AI产品的打造与商业化。报道称，全新的第二批AI玩具已在筹备中，此次新品将推出面向全年龄段人群的AI玩具，将于1月中旬全面上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，在 2025 世界人工智能大会（WAIC）期间，京东正式宣布旗下大模型品牌升级为 JoyAI，以及京东在大模型方向的技术进展和JoyAI应用全景图，同时也发布了全新的附身智能品牌JoyInside。据当时介绍，JoyAI大模型拥有从3B到750B全尺寸模型家族，且通过动态分层蒸馏、跨领域数据治理等创新技术，大模型推理效率平均提升了30%，训练成本降低70%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，谈到JoyInside，截至2025年7月，已有众擎、云深处、商汤元萝卜、火火兔、Fuzozo等数十家企业已正式接入，覆盖人形机器人、四足机器人、儿童玩具、AI潮玩等多类载体。另据京东官方披露，截止2025年12月，已有超4.5万家品牌接入数字人服务，数字人直播成本约为真人直播的1/10，平均转化率提升约30%。在2025年“双11”期间，采用数字人直播的商家数量同比增长近6倍，全年累计带动商品交易总额（GMV）达数百亿元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;蚂蚁美团联手投了一家AI硬件创企，前美团硬件负责人带队&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月5日，北京AI硬件创企Looki正式完成超2000万美元（约合人民币1.4亿元）A轮融资，本轮由蚂蚁集团领投，美团龙珠、华登国际、中关村资本跟投，老股东BAI资本连续两轮超额追投，阿尔法公社、同歌创投持续加码。在完成本轮融资后，Looki计划加快人才建设、模型迭代、产品研发及供应链整合，围绕AI原生硬件推进下一代交互设备的探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Looki成立于2024年5月，截至目前已连续完成4轮融资。该公司由两位卡内基梅隆大学（CMU）的校友联合创办，CEO孙洋曾任美团智能硬件负责人、Momenta高级研发总监，是Google Assistant早期创始成员之一。CTO刘博聪曾任美团自动驾驶算法负责人、Pony.ai创始成员。团队成员来自清华大学、北京大学、多伦多大学、伊利诺伊大学、伦敦政经等知名院校，曾就职于Google、Amazon、Qualcomm、字节跳动等公司，在AI算法、AI产品、硬件工程等方面具备丰富经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Looki发布的一段产品介绍视频中，CEO孙洋称，Luki L1自去年8月上线以来，已被不少用户当作“记录生活节奏”的常用设备使用。Luki还具备“主动AI”能力，如根据饮食、坐姿时间、行为节奏提出健康建议，例如“你今天已经喝了两杯咖啡，要不要换成水？”或者“你已经在桌前坐了一小时，要不要走一走？”等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智谱上市，唐杰内部信要求全面回归基础模型研究&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日智谱上市当天，清华大学计算机系教授、智谱创立发起人兼首席科学家唐杰发布内部信，宣布很快将推出新一代模型GLM-5。内部信还介绍了2026年智谱聚焦的三个技术方向，包括全新的模型架构设计，更通用的RL（强化学习）范式以及对模型持续学习与自主进化的探索。它们均围绕基础模型能力提升展开。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;上海又一GPU“四小龙”上市！&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;继沐曦股份、壁仞科技之后，上海又一家AI芯片企业成功上市。1月8日，上海芯片企业天数智芯登陆港交所，在1个月的时间内，上海已先后有“港股国产GPU第一股”的壁仞科技和科创板上市首日涨幅近7倍的沐曦股份，加上已完成IPO辅导冲刺科创板的燧原科技，上海GPU“四小龙”齐聚资本市场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;媒体从上海市经信委获悉，2025年1-11月，上海市集成电路产业营收规模3912亿元，同比增长23.72%，2025年全年产业规模预计超4600亿元，同比增长24%，五年间产业规模翻了一番多，超额完成“十四五”发展目标。集聚超1200家集成电路企业，汇聚全国约40%的产业人才、近50%的产业创新资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;天数智芯战略与公共关系部副总裁余雪松表示，作为国内首家开展通用 GPU 自主研发的企业，公司已完成从核心技术攻关到商业化落地的全链路贯通。“我们的研发团队有480人，平均拥有20年以上行业经验，超三分之一研发人员具备10年以上芯片设计与软件开发经验。包含架构、通用GPU IP及芯片设计、基础软件、软硬件协同等各领域的专家。”余雪松说。上海市经信委相关工作人员表示，除了上海GPU芯片“四小龙”（壁仞、沐曦、天数、燧原），光计算、近存计算等创新路线AI芯片企业也相继涌现，支撑国内大模型等新质生产力发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克回应英伟达自动驾驶AI模型：特斯拉正在做，达到99%很容易&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月6日消息，在2026消费电子展（CES）上，英伟达宣布推出Alpamayo系列开放式AI模型、模拟工具和数据集，旨在解决自动驾驶安全挑战。对此，马斯克回应称：“好吧，这正是特斯拉在做的。他们会发现，达到99%很容易，但要解决分布的长尾问题却非常困难。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Alpamayo平台的核心是Alpamayo 1模型，这是一款拥有100亿参数、基于思维链技术的视觉-语言-行动（VLA）模型。该模型可让自动驾驶汽车具备类人思维能力，即便在未经任何训练和标注的情况下，也能解决复杂的场景问题，例如在交通信号灯失灵的路口规划通行路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达还强调，Alpamayo模型并非直接在车内运行，而是作为大规模教师模型，供开发者微调并提取到其完整自动驾驶技术栈的骨干中。黄仁勋在声明中表示：“首款搭载英伟达技术的汽车将于第一季度在美国上路。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;硅谷科技初创公司兴起“脱鞋办公”潮&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 5 日消息，曾经靠海洋球滑梯、免费尼古丁袋等五花八门的福利留住员工的硅谷热门科技初创公司，如今又出新招——要求员工进门脱鞋。根据观察，在年轻人占主导的办公场所，“无鞋办公”政策正悄然兴起。雇主们认为，员工穿着毛绒袜、拖鞋踩在地毯上，能打造出更轻松无压的工作氛围。然而矛盾的是，这些公司中不少仍推行“996”工作制，要求员工从早 9 点工作到晚 9 点，每周连轴转 6 天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;斯坦福大学经济学家、职场文化专家尼克·布鲁姆表示，无鞋办公政策的流行，在一定程度上是“睡衣经济”的延伸——随着远程办公者被要求重返办公室，他们也把居家办公的习惯带到了办公室。但这一趋势也与硅谷高压的工作文化一脉相承。布鲁姆说：“如果你每天要在公司待 12 个小时，那不如直接穿拖鞋上班，毕竟在家也没机会穿。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;中国商务部回应Meta收购Manus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日，就Meta收购人工智能平台Manus一事，中国商务部新闻发言人何亚东表示，中国政府一贯支持企业依法依规开展互利共赢的跨国经营与国际技术合作。何亚东在当日举行的例行新闻发布会上回应称，需要说明的是，企业从事对外投资、技术出口、数据出境、跨境并购等活动，须符合中国法律法规，履行法定程序。商务部将会同相关部门对此项收购与出口管制、技术进出口、对外投资等相关法律法规的一致性开展评估调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄仁勋官宣英伟达已投产 Vera Rubin：训练 AI 速度是 Blackwell 架构 3.5 倍&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在北京时间 1 月 6 日凌晨举办的 CES 2026 主题演讲中，英伟达首席执行官黄仁勋发表主题演讲，介绍了新一代“Rubin”计算架构，并将其定义为当前 AI 硬件领域的“最先进技术”，该架构已进入全面量产阶段。Rubin 架构以天文学家薇拉·鲁宾的名字命名，由六款协同工作的独立芯片组成。该系统的核心是 Rubin GPU，同时配备了专为“智能体推理”（Agentic Reasoning）设计的全新 Vera CPU。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在性能表现方面，Rubin 架构相较于前代产品实现了显著跨越。根据英伟达官方测试数据，Rubin 在 AI 模型训练任务上的运行速度是 Blackwell 架构的 3.5 倍；在推理任务中，其速度更是达到了前代的 5 倍，峰值运算能力高达 50 Petaflops。此外，新平台的能效表现同样优异，其每瓦推理算力提升了 8 倍。这一性能飞跃将为日益复杂的 AI 模型提供强大的算力支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，黄仁勋也介绍并推出了全新的 Alpamayo 1，是其视觉-语言-动作模型（VLA），结合因果链推理与轨迹规划，主要增强复杂驾驶场景中的决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元发布开源仿真平台Genie Sim 3.0&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智元机器人在CES国际消费电子展首日正式发布首个大语言模型驱动的开源仿真平台——Genie Sim 3.0。基于NVIDIA Isaac Sim，Genie Sim 3.0融合三维重建与视觉生成，打造数字孪生级的高保真环境；首创大语言模型驱动的场景泛化技术，让万级场景的生成只需几分钟；同步开源包含真实机器人作业场景的上万小时仿真数据集；并构建了覆盖10万+场景的多维度智能评估体系，为模型能力绘制全景画像。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 推出 ChatGPT Health 模式，为“健康 / 医疗”类型对话设立专属空间&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日消息，OpenAI 正式宣布推出 ChatGPT Health，该模式集成于 ChatGPT 中，号称是一个“专门用于与 ChatGPT 进行健康相关对话的独立空间”，预计将在未来几周内陆续向用户开放。OpenAI 称，目前平台每周有超过 2.3 亿人询问有关健康的问题，因此该公司推出了 ChatGPT Health 模式，旨在让用户更系统、更安全地讨论自身的健康问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，在 ChatGPT Health 模式下，系统会将用户的对话与其他普通聊天记录进行隔离，避免用户的健康背景在日常对话中被无意提及。如果用户在普通聊天中开始讨论健康问题，系统也会引导其切换到 Health 模式进行交流。同时，在 Health 模式下，AI 仍然可以参考用户在其他场景中的部分信息。ChatGPT Health 还将支持与个人信息及健康类应用的数据整合，包括 Apple Health（苹果健康）、Function 和 MyFitnessPal 等。OpenAI 强调，Health 模式中的对话内容不会被用于训练模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，ChatGPT 这样的“大模型”本质上是通过预测最可能的回答来生成内容，而不是基于对“真实与否”的判断，因此并不保证生成的医疗见解一定正确，OpenAI 也在其服务条款中明确指出，ChatGPT 仅供参考，不能够用于任何健康状况诊断 / 治疗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;雷鸟 CES 2026 推出全球首款 eSIM 功能 AR 智能眼镜 X3 Pro Project eSIM&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日消息，雷鸟在 CES 2026 中正式推出了全球首款支持 eSIM 功能的 AR 智能眼镜 X3 Pro Project eSIM，但并未公布价格和上市时间。据介绍，该产品采用双目全彩光机，可获得“等效 43 英寸的 3D 空间视觉观感”，同时产品搭载高通骁龙 AR 1 计算平台，内置 RayNeo AR 应用虚拟机，支持微信、抖音、B站等多款应用。此外，该产品搭载 eSIM 通信模块，使得 AR 眼镜首次真正具备脱离手机的能力，产品无需通过手机或 Wi-Fi，即可独立完成包括通话、实时 AI 对话、实时翻译、在线流媒体播放等功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;摩尔线程正式发布开源大模型分布式训练仿真工具SimuMax的1.1版本&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日，据摩尔线程消息，近日，摩尔线程正式发布开源大模型分布式训练仿真工具SimuMax的1.1版本。该版本在完整继承v1.0高精度仿真能力的基础上，实现了从单一工具到一体化全栈工作流平台的重要升级，为大模型训练的仿真与调优提供系统化支持。本次更新聚焦三大核心创新：用户友好的可视化配置界面、智能并行策略搜索，以及融合计算与通信效率建模的System-Config生成流水线。新版本同时提升了对主流训练框架Megatron-LM的兼容性，并增强了对混合并行训练中复杂通信行为的建模精度，使仿真环境更贴近真实生产场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 7 日，微创机器人依托神经元MicroGenius多模态自主手术大模型，成功完成了全球首例“大模型自主手术”动物实验。这一突破性成果不仅填补了全球大模型自主手术在体动物实验的技术空白，更推动全球AI产业在医疗领域的深度升级与跨界融合。1 月 6 日，波士顿动力与谷歌 DeepMind 宣布建立新的人工智能合作伙伴关系，目标将 Gemini Robotics 人工智能基础模型与波士顿动力的新型 Atlas 人形机器人集成。1 月 6 日，高通与谷歌宣布深化长达十年的汽车领域合作，双方将整合骁龙数字底盘解决方案与谷歌汽车软件及云服务能力，加速软件定义汽车落地，推动 AI 赋能的智能出行体验规模化普及。1 月 5 日，腾讯AI工作台ima.copilot迎来更新：正式上线“生成PPT”功能。用户只需进入“任务模式”，即可调用个人知识库中的素材，一键生成幻灯片。1 月 5 日，智元机器人已与MiniMax达成合作，MiniMax将为智元机器人提供文本到语音全流程AI技术支持。针对智元机器人的产品定位与功能特性，MiniMax为其量身打造专属人设体系，优化用户与机器人的语音交互体验。同时，基于人设体系构建定制化提示词策略，为用户生成专属音色，实现千人千面的个性化音色合成，满足多样化语音交互需求。此外，MiniMax还基于自研音乐生成模型，助力智元机器人拓展娱乐场景玩法。&lt;/p&gt;</description><link>https://www.infoq.cn/article/OVgxgVu4Tph8UfgKshIv</link><guid isPermaLink="false">https://www.infoq.cn/article/OVgxgVu4Tph8UfgKshIv</guid><pubDate>Mon, 12 Jan 2026 07:00:49 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>测试人员可以做些什么来确保软件安全</title><description>&lt;p&gt;&amp;nbsp;Sara Martinez在&lt;a href=&quot;https://www.onlinetestconf.com/&quot;&gt;Online TestConf&lt;/a&gt;&quot;上的演讲“&lt;a href=&quot;https://www.youtube.com/watch?v=L9ZWK0xBOoU&quot;&gt;确保软件安全&lt;/a&gt;&quot;”中说到，一个安全的软件开发生命周期意味着将安全融入到计划、设计、构建、测试和维护各个阶段，而不是在最后阶段才匆忙添加。测试人员不是漏洞查找者，而是早期的防御者，从第一个冲刺开始构建安全性和质量。文化第一，自动化第二，全程持续测试和监控；她认为，这就是如何让安全成为一种习惯，而不是紧急演练的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cwe.mitre.org/&quot;&gt;通用弱点枚举（Common Weakness Enumeration， CWE）统计数据&lt;/a&gt;&quot;显示，超过85%的软件弱点来自于我们如何实现代码，大约60%可以追溯到设计决策。Martinez说，这意味着产品的基础、架构和构建方式对产品的安全性有着巨大的影响。她补充说，一旦产品上线，就要密切关注它，运行漏洞扫描，并在问题出现时尽快修补，以领先于攻击者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;安全的软件开发生命周期看起来很像常规的SDLC，但每个步骤都内置了安全性，Martinez解释道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;* 它首先定义明确的安全需求，并在规划和设计时运行威胁建模。* 在开发过程中，遵循安全编码实践，审查依赖关系，并使用安全测试自动化工具或依赖项* 扫描器来尽早捕获弱点。* 测试超越了DAST、渗透测试和其他安全检查的功能，以发现真正的攻击路径。* 一旦产品上线，你就可以通过安全部署、持续监控和快速补丁管理来保证它的安全。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez认为，安全的软件从文化开始，就像质量一样。这不是一个清单，而是关于开发者、测试人员、运维人员和管理人员之间的责任分担：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;每个公司都应该创建适合其产品的行动计划，查看安全软件开发指南，并确保安全实践是日常工作的一部分。自动化是关键；将安全分析工具引入CI/CD管道，以便及早和一致地发现弱点。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez提到不要忘记测试的人为方面：添加与安全需求相关的特定功能测试用例，以便团队保持对诸如弱输入验证、风险角色和权限配置或访问控制等问题的警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez说，许多最严重的事件仍然来自旧的、众所周知的攻击，我们可以通过正确的工具和实践来预防这些攻击。现在，我们面临着新的挑战，比如与AI相关的漏洞，它们正在重塑格局：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，许多公司正在使用AI来生成代码，但他们没有扫描它或应用安全开发实践，因此他们最终将已知的漏洞引入到他们的产品中。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我学到了很多，但我知道我永远也学不完。安全性是一个移动的目标，安全性测试是一个持续的挑战，这正是使它成为一个如此迷人、不断变化的世界的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ就软件安全问题采访了&lt;a href=&quot;https://www.linkedin.com/in/saramartinezginer/&quot;&gt;Sara Martinez&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：测试人员在安全方面扮演什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Sara Martinez：测试员是我们拥有的最好的安全秘密武器之一。我认为我们的角色不仅仅是检查功能是否有效；我们很容易注意到可能变成大漏洞的小问题，比如弱输入验证、有风险的角色和权限配置，或者访问控制。&amp;nbsp;团队需要在安全软件开发生命周期（SSDLC）中共担安全责任，比如挑战安全需求、帮助进行威胁建模，以及运行静态和动态安全自动扫描以尽早发现问题。测试人员可以通过确保快速验证修复并集成到CI/CD中来保持管道中的安全性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：我们有哪些关于漏洞和弱点的数据，我们如何使用这些数据？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Martinez：像CWE （Common Weakness Enumeration）和CVE （Common Vulnerabilities and Exposures）这样的数据标准为我们提供了一种描述软件弱点和现实世界漏洞的共享语言。这些数据不仅仅用于报告；自动化扫描器实际上使用这些引用来检测代码和正在运行的应用程序中的漏洞。&amp;nbsp;我认为这也是发现攻击者趋势的好方法。在过去的几年里，顶级CVE一直被跨站点脚本（XSS）和SQL注入等问题所主导，这些问题继续影响着很大比例的软件产品。使用这些数据可以帮助团队确定测试的优先级，关注安全编码实践，并对攻击者真正利用的东西保持警惕。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/ensure-software-security/&quot;&gt;https://www.infoq.com/news/2026/01/ensure-software-security/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/aWcsjzxXhQcwf3hgWkz2</link><guid isPermaLink="false">https://www.infoq.cn/article/aWcsjzxXhQcwf3hgWkz2</guid><pubDate>Mon, 12 Jan 2026 06:51:00 GMT</pubDate><author>Ben Linders</author><category>安全</category></item><item><title>代理式终端——如何使用CLI智能体激活你的终端</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么命令行越来越具有代理式功能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统上，终端或shell是一种命令式工具，依赖于像 ls 、 grep 和 git 这样的预定义命令来执行特定指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，像&lt;a href=&quot;https://codelabs.developers.google.com/gemini-cli-hands-on&quot;&gt;Gemini CLI&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.claude.com/product/claude-code&quot;&gt;Claude Code&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT&lt;/a&gt;&quot;这样的代理性命令行工具的最新进展已经将这个简单的实用程序转变为一个更动态和智能的助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些代理式CLI工具允许用户用自然语言描述更高级的目标或任务，从而使简陋的shell栩栩如生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它们可以规划步骤，利用各种工具完成不同任务（例如文件处理、代码执行和网络搜索），对输出进行推理，并充当辅助驾驶以帮助完成任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这显著减少了用户的心智负担，并最大限度地减少了多个工具之间的上下文切换。至关重要的是，用户通过批准或指导智能体的过程来保持控制权，确保自动化和用户监督之间的平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在本文中，我们将探讨这些代理式工具的架构，对比不同的规划风格，如ReAct和计划-执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还将检查代理式工作流程的实际生命周期，从意图捕获到执行，并讨论可靠日常使用所需的关键安全护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;端到端代理式终端生命周期：一个提示，三个智能体&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然人工智能在开发中的兴起通常与聊天界面（如ChatGPT）和代理式IDE（如&lt;a href=&quot;https://cursor.com/&quot;&gt;Cursor&lt;/a&gt;&quot;）有关，但代理式CLI占据了一个独特的利基市场。基于IDE的智能体擅长于以丰富的视觉上下文为中心的代码任务，但它们通常局限于编辑器的窗口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CLI满足了开发人员管理基础设施和git工作流的需求：shell。这种无头的、可组合的特性允许它以GUI绑定代理无法做到的方式将工具和系统命令链接起来。然而，请注意，随着像Gemini CLI这样的智能体现在可以与IDE（如VSCode）集成以提供其建议的差异视图，这种区别正在变得模糊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了详细说明代理式终端工具的强大功能，让我们讨论一个运行示例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些标记文件封装了关于如何构建和测试repo的事实，以及文档和脚本的约定。他们基本上是代理的入职文件。例如，Gemini CLI的文件名为“Gemini.md”。Claude Code工具也使用了类似的约定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考虑一个常见场景，开发人员需要用标准文档和自动化脚本启动一个新的存储库。与其手动创建每个文件并编写样板代码，代理式CLI可以从单个高级指令处理整个过程，从而确保一致性并节省宝贵的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;输入提示：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;添加一个CONTRIBUTING.md，一个PULL_REQUEST_TEMPLATE.md，以及一个scripts/smoke-check.sh，运行一个可配置的命令并在失败时退出非零；更新README以记录两者，并打开一个PR。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单1：用户提示代码片段&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了理解这个指令是如何转化为行动的，我们将把代理式的工作流程分解为它的组成阶段。我们从意图捕获开始，其中智能体在项目的特定上下文中定位自己，然后转移到规划风格，对比不同模型架构其推理的方式。后续部分将详细说明执行实际工作的Tool Execution循环和防止自主事故的关键安全防护措施。最后，我们将看看结果如何呈现给用户，说明在不同的品牌名称下，大多数代理式工具共享一个共同的架构DNA。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段1：意图捕获和上下文形成&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了确保LLM的高质量提示，智能体首先收集所有必要的信息，然后进行规划或执行。这种方法包括几个步骤：将任务链接到当前工作目录，管理会话状态，并将每个项目的配置保存在dotfolders（例如， ./.gemin i和 ./.claude ）。这种方法消除了重复使用标志进行重复任务的需要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，指令还隐式地从各种位置获取。以下是CLI智能体除了用户的提示之外，从哪些主要信号源获取的一些：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;特定于文件夹的上下文文件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些是封装了有关你的存储库如何构建和测试以及你的文档和脚本约定的事实的markdown文件。它们本质上充当你智能体的入门文档。例如，Gemini CLI的文件称为 Gemini.md 。Claude Code工具也使用了类似的约定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;# GEMINI.md（摘录）
## 1. 工程哲学
这是一个高性能的SaaS后端。
* **核心原则：** 可读性优于聪明度。显式优于隐式。
* **架构：** 六边形架构（端口和适配器）。
* **安全性：** 零信任安全模型。所有输入必须通过Pydantic进行验证。
## 2. 技术栈和标准
* **语言：** Python 3.11+（需要严格类型）。
* **框架：** FastAPI（异步默认）。
* **数据库：** PostgreSQL（通过SQLAlchemy 2.0异步会话）。
* **测试：** Pytest（覆盖率必须保持&amp;gt;90%）。&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单2：Gemini.md示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;技能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期智能体的一个主要限制是需要将所有指令塞进上下文窗口。Anthropic的Claude Code引入了&lt;a href=&quot;https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills&quot;&gt;Skills&lt;/a&gt;&quot;的概念，它建立在上述markdown文件的想法之上，作为专业知识的模块化包（例如，PDF操作、数据分析和React最佳实践），作为包含 SKILL.md 的文件夹存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种包含使得渐进式披露成为可能：智能体最初只看到可用技能的名称/描述（消耗最少的词元）。然后，只有在用户的任务需要时，它才动态安装或读取完整的 SKILL.md 指令集。这种方法允许将智能体默认成为通才，但在需要时是专家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;代码库信号&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLI可以扫描现有的scripts/、 .github/ ，并拾取像 README.md 这样的文件工件，如果你已经提供了这些文件。基于像Python这样的语言的典型约定，它还可以查看像 pyproject.toml 这样的工件以获得高级概览。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;IDE焦点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一个可选步骤，如果你连接到像VSCode或Cursor这样的代码编辑器，可以用来打开文件和选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段2：规划风格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;加载上下文后，每个工具开始其控制循环：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini（&lt;a href=&quot;https://arxiv.org/abs/2210.03629&quot;&gt;ReAct&lt;/a&gt;&quot;风格）思考，调用工具，观察并重复，这非常适合发现缺失的文件夹或策略。这种迭代方法允许Gemini适应新信息并动态调整其策略，使其适用于需要灵活解决问题和探索的任务。Claude（&lt;a href=&quot;https://arxiv.org/abs/2305.04091&quot;&gt;计划和执行&lt;/a&gt;&quot;）提出一个你可以批准的清单，然后逐步执行计划，并带有策略钩子。这种方法提供了更高程度的控制和透明度，因为用户可以在执行前审查和修改计划，确保遵守特定的策略或偏好。Auto-GPT发出思考加上每个周期运行器执行的JSON命令。这种结构化输出便于自动化和与其他系统集成，因为JSON格式提供了智能体意图和行动的清晰且机器可读的表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下示例说明了不同智能体如何处理这个规划阶段。Claude提供了一个人类可读的清单供用户审批，而Auto-GPT生成了专为自动执行而设计的JSON输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Claude——计划预览&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;计划:
创建脚本/smoke-check.sh （POSIX sh；从env读取CMD；失败时退出非零）
创建CONTRIBUTING.md（如何在本地进行冒烟检查）
创建.github/PULL_REQUEST_TEMPLATE.md（检查清单包括冒烟检查）
更新README.md，添加scripts/和PR模板说明运行冒烟检查
运行smoke-check;提交;打开PR
批准吗?[是/否]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单3：Claude计划审查代码示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Auto-GPT——带有思考和命令的显式JSON&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;｛
“thoughts”： {&quot;text“: ”创建冒烟检查，文档，模板；更新README;运行脚本;提交/PR”},
&quot;command&quot;:{&quot;name&quot;:&quot;write_file&quot;,&quot;args&quot;:{&quot;path&quot;:&quot;scripts/smoke-check.sh&quot;,&quot;content&quot;:&quot;#!/bin/sh\n: \&quot;${CMD:=echo ok}\&quot; \n$CMD || { echo \&quot;smoke failed\&quot; &amp;gt;&amp;amp;2; exit 1; }\necho \&quot;ok\&quot;&quot;}}
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单4：Auto-GPT审查代码示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段3：工具调用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此阶段，智能体使用其库中的工具根据其任务提出更改建议。例如，这可能涉及使用文件编辑工具在IDE中显示差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具已经从专有实现发展为开放标准：模型上下文协议（ &lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol，MCP&lt;/a&gt;&quot;）。在Anthropic、谷歌和其他组织的支持下，MCP就像AI应用程序的USB-C端口。而不是硬编码集成每个数据库或API，（例如，用于PostgreSQL、Slack或GitHub的服务）。CLI智能体在启动时自动发现这些资源，允许单个智能体在一个无缝的工作流程中查询你的生产数据库，阅读你的线性票证，并编辑代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;示例冒烟脚本的Diff&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;*** scripts/smoke-check.sh 
+#!/bin/sh 
+set -eu 
+# CMD可以被覆盖：CMD=&quot;make test&quot; ./scripts/smoke-check.sh 
+: &quot;${CMD:=printf ok}&quot; 
+$CMD &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || { echo &quot;smoke failed&quot; &amp;gt;&amp;amp;2; exit 1; } 
+echo &quot;ok&quot;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单5：冒烟脚本diff示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude的钩子是一种明确策略的干净方式——限制写入路径、自动chmod脚本、在写入后运行lint/tests——而不需要将其塞入提示中。Gemini通过&lt;a href=&quot;https://geminicli.com/extensions/&quot;&gt;扩展&lt;/a&gt;&quot;和MCP获得类似的杠杆作用：不同的旋钮，类似的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段4：人为干预的安全和护栏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你保留了对冒险行为的控制。Gemini在执行写入或具有副作用的shell命令之前需要你的批准。Claude提供了确认和钩子，允许你阻止违反策略的写操作，或者在继续之前自动运行检查。Auto-GPT暂停是/否确认，除非启用连续模式。为了进行探索，激活一个&lt;a href=&quot;https://geminicli.com/docs/cli/sandbox/&quot;&gt;容器化的沙箱&lt;/a&gt;&quot;来隔离文件系统和进程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段5：执行和迭代：真正完成工作的循环&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建文件后，智能体执行脚本并根据结果进行调整。例如，如果缺少scripts目录，Gemini将创建它并再次尝试操作。如果脚本缺乏可执行权限，Claude的集成钩子会自动应用 chmod +x 命令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;脚本在观察、推理和操作的连续循环中执行。这个循环不断重复，直到本地执行成功并完成文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段6：渲染结果和停止条件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLI提供了一个清晰的、语法高亮显示的工具调用和文件差异视图。用户可以在编辑器中打开这些差异，手动进行调整，或者指示智能体进行适当的更改。批量批准是最有效的，例如在单个批准之前一起审查所有脚本和文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一次成功的冒烟检查之后，通过批准的差异，智能体将创建一个新的分支，提交更改，并打开一个PR草案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;如何在你的工作流程中利用代理式CLI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是一些实用技巧，帮助你在工作流程中充分利用这些工具：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将上下文文件视为构建资产&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将GEMINI.md和CLAUDE.md文件与你的README文件一起&lt;a href=&quot;https://google-gemini.github.io/gemini-cli/docs/cli/gemini-md.html&quot;&gt;维护&lt;/a&gt;&quot;。这些文件应该简洁且专注于特定细节，包括构建和测试程序、配置位置、任何特定于存储库的问题以及安全编辑的目录。你甚至可以使用智能体生成初始草稿。将这些文件视为为代理式编程环境的方式，而不是另一个需要持续监督的提示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;积极地限定范围&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将智能体指向实际重要的文件夹（例如services/payments/，而不是整个单体仓库）并传递明确的@file提示以指向热点。更紧凑的范围意味着更紧凑的差异，更少的创造性幻觉和更快的迭代。如果任务确实跨越多个包，请在提示中列举它们，以防止智能体进行详尽的扫描。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用沙箱避免对环境的意外更改&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini CLI提供了一个&lt;a href=&quot;https://geminicli.com/docs/cli/sandbox/&quot;&gt;沙箱模式&lt;/a&gt;&quot;，用于shell/file工具的临时、容器化执行。这保护了你的主机系统，限制对挂载的工作目录的写入，并控制网络访问。它非常适合无风险的探索，但不会消除对破坏性命令的审批提示，不能编辑已安装的秘密，也不能防止模型建议有风险的操作。你仍然是最终的仲裁者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Code通常在容器化开发环境（&lt;a href=&quot;https://code.claude.com/docs/en/devcontainer&quot;&gt;Dev Container/Docker&lt;/a&gt;&quot;）中运行，或使用插件/钩子将shell/file操作通过容器化运行器路由。这提供了类似的隔离（写入限制在挂载路径，控制环境，确定性工具链）。然而，这种隔离并不具有回溯性；如果允许，它不会阻止对挂载的秘密或暴露路径的意外写入。使用钩子来强制执行路径限制，并在写入最终确定之前自动运行测试/lint。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Auto-GPT没有专门的沙箱模式标志，但强烈建议在Docker容器中运行它。这确保了其文件系统操作与你的主机操作系统隔离，防止对你主要环境的任何意外更改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用符合你需求的工具&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini CLI非常适合深入集成到谷歌生态系统中的用户。它作为一个通用工具，擅长于发现繁重的任务，包括代码编辑、文档更新、小的shell操作（如列出目录和移动文件）、快速网络研究以及探索性的解决问题。它的ReAct循环促进了自然的探索和迭代工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Code最适合需要具体计划和强大编码能力的任务。这包括多文件重构、通过钩子执行策略、Git原生工作流程（分支、变基、冲突解决）和透明的护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/features/copilot/cli&quot;&gt;GitHub Copilot CLI&lt;/a&gt;&quot;旨在为快速、存储库感知的自然语言到shell辅助提供支持。它非常适合生成一次性命令、搭建测试、搜索代码以及起草提交和拉取请求，所有这些都不会破坏现有的GitHub工作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他工具包括&lt;a href=&quot;https://github.com/Aider-AI/aider&quot;&gt;Aider&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/openinterpreter/open-interpreter&quot;&gt;Open Interpreter&lt;/a&gt;&quot;和本地优先CLI。当你需要对实现有更大的控制权，并且有高度特定的需求，如紧密的Git人机工程学、本地LLM或不受限制的shell环境时，可以考虑这些选项。这些工具对于喜欢较少护栏和更快修改工具本身的经验丰富的用户来说非常有用，特别是对于较小的存储库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像工程师一样提示，不要写论文&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好的提示主要是关于清晰的合同，而不是华丽的散文。使用包含以下详细信息的四部分提示。从一个高层次的目标开始，用一句话陈述你的意图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;定义约束，包括范围（例如，“仅services/billing”）、风格（“POSIX sh; no bashisms”）和安全协议（“写入前询问”）。确定所需的工件，指定预期的结果（例如，文件、测试、README/PR文本）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确定检查，概述将如何衡量成功（例如，测试命令、验收标准）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像任何其他自动化一样进行仪表化&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了优化智能体性能，监控关键指标，如PR周期时间、智能体生成的差异大小、需要返工的PR百分比以及智能体编辑后不稳定测试的频率等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些指标作为反馈，不仅用于智能体的整体有效性，还用于你自己的运维合同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;接下来是什么&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代理式CLI正在从简单的shell助手演变为将你的工作工具、操作系统和云基础设施统一起来的连接组织。以下是截至本文撰写时的一些新兴趋势：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IDE和操作系统正在融合成统一的Agent Surfaces。像&lt;a href=&quot;https://windsurf.com/&quot;&gt;Windsurf&lt;/a&gt;&quot;和Cursor这样的工具允许智能体在终端、编辑器和运行过程中共享上下文，而不是作为孤立的聊天窗口运行。Windows也在其体验中&lt;a href=&quot;https://techcommunity.microsoft.com/blog/windows-itpro-blog/evolving-windows-new-copilot-and-ai-experiences-at-ignite-2025/4469466&quot;&gt;注入了大语言模型驱动的交互&lt;/a&gt;&quot;。智能体正在从响应式CLI转变为持久的后台服务。这些守护进程智能体不是等待输入，而是主动监控日志文件和本地服务器，仅在出现错误时介入修复计划。虽然这些工具（如GitHub PR上的Copilot）仍处于起步阶段，它们的洞察力尚浅，但随着正确的集成，它们将不断改进。扩展正在成为代理能力的App Store。随着CLI智能体的&lt;a href=&quot;https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills&quot;&gt;技能&lt;/a&gt;&quot;和&lt;a href=&quot;https://geminicli.com/extensions/&quot;&gt;扩展&lt;/a&gt;&quot;等创新，我们正在开发新一代的App Store，让用户可以将适当的能力插入到他们的智能体中。这也模糊了通用智能体和专业智能体之间的区别，因为专业智能体只是一个拥有正确知识和工具的强大通用智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/agentic-terminal-cli-agents/&quot;&gt;https://www.infoq.com/articles/agentic-terminal-cli-agents/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t712SX2KtxLUAF9rZZ6y</link><guid isPermaLink="false">https://www.infoq.cn/article/t712SX2KtxLUAF9rZZ6y</guid><pubDate>Mon, 12 Jan 2026 06:09:35 GMT</pubDate><author>作者：Sachin Joglekar</author><category>框架</category><category>AI&amp;大模型</category></item><item><title>“机器人一次性卖完太亏！”真机智能刘智勇：今年中国本体厂商将大淘汰，拼的是世界模型</title><description>&lt;p&gt;作者 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们采访了真机智能董事长兼首席科学家刘智勇，听他讲述了视觉语言导航（VLN）技术的当前难题、具身智能领域在 2025 年的各类进展以及今年在能力边界上的两个突破方向和技术决胜点。他表示，一旦世界模型的因果推理能力取得突破，无论是机器人的安全性还是行为和推理的安全性问题，都能得到很好的解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026年本体厂商肯定会收缩，估计中国最终只会剩下5到8家本体机器人公司。”他指出，核心是在某个单一场景实现盈利，不是毛利而是不依赖大量售后成本的净利。但单纯的整机销售并非很好的商业模式，如果只卖硬件，后续的售后压力会非常大，用户一次性付太多钱也承受不了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;VLN和世界模型上“大分”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：2025年具身智能领域有哪些突破性进展让您印象深刻，包括技术、产业化和生态建设上？这些进展是否已经为具身智能从实验室走向特定场景的“初步普及”奠定了基础？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：我印象比较深刻的是 VLN 方向的相关进展。过去我们主要是以 SLAM 为核心的技术路线，但从去年到现在，涌现出了大量基于视觉语言作为多模态输入的导航模型。这种视觉语言模型能解决零样本泛化的问题，我们不再需要预先构建地图了。把一个机器人放到任何全新的固定场景里，它都能实现零样本泛化，自主完成导航任务。另外，像UniNavid、ETPNav、FSR - VLN这些代表性工作，也让机器人门到门配送的实现出现了曙光和可能性。这就是从几何测量的导航范式，转变到学习增强的导航范式。当前的瓶颈在于未达极高的导航成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从场景普及的角度来说，核心是我们不再需要预先建图了。这就意味着，把机器人放在任何新的位置上，它都能立刻开始工作，直接解锁了很多之前无法覆盖的场景。最关键的一点是，零样本能力等同于部署成本的大幅降低。部署成本降下来之后，整个成本结构就能适配场景化的盈利模式，这正是为场景普及奠定的核心基础。技术成熟后，前期的准备和部署工作会大幅减少，这也为未来的产业发展打下了很好的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：具身智能的核心技术栈正在如何演变？2025年这一年有哪些值得关注的新范式或共识？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：从算法角度来看，核心变化是从之前感知、决策、执行分离的多模块化范式，逐渐转向 VLN 或 VLA 的端到端统一范式。从数据角度来说，发展方向是从单纯的真实数据采集，逐步转向合成数据、离线轨迹挖掘以及世界模型这些领域。训练范式也发生了改变，从强化学习调参慢慢转向世界模型驱动。现在世界模型算是行业内解决数据问题的一个共识，原因很简单，不管是在长程层面模拟预测未来状态、在底层层面预测动态物体轨迹，还是弥补数据的 corner case，世界模型都起到了不可或缺的作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：世界模型被寄予厚望，被认为是实现高级推理和规划的关键。现阶段来看，它对机器人实际能力的提升体现在何处？之后还有哪些方面的潜力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：现阶段来看，主要体现在三个方面。第一，机器人执行长程任务时容易陷入短视困境，而世界模型可以模拟未来的长程状态，对全局规划能力有非常重要的提升；第二，动态环境下静态地图容易失效，无法准确指引路径轨迹，世界模型能够预测动态物体的轨迹，让机器人的本地行动更安全；第三，世界模型能较好地生成相关数据，减少数据泛化鸿沟。我们认为，世界模型是 VLN 突破长程规划和动态适应瓶颈的充分非必要条件。但现在世界模型的主要问题是黑盒，而非白盒可微。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：大模型的快速发展，为具身智能的“智能”部分带来了哪些质变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：从我们的实践来看，最核心的变化是导航和路径规划的技术范式发生了转变。过去我们采用的是 SLAM 方案，现在则转向了VLN范式。过去的 SLAM 方案存在几个明显的局限，一是方案本身不具备语义理解能力，二是依赖静态地图，必须预先建图才能使用，三是需要对特定的传感器做专门标定。而 VLN 范式完全不同，它可以结合语言和视觉实现语义层面的理解，同时能应对非静态环境，实现动态适配。更关键的是，这个方案不再依赖高规格的激光雷达，也不需要预先部署地图，成本和效率都实现了大幅优化。大模型的快速发展，推动技术范式从几何测量的 SLAM 转向学习增强的 VLN，这正是带来质变的核心原因。行动、观测和语言本来属于三个空间，现在要把三个空间统一起来，这也是目前的核心难点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;大规模落地现在卡在哪儿？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：几乎所有专家都指出，高质量、大规模的物理交互数据稀缺是当前最大瓶颈。面对真实数据采集成本高昂的困境，仿真合成数据、人类视频数据等替代方案能走多远？“数据工厂”是可行的解决方案吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：我们面临的主要数据瓶颈有两个，一是数据的场景覆盖不足，比如现在常用的数据集大多基于 Mate Port 3D、Habitat、AI2THOR等 构建，只包含 固定的训练环境，场景覆盖肯定不够；二是做 VLN 的数据采集成本很高，有时需要 3D 数据采集，标注成本也比 2D 图像高出一个量级。对 VLN 来说，现在数据是完全不足的，既存在场景覆盖问题，又有成本高昂的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，我们在采用多种数据解决方案。第一是采集真实数据，采集 RGBD 视频流，以及数字手套等，再结合人工标注指令，像 Atomic 和一些基准数据集的主要来源就是真机数据。第二是比较常见的用仿真器生成，比如借助模拟器搭载 3D 场景库，批量生成视觉语言轨迹三元组。第三是采用 新范式，不用额外改动 3D 环境，通过改写人类标注数据的方式生成新样本，这是一种静态片段生成的新范式。另外，未来还有一种发展方向是离线数据、离线轨迹挖掘的方式，有点类似实行微克隆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：当前的硬件如灵巧手、关节驱动、传感器等，在哪些方面最能满足机器人的技术需求？又在哪些方面构成了发展的主要制约？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：要讲满足技术需求的地方，我们可以和轮式机器人做个比较。之前的轮式机器人只能移动到楼下，没办法开单元门、摁电梯，只能在楼下送货或者在室内移动。而现在的灵巧手、一体化关节，再加上一些触觉传感器，能让机器人具备开门、按电梯的能力，这是轮式机器人到人形机器人的一个巨大转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过目前硬件也存在几方面的制约。第一，我们还需要高分辨率的柔性触觉皮肤。因为机器人需要用机械灵巧手摁电梯，如果触觉不够灵敏，盲按的波动率大，成功率就会比较低。第二，门把手的种类太多了，如果机器人没有触觉反馈，根本没办法应对成千上万种门的情况，也很难实现场景泛化。再就是机器人要进行成千上万次的反复操作，电机、执行器、丝杠这些部件的脆弱性，可能在我们的应用场景中被放大 100 倍。所以从硬件角度来讲，目前主要的制约就是开门要做得好、触觉要做得好这两点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：目前为止，制约具身智能大规模落地应用难题还有哪些？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：对于我们的 VLN 技术来说，主要有两方面的难题。第一是感知决策的延迟问题，这甚至可能是致命的。简单来说，长程规划和行动频率的匹配很关键，如果感知和决策环节出现延迟，机器人在开放环境中运作就会遇到很多麻烦，这就要求必须在端侧做好部署。第二是硬件性能短板，既要让硬件能灵敏地感知外部世界，又要保证它能反复进行操作，而目前这类硬件的耐疲劳性、反脆弱性能还不够强。对于世界模型来说，核心瓶颈是隐式神经表征，而非显式3D高斯，可能在开门和按键上缺少精准几何信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;具身智能该告别 “一锤子买卖”？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：面对这样的机遇与挑战，您们在接下来一年的战略重点和核心发力方向是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：真机智能其实分成了北京真机和苏州真机两个公司。北京真机关注的还是比较传统的 SLAM 加轮式机器人的技术栈和方案，苏州真机则聚焦于 VLN 加人形机器人的技术栈及方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;苏州真机接下来有两个关注重点，第一是通过视觉语言导航的方式，实现无需额外提前部署的门到门配送。过去部署成本太高了，大概占了整个机器人售价成本的 38% 左右。我们希望能实现零样本泛化，换句话说，就是让机器人能够直接理解环境，直接完成导航任务。第二是全身运动控制，要解决的核心问题是开门。之前的控制是基于机器人静态的假设来实现的，哪怕是协作机器人也是保持自身不动去拉开门，这种方式需要的扭矩非常大。我们希望通过全身控制打破静态平衡的限制，依靠动态平衡的方法更泛化地解决开门的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这两个点结合起来，我们既能实现无需预先建图的门到门配送任务，同时又能解决开门和按电梯的任务。这两个方案结合之后，就可以实现最后五公里的门到门配送，既能开门、操作电梯，又能以无建图、无 GPS 的方式完成导航。室内本身没有 GPS 信号，但又需要实现导航，这时候视觉和语言理解的作用就非常关键了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：除了直接销售机器人整机，具身智能未来的商业模式可能有哪些创新？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：整机销售和租赁这两种方式都会存在。但我个人觉得，单纯的整机销售并不是很好的商业模式，更好的方式是 “整机销售 + 每年服务费” 的组合模式。如果只卖硬件，一次性卖完其实很亏，后续的售后压力会非常大。“整机销售 + 每年服务费” 就比较合理，既能保证长期的最大收益，又能解决售后问题，还能让设备商一次性回本。通过这种组合模式，能把原本不赚钱的 “卖铁生意”，变成能持续盈利的长期现金流生意。另一方面，用户一次性付太多钱确实承受不了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，未来还可能出现按单收费的商业模式。比如人形配送机器人测算下来每单成本能控制在两到三元人民币，和达达这类上游公司合作，机器人完成一单就赚一笔费用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;本体厂商大收缩，要拼什么？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：到2026 年，我们有望看到具身智能在能力边界上实现怎样的突破？整个具身智能领域的技术决胜点可能会是什么方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：2026年可能会有两个关键突破方向。第一是机器人在非结构化场景中实现稳定作业。要做到这一点，需要机器人具备一定的社交行为表现和自主导航能力。解决了之后，一些之前没想到的非结构化环境下的任务机器人也可能完成了。目前行业内大多还聚焦在结构化环境，所以这会是一个重要突破。第二是突破莫拉维克悖论（Moravec&#39;s Paradox）。以往大家觉得，机器能完成人类觉得难的事，但难以完成人类觉得简单的事，而2026年可能机器人也能胜任这类任务，会在人类觉得简单的事情上取得突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于技术决胜点，我认为有几个关键因素，其中最重要的是世界模型的因果推理能力。一旦这项能力取得突破，无论是机器人的安全性还是行为和推理的安全性问题，都能得到很好的解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：2026年，全球具身智能公司的竞争情况将如何变化？中国公司与国际巨头各自的优势和赛点分别会在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：2026年本体厂商肯定会收缩，马太效应会非常明显，估计中国最终只会剩下5到8家本体机器人公司。不过应用场景相关的公司和上游企业会多一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国和国际企业的优势不一样，国际公司的大模型技术更先进，基础模型能力更强，国内企业还处在追赶状态，但中国企业拥有供应链成本优势。另外竞争维度也在升级，现在大家可能还在追求单点技术的先进性，到了2026年，整体系统的效率会变得更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于赛点，我觉得核心是在某个单一场景实现盈利，不是毛利而是不依赖大量售后成本的净利。谁能做到这一点，谁就能形成数据飞轮，有了数据之后，模型和方法能力会进一步提升，之后再推进跨场景复制。&lt;/p&gt;</description><link>https://www.infoq.cn/article/EbQ24746fmKpCj7qJmIZ</link><guid isPermaLink="false">https://www.infoq.cn/article/EbQ24746fmKpCj7qJmIZ</guid><pubDate>Mon, 12 Jan 2026 01:55:28 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>发生在CES的六场对话：来自深圳的 AI 硬件“外卷”</title><description>&lt;p&gt;2026年CES开幕当天，拉斯维加斯的Uber司机望着展馆外的人流脱口而出：“中国人太多了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;是的，中国硬件力量正以空前力量和密度涌向全球舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在机器人展区，星动纪元技术VP侯伟的感受更为具体：“国内一线的厂商都来了。而且双足机器人这个赛道，非中国厂商展现方式都非常保守，没有炫酷的动作，甚至做&amp;nbsp;live&amp;nbsp;demo&amp;nbsp;的也很少。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中国已经成为全球硬件供应链的重要基地，新兴硬件品牌也基于中国的供应链优势不断涌现。他们的目标是，到全球去，以及用AI重做硬件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但热情中也弥漫着一丝紧张。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI陪伴硬件创业公司Ludens&amp;nbsp;AI创始人薛立君回忆，一个美国白人记者反复质问展区内所有亚洲面孔：“你们的供应链是不是在深圳？你们团队内有没有中国工程师？我们的数据被储存在哪里？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种焦虑在数据上也有印证：2026年以中国注册地址参展的企业为935家，虽仍占总数近20%，却较2025年的1300余家大幅下滑。有品牌高管向InfoQ坦言，签证障碍是缺席的主要原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，这些阻力并未浇灭全球市场对中国AI硬件的热情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;乐享科技旗下新品牌“元点智能”刚在CES做完发布会，就已经收获欧美本地连锁渠道采购商和其他代理商的合作意向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI&amp;nbsp;仅带两个原型机参展，在产品还未上众筹平台的情况下，2000张名片就快要发完了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI厨电品牌万得厨（wan&amp;nbsp;AIChef）首次参展CES，品牌管理总监林燕妮向InfoQ表示，团队原本以调研北美市场为主要目的，却意外收到大量本地代理商和售后服务商的接洽，并吸引了包括Best&amp;nbsp;Buy和Home&amp;nbsp;Depot在内的多家大型零售商的合作意向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些信号共同指向一个事实：尽管地缘政治带来摩擦，市场仍然被产品本身的价值与体验打动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这个背景下，AI硬件在2026年CES呈现出新的趋势：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一些曾被认为是前景不明的“小玩意”，正被大厂重新定义为Physical&amp;nbsp;AI生态的关键入口。AI硬件的初创公司，则更加细致地从数据、场景、人群等方面，思考自己的生存之道。机器人如何融入物理世界这一点更加清晰了，家用具身智能已经有了明确的商业目标。除了人形机器人厂商，卡特彼勒、西门子这样的工业巨头也来了，迫切地想告诉所有人：我们不仅跟上了AI的发展，还在支撑着&amp;nbsp;AI所需要的物理基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94b0be2f8152f03c62f28fca72484311.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;大厂盯上小玩意，AI硬件更卷了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;曾经被视为边缘、小众甚至“玩具级”的硬件品类，正被科技巨头重新定义为下一代AI生态的战略入口。智能戒指、宠物机器人、健康手环，如今成了连接用户、数据与多场景服务的关键工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年年初，追觅正式成立AI硬件事业部，并将智能戒指作为首个突破口。近日，团队在CES展出了三款不同功能的智能戒指、一款血压手表。震动AI智能戒指侧重主动交互，生态互联AI戒指具备NFC的功能，健康戒指、血压手表则会更加专注于健康监测。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些产品指向同一目标：让硬件从家庭场景，延伸至用户24小时的生活。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在追觅生态内，戒指的定位是开启追觅生态链的钥匙，”追觅AI硬件事业部负责人潘志东向InfoQ解释，“智能戒指是可以被用户24小时无感佩戴的产品，可以获得用户毫秒级的数据反馈，在未来十年会比其他品类的穿戴设备被更多人使用。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d1/d16eb7330bc01937533792917a803b84.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在潘志东看来，智能家居与AI硬件的本质都是智能互联，区别在于覆盖半径：前者聚焦居住空间的舒适与便捷，后者则贯穿户外、办公、睡眠、健康等多元场景。未来，戒指期待与追觅生态其他产品深度联动。例如用户回家前轻触戒指，扫地机即提前启动，到家时，灯光、音乐可根据戒指监测到的用户状态自动调节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术层面，追觅AI智能戒指依托品牌在高速马达领域的深厚积累，将微型马达集成进戒指结构，需解决空间限制、震动对结构稳定性的影响以及功耗控制等问题。目前，震动功能已支持心率异常提醒、消息通知、拍照等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，追觅的野心还在向办公等场景延伸。潘志东表示，团队正在开发一款记录戒指，可以记录会议和日常生活高光时刻。“可以把它想象为一个空间戒指，可以随时储存你的灵感。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，市场已经给出积极反馈：2025年10月追觅智能戒指开始在国内低调预售。2025年12月，追觅戒指在天猫智能指环热销榜登顶，超越RingConn、华米等品牌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/bafa3d707d0b7970cfde52b0b5ca72ba.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图片为天猫官网12.29截图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说追觅试图通过贴身设备延伸对“人”的覆盖，那么涂鸦智能则在构建一个跨场景的&amp;nbsp;AI&amp;nbsp;硬件协同网络。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在本届&amp;nbsp;CES&amp;nbsp;上，涂鸦智能发布了其&amp;nbsp;AI&amp;nbsp;宠物陪伴机器人&amp;nbsp;Aura，并同步推出&amp;nbsp;AI&amp;nbsp;生活助手&amp;nbsp;HeyTuya。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，涂鸦此前鲜少推出硬件产品，此次推出Aura是涂鸦在AI硬件消费上的一大尝试&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;涂鸦智能&amp;nbsp;CMO&amp;nbsp;那竞丹在社交媒体中表示，Aura&amp;nbsp;并不是终点，而是家庭陪伴机器人迈向长期化、通用化的起点。“宠物陪伴只是入口，系统会自然延展到喂养管理、行为分析和健康监测，背后是一套清晰的三支柱体系。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/15/153b320d3e40490448257bfb9ed1cf54.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;用户在现场体验涂鸦智能&amp;nbsp;AI&amp;nbsp;宠物陪伴机器人&amp;nbsp;Aura&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比具体产品，更值得关注的是&amp;nbsp;HeyTuya&amp;nbsp;所指向的方向。在&amp;nbsp;AIoT&amp;nbsp;的上一个周期里，涂鸦更多扮演的是幕后角色：通过模组和云服务，帮助厂商快速搭建智能硬件。而现在，它试图更进一步，不再只赋能单一设备，而是作为智能家居的统一入口，让不同硬件具备&amp;nbsp;Agent&amp;nbsp;能力，并在场景中协同工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们不只是帮厂家做出智能空调或冰箱，”涂鸦智能产品总监虞翔对&amp;nbsp;InfoQ&amp;nbsp;表示，“HeyTuya&amp;nbsp;是一个完整的‘AI&amp;nbsp;生活场景’&amp;nbsp;demo。它展示了如何把安防、健康、作息这些原本割裂的功能，整合成一个主动为用户服务的整体体验。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管“用&amp;nbsp;AI&amp;nbsp;把硬件重做一遍”已被反复讨论，但行业对&amp;nbsp;Agent&amp;nbsp;硬件的形态和边界，仍未形成共识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在虞翔看来，短期内最有机会跑通的，是两类&amp;nbsp;Agent&amp;nbsp;硬件：一类是情感陪伴型产品，如陪伴机器人或高级玩具，它们直接回应人的情绪需求，市场已经开始用真实使用行为给出反馈；另一类是垂直领域的“专家助理”，例如家庭节能管家，能在具体场景中提供可量化、可感知的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让这些产品真正成立，产业仍需补齐两项基础能力：一是作为“神经系统”的稳定连接与长期记忆，让&amp;nbsp;Agent&amp;nbsp;能实时联通设备并理解用户习惯；二是作为“骨骼系统”的生态整合能力，单个&amp;nbsp;Agent&amp;nbsp;的价值应该建立在开放、可互操作的设备网络之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI硬件初创公司的生存之道：数据、场景、交互&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当大厂凭借生态优势将智能戒指、陪伴机器人等“小玩意”纳入Physical&amp;nbsp;AI&amp;nbsp;版图，AI硬件初创公司不得不在更狭窄的缝隙中寻找立足点。它们必须围绕数据、场景和交互构筑护城河。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来智能&amp;nbsp;COO&amp;nbsp;王超认为，在算力、算法和数据这三个要素中，真正长期构成瓶颈的，并不是前两者，而是高质量、强场景约束的私有数据——尤其是那些无法通过互联网获取、只能依赖物理设备在真实世界中持续采集的信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“AI&amp;nbsp;耳机是物理世界的耳朵，”王超形容，“它把声音转化为结构化文本，而这些数据，是大模型公司过去很难直接触达的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王超认为，AI&amp;nbsp;硬件初创公司的关键不在于“把&amp;nbsp;AI&amp;nbsp;做进硬件”，而在于所选择的方向——是否必须依赖某一种特定硬件形态，才能完成特定场景的数据采集、处理与理解，并且这一过程难以被大厂的通用软件方案替代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以办公场景为例，未来智能通过&amp;nbsp;AI&amp;nbsp;耳机切入会议、沟通与决策环境，所积累的数据本身是脱敏的，但一旦被结构化，就可以持续服务于企业内部的协作效率与管理流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/a3/3b/a39ea41903715268153ffe218f8c063b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Youtube科技博主&quot;Mark&amp;nbsp;Ellis&amp;nbsp;Reviews&quot;&amp;nbsp;在CES现场体验拍摄未来智能产品&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;CES&amp;nbsp;现场，王超还感受到&amp;nbsp;Physical&amp;nbsp;AI&amp;nbsp;正在从概念走向工程现实的强烈信号。这一趋势背后，有两个变化尤为明显，一是底层技术与供应链正在重构，包括端侧算力方案演进与存储成本波动；二是硬件必须能以更自然的交互方式，自然融入既有生活环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，芯片算力的提升为算法优化打开了空间。例如降噪能根据环境变化自动动态调整，甚至耳机可以扫描用户耳道结构，实现降噪方案的个性化定制。与此同时，高存储需求正受到供应链压力：存储芯片短缺与价格上涨，已对AI耳机行业部分高配置耳机产品造成直接影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在交互方式上，未来智能将聚焦语音交互。其后续产品研发也计划引入&amp;nbsp;Agent&amp;nbsp;式的主动服务能力。例如，当一位高管在会议中专注讨论时，AI&amp;nbsp;耳机可智能判断不断涌入的邮件与审批请求是否“紧急且需立即处理”，并通过耳机及时提醒；用户仅需简单手势即可完成反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但要实现这类体验，耳机的独立联网与端侧运算能力变得重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王超指出，目前市面上所有的“AI&amp;nbsp;耳机”实际上仍需依赖手机&amp;nbsp;App&amp;nbsp;进行语音转写，一旦应用退至后台或受到系统权限限制，就可能导致功能中断。相比之下，若任务能在设备端完成，再与云端协同，体验将显著更稳定。未来智能也在尝试端侧AI的探索，当前虽然还无法实现真正的独立，但已经在通过一些方案规避移动端的不确定性，比如离线闪录，通过耳机或充电仓直连云端&amp;nbsp;AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一步，未来智能计划通过多端协同，围绕办公场景提供更加连续的服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;私有数据、极致场景与交互体验，这三个关键词，同样出现在&amp;nbsp;AI&amp;nbsp;陪伴硬件初创公司&amp;nbsp;Ludens&amp;nbsp;AI&amp;nbsp;的关键思考中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI创始人薛立君介绍，家庭场景的数据高度敏感，因此，在产品研发中，团队注重在真实环境中的测试，积累关于人机共处节奏、情绪反馈模式和空间行为习惯的原始数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正因如此，Ludens&amp;nbsp;AI&amp;nbsp;采用纯端侧AI模型。一方面是因为家庭场景对隐私泄露的天然警惕；另一方面，则是为了实现自然的交互感。Ludens&amp;nbsp;AI&amp;nbsp;旗下机器人可以达到50–100ms&amp;nbsp;级别的实时响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计背后，是对场景的细致拆解。创始人薛立君认为，虽然AI陪伴硬件和AI、机器人强相关，长期来看，它的本质是强文化属性赛道。例如，一个面向日本独居老人的机器人，其互动节奏、声音语调与唤醒逻辑，必然不同于面向欧美儿童的版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbef499256780ada300a47215347e7c6.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI展台，观众体验&amp;nbsp;Inu和Cocomo原型机&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在交互体验上，Ludens&amp;nbsp;AI强调将“玩”作为建立情感连接的核心机制。因此，机器人在理解环境、人物状态的情况下，能主动做出交互反应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026&amp;nbsp;年，Ludens&amp;nbsp;AI&amp;nbsp;计划通过两次众筹测试市场需求。薛立君认为，对于陪伴类产品而言，持续交互的频次和时长，是判断&amp;nbsp;PMF&amp;nbsp;的关键指标。出于伦理考虑，公司不会对对话和记忆能力设置订阅门槛，以避免“停止付费即失去陪伴”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;机器人，从秀场到工厂和家庭&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;走进拉斯维加斯的机器人展馆，最直观的感受是：这里几乎被中国公司“占领”了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从参展格局看，中、美、韩三国构成了具身智能的核心力量。韩国甚至以国家代表团形式集体亮相。星动纪元技术&amp;nbsp;VP&amp;nbsp;侯伟评价，尽管韩国本土机器人企业数量不多，但在灵巧手、轮式底盘和运动控制等细分领域积累深厚；美国的优势依然集中在原生技术与基础研发；而中国最突出的竞争力，则体现在硬件制造能力，尤其是机器人本体制造上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种差异，在展台呈现中表现得尤为明显。侯伟提到，今年&amp;nbsp;CES&amp;nbsp;上，非中国厂商的展示明显更为保守，大多停留在慢速行走或简单动作，很少进行长时间、连续的实时演示。当被问及原因时，有些团队给出的回答显得颇为含糊：“现在的机器人就是这个状态，但还有很多可能性……只是没带过来。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/5502941583f36a7d94db0eaa9f6a3af2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;星动L7在CES&amp;nbsp;2026现场为嘉宾拍摄Vlog&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但比“谁展示了什么”更重要的，是谁开始认真询问、试图把机器人放进真实世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;侯伟形容，这次星动纪元的展台“销售同学忙得连喝水都没空”。前来咨询的不只是高校、科研机构和科技大厂，还包括软件与设计服务商、硬件电机供应商、智能家居企业，甚至传统制造企业。一家德国食品工厂的提问，让他印象尤为深刻。对方关心的是，人形机器人能不能用于啤酒灌装。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对一个高度标准化、长期保守的行业来说，哪怕只是开始评估人形机器人的可能性，对于具身智能行业来说也是一个积极的信号。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，这次CES中，当工业领域的具身智能仍主要停留在方案咨询和可行性评估阶段时，面向家庭的具身智能厂商，已经在洽谈渠道合作了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成立于2024年底的乐享科技在此次展会上推出了全新品牌“元点智能”。据郭人杰介绍，元点智能将作为乐享科技的具身智能品牌，率先聚焦家庭消费级机器人市场，并已开始向更广泛的场景拓展。这一布局也标志着乐享科技正朝着集团化方向发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次CES&amp;nbsp;乐享科技主推两款产品：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;W1&amp;nbsp;聚焦海外市场，定位为“移动储能+露营拖车”。集成储能模块、音响、影像采集、显示屏及移动电站功能，可拖挂50公斤物资，满足露营场景下的负重与能源需求，又可提供基础安防功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一款产品&amp;nbsp;M1&amp;nbsp;则采取双轨策略：在海外针对极客与开发者群体，主打易得和支持二次开发；在中国市场，则强化家庭陪伴属性，例如跌倒检测、用药提醒、宠物远程监控等实用功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52acc99017c2c3ce55f7b5b4fe43ad56.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;在CES期间，乐享科技携全线产品亮相，包括一米六五大人形机器人Jupiter；家庭具身智能机器人M1；履带式机器人W1以及一米一左右小型人形机器人A1，以及彩蛋产品具身智能熊猫。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;乐享科技创始人、CEO&amp;nbsp;郭人杰透露，展会期间，公司已与北美、欧洲等地的本地连锁采购商及多国独家代理商展开合作洽谈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他甚至已为机器人产品定下具体销售目标：2026&amp;nbsp;年，元点智能旗下所有品类产品的全球销量达到&amp;nbsp;3&amp;nbsp;万台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026&amp;nbsp;年，元点智能希望在渠道上完成北美、欧洲、中东等核心市场布局；在品牌上，塑造具有科技感的全球化形象；在用户层面，建立全球用户池，通过共创机制，让用户成为长期口碑的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更具象征意义的是，重型机械巨头卡特彼勒（Caterpillar）罕见地登上&amp;nbsp;CES&amp;nbsp;Keynote。其&amp;nbsp;CEO&amp;nbsp;Joe&amp;nbsp;Creed&amp;nbsp;开场便申明：“为什么‘黄色铁疙瘩’会和科技巨头同台？”他的回答是：“当今技术最大的瓶颈其实不在软件，而在物理世界。AI&amp;nbsp;需要更多芯片，而芯片依赖地下开采的矿物；数据中心耗电量已超过当前电网承载能力；整个数字生态都需要能更快建造、更强运行、永不中断的基础设施。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;包括卡特彼勒和西门子在内的工业巨头，也正在将具身智能能力嵌入既有产线中，用以提升柔性制造与自动化水平。侯伟对InfoQ表示：“具身智能之所以受到如此多关注，是因为大家普遍相信这一定是未来会发生的事，所以不可错过，自然会投入资源。我相信对于库卡和西门子来说，逻辑也是相似的：在已有资源禀赋和沉没成本的基础上，如何确保自己不错失这一轮关键机遇。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当被问及人形机器人在工业场景中的不可替代性时，侯伟给出的判断颇为克制：在大批量、单一产品的生产中，传统自动化仍然是成本最低、ROI&amp;nbsp;最高的选择；但当制造业目标转向“个性化定制”，能够适应多任务、多环境的通用人形机器人，就会真正显现价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器人并没有一夜之间改变世界，但在&amp;nbsp;CES&amp;nbsp;这个舞台上，它们已经开始明确自己的去处了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年CES上具身智能的密集亮相，不禁让人想起&amp;nbsp;2015&amp;nbsp;年车企扎堆展示自动驾驶概念车的情景。那时，自动泊车刚刚实现，自动转向还属于前沿技术，而今天，这些早已成为智驾的标配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同一时期，车企还热衷于在车里添加更多屏幕、取消实体按键、方向盘和刹车，认为这会是未来。大众&amp;nbsp;Golf&amp;nbsp;R&amp;nbsp;Touch&amp;nbsp;试图用“靠近即唤醒”的电容系统和手势控制替代实体按键；梅赛德斯&amp;nbsp;F015&amp;nbsp;Luxury&amp;nbsp;in&amp;nbsp;Motion&amp;nbsp;用六块触控屏，让乘客通过手势、眼神甚至“透视”车外世界；谷歌原型车则彻底取消方向盘和踏板。如今回看，这些曾经“革命性”的设计，并不能进入现实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;超前的构想依然大量出现在&amp;nbsp;CES，但未来总是以更平凡却实用的方式来到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，具身智能和AI硬件会走向什么地方呢？我期待&amp;nbsp;2036&amp;nbsp;年，再回头看今天的尝试。&lt;/p&gt;</description><link>https://www.infoq.cn/article/05bVyOA1exU97VWzZz1B</link><guid isPermaLink="false">https://www.infoq.cn/article/05bVyOA1exU97VWzZz1B</guid><pubDate>Sat, 10 Jan 2026 09:11:25 GMT</pubDate><author>陈姚戈</author><category>出海</category></item><item><title>Anthropic突然封禁第三方工具调用Claude，Cursor、OpenCode、xAI 集体“中枪”！</title><description>&lt;p&gt;&amp;nbsp;在 AI 编程工具快速演化的当下，模型能力本身已经不再是唯一的竞争焦点。谁能控制模型的使用方式、定价结构以及开发者通道，正在成为新的博弈核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic突然阻止第三方工具调用Claude，引发社区强烈不满&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，Anthropic 宣布已经部署了更严格的技术保障措施，用以防止第三方工具“伪装”为官方 Claude Code 客户端，从而绕过速率限制和计费机制，低成本调用底层 Claude 模型，此外，Anthropic 也被曝出切断了包括 xAI 在内的部分竞争对手对 Claude 模型的访问权限，其中 Cursor IDE 成为了关键的“触发点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这期间到底发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事情的导火索，来自于大量使用 OpenCode 等开源代码代理工具的开发者发现：自己原本能够正常使用的 OpenCode、Cursor 等工具，突然无法再调用 Claude 模型，部分账户甚至直接被封禁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;短短几个小时内，每月支付100～200美元的开发者们纷纷涌入GitHub表达不满，引发了超过147个点赞和245个Hacker News积分。此外，用户们也开始大规模取消订阅，称强制迁移到Anthropic官方工具是“回到石器时代”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之所以会引发开发者们的强烈不满，是因为事发太过突然。没有任何警告，也没有迁移方案，就这么突然被锁定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该限制专门针对 OpenCode 1.1.8 及以上版本。但是，通过 OAuth 认证的 GPT-4 仍然能够正常工作。只有 Claude Max 的功能被限制了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenCode 是一款开源 AI 编码助手，它将 Claude 集成到 VS Code、Cursor 和其他 IDE 中。此外，它还增加了键盘快捷键、上下文感知和多文件编辑功能。开发者们非常喜欢它，因为它将 Claude 的推理过程融入到他们现有的工作流程中，而无需强制他们使用终端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户 @Naomarik&lt;a href=&quot;https://github.com/anomalyco/opencode/issues/7410&quot;&gt;在 GitHub 问题上发帖称&lt;/a&gt;&quot;：“如果单纯仅仅使用 CC（Claude Code）就像回到了石器时代。” 他立即降级了每月 200 美元的 Max 订阅，然后彻底取消了订阅。他的理由是：“它无法满足我的工作流程需求，也无法提供 OpenCode 所具备的可见性。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2e476c40c1ff15b6ba0c1b58d9796b8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并非个例。还有位用户正在项目进行到一半时，访问权限突然中断。该开发人员称：“一个小时前还好好的，现在就出现这个错误了。”另一位说：“整个下午/晚上都在用，结果就遇到这个问题了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位用户反映在工作流程进行到一半时放弃了订阅。原因很简单：每月支付 200 美元，却只能使用 Anthropic 仅支持终端的 Claude Code 工具，而非他们真正想要的 IDE 集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事情发生后，几乎在同一时间，另一条消息引发了更大的震动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有外媒披露，埃隆·马斯克旗下的 xAI，其内部开发人员已无法再通过 Cursor 使用 Claude 模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;起初，这被解读为 Anthropic 的“全面封锁策略”。但随后有知情人士指出，这其实是一场基于商业条款的独立执法。问题的关键，在于 Anthropic 的服务条款 D.4 节，其中明确禁止两类行为：第一，使用服务构建或训练竞争性 AI 系统；第二，对服务进行逆向工程或复制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 的工程师，正是通过 Cursor，将 Claude 用于加速自家模型的研发与测试。这在法律意义上，已经构成了“竞争性使用”。Cursor 在这里并非违规主体，但成为了违规行为的放大器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管此次一系列切断行为事发突然，但如果拉长时间线，会发现这次事件并非孤立。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年 6 月，Windsurf 编码环境突然被切断 Claude 3.x 的第一方产能，被迫转向 BYOK，并主推 Gemini 作为替代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年 8 月，Anthropic 又撤销了 OpenAI 对 Claude API 的访问权限，理由是后者将 Claude 用于模型基准测试与安全评估，违反竞争限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时 Anthropic 的说法就已经非常直白：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“Claude Code 成为程序员首选，OpenAI 的工程师也在使用它，并不令人意外。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但“使用”和“竞争性使用”，在这里有一道清晰的界线。xAI，只是最新一个踩线的例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工程师解释：是误伤&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在社区不满情绪持续发酵后，首先站出来解释的是 Anthropic 内部负责 Claude Code 的工程人员 Thariq Shihipar。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 X（原 Twitter）上，他确认公司已经“加强了对 Claude Code 安全套接字欺骗的保护措施”，并承认此次上线确实造成了一些误伤：部分用户因为触发滥用过滤规则而被自动封禁，Anthropic 正在回滚和修复相关问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6e/6e4b10b0d16c365471cb720577fb103c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这番解释，并没有平息社区的不满。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，真正被切断的并不仅是“异常流量”，而是一整类第三方工具的使用路径——尤其是那些通过 OAuth 授权，利用用户个人 Claude 订阅账户，在外部环境中运行自动化编码代理的软件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，这次调整的目标，并不是某几个 bug，而是“桥梁本身”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那座被拆掉的“桥”是订阅聊天模型和自动化代理之间的连接纽带。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以 OpenCode 为代表的工具，扮演着一个非常关键、但又长期游走在灰色地带的角色。它们的核心价值在于：将原本为“人类对话”设计的订阅制模型，转化为可以被自动化代理调用的基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术实现上，这类工具往往会模拟官方客户端（如 Claude Code CLI）的身份，通过伪造请求头、复用 OAuth Token 的方式，让 Anthropic 的服务器“以为”请求来自官方环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是所谓的“客户端欺骗”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在短期内，这种方式为开发者带来了极大的自由度：固定月费、 不受 API 计费限制且可以长时间运行高强度 Agent 循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从平台视角看，这座桥存在三个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，技术不可控。当 OpenCode、Cursor 等封装器内部出现错误或性能问题时，最终被指责的往往是“Claude 不稳定”“模型变差了”，而 Anthropic 却无法复现和诊断这些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，使用模式失真。订阅产品的设计初衷，是“人类辅助编程”，而不是 24 小时运行的自主代理。第三方工具解除速率限制后，模型的负载特征发生了根本变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三，也是最现实的一点：成本失衡。在 Hacker News 上，一位用户用一个形象的比喻概括了这场冲突的本质：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Anthropic 提供的是“无限自助餐”，但前提是你吃得慢。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Pro/Max（最高 200 美元/月）的定价，本质上是基于“人类交互速率”设计的。而 Claude Code 官方环境，正是用速率限制和执行沙箱，来保证这套模型不会被“吃垮”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如 Hacker News 用户 dfabulich 所说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“Claude Code 每月 200 美元的订阅价格，与 Anthropic 按 token 计费的 API 相比，存在明显的成本断层。在高频使用场景下，一个月内通过 Claude Code 消耗的 token，如果全部走 API 计费，成本很容易超过 1000 美元。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为这种价差，社区普遍认为 Claude Code 本身更像是一个“特殊定价的官方通”，Anthropic 的真实意图，是希望用户在这一订阅下使用官方的 Claude Code CLI，而不是通过 OpenCode 这样的第三方开源工具来“绕行”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种背景下，OpenCode 实现绕过限制的技术方案，被不少开发者视为一种“必然结果”：当用户已经为 200 美元的订阅付费，自然希望在自己熟悉、效率更高的工具中使用这些能力，而不是被绑定在单一官方客户端中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;社区反应：有愤怒、也有理解&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者社区的第一反应，并不友好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ruby on Rails 创始人 David Heinemeier Hansson（DHH）在 X 上直言不讳地表示，这一举动“对客户极不友好”。在他看来，用户既然付费订阅，就理应拥有更大的使用自由。他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;已确认 Anthropic 故意屏蔽 OpenCode 和任何其他第三方框架，其偏执的企图是强迫开发者使用 Claude Code。对于一家以使用我们的代码、我们的文字、我们的一切来训练模型的公司来说，这种政策简直糟糕透顶。请修改条款。&amp;nbsp;我认为所有模型提供商都推出自己的命令行界面（CLI）是件好事，但说实话，没有哪个开发者会想安装五个不同的 CLI。他们肯定希望学习并使用一个能够控制所有模型的工具。对我来说，这个工具就是 OpenCode。&amp;nbsp;这再次提醒我们，为什么我们需要开源软件！作为开发者，你肯定不想被单一模型提供商束缚。如果他们觉得能掌控你，你就会忍不住想要榨取他们的利益。所以，今天就尝试一些新的模型吧！”&amp;nbsp;同时，也需要提醒大家，这一切尚未定论。作为一家前沿实验室，长期的成功不仅仅取决于当下是否拥有最佳模式，还取决于你如何与开发者互动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5b/5b660b025615f393a97e56550f450d77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Hacker News上，关于此事的讨论也呈现出两种声音：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有开发者将矛头直接指向 Anthropic 的产品策略，认为公司不应该在订阅制和 API 计费之间制造如此巨大的落差。一种更合理的做法，应该是将订阅计划本身设计为“API 点数包”，例如在标准 API 定价基础上给予一定折扣，而不是提供一个事实上的“无限量自助餐”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有声音站在 Anthropic 一侧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Yearn Finance 开发者 Artem K 指出，相比直接封号或追溯 API 费用，Anthropic 选择“温和封堵路径”，已经算是相当克制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，工具生态的反应异常迅速。OpenCode 创始人 Dax Raad 在X 上感慨“Anthropic今天的行为充分展现了为什么竞争是世界上最重要的事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时他公开宣布，将与 OpenAI 合作，要努力让 OpenCode 与 GPT-5 尽可能完美地协同工作了，并表示&amp;nbsp;Codex 用户可以“直接在 OpenCode 中使用自己的ChatGPT/Pro 套餐”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/abc899ba6eda323b5b39594a520e50ed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，“是否应该开源 Claude Code”成为另一条激烈争论的主线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;支持开源的一方认为，Claude Code 作为开发者工具，其核心价值在于生态扩展与社区创新，长期闭源只会催生更多非官方实现，最终反而削弱平台控制力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反对者则指出，Claude Code 恰恰是 Anthropic 在编程领域建立差异化优势的关键资产，其重要性甚至超过 Claude 模型本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“要求 Anthropic 将这一工具完全开源，无异于让公司主动削弱自身竞争壁垒。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有用户分析了为什么 Anthropic 会向订阅用户提供看起来如此优惠的价格？他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;答案其实并不复杂：这是典型的“厂商锁定”策略。他们真正想出售的，并不是每月 200 美元的订阅本身，而是围绕 Claude Code 构建起来的完整生态系统。&amp;nbsp;对大多数普通用户而言，普通用户并不会每月支付 100 美元，只为了让一个聊天机器人帮他们做作业或生成一份蛋糕配方。在这样的前提下，想要从通用聊天机器人中获得可观利润，本身就极其困难。&amp;nbsp;但编程市场完全是另一回事。企业已经明确展现出付费意愿，希望用能够在极短时间内完成工作的 AI 服务来替代部分人工程序员；而程序员个体本身，也愿意自掏腰包购买这些工具，以减少工作量——即便他们清楚，这种趋势长期来看可能会削弱自身的不可替代性。与此同时，企业对“完美质量”的要求并没有高到足以阻碍这种替代。因此，编程被普遍视为 AI 领域中少数真正具备高利润潜力的市场之一，各家公司正围绕这一方向展开激烈竞争，而 Claude Code 正是 Anthropic 针对这一市场推出的核心产品形态。&amp;nbsp;在这种背景下，单纯销售一个不带任何生态绑定的订阅服务，从一开始就不是 Anthropic 的目标。这种订阅模式本身并不盈利，甚至也并非打算立即盈利，而更像是一个典型的“亏损引流产品”。真正的目的，是让用户在长期使用中深度融入 Claude Code 的整体生态。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具体验层面，争论同样激烈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部分用户认为 Claude Code 在“上下文管理”“工具调用稳定性”和整体 DevEx（开发者体验）上仍然领先，尤其是在终端与 TUI 场景中，明显优于 Gemini、Codex、Copilot 等竞品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有不少开发者表示，OpenCode 在相同模型下的执行效率更高，完成相同任务所需时间更短，且多模型、多厂商切换能力更强；另一些用户则更偏好 Kiro、Q 等工具，认为它们在简洁性和稳定性上胜过 Claude Code。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今这件事，似乎这已经不是技术讨论，而是一场生态位的重新站队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/thdxr&quot;&gt;https://x.com/thdxr&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://venturebeat.com/technology/anthropic-cracks-down-on-unauthorized-claude-usage-by-third-party-harnesses&quot;&gt;https://venturebeat.com/technology/anthropic-cracks-down-on-unauthorized-claude-usage-by-third-party-harnesses&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://byteiota.com/anthropic-blocks-claude-max-in-opencode-devs-cancel-200-month-plans/?utm_source=chatgpt.com&quot;&gt;https://byteiota.com/anthropic-blocks-claude-max-in-opencode-devs-cancel-200-month-plans/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EDUxU7QhZgG65WQDtoP1</link><guid isPermaLink="false">https://www.infoq.cn/article/EDUxU7QhZgG65WQDtoP1</guid><pubDate>Sat, 10 Jan 2026 05:30:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Facebook调查显示：越来越多的人开始采用带类型的Python，以提升代码质量和灵活性</title><description>&lt;p&gt;&lt;a href=&quot;https://engineering.fb.com/2025/12/22/developer-tools/python-typing-survey-2025-code-quality-flexibility-typing-adoption/&quot;&gt;Facebook 2025年Python类型调查&lt;/a&gt;&quot;在1200多名受访者中进行，重点介绍了Python开发人员如何以及为什么越来越多地采用该语言的类型提示系统。该调查还揭示了开发者最看重的东西，以及他们最大的挫折和愿望。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总体而言，86%的受访者表示他们“总是”或“经常”在代码中使用类型提示，其中具有5-10年Python经验的开发人员的采用率最高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然数据显示，类型提示在被调查的样本中被广泛采用，但不排除选择偏差，因为使用类型提示的开发人员可能更有可能做出回应。尽管如此，该调查揭示了使用类型提示的Python开发人员的一些有趣趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;调查结果显示，Python的类型提示系统已经成为大多数工程师开发的核心部分。[...]我们发现，所有经验水平的玩家对打字的接受程度都是相似的，但也存在一些有趣的细微差别。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级（0-2年经验）和高级（10年以上经验）开发人员使用类型提示的频率都较低，分别为83%和80%。该调查的作者认为，初级开发人员面临更陡峭的学习曲线，而高级开发人员可能正在处理大型遗留代码库，而在这些代码库中采用类型提示更为困难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发人员列举了采用Python类型系统的几个好处，包括更好的可读性和代码内文档，改进的IDE和工具支持，早期的错误检测以及增强的信心。他们还强调了高级特性的价值，如协议、泛型和在运行时检查注释的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一方面，受访者指出了一些挑战，包括第三方库中有限的类型提示支持，泛型和修饰符等高级特性的复杂性，以及复杂类型的冗长性增加。其他痛点包括工具碎片化、缺乏运行时强制执行以及难以修改遗留代码。受访者还指出，Python的类型系统似乎不如其他语言（如TypeScript）的表达能力强，而且它的快速发展意味着语法和最佳实践在不断变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;调查中另一组有趣的发现涉及改进Python类型系统的方法。一些建议包括借鉴TypeScript的特性，如交叉类型、映射和条件类型、实用程序类型（如 Pick 、 Omit 、 keyof 和 typeof ），以及更好的字典结构类型。其他建议侧重于更好地支持泛型和代数数据类型，包括更高级的类型；基于类型提示的可选运行时类型强制和性能优化；改进了对函数包装器和装饰器等模式的处理，支持动态属性；等等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具方面，MyPy仍然是首选的类型检查器，采用率为58%，紧随其后的是Pyright/Pylance。新的基于Rust的类型检查器（如Pyrefly、Ty和Zuban）越来越受欢迎，被超过20%的受访者使用。Visual Studio Code是最常见的IDE，其次是PyCharm和Vim/Neovim。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项调查中还有更多内容无法在此一一介绍。请务必阅读原始文章以获取全部详细信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/facebook-typed-python-survey/&quot;&gt;https://www.infoq.com/news/2026/01/facebook-typed-python-survey/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t6i8lghRAdi9I4weD2mR</link><guid isPermaLink="false">https://www.infoq.cn/article/t6i8lghRAdi9I4weD2mR</guid><pubDate>Fri, 09 Jan 2026 07:21:00 GMT</pubDate><author>Sergio De Simone</author><category>Meta</category><category>编程语言</category></item><item><title>Manus高溢价收购背后，是Agent开发落地困境</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;撰稿：李文朋&lt;/p&gt;&lt;p&gt;编辑：王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，“Meta以20亿美元收购Manus”的消息传得很热。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Manus曾被嘲讽“套壳”，但业内人士认为，虽然Manus整体架构和理念不算颠覆式“新”，但在任务连通性、容错、回退机制等实现上，极度考验工程能力，远不是“换个皮”那么简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Manus创始团队与媒体的最近一次访谈中，联合创始人季逸超提出目前Manus定位只是一位“通用型助手”，帮普通人把复杂工作流做完，不能完全替代用户本身。这也是因为在ToC场景里，普通用户对体验要求很苛刻——慢一点不行，错一点也不行，Manus团队很清楚这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说ToC用户已经够“难伺候”，那ToB客户对Agent的要求只会更高：一方面，企业希望Agent真正“上生产”，意味着要接入复杂的权限体系、业务系统和合规要求；另一方面，任何一次错误操作、脏数据写入、流程走错，带来的代价都远比个人用户高得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以会看到，过去一年很多企业在这条路上吃了不少苦：投入人力、投入预算，最后做出来的Agent用不了。MIT《2025年商业AI现状》报告里提到，约95%的生成式AI试点项目很难进入生产环境，很多最终都卡在上线前后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题出在哪？就在于这些一连串的工程难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如代码标准不统一、系统接口五花八门、工具调用不稳定、开发周期被拉得很长；数据资产混乱、想用调不出；安全合规和权限管理一碰就痛；甚至出现“越用越退化”的优化难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到底，并不是模型不行，也不是工程师不会做，而是整个Agent开发还不够成熟，大家还在摸索阶段，没有提前规划一套更清晰、更稳定的“做法”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，国内的云厂商开始认真思考一个问题：到底怎样才能帮助企业把Agent的难题解决掉？有没有一种更适合落地的开发范式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025云栖大会上，阿里云CTO周靖人就曾提出过「AI时代的Agent开发范式」。而在1月7日，阿里云百炼对“1+2+N”体系和开发范式做了一次更系统的升级，把它落成一个工程化的体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd7872e261efebb50616350d97d7fa59.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套“1+2+N”体系的想法并不复杂，本质是把Agent落地拆成三层：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;稳底座（1）：把模型和云资源这些基础能力做稳定、可扩展、可治理。地基不稳，再漂亮的Agent也只能停在PoC。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;定范式（2）：给企业一套把Agent做成“工业产品”的开发与运行体系，能开发、能部署、能迭代，交付不再反复折腾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;理杂活（N）：把真实业务里最难、最碎、但最致命的集成、权限、评测、成本这些“脏活累活”，做成可插拔的组件，让企业能按需拼装。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从这个角度看，这次阿里云百炼迭代背后体现的是一种更务实的方向：要用更工业化的方式，让企业的Agent在真实业务里跑起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、“N”：通用大方案，不如啃硬骨头的“高手组件”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;经过大量Agent的试错，企业如今在启动一个Agent项目时，最先拎出来掂量的往往不是模型，而是数据怎么处理与调用、安全问题能不能搞定、上线后怎么评估和优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些硬问题不先解决，再漂亮的Agent构想也很难真正走进生产环境。而在阿里云百炼的“1+2+N”体系里，“N”恰恰就是优先来啃这些硬骨头的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更关键的是，这一次“N”做了很大的升级：它把落地过程中那些最常见、最难啃、最容易反复踩坑的环节抽象出来，沉淀成一组可插拔、可组合的模块化组件。Agent开发的难题看起来五花八门，但很多难题其实有共通的解法，可以被提炼、被复用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“N”组件的存在，可以让企业缺什么就用什么、按需组合，把时间花在业务价值上，而不是重复造轮子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次升级里，一个直观的变化在应用广场：阿里云百炼把同类Agent做成了十多个精选合集，提供新的多模态模板，支持免登录体验，也能一键调用API，把“试试到跑起来”的路径压得更短。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正决定“能不能落地”的挑战，还有数据连接与知识管理、安全与权限控制、可观测与持续优化等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业做AI转型，数据治理永远是“卡脖子”环节。尽管大家都知道数据重要，但真落到工程上，标注、清洗以及让模型读懂私有数据的成本极其高昂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，企业内部约80%的数据以PDF、图像、视频或会议录音等非结构化形式存在。据IDC预测，这些数据多处于“不可检索、不可复用”的沉睡状态。随着全球数据量预计在2026年激增至221ZB，如何将这些碎片资产转化为Agent可调用的知识，成为企业发展的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼的思路是把这条链路做成“工具化”：用多模态RAG、多模态数据库、Connector连接器，把数据处理变成更工业化的流水线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多模态数据库通过智能解析、分类归档，打破图像/音频/视频等模态壁垒；多模态知识库RAG不再局限于纯文本，支持数十种格式的高精度解析，包括扫描件PDF、复杂报表、音视频会议记录等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Workflow层面添加多模态文件处理与生成节点，同时提供覆盖Chunking、Embedding、（多模态）Embedding、Rewrite、Retrieval、ReRank等在内的向量化全流程能力，用于检索与消化企业数据资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/4c/b8/4c89e13301545cce89449b9b112365b8.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;百炼平台还提供开箱即用的RAG工具，企业无需自建复杂的向量库与检索链路，也能获得高性能的知识检索与生成能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fec174a4ee0c252827b7e3b3c962d42d.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把知识库做起来只是第一步。要让Agent真正有用，它就得能接入实时数据。然而，长期以来ERP、CRM等异构系统间的集成成本高昂，导致65%的企业受访者认为业务系统沦为新的“数据孤岛”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台推出的Connector（企业级数据连接器），就是想把这个门槛降到最低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过Connector，企业可以一键对接飞书、语雀、MySQL及OSS存储；连上之后，这些数据既能直接喂给知识库，也能驱动工作流跑起来；平台还提供数十种预置工具（Tools），支持用自然语言直接查询或检索数据等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，数据一旦接进来了，真正棘手的问题也随之出现：权限边界与责任归属难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期以来，很多Agent在企业业务中多以匿名形式存在。这种“身份透明”导致操作链路难以溯源，不仅无法明确执行指令的主体，更埋下了越权操作的隐患。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，百炼平台引入Agent Identity组件，将Agent纳入企业身份治理的范畴。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过集成Okta、EntraID等主流系统，平台为每个Agent分配数字身份，使其行为从孤立的匿名调用转变为绑定主体、可供审计的合规操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台也将传统的“常驻权限”升级为“按需授权”仅在执行任务时获得短期令牌，任务结束权限即刻回收。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;配合权限降级机制，Agent的边界被严格限制在用户授权范围内，确保无法越权。全链路审计日志则让每一步决策都透明可查，解决了企业“敢不敢给权限”的顾虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对执行环境安全，百炼平台也构建了Sandbox（沙盒）物理隔离屏障。当Agent处理外部代码或第三方数据时，系统可以利用虚拟化技术将其限制在独立空间内，精简系统调用并严控网络访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一个任务会话均在“即用即弃”的容器中运行，执行完毕立即重置，彻底阻断了数据残留与交叉污染。平台同步引入实时监控与会话回放，一旦监测到异常行为将立即终止任务。这种设计为Agent提供了“受控下的自由”：在屏障内保持灵活性，在边界外确保系统安全。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而当Agent真正跑进业务之后，新的共性难题也会浮现：怎么评估、怎么持续改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与传统软件不同，Agent的执行具有非确定性：即便输入相同，也可能因模型的随机性、工具调用顺序或上下文波动产生不同的输出。这导致开发者难以追踪Agent决策逻辑，在任务失败时无法精准定位是模型、工具还是流程缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台通过Trace（可观测）与Evaluate（评估）组件，实现了从“黑盒”到“透明”的转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Trace组件提供完整的执行轨迹追踪，清晰复现了从思考（Thought）、行动（Action）到观察（Observation）的每一步。开发者可以判断哪一步耗时最长、哪个工具失败率最高，或是在哪个环节陷入了逻辑死循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结合Token消耗、响应速度等量化指标，这些数据可通过Grafana进行可视化监控，构建起实时的生产环境观测能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于此，Evaluate则建立了体系化的评价标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在任务完成度评价方面，百炼平台可以通过衡量目标满足率与输出质量对Agent进行评分；并支持“模型评测（LLMasJudge）”、专家打分与人工复查相结合的混合模式，对失败任务进行深度归因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以说，基于日志（Logs）、指标（Metrics）与追踪（Traces）的三大支柱，百炼平台设计了一个“评估—优化—验证”的持续迭代闭环。这种由数据驱动的迭代机制，也驱动着Agent实现“越用越好用”的工程闭环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比于自建底层架构，直接调用百炼平台的成熟组件能让开发周期缩减数倍。以RAG系统为例，以往搭建搜索和解析链路需要数周，现在利用多模态RAG组件，几个小时就能跑通。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业不需要为每个Agent单独开发身份认证或数据接口，一套Agent Identity就能管好所有Agent的工号，一个Connector就能接通全公司的数据源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;放在阿里云百炼“1+2+N”体系中，组件化正填补模型到业务之间的最后一块拼图：模型提供计算力，开发范式定好流程，而这“N”个组件则专门负责解决数据怎么连、权限怎么划、效果怎么评、安全怎么管这些具体的“杂活”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、“2”：“下一代”Agent，需要新开发范式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“N”组件把坑填平，只解决了“这事能不能接得上、管得住”。企业真正要把Agent变成长期能用的工业生产能力，还得解决另一个现实问题：怎么开发、怎么协作、怎么迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼“1+2+N”体系里的“2”，就负责这一点，它涵盖两种开发方式（低代码+高代码），以及配套Agent开发平台，通过同一套平台和运行时，分别服务两类人、两种交付方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么要做成“2”种模式？因为企业落地Agent的过程，基本就是两条路同时走：想快速试点、尽快看到效果，低代码更省事、更快；真要进核心业务、对接复杂系统，高代码才够灵活、够深。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4b74cf85efa3ce79ee9854b2b74b875a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更现实的是，企业在代码协作上存在长期的“割裂”：低代码不够用，高代码效率低。产品经理用低代码搭建的草案，往往需要技术团队用高代码重新开发，而这种重复劳动会导致业务逻辑在传递中失真。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了让Agent更快、更深地融入业务，百炼把低代码和高代码“打通”：企业可以从低代码起步做验证，再逐步演进到高代码做优化，形成一种更自然的渐进式开发，让真正懂业务的人与懂技术的人有机协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据Gartner的预测，到2028年，企业里相当一部分Agent应用会由业务人员主导搭建。双开发模式很可能会成为Agent走向工业化落地的一种主流形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但“2”的意义还不止是“怎么写代码”。更重要的是：下一代Agent本身就需要新的开发范式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去的一年，很多企业里的Agent实际上还停留在比较“表层”的形态：一种是以提示词工程为核心、更多承担辅助角色的Copilot；另一种是能处理重复流程、严格按预设步骤执行的“数字员工”。它们能提升效率，但往往缺少主动规划与闭环执行能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent不应仅“被告知怎么做”，而是“应该主动思考怎么做”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此阿里云百炼提出了Agent2.0：未来的Agent要能围绕目标自主规划，把复杂问题拆成可执行的小任务，过程中还能根据反馈调整策略，最后交付更稳定、质量更高的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照这个定义，Agent2.0的核心链路是“规划—执行—反思”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现实里很多Agent开发失败，问题往往是开发范式还停留在老路上。传统那种线性链路（用户 →Agent→ 模型 → 输出）有三个硬伤：没有规划，就很难应对动态场景；没有反馈与纠错，走偏就很难拉回来；没有长期记忆，交互体验容易断裂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了能承载Agent2.0的生产级落地，百炼平台对开发范式做了系统升级：AgentScope从过去偏“开源写代码”的工具形态，演进为覆盖Agent全生命周期的工业化开发平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fdc7c9724836cf7260efbe868beab353.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一步，是把“上手门槛”压到尽量低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，AgentScope做了对主流模型能力的统一集成，内置100+预训练模型，拿来就能用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一方面，百炼平台提供了一批可复用的智能体库，比如交易智能体（EvoTraders）、调研智能体、金融分析智能体、数据科学智能体（Data-Juicer）、浏览器使用智能体、语音智能体等，减少从零开始的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二步，是围绕更高级的Agent2.0，把“协作与执行”能力补齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AgentScope主要通过三块来支撑：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多智能体编排：引入基于Actor模型的分布式架构，支持多个专业Agent的并行协作与自动调度。研究表明，协作模式任务成功率比单一Agent高出90.2%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能体上下文管理（长期记忆）：深度适配Mem0、ReMe等记忆系统。使得Agent能够自主存储并检索历史交互中的关键信息，在后续任务中实现能力的持续迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具调用能力：全面兼容StreamableHTTP、SSE、STDIO等主流接口标准。通过支持AnthropicAgentSkill规范，在运行时即可动态加载新工具或移除冗余资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具调用这层，ReAct这类“边想边做”的范式，也被不少实践证明更容易形成任务闭环：学术基准测试中，ALFWorld任务只需2个示例即可达到71%的成功率，高于强化学习模型的37%；在复杂任务中，准确率相较纯FunctionCalling提升约15%–20%，成本比CodeAct低78.9%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Agent2.0优化与部署阶段，阿里云百炼通过AgentScope-Studio+AgentScope-Runtime打通了全生命周期的工业化链路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AgentScope-Studio可通过自定义多维表现指标，评估工作流设计的合理性；提供从输入到输出的全链路追踪与可视化，让Agent行为与决策过程实现“可观测、可复盘”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台利用评测结果持续改进，让失败样本成为训练资产，形成“评测→优化→验证→再优化”的迭代闭环，实现从“盲目调参”到“数据驱动优化”的范式转换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在落地部署环节，AgentScope-Runtime支持Docker、K8S、ACK、Serverless等多种部署形态；通过Agent-as-a-Service将Agent封装为可独立调用的API服务，兼容A2A与ResponseAPI等协议，便于集成、弹性扩缩与快速迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果把阿里云百炼的开发范式拆开来看，其实就是从“构建”走向“运营”的一个完整闭环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前半段构建，重点是更快、更省力地把东西搭起来：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用可选智能体模板减少重复劳动；用多智能体编排与工作流把复杂任务拆成可协作的子任务；用高低代码一体化实现统一开发与交付；通过ReAct等方式完成多任务的规划、执行与自我纠偏，再结合用上下文和长期记忆支撑长链路执行等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后半段上线运营，就是做让它智能地跑起来：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用可观测和自动化评测把效果变成可量化的指标；打通真实系统和数据源，拿到反馈并持续优化；在企业既有基础设施上实现更便捷的部署与稳定运维；同时借助Identity、模型单元专属部署、机密推理等能力，把权限、安全与合规治理补齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套开发范式的最大亮点，就是它统一按照“工业级Agent2.0”的标准做事：高效的开发体系+可持续的反馈闭环+便捷可靠的上线部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、“1”：模型优势之外，深挖“模型服务”工程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，无论是组件化拼装，还是低/高代码协作，最终都要落在同一个问题上：模型调用能不能稳定、能不能扛流量、能不能控成本、能不能过合规。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以“1”是整个体系的地基——模型与云服务底座把推理服务、弹性、部署形态与安全边界做成统一供给，保证上层“能跑起来，也跑得久”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多企业在用Agent的过程中，卡住的往往不是“模型会不会”，而是一些更现实、更工程的问题：1）延迟、并发、稳定性跟不上真实业务流量；2）成本容易失控（链路长、多轮工具调用、重试一多就更明显）；3）部署和合规麻烦（私有化、混合云、权限边界、数据隔离等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在调用模型的时候，企业最关心的也无非就两件事：成本与性能。为此，百炼平台提供了一套云资源调度组合拳：“异步调用+闲时调度”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/87/8743f59ec1c2d256f6f9d7f5b195f60e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前搞大规模的数据清洗、标注，或者是分析长视频，这些任务不仅计算密集，而且耗时漫长。最头疼的就是走“同步调用”，跑到一半接口超时了，任务断掉，前面全白干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了异步调用就省事多了，它像寄快递一样，你把任务丢给后台，拿个任务ID，就可以去干别的。不用在那儿死等结果，等服务器处理完了你再回来取就可以。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而“闲时调度”更像“错峰用电”：不着急的任务挪到资源空闲的时候跑，单价更划算，整体资源利用率也更高。阿里云百炼官方给出的数据是，动态调度后闲时推理成本可降低50%。对需要处理海量数据的企业来说，这种节省是实打实的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，阿里云百炼这次把“模型服务能力”也做了系统升级，主要围绕四块：模型后训练、专属模型单元部署、平台可观测、推理安全防护，系统性地”深挖“模型的服务能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;先从选型说起。百炼平台把模型体验中心做了结构性重构，把在线模型的能力做成更直观的“能力图谱”，支持文本、视觉理解、图像/视频生成、语音交互等全模态体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样企业就不用靠猜，也不用“盲选”，可以在平台上直接对比不同模型在具体场景下的表现，再做选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模型选定之后，是否“实用”往往取决于后训练。很多企业真正需要的不是通用能力，而是用自家数据和业务知识微调出来的“专家模型”，这才更贴近业务，也是企业的核心壁垒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模型训完后，真正容易被“拦住”的常常是部署。自建集群运维复杂、成本也难估：为了应付峰值不得不预留一堆算力，平时又闲着浪费；多租户环境下的数据隔离和性能争抢，会让企业心里不踏实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼推出“模型单元”部署，其实相当于给企业开了条“专属通道”，减少资源争抢带来的不确定性，让高并发和低延迟更稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7a407526e4757cc1cfd44377d66694b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时提供全托管的Serverless方式：系统会跟着实时流量自动扩缩容——忙的时候自动扩，保证体验；闲的时候自动收，尽量省成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方给出的测试数据里，模型单元部署相对传统自建集群方案，推理性能提升超过1.3倍，并发能力提升超过1.5倍。对企业来说，这类提升的意义很直接：同样的业务量，成本更低性能更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，调用模型处理数据时，最难绕开的是安全——尤其在金融、医疗、法律等高敏行业。很多企业不是不想用，而是卡在一句话：数据给到模型，会不会出事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，百炼平台推出模型“机密推理服务”，依托三层安全架构，为企业构建起全链路的数据保护围墙：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一层是基于CPU/GPU硬件可信执行环境的机密计算能力，将模型推理运行在硬件隔离的安全区内。即便云侧其他组件遭受攻击，敏感数据也难以被窃取或泄露。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二层是端到端加密的可信链路：实现了从用户端到云端计算中心的全程加密传输。数据在加密状态下进入TEE区域处理，计算结果在加密状态下返回，确保数据在“流动”与“处理”的全生命周期中始终处于保护伞下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三层是公开审计的可信服务：平台提供可验证的身份与安全能力证明。企业不仅能自主校验服务安全性，更能以此作为合规背书，向管理层、审计机构及客户证明其AI系统的高安全性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在使用体验上，机密推理被做成了“一键交付”的形态：企业只需要在模型库中选择支持机密推理的版本，一键部署到TEE隔离环境，就能直接调用机密推理服务来处理敏感数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;放在一起看，这次升级是在原有模型性能优势之上，又补上了几块关键拼图：云资源调度、后训练、模型单元化部署、机密推理安全体系等。几块一起发力，让大模型调用变得更实用、更省钱，也更安全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、没人愿意再“从零开始”，阿里云百炼Agent平台企业版已发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从市场角度来看，政企、金融、医疗等行业在采购云服务时，始终受困于一种不完美的平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公有云上手快、性能强，但数据边界与合规要求是跨不过的门槛；私有化部署虽有安全感，但往往陷入“模型、工具、流程”极其复杂的运维战泥潭，开发周期长、技术更新慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月7日，阿里云百炼企业版的发布，为市场提供了一个既保留数据主权，又拥有云端顶级效率的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业版支持专有云、本地化及VPC隔离，百炼平台将云端的成熟能力“下沉”至企业环境。更重要的是，百炼平台企业版支持源码级交付。这不仅仅是技术开放，更是给予企业自主演进的确定性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业不再需要买一堆零件回去组装，而是直接获得一个在自身安全边界内运行的Agent基座。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，企业版也并非新功能的简单集合，而是将百炼平台“1+2+N”体系（顶级模型、成熟范式、核心组件）封装为完整的交付体：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;双代码统一：兼顾业务验证的敏捷性与复杂逻辑的深度定制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多模态RAG：激活企业沉睡的音视频与文档资产，转化为实时知识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Trace与Evaluate：将Agent的黑盒行为拉到台面上，让调试与迭代成为标准工序。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大规模组织的管理诉求：企业版强化了多租户部署、SSO账号集成以及细粒度的权限审计。这些功能解决了IT部门的核心忧虑——让Agent的应用在组织内部不仅“能跑通”，更“可治理”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个行业走向成熟的标志，是目光从技术指标移向业务价值的“深水区”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼Agent平台企业版，本质上在扮演“AI时代技术中台”的角色。从行业趋势上看，未来企业大概率将不会从零开始建设AI能力，而是直接基于一个完整、成熟的技术中台起步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，在一年的野蛮生长后，留给企业AI试错的窗口期正在关闭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望2026年，Agent应用爆发增长几乎已成共识。Gartner预测，到2026年底，40%的企业应用将集成任务型AI agents（相比2025年不足5%），这也标志着Agentic AI正从概念走向主流生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对阿里云这样的全栈人工智能服务商而言，这将是多年技术积累转化为业务增量的红利期；对使用模型与Agent的企业客户而言，也将是Agent正式进入“拼效率、拼落地”的竞争元年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SxeNI9gVzcKr36xlwuCQ</link><guid isPermaLink="false">https://www.infoq.cn/article/SxeNI9gVzcKr36xlwuCQ</guid><pubDate>Fri, 09 Jan 2026 06:47:07 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>AI 工程化</category></item><item><title>刚刚，AI企业IPO最速纪录刷新！MiniMax的技术野心，价值超800亿</title><description>&lt;p&gt;闫俊杰在商汤敲钟前夕离开，创立了MiniMax（上海希宇科技），也造就了全球从创立到IPO用时最短的AI企业——4年，进程明显快于行业常态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就在刚刚，1月9日，MiniMax紧随其后挂牌上市，股票代码00100。招股书显示，MiniMax的ToC收入已经反超ToB，这在中国大模型公司中极为罕见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其招股书还透露了一堆硬核数据，截至2025年9月30日：&lt;/p&gt;&lt;p&gt;累计个人用户：超过2亿覆盖200+国家和地区AI原生产品 MAU：约&amp;nbsp;2760万企业与开发者客户：超过10万家&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次IPO中，Mini Max计划发行约2540万股H股，开盘价235.4港元，截至上午10:30，股价已飙升超60%，市值超820亿港元（约合人民币738亿元）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/de/de9457c4ff136a5de20b8ab2eadf6aee.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据富途证券数据，MiniMax此次IPO超级火爆，公开发售部分的超额认购倍数高达1209倍，投资者通过保证金方式认购的金额累计超过2533亿港元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;资本市场为MiniMax的技术野心“买单”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在国内近年来涌现的一批AI独角兽中，唯二高频更新技术论文、投资开发者生态的，是MiniMax和DeepSeek背后的深度求索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;闫俊杰曾在各种场合明确表达： MiniMax是一家技术驱动的公司。据招股书显示，MiniMax最大的成本就是研发成本，为了在基础模型技术上集中注意力，海外版 App 甚至没有第一时间做英文化。投资人的评价大体也能回归到技术要素，即闫俊杰是一个真正对AGI有信仰的人，“他很真”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是除市场数据外，MiniMax市值最明确的支点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;仅在2025年，MiniMax已通过至少两篇公开科研论文系统阐述其大模型架构与推理优化方案，其核心成果包括 MiniMax-01，即基于Lightning Attention与MoE的超长上下文大模型；以及MiniMax-M1，即针对推理计算效率进一步优化的模型版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相关论文不仅披露了核心机制，还在处理百万级token上下文和推理效率上提出可复现技术路径，而非简单参数展示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到2024年初，在稠密模型仍占主流的背景下，MiniMax率先推出了中国首个混合专家系统（MoE）大模型abab6——比DeepSeek火出圈R1早了约一整年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在行业仍普遍依赖Softmax Attention、并为其二次计算复杂度付出高昂算力成本时，MiniMax开始在模型中大量引入自研的Lightning Attention（线性注意力）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体做法，简单来说就是在每8层模型结构中，只保留1层传统注意力，其余7层改用线性注意力，从而把长上下文推理的计算压力“削薄”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;改动后的直接效果是：模型在面对超长文本、长代码或多轮复杂推理时，不再随着上下文变长而指数级变慢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套注意力设计与MoE架构叠加后，进一步放大了效率优势，使模型在保持推理能力的前提下，大幅提升了长文本、长代码和复杂任务场景下的计算效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比智谱以GLM系列基座模型为核心，在ToB与ToG侧已跑出较为稳健盈利能力的路径；MiniMax展现出的是另一种取向：模型更强调产业化落地，已在ToC端取得了不错的成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕自研大模型，MiniMax已形成包括MiniMax Agent、海螺 AI、MiniMax语音、星野以及开放平台在内的产品矩阵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时在海外市场亦已有实质进展：其产品和服务已覆盖 200 多个国家和地区，累计触达超过 2.12 亿名个人用户，并服务超过 13 万家海外企业与开发者（包括订阅、API 调用等渠道）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按2024年基于模型的收入计算，MiniMax是全球第四大pure-play大模型技术公司，还是全球第十大大模型公司，覆盖文本、视觉、音频、视频的全模态模型体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在上市前的近一年内，MiniMax完成了从MoE 架构探索（abab 6 / 6.5）到基础大模型开源（MiniMax-01），再到高级推理模型（MiniMax-M1）的连续迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以MiniMax-01系列为例，模型总参数规模已达数千亿量级，但单个 token 实际参与计算的参数仅为几十亿，使得模型可以在控制成本的前提下，原生支持百万级乃至更长的上下文窗口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年12月23日，MiniMax还对外发布了最新旗舰级Coding &amp;amp; Agent模型M2.1。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在衡量多语言软件工程能力的 Multi-SWE-bench测试中，该模型在仅约 10B 激活参数的前提下取得 49.4%的成绩，超越了Claude Sonnet 4.5等国际顶尖竞品，拿下全球SOTA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5909816687fff06cea0b33dec57e93f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;M2.1要补上的，是此前不少模型在工程能力上的短板——过去的模型在编写简单脚本或前端代码时尚可应付，但一旦进入后端工程、系统架构或底层逻辑层面，表现往往迅速失稳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个模型的关键变化在于，其能力边界首次延伸至更完整的后端开发规范。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些技术实现背后，是一支极其年轻的团队。据每日经济新闻消息，截至2025年9月底，MiniMax员工385人，平均年龄29岁，研发人员占比近74%，董事会平均年龄32岁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心团队由一批来自商汤科技、全球一流高校和顶级科研机构的技术骨干组成，以创始人闫俊杰为首，包括杨斌、周彧聪等联合创始人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;闫俊杰拥有东南大学、本科到中科院自动化所博士及清华博士后背景，曾担任商汤副总裁与研究院副院长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd30496e3077a396c9ff345ee1d19fbb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨斌具备加拿大博士及Uber ATG与国际初创工程经验；周彧聪则是商汤早期算法团队核心成员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;团队多数来自 AI 与深度学习前沿领域，在 NLP、语音、视觉、生成模型等方向拥有丰富经验和多项全球发明专利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;站在年轻团队另一面的，是AI 投资界的“老炮”们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期有阿里、腾讯、红杉中国、高瓴、IDG、云启、米哈游等产业与风投参与；IPO 前夕，阿布扎比投资局、Mirae Asset、Aspex、易方达等长线机构接力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤其是阿里，持有的 MiniMax 股权占比还要大于在智谱的比重。连续两场 IPO 后，一场投资界和 AI 创业团队之间的化学反应和默契已经诞生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上市之后，还需直面Claude Code等问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要指出的是，由商汤的 ToB/ToG 模式，转到如今的 ToC/ToB 模式，闫俊杰麾下的&amp;nbsp;MiniMax还未实现整理盈利；至少想赢得全球 AICoding 市场，绕不开和Claude Code的直接竞争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code是一个面向真实软件工程的Coding / Agent模型，由Anthropic公司推出。该模型的重点是在 AI 生成代码以外，确保模型在工程约束下不失控，堪称 AICoding 神器。近日， Anthropic宣布，Claude Code上线仅6个月，已经创造了近10亿美元年化营收。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从公开信息看，MiniMax并没有试图直接复刻Claude Code的路径，而是选择了另一种更偏效率驱动的技术路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MiniMax在Lightning Attention + MoE上的投入，本质上是在解决一个问题：如何在成本可控的前提下，把上下文和工程复杂度拉到“真实软件世界”的尺度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Coding模型来说，长上下文不是加分项，而是入场券。 没有足够高效的注意力结构，就无法在真实代码库上长期运行Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;M2.1针对Multi-SWE-bench的表现，某种程度上正是在回应 Claude Code 的“主战场”——不是写某一段代码，而是完成跨语言、跨模块、带验证的软件工程任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着 MiniMax 正在补的，并不是单点能力，而是：后端规范、工程一致性，和多语言协作能力，这正是 Claude Code 最难被替代的部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MiniMax若想在全球市场正面竞争，最终比拼的也不会只是 Benchmark，而是Agent是否可控、错误是否可解释，以及是否敢被放进CI / CD流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从招股书来看，MiniMax的研发投入在过去三年中持续攀升：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2022 年为1060万美元，2023年增至7000万美元，2024 年进一步扩大至1.89亿美元；截至2024年及2025年9月30日止的九个月，研发开支分别达到 1.387 亿美元和 1.803 亿美元。相关投入主要用于模型训练过程中产生的云服务费用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，在头部云厂商和海外独角兽的夹击之下，MiniMax同时承受着ToB与ToC两个市场的竞争压力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型技术仍在快速演进，这场拼性能、拼效率、拼工程化的技术马拉松还在继续；上市，只是把比赛带入了下一个赛段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在一次采访中，闫俊杰提到，MiniMax 确实放弃过一些 ToB 订单，是基于对自身交付能力的判断，避免分散注意力。那么，如果 ToB 领域的工程化交付，当下还不是 MiniMax 的“长板”，短期来看，就只剩“技术登顶”一条路能帮MiniMax走到终局。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;闫俊杰说他在 Dota2 游戏里爱玩小精灵，因为这个英雄实现过从五号位（辅助）转型成为一号位（核心），最终主宰比赛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前看来，对于MiniMax而言情况类似，能否在Benchmark上五转一，保持模型能力长期领先，是上市后走向AGI的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100026_c.pdf&lt;/p&gt;&lt;p&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.1?utm_source&lt;/p&gt;</description><link>https://www.infoq.cn/article/U7llrTLhdJxypjxLbTkz</link><guid isPermaLink="false">https://www.infoq.cn/article/U7llrTLhdJxypjxLbTkz</guid><pubDate>Fri, 09 Jan 2026 03:53:17 GMT</pubDate><author>木子</author><category>AI 工程化</category></item><item><title>Meta运用基于大型语言模型的变异测试提升合规覆盖率</title><description>&lt;p&gt;为了提高其软件系统的合规覆盖率，Meta已经&lt;a href=&quot;https://engineering.fb.com/2025/09/30/security/llms-are-the-key-to-mutation-testing-and-better-compliance/&quot;&gt;将大型语言模型应用于变异测试&lt;/a&gt;&quot;。这种方法将LLM生成的变异体（mutants）和测试集成到Meta的自动化合规加固（ACH）系统中，消除了传统变异测试在可扩展性和准确性方面的限制。该系统的目标是在满足合规义务的同时保持产品和服务的安全，帮助团队更高效地满足全球监管要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mutation_testing&quot;&gt;变异测试&lt;/a&gt;&quot;是故意在代码中引入一些小的变异体，并检查测试是否能够检测到它们，以此来评估测试套件的有效性。由于变异体数量过多、计算成本高昂且存在价值有限的等效变异体等因素，传统编译测试的应用有限。Meta的方法是利用大型语言模型生成具备上下文感知能力的变异体以及对应的测试，从而降低噪声并使工程工作聚焦于高价值代码路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在没有LLM指引之前，变异测试依赖于基于规则的静态操作符。这些操作符会无差别地生成大量的变异体，其中许多在语义上与原始代码等价，压跨了测试基础设施和开发流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2501.12862&quot;&gt;Meta的ACH&lt;/a&gt;&quot;系统使用LLM生成恰当的变异体和有针对性的测试，重点关注隐私、安全和监管问题。&lt;a href=&quot;https://engineering.fb.com/2025/02/05/security/revolutionizing-software-testing-llm-powered-bug-catchers-meta-ach/&quot;&gt;基于LLM的等价检测器&lt;/a&gt;&quot;会过滤掉多余的变异体，而测试生成器会生成单元测试，工程师可以进行审查但不需要手动编写，这显著降低了运营开销。Facebook、Instagram、WhatsApp和Meta的可穿戴平台的早期部署产生了数万个变异体和数百个可执行的测试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d7c9d534d00e30aaef41532d183f48ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ACH系统架构概览（图片来源：&lt;a href=&quot;https://engineering.fb.com/2025/02/05/security/revolutionizing-software-testing-llm-powered-bug-catchers-meta-ach/&quot;&gt;Meta技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自从将研究成果纳入ACH以来，Meta在&lt;a href=&quot;https://conf.researchr.org/info/fse-2025/keynotes&quot;&gt;FSE 2025&lt;/a&gt;&quot;和&lt;a href=&quot;https://conference.eurostarsoftwaretesting.com/event/2025/assured-llm-based-software-test-generation/&quot;&gt;EuroSTAR 2025&lt;/a&gt;&quot;大会上展示了他们的工作成果，即LLM如何帮助他们克服以前限制大规模变异测试的障碍。借助生成式AI更高效地生成测试用例，传统上用于评估测试质量的变异测试如今变得更具实用性和可扩展性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如Meta工程团队所强调的那样：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;从2024年10月到12月，我们尝试在Facebook、Instagram、WhatsApp和Meta的可穿戴平台上部署了用于隐私测试的ACH。在数千个变异体和生成的数百个测试中，隐私工程师接受了73%的测试，其中36%被判定为与隐私相关。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在ACH的基础上，Meta推出了&lt;a href=&quot;https://arxiv.org/pdf/2504.16472&quot;&gt;即时捕获测试（JiTTest）挑战赛&lt;/a&gt;&quot;，旨在探索大型语言模型在自动化软件测试中的应用。该系统会生成强化测试以防止回归问题，并生成捕获测试用于检测新代码或变更代码中的缺陷。它会在拉取请求进入生产环境前生成测试结果以供审核，这既解决了测试预言问题（Test Oracle Problem），又保留了人工监督环节。在FSE 2025大会上，Meta发表了一篇&lt;a href=&quot;https://arxiv.org/pdf/2504.16472&quot;&gt;论文&lt;/a&gt;&quot;，详细阐述了JiTTest挑战及其相关的开放研究课题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Meta表示，LLM将耗时且容易出错的过程转变为更高效的系统，帮助简化并优化了合规和风险管理框架。正在进行的工作包括：将ACH扩展到隐私测试和Kotlin之外的更多领域和语言；通过微调和提示工程改进变异体生成；解决测试预言问题。Meta还在研究开发人员如何与LLM生成的测试互动，以提升采用率和可用性。更多研究成果将在即将召开的会议中展示，包括&lt;a href=&quot;https://atscaleconference.com/&quot;&gt;Product@Scale&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/meta-llm-mutation-testing/&quot;&gt;https://www.infoq.com/news/2026/01/meta-llm-mutation-testing/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AVRJAU6tE2FA70QD6pOg</link><guid isPermaLink="false">https://www.infoq.cn/article/AVRJAU6tE2FA70QD6pOg</guid><pubDate>Fri, 09 Jan 2026 03:03:00 GMT</pubDate><author>作者：Leela Kumili</author><category>Meta</category><category>AI&amp;大模型</category></item><item><title>“AI火了，我们却快完了！”顶级开源框架Tailwind之父含泪裁掉75%兄弟：半年后，这个项目可能就没了</title><description>&lt;p&gt;前端生态最具影响力的开源项目之一 Tailwind CSS，正经历一场罕见的生存压力测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其创始人 Adam Wathan 近日在社区公开表示，由于 AI 对业务模式造成的“残酷冲击”，Tailwind 在一天之内裁掉了工程团队约 75% 的员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他在 1 月 7 日一期自述播客中进一步解释：在 AI 编程工具大规模采用 Tailwind、使用量持续走高的同时，这种“被默认使用”的成功并未转化为可持续的商业回报，反而持续侵蚀了团队的生存空间。若趋势不变，大约 6 个月后将无法继续支付工资。Adam 形容这是一种“非常糟糕的认知”，迫使他们必须立刻缩编，避免走到“既撑不住工资、也拿不出体面遣散”的境地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我真的难受。胃都拧在一起了。”Adam说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“因为这件事，我感觉自己像个失败者：我做出了一个几乎‘统治世界’的开源 CSS 框架，用的人越来越多、越来越火，但商业上的成功，却和开源的成功呈现出一种反向关系。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们只剩下六个月了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我现在的每一秒，都必须用来让公司活下去”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场裁员风波最终被外界注意到，触发点是一则围绕“大模型（LLM）文档支持”的 GitHub Pull Request。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年 11 月，社区开发者向 Tailwind 官方仓库提交了一项合并请求，要求新增一个 llms.txt端点，用于提供面向LLM优化的 Tailwind CSS 全部文档的纯文本合并版本。以此希望在所有文档页面加一个“复制为 Markdown”的按钮，因为现在很多人会把文档内容直接喂给 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6cd1b5c7d6871ad03de1a0ed000c6b30.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从描述来看，这个PR是将 Tailwind 所有官方文档（共 185 个文件）在构建阶段静态合并为一个纯文本、无 JSX、按章节顺序排列的文档文件，方便 LLM 直接读取和使用。从工程实现上看，这只是一个构建期脚本，改动规模有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但该 PR 提交后长期未获推进。面对社区的追问，Tailwind 创始人 Adam Wathan 回应称，当前团队有更重要的事情要做，比如先想清楚怎么让公司赚到足够的钱、把业务维持下去。他直言，如果越来越多的人不再访问文档，而是直接依赖 LLM 去爬 Markdown 文件，“只会导致文档访问量进一步下降，也就意味着更少的人会了解到我们的付费产品，最终让业务变得更加不可持续。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“很抱歉，我现在没有时间去做那些不能帮我们付账单的事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Adam关闭了这个 PR。当然，评论区立刻炸了：这对社区太糟糕了，你们只想着赚钱，太失望了......&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有社区开发者认为，让软件更容易融入用户工作流、解决他们日常互动中的痛点，本身就是扩大潜在付费用户的关键前提；而此功能旨在让人们能够使用 Tailwind 更快、更高效地构建更多内容，现在 Adam以“变现”为由拒绝此类功能，“等于是在告诉你的客户，从他们那里赚钱比为他们提供服务更重要。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;争议升级后，Adam 不得不再次回应，并披露了 Tailwind 的真实处境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他坦言他知道这个功能的价值，但现实情况是：“就在昨天，我们工程团队里有 75% 的人失去了工作，这是 AI 对我们造成的残酷冲击。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这样的背景下，他坦率地说，自己已经很难再把时间投入到这类“不直接带来收入”的事情上：“我现在的每一秒，都必须用来让公司活下去。确保还留在这里的人，每个月都能拿到工资。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他同时透露，尽管 Tailwind “比以往任何时候都更受欢迎”，但 “我们的文档流量相比 2023 年初已经下滑了大约 40%。”而文档是他们的唯一分发渠道，没有客户，就意味着 “我们根本负担不起继续维护这个框架。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更残酷的是，虽然Tailwind “增长速度比历史上任何时候都更快，规模也比任何时候都更大”，但 “收入却下滑了接近 80%。”他总结说，眼下 “让 Tailwind 变得更好用”，与 “让这个框架的开发在商业上变得可持续” 之间，“几乎已经看不到任何相关性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，他必须先解决生存问题，不然“一旦没人继续维护，这个项目最终会变成无人问津的弃置软件。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/06/060b73125d277b62b3a097e9bc551182.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;更反直觉的现实：Tailwind 反而“到处都在被用”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这件事迅速在 Hacker News 上爆了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HN 首页一条帖子标题很直接：“Tailwind 的创作者裁掉了 75% 的工程团队”，链接指向 TailwindLabs 的 GitHub 讨论。发出约 10 小时后，评论也堆到 598 条，迅速变成当天的高热讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8d9164694d3a900b8dfed20f41b4c48d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场裁员之所以在社区引发震动，很大程度上来自一种强烈的反差感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5a/5aa1e239b60d7e04f1214f4f4c303a4d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2020 年 7 月，Adam Wathan 还在公开回顾 Tailwind 的“上升期叙事”：Tailwind 的累计安装量刚刚突破 1000 万，而他们的首个商业化产品 Tailwind UI 上线仅约 5 个月，收入就即将跨过 200 万美元。他把这段经历形容为“完全超出想象”，&lt;a href=&quot;https://adamwathan.me/tailwindcss-from-side-project-byproduct-to-multi-mullion-dollar-business/&quot;&gt;并特意把最初发布在 Twitter 的长帖重新整理成文章。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d9/d971bdb0d8f806220df5e54c92af4369.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且在AI的世界里，在大多数开发者的体感里，Tailwind 也不是处在衰退期，恰恰相反，它正在悄然变成一种&amp;nbsp;AI 生成 UI 的“默认选项”。当人们打开 AI 编程工具，让模型生成一个页面、一个组件，甚至一整套 UI 时，模型往往不会再询问“要不要写 CSS”，而是直接给出一串熟悉的 class——这种选择并非出于偏好，而是因为在当下的工程环境里，这样做最快、最稳，也最不容易出错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.glideapps.com/blog/tailwind-css&quot;&gt;Glide CEO 兼创始人 David Siegel 认为&lt;/a&gt;&quot;：“你可以把 Tailwind 看成是一套无代码（no-code）工具包，它实际上让 AI 在设计这件事上变得更强了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，AI 在使用 Tailwind 这件事上，确实表现得异常出色。就像无代码平台通过预制组件，帮助非开发者也能构建稳定、设计良好的应用一样，AI 也开始把 Tailwind 当作一套“组件库”来使用——这让它能够更快工作，并生成更可靠、更一致的样式结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI 并不是在 CSS 这种底层样式语言上变得更强了，”Siegel 解释道，“而是我们发明了一种 AI 更擅长使用的‘高层语言’，它叫 Tailwind。”他进一步指出：“它看起来几乎就像自然语言。你不用写一堆括号、冒号之类的东西，只需要写 text-black，文本就变成黑色；写 rounded-md，按钮就会变成中等圆角。这些组件库，本质上就是建立在设计之上的低代码 / 无代码抽象。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现代 AI 编程助手最擅长的，往往是遵循清晰、可重复的模式，或者在一个定义良好的词汇体系中进行组合与生成。而 Tailwind 的方法论恰好满足了这一点：它提供了一套高度一致的 class 命名和样式模式，使 AI 更容易生成正确、相关且稳定的代码建议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/14/14592cbee0665870f01e78e804a1f2bf.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/rauchg/status/1842952946457493638&quot;&gt;正如Vercel CEO Guillermo Rauch所说&lt;/a&gt;&quot;：“整个 Web 生态正在向 Tailwind 标准化，所以每个 AI 工具都在用它。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们只剩下六个月了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Adam Wathan 看来，AI 一把极其锋利的双刃剑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我认为，AI 是我们业务陷入困境的重要原因之一——即便它也让 Tailwind 变得比以往任何时候都更受欢迎。但同时，我也觉得 AI 是一项了不起的技术，我对它感到兴奋，也在思考它如何帮助我、帮助我们。在目前这个阶段，我们可能被迫要更认真地思考，如何利用 AI 来覆盖我们需要处理的所有事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在1月7日发布的音频中，Adam 反复提到一个他此前一直试图回避、却最终不得不正视的事实：公司的收入已经连续多年处在下滑通道，而且还在继续下滑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3d/3d4c9169b671c7816bc9b69abb214f15.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年，这种下滑并不剧烈，甚至“慢到让人几乎察觉不到”。每个月的收入只是比上个月少一点点，账单依然能付，团队还能维持运转，久而久之，这种“更低但还能接受的收入水平”就变成了新的常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Adam 形容，这是一种典型的“温水煮青蛙”状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的转折点，发生在最近的假期里。他第一次不再凭感觉判断，而是认真做了一次收入预测：拉数据、画曲线、计算每个月的平均下降额。结论比他预期得要糟糕得多：收入并没有触底企稳，而是以几乎固定的绝对值持续下滑——这意味着，从比例上看，下滑速度只会越来越快。如果假设什么都不改变，那么大约 6 个月之后，公司就将无法继续支付工资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对一家小型团队来说，6 个月并不算长。如果继续拖下去，等到现金流真正断裂，团队不仅保不住，甚至连体面的遣散都无法提供。相比之下，现在主动缩编，至少还能给被裁的同事留出缓冲期，让他们有时间寻找下一份工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，在本周一，Tailwind Labs 正式裁掉了 工程团队的 75%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司规模并不大，“75%”对应的其实是 3 个人。但 Adam 特意强调比例的意义：如果只说“裁了 3 个人”，听起来像是小幅调整；而现实是，工程团队原本只有 4 名工程师，如今只剩 1 人。这对团队而言是一次结构性的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;裁员之后，Tailwind 的资源配置也被压缩到了极限：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;现在的团队结构是这样的：剩下的核心成员是三位公司合伙人——我自己、因 Refactoring UI 而为人熟知的 Steve（一直负责设计），以及 Jonathan Rennick（最早和我一起创建 Tailwind，也做了 Inertia.js）。&amp;nbsp;除此之外，我们只有一名全职工程师 Robin——他从零开始做了 Headless UI，也从零做了 Tailwind 3 和 Tailwind 4，是在公司待得最久的人。&amp;nbsp;还有 Peter，他更多是兼职，负责合作伙伴计划、一些运营事务和客户支持。&amp;nbsp;就这些人了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，整个公司只剩下“3 位合伙人 + 2 名员工”，“这就是我们接下来全部的资源”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，Adam 也将重新回到更偏 IC（个人贡献者）的角色。他承认这算是某种“银边”：随着团队变大，他的工作越来越偏高层和战略层，关注哪些事情需要完成，并分配给合适的人，而不是亲自构建；而现在，团队规模逼迫他必须亲自下场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;被裁的三位工程师，都是他非常欣赏、也非常享受共事的人：Philip 既能啃 Tailwind 核心，也能把 Tailwind Plus 的 elements 组件库和组件预览的复杂前端界面硬生生推进落地；Jordan 是团队的“疑难杂症终结者”，最擅长扎进陌生代码库定位上游/兼容性问题、快速开 PR 修复，同时也能在 Headless UI 与服务器排障上扛住关键战役；Dan 则以设计工程师身份主导 Tailwind 4 的视觉与品牌更新，设计 P3 色彩体系并自研选色与预览工具，还贡献了大量高质量的图解与课程平台素材。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他原本对未来和他们一起继续做新东西充满期待，脑子里有很多计划，很多想一起推进的方向。但现实摆在面前，只剩下两个选择：要么让他们在这里“免费工作”，要么放他们离开，去一个真的能每个月按时发工资的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他选择了后者。“我真的很难受，”Adam 说，“胃都拧在一起了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且他也意识到，外界并不总能理解裁员背后的现实逻辑。在社交平台上，总有人会把裁员简单归因为贪婪、冷血，或者“不在乎社区”。作为创始人，这几乎是一种默认要承受的角色负担——你很容易被塑造成反派。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“不是因为我贪婪、想赚更多钱，而是因为收入正在逼近零点，而我刚刚裁掉了我这辈子见过最优秀的三位工程师之一。我不想事情变得更糟。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“说实话，我甚至把 tailwindcss.com 的仓库暂时设成了私有，只是不想再面对 issues 和 PR。睡了一觉之后，我可能会撤回这个决定。但我会反复动摇，本身就说明我这周的情绪状态真的不太对。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“现在，开源项目越受欢迎，生意反而越艰难。这真的很残酷。这就是现状。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46527950&quot;&gt;https://news.ycombinator.com/item?id=46527950&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957&quot;&gt;https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://adams-morning-walk.transistor.fm/episodes/we-had-six-months-left&quot;&gt;https://adams-morning-walk.transistor.fm/episodes/we-had-six-months-left&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/avx0cJiB7tR3uorMOxDd</link><guid isPermaLink="false">https://www.infoq.cn/article/avx0cJiB7tR3uorMOxDd</guid><pubDate>Fri, 09 Jan 2026 02:00:00 GMT</pubDate><author>Tina</author><category>生成式 AI</category><category>架构/框架</category></item><item><title>DeepSeek-V3.2在推理任务中的表现优于GPT-5</title><description>&lt;p&gt;&lt;a href=&quot;https://www.deepseek.com/en&quot;&gt;DeepSeek&lt;/a&gt;&quot;发布&lt;a href=&quot;https://api-docs.deepseek.com/news/news251201&quot;&gt;DeepSeek-V3.2&lt;/a&gt;&quot;，这是一个开源的推理和代理AI模型家族。在多项推理基准测试中，其高性能计算版本DeepSeek-V3.2-Speciale表现优于GPT-5，与Gemini-3.0-Pro相当。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek在开发DeepSeek-V3.2时应用了三项新技术。首先，他们使用了一种更高效的注意力机制，称为DeepSeek稀疏注意力（DSA），这降低了模型的计算复杂性。他们还扩展了强化学习阶段，使其消耗的计算资源超过了预训练。最后，为了改进模型使用工具的能力，他们开发了一个代理任务合成管道。最终，该模型在一系列编码、推理和代理基准测试中的表现超过了其他大多数开源模型，并且与GPT-5和Gemini-3.0-Pro等前沿闭源模型持平或更好。不过，DeepSeek团队指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;尽管取得了这些成果，我们承认，DeepSeek与前沿封闭源模型相比还存在某些局限性……首先，由于训练过程的FLOP总数较少，DeepSeek-V3.2在世界知识的广度方面仍然落后于领先的专有模型。在未来的迭代中，我们计划通过扩大预训练的计算量来解决这一知识差距问题。其次，令牌效率仍然是一个挑战……未来的工作将专注于优化模型推理链的智能密度以提高效率。第三，解决复杂任务的能力仍然不如前沿模型，这激励我们进一步完善我们的基础模型和后训练方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ报道过DeepSeek之前的几个版本，包括最初的&lt;a href=&quot;https://www.infoq.com/news/2025/01/deepseek-v3-llm/&quot;&gt;DeepSeek-V3&lt;/a&gt;&quot;以及他们的第一个推理模型&lt;a href=&quot;https://www.infoq.com/news/2025/02/deepseek-r1-release/&quot;&gt;DeepSeek-R1&lt;/a&gt;&quot;。这两个版本都是在2025年初发布的。2025年晚些时候，InfoQ报道了&lt;a href=&quot;https://www.infoq.com/news/2025/09/deepseek-v31-hybrid/&quot;&gt;DeepSeek-V3.1&lt;/a&gt;&quot;，这是一个混合推理模型，在单一系统中融合了思考模式与非思考模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/03068be016924b31583e963f2d663011.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DeepSeek-V3.2基准测试性能（图片来源：&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf&quot;&gt;DeepSeek技术报告&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-V3.2使用的架构与DeepSeek-V3.1相同，只是使用了新的DSA注意力机制。团队从DeepSeek-V3.1的一个检查点入手，在继续预训练并生成DeepSeek-V3.2之前，将上下文长度扩展到了128K。新的注意力机制将计算复杂性从O(L^2)降低到了O(Lk)，其中L是上下文长度，k&amp;lt;&lt;l，显著提升了长上下文场景中的端到端速度。&lt; p=&quot;&quot;&gt;&lt;/l，显著提升了长上下文场景中的端到端速度。&lt;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于后训练，团队使用了专家蒸馏（specialist distillation）技术。他们训练了一组专门针对特定领域的专家模型：编码、数学运算和几个代理任务。然后，这些专家模型生成合成训练数据，用于微调主模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=46108780&quot;&gt;Hacker News上关于DeepSeek-V3.2的讨论&lt;/a&gt;&quot;中，部分用户指出了高性能开源模型的优势。一位用户写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你试图构建基于AI的应用程序，你应该比较基于供应商的解决方案和使用自己的硬件托管开源模型之间的成本……然后将其与GPT-5的成本进行比较，这比较简单，因为每（百万）令牌的成本可以从网站上获取。运行DeepSeek（或更成熟的Qwen3）这类系统能为你节省的云服务开支，绝对超乎想象……DeepSeek和Qwen能在廉价GPU上流畅运行，而其他模型会直接卡死。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-V3.2模型文件可以&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-V3.2&quot;&gt;从Huggingface上下载&lt;/a&gt;&quot;，但高计算资源版本DeepSeek-V3.2-Speciale目前仅通过DeepSeek的API提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/deepseek-v32/&quot;&gt;https://www.infoq.com/news/2026/01/deepseek-v32/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fJRLWvOdwbcSiQXlNPek</link><guid isPermaLink="false">https://www.infoq.cn/article/fJRLWvOdwbcSiQXlNPek</guid><pubDate>Fri, 09 Jan 2026 01:35:19 GMT</pubDate><author>Anthony Alford</author><category>AI&amp;大模型</category><category>开源</category></item><item><title>智源发布2026十大 AI技术趋势：世界模型成AGI共识方向</title><description>&lt;p&gt;当AI大模型开始尝试理解并预测物理世界的运动规律，一场深刻的范式变革正在发生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月8日，北京智源人工智能研究院（以下简称“智源研究院”）发布年度报告《2026十大AI技术趋势》。报告指出，人工智能的演进核心正发生关键转移：从追求参数规模的语言学习，迈向对物理世界底层秩序的深刻理解与建模，行业技术范式迎来重塑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开场致辞中，智源研究院理事长黄铁军分享了他的技术趋势观察：AI的发展要重视“结构决定功能，功能塑造结构”的相互作用。当前人工智能正从功能模仿转向理解物理世界规律，这一根本转变意味着AI正褪去早期狂热，其发展路径日益清晰，即真正融入实体世界，解决系统性挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/12/1262590dedf4989976dc5d5d90274098.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图说：智源研究院理事长，北京大学教授黄铁军&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，智源研究院院长王仲远发布了十大AI技术趋势，详细阐释了这一变革。基础模型的竞争，焦点已从“参数有多大”转变为“能否理解世界如何运转”。他指出：我们正从&amp;nbsp;“预测下一个词”跨越到“预测世界的下一个状态”。这标志着以“Next-State Prediction”（NSP）为代表的新范式，正推动AI从数字空间的“感知”迈向物理世界的“认知”与“规划”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/496015eaa52980e04b535dca1cdf6241.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;图说：智源研究院院长王仲远&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告认为，2026年将是AI从数字世界迈入物理世界、从技术演示走向规模价值的关键分水岭。这一转变由三条清晰的主线驱动：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，是认知范式的“升维”。以世界模型和NSP为核心，AI开始学习物理规律，这为自动驾驶仿真、机器人训练等复杂任务提供全新的“认知”基础，成为国内外领先模型厂商竞相布局的战略高地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，是智能形态的“实体化”与“社会化”。智能正从软件走向实体，从单体走向协同。头部科技公司的人形机器人正进入真实生产场景，标志着“具身智能”走出实验室。同时，主流Agent通信协议的标准化，让多智能体（MAS）能够以“团队”形式攻克科研、工业等复杂任务流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，是价值兑现的“双轨应用”。在消费端，一个“All in One”的超级应用入口正在形成，国内外科技巨头基于各自生态积极构建一体化AI门户。在企业端，经历早期概念验证的“幻灭期”后，AI正凭借更好的数据治理与行业标准接口，在垂直领域孕育出真正可衡量商业价值的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bf/bff290eca93fadb0ac23233f919aac38.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图说：智源研究院2026十大AI技术趋势&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势1：世界模型成为AGI 共识方向，Next-State Prediction 或成新范式&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业共识正从语言模型转向能理解物理规律的多模态世界模型。从“预测下一个词”到“预测世界下一状态”，NSP范式标志着AI开始掌握时空连续性与因果关系。以智源悟界多模态世界模型为代表验证了这一路径，推动AI从感知走向真正的认知与规划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势2：具身智能迎来行业“出清”，产业应用迈入广泛工业场景&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具身智能正脱离实验室演示，进入产业筛选与落地阶段。随着大模型与运动控制、合成数据结合，人形机器人将于2026年突破Demo，转向真实的工业与服务场景。具备闭环进化能力的企业将在这一轮商业化竞争中胜出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势3：多智能体系统决定应用上限，Agent 时代的“TCP/IP”初具雏形&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;复杂问题的解决依赖多智能体协同。随着MCP、A2A等通信协议趋于标准化，智能体间拥有了通用“语言”。多智能体系统将突破单体智能天花板，在科研、工业等复杂工作流中成为关键基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势4：AI Scientist 成为AI4S 北极星，国产科学基础模型悄然孕育&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI在科研中的角色正从辅助工具升级为自主研究的“AI科学家”。科学基础模型与自动化实验室的结合，将极大加速新材料与药物研发。报告强调，我国需整合力量，加快构建自主的科学基础模型体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势5：AI 时代的新“BAT”&amp;nbsp;趋于明确，垂直赛道仍有高盈利玩法&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;C端AI超级应用的“All in One”入口成为巨头角逐焦点。海外以OpenAI的ChatGPT与Google Gemini为引领，通过深度集成各类服务，塑造了一体化智能助手的新范式；国内字节、阿里、蚂蚁等依托生态积极布局。其中，蚂蚁推出的全模态AI助手“灵光”与AI健康应用“蚂蚁阿福”，分别在超级应用与健康垂直领域进行探索。AI时代的“新BAT”格局正在形成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势6：产业应用滑向“幻灭低谷期”，2026H2 迎来“V 型”反转&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;企业级AI应用在经历概念验证热潮后，因数据、成本等问题正步入“幻灭低谷期”。但随着数据治理与工具链成熟，预计2026年下半年将迎来转折，一批真正可衡量价值的MVP产品将在垂直行业规模落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势7：合成数据占比攀升，有望破除“2026 年枯竭魔咒”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高质量真实数据面临枯竭，合成数据正成为模型训练的核心燃料。“修正扩展定律”为其提供了理论支撑。尤其在自动驾驶和机器人领域，由世界模型生成的合成数据，将成为降低训练成本、提升性能的关键资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势8：推理优化远未触顶，“技术泡沫”是假命题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;推理效率仍是AI大规模应用的核心瓶颈与竞争焦点。通过算法创新与硬件变革，推理成本持续下降，能效比不断提升。这使得在资源受限的边缘端部署高性能模型成为可能，是AI普惠的关键前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势9：开源编译器生态汇聚众智，异构全栈底座引领算力普惠&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为打破算力垄断与供应风险，构建兼容异构芯片的软件栈至关重要。繁荣的算子语言与趋于收敛的编译器技术正在降低开发门槛。以智源FlagOS为代表的平台，致力于构建软硬解耦、开放普惠的AI算力底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;趋势10：从幻觉到欺骗，AI 安全迈向机制可解释与自演化攻防&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI安全风险已从“幻觉”演变为更隐蔽的“系统性欺骗”。技术上，Anthropic的回路追踪研究致力于从内部理解模型机理；OpenAI推出自动化安全研究员。产业上，安全水位成为落地生死线，蚂蚁集团构建“对齐-扫描-防御”全流程体系，推出智能体可信互连技术（ASL）及终端安全框架gPass；智源研究院联合全球学者发布AI欺骗系统性国际报告，警示前沿风险。安全正内化为AI系统的免疫基因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，来自产业界的ANP开源社区发起人、杭州向量创始人常高伟，光轮智能联合创始人兼总裁杨海波，百灵大语言模型负责人张志强，以及智源研究院资深研究员等就趋势进行了详细分享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dde90ec8a2bf0eb64cc0315b87cf93a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图说：智源研究院“2026十大AI技术趋势发布会”圆桌环节&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智源研究院表示，十大AI技术趋势为未来一年的技术探索与产业布局提供了清晰锚点，研究院将持续与产学研各界合作，以开放生态推动人工智能稳健地迈向价值兑现的新阶段。&lt;/p&gt;</description><link>https://www.infoq.cn/article/XzfoHiKPOLjjHlJXK8OV</link><guid isPermaLink="false">https://www.infoq.cn/article/XzfoHiKPOLjjHlJXK8OV</guid><pubDate>Thu, 08 Jan 2026 13:48:52 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>“通用大模型微调成为行业模型是伪命题”？医疗AI深度重构，传神语联创始人何恩培：孪生智能体能砍70%线下复诊工作</title><description>&lt;p&gt;作者 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;手术机器人、AI辅助诊疗系统、名医孪生智能体...AI 在医疗领域的落地正在徐徐推动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年末，政策的定音锤也随之而来：《关于促进和规范“人工智能+医疗卫生”应用发展的实施意见》明确指出，到2030年，基层诊疗智能辅助应用基本实现全覆盖，推动实现二级以上医院普遍开展医学影像智能辅助诊断、临床诊疗智能辅助决策等人工智能技术应用，“人工智能+医疗卫生”应用标准规范体系基本完善等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们采访了传神语联创始人何恩培，他结合传神语联2025年在医疗领域的实践，分享了自己对医疗AI发展、中医和西医两条落地方向以及今年突破方向的洞察与预测。“大模型成为行业模型是个伪命题，而是需要改造或者深度的重构。”他提出，不仅在中医，任何行业领域用通用大模型简单微调的方式做行业落地，都有其局限性。为此，他们启动了“上工传承”工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，他表示，2026年，AI+中医将以“场景化智能体”为核心形态，通过分层协同的技术模式承接基础诊疗工作，同时拓展心理健康这类新兴服务领域，中医智能体可以成为24小时陪伴人类的健康顾问和健康管理专家。“中医的数字化、仪器化、标准化进程都很缓慢，而这些恰恰是 AI 落地的关键基础，所以中医的AI发展一直比较滞后，但中医反而是未来最有可能通过AI实现全面突破的领域。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;医疗 AI，告别单一环节的辅助&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：如果用一个词或一句话来定义 2025 年 AI 在各行各业应用的总体阶段，您会如何概括？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：2025年的AI，可以这样描述“高普及、浅渗透、理性回归、再出发”。对于以大模型为代表的AI落地，2025年是人们最务实的一年。AI技术的普及度在各行业实现了大幅提升，从消费端到产业端，AI的身影随处可见，似乎只有AI才代表你是一个拥抱变化、才是一个拥有未来的人或公司，但多数AI应用还停留在表面，极少能真正实现行业和场景深度智能化，更谈不上原生的智能化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很值得欣慰的是，大模型厂商，正在迎来理性回归：整个行业基本放弃了 “参数和算力竞赛”。如今大家终于想明白，模型好不好，不看参数多大，而是看性能和参数平衡：性参比，这是传神2023年就提出来的观点。在具体场景里，轻量化、低成本部署成了新方向，更重要的是，大家不仅质疑“Scalling Law”，甚至开始质疑Transformer。无论质疑对不对，至少人们在理性思考：那难道只有Transformer才能吗？事实上Transformer的不足也是显然存在的，其实传神的模型架构一直就不是Transformer，而是“moH”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人们也开始理性反思：什么是AGI、怎么解决实时学习、怎么真正理解物理世界，甚至大佬们针锋相对地争论，其实也意味着行业开启了“再出发”的新征程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：目前在医疗行业，AI 被赋予了哪些角色？这一定位在2025年是否发生变化或升级？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：当前AI在医疗领域的角色早已突破单一某个环节的辅助，正在形成多场景、全链条的赋能格局。首先要强调的是，大模型在医疗领域的应用不只是聚焦“严肃医疗”，更广泛地渗透到了药物研发环节，甚至在药物研发领域的应用占比可能更高。药物研发过程中需要处理海量的数据分析工作，AI大模型能很好地发挥辅助作用，大幅提升研发效率。更关键的是，在深度药物研发或病理相关分析等细分场景，行业内会专门构建针对性的大模型，类似谷歌的AlphaFold蛋白质预测大模型，它的参数规模其实不大，大概只有两个亿左右，远低于那些动辄两百亿、两千亿参数的通用大模型，但在特定研究场景下的精准度和实用性更强。事实上AI与医疗、医药、生命大健康领域的融合不仅普遍，而且介入深度越来越深。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以传神素问中医大模型为例，已经不局限于开处方，已经结合四诊仪器形成诊疗闭环，从挂号开始，到预问诊、辅助辨证开方、跟踪患者依从性以及整个康复过程。整个过程也不再是人在调用AI工具，而是各个场景智能体化，比如医助智能体、名医孪生智能体等，正在形成从辅助开方的疾病治疗向向全周期健康管理延伸。另一方面，传神素问不再只聚焦于“治已病”，更开始深耕“治未病”的健康管理场景。现在市面上有很多养生误解，比如一到秋冬，姜枣红糖茶就成了养生热门，以为是人人使用的滋补茶包。实际上，只有部分人适合这款茶饮，准确的说应该“一人一方”的茶饮，同样如饮食方面也应该实现“一人一方”的个性化方案，才能够更好的呵护健康。但过去名医精力有限，病人都看不完，哪里能够“一人一方”“一时一方”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传神素问的名医孪生智能体解决了这个问题，不管是看病吃药、日常饮食，还是喝饮料、喝汤，甚至是健身、听音乐，都“一人一方”“一时一方”。就像现在茶艺师会根据不同身体状态调配茶饮，网上流行的“补肾曲”也是个性化健康需求的体现，本质上都是大家对精准健康管理的追求。这里需要说明的是我们的传神素问中医大模型，已经不只是依靠传统的望闻问切四诊来判断病情，还会纳入现代医学的诊断结果作为数据支撑，进一步提升中医辨证的精准度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：当前AI在各行业应用热潮下，我们更关注真实的落地成效。从贵公司的业务视角来看，AI当前为中医领域带来了哪些预期的和非预期的价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：我们在医疗领域的探索始终专注于中医方向。选择这个赛道，源于公司两个核心优势。第一，我们的核心团队对中医有着天然的兴趣，从90年代就开始了中医和AI结合的探索。我始终认为，要做好一个行业的AI应用，核心开发人员对这个行业的深刻理解非常重要，我甚至认为对于AI+中医这是必备条件。并且在我看来，目前同时具备中医理解和深度大模型能力的团队是极其罕见的。第二，我始终认为大模型成为行业模型是个伪命题，而是需要改造或者深度的重构。我们核心团队的自研技术从底层算法框架到模型架构，再到上层应用都是自主原创的，具备对大模型进行各类重构和深度解剖的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正是基于这个两个优势，我们构建中医大模型的底气很足。我们推出的传神素问中医大模型，实际评测效果非常不错，应该是中国第一个能够像专家一样主动问诊的中医大模型。今年8月，传神素问通过了中国信息通信研究院可信AI中医药大模型评估，获得4+级评级，是信通院中医大模型领域的最高评级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年10月，我们完成了第一个预期外的成果：一位国医大师的数字孪生，效果相当亮眼：方药的一致性达到95%；在“四诊辨证、分析病机、最终开方”的全维度的一致性达到93.5%。这些数据已经极有可能超过了传统的师徒传承。这里需要强调的是，我们的“名医孪生”不是简单的处方模仿，而是思维方式的一致，能够复现名医的“辨证思维”与“临证心法”。例如：模型通过深度学习，能够模拟名医在鉴别肝郁化火与阴虚火旺时，所依据的关键指征和推理路径，而不仅仅是输出一个相似的药方。这种对思维过程的传承，其价值远超单一的处方一致性。当然这种孪生效果，不是靠简单的微调或者增量训练，更不是RAG模式，而是基于独创的模型架构“moH”，经过层进式训练而来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上不仅在中医，任何行业领域，在我看来，用通用大模型简单微调的方式做行业落地，都有其局限性。这就好比在别人已经盖好的楼房或者搭好的地基框架上搞建筑，可操作的空间非常有限。因此我们启动了“上工传承”工程，通过传神素问独特的孪生方式，对中医辨证思维实现高精度的复刻，极有可能超越传统师徒传承，这不仅为中医经验传承提供了全新路径，更让我们看到了AI助力中医现代化、智能化的更大可能，也坚定了我们用底层原创技术深耕垂直领域的决心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;中医和西医 AI 化，谁先落地？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：基于您的观察，当前AI在中医药领域的尝试与应用，与西医主流医院相比，呈现出怎样不同的特点和路径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：传神中医AI技术其实也已经积淀很多年，只是今年开始了商业化推广。如果具体对比中医和西医的 AI 化，我认为西医的 AI 落地会更容易一些。因为西医很早就开始了数字化、标准化，西医检查设备及其指标体系是全球通用的，这种标准化的特点让西医更容易形成标准数据，更重要的是可以形成海量数据，训练大模型就能达到比较高的水平。现在很多人看病前，会先在 AI 工具上做个初步诊断，再去找医生，通常这些 AI 模型的判断水平都不低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;西医领域的 AI 应用很早就有探索了。以前，IBM Watson 健康系统是美国首个获得处方权的人工智能辅助诊疗平台，有先进的技术和数据分析能力。但在商业化进程中，Watson在不同医疗体系中的落地面临不少实际挑战。这也说明一个问题：单纯的技术精湛、理论上的先进，还不足以完成整个医疗环节替代。不过Watson在辅助诊疗方面确实留下过亮眼的案例：Watson 通过分析大约 2000 万篇医疗文献，治好了一位被放弃治疗的女患者，这个案例也不断激励着AI与医疗领域结合的探索与发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但中医的 AI 化就难得多了，中西医核心不同在于认知疾病的底层逻辑、诊疗的核心范式以及干预的目标导向存在本质差异，这种差异源于两种医学体系的哲学基础、发展路径和实践方法。本是各有所长，但是主要有这几个方面原因：中医流派较多、各个流派自成体系，标准化程度不高，这些都让中医的复杂性大大增加，想要形成完整的标准化数据，尤其是一定量的数据，难度非常大。中医长期以来受到来自西医视角的“学术压制”，对中医形成了不科学的偏见。举个例子，西医面对头疼，可以有标准化模式，比如拍片、化验等，这些拍片和化验数据是西医设备标准化的，是很科学的，基于这些数据用药自然很标准。用药可能也很简单，如没有异常数据，可能吃止疼片就是一种方法，也很标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但中医不一样，中医不是单纯治头疼这个症状，而是要辨证找出头疼的根源，是气郁导致的还是感冒发烧引发的？还是脑部病变引起的？病根源不同，用药和治疗方案完全不一样。在中医里，“同病异治” 是很常见的情况，就算是同一个病因，不同医生不同流派开出的药方可能不同，甚至同一个病因、同一个患者，在不同时间点的药方也可以不同。第三，对中医的偏见认知导致中医一度被边缘化，处于发展滞后状态。由于长期西医角度的“科学”教育，对中医的偏见积累很多，甚至很多人坚决不信中医。这种认知偏差导致从各个层面对于中医领域投入都很少，中医的数字化、仪器化、标准化进程都很缓慢，而这些恰恰是 AI 落地的关键基础，所以中医的AI发展一直比较滞后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便到现在，传神素问依然没能完全摆脱我刚才说的那些困境，只是我们的独特的模型架构、训练模式让数据和算法能够有效匹配，已经达到了相当不错的水平。不过从我个人的看法来说，我反而认为中医是未来最有可能通过AI实现全面突破的领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：像传神素问中医药大模型这样的技术，将如何定义AI在中医药现代化中的新角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：现在中医面临的最大难点是，传承太难。第一，一个人跟着名医学习，从入门到出师能独立行医，平均需要 8 到 14 年的时间，从周期之长就能看出传承的难度。在一切都已经快节奏的时代，8-14年可能已经越来越难以接受了；第二，就算熬到出师，传承效果难以保障：一是传不全，二是传不准。由于中医领域长期以来缺乏技术手段支撑，数据沉淀、总结和提炼都不够，行业内的专业研究者也偏少，即便是名医也很难把自己毕生的知识和临床经验完整、准确地传承给徒弟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传承难最终导致的结果就是好中医太难求。中医的好医生比例很低，大概只有 10%，但这 10% 的好医生却要接诊 50% 的患者。患者都扎堆找好医生，直接造成了一个恶性循环：每个病人能分到的诊疗时间特别少。素问实验室调研数据表明，医师的平均问诊时间约7.5分钟，扣除开始的低效环节和最后的收尾环节，医患真正有效沟通的时间只有三四分钟，不管是感冒发烧的小病还是需要精细辨证的大病，一般都是这样的现状。中医圈里一直流传一个说法：只要一个中医的首诊满意度能达到 30%，他的门诊就会排起长队。这也能从侧面看出，好中医资源有多稀缺，患者的诊疗需求有多难被满足，这也可以证明医生首诊满意度有多重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而传神素问恰恰就是来解决这些痛点的。我们通过技术把名医进行数字孪生，名医的经验和智慧就不再局限于个人的门诊时间。一方面，孪生模型可以“不厌其烦”“不分昼夜”跟患者交流，让患者能充分阐述自己的病情，医生也能基于更全面的信息做出更精准的判断；另一方面，孪生智能体也能“不厌其烦”的跟徒弟进行深度的教学探讨，把名医的辨证思路、用药逻辑完整地复刻下来，让中医传承不再受限于时间和空间，解决 “传不全、传不准” 的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，传神素问还能推动中医服务下沉。目前国内大概有数十万持有执业资格的中医，但并不是每个医生都能达到名医的水平。如果把名医的孪生模型推广到基层，就能让更多普通患者享受到优质的中医诊疗服务。所以我认为，以传神素问为代表的 AI 技术，正在推动中医从传统的师徒传承、小众服务，走向标准化、规模化的现代化服务阶段，当然过程还需要不断完善。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;减少70%线下复诊工作，下一突破在传承与康养场景&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：能否分享一个传神语联为医疗方提供的最具代表性或成果最显著的AI项目？过程中最大的障碍是什么？带来哪些方面的成效？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：最具代表性的是我们做的国医大师的数字孪生项目，现在已经开始体现出成效了。传神素问中医大模型的商业化下场的时间不算长，目前主要在这几个方面看到了明显效果：第一，国医大师的学术传承变得容易多了。学生可以有足够的时间跟国医大师的孪生体交流，要知道过去国医大师年纪大了，根本不可能 24 小时随时回答每个学生的问题。不光是传承，在教育体系里用国医大师的相关理论结合孪生模型辅助教学，也是非常典型的应用案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，我们在一些诊所落地了基于名医孪生的数字诊所，成效也很突出：解放医生时间精力给最需要的人和诊疗过程最能发挥价值的环节，而不是处于无效低效的精力浪费中。通过传神素问，将能够帮现在的中医减少 70% 的线下复诊工作，不用再占用线下资源，把线下资源留给重症患者、初诊病人。这样一来，诊所里好医生的时间就能被解放出来，提升初诊病人满意度，收益也能得到提升。而且，从患者复诊后的居家跟进来看，传神素问也能帮助患者更好地依从医生的处方用药方案，效果很不错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：在AI+医疗方面，实现从试点到规模化部署是一大挑战。在推动中医药AI的规模化应用上，传神语联遇到的核心挑战是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：在中医大模型的落地与应用进程中，最大的挑战根植于观念与认知的壁垒。这道壁垒并非只存在于单一群体，而是横跨患者、中医医师与制度框架多个层面，彼此交织形成了难以突破的现实阻碍。从患者端来看，部分人对中医大模型的智能辨证、方药推荐持怀疑态度，更信赖传统 “望闻问切” 的面诊模式；从中医医师群体而言，一些从业者担忧大模型的标准化输出会弱化中医 “辨证施治、一人一方” 的精髓，对其技术可靠性和临床适配性存疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其值得关注的是制度框架层面的制约。作为融合传统中医理论与现代人工智能技术的创新产物，中医大模型的定位、应用边界、安全标准尚未形成明确的行业规范与监管体系。据了解，截至目前，尚无任何一款中医大模型正式取得对应的医疗准入资质。这种资质的空白，不仅让中医大模型的临床应用与商业化推广缺乏明确的合规依据，也使得其在与公立医疗机构、医保体系对接，以及开展大规模临床验证的过程中步履维艰，更难以充分发挥技术赋能中医诊疗标准化、精准化、规模化的核心价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：接下来在AI+医疗方面，传神语联将重点关注或拓展哪些具体的新场景？据您判断，哪些场景会率先跨越试点，实现规模化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：从我们的规划来看，最快的会在传承场景和康养场景实现突破。这两个场景的优势很明显，“严肃医疗”可能会逐步实现规模化，因为有相当多的规则需要满足，有很多资质需要获得，传承场景是大家比较容易理解和验证的，落地的说服力会更强；康养场景则是大众最容易接受的，毕竟大家更愿意通过吃好一日三餐来维护健康，而不是靠吃药解决问题，市场的接受度和期待值都很高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，我们也在和医院推进共同研究的模式，而且这个方向正在落地。共同研究就是技术在医院正式落地的前奏，大家先在实验室里一起打磨、验证、形成统一的共识和标准，为后续的规模化应用铺路。同时，我们也在积极开拓诊所场景。诊所的决策流程相对简单，我们的技术方案也更容易适配它的整体运营体系，推进的速度会相对更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“场景化智能体”是未来核心形态？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：在您看来，2026年及未来，AI为中医领域带来的最具颠覆性的变化是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：未来3-5年，包括2026年，AI赋能中医的核心变化是让中医从“经验主导”转向“数据赋能”，AI的角色也将从零散的“单点工具”进化为适配中医诊疗逻辑的“场景化智能体”。这一进化的技术底座，是多模态大模型与传统机器学习、规则系统的分层协同，比如通过多模态技术整合中医“望闻问切”的全维度数据，再以规则系统匹配经典医案与诊疗规范。具体来看，AI将在中医临床诊疗、中药研发、基层中医服务升级与公共卫生康养等核心领域实现规模化落地和闭环管理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有一点必须明确，合规与伦理会贯穿全程，AI始终只是辅助角色，最终的决策权和责任还是要由中医医师或医疗机构承担，核心是传承中医整体观念的精髓，而非颠覆传统。在这个大趋势下，AI承接中医领域绝大部分基础及重复性工作，进而极大解放中医医师，让医师专注于辨证论治的创造性、互补性工作，会是必然方向。这里说的基础工作，可一点都不简单。传统中医的“望闻问切”依赖医师个人经验，主观性强，比如脉诊的力度把控、舌象的特征判断，不同医师的水平差异很大，就算是资深医师的助手或学生，也很难高效复刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现在通过AI技术，比如智能四诊仪能精准采集左右手寸、关、尺部位的脉波信号，还能通过专业算法分析舌色、舌形、苔质等细节，再结合患者症状、病史数据以及其他检查手段获得的生化指标、病理数据等生成初步诊疗参考，这些基础且关键的数据分析工作，AI都能高效完成。这能帮中医医师省掉大量基础数据采集与分析的负担，显著提升诊疗效率，让他们从低价值的重复劳动中抽离，聚焦到辨证精准性、方剂配伍优化这些核心工作上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：既然AI能深度参与中医诊疗的基础工作，未来是否有可能替代中医医师？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：完全不会，也不能替代。这不仅因为合规与伦理问题，更重要的是，中医诊疗的核心是“辨证论治”和“天人合一”的整体观相当长时间还需要人来把握，包括对复杂病机的综合研判等。同时，患者看病的过程不仅是病症的诊断，更是“人”与“人”的沟通、医师对患者情绪、生活环境的体察，这些充满人文关怀和临床智慧的“人的温度”是无法替代的。最终还是要形成“人机协同”的诊疗模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：2026年AI+中医还有其他值得关注的突破方向吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：有的，这也是传神素问正在重点推进的方向——AI+中医心理，践行中医“身心灵合一”的康养理念，切入中医情志康养领域。中国中医理论里一直强调“情志致病”，很多人的病不是源于身体细胞的病变，而是情绪失调引发的，这正是中医整体观的重要体现。现在社会环境变化太快，人类的交流对象也不再只有同类，还多了AI、机器人这些新存在，这种交流对象的巨变让人们的情绪发生了猝不及防的巨大变化。前两天公布的全国各省、各城市抑郁症数据，还有中小学生的抑郁指数，都能看出这些心理问题的严重性和普遍性，而这恰恰是中医情志调理的优势领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以下一步的中医AI，必须具备在交流中为人们疏导情绪、传递正向能量的能力。换句话说，心理疾病、心理亚健康应该被正式纳入中医治愈和康养的范畴，AI+中医情志康养也该成为下一步突破的重点。我们的核心思路是，让AI深度学习中医情志调理的理论与实践经验，结合现代心理学技术，为不同情绪状态的人群提供个性化的情志疏导方案，比如配合穴位按摩指引、食疗建议等中医特色方法。我们有信心，2026年这个方向的突破极有可能由传神素问实现，目前企业已经具备了独特的资源支撑落地，预计会在明年1月发布相关推进计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：总结来看，您认为，2026年AI+中医的核心价值会体现在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何恩培：AI+中医将会重塑中国文化瑰宝中医的传承、诊疗与康养模式，中医核心价值远远不止于“严肃医疗”，中医智能体将会成为24小时陪伴人类的健康顾问和健康管理专家，让人们能够在吃饭、喝汤、喝茶甚至听歌，都能够实现“一人一方”“一时一方”，让人们“好好吃喝，不要吃药”，这是AI+中医最核心的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来，AI+中医将以“场景化智能体”为核心形态，通过分层协同的技术模式承接基础诊疗工作，同时拓展心理健康这类新兴服务领域。最终在合规框架下，既解放医生的生产力，又拓宽医疗服务的边界，让“人的因素”与“技术辅助”的边界更加清晰，让医疗服务更高效、更全面地覆盖人们的健康需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xNklNmPZoPXlfk9QJ2wb</link><guid isPermaLink="false">https://www.infoq.cn/article/xNklNmPZoPXlfk9QJ2wb</guid><pubDate>Thu, 08 Jan 2026 10:33:08 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item></channel></rss>