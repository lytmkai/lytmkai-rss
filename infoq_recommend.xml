<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Thu, 15 Jan 2026 12:06:07 GMT</lastBuildDate><ttl>5</ttl><item><title>刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖</title><description>&lt;p&gt;2026 年，AI 真正“下地干活”的第一战，被阿里打响了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 15 日，在杭州阿里园区举行的千问 App 发布会上，阿里巴巴集团总裁吴嘉做了一次并不复杂、却很直观的演示：他用千问给现场嘉宾点了 40 杯“伯牙绝弦”奶茶。整个过程没有人工介入。千问自行匹配附近奶茶店，下单，并调用支付宝完成支付。没一会儿，淘宝闪购的骑手把奶茶送进会场。发布会的气氛，也在这一刻被彻底点燃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事后，有杭州的网友恍然大悟“怪不得刚刚西溪附近叫不动外卖！”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba72fd559beaf5ece58490ff5c1ae8b3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比 PPT 上的参数和模型指标，这个场景更容易被理解：AI 第一次在公开场合，完整地替人把一件现实中的事情办成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次更新中，阿里将千问定位成&amp;nbsp;“每个人的生活助手”。路径也很明确：不从新场景做起，而是直接接入阿里现有的业务体系，让 AI 先把眼前的事干好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;日常生活&amp;nbsp;层面，千问首批接入了&amp;nbsp;淘宝闪购、支付宝、淘宝、飞猪和高德&amp;nbsp;五大业务，可以一句话&amp;nbsp;点外卖、买东西、订机票、订酒店、查路线，这些原本需要在多个 App 之间来回切换的操作，现在可以交给一句话来完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6c659e25b3b2f5fd57a8386e902d2613.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;“办事”&amp;nbsp;这一层，千问的能力被进一步拉长。它开始尝试处理更复杂的任务，比如打电话订餐厅、整理调研资料、处理财务文件、辅助搭建网站等。这类功能目前仍处于定向邀测阶段，&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴嘉在发布会上表示：“AI 在拥有超强大脑之后，正在长出能够触达真实世界的手和脚，在生活中实实在在地替用户‘干活’。&amp;nbsp;千问的优势在于‘最强的 Qwen 模型’与‘阿里最完整的商业生态’的结合。AI 办事的时代才刚刚开始，我们会持续探索，把千问打造成真正有用的个人 AI 助手。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自千问上线两个月以来，月度活跃用户已突破 1 亿。&amp;nbsp;吴嘉认为，随着 AI coding、全模态理解以及超长上下文等关键能力逐步成熟，AI 正在走出手机屏幕，进入更复杂、也更真实的生产与生活场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把阿里折叠进千问中，&amp;nbsp;通过统一的 AI 入口，让千问拥有&amp;nbsp;400&amp;nbsp;余项办事能力，在&amp;nbsp;生活、办公、教育&amp;nbsp;等方面全场景覆盖，让千问成为 AI 时代的超级应用入口，这正是阿里的野心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;办事之上如何理解需求，才能判断是不是一个合格的助手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随着模型能力的跃迁，思考让 Agent 做事，已经是近几年行业的集体共识。但&amp;nbsp;干的活好不好，这才是能否放心 AI 当助手的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里此次的更新方向，既在意料之中，又有些意料之外的惊喜，这个惊喜的落脚点就在于&amp;nbsp;对需求的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对千问用户数据观察中，用户主动询问商品推荐的月环比高达 300%，这引起了阿里的注意，利用好千问与淘宝的链接，让千问拥有更可用的商品推荐能力，这确实踩中了不少人的真实需求，也成为千问区别其他通用 Agent 的功能独特切入点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d77d96dc23f662ce274acc00f1023a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅发挥了阿里在电商上的传统优势，也让庞大的商品供给和相对成熟的推荐体系真正被用起来。用户只需一句话，就能完成从商品推荐到下单的完整流程。其背后，是&amp;nbsp;阿里各业务接口的打通和协同调用，用起来足够顺，也足够省事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但更令人惊喜的是&amp;nbsp;对决策层面的关注，这也是&amp;nbsp;模型深入理解真实需求的表现，如何调用工具做更好的决策，体现了阿里强大的整合能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，现场展示了要给老人购买一款家庭扫地机，并且家里还养了一只猫，预算在 2000-4000 左右。千问在综合产品的价格与能力之上，还进一步老人的便捷需求与对猫毛的清洁效果，在综合这些复杂的条件后，给出推荐产品与相关理由，这正是大模型方便人类决策的一个虚拟需求感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a1/a141ee5ec5f8bdcb211d4a17eb9197e7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一个徒步推荐的方案中，千问不仅推荐出行路线，结合天气情况给出建议，还将徒步需要的产品直接发送到了千问界面上，确实让人看到 AI 未来融入世界的真实摸样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94c795db2602bc1789406c2e8d10bbac.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不是只做简单的一件事，而是将好多事做好，形成闭环，阿里已经迈出第一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者能想到的弊端，可能就是如何避免大模型被商家刷的假好评和广告垃圾数据污染，根据错误数据给出错误推荐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在一个全家人考虑去三亚出行的案例中，千问综合了路线、预算、老人与孩子的需求等，给出了路线选择，并给出三套酒店方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fe5cc6ca26ffb34e7e474aba35271405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，酒店的均价都在两三千左右，不少人吐槽这恐怕没人住得起，方案不适用，不接地气，这或许是笔者认为的阿里迈出的是“半步”，还需要进一步的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现场还有一个小惊喜是，千问演示现场定饭店的时候，有一段与老板确定需求的打电话环节，从包间大小，价格，有小朋友等需求进行多方拉扯沟通，直到最后，电话结尾说，“我是千问 AI 助手在与你沟通”，大家才恍然大悟，原来是千问的语音功能在完成订酒店的“最后一公里”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是各种多模态打通后，AI 能做到的程度，留给人更多想象空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种好用，同时体现在在对办公需求上，在更专业的场景上，需要更好的交付结果，要求也更难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;千问可以集成各种复杂工具，完成做表格、整理数据、处理报表、汇报 PPT 等各种具体业务。从如何处理资料到最后成品展现，从效果来看，确实还不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07fa85f8f5b6844bf016d30842fb6e9a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次，阿里找来了专业人士来验收干活效果，千万财经博主小 Lin 说，亲自下场演示了用千问生成一份《2026 毕业生就业报告》，从信息汇总，消化资料，角度分析，文章演示到 PPT 的生成，千问干了一个完整的活。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，如果把千问当做个工作三年内的大学生，来干这些活，效果还是不错的，如果要求更高，可能就是把控 PPT 的内容重点质量，PPT 的设计是否美观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/40fb2d9103811e218d0cafb2f183e743.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在教育领域，千问也做出一些精心设计，令人印象深刻的是在各种题目中，除了思路的讲解，还会生成一段动态视频进行图示演说，能随时对话沟通，给出思路和解法，并且多模态展示，这让千问更像一个人一样解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/feaccba85e81a081dd48331badff811f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者也亲自进行了一个上手测评，一个是用千问点奶茶，还有一个是用千问询问如何落户问题，千问都给出了较为实用的操作结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b1/b1c582f476f9719185aa2a395af5352d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总体来看，千问并没有试图一下子把所有事都做好，而是在尝试把复杂的事做得更完整、更贴近人的真实需求。它距离“完全可靠的 AI 助手”还有距离，但已经明显走出了聊天框，开始进入决策和执行的真实环节。而对干活质量的进一步打磨，恐怕正是阿里下一步要发力的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在几家最受关注的 AI 巨头中，字节跳动&amp;nbsp;选择从系统层切入，通过豆包手机助手借助操作系统能力，去调度第三方应用，与现实世界建立连接；阿里&amp;nbsp;的路线则更为直接，依托自身已高度成熟的电商、支付、物流、出行等业务体系，将这些能力整体接入千问，形成一个以自有生态为核心的闭环。腾讯&amp;nbsp;目前尚未对外展示完整方案，但从近期在 Agent 和多模态方向上的密集招聘来看，其下一步布局大概率仍将围绕微信这一超级入口展开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c08da37ff9047adf91ec2784c9765e8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;表面上看，Agent 之争比拼的是模型能力，但更深层的竞争，实际上取决于谁能更稳定、更规模化地承接真实世界的复杂需求。&lt;/p&gt;</description><link>https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</link><guid isPermaLink="false">https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</guid><pubDate>Thu, 15 Jan 2026 11:13:35 GMT</pubDate><author>高允毅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>“商业版 HTTP”来了：谷歌 CEO 劈柴官宣 UCP，Agent 直接下单，倒逼淘宝京东“拆家式重构”？</title><description>&lt;p&gt;谷歌把“Agent 购物”这件事，推到了一个更标准化的层面：Universal Commerce Protocol（UCP）正式亮相。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日（1月11日），谷歌 CEO Sundar Pichai（绰号“劈柴”） 首次登上 NRF（美国零售联合会年会），在题为“人工智能平台转型及零售业的未来机遇”的主题演讲中宣布了该协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照谷歌的说法，UCP 是一项新的开放标准，目标是让 Agent 能够在线上直接买东西。在实现机制上，UCP 通过定义一组“代理商务的构建模块”，把端到端的购物流程拆解成可复用的能力组件：既覆盖推动商品发现与购买的关键动作，也延伸到下单后的体验与服务等环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，这套设计将让生态系统在同一套标准下实现互操作，使任何 Agent 都能与任意商家进行对话，并自主完成从商品发现到结账的完整购物流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该标准采用 Apache 2.0 开源许可证发布：&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/ucp&quot;&gt;https://github.com/Universal-Commerce-Protocol/ucp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8ac749381cca480abba528ff41699a7b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人一看到这条消息就意识到：大事可能真要来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;风险投资人 Linas Beliūnas 在LinkedIn 上评论称：“谷歌刚刚对‘商业’做了一件类似 HTTP 当年对 Web 所做的事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他看来，UCP 的野心，是把电商 20 年来那条固定链路，“搜索—广告—商品页—结账”——压缩成“意图—Agent 推理—购买”：用户不再需要点击跳转，不再被迫参与 SEO博弈，也不再被传统的转化漏斗一层层“导流”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;进一步说，Beliūnas 认为，UCP 试图成为商业领域的“HTTP”——也就是所有由 AI 介导的交易背后，那层看不见、但不可或缺的基础设施，“品牌不再争夺用户注意力，他们将竞相争取被Agent选中。网站变得可有可无。这就是非人类商业的开端。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2b1750b564c1fe87991e9db7109580b5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期关注零售的连续创业者 Scott Wingo 甚至把谷歌这次在 NRF 上的一系列动作形容为一次“震撼与威慑（shock and awe）式”的进攻。他感叹自己在这个行业干了 30 年，“从来没见过现在这样的场面，真的太疯狂了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Wingo 看来，NRF 过去一直带着点“昏昏欲睡”的气质：讨论的多是收银系统、收银机、POS，以及超市自助结账的传送带这些传统议题。而如今，它几乎已经变成了一场围绕 Agent Commerce（智能体商业）展开的大会。“这种变化，是我做梦都想不到的。”他说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;统一零售界的新标准？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，UCP 到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单说，UCP 的目标是让 Agent 能够贯穿用户购买流程的各个环节：从商品发现、对比，到下单结账，再到购买后的支持服务，都可以在同一套标准下衔接起来。它想解决的核心问题是：用一个统一标准承载这些流程能力，而不是让商家和平台为不同 Agent、不同系统反复做一遍又一遍的对接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/affc2226740f2bd401efbcd843d05752.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从谷歌给出的设计图可以看到整体思路：左侧是各种消费者触点——消费者在这些地方与 Agentic Commerce 交互。在谷歌的世界里，这些包括 Google AI Mode、核心搜索、Gemini 等。右侧是后台系统——零售商后台需要的订单管理、库存管理等能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六项能力：产品发现、购物车、身份绑定、结账、订单，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六个圆角矩形，其中三个是实线框，三个是虚线框。实线框的，是已经宣布、可用的能力。尚未上线的三项是：产品发现、购物车，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;围绕这六项能力，Scott Wingo 也给出了更具体的解读：&lt;/p&gt;&lt;p&gt;产品发现（Product Discovery）：目前官方并没有披露太多细节，但他判断，这很可能会与后续对 Google Shopping Feed 规范的扩展绑定在一起。未来 UCP 可能会提供类似“开关”的机制：商家可以决定哪些商品对 Agent 开放，Agent 也可以通过协议以不同方式拉取商品信息——某种程度上，这有点像 Stripe 的 Agentic Commerce 套件思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;购物车（Cart）：这是他认为“最值得盯”的部分。谷歌在图里用虚线框把它标出来，像是在释放一个强信号：UCP 可能要去挑战电商的“圣杯”——跨商家、多商品、由商家作为交易主体（merchant-of-record）的统一购物车。一句话：“一个购物车管全网”。他认为 ChatGPT/ACP 可能也有类似目标，但谷歌这次等于把这个方向直接摆到台面上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;身份绑定（Identity Linking）：他推测这会涉及“识别你的 Agent”（某种 know your agent 的机制）、银行卡 token 化等能力，类似 Link 或 ShopPay 那套：如果系统能把你的身份与支付凭据映射成 token，就有机会实现自动填充信用卡信息等体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结账（Checkout）：谷歌准备把 “Buy for Me” 做一次大升级——新结账入口将同时出现在搜索 AI Mode 和 Gemini 应用的符合条件商品页中，流程被压成三步“商品 → 确认订单 → 下单完成”，并将率先在美国上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;订单（Order）：一旦开始“在对话里结账”，就必须有一套双向的订单体验。一边是面向消费者：查看订单、取消、退货等；另一边是面向商家：拉取订单、处理履约、上传物流信息，并完成一整套购买后流程（退货、评价等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他垂直能力（Other Vertical Capabilities）：这部分目前更像一个“兜底项”，官方也没有给出更多细节。他猜测它可能用于未来扩展到更多品类/行业，比如汽配、生鲜、B2B 等。当天新闻里被提到的客户之一是 Papa Johns（达美乐/披萨这种即时零售/本地履约场景），因此也不排除这块会成为一种“插件位”，让类似“ChatGPT App”式的体验从 UCP 的侧边接入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这些能力下方，还有三个模块，代表底层通信方式：API、MCP，以及 A2A。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌同时强调，UCP 并不是一套孤立协议，它可以与其他 Agent 协议协同使用，例如其在去年发布的 Agent Payments Protocol（AP2）、Agent2Agent（A2A） 以及 Model Context Protocol（MCP）。Agent 与商家可以根据自身需求，灵活选择和组合协议中的不同扩展模块。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中，MCP 更像是一个“工具与上下文协议”，用于让 Agent 安全、标准化地访问各类工具；A2A 是谷歌推出的多 Agent 通信协议，用来支持 Agent 之间的协作与任务分工； 而 AP2 是去年底发布的，聚焦在支付层，试图为 Agent 执行交易提供可验证、可授权的支付机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 UCP，看起来就是在这些协议之上的一次延伸，专门聚焦在零售这一层。可以说，谷歌这段时间在 Agent 协议这件事上确实是在“加班加点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/243fc7c4415ac7a479eb2f2b3b8b95ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，谷歌并不是第一个做这件事的。OpenAI 的 Agent Commerce Protocol&lt;/p&gt;&lt;p&gt;几个月前，OpenAI 其实也推出过一个 Agent 商业相关的协议，主打“即时结账”，帮助 Agent 发现商品并完成购买。而谷歌的一个巨大优势在于：绝大多数零售商本来就非常熟悉谷歌——比如 AdWords、广告投放，以及一整套谷歌企业服务。谷歌正在尽可能地利用这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;UCP 真正要解决的问题：可发现性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 的核心想法，是用一套协议建立“通用兼容性”。商家只需要一次性把“我卖什么、我怎么卖”按标准描述清楚，理论上就能在不同平台、不同 Agent 之间通用。而它真正想啃下的硬骨头，是 “可发现性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这对传统零售网站而言，意味着一次不小的变革：页面不再是交易的唯一入口，商品数据本身开始成为入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，谷歌也在补“数据底座”。在扩展产品数据源部分，谷歌还在其 Merchant Seller 工具中为用户提供新的“数据属性”，以便品牌可以优化其产品列表，提升 AI 搜索排名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要知道，在 AI / LLM 时代，我们过去 20 年一直在为“关键词 + 四五个要点”优化商品页，但这恰恰是 AI 最不需要的东西。这些系统需要的是：内容爆炸 + 上下文，缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子：一个自行车脚踏。几乎所有线上商品都可以有 50–100 个属性：螺纹结构、反光片数量、材质、重量、兼容标准……这叫“内容”。而“上下文”是：它更适合山地还是公路？兼容哪些车型？能不能和某些配件一起用？内容和上下文就像阴与阳，缺了任何一边，Agent 都很难可靠地做判断、更难可靠地下单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去那套 Google 商品数据规范，更像一条长满杂草的碎石路；而 Agentic Commerce 需要的，是一条 30 车道的信息高速公路——是光纤，不是拨号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果谷歌继续用旧的商品 Feed 规范来做 Agentic Commerce，在发现环节一定会失败。Gemini 拿不到足够的信息。这次他们终于开始补这一块：新增描述性文本属性、产品规格、Q&amp;amp;A、评论、特性列表、形态、口味、主题、兼容性信息、推荐配件、替代品等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方说法是“新增数十个字段”。在 Scott Wingo 看来，这个数量大概会在 24–60 个之间；即便今天只先放出 20 个，也一定会很快扩展到 30、40 个——因为所有人都会意识到：这才是决定可发现性的关键。这些数据仍然通过 Merchant Center 上传，本质上可以理解为 Google Shopping Feed 2.0。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他对所有品牌和零售商的建议只有一句：尽可能“疯狂”地扩展你的商品级内容与上下文。这将直接决定你在 AI 时代能不能被 Agent 选中、能不能“占领 Buy Box”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谁站队了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 在发布之初，就集结了科技与金融领域的一批重量级玩家，包括 Shopify、Walmart、Target、Etsy、Wayfair、Visa、Stripe、Adyen 等。首日即吸引了 20 多家合作伙伴加入，这正是标准胜出的典型路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/134c45edcab47ba358657acb357548a6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从已公开的信息来看，这些合作方大致可以分为两类：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一类是零售商与电商平台，包括 Etsy、Wayfair、Target、Best Buy、Macy’s、Kroger、Home Depot、Gap Inc.、Sephora、Ulta、Zalando、Chewy、Carrefour、Flipkart、Shopee 等；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类则是支付与清算体系，如 PayPal、Stripe、Adyen、Visa、Mastercard、American Express、Worldpay。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，有网友注意到，蚂蚁金服（ANT Financial） 也已经出现在 UCP 的合作名单中。有人评论称：“蚂蚁已经接入 UCP，但阿里巴巴推出自己的 Agentic Commerce 平台和 AI 协议，恐怕只是时间问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而从阿里最近的动作来看，这个判断并不突兀。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 15 日（今天），阿里千问 App 上线全新 AI Agent 能力“任务助理”，并打通淘宝、闪购、飞猪、高德与支付宝等应用：用户只需一句“我要两杯奶茶”，Agent 就能自动完成选店、选地址、选商品并生成订单，最后一步再由用户确认支付。延伸阅读：《&lt;a href=&quot;https://mp.weixin.qq.com/s/WXM2h4Z9DXrhVaoCjnt-ew&quot;&gt;刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖了&lt;/a&gt;&quot;》&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体看下来，一个趋势已经很难忽视：走到 2026 年，Agent 不再是大厂用来展示技术实力的“玩具”，而是开始被当成真正的赚钱工具。Agent 正在明显加速进入真实的应用场景，尤其是交易和服务这些最硬的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说得更激进一点：AI 很可能会把“社交 + 电商 + 服务”这套组合重新洗牌一遍。虽然“重做一遍”这个说法已经被用烂了，但眼下发生的变化，确实不像是在原有体系上打补丁，而更像是在重写入口、链路和分发规则——估计淘宝、京东这种级别的平台，迟早都得跟着重构一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且，这种变化最近已经变得非常明显了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&quot;&gt;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OXUn970YHVo&quot;&gt;https://www.youtube.com/watch?v=OXUn970YHVo&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&quot;&gt;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</link><guid isPermaLink="false">https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</guid><pubDate>Thu, 15 Jan 2026 11:09:21 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>谷歌推出Conductor：一款面向Gemini CLI的上下文驱动开发扩展</title><description>&lt;p&gt;谷歌发布了新的Gemini CLI预览扩展&lt;a href=&quot;https://developers.googleblog.com/conductor-introducing-context-driven-development-for-gemini-cli/&quot;&gt;Conductor&lt;/a&gt;&quot;，为AI辅助软件开发引入了结构化、上下文驱动的方法。该扩展旨在解决基于聊天的编码工具的一个常见限制：跨会话丢失项目上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor将开发上下文从瞬态会话中转移到直接存储在存储库中的持久Markdown文件中。这些文件定义了产品目标、架构约束、技术选择和工作流偏好，并作为开发人员和AI智能体的共享真相来源。其目的是使AI辅助开发随着时间的推移更加可预测、可审查和可重复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor鼓励的不是直接从提示到代码的转换，而是规划优先的工作流。开发人员在调用代码生成之前定义规范和实现计划，并且这些构件在特性的整个生命周期中仍然是代码库的一部分。这种方法旨在支持更大的任务，如特性开发、重构和在已建立的项目上工作，在这些任务中，理解现有的结构和约束是至关重要的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor的一个核心概念是轨迹，它代表了一个离散的工作单元。每个轨迹包括一个书面规范和一个面向任务的计划，该计划被分解为阶段和子任务。只有在计划被评审之后，实施才能继续进行，并在计划文件中直接跟踪进度。由于状态存储在存储库中，因此可以暂停、恢复或修改工作，而不会丢失上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期用户强调了基于轨迹的工作流，认为这是对临时提示的实际改进。Forrester的工程和产品负责人Devin Dickerson&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407465019967238146?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407465019967238146%2C7408224250060378112%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287408224250060378112%2Curn%3Ali%3Aactivity%3A7407465019967238146%29&quot;&gt;说&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于这个扩展我最喜欢的特性是轨迹的概念。在这次发布之前，我一直在使用自己构建的Conductor开源版本，我最终构建了自己的特性切片。现在轨迹已经内置了，我可以扔掉那个了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor还支持团队范围的配置。项目可以一次性定义共享标准配置，如测试策略、编码约定和工作流程偏好，并将它们一致地应用于所有AI辅助的贡献。这使得扩展不仅适用于个人开发人员，也适用于寻求跨贡献者和机器一致性的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;试用预览版的开发人员指出，它强调了明确的规划和测试驱动的工作流。Navid Farazmand&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7414757320267575297?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7414757320267575297%2C7415108568544247808%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287415108568544247808%2Curn%3Ali%3Aactivity%3A7414757320267575297%29&quot;&gt;描述道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当Gemini CLI发布时，我立即尝试用.md文件创建类似的东西。Conductor要好得多——特别是它采用的测试驱动开发方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor是Gemini CLI的预览扩展，可以从其公共&lt;a href=&quot;https://github.com/gemini-cli-extensions/conductor&quot;&gt;GitHub&lt;/a&gt;&quot;仓库安装。谷歌将这次发布定位为初始步骤，随着开发人员和团队的反馈指导未来的迭代，计划进行进一步的改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-conductor/&quot;&gt;https://www.infoq.com/news/2026/01/google-conductor/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</link><guid isPermaLink="false">https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</guid><pubDate>Thu, 15 Jan 2026 07:28:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>AI&amp;大模型</category><category>性能优化</category></item><item><title>手握30亿、被蚂蚁狂挖人，转型被骂惨的王小川，真的翻身了？</title><description>&lt;p&gt;在“大模型六小虎”成为历史后，王小川终于等来了自己的风口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，国内外大厂在医疗领域动作频繁。1月8日，OpenAI高调入局，除了推出ChatGPT Health，还收购了医疗保健初创公司Torch。几乎同期，Anthropic、英伟达、苹果等都有产品和合作发布。国内，蚂蚁阿福自发布后短期内月活用户突破3000万，单日提问量超千万。资本市场上，AI 医疗板块逆势走强，成为最近市场热点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此前大模型竞争激烈的当口，AI 医疗并不是一个很性感的话题。那种不信任来自百川内外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年成立的百川在一年后战略收缩，决定聚焦医疗，成为国内较早专注到医疗的大模型创企。但内部“没有足够传达在医疗上的决心和路径要求，没有让每个团队在医疗价值创造中深度思考why和how，进而导致部分团队工作目标出现了摇摆和偏差。”“去年中途转过来时被骂惨了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不只内部，业界对AI医疗也存有疑虑，连带着对百川的路线选择也有质疑。“2024年跟医生谈AI，大家都不信。”王小川直言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到2025年，大家看到DeepSeek真的比百度靠谱很多；年末阿福发布，投了10亿来砸广告，看到了技术和应用进展；今年1月8日，OpenAI Health 正式上线，Anthropic 也发布了自己的两个技术能力：医疗计算和Agent，两个巨头都开始进入医疗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“所以，从市场判断来看，医疗作为AI‘皇冠上的明珠’这样的高级阶段，已经开始进入应用范畴。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6cdda9230d6c5e944f95f7509736824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从发布反思信至今9个月过去，王小川向 InfoQ 表示，百川如今的护城河主要有三个：一是模型结构的优先级，“医疗安全性”和“诊断准确性”始终是首位；二是切入点选择，百川聚焦严肃、高价的医疗场景，区别于其他企业的健康类打法，这类场景的壁垒更高，且有明确的付费意愿；三是产品形态的差异化，百川身份差异化服务和决策辅助能力，是现有产品不具备的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川尤其提到，大厂和创业公司不一样，他们有职业团队，需要的是更安稳的方案。“大创新靠小厂，小创新靠大厂，必须切入我们认为有高价值的事情，共识不是我们优先的突破点，而大厂更多的是注重共识，路线图和产品形态是不一样的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模型要低幻觉、能问诊，多模态非主战场&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“去年8月发布的M2作为百川重新聚焦医疗之后的主力模型，在行业得到很多好评。典型现象就是蚂蚁开始疯狂挖人，从技术人员到财务人员，所以属于小圈子认可技术路线图。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨天，百川正式开源了新一代医疗大模型 Baichuan-M3。据百川智能模型技术负责人鞠强介绍，Baichuan 系列采用SCAN框架，实现临床医生层级的推理与问诊。其核心在于不仅询问疾病类型，更通过定量问题将模糊主诉转化为可定位、可量化的临床证据；并且突破单一症状的局限，进行跨系统关联推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，团队高度重视并主动防控大模型在医疗中的“幻觉”，坚持正确知识并进行原子级事实检验：在模型推理过程中进行逐层事实核查，确保结论基于真实输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;鞠强介绍，在模型训练中，抑制“幻觉”与提升推理能力之间存在明显的“跷跷板效应”，容易陷入两种极端：若过度追求推理表现，其生成内容会更丰富、答对率上升，但幻觉也难以控制；若强力抑制幻觉，模型则会趋向过度保守，回答变得拘谨甚至回避问题，导致实用性下降。这也是团队在Baichuan-M3训练中重点攻克的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为破解这一矛盾，研发团队引入了 Fact-aware 强化学习技术。该技术核心在于，在强化训练过程中，既对幻觉进行充分压制，又确保推理能力不受损，反而同步提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果显示，相比前代模型M2，百川正式开源新一代医疗大模型 Baichuan-M3 的幻觉率大幅下降，同时在医疗专业评测HealthBench上的推理能力得分从34分显著提升至44分，位列榜首。在不依赖工具或检索增强的纯模型设置下，医疗幻觉率3.5，超越GPT-5.2。“这验证了我们通过强化学习方法，在抑制幻觉与增强推理之间取得了有效平衡。”鞠强表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e6/e66ada51aa81717429a27588ca063a4c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Hugging Face 地址：&lt;a href=&quot;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://github.com/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，模型深度集成的问诊能力，从日常症状中识别风险。团队设计了防御性思维追问，以甄别背后潜在的系统性疾病，还会进行组合症状敏锐识别，比如用户描述“情绪激动时左牙疼”时，模型能会关联“牙痛+情绪症状”，优先建议排查心脏系统问题，从而排除重大隐患，而非直接推荐牙医或止痛药。该能力已集成至产品，服务于医生与普通用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在AI医疗中，除了文字，还有影像等信息。不过，王小川认为，多模态并非当前AI主战场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他解释道，ChatGPT之所以令人震撼，正是因为它展现出一种“智力”，而智力的本质，是将具体事物进行抽象的能力，其核心在于符号系统。在这一逻辑下，智能主要依托于三种形式语言：自然语言、数学语言与代码语言。至今，评估一个模型能力的强弱，本质上仍是检验其符号处理与逻辑推理的水平，功能可用并不等同于智力高超。在医疗领域，这一观点尤为关键。医疗的核心是决策，而不仅仅是感知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，未来医学影像的初步解读可由专用小模型完成，许多厂商也已具备相应的图像引擎。但真正的价值在于：将影像符号化之后，如何用语言模型进行综合推理与判断。因此，感知模型与认知模型必须结合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，当前的一些工作，比如将CT影像转化为报告，或是专注于胰腺癌筛查的视觉模型，固然有其价值，但它们更像是“挂在智力之树上的叶子”，是整体流程中的一环，而非驱动智能演进的主战场。真正的突破，仍在于如何通过符号与语言，构建能够进行复杂医疗决策的认知核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“在中国To C比To B更好”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来巨大的增量是在院外，不在院内。”王小川说道。其核心是直接服务患者，而不是通过服务医生间接服务患者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反观OpenAI的入局是靠打造“个人超级助手”，Anthropic则从合规性与临床效率上做B端突围。对此，王小川的评价是：“美国是To C和To B都可以干，但在中国To C比To B更好。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川认为，国内的医疗现状是医生供给不足，互联网虽能连接信息却无法创造供给；医患权力不均，双方容易沟通不畅、患者无助；患者更倾向三甲医院，致使基层医疗薄弱；医疗知识分散于各科室，复杂病症往往缺乏整体视角。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于此，他的设想是AI 可以“造出高质量医生”，但不是要AI取代医生。“在某些维度上，AI超过医生是必然的，比如信息收集的完整性、医学知识的储备量、循证的精准度等。但AI不会取代医生的核心执行能力，比如手术、查体等。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在不取代医生的情况下，AI 可以推动“权力让渡”，即帮助患者理解病情与方案，获得更多参与权和知情权。另外，居家通过AI进行初步咨询，让“居家首诊”可能，减轻医疗系统负担。此外，复杂问题需要跨科室会诊，以前就是入院即入组，即进入某个科研队列，有了AI后能够做到“看病即入组”，更有机会做好生命模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在实现的产品形态上，百川目前主打还是百小应App，不过用户进入后可以选择医生和患者两种身份，给出的结果是不一样的：医生版更像OpenEvidence，答案更加专业、更加强调循证，引用的文章在系统中100%存在，让其能够做决策、信息够充分；患者版本则强调补充信息，进入启发式端到端的问诊，也给到患者决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们与OpenEvidence的区别在于，OpenEvidence只是服务于医生，百川是可复数、可懂、可决策、可行动、能够服务到患者的，这样的产品定位在全球是独一无二的。”王小川补充道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在其看来，做To C产品，重点是让产品价值触达真正的目标人群，即有严肃医疗需求、愿意为决策辅助付费的患者。他举例称，达摩院做的胰腺癌平扫CT模型，虽然技术门槛高，但解决了核心临床痛点，就有明确的付费方；而泛健康类服务看似覆盖广，但价值不突出，反而难以找到稳定的付费用户。百川目前的做法就是基本全覆盖，重点放在儿科、慢病和肿瘤，优先突破有明确痛点的领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;收费模式上，王小川认为，不是只赚医院或医生的钱，还可以向患者收费，也可以形成服务包，后面的医疗资源和药械以服务包形式收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我倒不担心商业模式本身，确实要过了这个门槛、为用户创造价值，之后不管直接收费还是生态收费都是很容易的事情。”王小川说道。目前，百川账上还有 30 亿人民币，这也留给了王小川证明的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据王小川透露，今年上半年，百川会完成两款产品的发布和推广，核心是回归决策层面，帮助用户（包括患者和医生）做出更好的医疗决策，最终实现“医生时刻陪伴式”的健康管理。“我们第二个产品已经可以当成院外医生来看了。”此外，百川也有计划硬件产品发布和出海计划，具体日程未定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了培养用户心智，百川未来也会增加一定的广告宣传投入，另外会重视医生对产品的认可度。“阿福跟我们的路线不一样，老医生都是无感的。我们希望医生和患者一体两面，共享一款产品，要让专家点头，而不只是患者鼓掌。产品做好以后确实能够取得一定的口碑效应。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“今年上市的两家主要还是踩在通用模型技术红利和政策支持的基础上，但目前他们的市值和商业化能力并不匹配，但AI医疗今天也是大模型竞争中的一个范式，虽然它的成熟会晚一点，在后面我们肯定也是奔着上市去的。”王小川给了自己两年的时间再看看。&lt;/p&gt;</description><link>https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</link><guid isPermaLink="false">https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</guid><pubDate>Thu, 15 Jan 2026 06:51:01 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>LangGrant推出LEDGE MCP服务器，赋能企业数据库启用代理式AI</title><description>&lt;p&gt;&lt;a href=&quot;https://www.langgrant.ai/&quot;&gt;LangGrant&lt;/a&gt;&quot;推出了LEDGE MCP服务器，这是一个新的企业平台，旨在让大语言模型在复杂的数据库环境中进行推理，而无需直接访问或暴露底层数据。该版本旨在消除组织在将代理式AI应用于受受控生产数据时面临的一些最大障碍，即安全限制、失控的token成本和不可靠的分析结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司表示，LEDGE MCP服务器允许LLM跨&lt;a href=&quot;https://www.oracle.com/&quot;&gt;Oracle&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;SQL Server&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;Postgres&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.snowflake.com/en/&quot;&gt;Snowflake&lt;/a&gt;&quot;,等数据库生成准确、可执行的多步骤分析计划，同时将数据完全保留在企业边界内。通过依赖模式、元数据和关系而不是原始记录，该平台消除了将大型数据集推送到LLM的需要，从而大大减少了token的使用并防止敏感数据泄漏。根据LangGrant的说法，通常需要数周手工编写查询和验证的任务现在可以在几分钟内完成，并具有完全的人工审查和可审计性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant首席执行官、首席技术官兼联合创始人&lt;a href=&quot;https://www.linkedin.com/in/rameshpar/&quot;&gt;Ramesh Parameswaran&lt;/a&gt;&quot;表示：“LEDGE MCP服务器消除了LLM和企业数据之间的摩擦。”他指出，企业现在可以安全、经济地将代理式AI直接应用于现有的数据库生态系统，而不会损害治理或监督。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在许多组织中，上下文工程和代理式AI正从实验阶段进入生产环境。许多企业已经接受了AI助手，但在操作数据库方面却停滞不前。安全策略通常禁止直接访问LLM，在分析原始数据时token和计算成本会激增，开发人员和业务用户都在努力应对企业模式的规模和复杂性。即使使用AI辅助编码工具，工程师也经常花费数周时间手动将部分上下文输入模型，以生成可用的查询和管道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant将LEDGE定位为一个全面解决这些问题的编排和治理层。MCP服务器管理LLM如何与企业数据交互，确保符合访问控制和策略。分析和推理使用数据库上下文而不是数据有效负载来执行，以降低成本并减少幻觉风险。该平台还可以自动创建可由人工团队检查、批准和执行的多阶段分析计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，LEDGE支持按需克隆和容器化类似生产的数据库，为智能体开发人员提供安全、隔离的环境来构建和测试AI工作流。通过跨异构系统自动映射模式和关系，该平台使LLM能够跨多个数据库进行推理，而无需读取底层数据本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了LEDGE MCP服务器，LangGrant认为企业对AI的采用将更少地依赖于更大的模型，而更多地依赖于安全的编排、治理和成本控制。该公司认为，通过保持数据原位，同时为LLM提供全面的上下文理解，企业最终可以准确、安全、大规模地将AI应用于其最有价值的数据资产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多公司正在采用MCP风格的服务器，在不暴露原始数据的情况下为AI智能体提供安全、结构化的环境，但它们的重点领域有所不同。 &lt;a href=&quot;https://www.infoq.com/news/2025/04/github-mcp-server-public-preview/&quot;&gt;GitHub&lt;/a&gt;&quot;的MCP服务器以开发人员的工作流程为中心，允许LLM在执行访问控制的同时对存储库、问题、拉取请求和CI元数据进行推理。同样，微软的&lt;a href=&quot;https://www.infoq.com/news/2025/07/azure-devops-mcp-server/&quot;&gt;Azure DevOps MCP&lt;/a&gt;&quot;向AI智能体公开结构化项目和管道上下文，以支持规划、故障排除和交付自动化，而不是深度分析数据处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了开发者平台，MCP概念也出现在基础设施和运营中。&lt;a href=&quot;https://www.infoq.com/news/2025/11/buoyant-linkerd-mcp-support/&quot;&gt;Linkerd&lt;/a&gt;&quot;等服务网格项目正在探索MCP集成，为AI智能体提供对服务流量、遥测和策略执行的安全可见性。云提供商还通过他们的AI服务（如&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt;&quot;和&lt;a href=&quot;https://cloud.google.com/?hl=en&quot;&gt;谷歌云&lt;/a&gt;&quot;）提供类似MCP的上下文层，这些服务允许智能体查询基础设施元数据和操作信号，而无需将敏感数据直接传递给模型。这些方法侧重于操作意识，而不是数据分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与这些产品相比，LangGrant的LEDGE MCP服务器以专注于企业数据库和分析而脱颖而出。总之，这些平台显示了MCP如何成为一种基础模式，每个实现都针对企业堆栈的特定层进行了定制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&quot;&gt;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</link><guid isPermaLink="false">https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</guid><pubDate>Thu, 15 Jan 2026 06:42:00 GMT</pubDate><author>作者：Craig Risi</author><category>AI&amp;大模型</category><category>数据库</category></item><item><title>QCon 北京 2026 启动｜Agentic AI 时代的软件工程重塑</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年，我们分别在&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing&quot;&gt;北京&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/shanghai/&quot;&gt;上海&lt;/a&gt;&quot;举办了两场 QCon 全球软件开发大会。过去一年里，我们和大量一线技术团队、工程负责人、开发者持续交流，感受到一个很明显的变化：大家讨论的重点，正在从“AI 能做什么”，转向“AI 怎么在生产系统里稳定运行、可控交付、持续产生价值”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不是热度退去，而是行业进入了更难、也更关键的阶段——从演示走向长期运行，从能力展示走向工程兑现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，越来越多团队开始回到同一个问题：当 AI 真正进入业务流程后，系统能不能“长期跑得住”？成本能不能算得清？质量、风险、合规能不能兜得住？组织的协作方式要不要跟着变？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，智能体（Agentic AI）&amp;nbsp;成为不少团队正在尝试的新方向：它不只是一次回答或一次推理，而是把感知、工具调用、任务执行、反馈迭代串成一个可运营的流程，逐步嵌入研发、交付与业务链路。可以预期，进入 2026 年，这类探索会从局部试点走向更体系化的工程建设：不仅是“加一个 AI 功能”，而是软件系统、研发流程乃至组织协作方式都要随之调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;软件工程正在发生的变化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们看到的变化不只是工具升级，更像是一套工程范式在被重写：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统架构开始围绕「智能体协作」重新设计工程方法论从「确定性流程」迈向「人机协同闭环」研发组织面临角色重塑与能力重构产品与交互从“界面驱动”走向“意图与行动驱动”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从基础设施、推理与知识体系，到研发与交付流程，再到前端、客户端与应用体验——AI 正在以更工程化的方式进入软件生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;QCon 北京 2026 的核心主线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot;&amp;nbsp;将以&amp;nbsp;「Agentic AI 时代的软件工程重塑」&amp;nbsp;作为大会核心主线，把讨论从&amp;nbsp;「AI For What」，走向真正可持续的&amp;nbsp;「Value From AI」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一主线，我们将从六个关键维度系统性展开探索：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;前沿技术雷达（Future Tech）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来 1–2 年最值得提前布局的方向：Agentic AI 的新形态、下一代模型、交互范式与系统架构演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;架构设计与数据底座（系统可演进）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论如何构建可扩展、可演进、可复用的 AI 系统：Agent 架构、数据治理、知识体系与工程实践，回答“能不能长期跑”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;效能与成本（拒绝盲目烧钱）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注让 AI “跑得起、跑得快、跑得稳”的工程方法：在算力、推理、工程效率与 ROI 之间，寻找真正可持续的平衡点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;产品与交互（体验提升）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚焦前端、客户端与产品层的 AI 原生改造：人机协作、意图驱动交互、任务闭环体验，以及 Agent 参与下的产品新范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;可信落地（守住底线）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论 AI 带来的新风险。从 Demo 到 &amp;nbsp;Production 的“最后一公里”信任危机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;研发组织进化（长期主义）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来团队如何生存与进化：重塑研发角色分工、协作模式与工程文化，构建面向 AI 时代的组织能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;我们希望在 QCon 北京 2026 呈现的&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026 想呈现的，不只是“又一场关于 AI 的大会”，而是这轮变化真正落到工程与组织之后的全景：哪些方向已经走通，哪些正在付出真实成本，哪些系统必须被重构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，北京站部分专题已上线，我们期望持续挖掘来自一线生产环境的长期实践，呈现 Agentic AI 融入软件工程后的真实样貌——成功经验、工程妥协与关键取舍并存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9d/9def3983a8e8e8f60f08f3560ccf39b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多嘉宾邀请进行中&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也欢迎你带着真实问题与实践加入其中，与更多同行一起，把这场正在发生的软件工程重塑讲清楚、做扎实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026，期待与你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📍&amp;nbsp;会议官网：&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;https://qcon.infoq.cn/2026/beijing/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📩&amp;nbsp;演讲申请：&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;https://jinshuju.com/f/Cu32l5&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲评审标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;观点：是否清晰、有判断力，能否帮助听众形成有效认知实践：内容须来源于真实工程或业务实践深度：是否具备可复用的方法论或经验价值专业声誉：演讲者在相关领域的实践背景与影响力不做广告：QCon 不是厂商宣传舞台听众所得：听众能带走什么，是我们最关注的标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲嘉宾福利&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🎟&amp;nbsp;免费参会：自由参加大会全部课程💸&amp;nbsp;专属折扣：提供特别优惠码，方便同事与朋友购票📰&amp;nbsp;独家报道：有机会接受 InfoQ / 极客时间的深度采访🏨&amp;nbsp;免费住宿：为外地嘉宾提供酒店入住✈️&amp;nbsp;无忧差旅：承担嘉宾往返会场的交通费用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/a4/a4b836a91798f84c96c46e2f4c7ec73a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</link><guid isPermaLink="false">https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</guid><pubDate>Thu, 15 Jan 2026 03:17:38 GMT</pubDate><author>Kitty</author><category>AI&amp;大模型</category><category>软件工程</category></item><item><title>中了！极客时间入围中国移动培训服务一采供应商</title><description>&lt;p&gt;极客时间企业版（极客邦控股（北京）有限公司）成功入围中国移动 2026–2028 年培训服务集采项目，正式成为其一级供应商。在技术、市场及政企、培训资源开发三大标段中均取得优异成绩，彰显了公司在 IT 与数智化培训领域的深厚实力和生态优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;中标概览：三大赛道，全面突破&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在本次集采中的表现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版在“标包 7（培训资源开发）”中勇夺魁首，依托成熟的课程研发体系与知识产品化能力，彰显了其在高质量、体系化数智课程开发方面的硬核实力；在“标包 2（技术）”中位列三甲，体现了在 AI、云计算、大数据等前沿技术培训领域的扎实积淀；同时强势入围“标包 3（市场及政企）”，进一步验证了其助力企业业务增长与数智化转型的全面解决方案能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7f41da54f421e75a37be20573c31cdde.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2ecdf1eff746c6b8b31dcf1de2c9e170.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;能力解读：“媒体+产品+生态”的复合优势&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版之所以能快速响应不同标包的需求，根源在于公司长期以来打造的“内容+产品+生态”模式：&lt;/p&gt;&lt;p&gt;极客时间企业版则凭借培训平台与课程产品，将培训需求转化为可落地、可衡量的学习成果。InfoQ 极客传媒提供前瞻行业洞察，精准把握人才培养方向。TGO 鲲鹏会链接高端产业资源与实战智慧，构建协同发展的高管智库。&lt;/p&gt;&lt;p&gt;依托公司各业务板块的协同效应，极客时间企业版将持续为包括中国移动在内的广大合作伙伴提供“严选内容、高效转化”的培训服务，践行“助力客户成功”的价值承诺，提升企业人才发展的综合回报。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52a9b1971f02f614f0ec5511ccc54aff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0898ab691c26366e1602f2a158deff38.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;时代召唤：AI 浪潮下的企业人才变革&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当 AI 重构千行百业，企业对“懂技术、会落地、能创新”的数智人才需求，已从“可选”变为“刚需”。极客时间企业版始终致力于将前沿数智技术与实战知识体系深度融合，此次入围正是对公司在应对时代命题、推动产业人才升级方面能力的高度认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深化创新，践行使命&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;立足新起点，极客时间企业版将以此次合作为引擎：对内，持续深化课程内容与服务创新；对外，将集采所带来的资源与平台优势，探索数智人才培养的新模式、新场景。我们坚信，专业的培训服务不仅是知识的传递，更是产业的赋能。未来，极客邦科技将继续秉持“推动数智人才全面发展，助力数智中国早日实现”的使命，与中国移动及所有伙伴一道，用人才之力点亮数智未来！&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a405ee19565ac3e822dd33d1fee8d4a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图为：极客时间企业版产品服务概览）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;合作咨询&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欢迎联系极客时间企业版，我们将按照您的企业场景、业务目标和人才发展要求，提供专属人才培养解决方案，助力您的企业致胜 AI 时代。敬请点击“阅读原文”访问官网，或扫描下图二维码&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d51c0dec780f590df20bbbd672190d33.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</link><guid isPermaLink="false">https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</guid><pubDate>Thu, 15 Jan 2026 02:59:38 GMT</pubDate><author>极客时间企业版</author><category>数字人才培养</category></item><item><title>实测谷歌Veo 3.1：新增原生竖屏模式和4K画质，换个语言翻车到离谱？</title><description>&lt;p&gt;刚刚，谷歌更新了其 Veo AI 视频生成器，新增原生竖屏视频生成与 4K 分辨率支持功能。此次对 “文生视频” 功能的调整，旨在提升画面清晰度的同时，确保不同场景中的主体元素保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bd/bd8233a10f0980d44fe46156c0a3fe68.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Veo 3.1 的更新，解决了生成式视频领域一项长期存在的挑战：保持镜头间的视觉一致性。谷歌表示，新款模型在场景切换时能更好地保留人物特征与背景纹理，从而更容易重复使用特定的视觉元素，或在多场景叙事中贯穿同一主题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fee0a6257754c1b1378f71f8e313eac7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最显著的改进是对“素材到视频”工具的重大优化。用户只需添加三张参考图片：一张用于主体，一张用于背景，一张用于展现所需的视觉效果或风格。然后，只需添加一些文字即可开始制作。即使提示信息较短，Veo 3.1 也能在提供参考图像后生成角色表情和动作更生动的视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;移动创作者是本次更新的核心受众。升级后的 Veo 可直接生成 9:16 比例的原生竖屏视频，创作者无需对横屏素材进行裁剪，也不必牺牲画质，就能制作出适配 YouTube Shorts 等平台的全屏内容。针对更专业的创作流程，谷歌还新增了 1080P 至 4K 的画质提升选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，这些新功能已率先在 Gemini 应用、YouTube Shorts 及 YouTube Create 工具中上线，并将逐步覆盖谷歌旗下更多创作者工具与企业级服务。为区分生成内容与真实拍摄素材，谷歌会在视频文件中嵌入肉眼不可见的 SynthID 数字水印。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有体验用户反馈，Veo 3.1似乎存在不同语言版本表现差距太大的问题。“巴西葡萄牙语的人物音频存在音画不同步、台词错乱的问题，其他语言版本的表现则相对更佳。我曾指令其生成一段鹦鹉以沙哑嗓音鸣叫的音频，但该需求最终未能实现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/32/32c77f6b5acf1e71fc9768effdd53ba3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，此次更新距苹果与谷歌官宣合作、计划在下一代 Siri 中集成 Gemini 模型仅过去一天。与此同时， OpenAI 已达成合作，计划将迪士尼角色引入 Sora 平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</link><guid isPermaLink="false">https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</guid><pubDate>Thu, 15 Jan 2026 02:43:36 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>估值1亿的“死了么”APP有多好抄？5分钟AI就能复刻，去年有人一下午做出原型</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，上了热搜、又在苹果付费软件排行榜的榜首上挂了多日的&quot;死了么&quot;APP，突然宣布更名为Demumu。据其称，“经团队审慎决策，‘死了么’APP将于即将发布的新版本中，正式启用全球化品牌名Demumu。继昨日获得BBC报道后，我们的服务在海外实现了爆发式增长。未来，Demumu将继续秉持安全守护的初心，把源自中国的守护方案带向世界，服务全球更多独居群体。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自推出后，该APP的热度剧增，下载量一度暴涨100倍。虽开发成本仅1000多元，但获得不少头部投资机构的青睐，现在的估值已经飙到了1亿元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而最值得一提的是，这一APP的完整原型竟然在去年初就有了，可做出来的却不是同一批人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;痛失1亿的“原作者”，5分钟复刻出海外版&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“好消息：我去年做了一个‘死了么’APP；坏消息：我只做了产品设计和UI并发了文，但我当时觉得这只是个用来博眼球的噱头。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在死了么APP爆火后不久，一位ID为“饼干哥哥AGI”的数据分析师公开表示，去年3月，他从小红书上经常看到这个需求，随后花了不到一个下午的时间用Cursor和Claude 3.7生成了完整APP原型，并且将提示词模版和操作步骤都发布了出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;提示词by @花叔：&amp;nbsp;我想开发一个类似外卖APP「饿了么」，APP叫「死了么」，用于养老的，每天问一句，以防独自一个人死在家里没人发现。APP也有骑手，哪里有人死了就去接单收尸。 注意这是专门为独居90后的年轻人设计的。风格要求清新好看、APP内的文案多用搞怪的网络用语。&amp;nbsp;现在需要输出高保真的原型图，请通过以下方式帮我完成所有界面的原型设计，并确保这些原型界面可以直接用于开发：1、用户体验分析：先分析这个 APP 的主要功能和用户需求，确定核心交互逻辑。2、产品界面规划：作为产品经理，定义关键界面，确保信息架构合理。3、高保真 UI 设计：作为 UI 设计师，设计贴近真实 iOS/Android 设计规范的界面，使用现代化的 UI 元素，使其具有良好的视觉体验。4、HTML 原型实现：使用 HTML + Tailwind CSS（或 Bootstrap）生成所有原型界面，并使用 FontAwesome（或其他开源 UI 组件）让界面更加精美、接近真实的 APP 设计。拆分代码文件，保持结构清晰：5、每个界面应作为独立的 HTML 文件存放，例如 home.html、profile.html、settings.html 等。- index.html 作为主入口，不直接写入所有界面的 HTML 代码，而是使用 iframe 的方式嵌入这些 HTML 片段，并将所有页面直接平铺展示在 index 页面中，而不是跳转链接。- 真实感增强：&amp;nbsp; - 界面尺寸应模拟 iPhone 15 Pro，并让界面圆角化，使其更像真实的手机界面。&amp;nbsp; - 使用真实的 UI 图片，而非占位符图片（可从 Unsplash、Pexels、APPle 官方 UI 资源中选择）。&amp;nbsp; - 添加顶部状态栏（模拟 iOS 状态栏），并包含 APP 导航栏（类似 iOS 底部 Tab Bar）。请按照以上要求生成完整的 HTML 代码，并确保其可用于实际开发。&amp;nbsp;操作步骤：1.打开Cursor编辑器（确保版本足够新，支持Claude 3.7）2.选择编辑Agent模式3.选择Claude 3.7 Sonnet作为模型，最好是用thinking4.粘贴上述提示词，填入你需要的APP类型5.等待生成完成，可能需要3-5分钟&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术已经不值钱了。现在再重新做的话，可能一个下午的时候都够从0到设计到APP STORE上架了。”这位数据分析师感叹道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，他又花了5分钟就复刻出了&quot;死了么&quot;APP海外版。据称，这次他没敲任何代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;三名95后共同开发，APP收费已涨了8倍&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个引爆全网的APP，到底有什么魅力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，该应用是为独居人群打造的低成本安全工具，核心功能十分简单。用户无需注册登录，首次使用只需填写姓名和紧急联系人，每天打开应用完成签到即可；若连续2天未签到，系统将于次日自动发送邮件告知紧急联系人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最开始，“死了么”APP的收费只要1元，现在涨到了8元。团队表示，这是为了让项目能够健康、持续地发展，并覆盖日益增长的短信、服务器等成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该应用由三名95后共同创立并独立运营，在走红后，其背后的创始人们也陆续“现身”并对外披露了项目相关情况。“死了么”APP创始人之一郭先生介绍，团队内只有三名95后成员，且各自有自己的本职工作，通过远程方式进行协作。项目大约在2025年年中立项，开发时间不到一个月，初始投入成本仅1000多块钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位创始人吕先生表示，早在两三年前他们就在社交平台上留意到相关需求，“近几年大家都会讨论‘什么APP是每个人都需要的，并且一定会下载的’，就有网友提到‘死了么’APP，这个创意出来之后有很大的讨论度，我们看到了其中的需求，并且这件事本身也很有意义，于是我们就尝试去注册这个名字，发现可以注册，后续又用了一个月时间完成了开发。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据创始人郭先生透露，增长从2026年01月09日左右开始，短短两天内，下载量相比之前暴涨100倍以上，且仍在持续攀升。吕先生表示，现在APP的下载量不太方便透露，但“确实增长速度非常快”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据点点数据，除中国市场外，目前“死了么”APP在多国苹果应用商店霸榜第一：在新加坡付费榜位居总榜第一，在比利时、荷兰、瑞典等国付费工具榜排名第一，在英国、澳大利亚、美国等10多个国家付费工具榜排名第二。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公开信息显示，“死了么”APP由月境（郑州）技术服务有限公司开发，这家成立于2025年03月10日的公司，注册资本仅10万元，法定代表人为郭孟初，由其100%持股。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前有消息称，该公司已经接触到投资意向，正计划以100万元出让公司10%的股份。也就是说，这时其估值已达到1000万元。当前的最新消息是，如今该APP用户已增长800倍。并且，随着与六七十家投资方的深入接触和洽谈，短短两日间，“死了么”APP的估值飙升至近1亿元。但目前，其团队仍维持出让公司10%股份的计划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;遍地都是“活了么”，免费版卷疯了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“死了么”APP爆火后，陆续有网友提出优化建议，有人提出可以改成通过短信通知紧急联系人、优化签到形式等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，其团队已透露了后续的发展规划：接下来将把主要精力投入到产品打磨中，例如丰富短信提醒功能、考虑增加留言功能，并探索推出更适老化的新产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于为软件带来广泛共鸣与关注的名字，也开始迎来争议。不少网友认为，“死了么”不好听，建议改成“活着么”。苹果官方客服也于1月9日作出回应，称用户若对APP名称不满，可提供应用基础信息，客服将同步至相关业务部门，协调联系开发者沟通处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，“死了么”APP已改名为Demumu。有网友认为，“死了么APP之所以火，这名字最起码占一半功劳。改名，大概率算是把魂给丢了。只有当它从工具属性进化到社区属性，它才有可能活下来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而另一款名为“活了么”的APP已上架苹果应用商店，其功能与“死了么”相似，但目前是免费版的。此外，在苹果应用商店，有十几个类似名称与功能定位的APP也扎堆上线了，如“活着么”、“还活着么”“我还在”“我还活着呢”“我还好”等。其中，“活着么”目前就有9个，大部分都是免费版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da56da2e0e8757d5e369b1f2807e55d7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&quot;&gt;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</link><guid isPermaLink="false">https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</guid><pubDate>Thu, 15 Jan 2026 02:36:04 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>亚马逊云科技为S3 Tables添加智能分层存储和复制功能</title><description>&lt;p&gt;亚马逊云科技最近宣布为&lt;a href=&quot;https://aws.amazon.com/s3/features/tables/&quot;&gt;S3 Tables&lt;/a&gt;&quot;引入两项新功能，第一项功能是新的智能分层存储类，该存储类能够根据访问模式自动优化成本，第二项功能是支持跨AWS区域和账户自动维护一致的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html&quot;&gt;Apache Iceberg&lt;/a&gt;&quot;表副本的复制功能，该过程无需手动同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/tables-intelligent-tiering.html&quot;&gt;智能分层存储类&lt;/a&gt;&quot;会将数据自动分配到最具成本效益的三个低延迟层级之一，即Frequent Access、Infrequent Access或Archive Instant Access。据公司介绍，最后一种是最低成本的层级，比Infrequent Access层级便宜68%。亚马逊云科技的主任开发者倡导者Sebastian Stromacq这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在无访问达30天后，数据会被移动到Infrequent Access层级，在90天后，则会迁移到Archive Instant Access层级，这一过程不会对应用程序造成影响或性能降低。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;默认情况下，表使用标准存储类，但创建表时可以指定智能分层（Intelligent-Tiering）作为存储类，用户也可以在表存储桶级别配置默认存储类。用户可以将智能分层设置为表存储桶的默认存储类，如果在创建表时未指定存储类，那么表将自动存储在智能分层中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以利用&lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS命令行界面（AWS CLI）&lt;/a&gt;&quot;，通过put-table-bucket-storage-class和get-table-bucket-storage-class命令来更改或验证其S3表格存储桶的存储层级。相关命令如下所示：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;aws s3tables put-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \
   --storage-class-configuration storageClass=INTELLIGENT_TIERING


# Verify the storage class
aws s3tables get-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \


{ &quot;storageClassConfiguration&quot;:
   {
      &quot;storageClass&quot;: &quot;INTELLIGENT_TIERING&quot;
   }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;来自Imperious Enterprise的AWS架构师Adefemi Adeyemi在LinkedIn的&lt;a href=&quot;https://www.linkedin.com/posts/adefemi-adeyemi_if-you-are-working-with-apache-iceberg-on-activity-7402006733004406784-SMSW&quot;&gt;帖子&lt;/a&gt;&quot;中指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数分析数据集在一段时间内是“热”的，但随后会逐渐“冷却”。借助S3 Tables的智能分层功能，你无需不断调整Iceberg数据的生命周期策略。该服务会根据访问模式自动将对象移至更便宜的存储层级，这对长期存在的数据湖来说是一大优势。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，S3 Tables的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-replication-managing.html&quot;&gt;复制功能&lt;/a&gt;&quot;可以帮助用户跨AWS区域和账户维护表格的一致性只读副本。当声明目标表格的存储桶时，服务会创建只读的副本表格，并以时间顺序复制所有更新，同时保持父子快照关系。这些副本表格将在源表格更新后的几分钟内得到更新，并支持独立于源表格的加密和保留策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Stromacq说到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用户可以通过&lt;a href=&quot;https://aws.amazon.com/sagemaker/unified-studio/&quot;&gt;Amazon SageMaker Unified Studio&lt;/a&gt;&quot;或任何兼容Iceberg的引擎（包括&lt;a href=&quot;https://duckdb.org/&quot;&gt;DuckDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://py.iceberg.apache.org/&quot;&gt;PyIceberg&lt;/a&gt;&quot;、&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&quot;和&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&quot;）查询副本表格。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助AWS Management Console、API或&lt;a href=&quot;https://aws.amazon.com/tools/&quot;&gt;AWS SDK&lt;/a&gt;&quot;，用户可以创建和维护表格副本。此外，他们可以指定用于复制源表格的目标表格存储桶。当用户启用复制功能时，S3 Tables会在这些存储桶中创建只读副本，使用最新状态进行回填，并持续监控更新以保持同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在同一篇LinkedIn帖子中，Adeyemi指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对复制功能的原生支持让你能够快速创建只读副本，这些副本在几分钟内即可与源表保持同步，并且可作为Iceberg表进行查询。减少了自定义集成的工作量，让你有更多时间真正使用数据。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以通过&lt;a href=&quot;https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html&quot;&gt;AWS Cost and Usage Reports&lt;/a&gt;&quot;和&lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;Amazon CloudWatch&lt;/a&gt;&quot;指标跟踪各访问层的存储使用情况。配置智能分层无需额外费用，用户仅需支付各层的存储成本。至于S3 Table的复制，用户需支付目标表格的S3 Table的存储费用、复制PUT请求的费用、表格更新（提交）以及复制数据的对象的监控费用。更多详情可参见&lt;a href=&quot;https://aws.amazon.com/s3/pricing/&quot;&gt;定价页面&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/s3-tables-intelligent-tiering/&quot;&gt;&amp;nbsp;AWS Adds Intelligent-Tiering and Replication for S3 Tables&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</link><guid isPermaLink="false">https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</guid><pubDate>Thu, 15 Jan 2026 02:34:41 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>数据库</category></item><item><title>Data+AI 新年特辑：2025 的顿悟时刻与 2026 的关键十问 | Q推荐</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，Data + AI 的讨论正在悄然发生变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的关注点，逐渐从模型能力本身，转向企业是否真正具备承载 AI 的系统能力：数据是否准备充分，工程体系是否稳定，AI 是否真的进入业务流程并长期运行。这些问题开始频繁出现在一线实践中，也成为企业在推进 Data + AI 过程中无法回避的现实考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的变化并非源于某一次集中发布，而是在一次次真实落地、反复试错和持续修正中逐步显现。也正因为如此，2025 成为了一个值得回望的年份，许多重要判断，往往产生于具体实践中的“顿悟时刻”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，InfoQ 联合 Snowflake 发起了&amp;nbsp;「MAKE IT SNOW｜2025–2026 Data + AI 年度时刻」&amp;nbsp;直播活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一场围绕企业 Data + AI 战略展开的年度复盘与前瞻对话。活动邀请来自数据平台、开源社区，以及制造、医疗、汽车等行业的一线技术与业务负责人，围炉而坐，如老友般对谈 。我们将共同回到真实的问题本身，剖析企业在推进 Data + AI 规模化过程中遇到的关键抉择 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;那个拨云见日的「Aha Moment」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每位嘉宾将回顾自己在 2025 年经历的&amp;nbsp;3 个关键认知转折点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能是一段产品体验、一次落地尝试，或是某个业务场景中的重新理解。正是这些具体经历，推动了对 Data + AI 的判断不断修正，也构成了企业能力演进的真实轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用「年度十问」对齐关键判断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“十问 Data Strategy，AI Strategy&amp;nbsp;”环节，问题覆盖数据底座与 AI 融合架构、Agentic AI 与可信 AI、多云时代的数据治理、平台整合浪潮下的生态协同，以及工业、医疗、汽车等行业的落地实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题没有预设答案，却直指企业当下面临的核心挑战，更接近真实决策场景中的思考方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;留待未来打开的「时间胶囊」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场直播的尾声，每位嘉宾将基于当下的判断，留下&amp;nbsp;一个关于 2026 的预测或猜想。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可能并不成熟，也未必已经被验证，更像是一种站在当下时刻，对下一年走势的直觉判断。这些判断不会被立即评判对错，而是被完整地保存下来，等到 2027 年，我们会再度打开它们，回看哪些判断被现实印证，又有哪些想法在时间中发生了意料之外的转向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一刻行业领袖们的技术直觉，将成为未来回望时的重要坐标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正在思考企业 Data Strategy 与 AI Strategy 的下一步，这场对话，值得关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 19 日 17:30-19:30，我们不见不散！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66f092bc1cea165a029ae8e1592c162e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</link><guid isPermaLink="false">https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</guid><pubDate>Wed, 14 Jan 2026 10:49:20 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>辣评 AI编程工具： 是它们不行， 还是你不会玩？｜InfoQ 2025 年度盘点与趋势洞察</title><description>&lt;p&gt;年度盘点来啦！辣评AI编程工具。&lt;br&gt;
总出 bug？隐性成本拉满？是你没摸透AI编程工具内核，还是工具自己拉胯？三位资深AI Coding 专家详解如何把 AI 用成提效外挂！&lt;/p&gt;
&lt;p&gt;本期内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;说评国内外的Al coding 工具，哪个最受开发者青睐？&lt;/li&gt;
&lt;li&gt;Vibe Coding怎么交付成果？&lt;/li&gt;
&lt;li&gt;写代码时间正在被“和Al聊天”取代？&lt;/li&gt;
&lt;li&gt;计费模式变变变，开发者怎么省钱？&lt;/li&gt;
&lt;li&gt;架构师、前端、新人，如何借Al成长？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本期嘉宾：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;马工，Al Orchestrater @ Kreditz&lt;/li&gt;
&lt;li&gt;松子，资深AI产品专家&lt;/li&gt;
&lt;li&gt;张汉东，资深独立咨询师&lt;/li&gt;
&lt;/ul&gt;
</description><link>https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</link><guid isPermaLink="false">https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</guid><pubDate>Wed, 14 Jan 2026 09:16:38 GMT</pubDate><author>InfoQ 中文站</author><category>AI&amp;大模型</category></item><item><title>Zed 为什么不用自己造 Agent？OpenAI 架构师给出答案：Codex 重划 IDE × Coding Agent 的分工边界</title><description>&lt;p&gt;Coding agents（编码智能体） 已成为应用型 AI 中最活跃的领域之一，但许多团队在模型或服务商更迭时，仍不断重复构建脆弱的基础设施。那么，如何在生态不断变化的背景下保持快速迭代与高度韧性，并将更多精力投入到领域特定的工作流程和用户体验上？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为行业内的动向标杆，OpenAI的Codex提出了解决方法——“模型和Harness（工具集）的共同构建”。最近，OpenAI 的架构师 Bill Chen 和 Brian Fioca 在演讲里一起详细介绍了该构建过程中克服的挑战，以及这个Coding Agent本身一些新兴的使用模式。基于该演讲视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent的构成&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们来谈谈Coding Agent的构成。其实非常简单，一个Coding Agent由三部分组成：用户界面、模型和Harness。用户界面显而易见，可能是命令行工具，也可能是集成开发环境，或者是云端或后台Agent。模型也很直白，比如我们最近发布的GPT-5.1系列模型或其他一些供应商的模型。至于Harness，这是一个稍微复杂一点的部分，它直接与模型交互，最简化地说，可以将其看作是由一系列提示和工具组合而成的核心Agent循环，它为模型提供输入和输出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/2099b27dcfa604ecbaba2ea6937a64e4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coding 领域是应用人工智能最活跃的前沿之一，而随着新模型的不断发布，我们面临的挑战也在增加。更为复杂的是，大家不得不不断调整Agent以适应新发布的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们将聚焦于Harness的部分。Harness是模型的接口层，它是模型与用户、代码之间进行交互的媒介。它包括了模型需要的所有组件，以便在多轮对话中进行工作，调用工具，并最终为你编写代码，解读用户的需求。对一些产品来说，Harness可能是其中的关键部分。不过，构建一个高效的Harness并不是一件轻松的事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，构建Harness过程中遇到的挑战有哪些呢？首先是AV（音视频工具）问题。你可能会为Agent提供一个全新的、创新的工具，但它可能是模型之前从未见过的，它可能并不擅长使用这种工具。即使它曾经见过，你也需要花时间根据该模型的特点调整Prompt。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新模型不断发布，延迟问题也是一个挑战。模型在处理某些问题时需要时间，那么，我们应该如何设计提示，避免延迟过长？如何在用户体验上展示模型思考的过程？它在思考时是否与用户沟通，还是我们需要总结其输出结果？此外，管理上下文窗口和数据压缩也是一大难题。另外，API接口也在不断变化，现在我们有完成功能、响应功能，以及未来可能出现的其他功能，模型是否能熟练使用这些工具以便发挥最大的智能也是一个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将模型适配到Harness中需要大量的Prompt设计。实际上，模型的训练方式会带来一些副作用。我喜欢这样理解：（Steerability = Intelligence + Habit）智能加上习惯。一方面，智能是指：模型擅长什么？熟悉哪些编程语言？在某些框架中，模型能把代码写得多好？另一方面，它又养成了哪些习惯来解决问题？我们在训练模型时，培养了它在规划解决方案、查找背景信息、思考问题后再动手写代码，并在最后测试工作的习惯。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解这些习惯是成为一名优秀的Prompt工程师的关键。如果你没有按照模型熟悉的方式来指导它，可能会遇到问题。当我们发布GPT-5时，许多不习惯使用我们模型的人，尝试将其他模型的Prompt直接套用到我们的Harness中，结果发现我们的模型做的事情比其他模型要更为细致，导致了响应速度慢，效果不如预期。我们最终发现，如果让模型按照它习惯的方式进行工作，而不是过度引导，它的表现会更好。通过与模型的对话，我问它：“我喜欢这个解决方案，但它花了太长时间。下次你能做得更快吗？”模型回答说：“你让我去看所有的内容，其实我并不需要这样做，正是因为这个原因，才耗费了这么长时间。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Codex作为Harness/Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex被设计成一个适用于各种编程环境的Agent，它可以作为VS Code插件、CLI工具使用，甚至可以通过VS Code插件或手机上的ChatGPT在云端调用。它的功能非常基础：你可以通过提示将想法转化为可运行的代码，具备规划能力。它能在代码仓库中导航并编辑文件，执行命令和任务，你也可以从Slack或GitHub上调用它来审查PR。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着Codex的Harness需要能够完成许多复杂的任务：需要处理并行工具调用、线程合并等问题，还要考虑安全性，例如沙箱管理、提示语转发、权限设置、端口管理等。数据压缩和上下文优化的管理也非常复杂。何时触发压缩，何时重新注入数据，如何优化缓存，所有这些都是必须要解决的挑战。如果你要从零开始构建这些功能并保持其更新，工作量巨大。幸好，我们已经将这些功能集成到一个Agent系统中，它能安全地编写自己的工具来解决遇到的新问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这听起来比普通的Coding Agent强大多了，不是吗？但想想看，其实在浏览器和图形用户界面出现之前，我们操作计算机的方式不就是通过命令行界面写代码并将其串联起来吗？这意味着，如果你能将任务以命令行方式以及文件任务的形式表达出来，Codex就能知道该如何执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子，我喜欢使用Codex将我的桌面上的照片整理到一个文件夹里，这是一个非常简单的应用场景。但它还能做的不仅如此，它能够分析文件夹中大量的CSV文件，进行数据分析，这并不一定是Coding 任务，只要能够通过命令行工具来完成，Codex就能帮你做。现在我们可以看到，Codex是如此强大和有趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Codex构建自己的Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你希望将Codex集成到自己的Agent中，该如何操作呢？如果你打算创建下一个Coding 初创公司，一个关键的模式是：Harness成为新的抽象层。这个模式的好处非常明显，你不再需要在每次模型升级时都优先优化提示语和工具。但这是不是意味着你仅仅是在构建一个包装器呢？不是。正如我所说，单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看看一些我们与客户合作时所遇到的模式，这些模式实际上帮助他们成功构建了产品。Codex是一个SDK，你可以通过TypeScript库来调用它，也可以通过Python执行它。它还提供了一个GitHub动作，能够自动合并PR中的冲突，解决大家讨厌的合并问题。此外，你还可以将它添加到AgentSDK中，并为你的产品提供MCP连接器。这样，你就可以拥有一个Agent系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ece4a16f5c5cdcc38d4eba20b8c4fd24.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我喜欢说，我们从最初的聊天机器人开始，它们能与用户对话；然后我们为这些聊天机器人提供了使用的工具；如今，你可以为聊天机器人添加更多工具，使它能够自己生成尚未拥有的Harness。现在，你可以构建一个企业级的软件，允许它为每个客户即时编写插件连接器，这曾是专业服务团队的工作。你可以获得完全可定制的软件，且它可以与自己对话。我曾为开发日创建了一个看板，它能够自动修复自己的bug，非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，你也可以像Zed一样，将Codex嵌入到一个层级中，为IDE提供接口，使其能够与用户互动并进行代码编辑。这样，Zed就不必处理我们擅长的部分，而是可以专注于打造最好的代码编辑器。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的顶级合作伙伴，如GitHub，已经利用这些模式取得了巨大成功。我们为GitHub创建了一个SDK，允许他们直接与Codex集成。你也可以使用这个SDK将Codex作为你CI/CD管道的一部分，或者将它作为与自己Agent直接互动的工具。如果你想定制Agent层，完全可以这么做。举个例子，我们与Cursor团队紧密合作，他们将自己的Harness与我们开源的Codex CLI实现对接，成功地优化了系统性能，所有这些都是公开可用的，你可以克隆我们的代码库，随意使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex的未来是什么样的呢？它还没有发布一年，尤其是在推出Codex Max之后，变化非常迅速。它目前是增长最快的模型，每周服务数十万亿个token，这个数字从开发日以来翻了一番。我们可以合理假设，模型将变得更强大，它们能处理更长周期的任务，而且不需要监督。新模型的信任度将进一步提高，我相信这些模型已经能够处理比六个月前更复杂的工作，而且这种信任感将不断增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。SDK也将不断发展，以更好地支持这些模型的能力，使模型能够在执行任务的过程中不断学习，避免重复错误，并为写代码和使用终端解决问题的Agent提供更多支持，你将能够通过SDK在自己的产品中使用这一切。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，我们从中学到了什么呢？Harness构建非常复杂，特别是在新的模型不断发布的背景下。我们已经为你在Codex里构建了一个集成的工具，你可以直接使用它，或者查看源代码自行改进。除Coding 以外，通过它你还可以构建更多全新功能，而我们会处理确保你的计算机Agent具备最强的能力。同时，我们非常期待看到你们用它创造出的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=wVl6ZjELpBk&lt;/p&gt;</description><link>https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</link><guid isPermaLink="false">https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</guid><pubDate>Wed, 14 Jan 2026 08:56:01 GMT</pubDate><author>Bill Chen、 Brian Fioca</author><category>生成式 AI</category></item><item><title>不到百万级，看不见 MCP 的真实问题：创始人亲述这疯狂的一年</title><description>&lt;p&gt;一年前，MCP 还只是一个“把模型连到工具”的开源协议；一年后，它已经冲进了一个很少有协议能抵达的位置：事实标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场一年狂飙的亲历者之一——MCP 联合创作者、核心维护者 David Soria Parrra看来，最戏剧性的分水岭发生在四月前后：当 Sam Altman、Satya Nadella、Sundar Pichai 先后公开表态，Microsoft、Google、OpenAI 都将采用 MCP，“大客户”突然从 Cursor、VS Code 扩散到整个行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一年，MCP 从本地 “桌面玩具”，一路演进到远程 server、认证机制、面向企业可用的 OAuth 重构，再到 11 月引入 long-running tasks，把深度研究、甚至 agent-to-agent 交互变成协议的一等公民。David 的总结很直接：“这一年真的非常疯狂。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段对谈里，David 也很坦率地复盘了 MCP 这一年的取舍：做对的，是死磕标准 HTTP；踩坑的，是把关键能力做成了‘可选项’，结果客户端大多不实现，双向能力被削掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更现实的问题是扩展性：规模一上来，多实例、多 Pod 下同一段交互可能打到不同机器，不得不用 Redis 之类的共享存储来“拼状态”，请求量到百万级就开始吃力：“当规模一上来，这件事一点都不好玩。”“一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客内容整理，略有删节：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 的一年：从发布到行业事实标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：要不你先简单讲讲 MCP 的发展情况，以及之前为什么决定把它捐赠给基金会？接下来我们再系统回顾 MCP 这一年的演进，然后再请基金会的其他负责人加入，聊一些更宏观的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David Soria Parrra（MCP Co-creator）：如果回到一年前，MCP 刚发布的时候，其实谁都没想到它会在这一年里迎来如此疯狂的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老实说，这一年感觉像过了一个世纪。一开始是在感恩节和圣诞节前后，很多开发者开始自发地用 MCP 搭东西。随后，像 Cursor、VS Code 这样的“大客户”开始出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的拐点出现在四月左右——当时 Sam Altman、Satya Nadella、Sundar Pichai 等人陆续公开表示，Microsoft、Google、OpenAI 都会采用 MCP。那是一个非常明显的“分水岭”。&lt;/p&gt;&lt;p&gt;与此同时，我们也一直在推进协议本身的演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最初，MCP 几乎只支持本地使用：你在桌面上跑一个 MCP server，通过本地 stdio 和客户端通信。但到了今年三月，我们开始推进“远程 MCP server”——也就是如何通过网络连接 MCP，并且第一次引入了认证机制。到了六月，我们又对这套认证方案进行了比较大的修订，尤其是为了让它真正适用于企业场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们非常幸运，在三月到六月这段时间里，有真正做 OAuth 标准的行业专家，直接参与进来，帮我们把一些关键细节“拉正”。我们也在这段时间里大量投入在安全最佳实践上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了 11 月底，我们发布了新一轮重要版本，引入了 长时间运行任务（long-running tasks） 这一关键原语，用来支持深度研究类任务，甚至是 agent-to-agent 的交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的感觉是：MCP 的基础已经非常扎实了。接下来还有一两个关键原语和可扩展性问题需要解决，然后协议整体会进入一个相对稳定的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，这一年真的非常疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你刚刚提到 agent-to-agent，那是不是也涉及 A2A 协议？在 Agentic AI Foundation 成立时，有没有讨论过把其他协议也纳入进来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，这几乎是必然会发生的讨论。我们当然讨论过市场上其他协议，比如一些支付协议之类的东西。但在决定成立基金会时，我们有两个非常明确的原则：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，我们想 从小开始。这是 Anthropic 第一次参与开放源代码基金会，一切都是新的。我们希望先在一个相对可控的范围内学习如何把这件事做好，并且和 OpenAI、Block 一起，把基金会的节奏掌控住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，在协议层面，我们非常在意“事实标准（de facto standard）”。目前来看，真正已经具备广泛采用度的协议，只有 MCP。其他协议还没有“走到那一步”。当然，如果未来某个协议发展到那个阶段，并且在功能上是互补的，我们是完全开放的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在应用层，我们会更灵活；但在协议层，我们不希望一个基金会里同时维护五个做同一件事的通信协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在在基金会和 MCP 之间，是不是有点“戴两顶帽子”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：确实如此，但我主要精力仍然在 MCP 上。基金会本质上是一个“保护伞”，它最重要的作用是保证项目的中立性。至于基金会预算怎么用、办什么活动，这些相对来说反而是“比较枯燥”的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 MCP 的技术治理上，其实并没有发生本质变化。我依然是核心维护者，继续推动协议演进。&lt;/p&gt;&lt;p&gt;另外，我也会参与基金会的技术指导委员会（TSC），负责判断：哪些项目适合进入基金会？它们是否被良好维护？是否有真实采用？是否具备长期价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不希望基金会变成一个“项目垃圾场”。我知道有些基金会最终会落得什么下场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这一年 MCP 发布了四次规范更新，节奏非常快。尤其是三月和五月那次，引入了 HTTP Streaming 和认证。要不要给大家系统梳理一下？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：HTTP Streaming 那次更新非常关键，也是用户呼声最高的一次。我们在 11、12 月就已经意识到：下一步一定是远程 MCP，而远程就绕不开认证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP 的一个特点是：它在每一层都非常“有主见（prescriptive）”。比如，在客户端和服务端互不认识的情况下，认证该怎么做，我们希望只有“一种正确方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;三月版本里，我们做了一版认证方案。现在回头看，它“还行”，但确实有问题。说白了，是我对企业认证场景理解不够。MCP 的一个核心优势，是它的社区：当我不懂的时候，会有真正懂的人站出来帮我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：三月那版认证，主要问题出在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 里有两个核心角色：&lt;/p&gt;&lt;p&gt;身份提供方（Authorization Server / IdP）：发放 token资源服务器（Resource Server）：接收 token 并给相应的资源作为回报&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在第一版 MCP 认证规范里，我们把这两个角色合并进了 MCP server。对于创业公司来说，这没问题：你自己有账号体系，把 MCP server 直接绑在用户账号上，完全可用。但在企业环境里，这根本行不通。企业几乎总是有一个中央身份系统（比如 Google 登录、企业 SSO），用户每天早上只感知到“我登录了一次”，但背后其实是 IdP 在工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在六月的规范中，我们做了一个关键调整：明确把 MCP server 定义为资源服务器，和身份系统解耦。我们对“怎么拿 token”依然有建议，但不再强行绑定在 MCP server 里。同时，也补齐了动态客户端注册等细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 agent 代表用户去操作，比如帮我用 Linear、Slack，这个问题现在解决了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 本身是一个非常“以人为中心”的协议。它只定义：如果你没有 token，该怎么拿 token。一旦你有 token，后面就只是把它放进 Bearer Token 里而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们目前并没有对 agent-to-agent 或 agent 代表用户的认证方式做强约束。在企业内网、封闭环境里，大家已经可以通过 workload identity 等方式做到。但如果客户端和服务端彼此不认识，我们目前还没有一个“完美方案”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们从本地服务器（比如基于 stdio 的方案），一路演进到可流式的 HTTP。在这个过程中，有哪些经验教训值得分享？有没有什么后悔的地方，或者对其他人有什么建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：关于传输层这件事，其实有一个讨论，从过去几年一开始就从未停过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在最近两天，我们还在 Google 的办公室里，和一群来自 Google、Microsoft、AWS、Anthropic、OpenAI 的资深工程师坐在一起，专门讨论：到底需要做什么，才能把这件事真正、彻底地打牢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回到今年三月，当时我们希望引入一种新的传输方式，它能够尽量保留我们在标准 IO（stdio）里拥有的很多特性。因为我们当时——而且直到今天我依然坚信——MCP 不只是为了简单的请求-响应，它还应该支持 Agent。而 Agent 天生就是某种程度上“有状态”的，它需要在客户端和服务器之间进行一种长期存在的通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我们一直在寻找一种具备这些特性的方案。我们当然也研究过一些替代方案，比如 WebSocket。但在实践中，我们发现，要真正把一个可靠的双向流（bidirectional stream）做好，其实会遇到很多问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们就在思考：有没有一种“中间态”？这种中间态需要满足两个条件：一方面，它要足够简单，适合那些最基础的使用场景——比如用户只是想提供一个工具；另一方面，它又必须能够在需要的时候，升级成一个完整的双向流，因为你可能真的会遇到那种复杂的 Agent 之间相互通信的场景。正是在这样的背景下，可流式 HTTP（streamable HTTP）诞生了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事后回看，我觉得我们有些地方做对了，也有些地方做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做对的地方在于：我们非常坚定地只依赖标准 HTTP。但做错的地方在于：我们让太多事情对客户端来说是“可选的”。比如，客户端可以连接服务器，并打开一个从服务器返回的流，但它并不是必须这么做。而现实情况是——几乎没有客户端会这么做，因为这是可选的。结果就是，很多双向能力实际上被“抹掉”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，一些功能，比如 elicitation（征询） 和 sampling（采样），对服务器来说就变得不可用。原因很简单：服务器没有一个打开的返回流；而客户端在实现时会想，“这已经满足我产品的最小可用版本（MVP）了，我没必要再多做这些。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这最终成了一个问题。我觉得这是一个非常明确的教训。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个教训来自于协议设计本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们设计的这套传输协议，要求服务器端持有一定的状态。如果你只有一台服务器，这当然没问题。但一旦你要做水平扩展——比如跑在多个 Pod、多个容器里——问题就来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;设想这样一个流程：一次 tool call，然后是一次 elicitation，再接着是 elicitation 的结果返回。很可能，这几个请求会打到不同的服务器实例上。那你就必须想办法，让这几台服务器把这些信息“拼”在一起。现实中，这往往意味着你需要某种共享状态机制：Redis、Memcached，或者别的什么共享存储，总之你需要一个地方，能够让这些服务器共享状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，这当然是可行的。我们在 PHP 应用、Python 应用里早就见过类似的模式。但说实话，当规模一上来，这件事一点都不好玩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们也知道，一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们现在坐在这里，不断地问自己：如何在协议的下一次演进中，做到这几件事？&lt;/p&gt;&lt;p&gt;对简单的 MCP Server 来说，仍然尽可能简单；在需要的时候，允许完整的双向流；同时，还要具备良好的可扩展性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，我们正在逐步找到正确的解法，但这件事本身真的很复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为今天的大多数技术选择，其实都非常极端：要么你做一个很简单的东西，比如 REST；要么你直接上“全双工”的方案，比如 WebSocket、gRPC。而我们需要的，其实是两者同时存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在巨头之间“做标准”是什么体验？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：和这么多顶级公司一起做标准，是什么感觉？在那样的场合，大家都是资深人士，每个人都有自己的观点。谁来做最终决定？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：真的太有意思了。我能和业内最顶级的工程师一起工作。通常我们的目标是尽量达成共识。现实情况是，从技术角度讲，最终拍板的人是我，但说实话，这更多是一种形式上的存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正重要的事情在于：我们努力把讨论不断收敛，明确哪些是真正大家都认可的问题，哪些是暂时还存在分歧的问题，然后在这些边界之内，去构建我们能做到的最佳解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个过程需要时间，需要大量迭代，但说真的，这件事本身非常有意思。因为你能看到来自不同公司的、非常独特的问题形态。你甚至能从问题本身，看出一家公司的“性格”——比如 Google 面临的问题和 Microsoft 就完全不同，而这些差异，很大程度上来自他们各自构建系统的方式。同样，Anthropic 的问题看起来也和 OpenAI 的问题不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我最喜欢的一点在于：有时候你会突然意识到，自己正坐在一个房间里，周围全是彼此竞争的公司，但大家却在一起构建同一件东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在开源世界已经待了大概 25 年了，我真的非常热爱这种状态。当一个标准真正运转起来时，这就是理想状态。而且这些人都非常优秀，我从每一位同行身上都学到了很多。所以我非常感激，自己能处在这样的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来有点像 IETF 的标准制定流程？你们有没有讨论过，这种“私下的小圈子”运作方式，和更传统的标准组织之间的差异？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这是个很有意思的问题。某种程度上，它确实有点像 IETF，但也有明显不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IETF 是一个完全开放的论坛，任何人都可以参与。它的结果是——不是因为刻意如此，而是“偶然地”——整个流程非常依赖共识，因此速度也相对较慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这种慢，在很多方面其实是优点。因为一旦标准定下来，基本上是不可逆的。比如你看看 OS 2.1 规范，它已经制定了三四年，到现在都还没完全结束。这就是 IETF 标准化的节奏：这些事情本来就会花非常非常长的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为这对某些领域是好事，但在 AI 领域，目前的变化实在太快了，你几乎被迫要选择一个更小的核心群体。因此我们选择把 MCP 运作成一个非常传统的开源项目：有一个大约 8 人的核心维护者小组，基本上由他们来做最终决策；其他人可以提供输入、提出建议，而且很多变更并不是来自核心维护者，但决定权在他们手里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一种折中方案：一部分是共识驱动，一部分则是带有一点“技术独裁”的意味。如果你想要快速前进，这种模式在当前阶段对 MCP 来说是合理的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你们是如何平衡模型能力演进与协议设计之间的关系的？毕竟 Anthropic 和 OpenAI 都在做大量后训练（post-training），让模型更擅长工具调用；这会不会影响你们对协议形态的偏好？反过来，协议是否也会反向影响模型训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，我不敢说自己对研究侧的所有事情都 100% 熟悉——我更多是产品背景。但从我了解的情况来看，协议确实会在一定程度上影响后训练，比如我们在模型卡中会使用 MCP Atlas，确保模型在面对真实世界中大量存在的工具时，能正常工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从另一个角度讲，协议的底层原语，其实很少直接被模型能力的提升所驱动。我们更像是在预期模型能力将会呈指数级增长，因此在协议中，依赖了一些你可以通过训练不断强化的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个更具体的例子。很多人都讨论过 MCP Server 的上下文构建问题。因为 MCP 打开了通往大量工具的大门，如果你天真地把所有工具一次性塞进上下文窗口，那只会造成严重的膨胀。&lt;/p&gt;&lt;p&gt;这就好比把所有技能、所有 Markdown 文件一次性丢进上下文里，结果当然会一团糟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们其实从一开始就知道，可以采用一种叫做渐进式发现（progressive discovery）的方式：先给模型一小部分信息，让模型在需要的时候，再主动请求更多信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这本质上是一个通用原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这里正是我们这些“大模型公司”具备的一点前瞻性所在——我们知道，如果愿意，是完全可以通过训练，把这种能力系统性地强化出来的。模型在原理上已经能做到这些事情了，训练只是让它做得更好。任何支持工具调用的模型，都可以做到这一点；只是如果你专门为此训练过，它的表现会更好。所以在这个层面上，协议设计和模型训练是相互配合的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但归根结底，渐进式发现这种机制，本身就内生于任何具备工具调用能力的模型之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也引出了“上下文腐烂（context rot）”的问题。还有 MCP 和所谓 “code mode” 的讨论——比如有人会说，“Anthropic 提倡 code mode，而 MCP 又是 Anthropic 做的，那是不是说明 code mode 才是正确方向？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：首先澄清一下，官方博客其实从来没用过 “code mode” 这个词，那是大家后来叫出来的。我们内部更常说的是 “programmatic MCP”，但本质上讨论的是同一件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键在于：MCP 是应用和服务器之间的协议，模型本身在技术上并不直接参与 MCP。所以问题其实变成了：应用拿到一堆工具之后，该怎么用？你可以用最朴素的方式：把工具直接暴露给模型，让模型逐个调用。但你也可以更“创造性”一点：模型非常擅长写代码，那如果我们把这些工具当成 API，交给模型生成一段代码，让它提前把多个调用组合好，再在一个 sandbox 里执行呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本质上，模型原本就会做这样的组合：调用 A → 拿结果 → 回到推理 → 调用 B → 再组合成 C。你只是让模型提前优化了这个过程，把它编译成一段可执行代码而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 MCP 的价值并没有因此消失：&lt;/p&gt;&lt;p&gt;认证（authentication）仍然由 MCP 处理；接口是为语言模型设计的；工具是可发现的、自文档化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力依然存在。你只是换了一种使用方式而已。所以当有人说，“那 MCP 是不是就没用了？”我其实挺困惑的。它不是没用，而是被用在了不同的层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着模型和基础设施逐渐成熟——比如你可以默认 AI 应用都有 sandbox 执行环境——你确实可以玩出更多有意思的花样。但这并不意味着，一个把模型连接到外部世界的协议就失去了价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人更愿意把这种变化，看作一种优化，说得直白一点，就是 token 级别的优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 有没有竞争对手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这正好可以引出 skills。skills 是一个相对较新的概念。我之所以提到它，是因为在我脑子里，它和渐进式发现、预置代码脚本这些概念是连在一起的。而且 skills 还能生成 skills，本身就很有意思。很多人试图把 MCP 和 skills 放在对立面来比较，显然它们并不重叠，但你是怎么看待这个问题的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我同意。我觉得有意思的点就在于：它们并不重叠。它们解决的是不同的问题。&lt;/p&gt;&lt;p&gt;我觉得 skills 非常棒，而且你知道的，我认为 skills 最核心的出发点之一，就是渐进式发现（progressive discovery）这个原则。但我也认为，“渐进式发现”这种机制，其实是通用于你能用模型做的几乎任何事情的——它不是 skills 独有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 skills 到底提供什么？它提供的是某一类任务的领域知识（domain knowledge）：比如你应该如何做事、如何表现，模型应该如何扮演一个数据科学家，或者如何扮演一个会计之类的角色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 MCP 提供的，是你能对外部世界采取的真实动作的连接性（connectiveness）——也就是你能执行哪些实际操作、如何把这些操作真正连到外部系统上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为它们在某种意义上是正交的（orthogonal）：skills 给你的是更“纵向”的能力——偏领域、偏角色、偏方法论；而 MCP 给你的是更“横向”的能力——偏连接、偏动作、偏“给我那个具体操作”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，skills 也可以执行动作。它能执行动作，是因为你可以在里面放代码和脚本，这当然很棒。但这里有两个关键点，我觉得很多人容易忽略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，你需要一个执行环境（execution environment）——也就是你需要一台机器来跑这些代码。是的，你需要“机器”。这在很多场景下完全没问题：比如你在本地跑一个东西（像 Cloud Code 之类），那我们就可以讨论 CLI；在这种你确实拥有执行环境的场景里，这套方式就非常合理，也很好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;或者，如果你有一个远程执行环境，那同样也说得通。但即便如此，你在这条路径上仍然得不到认证（authentication）这一块能力。所以我认为 MCP 带来的关键价值之一，就是它把认证这件事补齐了——这是 skills 本身不提供的那部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个点是：你不必去处理“外部方的持续变化”。举个例子，如果你接的是一个 Linear 的 MCP server，那么对方可以持续改进它，而你不需要在自己的 skill 里去处理这些变化——它不是被“固定在某个时间点”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三个点是：你其实不一定需要一个本地的执行环境，因为执行环境在某种意义上是“在别处”的——它在服务器端。也就是说，执行发生在 MCP server 那边。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，如果你在构建的是一个 Web 应用，或者一个移动应用，这些特性在某些方面会更契合、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以整体来看，我认为它们大多数时候都是正交的。并且我确实看到过一些很酷的落地方式：人们用 skills 去探索不同的功能、不同的角色（比如会计、工程师、数据科学家），然后再用 MCP servers 把这些 skills 连接到公司内部真正的数据源上。我觉得这是一个非常有趣的模型，也最接近我理解和看待它们关系的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以 MCP 是连接层？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我会说是通信层。是的，通信层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从架构上讲我很好奇：MCP client 是放在每个 skill 里面，还是大家共享一个 client？比如共享 client 还能发现 skills 之类的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们是共享的方式。我觉得从技术上你确实更想走“共享更多”的方向——共享越多，你能做的事情就越多：比如做 discovery（发现）、做连接池（connection pooling）、做自动发现，甚至你可以让 skill 只用很“松散”的方式描述它想要什么，然后系统去你有权限访问的 registry 里帮你找一个合适的 MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力只有在 shared 的架构里更容易做出来。当然，最终两种方式都能工作，只是这仍然是一个值得继续实验的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 怎么用 MCP？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我想强调一下，可能很多人都没意识到——你刚才一直说“我们怎么做怎么做”，但实际上我觉得外界并不理解 Anthropic 内部到底 有多大规模地在 dogfood MCP。我也是看了 John Welsh 的演讲才真正理解，他说：“我们有一个 MCP gateway，一切都要走这个 gateway。”你能多讲讲这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：当然。我们内部两种都用：skills 用得很多，MCP servers 也用得很多。因为你要让大家很容易部署 MCP，你需要和公司内部的 IdP（身份系统）打通之类的东西。所以我们为自己定制开发了一个 gateway。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你只需要把 MCP server 部署起来，剩下的都是内部应用、内部系统在用。有些东西“技术上”算外部系统，但因为它们没有提供第一方 MCP server，我们就自己做了。比如我们有一个 Slack 的 MCP server——我特别爱用。它可以让 Claude 帮我总结 Slack。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们内部还有很多类似的用法：例如我们每半年（或者一年两次）会做一次员工调查，问大家对公司、对未来、对 AI、对安全等议题的感受。我们也有一个 MCP server 支持这件事，然后你可以围绕结果问很多问题，这非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这些都是你们团队维护的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不是。我们维护的是 gateway。但有意思的地方在于：MCP 从一开始的想法就是——在我们开源之前，它源自一个很现实的困境：公司增长太快了。我在研发工具、开发者工具这一侧，增长速度一定跟不上业务扩张。那我怎么做一个东西，让大家能“自己为自己构建工具”？&lt;/p&gt;&lt;p&gt;这就是 MCP 的起源故事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你现在回头看，一年之后发生的事情，正好就是我们当初想要的：大家真的在为自己构建 MCP servers。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我甚至可能完全不知道 Anthropic 内部 90% 的 MCP servers，因为它们可能在研究团队里，我看不到；或者人们就是自己做给自己用，我也不会被同步到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那它们是自己 host 吗？还是有远程托管？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：基本上大家只需要一条命令启动，它就会在一个 Kubernetes 集群里跑起来。算是“半托管”的形态。对任何大公司来说，这类平台基础设施都很重要。外部也有一些平台会帮你做这件事，但从安全角度，我们倾向于自己做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过外界也有类似的产品。比如有人做了一个叫 fast MCP 的东西——Jeremiah 他们做的 fast MCP cloud，有点像这样：两条命令，你就能跑起一个 MCP server 实例，支持 HTTP 流式传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多企业还会用类似 LiteLLM 这样的东西做 gateway：你甚至可以启动标准 IO 的 server，把它接到 gateway 上，然后由 gateway 来处理认证等“所有麻烦的部分”。所以落地路径其实很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为你真正想要的“理想基础设施”是：让部署变得极其琐碎、极其简单——比如“一条命令”启动一个原本只是 stdio 的 MCP server，然后它瞬间变成一个带有 HTTP streaming、并且集成了认证的远程 MCP server。最终开发者只需要做“标准部分”，其他复杂部分都由平台替你完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你把这个点讲出来，因为很多人会直接把这套思路拿回公司里落地。否则替代方案就是：混乱、重复造轮子、各自重建一遍。顺便 shout out Jeremiah——我还邀请他来我在纽约的峰会做一个 fast MCP 的 workshop。他写过一篇很棒的博客，说我们看到的 MCP 使用，很大一部分其实都发生在企业内部。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我们也观察到同样的现象：在大型企业内部，你几乎到处都能看到 MCP。它的增长速度，比你想象得快得多——因为它多数都在企业内部发生，外界根本看不见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Registry怎么演化？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说到 discovery，你们推出了官方 registry。然后又出现了各种 registry 公司、gateway 公司。现在官方 registry 里甚至出现了“自动把自己的 MCP server 放进官方 registry”的子 registry。你们是不是需要更多 registry？你从推出 registry 这件事上学到了什么？你觉得未来会怎么演化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们看到很多不同的 registry 冒出来。我们一直觉得，生态确实需要一种类似 npm / PyPI（MPM） 的模式：有一个更中心化的地方，任何人都可以把 MCP server 发布上去。&lt;/p&gt;&lt;p&gt;这就是官方 registry 最初的出发点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们同时也想推动：至少整个生态要有一个共同的标准，让不同 registry 之间能“说同一种语言”。因为我们真正想实现的世界是：模型可以从 registry 里自动选择一个 MCP server，安装它，用在当前任务上——像魔法一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要做到这一点，你需要一个标准化接口。我们很早就开始和 GitHub 团队合作（大概四月份），但后来我被别的事情分走了注意力，比如认证，去集中解决那块了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我希望看到的方向是：未来会有一个“官方 registry”，任何人都可以往里放 MCP server。它的角色就像 npm ——而 npm 也有完全相同的问题：任何人都能发布，你并不知道该信谁、不该信谁；会有供应链攻击。这是公共 registry 的基本属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们才提出了 子 registry（sub-registries） 的概念：像 Smithery 这类服务可以在官方 registry 之上做过滤、做精选、做策展（curate）。我们希望生态最终能形成这样的结构。&lt;/p&gt;&lt;p&gt;我们现在还没完全到那个状态，但正在往那个方向走。比如 GitHub 的 registry 是“策展式”的，同时它和官方 registry 讲的是同一种格式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终我们想要的是：作为一家企业，你可以有一个内部 registry——它基于官方 registry 的镜像，再加上你自己的私有 MCP servers；它是你信任的来源，同时它暴露的 API 和官方 registry 一样。这样无论是 VS Code 还是其他客户端，只要指向你的内部 registry，就可以顺畅工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这很有意思，因为 npm 在某种意义上更像一个“下载网关”。我其实不太会去 npm 做发现，我更多是在别处看到包，然后再去 npm 安装。你觉得 registry 的核心是 discovery 吗？还是 agent 会用别的方式完成发现？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我认为 discovery 在模型世界里会更重要。这里和 npm 的差别在于：&lt;/p&gt;&lt;p&gt;我们是在做一个 AI-first 的东西，我们可以假设：有一个聪明的模型，它“知道自己想要什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在过去是不存在的。如果你今天重新设计现代包管理系统，并且把模型当作核心，你可能会做出类似的交互：“这是我想做的事，你自己决定装哪些包，我不在乎，反正把事情做成就行。”这就是它的类比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但再次强调：公共 registry 不应该直接让模型这么做，因为公共 registry 很容易变成一个“垃圾场”。你应该在一个可信、被策展过的 registry 上做这种自动化选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你那句话——模型知道自己想要什么。因为现在很多人都有一个梦想：agent 能用 MCP 目录去发现新的 server，自己安装自己使用。这听起来非常 AGI。如果真能跑通当然很牛，但也可能跑不通。要做到这一点，到底需要什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我觉得需要两件事：&lt;/p&gt;&lt;p&gt;第一，你需要一个好的 registry 接口。&lt;/p&gt;&lt;p&gt;第二，你需要真的去为这个目标做工程、做实验，看看什么可行、什么不可行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你肯定需要信任等级（trust levels）。你可能还需要签名（signatures）。我有一个想法——不确定会不会真的做——比如：你可以附带来自不同模型提供商的签名，表示他们扫描过这个 MCP server，并且愿意为它背书：&lt;/p&gt;&lt;p&gt;“Anthropic 的签名：这些 tool descriptions 是安全的”“OpenAI 的签名：我们认为这些是可信的”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后你就可以基于这些签名自行决策。这有点像分布式代码签名——不过也不完全分布式，本质上可能还是中心化的。但我认为这是你最终会需要的一类机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过最先跑通的场景，可能反而是企业内部：企业会用私有 registry，本身就带有隐含信任。就像他们今天已经在用私有 npm / 私有 PyPI 一样，他们也会用私有 MCP registry。在这种环境里，你天然有 trust，然后就可以开始做搜索和自动选择。我们自己其实就有内部 registry：当你通过 John 那套基础设施启动一个 MCP server，它就会被注册进去。所以我们也需要在内部继续做实验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Sampling：理想很美，但客户端不配合&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你今年在伦敦办了一些活动，你看到什么好的 sampling 用例了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：还没有特别多。我从 sampling 这件事学到的一点是：人们想在 sampling 的过程中使用一些“只在 sampling 时出现”的工具——这些工具并不是 MCP server 暴露出来的那套工具。但我们之前没有能力做到这一点。在这次迭代里我们刚修复了这个问题，所以我们希望未来能看到更多 sampling 用例。偶尔会有一些 MCP server 在用 sampling，但不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其是当 MCP servers 从“本地为主”走向“远程为主”，在远程场景里，通常更好的选择可能是直接提供 SDK：你完全控制它、自己部署，甚至还可以收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在本地场景里，sampling 的价值更大：因为你是在给很多人分发一个东西，你并不知道他们用的是哪个模型、哪个应用（可能是 VS Code，也可能是 Claude Desktop），这种情况下 sampling 才更有意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在的问题是：客户端基本都不支持 sampling。所以 sampling 这件事让我挺沮丧的——我仍然觉得这是个很强的想法，但你知道的，有时候你总得赢一些、也得输一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你们也在升级它，我还是很期待。有点奇怪——如果采样这件事做对了，它某种意义上会变成真正的 agent-to-agent 协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你看到的大多数用例还是偏“数据消费”吗？我自己的 MCP 用法也 mostly 是拿上下文、拿数据。最多的 action 可能就是更新一下 Linear 任务状态。你见过很复杂的“用 MCP 做动作的工作流”吗？还是大家基本都在用它做上下文？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：大多数人确实是用它做上下文，这占了绝大多数。毕竟它的名字就叫 Model Context（模型上下文）。顺便说一句，OpenAI 的 Nick Cooper 经常跟我说——而且他说得对——MCP 这个名字可能取错了，它确实会让人感觉用途被“限制”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我看到的主要还是数据用例。也有人把它用于 deep research，一些更复杂的 agent 暴露出来，但并不普遍。deep research 这种自定义研究用例不算罕见，但除此之外，大多数还是数据、以及围绕数据的深度研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在你还会看到一个新方向：通过 MCP UI（未来我们可能叫 MCP Apps / MCPI）暴露 UI 组件。我觉得这非常有前景，也非常有意思。现在在一些 chat apps 里已经能看到不少类似实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Tasks：为长时间、异步 agent 操作而生的新原语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇，因为如果大多数用例是“上下文”，你们做 tasks 这个原语，就好像大家暂时还没怎么用它。你们设计 tasks 的出发点是什么？你期待它怎么被用起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们做 tasks，是因为很多人来找我们说：“我们真的需要长时间运行的操作——也就是 agents。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想要那种“深度研究任务”，可能一小时才完成；甚至可能一天都跑不完。过去人们会很别扭地用 tools 去实现这类事情——工具本质上就是 RPC 接口，理论上你能凑出来，但很快就会变得别扭：模型需要理解“我得去轮询、我得去拉取”，体验很差，也不是一等公民（first-class primitive），限制很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这类诉求太普遍了：大家都想要长时间运行的 agents。GitHub issue 里，大公司也一直在说“我们需要 long-running operations”。所以我们觉得必须做点什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在 tasks 刚刚落地到 SDK，还需要落地到客户端，然后我们才会看到更广泛的使用。但我非常确信：自定义研究类任务会大量用上它，其他场景也会逐步跟进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我对 tasks 非常看好。我觉得任何编排系统或协议都得有 sync 版本和 async 版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：完全同意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 tasks 的设计上，有没有哪些重要分岔点？比如本来有两条路，你们选了其中一条。&lt;/p&gt;&lt;p&gt;David：讨论非常多。有人提议：tasks 其实就是“异步 tools”，做成一个新的 tool primitive 就行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但对我来说，我的 试金石（litmus test） 一直是：如果未来我想把 Claude Code 或任何 coding agent 当作一个 MCP server 暴露出来，那么 tasks 必须能支撑这种形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;纯粹的异步工具调用做不到这一点。你需要一种操作方式：它能够在长时间运行的过程中返回中间结果。理想状态下，你会想暴露这样的东西：“我通过调用这个工具、那个工具、还有那个输入，得到中间产物……最后得到结果。”这才是你希望一个长任务能够表达的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;tasks 现在还没完全做到这一步，但它的设计是“足够通用”的，未来可以支持这种更丰富的表达——这就是最核心的约束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个关键约束是：我们不希望 tasks 成为 tools 的复制品 ——只是语义稍有不同。我们希望它是一个更抽象的概念：你通过一次带元数据的 tool call 来创建一个 task，然后系统自动创建并管理这个 task。所以 task 更像一个“容器（container）”：它描述了一段从开始到结束的异步过程，而我们当前用 tool call 作为触发方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样的抽象会打开很多未来可能性。所以我觉得，真正的设计目的是让实现变得更抽象。（虽然）实现起来很复杂，但也最终被解决了，因为复杂性会被 SDK 吞掉：SDK 会帮你实现细节，在开发者视角里，它就是一个 async 调用，然后返回结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来会和很多异步 RPC 框架有点重叠，比如 JS 世界的 tRPC、或者各种 protobuf 体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。从接口风格来说，它很像经典的操作系统接口：你创建一个 task，然后不断 pull（轮询）直到它完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后我们下一轮会做一个优化——这次没来得及做：你不用每隔几分钟/几小时去 pull，server 可以回调你（发事件、webhook 之类的）告诉你“我完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是优化，但核心接口始终是：客户端可以 pull。这也很像操作系统里的一些文件系统操作：客户端轮询是一种最通用、最可靠的基线能力……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以一直 pull（轮询）：文件变了吗、文件变了吗……但你也可以用现代一些的内核接口，比如 inotify 之类的通知机制，或者 io_uring 之类的方式，它会告诉你：哦，我完成了——很好，文件变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我学到一个“骚操作”——server 可以一直把 HTTP 连接挂着，等它做完了再断开；连接断开本身就成了一个信号，告诉后端“完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，但我们不一定想这么做。因为它可能要跑几天，我也不知道别人会怎么处理这种连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这其实挺不负责任的，但确实很酷。老实说，tasks 真的很有意思——我们在做 Devin API、以及 Cognition 那些东西时，也基本被迫“重新发明”过类似机制。这也很有代表性：每个人最终都会需要某种 long-running operation。而当你在调用一个 agent 时，你同样需要这个能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。但对我们来说，有一个有意思的点是：MCP 一直在做的事情，是把大家“此刻正在尝试做的东西”封装起来；我们并不想强行规定一年后大家“应该怎么做”。因为我们不知道，我们不预测未来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做 tasks，是因为大家说：我们现在就需要它。实际上我们六个月前就需要它了。于是我们说，好吧，那现在就是动手的时候了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不想做那种“预测未来”的协议，所以才努力让协议保持相对最小化。虽然也有人会觉得：现在协议里的 primitive 已经太多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;超长任务与上下文压缩&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一个小问题。假设是超级长的任务，过程中会来回传很多消息。Anthropic 在上下文压缩（或者叫 compaction）这件事上算是领先者之一，其他实验室也在做类似事情。那这种场景怎么处理？我们是不是就无状态地把上下文截断也没关系？你需要保留“全过程完整日志”吗？还是说删掉就删掉了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不需要。你看，我们现在这个行业还是非常早期，我们一直在学习：模型到底需要什么、不需要什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;甚至到今天，有些 agent 已经开始在跑了几轮之后丢弃 tool call 的结果，因为它不再需要了。我觉得这非常好。所以除了 compaction 之外，我觉得你还会看到更好的机制：更清楚地理解“该保留什么、不该保留什么”。比如对一个长时间异步任务，你可能会这样：某段时间模型确实需要看到全部过程，但当你拿到最终结果之后，你就把其他东西都丢掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你甚至可以调用一个更小的模型——比如 Haiku ——让它来判断：这些内容里哪些该保留？告诉我。也可能最“AGI build”的方式就是：让模型自己决定它需要保留什么。所以你会看到两种世界并存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在还没有唯一答案，因为大家仍在摸索。compaction 是一个很好的阶段性方法，但它也不会是最后一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，如果你更认真地思考：你能训练模型在这里做什么，我觉得会有更好的方式。但这些都和“你如何获取上下文”是相互独立的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直把 MCP 看作一个 应用层协议：它只负责“你如何获得上下文”。至于“你如何选择上下文”，那是应用层问题——所有 agent 应用最终都会面对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来会有很多技术路径。一年前所有人都会说：RAG 才是答案；现在大家又说 RAG 好像“死了”。我们开始用模型、用 compaction。至于一年后会怎样，我也不知道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我还有个问题：你怎么看 MCP servers 在未来的定位——它们是给开发者用来构建 AI 应用的？还是一个面向 AI 消费者、让他们把各种服务“插上就能用”的协议？我觉得很多人会把它理解错：他们说“我有 REST API，为什么还需要 MCP？”在我看来，MCP 可能并不是“给开发者用的”，而是给使用 AI 工具的人，用来把东西插进去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我经常被拿来和 REST API 比。这个对比挺有意思的，因为这里其实有两个问题：第一，REST 并不告诉你认证该怎么做。第二，你们已经在跟我抱怨 “tool bloat（工具膨胀）” 了，但你们有没有看过平均一个 OpenAPI spec 有多长？你把那个塞进模型里，膨胀只会更严重——实际上会糟得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有意思的是，当人们尝试一比一映射时，模型经常会有点迷糊：你会有“按名字搜索、按 ID 搜索、按某字段搜索”等等，突然冒出五个长得很像的工具，模型就会问：你到底要用哪个？我也不知道了。所以这是个关于 REST vs MCP 的小插曲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我确实希望 MCP 生活在一个更“消费者导向”的世界：这是使用者应该知道的能力。我想要的世界是：你打开应用，直接说“做这件事”，它就把事情做完——它在底下自动连到合适的服务。MCP 是幕后细节；开发者需要知道它，因为这是通信通道；但对最终用户来说，你只需要拿到结果、把任务完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;坦白讲，我更喜欢一个世界：没人需要知道 MCP 是什么。比如我妈如果要用 Claude，她不应该知道 MCP 是啥。但我认为 MCP 的重点确实是：让外部服务“可插拔”。在这个意义上，它更偏消费者侧。当然开发者也有用例：他们作为 builder 要构建这些东西；而且我也仍然很爱我的 Playwright MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇你说的 MCP Apps / UI。现在每个客户端——比如 ChatGPT——都有自己的一套渲染方式。所以如果我习惯了某个产品的 MCP app，但换到另一个地方，它可能就是另一个版本、另一种策展方式，体验会很不一样。我想知道你怎么看：尤其现在 OpenAI 也进了基金会，你觉得会不会形成统一结构？让大家按同一个标准来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这里有两个影响源：一方面，MCP UI（或者 MCPY）作为项目本身已经存在一段时间了，它有很多很好的想法。OpenAI 也吸收了其中一些想法，并做了不少改进。更重要的是：我们三周前在 MCP 博客上刚宣布——我们正在和他们一起做一个共同标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的目标是回到一个世界：你为一个平台开发一次，就可以在所有平台用；或者说 “一次构建，到处运行”——你在 ChatGPT 里能用，也可能在 Claude、在 Goose、或任何实现了该标准的程序里用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这件事的核心驱动力是：现代 AI 应用几乎一切都是文本交互，这没问题，也挺好；但有些事情，人类就是更擅长用视觉来做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最典型的例子：选飞机座位。如果让你用纯文本选——“这里有 25 个座位可选”——谁愿意这么干？你根本不知道这些座位在机舱图上是哪里。你当然想要一个 UI：你能点着选；而模型也能在这个 UI 上导航、交互；并且你作为人类也能同时交互。这就是我们想要的方向：做更丰富的界面。纯文本界面确实有天然限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你会在音乐制作等场景看到这种需求；你也会看到品牌方非常在意界面呈现。购物也是一个极好的例子：购物行业 20 年的 A/B 测试，研究“怎么把东西卖给你”最有效——购物界面其实非常复杂。所以我们需要一种方式，把这些熟悉的复杂 UI 展示给用户，让用户能交互。这就是 MCP Apps 最终要做的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：技术方向上是 iframe？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，是 iframe。本质上你通过 MCP resource 提供 原始 HTML，把它放进一个 iframe，然后通过一个明确的接口用 postMessage 和外部通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为是 raw HTML，而且不是加载外部内容，你如果愿意，理论上可以提前做安全分析。同时 iframe 也天然能提供比较清晰的隔离边界，让外部应用在一个安全边界内与之交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：iframe 在浏览器里用了很多年。我唯一担心的是 CORS……我太讨厌 CORS 了，而 iframe 总会遇到 CORS 问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，但这里理论上不加载任何外部内容——至少我们不希望它这么做。当然，未来我们可能会不停迭代，五年后可能会出现一堆 CORS header 之类的复杂东西。但现在我们还是从小做起：纯 raw HTML，最好不要有外部引用，这样就不会碰到那些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那能继承宿主应用的样式吗？&lt;/p&gt;&lt;p&gt;David：不能。iframe 里你得把样式内联进去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来很小，但 UI 团队会非常在意。大家会希望它看起来像 ChatGPT。&lt;/p&gt;&lt;p&gt;David：完全同意。品牌方和设计师会非常非常在意。这也是我们需要解决的问题：先把东西推出去，让大家用起来，然后基于真实使用方式迭代。这也正是为什么我觉得长期来看它不应该一直是 iframe。我不知道最终解决方案是什么，但我们可能需要一种“新的 iframe”，它允许一定的“渗透性/可融合性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得这挺合理。另一条路可能就是“AGI build”的方式：给它一个 tool 说“给我样式”，模型再去问宿主应用“我应该长什么样”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 MCP app 应不应该知道自己被嵌在哪个父应用里？比如父应用也暴露工具给模型调用，对吧？那是不是需要一个标准接口让父应用把样式传下去？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：可能是。这个问题很大。我得去问问团队。我自己并不在最底层细节里，我更多是站在整体方向上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这对我来说有点意外。我以前从没关注 MCP UI，结果你们突然都采纳了。我就想：好吧，那看来它已经是 MCP 的一部分了——它让 MCP 从纯后端议题，变成了前端议题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：需要说明的是：技术上它是 MCP 的一个扩展（extension），它不是 MCP 核心的一部分。这更多是治理层面的区分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你是一个能渲染 HTML 的客户端，你可以考虑实现它；但就算你不实现，你仍然是一个 MCP client。现实是：很多 CLI agent 根本渲染不了 HTML，所以它们永远不会实现。这没问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有其他类似的扩展吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们可能会在金融服务方向做一些扩展。比如一年后，你可能会看到这样的世界：客户端会有某种“认证/资质”，并得到一个签名——证明它是“金融服务 MCP 客户端”，然后向 server 出示这个证明，server 才允许连接，因为它知道客户端会遵守归因（attribution）等法律合同要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;类似的机制也会出现在 HIPAA（医疗健康数据）这类场景：当你面对公共 server 和公共 client，同时还要处理敏感数据时，你必须提供一些保证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这不是 OAuth 的一部分吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不一定。举个例子：假设客户端同时装了五个 MCP servers，其中有一个是医疗 server。这个医疗 server 可能会要求：在这个 session 里，你不允许使用其他 MCP servers，因为我给你的数据不能泄露到任何地方。你必须保证它不会跑出去——因为这是 HIPAA 数据、或者金融数据。这是一个很典型的约束：你不希望自己的社保号、健康数据不小心出现在别的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;加入 Linux 基金会会不会分心？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们接下来会切到 AAIF ，最后，有没有什么行动号召？比如招人、或者呼吁大家参与 MCP spec？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：最重要的还是——每天都去用 MCP 去构建：去做真正好的 MCP servers。我们看到很多很一般的 MCP servers，也看到一些非常非常优秀的。把 server 做好、把用法做扎实，这很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，我们是一个相当开放的社区，按传统开源方式运作：本质上取决于大家愿意投入多少时间和精力。所以你可以通过很多方式参与：给反馈、在 Discord 里交流、给点子；也可以帮我们做 SDK，比如 TypeScript SDK、Python SDK。我们也一直在找新的 SDK——比如我们有 Go SDK 在推进，但我们没有 Haskell SDK。如果你是 Haskell 开发者，你也许可以来写一个（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，可以做的事情很多。不要低估“参与社区”本身的价值。当然也别忘了去构建：现在机会太多了，尤其是我们对 progressive discovery 的理解更成熟了，对 code mode 的理解也更成熟了——接下来会出现一代新的客户端、一代新的 server，我非常期待大家去做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我最后一个问题，是想让大家直接听你说。我能感受到你的能量，我也对你们做的事情非常兴奋。但很多人对 MCP 加入 Linux 基金会有点焦虑：他们会说，“这是不是意味着 Anthropic 分心了？”你能回应一下吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我很喜欢你问这个问题。我完全理解大家为什么会这么想，但事实恰恰相反。Anthropic 的投入和承诺没有变：我们还是同一批人在做 SDK，我们的产品仍然高度依赖 MCP。我还是 MCP 的核心维护者。技术上什么都没变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基金会真正带来的核心变化只有两点：第一，它让整个行业确信：MCP 会永远开放，永远不会被拿走。历史上确实有公司把开源项目又变回专有。协议领域也有很多专有例子——比如 HDMI。你看 HDMI 在 Linux 上的那些问题。HDMI 2.1 的 HDMI Forum 不愿意让 AMD 开发 HDMI 2.1 的开源 Linux 驱动——真的，有些资料你可以去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以行业里很多人会盯着这些风险。基金会的意义就是：现在 MCP 归属一个中立实体，它会一直开放。你可以使用 “MCP” 这个名字，也不会有人因为商标去起诉你。这会给生态巨大的信心：它是中立的、可持续的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，如果说我最骄傲的是什么：我觉得我们已经在行业里为“开放标准”定下了基调。现在我们可以利用这个势能，在一个中立空间里建立社区：让大家把真正做得好、维护得好、长期可靠的项目放进来，成为基金会的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们的门槛会很高：项目必须维护得很好。我们不想、也不会把基金会做成“分心”或“甩包袱”的地方。对我们来说，MCP 仍然是产品核心、仍然超级重要；Anthropic 的承诺和投入一如既往。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=z6XWYCM3Q8s&quot;&gt;https://www.youtube.com/watch?v=z6XWYCM3Q8s&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</link><guid isPermaLink="false">https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</guid><pubDate>Wed, 14 Jan 2026 08:52:18 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>“Gemini 3 不错，但我们也快发了”：Mark Chen 评价谷歌大模型，讲清 OpenAI 如何给 300 个项目排 GPU 优先级</title><description>&lt;p&gt;12月，OpenAI 首席执行官萨姆·奥特曼宣布拉响「红色警报」，将调配更多内部资源以加速改进 ChatGPT。在当前白热化的 AI 模型竞赛中，作为行业内屈指可数的 “明星企业”，OpenAI 不仅要应对持续升温的人才争夺战、内部组织结构的频繁震荡，还需承接外界对其技术突破的高期待。面对 “开创下一个 AI 技术范式” 的巨大压力，OpenAI将采取怎样的策略破局？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，OpenAI 首席研究官 Mark Chen 在播客节目中，与主持人Ashlee细致分享了OpenAI在推理模型的突破性进展、预训练研究的重新聚焦、GPT-5 Pro已在取得的科学发现。基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;一个组织要成功，需要两个条件：宏大的愿景和与之匹配的天才。成为一个好的领导者，就意味着必须明确地告诉大家：这是优先级，这是我们认为真正推动研究方向的成果，其余的只能排在第二位。未来的科研是“AI + 人类直觉”的组合，会产生新的突破。完全冻结研究部门的新增人头，如果团队想招人，就必须自己决定谁不再适合继续留下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;目标是找到“下一个范式”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：“人才争夺战”最近引发了大量关注，外界普遍认为 Meta 的动作非常激进。你能具体谈谈这种你来我往的竞争现状吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：整个行业的人才池其实很有限，大家都知道最关键的资源之一就是顶尖人才。Meta 的积极挖人并不令人意外，但我们也没有袖手旁观。媒体往往强调“人才单向流向 Meta”，但我看到的情况并非如此。比如在他们从我们团队挖到第一名员工之前，先后接触过我近一半的直接下属，但这些人全部拒绝了他们。当然，如果 Meta 每年能投入约百亿美元用于人才，他们总能挖到一些人。但总体来看，我们很好地保护了核心人才。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;竞争过程中也发生过不少颇具戏剧性的事：扎克伯格曾亲自给我们团队成员送去他亲手熬的汤，以此示好。当时我非常震惊，但后来也理解这类方式确实可能有效。之后我也给从 Meta 挖来的对象送过汤，甚至还想过下次团队外出活动就带大家去上烹饪课。顺便说一句，我自己并不亲自熬汤，米其林餐厅的汤当然比我做得好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但真正让我有信心的是：即使面对 Meta 的高薪挖角，在 OpenAI，无论是来自 Meta 的员工，还是我们原本的研究人员，都没有人认为“AGI 会首先在 Meta 诞生”。他们对 OpenAI 的研究路线都有高度信心。我也一直非常明确告诉团队，我们不会与 Meta 进行“薪资逐美元匹配”的竞争。在远低于 Meta 的薪酬下，关键成员仍然选择留下，这让我更加确信：他们真正相信 OpenAI 的未来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在这种竞争中，有没有类似“博弈策略”的考虑？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：关键在于：目标不是留住组织内的每一个人，而是认清必须保留的核心力量，并确保他们留下来，我们在这点上做得很好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在我看来，Sam 是真正沉浸于研究的那个人，是最顶层的决策者。而你和 Jakub 负责共同制定 OpenAI 的研究方向，同时你还要决定算力如何分配到具体项目上，既要决定公司往哪里走，又要管理执行路径。听起来像是一份非常艰难的工作，因为我想象得到大家会竭尽所能从你那里争取 GPU。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实如此。人们为了获得 GPU，会想尽各种“幕后交易”。但这确实是我职责的重要部分：确定研究优先级，并对最终执行负责。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jakub 和我每隔一两个月会做一次“项目盘点”，梳理一份包含约 300 个项目的大型表格，尽可能深入了解每个项目，并对它们进行排序。对一家约 500 人规模的组织来说，明确“核心优先级”，并通过口头沟通及算力分配来传达，是非常重要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这 300 个项目里既有大型前沿模型，也有各种实验性方向。你们如何管理、追踪并判断哪些项目值得投入 GPU？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：关键在于始终聚焦核心路线图。与其他大实验室不同，OpenAI 始终把“探索性研究”放在最中心的位置。我们并不追求复现别人的成果，也不以追赶他人在基准测试上的成绩为目标。我们的目标是找到“下一个范式”，并愿意投入大量资源。很多人可能会惊讶：我们的算力大头，其实花在“探索”上，而不是训练最终的成品模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所有团队都会说自己的项目最重要、最值得，怎么判断优先级？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：最困难的决策通常是：我们无法在当下为某个项目提供支持。但成为一个好的领导者，就意味着必须明确地告诉大家：这是优先级，这是我们认为真正推动研究方向的成果，其余的只能排在第二位。&lt;/p&gt;&lt;p&gt;Ashlee：你们也强调不要“对竞争者做出反应”。如今 AI 领域的竞争比以往都激烈，你们如何保持独立判断？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：现在的 AI 研究竞争确实空前激烈，但不能陷入这种竞争节奏。你随时可以发布一个小更新，在几周或几个月内领先别人，但这种方式无法长期维持。真正重要的是“破解下一个范式”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如 RO（reasoning optimization）项目，我们早在两年多前就押注语言模型的“思考能力”可以被突破。当时这个方向并不受欢迎，因为大家都觉得预训练和后训练机制运转良好，没必要做别的。但现在，“思考能力”已经变成不可或缺的基础能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的使命就是大胆押注，并构建足够强的算法，使它们能扩展到未来数个量级的算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研究员 vs 工程师&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：随着 OpenAI 成为一家有明确产品线的公司，你们如何不被“商业优先”压过“研究优先”？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：OpenAI 最特别的地方在于：我们仍然是一家“纯粹的 AI 研究公司”，这点在业界非常罕见。我们以非营利形式创立，我加入时公司仍是非营利组织，那时的精神是“全力推进 AGI 研究，并保证安全”。我认为这依然是创造价值的最佳方式：只要研究领先，价值创造自然而然会发生。我 2018 年加入时的那种“核心文化”，至今依然存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：马斯克曾说：“这帮人不是研究员，只是在做工程。”你怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：在构建大模型时，优化每一个百分点、加速每一个 kernel、确保数值稳定，都是极深的工程实践。如果把研究凌驾于工程之上，其实已经输了。一旦缺少工程能力，就无法在当今这种规模的 GPU 上运行模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但外界确实把“研究员”和“工程师”赋予了不同的神秘感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：研究人员形态各异，有的人每天都有无数想法，其中很多并不好，但总能在某个时刻提出改变方向的优秀点子，而有的人特别擅长沿着清晰路径执行。研究从来不是一种单一类型的人能完成的工作，因此也无法简单划分为某种刻板印象。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：当竞争对手发布新模型，你和你们团队会做什么？大家会第一时间去试吗？有没有你们常用来测试新模型的“那一道题”？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：会。以 Gemini 3 为例，它是个不错的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我们内部其实有能力相当的模型，而且快要发布了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Benchmark 只能说明一部分，大家还是会用自己独特的方式去试模型。我个人喜欢用一一个数学题去测，目前还没看到模型完全解出来，就算是“thinking model”也不行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：是秘密题目吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：不算，不过如果我现在说出来可能就会被拿去训练。这是我去年很喜欢的谜题，叫“42 problem”。你要构建一个 mod 42 的随机数生成器，你有的原子操作是一些模 42 以下素数的 RNG，你要让期望调用次数最小。挺可爱的题目，但还没人类语言模型做到最优。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我原本以为你会在对手发布模型当天半夜就冲上去丢题测试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：没有那么夸张。我更相信“长周期”。我们过去半年都在强化预训练能力，把整个团队的肌肉练起来，做出现在能跟 Gemini 3 一较高下的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所以你现在更关注长线构建，而不是每次新品发布就冲去试题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我知道你和 Jakub 都有竞赛背景。我当初第一次见 Jakub 是在 Facebook Hacker Cup。你以前也是数学比赛选手吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，我从小学、高中都在做数学竞赛。不过我真正写代码很晚，是大学室友怂恿的。当时我还有点数学系学生的傲气，觉得数学才是最纯粹的困难学科。后来发现编程竞赛太好玩了，而且是我和大学同学保持联系的方式。我们毕业后每周末都会上线一起比赛，算是朋友间的活动。后来我发现自己还挺有天赋，又开始给美国国家队出题、最后去带队。既是激烈比赛，也是一个紧密社区，大家之后都会在科研界再相遇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那你这么忙，还能当教练？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：其实孩子们本身都特别自驱。教练的作用更多是帮他们管理状态。竞赛很像科研：有好时段、有坏时段，你不能因为连续失败就被心理打倒，很大部分是士气管理。我最近在带模型做竞赛题时也发现，模型的“难度直觉”跟人完全不同，人认为 ad-hoc 的题模型反而容易。这让我更相信未来的科研是“AI + 人类直觉”的组合，会产生新的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：有点像 AlphaGo 的“Move 37”时刻？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的。我觉得 GPT-5 Pro 发布之后，前沿科研有了拐点。发布三天后，一个物理学家朋友把他的最新论文丢进去，模型想了 30 分钟就完全搞懂，他的反应就像见证了围棋的那一刻。而这种事情未来会在数学、生物、材料科学不断出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但当 AI 开始做那些原本属于顶尖人类智力的事，会不会让你觉得有点伤感？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：竞赛是我很喜欢、也曾经擅长的东西，但我也看着模型从普通选手水平爬到超过我，再超过 Jakub，就像亲眼看到自动化的速度快得不可思议。去年模型在 coder 比赛还只是排 100 多名，今年已经能冲进前五。变化太快了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那十年后还会有人类比赛吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：会的，因为它本质上就很有趣。那些只是为了简历而参加的人会消失，但真正热爱的人不会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我采访别人时，他们说有些国家只要 IOI 奖牌就能直接保送大学。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是，但我觉得未来这些考试本身会被 AI 打破。技术面试、大学作业这些已经没法用旧方式评估了。我甚至想未来面试可以让候选人跟 ChatGPT 对话，由一个不会被越狱的特别版 ChatGPT判断他们是否具备在 OpenAI 工作的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你家里有很强的科技背景，你父母都在 Bell Labs，对你影响很大吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我从小吃饭桌上就是各种科学谜题。后来搬到西岸，我爸做创业，让我看到初创公司的另一面。再搬到台湾读书，又是完全不同的文化，纪律性更强。各种经历混在一起，形成了今天的我。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你 MIT 那届是名人辈出的超级年份吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是，2012 年那一年特别厉害。Jacob Steinhardt、Paul Christiano，还有后来 AI 领域很多重要的人物都在那一届。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你也通过竞赛认识了 Cognition 的 Scott Wu，那些在 X 上被当成数学 meme 的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，我们就在竞赛社区认识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你从MIT毕业后，直接去了华尔街。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：老实说，我对在华尔街做高频交易并没有太多自豪感。当时在 MIT，许多擅长量化的学生都会选择类似的道路。这份工作确实很“绩效导向”，只要足够聪明，你就能获得对应的收益。然而文化上我并不适应。在那种环境里，当你发现了什么突破，第一反应是把知识藏好，因为知识本身就是你的价值来源。这造成团队内部竞争激烈、彼此不够信任。整个行业也像一个封闭的生态系统：即便某家 HFT 公司的算法快了一点，外界其实几乎没有任何感受。我做了四五年后发现，我们始终在跟同一批对手竞争，大家都稍微变快了，但世界并没有因此改变多少，我觉得是时候做点别的事了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时 AlphaGo 的比赛对我触动很大。虽然我并不下围棋，但看到模型展现出的创造性，我特别想弄明白背后的原理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所以你是看到了那场比赛后，才开始关注 AI？当时你有在读论文吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦白讲，没有。直到 AlphaGo 之后我才开始深入研究 AI。我的第一个目标就是复现 DQN 的结果，复现一个能在 Atari 游戏中达到超人水平的网络，那基本就是我踏入 AI 的起点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你是在上班后业余时间做这些吗？我记得我大概 2018 年采访 George Hotz，他在自家车库做自动驾驶。他当时说，AI 仍然很年轻，只要读 10 到 30 篇论文，就能掌握整个领域。当然他的话未必完全准确，但 AI 的确很特别：历史很长，但此刻却异常“浅”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实非常“浅”。我常建议对 AI 望而却步的人：只要花三到六个月做一个项目，比如复现 DQN，就能很快触达前沿。过去几年虽然增加了一些深度，但远没有理论数学或物理那么深奥。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你觉得 AI 会像数学一样，天才都在二十几岁出现突破吗？还是这是一个可以做一辈子的领域？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为完全可以持续做下去。OpenAI 的文化确实偏年轻，但做好研究并不需要年轻。年轻人确实因为“先验少”，更容易突破传统路径，但随着经验增长，你也会形成自己的视角和框架，这既是优势，有时也会让你更固化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI的内部故事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你在 2018 年加入 OpenAI，那时公司应该只有50人左右？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：差不多 20 人而已。我是以“研究员 resident”的身份加入的，也就是 OpenAI 会从其他行业招人进来集中训练半年，像压缩版 PhD，然后再参与更深入的研究项目。我很幸运能向 Ilya 学习，他基本决定了我的项目、学习路径和方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但如果去 LinkedIn 看，你的第一份 OpenAI 的头衔看起来像是“前沿研究主管”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：并不是，我做了三年左右的个人研究（IC）。当时我主要研究生成式模型，因为那是 Ilya 最关注的方向。之后我才开始带团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：公众最早看到的大项目可能是 DALL·E，对吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的。其实在那之前，我最自豪的项目之一是 Image GPT。它证明了 Transformer 不止能处理文本，也能在图像上学到强大的表示能力，是 DALL·E 的前身。而另外一个我非常自豪的项目是 Codex，我们搭建了代码模型的评估体系，也探索了如何让语言模型在代码任务上达到高水平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那你当初为什么选 OpenAI？是因为当时这个小公司里有很多有意思的人吗？没钱、没人、前景很不确定，居然要挑战 Google 这种巨头。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我觉得一个组织要成功，需要两个条件：宏大的愿景和与之匹配的天才。当时 OpenAI 两者兼具，这非常罕见。而且我认识 Greg，我们以前参加过数学竞赛，我给他发消息说：“我不确定自己是否适合，但这里似乎在做重要的事情。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你从外部加入，然后现在成为研究负责人，这听起来还是很不可思议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对我来说也很不真实。从 IC 转管理者，我其实非常犹豫。不过一路上我遇到的管理者都非常支持我，他们看到了我的潜力，会主动为我争取机会。我从没主动要求升职，每次都是自然而然的结果。管理这件事，本质上主要靠经验累积，而 OpenAI 是一个能让你不断获得“经验值”的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我认识的你是一个温和、稳重的人。但 OpenAI 过去几年经历了很多戏剧性的风波，甚至像“权力的游戏”。你要在这种环境里做管理，这和你的性格几乎相反。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：老实说我在 OpenAI 算是很幸运。一路上都有人支持我、给我建议，也在关键时刻为我发声。这些帮助让我能持续成长、建立信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：不过你在“政变事件”那段时间做了两件很重要的事：你先帮助研究员们统一意见、促成那封让Sam回归的请愿信。然后一两天之后，你在Chelsea家做了一次很重要的短讲。这两个瞬间对我而言都很震撼，在危机时刻挺身而出、凝聚团队……这对你意味着什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对我而言，那确实是一个关键时刻。“风波”后的几天里，整个团队都处在高度不确定中。那段时间，我、Nick 和 Barrett 都感到一种责任感：竞争实验室正不断向我们的研究人员打电话，试图把他们挖走。我当时给自己设下目标：不能失去任何一个人。最终我们也做到了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那几天，我们每天都把自己的家打开，让同事随时过来，释放焦虑，同时保持他们与领导层的沟通渠道畅通，让大家知道自己仍然能发挥作用。渐渐地，团队形成了一种“我们一起面对外界”的精神，大家都在思考：如何向世界传达“我们仍然站在一起”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我在几处房子之间来回协调，我们提出了组建请愿书的想法，表达我们支持 Sam 的立场。大概凌晨两点，这个想法最终确定下来。到第二天早上，研究团队已有 90% 以上的人签署，到最后接近一百人都签了。那一整晚，大家都在互相打电话确认：“你参加吗？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你当时的处境应该挺尴尬的吧？毕竟一开始似乎是 Ilia 和 Sam 立场对立，而 Ilia 又是你的导师。后来 Ilia 又回来了……那会不会让你很尴尬？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：不会说尴尬，但确实很困难，因为那是个信息极少的环境。那时候确实很容易怀疑：Sam 到底做了什么？但换个角度想，如果真有严重问题，Greg 和 Jakub 这种极其正直的人会因此辞职吗？我觉得肯定有部分事实被误解了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：Jakub 在那里工作很久了。关于他，有什么是外界不了解的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：他其实非常幽默，带着强烈的讽刺感，我常常被他逗得发笑。和他共事让我最珍惜的一点，是我们之间高度的默契。进会议室后，我们能迅速碰撞出一致的结论，然后分别负责路线图的不同部分。&lt;/p&gt;&lt;p&gt;说到“把团队留在一起”，我现在仍有这种使命感。我认为我们仍然“被攻击着”，任何公司想要招人时，第一选择往往是从 OpenAI 下手，因为他们想要我们的专业能力、愿景和世界观。OpenAI 造就了今天 AI 领域最多的明星研究员，因此我们对团队有强烈的保护欲。只要有人来挖，我就会尽一切努力确保团队感到被重视、被理解，并清楚自己在整个路线图中的位置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在写书、回顾历史的过程中，我一直在想：这是否是一个高度依赖“天才个体”的领域？从 2012 年 Ilia 的突破，到 2017 年 Transformer，再到 Alec Radford……似乎每隔几年就有那么 8–10 个关键人物在推动整个领域。如果他们离开了，比如 John Schulman、Alec 离开了，那对团队不是巨大损失吗？但你们之后仍然在推理和其他方向取得了突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我并不同意“完全依赖明星个体”这种说法。OpenAI 的确会从上层做方向性押注，但我们内部有非常深厚的自下而上文化，很多好点子来自意想不到的地方。看到这些想法成长、成形、被扩展，是非常美妙的事，推理方向就是典型例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但行业确实会花大价钱去挖“明星”，比如 Google 花巨资请回Noam Shazeer。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：当然，人才既有培养也有争夺。反过来，我从 Meta 学到的一点就是：OpenAI 自己也可以非常积极地争取顶尖人才，我自己也从他们那套激进的招聘策略中学了几招。归根结底，我们的目标始终是：组建一支最强的团队，完成我们要实现的使命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这个圈子其实很小，你们虽然竞争激烈，但私下也都是朋友。那边做研究，这边又试图挖对方的人，这不是很微妙吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：这是残酷竞争的行业，但我个人也非常享受竞争。我讨厌失败，因此无论是研究还是招聘，我都会全力以赴。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这让我想到半导体行业早期也是这样：工程师们不断突破物理极限，在酒吧里分享最新发现，同时又被各家疯狂挖角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，任何行业都会有“知识扩散”的基本速率。而公司可以有两种反应：一种是建立深度信息隔离层，严密保护一切；另一种是继续保持开放文化，用速度压制对手。OpenAI明显是第二种，我们不认为封闭是正确方式。我们的方法是跑得比别人更快。我们鼓励研究人员自由分享想法，这才是最快的前进方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那现在你、Sam 和 Jakub 之间的合作方式是怎样的？大家都能看出来 Sam 更偏研究，而你们两位更深度参与技术细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们三个人联系非常紧密，我每天都会和他们交流。Sam 热爱研究，也热爱了解研究。他能从研究人员那里捕捉“团队脉搏”，比如潜在问题、工作环境中的隐形障碍，他能帮我把这些提前揪出来。Jakub 和我则更专注于如何设计组织、让团队以最高效率协作，比如如何安排座位布局、如何组建互补的小组、如何引导大家关注我们认为重要的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：Sam 平时看论文、和你们聊天吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，他会看论文，也会经常与研究人员交流，理解他们的研究方式。当然，他还负责范围远超研究的事务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI 到底发现了什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我知道你们最近在预训练方面似乎有了重大突破，也明显比之前更有信心，能透露一下你们到底发现了什么吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我对过去两年的总体观察是：我们把大量资源投入到“推理”这一能力的研究上，努力理解并打磨这个核心原语，这条路确实走通了。但副作用是，模型的其他重要环节，特别是预训练和后训练，相对失去了些“肌肉”。过去六个月里，Jakub 和我花了很多时间把这部分能力重新练起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我一直把预训练看作一种“肌肉”，必须持续锻炼：信息要保持最新，团队要在优化、数值计算等前沿方向持续投入，同时也要确保有足够的心智关注度。所以我最近一个重要工作，就是引导公司内部的讨论重回预训练，我们认为预训练还有极大空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;外界有人说“Scaling已死”，但我们完全不认同。某种意义上，行业现在把注意力集中在 RL，这反而给了我们“信息优势”，因为我们看到预训练还有巨大的未开发潜力。得益于这套新努力，我们最近训练出的模型明显更强，这也让我们对包括 Gemini 3 在内的接下来一系列发布更有信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我脑中对这段历史的画面是这样的：你们跑得太快了，整个领域也跑得太快。突然之间，我们从互联网收集到巨量资料，把它扔进一台超级计算机，于是 ChatGPT 诞生了，然后所有人就开始疯狂冲刺。但对于不紧密跟进的人来说，问题可能是：最初那波数据其实非常粗糙，只是稍微清洗了一下就丢给模型。而现在你们说在“学习更高效地塑造数据”，但外界很难理解到底之前的“错误”是什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：你触及了我最近一直在思考的问题。预训练本质上是在用人类写下的内容教模型模仿人的表达方式，模型学会了人类写作的结构和模式。但这种模仿式学习天然设定了上限：当你模仿人类时，你很难真正超越人类。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么 RL 重要，它让我们有机会把模型推向更难的任务，让它从人类范式之外思考，拓展能力边界。但随之而来的，是一个更困难的问题：如果我们要让模型真正超越人类，该怎么衡量？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在科学领域，当能力达到了“超人类”水平，人类真的能够判断 A 比 B 强吗？如何判断一个“超人类数学家”比另一个更厉害？我们需要更好的评估体系。迄今为止，我们很幸运，IMO、IOI 等竞赛提供了一种衡量“世界最强人类”的方法。但当模型超过人类，这些测验本身就失效了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我常看到那些竞赛牛娃后来进 Google、Facebook，但他们不一定是最顶尖的工程师，也不一定愿意或适合进入工业界。所以单纯在竞赛上拔尖并不等于就是“最强工程师”。那如果未来 AI 在这些竞赛上表现极佳，我们到底能从中学到什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：这正是我喜欢 AI 研究的地方，它比传统工程更接近真正的“技术能力的精英制度”。我反复学到的一点是：你无法让一个研究者不尊重的人来带领他们。研究团队的领导必须做出艰难且正确的技术判断，例如路线选择、资源配置、项目方向。如果判断错误，很快就会失去团队的信任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我很享受与这样一群极度技术驱动的人共事，他们都深度投入、极高水准，与他们讨论技术本身，是一件真正的乐趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在我心里，Transformer 是一次巨大飞跃，而“推理能力”的突破甚至可能更惊人。最近与你、Greg、Jakub、Sam 交流时，我感觉你们说过去三到五年投入的大量工程工作，其实还没有完全显现出来。你们现在看到的，是另一场类似 Transformer 的跃迁吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为是的。比如在 GPT-5 时，我们谈到了大量关于“合成数据”的内容。还有许多类似的方向都显示了很强潜力，我们正在快速扩大投入。关键仍是维持一组多样化的探索，把最有实证价值的方向加大力度推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但两周前，Karpathy 在播客上说 AGI 可能要十年；上周 Dario 又说更接近两年。行业内部声音完全不一致。你怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：Twitter 很喜欢那种“结束了！”“又回来了！”的戏剧化循环。但 AGI 本身连定义都不统一，在 OpenAI 内部，你把所有人叫到一个房间，也不可能给出一个完全一致的 AGI 定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我更把它类比成工业革命：你说纺织机是工业革命，还是蒸汽机是？视角不同，切点也不同。对我而言，我更看重的是：模型是否开始产出真正新的科学知识？是否推动科学前沿？从今年夏天以来，我感觉在这方面出现了巨大的相变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你说的新科学成果，是不是指最近那些生物科技初创公司，比如一次性设计抗体、分子结构那类突破？还是你指的另有其事？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：那次与物理学家的交流给了我很大启发，我回去后就想，我们应该创建一个“OpenAI for Science”。目标是让目前那小部分真正意识到模型潜力、愿意投入并加速研究的科学家，能够获得最大程度的支持。我知道其他公司也在推动科学前沿，但我们和谷歌等机构的不同之处在于：我们希望让所有科学家都有机会借助工具做出诺奖级突破，而不是让 OpenAI 自己拿诺奖。我们要构建的是通用的工具与框架，让科学界整体都能加速。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你能具体说说有哪些让你兴奋的发现吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：当然。你可以去看 Sebastian 的推特，他最近发了关于 GPT-5 在一个开放凸优化问题上取得进展的论文，这与我们正在研究的一些核心机器学习问题密切相关。有些人会把这些成就简单理解成“更厉害的文献检索”，但远比这复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这两天听到有人声称“我们做出了 AI 科学家”“我们一次性设计出增强型蛋白质”，这些公司里不少是真正的科学家，我也多少会兴奋。但数量实在太多，我很难判断哪些是真正的突破、哪些只是噪音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：如果这些突破发生在生物领域，我一点也不意外。尽管我主要的专业在计算机科学和数学，但我们团队里有顶级专家，他们确认了不少是真正的科学发现，生物学里出现类似情况并不令人惊讶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你描述的情况似乎与最近几周不断变化的公众叙事不同。比如一些播客里的人会说 AI 没什么进展，都是虚幻的。如果这些发现是真的，公众应该会感受到变化才对。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们在筹建 OpenAI for Science 时与许多物理学家和数学家交流过，其中大多数人对 AI 其实并不乐观，他们觉得模型不可能证明新定理。但正因为如此，我们更希望扶持那一小批愿意相信并深入使用模型的人。他们会跑得比所有人都快，我们希望为他们提供工具，也希望说服更多研究者：这是未来科学研究的正确方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：每个人对 AGI 的定义不同，但你似乎认为未来一两年会发生非常剧烈的变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：“AGI 两年后到来”一直是个梗，但我觉得我们已经不在那个戏谑阶段了。是数学和科学领域不断出现的结果，让我真正产生了信念。在 OpenAI 内部，我们设定了两个非常具体的目标：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，1 年内改变研究方式：让研究过程可以依赖 AI 实习生。也就是：研究者负责提出想法，模型负责实现、编写代码、调试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，2.5 年内让 AI 能进行端到端研究。这意味着：研究者只确定方向，模型完成从构思到执行到验证的全过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与今天相比，这是完全不同的研究范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;算力、GPU与AI硬件&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在与 OpenAI 的人聊时，我常听到一句话——基础设施扩张得很快，模型只要算力再提升 10 倍就会变得更好。但也有人说从 GPT-4 到 GPT-5，你们算力增加了，却没有看到预期的效果。可你们的叙述又让我觉得：其实我们还没真正看到“10 倍算力飞跃”带来的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：有人问我：“你们真的需要这么多算力吗？”我总是被这个问题震惊，因为我每天面对的都是海量算力需求。如果我们今天多 3 倍算力，我能立刻高效用完；如果多 10 倍，大概几周内就能全部吃满。所以算力需求是真实、巨大、并且没有放缓迹象的。有人质疑“你们真的需要更多 GPU 吗？”对我来说毫无意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那除了算力需求，你们对模型规模继续扩大是否同样乐观？你们是否看到，类似“规模效应”会再次推动巨大跃升？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，我们非常明确要继续扩大模型规模；而且我们有突破性的算法能支持更有效地扩展。我认为 Gemini 3 也很令人印象深刻，但从细节看，比如 SWE-bench 等指标，他们在数据效率方面仍没有重大突破，而这是我们非常强的部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我看到了一份泄露的备忘录，Sam 对 Gemini 3 的语气听起来相当严肃，仿佛是一个转折点。你们内部应该都看过吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，但你要知道，Sam 的工作之一就是不断在组织里注入紧迫感，我也一样。我们必须保持专注，加快节奏。Gemini 3 是谷歌该做的正确押注，但与此同时，我们也有明确的策略与回应，并且我们有信心执行得更快。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你们会参与像 Jony Ive 的 AI 设备这样的项目吗？比如研究团队在其中扮演怎样的角色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，事实上，就在昨天我和 Jony Ive 以及几位研究负责人一起吃了晚饭。我一直在思考未来的 ChatGPT 会是什么样子。现在的交互方式对我来说还很“笨”，非常非思维原生：你给一个提示，它回答；你不提示，它就停止思考。而且如果你再给出类似的问题，它会重新花一样多的时间推理，仿佛没有从第一次的上下文中变得更聪明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来显然应该不同。记忆会是核心能力：每次你使用 ChatGPT，它都会学到关于你的更深层次信息，思考你为什么会问这个问题、你之前问过什么、你接下来可能需要什么。下一次你来，它会变得更好。我认为这会彻底改变“设备”的范式，因此我们必须思考：如果 AI 的主导逻辑是持续学习与反思，那硬件设备应该怎么重新设计？ 这就是和 Jony 合作非常有价值的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你们已经有设备原型了吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我不能说有没有，也许有，也许没有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我想到的是：苹果时代的核心是“硬件品味”，这是 Steve Jobs 极度执着的东西。而你们似乎都没有真正做过硬件产品。Sam 的审美看得出来不错，但还没到“乔布斯式品味”的程度。硬件是极其依赖品味的，你们怎么确定自己能做出好产品？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦白说，我们不需要自己拥有那种品味，那是 Jony 的价值，他就是我们关于“品味”的判别器。而且很有趣的是，我们发现设计流程与 AI 研究流程之间有深刻的相似性：大量探索与假设、不断迭代、收敛成一个最终满意的成果。现在双方的融合非常顺畅：他们根据我们即将发布的能力去思考外形，我们根据他们的外形需求去思考能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我有时会担心：一群数学与模型天才是不是适合造“下一代电脑”。但听你这么说，似乎你们形成了一个合理的搭配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实，打造 AI 能力的人和拥有“美学品味”的人往往不是同一类。但我们内部其实有一些团队非常擅长判断“模型行为的品味”。比如有一种经典的测试题：“ChatGPT 最喜欢的数字应该是什么？”这种问题能检验模型的“人格品味”一致性。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最后的问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：ChatGPT 建议我问你：如果五年后回看，现在有哪些“还很脆弱”的想法，你直觉认为可能是大突破的核心？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实有几个，我非常期待把它们规模化。主要集中在预训练，一些在 RL，还有一些是如何把所有组件整合在一起的整体性想法。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你觉得现在外界对 OpenAI 最大的误解是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：最重要的一点：OpenAI 从上到下都是一个“研究中心化”的组织。我们的核心赌注永远是 AGI，其他所有产品都会自然从研究突破中流出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们关心三件事：自动化 AI 研究本身、自动化科学发现、自动化经济性工作。今年最大的更新，其实是第二条：科学研究的自动化开始真实发生了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你几岁了？还有社交生活吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：34，快 35。老实说，没有什么社交生活。最近两周每天都是工作到凌晨一两点。但我热爱这样做。我们招人、推进研究、做关键决策。如果我们正站在类似工业革命的巨大转折点，那就必须抓住它。Barret离开去创业之后，我在办公室睡了一个月。那段时间我非常强烈地感到：我必须保护研究，这是我最在乎的东西。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：DeepSeek 事件之后，你们怎么看开源模型？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：那是第一次让我深刻意识到：必须坚定走自己的研究路线。DeepSeek 当时引发巨大舆论，大家都在问：“OpenAI 落后了吗？要怎么回应？” 但我们做得最正确的一件事，就是继续执行自己的研究规划。DeepSeek 的工作非常强，但主要是对我们 O 系列理念的复刻。关键是，我们必须继续创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你认为 500 人是一个最优规模吗？随着公司扩大，这个数字会增长，还是说为了同时推进若干重大想法，500 人已经是最合适的规模？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦率说，我认为甚至可以更少。尤其在我们开始引入 AI 研究员或 AI 实习生之后，我们必须重新思考团队结构。我非常在意“高密度人才”。例如今年第二季度，我做过一个实验：完全冻结研究部门的新增人头。如果团队想招人，就必须自己决定谁不再适合继续留下。我认为这种做法能防止组织失控膨胀，并保持极高的能力标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我记得之前在一次会议上，你和 Jakub 的观点比较一致：你们认为大家过度关注“谁在项目里获得署名”这个问题。AI 起源于学术界，在学术环境中署名极其重要。但那次会议里，你似乎在强调：大家可能对这个问题有点太执着了。是这样吗？是不是因为现在 OpenAI 已进入新的阶段，在公司环境下，这件事不再那么重要？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为过度关注“功劳归属”是件坏事。但另一方面，我又认为公司必须在内部与外部都正确地给予功劳。很多公司已经逐渐远离论文署名制度，但 Jakub 和我最终决定OpenAI 必须保留署名。反对意见常常是：“你们把顶尖人才的名字摆在台面上，其他公司会更疯狂地挖角。”但我认为这不重要。出色的人就应该被看到，我们应该持续培养 AI 领域的明星研究者，也应该让真正做出贡献的人建立起自己的声望。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你似乎又同时认为，研究员个人不应该过分执着于署名了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：现场确实有人表达过那种观点，但其实 Jakub 和我对这个问题持不同意见。我们俩更坚持应当在可能的情况下给予功劳，哪怕这意味着外界能清楚知道我们最优秀的人是谁。我甚至会再进一步说：OpenAI 可能是整个行业里，最愿意给研究者公开署名的公司，没有之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你 2018 年加入时，OpenAI 还是一个研究导向、非营利的组织，创始人希望它成为 Google 的平衡力量，并以“确保 AGI 安全到来”为目标。而你来自华尔街高频交易，只是被 AI 的进展吸引过来。说实话，你并不“必须”对 AGI 的哲学问题深怀使命感。那你究竟为什么要做这件事？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我同时管理 OpenAI 的对齐团队。坦白说，未来一两年最重大的难题，就是对齐问题。在这个研究方向上，OpenAI 在过去一年做出的成果可能是整个领域里最好的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因之一是：在RL与算力增加后，我们开始能测量模型的自我意识、自我保护倾向、甚至可能的“Scheming”行为。这非常危险，因为模型最终给你的答案可能是“正确的”，但它得到答案的过程却完全偏离我们能接受的路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着模型替我们执行的任务越来越复杂，理解它的思维过程将变得极其关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这和机械可解释性有关，也就是试图理解模型内部机制的问题。核心问题是：我们的理解能力能否跟得上模型复杂性的提升？还是会最终被模型甩得太远？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们在发布 O1 时做了一个关键决策：我们不监督模型的思维过程。一旦你要求模型给出“看起来让人类舒服的思考过程”，它就可能开始伪装自己的真实意图。因为坚持不监督、不过度干预，我们仍然能“看到”模型真实的思维轨迹，并将其作为研究对齐的重要工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几个月前，我们与 DeepMind、Anthropic 合作发表了一篇论文，探讨未来如何通过这种方式理解模型。我确实担心未来某一天，模型给出非常有说服力的答案，但我们无法确认它是否真正与人类的价值一致。&lt;/p&gt;&lt;p&gt;因此有很多值得探索的方向，例如：能否设计一种博弈或环境，让模型在互相监督、共同演化的过程中，唯一稳定的均衡，就是“诚实”？我认为这里还有大量非常重要的研究要做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：https://www.youtube.com/watch?v=ZeyHBM2Y5_4&amp;amp;t=9s&lt;/p&gt;</description><link>https://www.infoq.cn/article/27qcglz0p9bX17etWNQg</link><guid isPermaLink="false">https://www.infoq.cn/article/27qcglz0p9bX17etWNQg</guid><pubDate>Wed, 14 Jan 2026 08:47:47 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>Cloudflare通过左移安全实践扩展基础设施即代码</title><description>&lt;p&gt;Cloudflare通过&lt;a href=&quot;https://blog.cloudflare.com/shift-left-enterprise-scale/&quot;&gt;实施&lt;/a&gt;&quot;基础设施即代码和自动化策略执行，消除了数百个生产账户中的手动配置错误，每天处理大约30个合并请求，并在部署前而不是事件发生后捕捉安全违规。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司的Customer Zero团队面临一个关键问题：单一配置错误可能在几秒钟内传播到Cloudflare的全球边缘，可能会导致员工被锁定或生产服务瘫痪。在这种规模下，对数百个账户进行手动仪表板管理为人为错误创造了太多机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该解决方案的核心是将所有基础设施配置视为代码，进行强制性的同行评审和自动化安全检查。现在，每个生产变更都要经过一个验证管道，该管道在部署前执行大约50个安全策略。团队仍然使用仪表板进行分析和可观测性，但关键的生产变更需要提交与用户、工单和自动化合规性检查相关联的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Cloudflare团队的Chase Catelli、Ryan Pesek和Derek Pitts的说法，这种左移方法将安全验证转移到开发的早期阶段，在补救成本最低时捕捉问题。该模型防止事件发生，而不是对事件作出响应，同时通过让团队相信他们的变更是合规的，从而实际上提高了工程速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实施以&lt;a href=&quot;https://developer.hashicorp.com/terraform&quot;&gt;Terraform&lt;/a&gt;&quot;和&lt;a href=&quot;https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs&quot;&gt;Cloudflare Terraform Provider&lt;/a&gt;&quot;为中心，集成到一个自定义的持续集成和部署管道中，该管道在&lt;a href=&quot;https://www.runatlantis.io/&quot;&gt;Atlantis&lt;/a&gt;&quot;上运行并与&lt;a href=&quot;https://about.gitlab.com/&quot;&gt;GitLab&lt;/a&gt;&quot;集成。所有生产账户配置都存储在一个集中的单体存储库中，各个团队作为指定的代码所有者拥有和部署他们的特定部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c4/c40cdab3e995277240a775da050ff294.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare的基础设施即代码数据流图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个名为tfstate-butler的自定义Go程序充当Terraform的HTTP后端，充当安全状态文件代理。该设计通过确保每个状态文件的唯一加密密钥来优先考虑安全性，从而限制了任何妥协的潜在爆炸半径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;策略执行使用&lt;a href=&quot;https://www.openpolicyagent.org/&quot;&gt;Open Policy Agent&lt;/a&gt;&quot;框架和&lt;a href=&quot;https://www.openpolicyagent.org/docs/policy-language&quot;&gt;Rego&lt;/a&gt;&quot;语言来验证安全要求。策略在每个合并请求上自动运行，并以两种模式运行：允许部署并带有评论的警告，或者完全阻止变更的拒绝。异常处理需要基于Jira的正式批准，然后是一个拉取请求来记录偏差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移揭示了扩展基础设施扩展即代码（&lt;a href=&quot;https://www.hashicorp.com/en/resources/what-is-infrastructure-as-code&quot;&gt;Infrastructure as Code&lt;/a&gt;&quot;）的关键教训。最初，由于团队之间的Terraform熟练程度不同，进入门槛很高，阻碍了最初的采用。cf-terraforming命令行实用程序，它自动从Cloudflare API生成Terraform代码，通过消除手动资源导入，显著加速了上手速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当团队在事件期间进行紧急仪表板变更时，配置漂移就会出现，从而使Terraform状态与部署配置不同步。Cloudflare实施了自动漂移检测，该检测连续比较状态文件与部署配置，并在检测到差异时自动创建具有服务级别协议的补救工单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare Terraform Provider落后于API能力，因为Cloudflare的快速产品创新速度超过了Terraform的支持。v5提供者版本通过从OpenAPI规范自动生成代码，解决了这个问题，保持了产品API和基础设施代码能力之间的持续对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;左移模型展示了组织如何在保持严格的安全治理的同时扩展基础设施即代码。通过将验证从反应性审计转移到主动自动化检查，Cloudflare既提高了安全性，又提高了工程速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多公司正在采用左移方法。谷歌云&lt;a href=&quot;https://cloud.google.com/blog/products/identity-security/shift-left-on-google-cloud-security-invest-now-save-later&quot;&gt;指出&lt;/a&gt;&quot;，在生产中定位安全问题可能导致重大的财务处罚，例如高达全球收入4%的GDPR罚款。通过自动化CI/CD安全检查进行早期检测可以大大降低补救成本，减少对架构更改的需求。OpsMx&lt;a href=&quot;https://www.opsmx.com/blog/shift-left-security-implementation/&quot;&gt;指出&lt;/a&gt;&quot;了实施障碍、自动化差距、复杂工具和组织孤岛等挑战，同时强调使用NIST和OWASP等框架的自动化策略执行可以帮助团队识别和优先考虑风险，而不会给开发人员带来负担。根据&lt;a href=&quot;https://www.splunk.com/en_us/blog/learn/shift-left-security.html&quot;&gt;Splunk&lt;/a&gt;&quot;的研究，73%的公司认为缺乏自动化是他们在左移实践中的主要挑战，但AI驱动的工具正在通过智能自动化迅速改进安全测试，采用率在短短一年内从64%体升到78%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;左移运动已经超越了简单地将安全检查提前。组织现在正在追求通过自动化扫描（&lt;a href=&quot;https://www.checkpoint.com/it/cyber-hub/cloud-security/what-is-static-application-security-testing-sast/&quot;&gt;SAST&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.blackduck.com/glossary/what-is-software-composition-analysis.html&quot;&gt;SCA&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_application_security_testing&quot;&gt;DAST&lt;/a&gt;&quot;、秘密管理）、策略即代码执行和AI驱动的漏洞优先级排序进行持续的安全验证，在现有的工作流程中为开发人员提供即时、可操作的反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-security-shift-left/&quot;&gt;https://www.infoq.com/news/2026/01/cloudflare-security-shift-left/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/eNFdj9rUSIFPPl3J9Qgr</link><guid isPermaLink="false">https://www.infoq.cn/article/eNFdj9rUSIFPPl3J9Qgr</guid><pubDate>Wed, 14 Jan 2026 07:03:00 GMT</pubDate><author>Tim Anderson</author><category>生成式 AI</category><category>安全</category></item><item><title>直播预告：新瓶旧酒还是涅槃重生？操作系统的 AI 进化终将走向何方？|《AI 进化论》第八期</title><description>&lt;p&gt;在 AI 与本土化双重浪潮之下，服务器操作系统正迎来历史性变革。由龙蜥社区理事长单位阿里云联合 InfoQ 打造的直播 IP 栏目《AI 进化论：智算时代操作系统的破局之路》，以云、AI、安全等技术与服务器操作系统如何融合演进为主线，聚焦服务器操作系统在智算时代的进化之路，特邀学术权威、行业专家、客户代表围绕原生智能、原生安全、软硬协同等热点议题展开深度对话。截至目前，已直播七期，线上观看人次达 60 万+。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 浪潮引发基础设施革命，服务器操作系统也正在迈入“(Cloud+OS)xAI”多维赋能的全新阶段。从国内外主流 OS 的差异化演进、阿里云 Alibaba Cloud Linux 4 的内核突破与性能跃升，到 “GPU 时代”的内核争议；从 OS-Copilot 的升级赋能，到 RISC-V 异构算力适配的前沿探索，操作系统的 “涅槃重生” 需要跨越哪些技术鸿沟？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《AI 进化论：智算时代操作系统的破局之路》系列直播第八期将于&amp;nbsp;1 月 22 日 14:00&amp;nbsp;开始，特别邀请到，阿里云智能集团总监、龙蜥技术委员会主席杨勇，中国科学院软件研究所高级工程师、RISC-V 行业生态负责人郭松柳，InfoQ 极客传媒策划编辑凌敏三位嘉宾，聚焦从业者核心困惑，结合龙蜥社区理事长单位阿里云 AI 增强套件、百万级服务器运维实践与 RISC-V 适配经验，深度拆解 AI 时代操系统的技术重构与价值重塑！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多直播亮点，可点击下方海报了解，欢迎大家打开微信，扫描二维码预约直播：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/40/40fa9f217d7e517813f953f26de25eae.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;—— 完 ——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</link><guid isPermaLink="false">https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</guid><pubDate>Wed, 14 Jan 2026 03:14:30 GMT</pubDate><author>阿里云</author><category>AI&amp;大模型</category><category>操作系统</category></item><item><title>待到山花烂漫时：鸿蒙开发者 用代码灌溉鸿蒙花园</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用代码浇灌春天，最终必将见证万紫千红的生态盛景。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;她说：“我愿在这群芳争艳的时代，绽放一抹‘吉祥’红！”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的 “金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X 框架迁移及原子化服务开发等技术突破，历时21个自然日完成 HarmonyOS NEXT 核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配HarmonyOS NEXT的故事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自22年加入吉林银行以来，卢妍娆便先后投入到了新一代核心系统建设以及吉林银行手机银行6.0迭代建设。23年年末吉林银行对应用鸿蒙化表示明确认可，认为鸿蒙生态适配不仅仅是吉林银行构建数字金融护城河的战略突破口，更是实现技术自主可控的关键战役，如春潮涌动时抢占滩头的先锋。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们非常期待能在HarmonyOS NEXT这个种满花卉的生态里，迅速绽放并共同成长，掌握一定的话语权。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在“打仗”之前，吉林银行研发团队完成了鸿蒙开发的学习，并于2024年2月与华为达成鸿蒙适配的合作意向。“华为为我们提供了技术上的答疑指导，帮助我们打通开发道路，让后面的开发更加便利。”万事俱备只欠东风，2024年5月底立项申请通过，项目正式启动，基于手机银行6.0功能及性能提升后的框架，6月18日正式上架核心交易功能版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c86c07962f19e5db24e0bbbf7a62a405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;卢妍娆在HDD活动照片&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“HarmonyOS NEXT跟安卓不一样，是个全新的系统，也是全新的体验”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;卢妍娆最初担心，吉林银行App适配鸿蒙的时候会很困难，因为原有的代码架构需要大规模重构。在鸿蒙声明式开发里，UI 是通过声明式语法描述的，需要重新编写大量的 UI 代码。事实上，开发过程真的很艰难吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“遇到技术难题的时候，你可以直接提出问题，鸿蒙的官方技术人员会回复，甚至提供样例代码手把手帮你解决问题。例如，我们开发团队在遇到微信分享无法获取uicontext，自定义弹窗无法展示的问题时，华为团队提供了示例代码解决问题；由于医保缴费框架存在中断逻辑，导致页面存在多次跳转，华为团队根据每次ID的不同，提供样例代码规避了消费者界面多次跳转的问题；开发语音识别功能的时候，我们团队没有足够的经验，华为技术人员提供了语音识别代码Demo以及UI代码，帮助我们快速实现语音识别功能。”卢妍娆回忆道。相比安卓开发中依赖第三方论坛的“投石问路”，鸿蒙的这种开发者帮扶模式更高效更贴心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea809d09e8659a816a8eef3713be9383.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;应用适配鸿蒙生态架构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;HarmonyOS SDK接入：纯净之境，开启开发新篇章&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们的手机银行集成第三方SDK有18个，HarmonyOS SDK替代了部分，不仅协同加速，提升了我们开发的效率，还为我们节省了大量成本。” 卢妍娆跟我介绍她们的应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统SDK在架构设计上往往存在冗余和复杂的问题，在接入时会引入大量不必要的代码和依赖库。而HarmonyOS SDK采用的原子化服务架构，将功能拆解为最小可复用单元，使用起来就像搭建积木一样，我们可以根据需求灵活选择和组合这些原子化服务。这种模块化设计使得代码更加简洁、清晰，如同月光下的水晶棱镜，每一个模块都剔透纯净。以一个简单的天气卡片组件为例，在HarmonyOS SDK中，开发者可以通过简洁的代码实现其功能，非常高效简洁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/49965718a3998fc12793e6a4f47f93bd.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;小组开会研讨方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&quot;HarmonyOS NEXT不是简单的系统升级，而是给开发者重新定义了工具类应用的魔法棒。当设备间的界限消失，我们才能真正聚焦于用户需求本身。&quot;对于吉林银行来说，鸿蒙生态带来的意义不仅仅优先他人一步，更重要的是带来了万物互联的时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;夜幕降临，金融街的灯火次第亮起。在这场由鸿蒙系统掀起的数字化浪潮中，银行正从传统的 &quot;金融服务提供者&quot; 转变为 &quot;智能生态构建者&quot;。当吉林银行以金融级安全纽带编织起千万用户的数字生活场景，既筑牢数字经济时代的安全护城河，又为银行生态的生长埋下战略伏笔；当意图框架读懂用户每一个潜在需求，各个企业正在书写属于自己的全场景智慧篇章。而这，仅仅是鸿蒙星河下的序章。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解鸿蒙开发认证详情，探索鸿蒙开发者联盟丰富资源，点击链接：&lt;a href=&quot;https://developer.huawei.com/consumer/cn/training/certifications/harmonyos?ha_source=51cto&amp;amp;ha_sourceId=70000008&quot;&gt;鸿蒙开发者联盟&lt;/a&gt;&quot;。这里有开发文档、论坛、工具等，快加入，开启鸿蒙开发之旅！&lt;/p&gt;</description><link>https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</link><guid isPermaLink="false">https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</guid><pubDate>Wed, 14 Jan 2026 02:49:38 GMT</pubDate><author>HarmonyOS</author><category>华为</category><category>HarmonyOS</category></item><item><title>规范驱动开发：让架构变得可执行</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第五代抽象&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程历史上的每一次重大转变都是由一种一致的力量驱动的：抽象的兴起。最早一代的软件是用原始机器代码编写的，后来汇编语言引入了可读性和控制层。更高级的语言已经跨越多个不同的范式发展，使得像C、Java和Python这样的通用语言及其衍生品得以发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些语言使得抽象得以进步，其中像内存管理和特定平台的怪癖这样的概念在日常工作流程中被掩盖，并且由开发者代表进行处理。这种级别的可访问性允许更广泛的生态系统发展，因为随着&lt;a href=&quot;https://medium.com/@ajuatahcodingarena/generations-of-programming-languages-bed30d19ea8e&quot;&gt;每一代语言&lt;/a&gt;&quot;的出现，相应的支持工具链也在发展。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4914486123b5441a9001b80e8ff7272b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图1：编程语言的世代&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第五代，即使用自然语言的第五代，长期以来一直是编程语言的目标演变，其中人类用他们的母语交谈，并且以可执行的方式进行解释。这种演变直到最近才真正成为主流。推动这一点的是人工智能（AI）的代际能力，现在已经成熟到可以接收以人为中心的输入，并用你选择的编程语言构建解决方案。几十年的学术研究记录了这一进化过程，&lt;a href=&quot;https://www.dreamsongs.com/RiseOfWorseIsBetter.html&quot;&gt;博客也非正式&lt;/a&gt;&quot;地对此进行了讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些转变中的一个共同主题是开发人员角色的演变。每一次抽象提升都允许开发人员更多地关注意图，而不是机制，我们现在进入了另一个拐点。第五代不仅因为广泛使用生成式AI而加速，而且与行业主导的采用相吻合。这代表了开发者如何从根本上接近他们手艺的新纪元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着前几代的成熟，支持它的工具链也出现了。在前几代中，工具是在一段时期稳定后出现的；然而，随着AI研究和产出的快速发展，工具链现在有望塑造和定义这一代，而不仅仅是支持它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为这些进步的一部分，一套关键的工具出现了，集中在规范驱动开发（SDD）上。这种趋势是由AI辅助代码生成的接受所驱动的，它允许开发人员提升自己的抽象，并表达系统应该做什么，而智能工具则实现了它的实际完成方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变重新定义了我们如何接近系统的架构和设计。现在，团队维护的是活的规范，而不是随着时间的推移而偏离原始意图的静态架构图。这些定义了系统契约、边界、不变量和行为。这些规范在设计上是可执行的；它们可以生成代码、文档、SDK、模拟甚至服务基础设施。AI智能体，通过角色映射的能&lt;a href=&quot;https://github.com/ambient-code/platform/tree/main/agents&quot;&gt;力来播种&lt;/a&gt;&quot;它们的上下文，其中角色的专业知识被捕捉为智能体的可消费输入，现在可以作为解释器、验证器和特定于领域的协作者行使权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在本文中，我们将SDD作为一种架构范例进行研究，详细说明规范如何成为系统的可执行支柱，漂移检测和持续验证如何将架构转变为运行时不变量，AI和代理工具如何重塑生成和管理，以及这种模型如何代表软件抽象长期演变中的下一个主要拐点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SDD架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规范驱动开发（SDD）这个名字可能暗示了一种方法论，类似于测试驱动开发。然而，这种框架低估了它的重要性。更准确的理解是，SDD是一种架构模式，它通过将可执行规范提升到代码本身之上，从而颠倒了传统的事实来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD代表了软件系统架构、治理和演变方式的根本转变。在技术层面上，它引入了一个声明性的、以契约为中心的控制平面，将规范重新定位为系统的主要可执行工件。相比之下，实现代码成为了次要的、生成的架构意图表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统架构依赖于源代码作为规范的控制面。在SDD中，控制面向上移动到规范控制面。这个控制面正式允许我们定义诸如：&lt;/p&gt;&lt;p&gt;接口契约（功能、输入/输出、行为保证）数据模式和不变量（结构、约束、验证规则）事件拓扑（允许的流、排序、传播语义）安全边界（身份、信任区域、策略实施）兼容性规则（包括向后和向前）版本控制语义（演进、降级、迁移）资源和性能约束（延迟、吞吐量、成本）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一变化涵盖了跨行为和治理的经典体系结构表面区域的组合，并具有正确性的时间维度。SDD不是独立地在服务和存储库之间协调这些领域，而是将它们集中到一个单一的权威模型中。这种模式更接近于语言类型系统或编译器：它不执行程序本身，而是定义了什么是可表达的，拒绝什么是无效的，并限制演变以保持随时间的正确性和兼容性。架构不再是咨询性的；它现在变得可实施和可执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管越来越多的工具被冠以SDD的标签，但从根本上说，它不是一个产品、框架或正式语言。相反，它是一个架构构造，以惊人的一致性作为一个五层执行模型重新出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层中，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/828f7550d967b76f786cb020c8143027.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图2：SDD 5层执行模型&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些层次共同构成了一个封闭的、由规范控制的控制系统，其中意图不断塑造执行，而执行不断地验证意图。由此产生的并不是对现有架构的渐进式改进，而是权威、控制和真实性所在位置的根本倒置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在让我们来看看这五个层次。在整个过程中，我们将遵循一个经典的订单管理服务的简化示例，以展示各层之间的进展以及它们是如何相互加强的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是系统行为的权威定义。它捕获了系统的声明性意图，而不是如何实现。这一层通常包含API模型、消息传递契约、领域模式和特定于系统、以策略为中心的约束。从抽象的角度来看，它既是人类可读的，也是机器可执行的，同时作为设计工件和操作控制面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能的样子：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;javascript&quot;&gt;service: Orders
api:   
  POST /orders:
      request:       
        Order:         
          id: uuid         
          quantity: int &amp;gt; 0     
      responses:       
        201: OrderAccepted       
        400: ValidationError  
policies:   
  compatibility: 
  backward-only   
  security:     
    auth: mTLS&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个规范明确声明了我们的期望：&lt;/p&gt;&lt;p&gt;订单必须是正数API不得引入破坏性变更请求必须经过认证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里没有引用任何语言、框架或基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层将声明性系统意图转化为可执行的形式。它作为一个多目标系统编译器，但与发出机器指令的经典编译器不同，这一层发出系统形状和跨语言、框架和平台的可执行运行时界面。在这里，问题空间由规范层声明，工具将其操作形式具体化。典型的输出包括类型模型、契约存根、验证中间件、文档以及一系列集成和一致性测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单示例为例，规范被摄取并发出可执行的系统表面。从概念上讲，这看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;spec.yaml
   → Type models (Java, TypeScript)
   → Request validators   
   → API stubs  
   → Contract tests&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具将声明的意图转化为具体的、可执行的形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;构件层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层包含了生成阶段的具体输出：生成的服务、组件、客户端、数据模型和适配器。关键的是，这些工件不被视为主要资产。相反，它们是可再生的、可丢弃的、可替换的，并且可以持续协调。这颠覆了传统软件架构的一个基本假设：代码不再是系统的记录；规范是。随着代码变得无限可复制和按需生成，新出现的术语环境代码恰如其分地抓住了这种范式转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的订单的形状现在可以用生成的一次性代码实现了。这可以看到类似于类型化模型的输出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;export interface Order {
   id: string;
   quantity: number; 
}
With a validator:
if (order.quantity &amp;lt;= 0) {
    throw new ValidationError(&quot;quantity must be greater than zero&quot;); 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些构件不是真相的来源。如果规范发生变化，它们将被重新生成。如果它们被删除，什么也不会丢失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;验证层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层强制执行意图和执行之间的持续一致性。它由契约测试、模式验证、有效载荷检查、向后兼容性分析和架构漂移检测机制组成。它在结构上扮演了编程语言的类型系统和管理程序对虚拟机所扮演的角色：积极防止架构违规传播到运行时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的生成层创建的工件最终在这里进行管理，其中验证确保运行时不能偏离意图：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;✓ Reject requests with quantity &amp;lt;= 0&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;违规行为在构建时、部署期间以及我们的持续集成系统中被检测到。架构正确性是持续强制执行的，而不是手动审查的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是操作系统本身，由典型的一系列构件组成，例如：&lt;/p&gt;&lt;p&gt;API消息代理和流处理管道函数、方法和等效结构集成服务&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至关重要的是，运行时的形状完全受到上游规范和验证层的约束。因此，运行时行为在架构上是确定的，而不是涌现性的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果我们尝试在我们的订单服务使用负数量，如下所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;POST /orders 
{ 
  &quot;id&quot;: &quot;123&quot;, 
  &quot;quantity&quot;: -1 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们返回了一个400 ValidationError，并不是因为运行时拒绝了请求，而是因为在系统执行任何请求之前，该行为在规范层中声明，由生成层具体化，由构件层实例化，并由验证层持续强制执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构反转&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几十年来，软件架构一直在一个基本上未受挑战的假设下运作，即代码是最终的权威。架构图、设计文档、接口契约和需求规范都是用来指导实现的。然而，运行中的系统总是从最终部署的内容中获得其真相。当出现不匹配时，标准的反应是“更新文档”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD完全颠覆了这种关系。规范成为系统现实的权威定义，实现是持续派生、验证的，并且在必要时重新生成以符合该真实性。这不是一个哲学上的区别；它是软件系统治理的结构性反转。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统的软件交付遵循线性、有损失的管道，如图3所示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/98bba58fc95d62fbe1391268cfc625d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图3：传统的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一步翻译都引入了重新解释、手动适应和隐藏的假设。因此，不能阻止架构漂移；它是在晚期被发现的，通常是通过生产事件、失败的集成、安全审计或合规性违规。当检测到不一致时，它是取证而不是纠正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD从根本上将这种流程重构为一个受控的控制循环：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5a9a1f07925b2e6f8103b15cdc647cf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图4：SDD受控的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个控制循环用积极的架构强制执行取代了延迟发现。漂移检测不会修补运行时行为；它纠正规范权威，并触发系统的受控再生。传统架构假设代码随时间的推移成为事实；SDD通过确保规范保持永久的事实来源，并且运行时持续被迫符合它，从而颠覆了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种架构反转可以概括如下几点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在SDD中，代码不再是真相出现的地方，而成为真相仅仅被实现的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种反转在结构上等同于早期的范式转变，即从人的责任中移除整个类别的正确性约束，并使其在机械上可执行：&lt;/p&gt;&lt;p&gt;从手动内存管理到垃圾收集，内存安全成为运行时不变量从裸机到虚拟机，隔离和资源边界成为平台保证从物理服务器到声明性基础设施，其中配置漂移和拓扑正确性不断得到协调从无类型语言到静态类型系统，在编译时强制执行结构正确性从非正式的接口协议到模式和契约强制执行的API，交互正确性被机械地验证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在每种情况下，正确性都从传统上由人类强制执行转变为由平台结构性强制执行。SDD将这一原则应用于系统边界、架构和行为本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;漂移检测：使架构自我强制执行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦规范成为权威，漂移检测不再是一种测试便利；它成为一种强制性的架构能力。它是将意图转化为不变性的执行机制。在这个模型中，漂移不仅仅是模式不匹配；它是声明的系统意图和观察到的系统行为之间的任何偏差。这种偏差可能是结构性的、行为性的、语义性的、与安全相关的或进化性的。我们在实验中遇到的一些例子包括：&lt;/p&gt;&lt;p&gt;一个API返回了规范中未声明的字段一个服务在重构过程中默默地省略了必需的字段消息负载在没有协调的模式版本控制的情况下不断演变错误处理偏离了合同保证相对于最初的策略意图，安全范围正在退化&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没有漂移检测，SDD就会退回到文档驱动的开发。有了它，系统就变成了自我监管。漂移检测形成了一个闭环反馈控制系统。它不断地比较系统声称要做的事情和它实际做的事情。这与古典测试相比，后者只提供周期性的、基于样本的保证，是一种根本不同的操作姿态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统架构中，意图的偏差会悄无声息地传播，通常数月之后才会以中断、审计失败或安全漏洞的形式显现出来。在SDD系统中，漂移变成了机器默认可以检测到的。规范验证器可以直接嵌入我们的CI管道中，运行时执行层：模式验证、有效负载检查、契约验证和规范差异引擎都成为了一等的架构组件。当输出违反规范时，系统会快速失败，并允许进行航向修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种强制要求在一个固有的多模型未来中变得更加重要。软件系统将越来越多地受到人类驱动开发和机器驱动生成的影响，通常在同一规范表面上并行操作。系统中不再有单一的线性路径。更改可能来自开发者、AI代理、自动化重构工具或政策驱动的生成器。这种进化路径的多样性极大地放大了漂移问题：分歧不再是边缘情况；它是必须持续治理的自然状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总体效果是治理方式的深刻转变。架构不再是设计阶段的产物；它变成了一个持续执行的运行时不变量。规范从被动的参考资料转变为主动的控制表面，漂移检测作为反馈信号，保持系统与意图一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，这并不意味着一个完全自主的系统，机器单方面定义正确性。规范不仅仅是一个机械合同；它是人类对目的、风险容忍度和权衡的表达。漂移检测可以识别系统已经偏离，但它不能单独决定这种偏离是可以接受的、偶然的还是可取的。一些漂移代表缺陷，而其他漂移代表进化。在这个边界上，当自动化执行遇到解释性判断时，人类的角色再次变得至关重要。不是作为失败后被动审查日志的审查者，而是作为治理意义、意图和受控变更的积极参与者。这就是Human-in-the-Loop（人在循环中）不再只是一个安全网，而是一个一等的设计原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人在循环中：在自动化架构中保留意图&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们最初探索这种系统设计模式时，我们以一种天真的“氛围编码”心态来接受生成的变化，最小化阻力，并信任SDD工具链为我们处理边缘情况。那个假设很快就失败了。取而代之的是一个更强大的认识：SDD并没有将人类从循环中移除；它将人类判断重新定位到更高的控制层面。问题不再是人类如何实现系统，而是如何以及在哪里治理系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD并没有消除人类在软件设计中的参与。它重新分配了人类认知的应用领域。传统上，一旦功能实现，开发人员就会花费大量的精力来解决不匹配、调试集成故障、协调分散的服务以及修复更改的意外副作用。随着时间的推移，这被错误地等同于软件工程本身的手艺。实际上，这是维护大型、长期、面向生产的系统的负担。SDD将这个负担转移到机器上，同时故意保留人类对意图、策略和意义的权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引入了一种新型的人机界面。人类仍然是领域语义、风险容忍度、安全范围和系统进化方向的最终守护者。这种权威也扩展到隐含地塑造工程决策的法律、伦理和道德框架中。这些维度不能仅从执行跟踪或行为观察中推断出来。它们存在于机器无法完全拥有的抽象层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，人类将这些约束明确编码到规范层中，机器承担起执行、生成和持续一致性的责任。这反映了我们技术的历史演变：就像我们曾经将手动内存管理交给垃圾收集一样，我们现在正在将结构性执行和机械一致性委托给SDD。取代这种委托的不是盲目的自动化，而是明确的审批边界：&lt;/p&gt;&lt;p&gt;破坏模式更改需要人工批准策略转变需要人工授权AI提出的重构需要人工确认兼容性降级需要人工解释&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，SDD实现了有限的自主性，而不是完全自动化，并且在这些限制内，长期架构意图可以得以保留。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过强制漂移检测和人工意图监督，SDD在人和机器之间建立了新的责任分工。执行变得自动化。意义仍然是人类的。这种分离不是哲学上的；它是架构上的，正是这种分工产生了一类新的基础设施能力。一个的规范原生系统现在必须将执行、演化、验证和治理直接编码到其核心原语中。我们接下来探讨这些能力，以及它们为什么在结构上与经典软件架构中的能力不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规范原生系统的核心能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD不是由单一工具、框架或平台启用的。它源于一组紧密耦合的架构能力，这些能力共同允许规范变得可执行、可强制和可扩展。当这些能力中的任何一个缺失时，SDD就会退回到文档驱动开发或临时代码生成。要从理论进入操作范式，系统必须内化五个核心能力：&lt;/p&gt;&lt;p&gt;规范编写作为一等工程表面正式验证和类型强制确定性生成和组合持续的一致性和漂移执行受控演化和兼容性控制&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们将这种操作规程称为SpecOps，即规范操作。从SpecOps的角度来看，规范被视为一等的、可执行的系统资产，这些能力并没有定义一个产品类别；它们代表了软件意图的控制平面。在规范原生系统中，规范编写不是在实现之前进行的活动；它就是实现活动。因此，系统必须支持多模型规范，其中结构、行为和策略定义共存于统一的模式空间内。这需要可组合的领域建模，使得分层规范成为一种可行的架构策略，而不是文档便利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着规范成为主要的系统构件，它们必须像源代码一样被严格地处理：版本控制、同行评审、分支和受控合并策略都要是强制性的。此时，规范不再是描述性的，而是成为系统本身的可编程模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦规范是可执行的，它还必须是机器可验证的，就像编译器前端或类型系统一样严格。这种执行涵盖了结构验证、语义一致性和领域不变性执行。条件约束、引用完整性和跨规范一致性必须是可证明的。效果不仅仅是提高了正确性，而是从可以表示的所有内容的空间中消除了整个类别的系统故障，就像静态类型限制非法程序一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个范式中，生成不是脚手架的一种形式。它是声明的系统真理的具体化。这需要严格确定性行为。一个生产级别的规范原生系统必须保证输入的确定性：相同的规范总是产生相同的构件。它必须是目标无关的，跨语言、平台和运行时环境产生一致的输出。最关键的是，生成必须是逻辑可逆的。系统必须始终能够回答一个简单但基础的问题：哪个规范状态产生了这种行为？这种决策的谱系可追溯性是将生成从生产力辅助提升到架构权威的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦生成自动化，执行就必然变得连续。运行时系统不能再悄悄地偏离声明的意图。实现不能引入未记录的行为。消费者不能依赖于未定义的属性。因此，架构从设计时断言转变为运行时不变性，由系统本身积极维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD中最困难的能力不是生成或验证，而是不断裂的更改。规范原生系统必须自动将更改分类为添加性、兼容性、破坏性或模糊性，并执行明确的兼容性策略。这引入了受控演化的正式概念：需要并行版本表面、已知的兼容性窗口、受控的弃用曲线和用于破坏性更改的显式批准门。没有这个，SDD在架构上就会变得脆弱。有了它，系统可以在不违反自己的正确性保证的情况下进行演化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这五个能力引入的最深刻的转变不是技术上的；它是结构上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交付的单元不再是服务或代码库。交付的单元变成了规范本身。这将结果与产出重新对齐：声明的是什么，交付的就是什么。这与以氛围驱动、生成性编码方法形成鲜明对比，在这些方法中，偏差是创造力（或幻觉）的涌现属性，而不是设计中受控的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论：存在的工程权衡和挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程中每一次重大的抽象飞跃都带来了非凡的生产力提升，同时引入了全新的系统性风险类别。垃圾收集消除了大量内存错误，同时引入了暂停时间行为和新的故障模式。虚拟机简化了部署，同时增加了业务编排的复杂性。云平台消除了基础设施负担，同时引入了深层操作耦合。规范驱动开发也不例外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将系统的真实来源提升到规范和生成器中，SDD并没有消除复杂性；它只是简单地重新定位了复杂性。下面的权衡定义了我们在大规模采用这种范式时所经历的真实工程成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范成为主要的复杂性表面&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，规范不再是文档构件，而是成为长期存在的可执行基础设施。因此，它们获得了传统上与源代码相关的属性。它们继承了通常与源代码相关的所有属性：技术债、跨团队耦合、兼容性惯性和架构引力。因此，模式工程成为了与数据建模和分布式系统设计同等重要的一级架构学科。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成器信任成为供应链问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，AI代码生成器不再是开发者的便利工具。它们成为系统可信计算基础的结构组件。确定性、可重复性、可审计性、沙箱执行和可验证的出处不再是可选属性；它们是强制性的。代码生成从工具提升为关键基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时执行有实际成本&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD将执行从社会过程转移到技术控制。这种转变是强大的，但不是免费的。运行时契约验证引入了实际的计算开销。在小规模上，这个成本是微不足道的。在大规模上，我们需要考虑系统的目的，无论是高频API、实时流还是对延迟敏感的系统。这成为了一个明确的架构预算项目。正确性成为了计量资源，而不是默认的免费属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;认知转变非同小可&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD用契约优先推理取代了实现优先的思维。这要求工程师采用新的心智模型：&lt;/p&gt;&lt;p&gt;用不变量而不是行为来思考关于兼容性而不是功能的推理用声明式而不是过程式表达意图将模式视为可执行程序&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;历史上每一次抽象的转变都扩大了人类的影响力，同时引入了不熟悉的失败模式，需要多年才能掌握。SDD现在正进入相同的成熟曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;架构权威的价格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然新的范式转变通常令人兴奋，但最终是否采用这一转变归结于平衡所涉及的实际权衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，SDD提供了：&lt;/p&gt;&lt;p&gt;架构确定性持续的正确性执行系统性减少漂移多语言平价可复制的系统边界&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但它以以下代价：&lt;/p&gt;&lt;p&gt;模式复杂性生成器信任要求运行时验证成本长期兼容性负担工程角色的认知转变&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不是避免SDD的理由。这是一个有意识地采用它的理由，要有明确的治理、有纪律的规范实践，以及对其成本的清醒认识。每一次抽象的飞跃都需要新的严谨形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD只是将这种严谨重新定位到它一直属于的地方：系统真理本身的定义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/spec-driven-development/&quot;&gt;https://www.infoq.com/articles/spec-driven-development/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</link><guid isPermaLink="false">https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</guid><pubDate>Wed, 14 Jan 2026 02:11:22 GMT</pubDate><author>Leigh Griffin</author><category>架构</category><category>AI&amp;大模型</category></item><item><title>Java近期资讯：Spring gRPC、Quarkus、Gatherers4j、Keycloak、Grails、Java Operator SDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B29&quot;&gt;Build 30&lt;/a&gt;&quot;在上周发布，其中包括对Build 29的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B29...jdk-26%2B30&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2026%20and%20%22resolved%20in%20build%22%20%3D%20b30%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B4&quot;&gt;Build 4&lt;/a&gt;&quot;也在上周发布，包含了从Build 3的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B3...jdk-27%2B4&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2027%20and%20%22resolved%20in%20build%22%20%3D%20b04%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-grpc&quot;&gt;Spring gRPC&lt;/a&gt;&quot; 1.0.1，&lt;a href=&quot;https://spring.io/blog/2026/01/07/spring-grpc-1&quot;&gt;第一个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、依赖升级和增强功能，例如：与跟踪相关的更详细的错误消息；以及使用Spring Security SecurityContextHolder 类中定义的 getContext() 方法与gRPC特定的Kotlin协程的能力。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/spring-projects/spring-grpc/releases/tag/v1.0.1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Quarkus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Quarkus 3.30.6，&lt;a href=&quot;https://quarkus.io/blog/quarkus-3-30-6-released/&quot;&gt;第六个维护版本&lt;/a&gt;&quot;，带来了显著的变化，例如：解决了在&lt;a href=&quot;https://quarkus.io/extensions/io.quarkus/quarkus-jfr/&quot;&gt;JDK Flight Recorder&lt;/a&gt;&quot; 扩展在发出运行时信息时由于关闭时失败而导致的 NullPointerException ；以及移除了官方&lt;a href=&quot;https://github.com/lz4/lz4-java/blob/master/README.md&quot;&gt;LZ4 Java&lt;/a&gt;&quot;项目（ org.lz4:lz4-java ），转而使用由Oracle的首席技术员工&lt;a href=&quot;https://www.linkedin.com/in/yawkat/&quot;&gt;Jonas Konrad&lt;/a&gt;&quot;维护的分支（ at.yawk.lz4:lz4-java ），因为前者在2025年底停止维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.30.5&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gatherers4j&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://tginsberg.github.io/gatherers4j/&quot;&gt;Gatherers4j&lt;/a&gt;&quot; 0.13.0版本发布了新的中间方法 -uniquelyOccurringBy() ，旨在将流限制为由给定函数测量的唯一发生元素，以及添加到 Gatherers4j 抽象类中以计算 Java Stream&lt;t&gt; 接口的移动和运行中的中位数、最大值和最小值的 movingMedian() 和 movingMedianBy() ， runningMedian() 和 runningMedianBy() ， movingMax() 和 movingMaxBy() ， movingMin() 和 movingMinBy() ， runningMax() 和runningMaxBy() ， runningMin() 和 runningMinBy() 等方法。&lt;/t&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gatherers4j由德意志银行的主管和首席工程师&lt;a href=&quot;https://www.linkedin.com/in/tginsberg/&quot;&gt;Todd Ginsberg&lt;/a&gt;&quot;于2024年7月推出，是一个基于JEP 485，&lt;a href=&quot;https://openjdk.org/jeps/485&quot;&gt;Stream Gatherers&lt;/a&gt;&quot;的中间流库，在JDK 24中交付。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/tginsberg/gatherers4j/releases/tag/v0.13.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Keycloak&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.keycloak.org/&quot;&gt;Keycloak&lt;/a&gt;&quot; 26.5.0&lt;a href=&quot;https://www.keycloak.org/2026/01/keycloak-2650-released&quot;&gt;版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：&lt;a href=&quot;https://www.keycloak.org/securing-apps/jwt-authorization-grant&quot;&gt;JWT授权授予预览版&lt;/a&gt;&quot;，用于OAuth 2.0客户端身份验证和授权授予（&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc7523&quot;&gt;RFC 7523&lt;/a&gt;&quot;）规范的JSON Web令牌（JWT）配置文件的实现，用于使用外部签名的JWT断言请求OAuth 2.0访问令牌；以及OpenTelemetry增强功能，包括将日志导出到OpenTelemetry收集器和使用Quarkus &lt;a href=&quot;https://quarkus.io/guides/telemetry-micrometer-to-opentelemetry&quot;&gt;Micrometer和OpenTelemetry&lt;/a&gt;&quot;扩展导出指标。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Grails&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://grails.apache.org/&quot;&gt;Grails&lt;/a&gt;&quot; 7.0.5，第五个维护版本，提供了缺陷修复和增强功能，例如：添加了缺失的应用程序类名和脚本名参数到 url-mappings-report Grails控制台命令；以及移除了 org.apache.tomcat.embed:tomcat-embed-logging-log4j 模块，因为它自2016年5月以来一直没有维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Java Operator SDK&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://javaoperatorsdk.io/&quot;&gt;Java Operator SDK&lt;/a&gt;&quot; 5.2.2版本发布，这是一个用于与Kubernetes操作符一起工作的工具，带来了显著的变化，例如：在 ExpectationIT 和 PeriodicCleanerExpectationIT 类中添加了 @Sample 注解，以改进集成测试；以及解决了在启动出现错误时线程池不停止的问题。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.2.2&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</link><guid isPermaLink="false">https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</guid><pubDate>Wed, 14 Jan 2026 01:17:20 GMT</pubDate><author>作者：Tim Anderson</author><category>编程语言</category></item><item><title>模力工场 028 周 AI 应用榜：AI “身体”觉醒，从工业前线到情感陪伴</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模力工场新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;将亮相 OceanBase 社区嘉年华！诚邀您加入我们的上海现场展位。作为 OceanBase 合作的创新社区，模力工场将于 1 月 31 日 登陆上海社区嘉年华，并拥有专属展位。这不仅是一次技术交流——我们更希望和您一起，在现场用 AI Coding 展现创造力、在开放麦分享您的项目故事、与行业先锋面对面切磋、在开源市集交换灵感。我们为您预留了专属席位，期待与您共同呈现：当开源精神遇上 AI 创造力，能碰撞出多少令人惊艳的可能。立即报名，锁定与数百位技术同行深度连接的一天！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fb3d72f8356cfc07975e9e5f9d2e177c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;028 周榜单总介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 028 周 AI 应用榜来啦！本周上榜的应用大多来自美国 CES 展及阿里云通义智能硬件展，从优必选的集群物流调度系统到银河通用的零样本抓取机器人，从众擎的拟人步态双足机器人到 Walulu 的情感陪伴毛绒玩具——这些应用共同见证了一场时代风暴：AI 硬件正在集体跨越“工具”属性，进化为真正的“智能体”。它们不再是被动响应指令的机械装置，而是具备了理解环境、自主规划、闭环执行乃至情感交互能力的“数字生命体”。这场从“功能叠加”到“语音助手”再到“智能体化”的范式革命，正同时重塑生产力与生产关系：在工业场景成为可靠的“数字员工”，在消费领域则成为可建立羁绊的“数字伙伴”，标志着人机协同进入了全新的历史阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;: 一款面向创作者与普通用户的 AI 互动式内容生成应用，通过自然语言或轻量交互，快速生成有趣、可分享的内容。&lt;a href=&quot;https://agicamp.com/products/deeprobotics?utm_source=20260112infoQ&quot;&gt;云深处巡检机器人&lt;/a&gt;&quot;: 专注于工业复杂环境的自主巡检解决方案。其四足机器人具备强运动与感知能力，可在无网络支持下独立完成巡检任务并安全返回，已在电力、能源等领域实现落地应用。&lt;a href=&quot;https://agicamp.com/products/ubtrobot?utm_source=20260112infoQ&quot;&gt;优必选（UBTECH）搬运/物流机器人&lt;/a&gt;&quot;: 提供从智能搬运机器人到集群调度系统的软硬件一体化智慧物流方案，帮助企业实现仓储搬运环节的自动化升级与效率提升。&lt;a href=&quot;https://agicamp.com/products/engineai?utm_source=20260112infoQ&quot;&gt;众擎机器人&lt;/a&gt;&quot;: 聚焦高动态双足人形机器人的研发，致力于突破拟人步态与平衡控制技术，为未来机器人在人类环境中的通用移动能力提供底层支撑。&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu 📍成都&lt;/a&gt;&quot;: 一款具备情感交互与离线记忆能力的 AI 智能毛绒玩具，通过多模态交互设计，为用户提供个性化、可长期互动的陪伴体验。&lt;a href=&quot;https://agicamp.com/products/galbot?utm_source=20260112infoQ&quot;&gt;银河通用机器人&lt;/a&gt;&quot;: 研发面向仓储、零售等场景的通用移动操作机器人，具备视觉识别与自主抓取能力，可在动态环境中完成物品拣选、搬运等任务。&lt;a href=&quot;https://agicamp.com/products/spirit?utm_source=20260112infoQ&quot;&gt;千寻智能Spirit AI&lt;/a&gt;&quot;: 从事通用人形机器人系统研发，整合高性能硬件平台与 AI 算法栈，探索机器人在多场景下的感知、决策与执行能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周必试应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：全流程托管｜零门槛动画｜AI 协同创作&lt;/p&gt;&lt;p&gt;模力小A推荐：通过七个 AI 智能体（导演、编剧、美术等）分工协作，将你的文字想法自动转化为包含分镜、角色与场景的动画视频，大幅降低了专业动画内容的制作门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上榜冷门但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：AI 硬件｜可成长陪伴｜离线记忆&lt;/p&gt;&lt;p&gt;模力小A推荐：一款结合了情感计算模型的智能玩具。它能够记住与你的互动，并做出个性化的反应，提供一种注重私密性与持续性的陪伴体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 正在从虚拟世界走向物理世界，为自己寻找真实的“身体”。本周&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;榜单上的应用清晰地展示了这一趋势——AI 不再是停留在软件层面的算法，更是成为驱动各类硬件的“大脑”。这次上榜的八大应用，集中体现了AI 硬件在两大关键赛道的爆发：工业效率革命与情感陪伴需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在工业领域，AI 机器人正从简单的机械臂进化为真正的“智能员工”。云深处的巡检机器狗能够在无网络环境的复杂场景中自主完成巡检任务，实现了从“自动化”到“自主化”的跨越；优必选的智慧物流方案已超越单台设备，提供机器人群调度与仓储管理系统深度集成的整套解决方案；银河通用的物流机器人则实现了“零样本抓取”能力，即使面对全新商品也能准确识别搬运。这些进展表明，工业机器人正从实验室原型走向工程化落地，其核心价值在于可量化的投资回报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在消费领域，情感陪伴型机器人正开辟全新市场。Walulu 的 AI 毛绒宠物通过情感模型与离线记忆技术，创造出能随互动成长的“伙伴关系”，本质是在贩卖情感价值而非功能价值。这反映了 AI 正从解决效率问题，转向满足更深层的心理需求。未来，能否建立稳定、专属的“数字亲密关系”，或将成为此类产品发展的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得关注的是众擎的人形机器人——虽然步态尚显蹒跚，但其对双足行走、自然步态的追求，瞄准的是机器人无缝进入人类环境的终极目标。这种对“通用形态”的前瞻布局，代表着产业在为更广阔的未来场景做技术储备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了实体硬件产品，OiiOii 这款 AI 动画创作应用近期也备受瞩目。其“全流程托管模式”尤为亮眼——平台将传统动画制作中的艺术总监、编剧、分镜师、角色设计师、场景设计师、动画师、音效总监等七个核心角色，分别由七个 AI 智能体担任。这些智能体不仅形象亲切可爱，更如导师般指引用户一步步完成创作。用户只需输入创意想法，并在关键节点进行确认，即可产出完整动画作品。这极大降低了创作门槛，让普通用户也能轻松上手动画制作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综上，AI 硬件已越过“加个语音模块”的简单升级阶段，进入以智能体化为特征的第三阶段。产业不再满足于制造“能联网的工具”，而是致力于创造“能自主行动的数字生命体”。从工业现场到家庭空间，AI 正在改写生产力与生产关系的定义——在工厂成为可靠的数字员工，在生活场景成为温暖的数字伙伴。当 AI 真正获得在物理世界中感知、决策和执行的能力，人机协同或将进入一个前所未有的新纪元。&lt;/p&gt;</description><link>https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</link><guid isPermaLink="false">https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</guid><pubDate>Tue, 13 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>Claude Code创建者的开发工作流程</title><description>&lt;p&gt;Claude Code的创造者Boris Cherny描述了他如何在&lt;a href=&quot;https://x.com/bcherny/status/2007179832300581177?s=20&quot;&gt;Anthropic&lt;/a&gt;&quot;上使用Claude Code，强调了诸如运行并行实例、共享学习成果、自动化提示和严格验证结果等实践，以随着时间的推移提高生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny没有定制Claude Code，因为他发现它开箱即用，非常好用，可以并行运行许多会话，包括在他的&lt;a href=&quot;https://x.com/bcherny/status/2007218725754442106?s=20&quot;&gt;MacBook&lt;/a&gt;&quot;终端本地运行的5个会话和在Anthropic的网站上运行的5-10个会话。为了避免冲突，&lt;a href=&quot;https://x.com/bcherny/status/2007200880081436864?s=20&quot;&gt;每个本地会话使用自己&lt;/a&gt;&quot;的 git checkout ，而不是分支或工作树。他从CLI开始与 &amp;amp; 进行远程会话，并经常使用 -teleport 将它们来回移动。然而，由于意外情况，&lt;a href=&quot;https://x.com/bcherny/status/2007219912411115702?s=20&quot;&gt;这些会话中有10-20%被放弃了。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny更喜欢使用Opus 4.5进行所有编码工作，他重视其比Sonnet更高的质量和可靠性，尽管Sonnet的速度较慢。他还发现Opus更擅长使用工具，并指出其总体上比小模型更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic的每个团队都在git中维护一个 CLAUDE.md 文件，以便Claude可以随着时间的推移而改进，以及最佳实践，如风格约定、设计指南、&lt;a href=&quot;https://x.com/bcherny/status/2007212366094811401?s=20&quot;&gt;PR模板&lt;/a&gt;&quot;等。Cherny经常经常在同事的PR上使用 @.claude 标签，将学习成果添加到 CLAUDE.md 中，确保每个PR的知识都被保存下来。Cherny说，目前，他们的 CLAUDE.md 有2.5k的token。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他的工作流程的一个关键方面是，先制定一个计划，然后迭代完善，再切换到自动编辑：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果我的目标是写一个Pull Request，我会使用Plan模式，然后和Claude来回交流，直到我喜欢它的计划。从那里，我切换到自动接受编辑模式，Claude通常可以一次性完成。一个好的计划真的很重要！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny使用斜杠命令执行提交、PR、简化和验证等日常工作流程来启动子智能体。所有的命令都存储在 .claude/commands/ 中，这也有助于减少对明确提示的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，Claude和我每天使用/commit-push-pr斜杠命令数十次。该命令使用内联bash预先计算git状态和其他一些信息，以使命令快速运行，并避免与模型来回切换。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然Claude的代码通常格式良好，但不一致有时会导致CI失败。为了防止这种情况发生，Cherny运行了一个PostToolUse钩子来清理代码：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;&quot;PostToolUse&quot; : [
     &quot;matcher&quot;: &quot;WritelEdit&quot;,
      &quot;hooks&quot;: [         
        {             
          &quot;type&quot;: &quot;command&quot;,             
          &quot;command&quot;: &quot;bun run format || true&quot;         
        }     
      ]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;出于安全考虑，Cherny几乎从不使用 --dangerously-skip-permissions 。相反，他通过 /permissions 启用在他的环境中安全的常用bash命令。这省去了他在诸如 bun run build:* 、 bun run test:* 、 cc:* 等命令上不必要的许可提示。他使用 --dangerously-skip-permissions 的唯一情况是在沙箱中运行长期任务，以防止Claude重复停止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最重要的技巧是给Claude提供一种通过反馈循环验证其工作的方法，例如运行bash命令、测试套件、或通过浏览器或模拟器测试应用程序。这可以将最终结果的质量提高2-3倍：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Claude会使用Claude Chrome扩展测试我给他的每一个claude.ai/code变更。它打开一个浏览器，测试UI，不断迭代，直到代码正常运行，用户体验很好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总的来说，Cherny解释说，&lt;a href=&quot;https://x.com/bcherny/status/2007290414961963524?s=20&quot;&gt;这种工作流程让他的团队专注于代码审查和指导&lt;/a&gt;&quot;，并指出当工程师阅读PR时，代码已经处于良好的可用状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny的推文在X.com上引发了广泛的讨论，包括一些我们在这里包含的有用澄清，但请务必阅读原文以了解全部细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&quot;&gt;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</link><guid isPermaLink="false">https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</guid><pubDate>Tue, 13 Jan 2026 07:03:00 GMT</pubDate><author>Sergio De Simone</author><category>性能优化</category></item><item><title>亚马逊云科技推出VPC加密控制，在传输过程中实施强制加密</title><description>&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2025/11/aws-vpc-encryption-controls/&quot;&gt;亚马逊云科技（AWS）最近推出了VPC加密控制功能&lt;/a&gt;&quot;，允许客户验证VPC内部和VPC之间的流量是否加密，并在支持的地方要求加密。该功能提供了对未加密流量的可见性，支持使用兼容的基于Nitro的基础设施进行强制执行，并允许排除无法加密流量的资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据云服务提供商称，这项新功能有助于组织在他们的AWS环境中应用一致的加密标准，并展示符合HIPAA、PCI DSS和FedRAMP等监管框架的合规性，这些框架要求全面加密。AWS的首席开发者倡导者&lt;a href=&quot;https://www.linkedin.com/in/sebastienstormacq/&quot;&gt;Sébastien Stormacq&lt;/a&gt;&quot;&lt;a href=&quot;https://aws.amazon.com/blogs/aws/introducing-vpc-encryption-controls-enforce-encryption-in-transit-within-and-across-vpcs-in-a-region/&quot;&gt;解释道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;金融服务、医疗保健、政府和零售等行业的组织在维护云基础设施的加密合规性方面面临着重大的操作复杂性。传统方法需要将多个解决方案拼凑在一起，并管理复杂的公钥基础设施（PKI），同时手动使用电子表格跟踪不同网络路径上的加密。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然社区的反应大多是积极的，但许多人&lt;a href=&quot;https://www.reddit.com/r/aws/comments/1p3jgtg/introducing_vpc_encryption_controls_enforce/&quot;&gt;最初对定价方法&lt;/a&gt;&quot;表示困惑，或者质疑为什么应该为安全控制付费。用户kei_ichi写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这个功能应该默认启用并且免费。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员可以为现有的VPC启用该功能，以监控流量流的加密状态，并识别无意中允许明文流量的VPC资源。云安全顾问和AWS安全英雄&lt;a href=&quot;https://www.linkedin.com/in/jcfarris/&quot;&gt;Chris Farris&lt;/a&gt;&quot;在他的&lt;a href=&quot;https://www.chrisfarris.com/post/reinvent2025/&quot;&gt;re:Invent&lt;/a&gt;&quot;概述中写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;让我们从为什么应该避免这种情况开始——每个非空VPC每月110美元。如果你需要“满足像HIPAA和PCI DSS这样严格的合规标准”和“展示符合加密标准”，这绝对是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC加密控制有两种操作模式：监控和强制执行。激活后，强制执行模式确保所有新资源仅在兼容的Nitro实例上创建，并且在检测到错误的协议或端口时丢弃任何未加密的流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba9147fdbbff2c458bbff4b8e9870f93.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：AWS博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员只有将所有资源迁移到兼容加密的基础架构后，才能启用强制模式。Farris指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你的VPC中有未加密传输的资源，你不能启用强制执行模式。这里的迁移工作将非常巨大，但如果你的审计员要求你手工完成这项工作，这些成本是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这需要首先升级到支持的硬件和通信协议。可以为不支持加密的资源（如互联网或NAT网关）配置特定的排除，因为它们的流量离开了AWS网络。在“理解现代云安全中的VPC加密”的文章中，Anish Kumar&lt;a href=&quot;https://medium.com/@anishkumarait/understanding-vpc-encryption-in-transit-for-modern-cloud-security-0cee62cd6501&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于你的云安全态势，你可以自信并有证据地回答这个问题：“我所有的VPC中的流量都加密了吗？”从合规审计的角度来看，你可以在流量日志和排除列表中展示加密状态。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项新功能目前在AWS的一些区域可用，包括弗吉尼亚北部、爱尔兰、伦敦和新加坡。在3月1日之前，VPC加密控制将免费使用，之后将对每个非空VPC收取固定的小时费，每小时0.15美元起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls/&quot;&gt;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</link><guid isPermaLink="false">https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</guid><pubDate>Tue, 13 Jan 2026 06:11:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>云安全</category></item><item><title>TanStack发布框架无关的AI工具包</title><description>&lt;p&gt;&lt;a href=&quot;https://tanstack.com/&quot;&gt;TanStack&lt;/a&gt;&quot;是广受欢迎的TypeScript库（如&lt;a href=&quot;https://tanstack.com/query/latest&quot;&gt;TanStack Query&lt;/a&gt;&quot;和&lt;a href=&quot;https://tanstack.com/table/latest&quot;&gt;TanStack Table&lt;/a&gt;&quot;）背后的团队，该团队最近发布了&lt;a href=&quot;https://tanstack.com/ai/latest&quot;&gt;TanStack AI&lt;/a&gt;&quot;的alpha版本。这是一个与框架无关的AI工具包，旨在消除供应商锁定，让开发者完全掌控自己的AI技术栈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI引入了跨多个AI供应商的统一接口、多语言服务器支持以及开放式协议架构。该alpha版本提供了对JavaScript/TypeScript、React和Solid的支持，并内置了&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.anthropic.com/&quot;&gt;Anthropic&lt;/a&gt;&quot;、&lt;a href=&quot;https://gemini.google.com/&quot;&gt;Gemini&lt;/a&gt;&quot;和&lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt;&quot;的适配器。此次发布代表了一种全新的AI工具理念，即将自身定位为中立于供应商的基础设施，而非平台服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI的突出特性之一就是其同构（isomorphic）工具系统，允许开发者通过toolDefinition()一次性地定义工具，并通过.server()或.client()方法提供特定环境的实现。这种架构在整个应用中提供类型安全性，同时支持工具在服务器和客户端上下文中执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具模式有两种定义方式：推荐使用&lt;a href=&quot;https://zod.dev/&quot;&gt;Zod&lt;/a&gt;&quot;&amp;nbsp;Schemas，或者使用&lt;a href=&quot;https://json-schema.org/&quot;&gt;JSON Schema&lt;/a&gt;&quot;（适用于已有JSON Schema定义的项目）。该工具包还提供了模型粒度的类型安全性，使开发者能够针对每个模型获得完整的、针对特定供应商选项的类型提示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;客户端库包括原生JavaScript、React和Solid，未来还将支持更多框架。alpha版本还附带了同构的开发工具，可洞察大语言模型（LLM）在服务器端和客户端的行为，使开发者能使用熟悉的模式调试AI工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在开发者社区中获得了积极反响。开发者Stanley Ulili在Better Stack的一篇&lt;a href=&quot;https://betterstack.com/community/guides/ai/tanstack-ai/&quot;&gt;详细指南&lt;/a&gt;&quot;中这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;虽然仍处于alpha阶段，但是它已经展现出了巨大的潜力。它注重清晰的架构、强大的TypeScript支持，并强调融入现有技术栈的自由，而非强制绑定特定框架或供应商。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1peowss/tanstack_ai_alpha_your_ai_your_way/&quot;&gt;Reddit上&lt;/a&gt;&quot;，一些评论者对SDK的使用场景以及这个新库试图解决的问题提出了疑问，这促使TanStack生态系统的创始人Tanner Linsley作出了回应：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;最近，我和TanStack的所有其他维护者都在深入探索AI，我们发现Vercel的解决方案仍有足够的改进空间，因此决定自己打造一个更贴近我们&lt;a href=&quot;https://tanstack.com/tenets&quot;&gt;产品原则&lt;/a&gt;&quot;的方案。&amp;nbsp;到目前为止，这带来了更好的类型安全性、更优的同构模式，坦白说，这也能够让我们自由地朝着自己想要的方向发展，而不必受制于其他团队。&amp;nbsp;竞争是好事，它能推动整体进步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI将自己定位为Vercel AI SDK的直接替代品，后者目前是JavaScript AI工具领域的主导者。与Vercel的做法不同，TanStack AI作为纯粹的开源基础设施，不包含服务层、不收取平台费用，也不存在供应商锁定。团队强调，开发者直接连接到自己选择的AI提供商，无需通过中间商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于这是新库的alpha版本，因此不存在从早期版本迁移的路径。开发者可通过npm安装核心包并开始使用：npm install @tanstack/ai @tanstack/ai-react @tanstack/ai-openai。&lt;a href=&quot;https://tanstack.com/ai/latest/docs/getting-started/quick-start&quot;&gt;快速入门指南&lt;/a&gt;&quot;提供了创建聊天应用的分步说明，而&lt;a href=&quot;https://tanstack.com/ai/latest/docs/guides/tools&quot;&gt;工具指南&lt;/a&gt;&quot;则深入讲解了同构的工具系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI是由TanStack团队开发和维护的开源项目。它延续了该团队在构建框架无关的开发者工具方面的良好声誉，目标是提供真正开放的工具，兼容任何技术栈，而非将开发者捆绑进专有的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/tanstack-ai-sdk/&quot;&gt;TanStack Releases Framework Agnostic AI Toolkit&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</link><guid isPermaLink="false">https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</guid><pubDate>Tue, 13 Jan 2026 02:40:29 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category><category>AI&amp;大模型</category></item><item><title>Anthropic深夜放出王炸！白领饭碗要被AI砸了？网友：不支持Linux，差评</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;在开发者工具 Claude Code 推出之后，Anthropic 团队很快意识到一个出乎预料的现象：开发者并没有把它局限在“写代码”这件事上。相反，Claude Code 被迅速用于整理资料、撰写文档、生成报告、分析数据，甚至承担起类似“数字同事”的角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种使用方式的外溢，最终促使 Anthropic 做出一个更激进的产品判断——如果大模型已经被当作工作伙伴使用，那么是否应该为“所有人”，而不仅仅是开发者，提供一种真正面向日常工作的智能协作形态？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是今天，Anthropic 正式推出了 Cowork。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/2798224b81184116211242f420b586d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 工程师、Claude Code&amp;nbsp;创建者&amp;nbsp;Boris Cherny 在 X 上发帖宣布了该消息。他写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;自 Claude Code 发布以来，我们发现用户将其用于各种非编码工作：例如进行度假研究、制作幻灯片、清理电子邮件、取消订阅、从硬盘恢复婚礼照片、监测植物生长、控制烤箱等等。这些应用场景丰富多样，令人惊喜——原因在于底层 Claude Agent 是最佳代理，而 Opus 4.5 是最佳模型。今天，我们非常激动地推出 Cowork，这是我们让 Claude Code 服务于所有非编码工作的第一步。该产品目前仍处于早期阶段，功能尚不完善，与 Claude Code 最初发布时的状态类似。Cowork 包含许多我们认为使其真正与众不同的创新用户体验和安全功能：内置虚拟机用于隔离、开箱即用的浏览器自动化支持、以及对所有非编码工作的支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa95ea01942eb8bff88234f7195c111d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Cowork 是一款基于 Claude Code 底层架构构建的全新产品，目前以“研究预览版”的形式，率先面向 macOS 平台上的 Claude Max 订阅用户开放。与传统对话式 AI 不同，Cowork 的核心定位并非“聊天”，而是“协作”：它试图让 Claude 从一个被动响应指令的助手，转变为能够理解任务、制定计划、持续执行，并与用户保持协同关系的智能工作体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从“对话助手”到“数字同事”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长期以来，大模型产品的主流交互形态仍然是对话。用户输入问题，模型生成回答；用户提出修改，模型再次响应。这种模式在信息查询、文本生成等场景下行之有效，但在真实工作流中却暴露出明显局限——上下文需要反复提供，文件需要人工整理，输出结果往往还要用户自行转换为可用格式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 试图解决的，正是这一断裂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 模式下，用户可以直接授予 Claude 对本地指定文件夹的访问权限。需要强调的是，这种访问并非“全盘授权”，而是由用户明确选择、逐一控制的结果。Claude 只能看到、读取、编辑或创建那些被允许的文件和目录，而无法触及任何未授权内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦获得权限，Claude 的能力边界就发生了质变。它不再只是基于文本上下文“想象”文件内容，而是可以直接操作真实存在的工作材料。例如，它可以扫描一个杂乱无章的下载文件夹，按照文件类型、时间或用途进行分类和重命名；可以从大量截图中提取关键信息，自动生成一份结构化的费用清单；也可以将零散的会议笔记、草稿和片段，整理成一份逻辑清晰的报告初稿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种能力的本质，并不是简单的“更聪明”，而是 Claude 被嵌入进了用户的实际工作环境之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在产品说明中多次强调，Cowork 的体验更接近“给同事布置任务”，而不是与机器人来回对话。一旦任务被下达，Claude 会自行拆解步骤、规划执行路径，并在执行过程中持续向用户同步进展。用户无需等待任务完成即可插入新的反馈或补充想法，这些指令会被自动排队、并行处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是 Cowork 与普通对话模式最根本的差异之一：它默认假设用户的工作是多线程的，而不是线性的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，“更自主”的能力，意味着更高的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让 AI 进入文件系统，甚至具备修改、创建和删除文件的能力，无疑是一种能力跃迁，同时也是风险跃迁。Anthropic 并未回避这一点，反而在产品介绍中反复提醒用户保持警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是操作层面的风险。如果收到明确指令，Claude 确实可以执行具有破坏性的操作，例如删除本地文件或批量修改内容。一旦指令本身存在歧义，或者模型误解了用户意图，后果可能是不可逆的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在 Cowork 中，Claude 在执行任何“重要操作”之前，都会主动征求用户确认。这种设计并非形式上的“弹窗提示”，而是希望用户在关键节点重新审视任务目标，必要时进行纠正或细化指令。Anthropic 也明确建议，在涉及高风险操作时，用户应提供尽可能清晰、具体的指示，而不是依赖模糊的自然语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一类更复杂、也更具行业共性的风险，是“提示注入”（Prompt Injection）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 的工作过程中，Claude 可能会接触来自互联网的内容，例如网页、文档或第三方信息源。如果这些内容中被恶意嵌入了指令，试图诱导模型偏离原本的任务计划，就可能引发安全问题。Anthropic 表示，他们已经构建了针对提示注入的多层防御机制，但也坦言，“代理安全”——即确保 AI 在现实世界中执行操作时的可控性——仍然是整个行业正在积极探索的前沿问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，Cowork 并不是一个“已经完全成熟”的产品，而更像是一次对未来工作方式的现实实验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 也明确指出，这些风险并非 Cowork 独有，而是所有具备“行动能力”的 AI 工具都会面临的问题。只是对许多用户来说，Cowork 可能是第一次接触到一个超越简单对话、真正能够影响本地环境的 AI，因此更需要建立正确的使用习惯和风险意识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研究预览版背后的产品逻辑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 目前被定义为“研究预览版”，这一定位本身就释放了明确信号：Anthropic 并不认为自己已经找到了最终形态，而是希望通过真实用户的使用反馈，加速产品迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据官方披露，Anthropic 计划在后续版本中引入多项重要改进。其中包括跨设备同步能力，使 Cowork 不再局限于单一终端；以及将其移植到 Windows 平台，从而覆盖更广泛的办公人群。同时，安全机制也将持续强化，尤其是在代理行为可解释性和可控性方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从产品路径上看，Cowork 与 Claude Code 之间存在清晰的继承关系。两者共享相同的底层架构，这意味着 Cowork 在能力上，理论上可以完成 Claude Code 已经证明可行的许多复杂任务。不同之处在于，Cowork 将这些能力重新封装为更偏向非技术用户的交互方式，降低了使用门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 Claude Code 面向的是“愿意为效率付出学习成本”的开发者群体，那么 Cowork 的目标人群显然更加广泛：内容创作者、产品经理、运营人员、行政人员，乃至任何需要与文件、资料和信息打交道的知识工作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在掌握 Cowork 的基本使用方式后，用户还可以进一步扩展 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是连接器。Claude 可以通过用户已有的连接器，访问外部信息源，从而将本地任务与外部数据打通。这使得 Cowork 不再只是一个“本地整理工具”，而是可以承担跨系统的信息整合角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次是新增的一系列技能。这些技能专门用于提升 Claude 在创建文档、演示文稿以及其他常见办公文件时的表现，使其输出更加贴近真实工作场景的格式和标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，如果用户在 Chrome 浏览器中将 Cowork 与 Claude 配对使用，Claude 还可以完成需要访问浏览器的任务。这一步，实际上进一步模糊了“对话 AI”“自动化工具”和“数字员工”之间的界限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从整体设计来看，Cowork 试图减少用户在“提供上下文”和“整理结果”上的认知负担。用户无需手动拼接背景信息，也无需将 Claude 的输出再加工成可用成果。更重要的是，用户不必为了等待 AI 完成某个任务而中断自己的工作节奏——任务可以被连续布置、并行执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在描述这种体验时，用了一个耐人寻味的比喻：这更像是给同事留言，而不是来回沟通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用户：没有Linux版本，差评！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 发布之后，迅速在开发者社区、AI 产品圈以及更广泛的知识工作者群体中引发讨论。与以往单纯围绕模型能力、跑分或价格的争论不同，这一次的焦点明显转向了一个更现实的问题：“AI 是否真的开始成为一个可以被信任、被授权的工作参与者？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上的最新讨论串里，有用户评论指出他们“很期待尝试这个功能”，认为 Anthropic 近来在产品和用户信任构建上做得不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/842f272a3e6e92967e9aabc5c537d87e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;**因为仅限 macOS 和订阅计划，部分用户感到遗憾。**在另一个 Reddit 讨论串中，有用户对 Cowork 的平台限制表达了不满或遗憾，评论集中在“只支持 macOS”这一点上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/8060c0e9f39720be6dc7c63fd4acf667.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，值得注意的是，有些评论虽然不是专门针对 Cowork，但有一些用户还是对 Anthropic 近期产品策略与沟通的不满，对 Cowork 的发布背景和用户关系具有间接关联语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 平台，有长期用户表示，自己已经从忠实支持者变成对 Anthropic 的信任下降甚至不满。该用户指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“作为很早一批用户，我原本极力推荐 Claude，但最近几个月感觉 Anthropic 的产品质量沟通都变差了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://claude.com/blog/cowork-research-preview&quot;&gt;https://claude.com/blog/cowork-research-preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/bcherny/status/2010809450844831752&quot;&gt;https://x.com/bcherny/status/2010809450844831752&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</link><guid isPermaLink="false">https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</guid><pubDate>Tue, 13 Jan 2026 01:30:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>活久见！连Linux之父等“顽固派”大佬，都在用AI编程了</title><description>&lt;p&gt;程序员中的超级“保守派”、Linux 之父Linus Torvalds，现在也用起了 AI 编程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b675a5ab379b540e9e7b3a6cb345baa0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图源：GitHub&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，Linus 在 GitHub 上悄悄上传了一个小项目。项目本身不大，但特别的是，它是他用一款谷歌系 AI 编程助手&amp;nbsp;进行&amp;nbsp;Vibe Coding&amp;nbsp;完成的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个仓库很快就被眼尖的网友挖了出来，目前已经收获了&amp;nbsp;1600+ 颗 Star。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8afd55b114c702572f76a00a2f8f99d7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 缔造的 Linux，与 Windows、macOS 一起，构成了当今计算世界的三大通用操作系统阵营之一。&lt;/p&gt;&lt;p&gt;不过他曾直言：“在过去将近 20 年里，我并没有从事编程工作。”这并不是他远离技术，而是早就从亲手写代码的人，转变成了为整个系统长期演进负责的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种角色下，这位老哥过去对“AI 帮你写代码”这套叙事，一直保持高度警惕甚至是嗤之以鼻——他关注重点的不是代码写得快不快，而是代码在多年之后是否还能被理解、维护和演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，Linus 对 AI 编程的态度可谓是“大转弯”：不仅开始亲自尝试 Vibe Coding，还公开表示自己对这种方式“相当积极”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些事情的冲击力并不在于“AI 又进步了”，而在于连最不吃 AI 编程这一套的人，也开始松动了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;反 AI 编程的“顽固派”们，也开始接受 Vibe Coding 了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生成式 AI 席卷软件行业的当下，有这么一群特殊的 “顽固派”， 他们定义了现代计算机的技术基石，却曾长期对 AI 编程嗤之以鼻，甚至公开泼冷水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 Linux 之父 Linus Torvalds、Java 之父 James Gosling、Redis 之父 antirez（Salvatore Sanfilippo），个个都是编程界的殿堂级人物。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但有意思的是，随着 AI 工具能力的突飞猛进，这群昔日的 “反 AI 先锋”，正以各自的方式重新划定 AI 的边界：有人有限度拥抱，有人批判中认可，还有人干脆彻底转身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Linus 老哥，之前对生成式 AI 一直保持观望的态度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并不否认 AI 的潜力，但极度厌恶围绕 AI 的过度炒作。在一次开源峰会上，他直言当前关于生成式 AI 的讨论“90% 是行销炒作，只有 10% 是现实”，并毫不掩饰自己的反感。正因为讨厌炒作，他选择在相当长一段时间内&amp;nbsp;主动忽略 AI 热潮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 之前一直没有使用各种 AI 编程工具。不过，这并不代表他对新范式抱有敌意。相反，他对&amp;nbsp;Vibe Coding 总体持正面态度，只是并未急于亲自下场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，随着工具逐渐成熟、噪音开始下降，Linus 也终于对 Vibe Coding 上手了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他用上了谷歌的智能体优先开发平台 Antigravity，靠 Vibe Coding 搞定了项目里的 Python 音频采样可视化工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从最初的 “搜索 + 照猫画虎”，到后来直接让 AI 写代码，甚至自定义组件，最终效果比他手写的还要好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对内核社区里 AI 生成补丁泛滥的争议，他的立场很清醒：问题不在于 AI 本身，而在于维护者是否真正理解代码、承担责任。在他眼里，AI 可以当帮手，但不能当甩手掌柜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而&amp;nbsp;Redis 创始人 Salvatore Sanfilippo（网名：antirez）&amp;nbsp;的转变更具戏剧性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位以 “简洁、可预测” 为信仰的系统级程序员，曾固执地坚持一行行手写代码，对自动化工具保持高度警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最近，他公开抛出了一句颠覆自己过往理念的话：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“对于大多数项目而言，除非是为了娱乐，现在自己写代码已经不再明智了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/35/359c6853cabb8f0a7b9e22d03877bc92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让他改口的，是实打实的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用 Claude Code 的过程中，他发现 AI 在极少人工干预的情况下，就能完成原本需要数周的系统级任务：修复 Redis 测试中的并发与时序问题、重写核心库、复现复杂的数据结构改动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更夸张的是，他只提出需求，Claude Code 5 分钟就生成了一个 700 行的纯 C 库，用于 BERT 类嵌入模型推理，性能仅比 PyTorch 慢约 15%；而他耗时数周完成的 Redis Streams 内部改动，AI 根据设计文档，20 分钟便复刻完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他坦言，对抗浪潮没什么意义，不如主动拥抱：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“忽略人工智能对你或你的职业生涯都没有好处。花几周时间仔细研究，而不是五分钟浅尝辄止。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 antirez 强调，这不是编程乐趣的终结，而是转移：“真正有趣的事情，已经从‘如何写代码’，变成了‘要做什么、为什么这样做’。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这位技术极客也没丢掉警惕性。他担忧 AI 技术的集中化风险。少数公司掌握核心能力，可能引发程序员失业、技术权力失衡等问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比前两位，Java 之父 James Gosling&amp;nbsp;的态度要尖锐得多。他多次炮轰，当前的 AI 热潮 “基本上是一场骗局”，AI 已经沦为“自带误导属性的营销术语”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，生成式 AI 编程的本质，不过是对已有代码和模式的重组，根本谈不上真正的创造力。那些看起来惊艳的演示，一旦碰上复杂项目就露馅：“刚开始接触氛围编程，会觉得它特别酷炫。可一旦项目变得稍微复杂一点，氛围编程就会很快耗尽开发者的脑力。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gosling 的核心质疑点很明确：AI 只能复刻见过的代码，但专业软件开发的精髓，在于开拓性的创新 —— 这些内容从来不在现成的代码库里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也没把话说死。他承认 AI 技术背后的数学与统计原理很复杂，也认可它的实用价值，不是取代程序员，而是 “生成没人愿意去写的文档”，或者解释现有代码的功能。说到底，AI 更像一个智能搜索引擎，而非编程大神。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还不忘吐槽一把资本：“科技行业里骗子和炒作者的数量之多，令人难以置信。风险投资者只关心成功获利，而不是开发出真正有用的技术。” 他甚至预言，“绝大多数 AI 投资都会被烧个精光。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说到底，这三位大佬的转变，都不是向 AI “投降”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们认可的，是 AI 在重复劳动上的效率；他们坚守的，是人类程序员不可替代的核心价值，对复杂系统的理解、对工程架构的判断、对长期维护的责任，以及开拓性的创新能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;对 Linux 内核开发，Vibe Coding 还欠火候&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要说明的是，虽然 Linus 现在对 Vibe Coding 的态度很积极，但他也直言称，这种方式&amp;nbsp;并不适用于 Linux 内核开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个重要原因在于，今天的计算机系统早已比他学习编程的年代复杂得多。Linus 曾回忆，当年他接触的一些输入程序，甚至是从计算机杂志上照着敲下来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然他已经很久没有深度参与具体功能编程，长期为整个内核的演进负责。在他的“系统维护者”视角下，稳定性、安全性和可维护性，远比“写得快不快”更重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一点，其实在他最近上传到 GitHub 的那个项目里有所体现：AI 主要写的只是对 Python 可视化工具部分，核心 C 语言部分（音频效果的数字信号处理等）还是他亲自写的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Linus 看来，Vibe Coding 在小项目和探索性场景中确实优势明显：进入门槛低、反馈速度快，能迅速把模糊的想法变成可运行的程序，用来生成样板代码、辅助脚本，或者“先跑起来看看”，都非常合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这种方式的短板同样明显——生成代码往往风格不稳定、抽象边界模糊、依赖隐性假设，短期能用，长期却很难维护。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 Linux 内核，恰恰是一个对“可维护性”极端苛刻的系统：代码需要被不同年代、不同背景的维护者反复阅读、修改和重构，任何一次“看起来省事”的生成式决策，都可能变成未来十年的技术债。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过话说回来，即便不能“全靠 AI 写代码”，“部分交给 AI”本身，就已经在重塑程序员的工作方式。&lt;/p&gt;&lt;p&gt;在另一条时间线上，有些工程师甚至已经开始用 AI 来开发 AI 本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Boris Cherny。作为 Anthropic 工程师、也是&amp;nbsp;Claude Code&amp;nbsp;的创造者，他已经几乎不再以传统方式写代码了，而是把自己打造的 AI 编程工具玩儿出了花：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他让 Claude Code 自己参与开发自己，然后竟在一年内完成了 1096 提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7fda4d0dceb79c4ffd024e6d5db386b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个工具已成为全球最受欢迎的 AI 编程工具之一，去年还给 Boris 带来了超过&amp;nbsp;10 亿美元（约合人民币 70 亿元）&amp;nbsp;的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://github.com/torvalds/AudioNoise&lt;/p&gt;&lt;p&gt;https://www.theregister.com/2025/11/18/linus\_torvalds\_vibe\_coding/&lt;/p&gt;&lt;p&gt;https://antirez.com/news/158&lt;/p&gt;&lt;p&gt;https://www.bnext.com.tw/article/81200/linus-torvalds-gen-ai-bubble&lt;/p&gt;&lt;p&gt;https://x.com/bcherny/status/2009072293826453669&lt;/p&gt;</description><link>https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</link><guid isPermaLink="false">https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</guid><pubDate>Tue, 13 Jan 2026 01:16:08 GMT</pubDate><author>木子,高允毅</author><category>生成式 AI</category></item><item><title>刚刚，DeepSeek 突发梁文峰署名新论文：V4 新架构提前曝光？</title><description>&lt;p&gt;今天凌晨，喜欢闷声做大事的 DeepSeek 再次发布重大技术成果，在其 GitHub 官方仓库开源了新论文与模块 Engram，论文题为 “Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models”，梁文锋再次出现在合著者名单中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1e/1edd54354aa449ddce92b7a3365002b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统的大模型架构相比，该方法提出了一种新的“查—算分离”机制，通过引入可扩展的查找记忆结构，在等参数、等算力条件下显著提升模型在知识调用、推理、代码、数学等任务上的表现。代码与论文全文均已开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram&quot;&gt;https://github.com/deepseek-ai/Engram&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种查和算分离的Engram新方法的整体架构如下图所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba205b29dd447e4968a93381616947d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么需要 Engram？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，我们为什么需要 Engram ？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前主流的大语言模型架构依然基于 Transformer 和 Mixture-of-Experts（MoE）结构。MoE 是目前推进参数规模和能力扩展的关键技术之一，通过动态路由机制，只激活部分参数以降低计算成本，同时在任务容量方面实现大规模扩展。DeepSeek 自家系列模型（如 DeepSeek V2、DeepSeek V3 等）也采用了先进的 MoE 方法进行扩展训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这些传统的 Transformer 架构（无论是 Dense 还是 MoE）中，模型的参数实际上承担着两种截然不同的角色：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实性记忆（Memorization）： 存储海量的知识事实。例如，“法国的首都是哪里？”、“世界最高的山脉是哪座”等。这类信息相对死板，更多依赖于“查表”式的检索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;逻辑推理与计算（Calculation）： 负责复杂的逻辑链条、多步推理和情境理解。例如，“根据这段代码的逻辑推导可能的 Bug”、“解析一段复杂的哲学论证”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前的大语言模型倾向于将这两者混在一起。当你试图让模型记住更多知识时，你不得不增加参数量。而在传统的 Dense 模型中，参数量增加意味着前向传播时的计算量（FLOPs）也会同步激增。MoE 架构虽然通过稀疏激活解决了“算力随参数同步爆炸”的问题，但 DeepSeek 研究发现，MoE 专家在处理“死记硬背”的任务时依然不够高效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;神经网络本质上是连续的数学变换，用高昂的矩阵运算去模拟简单的“查表检索”，本身就是一种极大的浪费。DeepSeek 的 Engram 正是为了打破这一困境——“该查表的查表，该算的算”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Engram 的核心思想与架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;聚焦到问题本身，Engram 方法为什么能解决上述问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Engram”一词源自神经科学，意为“记忆痕迹”，它是一个可扩展、可查找的记忆模块，用于语言模型在推理过程中过去可能已经见过的模式或片段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 的核心技术之一是现代化的哈希 N-Gram 嵌入（Modernized Hashed N-gram Embeddings）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统方式： 模型通过多层自注意力（Self-Attention）和 MLP 层的非线性变换，反复提取输入文本中的特征。Engram 方式： 它对输入的 Token 序列进行 N-Gram（连续 N 个词）切片，并利用哈希算法将这些片段映射到一个巨大的、可学习的查找表（Lookup Table）中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于采用哈希索引，这种查找是确定性且 O(1) 时间复杂度的。这意味着无论模型存储了多少万亿个记忆片段，检索的速度几乎是恒定的，且算力消耗极低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;O (1) 的含义是： 一次查找的耗时是常数级的，与 N-gram 表的规模无关。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也就是说，这种设计本质上将一部分“记忆职责”从深度神经计算中卸载出来（例如序列模式、固定知识段的识别与回填），使得模型既拥有活跃神经通道（例如 Transformer + MoE）处理复杂计算，也有静态记忆通道高效处理固定模式，这就是所谓的 “稀疏性的新轴”（a new axis of sparsity）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说就是 MoE 负责：“计算密集”神经推理与复杂组合功能、Engram 负责：“记忆查找”固定模式以及模式重建，两者协同构成一个更高效的整体架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它还具备条件记忆（Conditional Memory）。与简单的静态查找表不同，Engram 是“条件化”的。它会根据当前上下文的隐向量（Hidden States）来决定提取哪些记忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在架构设计上，Engram 模块位于 Transformer 层的早期阶段。它负责“模式重构（Pattern Reconstruction）”，即在计算层（MoE 或 Dense）开始干活之前，先把相关的背景事实和历史模式检索出来，作为“素材”喂给后续的逻辑层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它与 MoE（Mixture of Experts）的关系是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文特别指出：Engram 提供了一个新的稀疏性轴，与 MoE 的条件计算不同，它通过条件查找提供静态记忆容量。下面图表中从目标、计算方式、优化方向和作用位置四个维度解释了Engram 和 MoE的区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，DeepSeek 将 Engram 与 MoE 结合，形成了一个双系统：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 模块： 负责海量知识点的“存储与快速检索”。MoE 专家： 摆脱了沉重的记忆负担，全身心投入到“逻辑推理与合成”中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种分工极大地优化了参数效率。在 27B 的实验模型中，Engram 模块可以占用大量的参数用于记忆，但在实际推理时，它只消耗极少的计算量（FLOPs）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/258f97af1beff0b2daba80ee30e0a5e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：V4 将采用这种架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit、X和其他平台的相关帖子中，Engram 的技术核心受到了不少用户的肯定和技术肯定。众多网友认为这个模块的特点在于让模型架构处理“记忆模式查找”和“神经计算推理”两块职责分离，从而开启了新的稀疏性方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 平台有用户评论说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&amp;nbsp;“Engram嵌入方法很有意思。大多数模型仅通过MoE进行扩展，但Engram增加了静态记忆作为补充的稀疏性轴，查找复杂度为O(1)。他们发现 MoE 和 Engram 之间存在 U 形缩放规律，这指导着如何在两者之间分配容量。分析表明，这减轻了早期层级静态模式重建的压力，从而保留了用于复杂推理的深度。确定性寻址意味着它们可以将嵌入表卸载到主机内存中，而不会增加太多推理开销。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0f96995f96abcc3dd5d2e6ca94b84029.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，有用户对这种基于 n-gram lookup 的机制表达了直观兴趣，他评论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“即便是在不依赖 GPU 的环境下也能实现这种 O(1) 查找方式，让不少开发者对本地部署这样的大模型功能有了更实际的期待。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b75e18cfe6ca43ea711351ec52468e1f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在部分技术性评论中，有人指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;即从已有技术逻辑来看，在 LLM 中加入静态记忆查找似乎是“顺理成章”的发展方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这类观点反映了一个重要观点：专家群体开始从纯参数扩张思维转向更“智能”的架构设计，包括查表式模块和神经网络的协同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不少高级开发者在讨论中进一步提到，这种设计在理念上类似于对传统 NLP 技术（如 n-gram embedding）的现代化转换，结合了高效寻址机制（deterministic addressing）和神经推理模块，这种组合在纸面上看具有较高的可行性和实用性（这一点正是 Engram 的核心贡献）。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一条社区评论指出，Engram 很可能是 DeepSeek 即将发布的 V4 模型的核心技术基础：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;业内观察者认为 Engram 模块可能会成为 DeepSeek V4 的重要组成部分，并预示 DeepSeek 下一代模型会在记忆和推理协同上实现架构级提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X平台，也有网友表达了同样的猜测，认为V4 也将采用这种架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6ba93d5100cbacfee2b85d806c7ebc87.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友调侃，原本想抄袭下谷歌的技术，但现在要抄袭DeepSeek了，因为它比谷歌更好！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b09f2dec8c0db4892ed0b417422d8754.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友表示，其实Meta之前也有过类似想法，但用到的技术不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9b474fc250b9518bd87e5af00a84076e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/scaling01/status/2010748516788777445&quot;&gt;https://x.com/scaling01/status/2010748516788777445&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</link><guid isPermaLink="false">https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</guid><pubDate>Tue, 13 Jan 2026 00:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>阿里云把“华强北”们推向CES</title><description>&lt;p&gt;今年的&amp;nbsp;CES，中国硬件又一次成为主角。活跃在拉斯维加斯展台上的诸多出海产品，背后依托的是深圳的研发效率与供应链能力，而其智能化核心，则越来越多建立在以&amp;nbsp;Qwen&amp;nbsp;为代表的多模态、全尺寸的大模型基础上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与沙漠赌城的&amp;nbsp;CES&amp;nbsp;同期，在深圳蛇口，阿里云也举办了一场智能硬件展。这场展会面向公众免费开放，选址于本地居民日常散步、观海和看展的滨海文化地标，却意外成为&amp;nbsp;AI&amp;nbsp;硬件从实验室走向真实市场的缩影。1000&amp;nbsp;余款智能硬件在这里集中亮相，其中超过&amp;nbsp;200&amp;nbsp;款与&amp;nbsp;CES&amp;nbsp;同款甚至首发。这里既有来自北京、杭州的创新团队，也有来自义乌、华强北等产业带的制造与渠道力量——他们对技术趋势的嗅觉，向来快过任何市场报告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术验证与市场反馈在同一空间同时发生。在这里你可以听到合作方直接询价“多少钱，做OEM吗，能做多少套”，也可以看到消费者直接下单，把&amp;nbsp;399&amp;nbsp;元的&amp;nbsp;AI&amp;nbsp;玩具带回家。许多普通家庭第一次在这里集中体验到能对话的毛绒玩具、教用户跳舞的镜子、能翻跟头的机器狗，和具备实时提醒能力的&amp;nbsp;AI&amp;nbsp;眼镜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在&amp;nbsp;2024&amp;nbsp;年云栖大会上，阿里云董事长吴泳铭就明确指出，未来&amp;nbsp;AI&amp;nbsp;最大的想象力会来自于物理世界：“我们不能只停留在移动互联网时代去看未来，深层次AI最大的想象力绝对不是在手机屏幕上做一两个超级APP，而是接管数字世界，改变物理世界。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在这轮&amp;nbsp;AI&amp;nbsp;硬件浪潮中，阿里云没有选择去做终端硬件的制造者，而是以软硬一体的融合理念，向产业提供底层模型能力、云基础设施与生态支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据显示，通义大模型的多模态能力已深度赋能超过&amp;nbsp;15&amp;nbsp;万家智能硬件厂商。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d28fc8e8d4409f5e48b4f7b2dea413ff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从雷鸟的&amp;nbsp;AI&amp;nbsp;眼镜、听力熊的儿童&amp;nbsp;AI&amp;nbsp;Pin，到优必选机器人、趣丸科技的生成式&amp;nbsp;AI&amp;nbsp;吉他，这些走进全球家庭的产品背后，都能看到以通义为代表的阿里云基础设施的支撑。而它们从概念到量产、从深圳到世界的惊人速度，也再次印证了深圳这座“硬件硅谷”在研发、供应链与商业化效率上的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;For&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的&amp;nbsp;AI&amp;nbsp;硬件&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;逛完阿里云通义智能硬件展，一个强烈的感受是，这是我经历过为数不多，能让普通人玩得开心、让创业者看到机会、让厂商验证商业模式，同时清晰传递主办方战略意图的展会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云租下深圳海上世界文化艺术中心三层空间，用一种近乎“生活化”的方式，向公众展示：AI能长在玩具里、眼镜上、健身镜中，甚至成为家庭一员的日常存在。向企业展示：你能快速依托阿里云的生态，快速做出能进入全球家庭的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展会围绕两条主线展开：一是呈现阿里云的底层能力，二是展示其赋能下的千款智能硬件成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一楼以“智能中枢”为核心，展示通义大模型的能力：观众上传一张照片，就能生成一段短视频；走过一段互动迷宫，便能直观感受多模态AI如何理解图像、语音和动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能中枢周围环绕着“创造有AI”“生活有AI”“AI实训营”等主题区，OPPO、理想、影石等品牌在此展示手机、智能座舱和AI影像设备，而像趣丸科技的AI吉他、Looki这样的新奇产品，则让人看到AI如何重塑音乐、娱乐等日常互动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/27e1fe905091def2e73df36bc3297948.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;趣丸科技与阿里云合作推出的全球首款生成式AI吉他TemPolor&amp;nbsp;Melo-D，在通义大模型的支持下，重新定义了人与音乐的交互方式，提供了个性化的AI音乐创作体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三楼聚焦陪伴、健康与安防，专设义乌厂商展区；四楼覆盖家居、教育、健身等提效场景，华强北的硬件老板们也把“一米柜台”搬到了现场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea7a53958edc4cb3f43237c85e9cedcd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;通义联合听力熊为青少年定制随身AI对话智能体，打造国内首款儿童&amp;nbsp;“AI&amp;nbsp;Pin”&amp;nbsp;Mooni&amp;nbsp;M1，提供多种角色选择。经过通义千问大模型加持，用户的&amp;nbsp;AI&amp;nbsp;使用时长提升40分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云想让大家知道，AI有能力在所有场景里带来更好的体验。它同时也呈现出一种可能——不管是软件应用还是硬件产品，每个人都可以在这个时代搭建些什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c18195f4e17f32ac1f9330d8a1ccb08f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;阿里云&amp;nbsp;AI&amp;nbsp;实训营的 Agent 硬件搭建小课堂&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于普通人来说，硬件展是一个游戏体验。孩子和AI毛绒玩具对话，年轻人跟着镜子学舞，有人让AI解读运势、推荐香水，还有中学生在阿里云&amp;nbsp;AI&amp;nbsp;实训营中搭建了自己的第一个交互硬件。我们这代人仍然处于有“AI硬件”概念的时期，而对于下一代人来说，可能已经不存在“AI硬件”。当生活总所有一切都有AI，AI之于人，阿里云之于硬件和应用产品，就是水之于人的存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对创业者和企业主而言，展会成了高效的信息源。有用户的直接提问和反馈，也有工程师在展位前递上简历。采购顾问带着非洲、拉美的客户穿梭其间，现场询价、谈订单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e4/e4fd0f58e096c36684adf9311b2e1962.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL、影石、安克创新的案例，更是为想要入局AI硬件和出海的企业打气——依托阿里云全球全栈AI基础设施，大型制造企业可实现研发、服务、出海一体化，新锐品牌也能快速站稳全球舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚在CES获得Best&amp;nbsp;of&amp;nbsp;Innovation奖项的影石，依托&amp;nbsp;Qwen-VL&amp;nbsp;实现视频与图片的分类打标和场景识别，结合&amp;nbsp;Qwen-Plus&amp;nbsp;生成剪辑脚本，赋能全球百万视频创作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60f1d931f1cedfe5b371d8454ec23495.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;安克创新依托阿里云“全球一张网”，实现跨境资源调度与合规部署，核心系统互访提速30%，并将&amp;nbsp;Qwen&amp;nbsp;与&amp;nbsp;Wan&amp;nbsp;深度融入语音助手、多模态交互等产品功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa1a742f457dce5707bd11d956d5ace3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL则基于通义大模型打造了半导体显示专家系统&amp;nbsp;X-Intelligence，支撑其全球研发体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，阿里云把义乌、华强市场这些产品背后的“制造和分发网络”呈现在大家面前。在他们的摊位上，你可以看到很多产品尽管“粗糙”，却仍然有市场。在很多欠发达国家，AI硬件需要的不是精致，而是先以成本最低的方法被用上。很多义乌玩具、小3C产品的批发商，嗅到AI风潮后，已经在深圳有了自己的硬件工厂。华强科技生态园等孵化器，也开始重点招募AI硬件的创业公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如阿里云智能集团通义大模型业务总经理徐栋所说：“这样一个平台（以通义多模态交互开发套件为代表的AI硬件赋能平台）是我们非常重要的业务的选择，我们需要更多贴近阿里云的智能硬件开发伙伴。很多场景是碎片化的，只有做更贴近实际的生产环节、消费环节，每个人对AI硬件的体验才能更深。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，正在成为&amp;nbsp;for&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的日常现实。而阿里云的角色，不是站在台前造产品，而是站在幕后，让创新更快实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阿里云，在&amp;nbsp;AI&amp;nbsp;硬件变革前夜&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件从极客圈层走向大众日常，标志着市场已从“启蒙期”进入“挑剔期”。当用户开始为AI服务付费、并将设备融入日常生活，产品的成败就不再取决于功能数量，而在于能否持续兑现可感知的价值——这要求厂商必须拥有一套覆盖模型、工程、服务与生态的系统性能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，特别是在消费级市场，正经历着一场根本性的转型。从传统的联网设备到如今的“端侧智能体”，AI不再只是硬件的附加功能，而是直接决定产品核心价值的引擎。这一转变的核心标志在于：AI&amp;nbsp;不再作为附加功能嵌入硬件，而是成为产品定义、体验构建与价值交付的底层引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期智能硬件以“连接+控制”为基本范式，其智能化主要体现在远程操作与数据回传；而新一代AI硬件则要求设备具备持续感知、上下文理解、自主决策与协同执行的能力，成为一个能在真实场景中与用户形成闭环互动的“智能体”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一转变正在重塑硬件的设计逻辑、用户的价值预期与厂商的技术路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用对AI硬件的认知早已超越“新奇感”，转而关注端到端体验是否流畅、可靠、有用。更重要的是，用户开始愿意为持续服务付费。例如按月订阅儿童AI陪伴内容，或为高级健身指导功能续费。这催生了“硬件+服务”的新商业模式，但也带来新挑战，如果AI不能提供可感知的显性价值，订阅就难以为继。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术架构也随之重构。端云协同的逻辑发生了变化。之前的端云协同更多指向算力分工，即端上承载不了的算力放在云上，但现在的端云协同是指能力互补。安全、延时、功耗的问题必须在端上解决，而生态打通这些能力可能在云上做。同时，交互方式正走向“无感化”——不是让用户察觉不到AI存在，而是让使用门槛足够低，无需学习就能自然融入原有生活节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，对大多数硬件厂商而言，这场转型并不轻松。模型迭代速度远超硬件研发周期，而一个产品往往需要组合多个模型才能实现完整功能，集成复杂度陡增。与此同时，Agent架构、工具链和工程平台快速演进，传统硬件团队难以跟上软件层的节奏。更棘手的是，许多厂商擅长制造和渠道，却缺乏用户运营、数据闭环和订阅服务能力，难以构建可持续的商业模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对这些系统性挑战，阿里云提供了AI硬件的全链路支持体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在基础设施层面，阿里云面向&amp;nbsp;AI&amp;nbsp;应用场景全面升级计算、存储与网络能力，为高并发、低延迟的智能硬件业务提供稳定底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在模型层面，通义大模型家族（包括&amp;nbsp;Qwen3、Qwen-VL、QwQ&amp;nbsp;等）全面开源，并提供闭源高阶版本，同时接入第三方优质模型，帮助厂商一站式、低成本调用全球先进&amp;nbsp;AI&amp;nbsp;能力。针对多模态交互场景，阿里云还推出专有优化模型，降低端到端语音和视频交互时延。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云的模型能力，已经获得顶尖手机、汽车、具身智能、智能配件品牌的认可和验证：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，全球&amp;nbsp;Top&amp;nbsp;10手机厂商已都在使用阿里云的大模型能力。例如，OPPO利用阿里云人工智能平台&amp;nbsp;PAI&amp;nbsp;对&amp;nbsp;Qwen&amp;nbsp;开源模型进行后训练，以支持其AI多场景应用；荣耀则联合阿里云百炼打造&amp;nbsp;VQA&amp;nbsp;端到端方案，图片细分场景识别率提升近40%，延迟降低30%。荣耀Magic&amp;nbsp;V5&amp;nbsp;接入飞猪旅行、高德地图两个垂直Agent&amp;nbsp;两个月即斩获百万级用户好评。基于“模型+工程+生态”三位一体的战略，阿里云正持续加速手机行业的AI功能创新与规模化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/3f/16/3f3f3ec409b7a511c1dbc3d97b9f5f16.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想汽车基于阿里云MindGPT大模型，整合高德、飞猪、支付宝等生态，实现全球首个“车机AI扫码支付”；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/70/e6/70de20e17376b8709151305c4cbb4fe6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷鸟创新联合阿里云推出行业首个面向智能眼镜的AI大模型，意图识别准确率达98%，搭载该模型的雷鸟眼镜出货量领跑AR行业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/fd/10/fdb4be397f9d2375b7950bb10c61ec10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优必选的萌&amp;nbsp;UU&amp;nbsp;陪伴机器人，搭载通义千问与自研情感智能体“点灵”，且具有长期记忆&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90b730812c7aeda3120052d8ccd77d00.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得注意的是，阿里云此次还推出了全模态智能交互开发套件，将上述能力封装为标准化工具。该套件适配&amp;nbsp;30&amp;nbsp;多款主流&amp;nbsp;ARM、RISC-V&amp;nbsp;和&amp;nbsp;MIPS&amp;nbsp;架构芯片，覆盖市面上绝大多数终端设备。未来，通义大模型还将与玄铁&amp;nbsp;RISC-V&amp;nbsp;实现软硬全链路协同优化，进一步提升在国产芯片上的部署效率与推理性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b84bac627e24c2d372a32ada8ec958ea.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套开发套件不仅提供基础能力，还预置十余款&amp;nbsp;MCP&amp;nbsp;工具和&amp;nbsp;Agent，覆盖生活、工作、娱乐、教育等高频场景。例如，基于出行规划&amp;nbsp;Agent，用户可直接调用路线规划、旅行攻略、本地探索等功能。同时，套件深度集成阿里云百炼平台生态，支持开发者添加社区模板，或通过&amp;nbsp;A2A&amp;nbsp;协议兼容第三方&amp;nbsp;Agent，极大扩展了应用边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99dcc21e3fc0751d1c9fa8ff12bf4c50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是&amp;nbsp;OPPO、理想这样的品牌厂商，还是华强北的创客、义乌的出海团队，甚至“一人公司”，都能借助阿里云的解决方案快速验证想法、打造产品，并参与全球竞争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是阿里云“基础设施先行”的思路，让展会上那些看似天马行空的产品，得以从概念走向量产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有趣的是，阿里云大模型能力的升级节奏，与AI硬件的集中爆发高度同步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2023年8月，阿里云开源Qwen-VL视觉语言模型，首次让中小厂商能免费调用工业级多模态能力；2024年，Qwen-Audio、Qwen2-VL等模型集中发布，补齐了语音、图像与文本融合交互的关键拼图；到2025年初，原生端到端的Qwen3-Omni模型的发布，以及Qwen-Agent，进一步支持硬件端构建任务型智能体。这一连串技术释放，恰好为AI硬件创新提供了可落地的底层支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从2024年下半年起，阅读器、眼镜、耳机、学习机等细分品类迎来AI功能的规模化落地：文石、闪极、AIxFU、听力熊、云希谷等能纷纷接入阿里云大模型能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些产品的共同点，是都受益于通义的“全谱系开源”策略——0.5B到480B的模型全覆盖，文本、语音、视觉、视频能力一应俱全。无论是大型企业，还是华强北的硬件作坊，都能找到适合自己的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是这种低成本接入到快速验证的正向循环，让AI硬件从概念走向规模化落地。阿里云没有造AI硬件产品，却通过持续开源和能力迭代，成为这场硬件浪潮背后最坚实的推手。&lt;/p&gt;</description><link>https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</link><guid isPermaLink="false">https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</guid><pubDate>Mon, 12 Jan 2026 10:42:48 GMT</pubDate><author>陈姚戈,王一鹏</author><category>AI&amp;大模型</category></item><item><title>英伟达发布了跨AI、机器人和自动驾驶的开放模型、数据集和工具</title><description>&lt;p&gt;英伟达（NVIDIA）&lt;a href=&quot;https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/&quot;&gt;发布&lt;/a&gt;&quot;了一套涵盖语言、智能体系统、机器人技术、自动驾驶和生物医学研究的开放模型、数据集和开发工具。此次更新扩展了多个现有的NVIDIA模型家族，并通过GitHub、Hugging Face和NVIDIA的开发者平台提供了相应的训练数据和参考实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在代理式AI领域，NVIDIA扩展了Nemotron模型家族，为语音识别、检索增强生成和安全提供了新的组件。Nemotron Speech包括针对低延迟、实时用例优化的自动语音识别模型。Nemotron RAG引入了用于多模态文档搜索和检索流程的嵌入和重排视觉语言模型。Nemotron Safety增加了用于内容过滤和敏感或个人身份信息检测的更新模型。NVIDIA还发布了用于选定Nemotron模型的数据集和训练代码，包括在公共基准上评估的嵌入模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于机器人技术和物理AI，NVIDIA引入了新的Cosmos世界基础模型，这些模型支持在真实环境中的感知、推理和合成数据生成。Cosmos Reason 2是一个多模态推理模型，旨在增强智能体在物理环境中操作的场景理解。Cosmos Transfer 2.5和Cosmos Predict 2.5专注于在不同环境和条件下生成合成视频数据，支持仿真和数据增强工作流程。基于Cosmos，NVIDIA发布了Isaac GR00T N1.6，这是一个用于人形机器人的开放视觉-语言-动作模型，支持全身控制并将视觉感知与动作规划集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公告的一个组成部分是NVIDIA Alpamayo，一个用于基于推理的自动驾驶的新开放模型家族。Alpamayo结合了感知、规划和可解释性，采用视觉-语言-动作架构，并与仿真工具和大规模驾驶数据集相匹配。NVIDIA还引入了AlpaSim，这是一个用于自动驾驶汽车模型闭环评估的开源仿真框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据NVIDIA汽车部门负责人吴信洲&lt;a href=&quot;https://www.linkedin.com/posts/xinzhouw_level-2-autonomous-driving-in-san-francisco-activity-7414325238630342656-moGb?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAACX5yoEBhsg1xPtc5iaJXHCu_Rv298CmfZA&quot;&gt;表示&lt;/a&gt;&quot;，Alpamayo和相关工具反映了跨研究、模拟、数据工程、安全和集成团队多年的开发努力。吴指出，这项工作涉及广泛的道路测试、使用Cosmos等平台进行持续的大规模模拟，以及与包括梅赛德斯-奔驰在内的汽车合作伙伴的紧密合作，计划在即将推出的量产车辆中进行初步部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;医疗保健和生命科学更新通过新的NVIDIA Clara模型提供。这些包括用于原子级蛋白质设计的La-Proteina，用于合成感知药物设计的ReaSyn v2，用于早期安全和相互作用预测的KERMT，以及用于RNA结构建模的RNAPro。NVIDIA还发布了一个包含45.5万个合成蛋白质结构的数据集，以支持该领域的训练和评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有模型和数据集均在开放许可下发布，可通过GitHub和Hugging Face访问。NVIDIA表示，许多模型还被打包为NIM微服务，以便在从本地推理环境到云基础设施的NVIDIA加速系统上部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/nvidia-open-models/&quot;&gt;https://www.infoq.com/news/2026/01/nvidia-open-models/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</link><guid isPermaLink="false">https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</guid><pubDate>Mon, 12 Jan 2026 09:12:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>英伟达</category><category>AI&amp;大模型</category><category>跨端开发</category></item><item><title>MongoBleed漏洞允许攻击者从MongoDB的堆内存中读取数据</title><description>&lt;p&gt;MongoDB最近修补了&lt;a href=&quot;https://www.mongodb.com/community/forums/t/important-mongodb-patch-available/332977&quot;&gt;CVE-2025-14847&lt;/a&gt;&quot;，这是一个影响多个支持和遗留MongoDB服务器版本的漏洞。根据披露，该漏洞可以被未认证的攻击者以较低的复杂度远程利用，可能导致敏感数据和凭证的外泄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞被称为MongoBleed，以臭名昭著的&lt;a href=&quot;https://en.wikipedia.org/wiki/Heartbleed&quot;&gt;Heartbleed&lt;/a&gt;&quot;命名，CVSS得分为&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2025-14847&quot;&gt;8.7&lt;/a&gt;&quot;，由对zlib压缩网络流量处理不当触发，允许未经身份验证的攻击者泄露未初始化的内存，并可能从受影响的MongoDB服务器窃取敏感数据，如凭证或令牌。根据&lt;a href=&quot;https://www.wiz.io/blog/mongobleed-cve-2025-14847-exploited-in-the-wild-mongodb&quot;&gt;Wiz&lt;/a&gt;&quot;的安全研究人员，该漏洞正在被广泛利用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如MongoDB的声明所述，MongoDB Atlas上的托管实例已经被修补，但是如果自托管MongoDB不更新，仍然存在风险。强烈建议组织立即应用安全补丁，或禁用压缩并限制网络暴露。Merav Bar、Amitai Cohen、Yaara Shriki和Gili Tikochinski解释：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;CVE-2025-14847源于MongoDB服务器基于zlib的网络消息解压缩逻辑中的一个缺陷，该逻辑在认证之前进行了处理。通过发送畸形的压缩网络数据包，未经身份验证的攻击者可以触发服务器错误处理解压缩的消息长度，导致返回给客户端未初始化堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Wiz文章，42%的云环境中至少有一个易受攻击的MongoDB实例，Censys报告称全球大约有87,000台服务器存在潜在的风险。由于该漏洞可以在没有认证或用户交互的情况下被利用，暴露在互联网上的数据库服务器面临特别高的风险。Wiz团队补充道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在代码层面，这个漏洞是由message_compressor_zlib.cpp中的错误长度处理引起的。受影响的逻辑返回了分配的缓冲区大小（output.length()），而不是实际解压缩数据的长度，从而允许过小或畸形的有效载荷暴露相邻的堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞影响了自&lt;a href=&quot;https://github.com/mongodb/mongo/pull/1152&quot;&gt;2017&lt;/a&gt;&quot;年以来发布的所有MongoDB版本。Linkfields Innovations的软件开发人员Gourav Boiri&lt;a href=&quot;https://www.linkedin.com/posts/bgourav2287_cybersecurity-mongodb-infosec-activity-7411163710372835328-DQQh&quot;&gt;评论道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MongoBleed突出了即使是成熟的数据库，当暴露或打补丁时，也可能成为关键的攻击面。预认证内存泄露、主动漏洞攻击和87K+暴露实例——提醒我们，数据库安全就是基础设施安全。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在“&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;简单解释MongoBleed&lt;/a&gt;&quot;”的文章中，&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;Stanislav Kozlovski&lt;/a&gt;&quot;解释了这一漏洞的工作原理，并警告说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;它非常容易被利用——只需要连接到数据库（不需要认证）。截至撰写本文时，它已经被修复，但一些EOL版本（3.6、4.0、4.2）将不会得到修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoSec创始人和实践者&lt;a href=&quot;https://www.linkedin.com/in/ecapuano/&quot;&gt;Eric Capuano&lt;/a&gt;&quot;解释了&lt;a href=&quot;https://blog.ecapuano.com/p/hunting-mongobleed-cve-2025-14847&quot;&gt;如何从日志中检测数据库服务器是否被利用&lt;/a&gt;&quot;。在一个流行的&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1py2c0w/mongobleed_vulnerability_explained_simply/&quot;&gt;Reddit&lt;/a&gt;&quot;帖子中，用户misteryuub争论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;很多人争论说开源代码比闭源代码更安全，或者安全问题会在开源代码中更快被发现。这种级别的漏洞存在是对这个论点的反驳。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kozlovski不同意：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当人们说开源更安全时，他们通常指的是有活跃社区的开源项目。Mongo在2017年似乎没有这个，因为引入这个漏洞的PR没有在公共GitHub上被审查。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mongodb.com/try/download/community&quot;&gt;MongoDB补丁版本&lt;/a&gt;&quot;现在可用于从4.4到8.0的所有支持版本。&lt;a href=&quot;https://www.percona.com/blog/urgent-security-update-patching-mongobleed-cve-2025-14847-in-percona-server-for-mongodb/&quot;&gt;像Percona Server for MongoDB这样的分支也受到上游漏洞的影响&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&quot;&gt;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</link><guid isPermaLink="false">https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</guid><pubDate>Mon, 12 Jan 2026 08:23:00 GMT</pubDate><author>Renato Losio</author><category>云端开发</category><category>数据库</category></item></channel></rss>