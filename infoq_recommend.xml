<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Sat, 07 Feb 2026 10:05:27 GMT</lastBuildDate><ttl>5</ttl><item><title>AI 智能体界的 npm 来了！Vercel 推出 Skills.sh，欲统一智能体指令集</title><description>&lt;p&gt;Vercel 最近发布了开源项目 &lt;a href=&quot;https://vercel.com/changelog/introducing-skills-the-open-agent-skills-ecosystem&quot;&gt;Skills.sh&lt;/a&gt;&quot;，想要给 AI 智能体（Agents）配上一套“标准动作库”。简单来说，它让智能体能通过命令行执行各种可复用的操作，也就是所谓的“技能”（Skills）。Vercel 将其定义为一个&lt;a href=&quot;https://skills.sh/&quot;&gt;开放的智能体技能生态系统&lt;/a&gt;&quot;，开发者可以在这里定义、分享并运行一个个独立的指令，供智能体在工作流中随时调用。这一工具的核心逻辑，是把智能体的“推理”和“执行”分开——让智能体去调用那些受控、预定义的命令，而不是由它自己去瞎猜、乱写 shell 逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术架构上，Skills.sh 充当了一个轻量级运行时环境，允许智能体调用以 &lt;a href=&quot;https://learn.microsoft.com/en-us/powershell/utility-modules/aishell/concepts/what-is-a-command-shell?view=ps-modules&quot;&gt;shell 脚本形式&lt;/a&gt;&quot;实现的各种技能。每一项技能都遵循简单的契约协议，明确定义了其输入、输出和执行行为。这使得智能体能够以一种可预测、可审计的方式执行各项任务，例如读取或修改文件、运行构建步骤、调用 API 或查询项目元数据。由于技能具有显式定义和版本控制的特性，开发团队可以更清晰地了解智能体被授权的操作范围，并在开发或生产环境中对其行为进行审查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些技能的设计兼顾了本地开发与自动化环境（如 CI 流水线）。开发者可以在&lt;a href=&quot;https://skills.sh/docs&quot;&gt;本地机器安装 Skills.sh&lt;/a&gt;&quot; 直接运行技能，同时将相同的技能无缝集成到由智能体驱动的工作流中。这种一致性旨在减少从实验阶段转向结构化应用场景时的阻力。此外，技能通过简单的配置文件进行描述，这使得开发者无需引入额外的框架或沉重的依赖库，即可轻松地检查、扩展或自定义功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vercel 将该生态系统定位为开源及社区驱动。开发者可以发布自己的技能，并复用他人创建的成果，从而形成一个共享的常用智能体动作库。根据公司分享的早期使用数据，该项目在发布后迅速获得了广泛关注，安装量据报已达数万次。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社区评论更多地聚焦于该方案的实用性而非新颖性。X 平台上的开发者指出，许多智能体任务的失败并非源于推理能力不足，而是由于执行环节的不可靠，而“技能层”的引入正好填补了这一空白。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件开发者 Thomas Rehmer &lt;a href=&quot;https://x.com/thomas_rehmer/status/2016018978250834305&quot;&gt;评价&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;确实该这么搞。有了这些‘可发现’的技能，总算把智能体架构里那个‘你能干嘛？’的经典难题给破了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，AI 工程师 Aakash Harish &lt;a href=&quot;https://x.com/0_Aakash_0/status/2014024888575729964&quot;&gt;发文&lt;/a&gt;&quot;称：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这就是 AI 智能体界的 npm。它的精髓在于：比起纠结协议有多复杂，Skills 更看重好不好组合。如果说 MCP 搞定的是‘智能体怎么跟工具搭火’，那 Skills 搞定的就是‘开发者怎么分享和找现成的能力’。这俩以后肯定不是谁取代谁，而是强强联手：用 Skills 搞定发现和共享，用 MCP 去啃那些对确定性要求极高的企业级硬骨头。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不少开发者将 Skills.sh 与目前围绕智能体执行兴起的其他工具和标准进行了对比。类似的理念也出现在其他协议驱动的方案中，例如 Anthropic 推出的 &lt;a href=&quot;https://www.infoq.com/articles/mcp-connector-for-building-smarter-modular-ai-agents/&quot;&gt;Model Context Protocol&lt;/a&gt;&quot; (MCP)（侧重于通过结构化的 API 访问工具和数据），以及 OpenAI 的 &lt;a href=&quot;https://www.infoq.com/news/2023/06/openai-api-function-chatgpt/&quot;&gt;Function Calling&lt;/a&gt;&quot;（通过 JSON schema 暴露预定义动作）。此外，包括 &lt;a href=&quot;https://docs.langchain.com/oss/python/langchain/tools&quot;&gt;LangChain&lt;/a&gt;&quot; 的 tools 和 CrewAI 的 &lt;a href=&quot;https://docs.crewai.com/en/concepts/tasks&quot;&gt;tasks&lt;/a&gt;&quot; 在内的其他项目也致力于为智能体提供受控的执行权限，不过它们通常依赖于更高层的 Python 抽象，而非基于 shell 的命令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/vercel-agent-skills/&lt;/p&gt;</description><link>https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</link><guid isPermaLink="false">https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</guid><pubDate>Sat, 07 Feb 2026 00:00:00 GMT</pubDate><author>作者：Daniel Dominguez</author><category>AI&amp;大模型</category></item><item><title>“16个Agent组队，两周干翻37年GCC”？！最强编码模型Claude Opus 4.6首秀，10万行Rust版C编译器跑通Linux内核还能跑Doom</title><description>&lt;p&gt;Anthropic 正在升级它“最聪明的模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着新一代旗舰模型 Claude Opus 4.6 的发布，Anthropic 释放出的信号十分明确：这并不是一次常规的性能小修小补，而是一轮围绕长任务、复杂工作，以及智能体（agent）如何真正干活展开的系统性升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c1550330d571d4dfdb7e5f7f8795d540.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这次发布之前，Anthropic 内部和部分早期用户已经开始让 Opus 4.6 参与一项持续时间很长的工程任务：从零开始，用 Rust 编写一个完整的 C 编译器，并要求它能够编译 Linux 内核。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项实验持续了约两周时间，期间累计运行了近两千次 Claude Code 会话，最终产出了一个规模约 10 万行代码的编译器。该编译器不仅能够在多种架构上构建 Linux 6.9，还可以编译 FFmpeg、Redis、PostgreSQL、QEMU，并通过了 GCC 自身 99% 的 torture test，甚至能够成功编译并运行 Doom。整个实验的 API 成本约为 2 万美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了让外界更直观地理解这一成果的尺度，有网友在社交平台上给出了一个对照：GCC 的开发从 1987 年开始，历经 37 年，投入过数以千计的工程师。而这一次，是一名研究者加上 16 个 AI 智能体，在短短数周内完成了一个能够通过大量 GCC 测试集、并编译真实大型项目的编译器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbfb1e958df9b19689507e724aefd217.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样一段持续推进的工程实践之后，Anthropic 对外发布了 Claude Opus 4.6。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成立于 2021 年、由一批前 OpenAI 研究人员和高管创立的 Anthropic，一直以 Claude 系列大模型为核心产品；在这一体系中，Opus 代表最大、能力最强的型号，Sonnet 和 Haiku 则分别覆盖中等与轻量级使用场景。某种程度上，Opus 系列承担的角色，就是在更复杂、更长期的任务环境中检验 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最强的编码模型：从跑分看 agentic 编程能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 对 Opus 4.6 的定位，并不只是“更会写代码”。他们强调，新模型在编程能力上的提升，已经从单纯的代码生成，扩展到更前置的任务规划，以及更后置的代码审查与调试流程。这种变化，使模型能够在大型代码库中更稳定地工作，也直接决定了它是否有能力脱离短对话模式，持续参与多阶段、长周期的工程任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种定位在评测结果中体现得比较清楚。Anthropic 公布的多项基准测试显示，Claude Opus 4.6 在 agentic 编程、计算机使用、工具调用、搜索以及金融等任务上，整体跑分都有所提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/67/678307d2b742d377329cb0b226c856aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在终端 agentic 编程能力上，Opus 4.6 得分 65.4%，对比来看，略高于 GPT-5.2 的 64.7%，明显领先 Gemini 3 Pro（56.2%）和 Sonnet 4.5（51.0%）。这说明在纯终端环境下执行多步编程任务时，Opus 4.6 的稳定性和自我修正能力处在第一梯队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 SWE-bench Verified（Agentic coding） 上，各家分数非常接近，Opus 4.6（80.8%）与 Opus 4.5（80.9%）、GPT-5.2（80.0%）基本处于同一水平。这里可以理解为：在标准化的软件工程任务上，能力已经开始趋同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在电脑操作（OSWorld）上，代际差异开始显现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OSWorld（Agentic computer use） 是一个比较关键的分水岭。Opus 4.6 达到 72.7%，相比 Opus 4.5 的 66.3% 有明显提升，而 Sonnet 4.5 只有 61.4%，其他模型则未给出对等数据。这类评测关注的是 GUI 操作、跨应用流程和状态理解能力。放在整张表里看，它与编程能力的同步提升，意味着 Opus 4.6 不只是“会想”，而是更擅长把计划落到具体操作上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agentic search（BrowseComp）：明显拉开差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BrowseComp 是整张表里差距最清楚的一项。Opus 4.6 为 84.0%，而 GPT-5.2 Pro 是 77.9%，Opus 4.5 只有 67.8%，Sonnet 4.5 更低。这一项测的是在真实开放网络中定位、筛选和组合信息的能力，结果说明 Opus 4.6 在“研究型 agent 行为”上已经明显领先，而不是只在封闭工具或结构化任务中占优。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，在 Humanity’s Last Exam（跨学科推理）和 ARC-AGI-2（新问题解决） 上，Opus 4.6 的优势更加明显，尤其是 ARC-AGI-2 的 68.8%，相比 GPT-5.2 Pro 的 54.2% 和 Gemini 3 Pro 的 45.1%，已经不是细微差距。这类评测通常更难通过“提示工程”或策略优化取得跃升，更像是在反映模型本身的泛化推理能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“上下文腐烂”与模型可用性的分水岭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Opus 4.6 还扩大了上下文窗口，也就是单次会话里可记住、可处理的信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新模型在 Beta 阶段提供 100 万 token 的上下文长度，与该公司现有的 Sonnet（4 和 4.5 版本）相当。Anthropic 表示，这样的上下文容量更适合处理更大型的代码库，也能支持对更长文档的分析与处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但Anthropic 特别强调，Opus 4.6 的提升并不是“能塞更多 token”，而是“塞进去之后还能用”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们在说明中提到，Opus 4.6 在大规模文档中检索关键信息的能力显著增强，这一点在长上下文任务中尤为明显：它可以在数十万token 范围里持续跟踪信息，偏差更小，也更容易捕捉到埋得很深的细节——包括一些 Opus 4.5 本身就已经容易漏掉的信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正好对应了开发者长期吐槽的一个问题：“上下文腐烂（context rot）”。很多模型在对话或任务一旦拉长之后，要么开始遗忘早期信息，要么虽然“看过”，但已经无法在后续推理中正确调用，最终表现为前后不一致、定位问题跑偏、重复试错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MRCR v2（8-needle、100 万 token）这类“草堆找针”测试，本质上就是在专门检验这种能力：把多个关键线索埋在超长文本里，看模型能否在不迷路的情况下把它们重新找出来。Opus 4.6 在该测试中的得分为 76%，而 Sonnet 4.5 仅为 18.5%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并不是简单的“高一点、低一点”，更像两种不同的可用性状态：一个模型在超长上下文中仍然能稳定检索并利用信息，另一个则在任务拉长后迅速失效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/746521ff51da955e770b0155c22f7bec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种长上下文的稳定性，直接影响模型能否胜任更“工程化”的工作，尤其是复杂代码分析与故障诊断。在 Anthropic 给出的能力图中，Opus 4.6 被特别标注为擅长做 root cause analysis（根因分析）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/caf21288bc8205c65017476277640bed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Agent团队，构建一个C编译器&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4.6 最醒目的新增功能，是 Anthropic 所称的“智能体团队”（agent teams）：由多个智能体组成的小队，可以把一个大任务拆成若干独立的子任务分别推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 的说法是：“不再让单个智能体按顺序把任务一路做到底，而是把工作分给多个智能体——每个智能体负责自己的一块，并直接与其他智能体协调。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 产品负责人 Scott White 将其类比为“雇了一支很能干的人类团队”，因为职责拆分后，智能体可以并行协作，从而更快完成工作。目前，“智能体团队”以研究预览（research preview）的形式向 API 用户与订阅用户开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编译器本身固然是一个高度复杂、且极具工程价值的成果，但在 Anthropic 团队看来，它更像是一次“能力压力测试”的载体。真正值得总结的，是围绕 长时间运行的自治 Agent 团队（long-running autonomous agent teams） 所形成的一整套工程方法论：如何设计无需人工干预的测试体系、如何让多个 Agent 并行推进复杂工作、以及这种架构在现实工程中究竟会在哪些地方触碰到上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“协作式 Agent”到“自治式 Agent”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现有的 Agent scaffolding（例如 Claude Code）本质上仍然是人机协作系统：模型在解决复杂问题时，往往会在某个阶段停下来，等待操作者继续输入新的指令、确认状态，或澄清歧义。Anthropic 的实验目标是消除这种对“人类在线”的依赖，让 Claude 能够在无人监督的情况下，持续推进一个长期任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了实现持续自主的进展，Claude 工程团队并没有引入复杂的调度系统，而是构建了一个程序，让 Claude 进入一个简单的循环（如果你见过 Ralph 循环，应该会觉得眼熟）：每完成一个任务，就立刻进入下一个任务，而不是回到“等待用户”的状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/24da9de1991a3726416c38d0c3099bb2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Agent prompt 中，Claude 被明确要求将问题拆解成可执行的小任务、记录当前进展、判断下一步行动，并持续迭代，直到系统判定“没有明显改进空间”。（在这最后一点上，Claude 没有选择，因为循环会一直运行——不过在一次实验中，团队确实看到 Claude 不小心执行了 pkill -9 bash，结果把自己杀掉了，循环也就随之结束了。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;并行运行 Claude&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并行运行多个实例，可以缓解单一 agent harness 的两个弱点：&lt;/p&gt;&lt;p&gt;一次 Claude Code 会话同一时间只能做一件事。随着项目范围扩大，并行调试多个问题会高效得多。&amp;nbsp;运行多个 Claude agent 可以实现“分工”。当一部分 agent 负责解决核心问题时，其他专门的 agent 可以被调用来（例如）维护文档、盯代码质量，或处理更专门的子任务。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude 工程团队的并行实现非常基础：先创建一个新的裸 Git 仓库；然后为每个 agent 启动一个 Docker 容器，把仓库挂载到 /upstream。每个 agent 会在容器内克隆一份本地副本到 /workspace，完成工作后，从各自的容器把改动推回 upstream。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为避免两个 agent 同时尝试解决同一个问题，harness 使用了一个简单的同步算法：&lt;/p&gt;&lt;p&gt;Claude 通过在 current_tasks/ 下写入一个文本文件来“锁定”某个任务（例如，一个 agent 可能锁定 current_tasks/parse_if_statement.txt，另一个锁定 current_tasks/codegen_function_definition.txt）。如果两个 agent 试图认领同一任务，Git 的同步机制会迫使第二个 agent 改选另一个任务。&amp;nbsp;Claude 在任务上工作完成后，会从 upstream 拉取、合并其他 agent 的改动、推送自己的改动，然后移除锁。合并冲突很常见，但 Claude 能够处理。&amp;nbsp;无限的 agent 生成循环会在一个全新的容器里启动新的 Claude Code 会话，然后重复上述流程。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一个非常早期的研究原型。Claude 工程团队尚未实现任何其他 agent 之间的通信方法，也没有强制任何高层目标管理流程，也没有使用 orchestration agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，团队把“如何行动”的决定权交给每个 Claude agent。多数情况下，Claude 会选择“下一个最显而易见”的问题继续做；当卡在某个 bug 上时，Claude 往往会维护一份持续更新的文档，记录失败过的方法和剩余任务。在项目的 Git 仓库里，可以通过历史记录看到它如何在不同任务上获取锁并推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用 Claude 团队写代码：一些更管用的做法&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把 Claude 放进循环只是起点，真正决定它能否持续推进的，是它能不能从环境和反馈中判断“下一步该做什么”。因此，Claude 工程团队把大量精力放在模型之外：测试如何设计、反馈如何呈现、运行环境如何约束，才能让 Claude 在无人干预的情况下仍然保持方向感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个核心前提是：必须围绕语言模型的固有限制来设计系统。在这次实践中，团队重点应对了两类限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是上下文窗口污染。测试框架不能输出成千上万字节的无用信息，最多只保留几行关键输出，其余重要内容统一写入文件，供 Claude 在需要时自行查阅。日志也需要便于自动处理：一旦出现错误，必须在同一行明确标出 ERROR 以及失败原因，方便grep直接检索。同时，能提前算好的汇总统计信息会被预先计算，避免 Claude 在上下文中反复做同样的推导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类限制是时间盲。Claude 无法感知时间，如果无人干预，很容易长时间沉浸在跑测试里而不推进工作。为此，测试框架很少输出增量进度，避免不断污染上下文，并提供默认的 --fast 选项，只运行 1% 或 10% 的随机子样本。这个子样本对单个 agent 是确定的，但在不同虚拟机之间是随机的，从整体上仍能覆盖所有文件，同时又能让每个 agent 精确识别回归问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在并行方面，团队也很快意识到：并行是否有效，取决于问题是否“好拆”。当失败测试数量多且彼此独立时，并行非常直接——每个 agent 处理一个不同的失败测试即可。在测试通过率接近 99% 后，团队让不同 agent 分别去完成不同小型开源项目的编译，例如 SQLite、Redis、libjpeg、MQuickJS 和 Lua。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但当任务升级到编译 Linux 内核时，情况发生了变化。内核编译本质上是一个高度耦合的整体任务，所有 agent 都会命中同一个 bug，修完再相互覆盖。即便同时运行 16 个 agent，也无法带来实质进展，因为大家都卡在同一件事上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决办法是引入 GCC 作为在线的、已知良好的对照编译器。团队编写了新的测试框架：随机选择内核中大部分文件用 GCC 编译，只把剩余文件交给 Claude 的 C 编译器。如果内核能够正常运行，说明问题不在 Claude 负责的那部分文件；如果失败，则再通过把其中一些文件切回 GCC 编译，逐步缩小范围。这样一来，不同 agent 就可以并行地修复不同文件中的不同错误，直到 Claude 的编译器最终能够编译全部文件。即便如此，后续仍需要配合增量调试（delta debugging），找出那些“单独没问题、组合在一起就失败”的文件对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;并行运行也带来了另一层收益：角色分工成为可能。在实践中，Claude 工程团队发现，LLM 生成的代码很容易重复实现已有功能，因此专门安排了一个 agent 负责扫描并合并重复代码；另一个 agent 聚焦于提升编译器自身的性能；第三个 agent 负责改进生成代码的效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，还有 agent 从 Rust 开发者的视角审视整个项目的设计，提出结构性调整建议，以提升整体代码质量；另一个 agent 则专注于文档维护。通过这种方式，不同 Claude 实例在同一代码库中承担起相对稳定的职责，而不是反复在同一层面“重新发明轮子”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;评估结果与能力边界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在两周内接近 2,000 次 Claude Code 会话中，Opus 4.6 共消耗约 20 亿输入 token、生成约 1.4 亿输出 token，总成本略低于 2 万美元。该团队表示，即便与最昂贵的 Claude Max 方案相比，这仍是一次成本极高的实验；但这一成本依然远低于由单人、甚至完整人类团队完成同等工作的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器是一次完全的 clean-room 实现：开发过程中 Claude 从未获得互联网访问权限，仅依赖 Rust 标准库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终得到的约 10 万行代码，能够在 x86、ARM 和 RISC-V 架构上构建可启动的 Linux 6.9，同时也可以编译 QEMU、FFmpeg、SQLite、Postgres、Redis，并在包括 GCC torture test 在内的大多数编译器测试套件中达到约 99% 的通过率。此外，它还通过了开发者的终极考验：它可以编译并运行 Doom 游戏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，这一项目也把当前 Agent 团队的能力边界暴露得相当清晰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;缺乏启动 Linux 所需的 16 位 x86 编译能力，因此在 real mode 阶段会调用 GCC（x86_32 与 x86_64 编译器由其自身实现）。尚未拥有稳定可用的 assembler 与 linker；这些是 Claude 开始自动化的最后环节，目前仍存在问题，演示中使用的是 GCC 的相关工具。该编译器能够成功编译许多项目，但并非所有项目都能成功。它目前还不能完全替代真正的编译器。生成的代码效率不高。即使启用所有优化，其效率也低于禁用所有优化的 GCC 生成的代码。Rust 代码质量尚可，但远不及 Rust 专家级程序员编写的代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体实现已接近 Opus 的能力上限，新增功能或修复 bug 时，经常会破坏已有功能。其中一个最具代表性的难点是 16 位 x86 代码生成。尽管编译器可以通过 66/67 opcode 前缀生成语义正确的 16 位 x86 代码，但生成结果超过 60KB，远高于 Linux 强制的 32KB 限制。因此，在这一阶段，Claude 选择调用 GCC 作为替代（该情况仅出现在 x86 上；在 ARM 与 RISC-V 架构下，编译可完全由 Claude 自身完成）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器的源码已经公开：&lt;a href=&quot;https://github.com/anthropics/claudes-c-compiler&quot;&gt;https://github.com/anthropics/claudes-c-compiler&lt;/a&gt;&quot;。Claude 工程团队建议直接下载、阅读代码，并在自己熟悉的 C 项目上尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dc71d0e9a3b680a980d90fc83313bccf.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/news/claude-opus-4-6&quot;&gt;https://www.anthropic.com/news/claude-opus-4-6&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/engineering/building-c-compiler&quot;&gt;https://www.anthropic.com/engineering/building-c-compiler&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</link><guid isPermaLink="false">https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</guid><pubDate>Fri, 06 Feb 2026 08:33:16 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>TypeScript 之父 Anders Hejlsberg：别折腾“AI新语言”了，真正变天是 IDE 让位给 Agent</title><description>&lt;p&gt;过去十年，TypeScript 被很多团队当作“工程化 JavaScript”的答案；到了 AI 编程时代，它又意外变成了 AI 最顺手的语言之一——原因很简单：AI 写代码的能力基本取决于它见过多少这种语言的代码，而 TypeScript/JavaScript 恰好是训练语料最丰富的那一档；更关键的是，TypeScript 还把类型与接口这些“语义线索”明明白白写在代码里，正好让 AI 更容易理解、重构和补全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这种背景下，微软在 2025 年 12 月 16 日完成了 TypeScript 史上最激进的一次重构：用 Go 语言迁移（重写）编译器与部分工具链，宣称带来 10 倍性能飞跃。但消息一出，社区立刻炸锅——明明 Rust 才是当下重写系统级工具的“默认答案”，为什么偏偏选 Go？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 之父、同时也是 Turbo Pascal、Delphi、C# 等语言的核心设计者 Anders Hejlsberg，在与 GitHub 研究顾问 Eirini Kalliamvakou 的对谈中正面回应了这些质疑：很多人认为他们“应该选另一门语言”，但他坚持“我们选了最合适的工具，而且过去一年已经证明了这一点”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，Hejlsberg 也谈到 AI 在这次迁移中的真实位置：团队曾尝试让 AI 直接把 TypeScript 代码迁移到 Go，“结果不太理想”，因为他们需要的是五十万行代码级别、行为完全一致的确定性迁移；AI 只要偶尔“偏一点”，就会把成本转移到逐行审查上，得不偿失。相比之下，让 AI 去生成迁移工具、以及在迁移之后自动同步旧代码库新增的 PR 变更，反而更有效——这部分他们“已经相当成功”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Hejlsberg 也明确指出：在“AI 无处不在”的新环境里，TypeScript 的语言服务（补全、跳转、重构、快速修复）不会只是原样搬家，而是在被大幅重塑——因为很多过去必须靠 IDE 才能做的事，AI 将会做得更好。未来真正不确定的也不是 TypeScript 语言本身（它仍沿着 JavaScript 标准化路径演进），而是工具形态：AI 正从 IDE 的助手变成主要工作者，人类转向监督与审阅；这也是为什么把语言服务接入 MCP 这类机制会突然变得诱人——让 AI 能提出语义级问题、发起重构请求，用“智能体方式”完成过去只能在 IDE 里完成的工作流，开发工具将因此被彻底改写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。找到那些无聊却昂贵的事情，把它们交给 AI。工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;不如直接修 JS：TypeScript 的顿悟时刻&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从 Turbo Pascal、Delphi、C# 到如今的 TypeScript，你的工作塑造了数以百万计开发者每天写代码的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我第一次接触计算机大概是在高中时期，那是 70 年代末，我很快对编程产生了浓厚兴趣。后来，随着 8 位微型计算机开始出现，我决定自己动手组装一台套件机，并为它编写大量软件。我发现自己在这方面做得相当不错，而且也真的很享受这个过程。那时无论是结构化编程还是汇编语言，对我来说都不成问题。当然，还要考虑一个现实条件：只有 64K 内存，能塞进去的东西非常有限，还得给用户留出空间，所以当时一切都还能装在脑子里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 在很大程度上革新了开发者体验，核心在于缩短开发者的反馈回路。这在多大程度上是你一开始就有意识的设计理念？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：首先，人们之所以喜欢早期机器上的 BASIC，很大原因正是它的短反馈周期。BASIC 是解释型语言，输入代码就能立刻运行，但代价是运行速度慢，而且编辑器是基于行的，体验很糟。相比之下，当时的文字处理软件已经是所见即所得的屏幕编辑器，可以自由移动光标，这显然更适合写代码。与此同时，“输入—运行—立刻看到结果”的模式又非常吸引人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要在编译型语言中实现这一点并获得性能，就必须有极快的编译器。Turbo Pascal 的做法是：你一按下运行，它立即在内存中完成编译，甚至不需要访问磁盘，然后直接运行。如果出现错误，就立刻回到编辑器。编译器本身非常原始，你甚至需要通过出错地址反推源码位置。但正因如此，突然之间就获得了一种高度交互的体验，在当时堪称革命性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：那种“编译要等一个下午”的体验，会不会让你感到沮丧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：当然会，没有人喜欢等待。代码已经写完了，你只想立刻运行，而不是坐在那里干等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 还有一个重要影响，就是以低价让更多人接触到编程。回头看，这一点你有什么感受？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这里有个有意思的故事。Turbo Pascal 的前身叫 Poly Pascal，是我在丹麦一家小型软件公司里开发的 Pascal 编译器。后来我们联系上了 Borland 的创始团队，他们看过之后觉得非常惊艳，提议一起把它作为产品推向美国市场。然后他们决定定价 49.95 美元。我当时的反应是：“你们疯了吗？这样根本赚不到钱。”事后看来，这个决定非常聪明。虽然价格只有原来的十分之一，但销量却高出了三到四个数量级。最终结果非常成功，这个功劳主要要归于 Borland 的创始人Philippe。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Delphi标志着你从独立创作者向团队领导者的转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：一开始我基本上是单打独斗，虽然在丹麦的 Borland 办公室也会和两三个人合作，但随着机器性能飞速提升、用户期望不断提高，这种模式显然无法持续。我必须学会团队协作，这在 Turbo Pascal 期间就已经开始，而在 Delphi 项目中尤为明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你必须接受事情不会完全按照你个人偏好的方式完成，代码也不一定长成你心目中的样子。而且你往往没有时间亲自去“修正”这些细节，何况那样做也未必真的改变产品行为，更重要的是学会放权。只有当团队成员在各自负责的功能和模块中感到被信任、被赋权，团队才能真正运转起来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你在 Microsoft 参与 Visual J++ 的经历，对 C# 和 .NET 平台的目标产生了怎样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我是在 1996 年底加入 Microsoft 的，担任 Java 开发工具集的首席架构师。我们刚发布了 Visual J++ 1，本质上只是把 C++ 的 IDE 换成 Java 编译器，谈不上真正的集成，更没有快速的应用开发体验。于是我们着手改进，这最终成为 Visual J++ 6.0，并开始与 Visual Basic、Visual Studio 的版本体系对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要为 Microsoft 的 DOS 和 Windows 平台做 Java，就必须与这些环境高度互操作。但 Java 当时强调“一次编写，到处运行”，强制使用最小公分母的 UI 接口，最终只能做出体验很差的小程序。我们不得不引入一些扩展，简化与原生平台的互操作，并构建封装 Windows UI 的类库。很快就发现，这条路走不通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果真正为用户着想，就必须允许构建针对特定环境的最佳方案。人们既想要 Visual Basic 的易用性，又想要 C++ 的表达能力。于是我们尝试把这两者结合起来，并构建在 .NET 这样一个可持续演进的平台之上，最大限度地利用用户所运行的系统能力，这正是整个构想的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你既在谈不同类型的用户，又在描述为整个生态系统服务的思路，这种整体视角是如何形成的？&lt;/p&gt;&lt;p&gt;Anders：用户并不在乎这是语言特性、框架特性、平台能力，还是编辑器或调试器的问题，对他们来说，一切加在一起才是“体验”。因此，这些部分必须协同设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们构建了运行时、JIT 编译器、垃圾回收器，设计了平台的字节码，开发了类库。我既参与语言设计，也参与类库和运行时的设计，与负责这些组件的工程师密切合作，最终效果因此更好。否则，各自为政会形成孤岛，最后只能靠复杂的互操作层勉强拼接，结果自然谈不上“最佳实践”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，我们也不惧怕与底层平台深度互操作。过于教条地坚持最小公分母，拒绝利用具体平台的优势，这样永远不可能做到真正意义上的“最佳体验”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当时是否有某个“顿悟时刻”，让你意识到 JavaScript 在规模化发展中的阵痛已经成为必须解决的问题，而且这是一个需要由你、由 Microsoft 来解决的问题？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：行业里的运行时环境开始变得足够成熟，比如 Google 在 V8 上做了非常出色的工作，JavaScript 的运行性能突然变得相当可观。HTML5 也正式定稿，UI 能力大幅提升。同时，手机、iPad 等各种形态的设备出现了，而它们并不运行 Windows。整个行业突然意识到，真正的平台竞争不在 Java，而在 JavaScript、浏览器和 HTML 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，人们开始编写越来越庞大的应用，因为新的运行时和 UI 技术已经允许这样做。但很快大家就发现，在一种动态语言里、缺乏成熟工具支持的情况下，这件事难得令人发指，于是我们看到了各种奇怪的扭曲做法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中一个典型案例是 Outlook.com 团队找到我们，主要是 .NET 和 C# 团队，询问是否可以把他们内部的一个叫 Script# 的东西产品化。我当时一头雾水：“Script# 是什么？”他们说，这是一个允许你用 C# 编写代码，然后编译成 JavaScript 来运行的工具。我第一反应是：这听起来像是两边的缺点都占全了。&lt;/p&gt;&lt;p&gt;但事实是，这么做的真正原因在于可以获得“成熟的工具链”：类型检查、团队协作能力、接口定义，以及对模块之间交互方式的清晰描述。因为他们有成百上千名程序员参与开发，不可能只靠裸写 JavaScript，在没有检查、没有自动补全、没有重构支持的情况下完成工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我开始反思：JavaScript 真的已经糟糕到这种程度了吗？而“先写另一门语言，再把 JavaScript 当成中间表示或字节码来编译”，真的是解决问题的最佳方式吗？如果能直接修复 JavaScript 本身，会发生什么？于是我们开始探索这条路，事实证明，这个方向效果相当不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JavaScript 单线程的天花板：我们在浪费 90% 的算力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：TypeScript 被设计成 JavaScript 的严格超集，这背后显然有深思熟虑的战略考量。你是如何坚持这一决策的？这又能给其他语言设计者哪些启示？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：每当有人跑来跟我说：“我在考虑做一门全新的编程语言，能解决这个、解决那个”，我给出的第一条建议通常是：这个世界对新编程语言的需求，就像对头上再多一个洞的需求一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二条建议是：如果你真的要做一门语言，要意识到其中 90% 的工作，和其他所有语言是完全一样的，而且这个比例还在不断上升。如今，程序员的期望早已不只是一个编译器，你还需要完善的语言服务，能够集成到几乎所有主流 IDE 中，需要能与 AI 交互的 MCP 服务器，需要调试器、性能分析工具……&lt;/p&gt;&lt;p&gt;此外，你还得有至少十年的时间，因为一门语言真正站稳脚跟、获得有意义的用户规模，往往就是这么长的周期。没有人会在一开始就拥抱一门全新的语言，头五年你很可能用户寥寥，还得不断回答“我们真的要继续投入吗？”这是一门非常艰难的生意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。在 Visual Studio Code 里，我们的语言服务对 TypeScript 和 JavaScript 是同一套。对我们而言，JavaScript 只是没有类型注解的 TypeScript，或者使用 JSDoc 注解的另一种形式。这意味着我们不是在做两套东西，而是在同一项投入之上，精确地构建真正必要的能力，只为让整个生态系统变得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：2012 年在 GitHub 上启动 TypeScript 的开源项目，在当时的 Microsoft 可以说是一次相当激进的举动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我们当时对 JavaScript 生态的运作方式、价值观以及参与社区所需的前提，其实有着非常清晰的认识。大家都明白，如果不开源，这个社区根本不会理你，一个封闭的商业产品对他们毫无吸引力。因此，从一开始我们就主张开源。同时，Microsoft 内部也逐渐意识到，开源并不是“洪水猛兽”，而是如果想真正与开发者对话，就必须拥抱的现实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你刚才提到 2012 年发布在 GitHub，其实并不准确。2012 年我们发布在 CodePlex——Microsoft 自家的、并不太受欢迎的开源平台上。结果就是，反响寥寥。那时所谓的“开源”，更多只是把代码丢到仓库里，让大家提 issue，然后我们再把这些 issue 抓回内部系统，按内部流程处理。某种程度上，这也解释了为什么几乎没人关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再加上当时 Microsoft 在开源社区中的信誉并不高。真正的“主战场”在 GitHub。于是 2014 年我们迁移到了 GitHub，全面采用开放式开发流程。内部成员和外部贡献者遵循完全相同的规则，所有功能都通过 Pull Request 提交，所有讨论都公开进行。直到那时，项目才真正开始起飞，社区的兴趣也随之而来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从“把代码扔给社区”到真正的开放式开发，你们学到了哪些经验？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这是一个彻头彻尾的双赢。对用户来说，他们能看到“香肠是怎么做出来的”，所有讨论都在公开的 issue 里完成，而不是私下做决定后再给出一个结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对项目开发者而言，这种方式同样令人满足。你每天都能感受到社区的参与和认可，看到点赞、看到讨论，远比关起门来做六个月或一年，然后祈祷产品方向正确要有趣得多。在这里，用户每天都在用投票告诉你他们最想要什么功能，我们只需按票数排序，就能清楚地知道优先级。你解决这些问题，社区的热情就会进一步增强，形成正向循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：把 TypeScript 编译器迁移到 Go 背后的动机、权衡以及所面临的挑战是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：TypeScript 从一开始就是自托管的，用它自身编写，这意味着编译器和整个工具链本质上都是一个 JavaScript 应用。这带来了很多好处，比如你甚至可以在浏览器里运行编译器，在浏览器中构建一个完整的 IDE，一切都能正常工作。但随着 TypeScript 的广泛采用，以及用户项目规模不断扩大，可扩展性逐渐成为头号问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里有 JavaScript 本身的一些内在限制。它在设计上是单线程的，不支持共享内存并发，这意味着你实际上浪费了 90% 的计算能力。此外，JavaScript 的执行成本也很高，它的对象模型极为宽松，可以随意给对象加属性，这使得优化非常困难，底层往往演变成复杂的哈希查找和缓存机制，远不如原生代码高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们当时就清楚，自己正接二连三地白白损失收益。尽管放弃自托管让人非常不舍，但即便用尽所有优化技巧，性能瓶颈依然无法突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，在 2024 年夏天，我们开始做原型验证，先从扫描器、解析器这些容易量化的模块入手。很快就发现，性能提升可以达到 10 倍：一半来自原生代码，一半来自共享内存并发。这样的提升能把原本需要几分钟的事情缩短到十几秒，完全改变了游戏规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也很清楚不想从零重写一个全新的编译器。社区里有不少声音主张“用 Rust 全部重写”，但我们认为这并不可行。TypeScript 的类型检查器极其庞大复杂，许多行为只体现在现有代码的精确语义中。如果重写，就会陷入永无止境的差异追赶，最终无法与旧编译器对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此我们的目标是“迁移”，逐函数地把同样的逻辑搬到原生语言中。当然，这意味着必须重构数据结构，因为原生语言不允许像 JavaScript 那样随意给对象加属性。我们尝试过多种语言，很快排除了 Rust，因为我们的编译器充满了循环数据结构，并且高度依赖自动垃圾回收，使用 Rust 等同于重写。&lt;/p&gt;&lt;p&gt;最终我们选择了 Go。它在很多方面与 JavaScript 相似，这个选择对我们来说非常奏效。现在我们拥有了一个原生编译器，在功能上几乎是旧编译器的拷贝，连那些小怪癖都一模一样，只是快了 10 倍，这意味着社区不需要推倒重来。当我们正式切换时，我相信大家会非常满意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最适合 AI 的语言”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：在 AI 辅助编程的背景下，你认为 TypeScript 为什么特别适合 AI 工作流？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：很多人问我，为什么不干脆设计一门“最适合 AI 的完美编程语言”。我的回答通常是：那样的语言反而会成为最不适合 AI 的语言，因为它不会出现在 AI 的训练数据中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 在某种语言中写代码的能力，与它见过这种语言代码的数量几乎成正比，它本质上是在大量样本的基础上进行再组合和外推。AI 已经见过海量的 JavaScript、Python 和 TypeScript，因此在这些语言上表现得非常好。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 的确正在改变我们构建产品的方式。以这次 TypeScript 迁移为例，我们也尝试过用 AI 自动完成代码迁移，但效果并不好。一方面那是一年前的 AI，能力还有限；另一方面，迁移五十万行代码并保证行为与原代码完全一致，我们需要的是极其确定性的结果。AI 在翻译过程中可能会产生细微的“幻觉”，而你又不得不逐行检查，这并不划算。在这种情况下，更好的方式或许是让 AI 帮你生成“迁移工具”，而不是直接迁移代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 项目中有很大一部分是语言服务，为 IDE 提供补全、跳转、重构和快速修复。现在我们也在思考：既然 AI 已经能在很多场景下做得更好，是否还有必要原样迁移这些功能？类型检查器我们完整迁移了，但语言服务正在被大幅度重塑，以适应一个“AI 已经无处不在”的新环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你如何看待 AI 工具正在改变编程本身，以及语言设计的方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在理想状态下，AI 应该帮助我们消除编程中那些繁琐、重复的劳动。以 TypeScript 的迁移为例，在我们开发新代码库的同时，旧代码库里仍然不断有新的 Pull Request 出现，我们已经相当成功地用 AI 把这些变更迁移过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再比如，项目里有成千上万条 issue，其中很多非常古老。它们是否仍然可复现？是否还相关？能否根据社区反馈排序？这些清理和维护工作过去总是被一拖再拖，最终却成为拖慢项目前进的“锚”。现在，我们可以构建 AI 机器人来完成这些工作。这是我认为非常重要的一点：找到那些无聊却昂贵的事情，把它们交给 AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这也带来新的困惑。如果 AI 取代了初级工程师，我们又该如何培养资深工程师？难道指望 AI 自己成长为“高级程序员”，而人类程序员逐渐消失吗？我并不这么认为。我们似乎正在逼近某种上限：AI 能做很多事，但仍然需要人类以某种监督者的角色参与其中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见的是，工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果把时间拉长到未来五到十年，在一个更加 AI 原生的世界里，你认为 TypeScript 作为一门编程语言会如何演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我认为它的演进方向其实相当清晰。JavaScript 已经不再是一门年轻的语言，它有一套成熟的标准化流程，而我们也深度参与其中。TypeScript 会沿着 JavaScript 的标准化路径继续发展，同时在其之上补充必要的类型系统能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正充满不确定性的，是工具层面的变化。谁能想到，随着“对话式编程”这类形态的出现，命令行工具又重新变得如此重要？过去，AI 更多是作为助手存在：开发者在 IDE 里，AI 帮你更快地输入和补全代码。但现在，这种关系正在反转。AI 开始承担主要工作，而人类转向监督和审阅。此时，AI 并不一定需要我们传统意义上的 IDE，尽管它仍然需要语言服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么 MCP 这类技术开始变得有吸引力：通过把语言服务接入 MCP，让 AI 能够提出语义级的问题、重构请求等，并在一定的确定性边界内完成工作流。这本质上是在用 LLM 或 Agent 的方式，完成过去只能在 IDE 中完成的事情，这将深刻改变开发工具的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：围绕 TypeScript 或你们的工作，有没有哪些你希望公开澄清的误解？或者哪些你觉得被社区忽视、但其实很重要的事情？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在开源领域，我们与社区的距离非常近。如果哪里不对劲、存在摩擦，几乎会第一时间被反馈出来。比如这次转向原生代码的决定，确实引发了不少争议，有人认为我们应该选择另一种编程语言。但我始终坚信，我们为这个目标选对了工具。过去一年里，这个决定的成果已经逐步显现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，开源本身就是一种微妙的平衡。一方面，你在无偿地把成果交给世界；另一方面，这个项目往往由一家商业公司资助，而公司必须以某种方式生存下去。总得有人支付账单。因此，我们团队始终处在一种张力之中：如何让开源项目既符合社区期待，又与公司使命保持一致。这并不是 TypeScript 独有的问题，而是当今几乎所有开源项目都面临的现实。至于是否存在一种更好的激励机制来回馈长期投入的人，目前还没有答案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：开源的可持续性，确实是一个反复被提起的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：大量企业的正常运转，事实上依赖于那些支撑其后端的开源项目得到持续维护。但现实中，很多人对开源的态度仍然是“索取多于付出”。在微软，我们至少在努力以一种更真诚的方式参与其中。仅 TypeScript 项目，就已经累计投入了数百人数年的工作量；而 Visual Studio Code，投入甚至可能达到上千人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果不算你自己参与创造的语言，你最敬佩、最尊重哪一门编程语言？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：任何一门能被程序员广泛使用、甚至能进入讨论范围的编程语言，都一定有其可取之处，因此都值得尊重，我深知一门语言要走到这一步有多难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Rust 为例，我非常敬佩它通过借用检查器来探索一种不同的内存管理方式，试图在不依赖昂贵自动垃圾回收的情况下保证安全性。我也很尊重 Go，尽管它在设计上有些“怪”，常被认为不太正统，但它实际上提供了一种简单、内存安全、类型安全的“现代 C”的思路。至于 Python，它的成功不需要多说，它驱动着 AI 和机器学习的发展，令人难以不心生敬意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从语言设计者的角度看，我们都站在前人的肩膀之上。如果设计一门语言却不向其他语言学习，那是愚蠢的。经验与智慧就在那里，理应被尊重和继承。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当你放眼整个以 GitHub 等协作平台为中心的软件开发生态时，是什么让你对未来保持乐观？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：仅仅是它依然存在这一事实，就足以让我感到乐观。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，其中也有大量噪音，有不少项目缺乏维护，但不可否认的是，这些代码记录了软件演化的全过程。以我们为例，转向这种协作工作流后，十二年的历史都被完整地保存在那里，可搜索、可追溯。如果我记得某个问题曾被讨论过，只需要去查找，而不必面对一封再也找不到的旧邮件，这种价值是巨大的。正因如此，我由衷地高兴看到它仍在持续增长，并顽强地存活下来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=uMqx8NNT4xY&lt;/p&gt;&lt;p&gt;https://devclass.com/2026/01/28/typescript-inventor-anders-hejlsberg-ai-is-a-big-regurgitator-of-stuff-someone-has-done/&lt;/p&gt;</description><link>https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</link><guid isPermaLink="false">https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</guid><pubDate>Fri, 06 Feb 2026 08:23:20 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>奥特曼重磅发声：全AI公司是未来！OpenAI官宣Frontier，让管理Agent像管人一样简单</title><description>&lt;p&gt;在OpenAI与Anthropic对轰AI Coding新产品，争夺编程王座之际，Open AI偷偷放大招，又推出智能体中枢平台 Frontier。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，Frontier 就是一个把智能体当成 AI 员工来管理的企业级平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c71db6c1f404abbbf9919be21668a6a9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年，智能体开始从“陪聊工具”走向企业一线业务，但一个关键问题成为不少企业的烦恼，即智能体越多，系统反而越复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在不少企业内部，云平台、数据系统和应用长期割裂，智能体被零散地塞进各个业务场景。每一个智能体都像一座信息孤岛，权限受限、上下文缺失。伴随智能体数量的暴增，带来的往往不是效率提升，而是运维、治理和协同成本的持续叠加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在这一背景下，Frontier应运而生。它将企业内部分散的系统与数据整合在一起，通过构建统一的业务上下文，提供一套端到端的方法，覆盖智能体的构建、部署与管理流程，让智能体能够真正进入生产环境稳定运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2月4日思科AI峰会上，Open AI CEO奥特曼曾激进发言，不能快速用上 AI 员工的公司，会被甩在后面。他甚至提出“全 AI 公司”的概念，未来或许每个流程、每个环节，AI都能真正参与进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，正是OpenAI 对企业级市场的提前卡位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据最新数据显示，Anthropic在企业级大模型市场占据了40%的惊人份额，稳坐第一把交椅，远超OpenAI的27%和谷歌的21%。随着大模型逐步进入真实业务流程，企业级场景正成为决定长期竞争格局的关键阵地，OpenAI 显然不希望在这一入口层面处于被动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 的目标已不再局限于打造“更聪明的模型”，而是试图通过基础设施，让各种智能体优先部署在自家平台之上，包括竞争对手的产品，从而将更多企业用户纳入其整体 AI 生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 OpenAI 官方披露，过去几年，已有超过 100 万家企业在使用 AI 提升效率。比如一家大型制造企业借助智能体，将原本需要六周完成的工作压缩到一天；另一家全球投资公司通过智能体优化销售流程，为销售人员释放出 90% 以上 的时间；还有一家大型能源生产商利用智能体提升 5% 的产量，额外创造了 超过10亿美元的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，能否在组织内部高效使用智能体，正在成为企业之间拉开差距的关键变量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，从制造业到互联网、从金融到生命科学，已有多家行业巨头率先试用 Frontier，包括 惠普、Intuit、甲骨文、州立农业保险、赛默飞世尔和优步。此外，BBVA、Cisco、T-Mobile 等数十家现有客户也已参与试点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台仍处于有限开放阶段，仅向少量客户开放体验，预计将在未来几个月逐步扩大范围，具体定价方案尚未披露。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Frontier平台的四大板块：上下文、执行环境、评估学习与安全管理&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更好地理解 Frontier，可以把它类比为一家公司的 “AI员工管理体系”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的作用不仅是要让AI员工了解公司是如何运作的，还要为其提供跨部门协作的能力、必要的资源支持，以及清晰的权限边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一目标，Frontier 将企业级 AI 智能体的运行拆解为四个关键模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（1）共享业务上下文：让 AI “知道公司怎么运作”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让 AI 真正参与企业工作，第一步不是分配任务，而是让它理解企业本身的运作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier 做的第一件事，就是构建一个共享的业务上下文环境。通过打通企业内部长期割裂的系统，包括 CRM、数据仓库、工单系统以及各类内部应用，将原本分散在不同系统中的业务信息连接起来，形成一个统一的“语义层”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，智能体就能理解信息如何在企业内部流动、关键决策发生在什么环节、哪些指标才是真正重要的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（2）提供执行环境：让 AI 不只会想，还能真的“干活”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在理解业务之上，Frontier 为 AI 员工提供了一个开放且可靠的执行环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可以打开和使用企业内部的各种工具，自己写代码处理数据，整理和生成文件，并在不同系统之间来回切换，把一整套原本需要人反复操作的流程跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对企业来说，这相当于把 AI 从“问答工具”，升级成了能独立完成任务的AI同事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（3）学习与评估：让 AI 在“反思”中不断优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了让AI像人一样能不断优化，自我迭代，Frontier 内置了绩效评估和优化机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套机制能够持续监控 AI 代理在实际任务中的表现，包括任务完成情况、错误率、资源消耗等关键指标，而人变成了监督者，可以清楚地看到哪些行为有效、哪些无效，并据此调整规则和流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/08b3e13c61a876c0825f2bced707b72e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着运行时间的增长，AI 智能体会逐步积累“记忆”，将过往交互转化为有用的上下文信息，从而不断优化自身表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（4）安全保障：让 AI 在清晰边界内工作&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了防止AI乱操作，Frontier 为每一个AI员工设立严格工作边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体能进哪些系统、能做哪些操作、权限到哪一步，都提前规定好。这样一来，AI 就只能在允许的范围内工作，不会乱动数据、越权操作，也不会给公司带来额外风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64d17bb0477c5cd67960a86f6681af37.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样一整套系统化设计下，Frontier 补齐了 AI 进入公司所需的基础设施。既给予足够的灵活性，又保留必要的安全和控制，使智能体能够真正融入企业的日常工作流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Frontier平台上，公司可以创建多个 AI 员工，也可以混合其他厂商的智能体或自行开发的 AI 服务。Frontier 的核心作用是公司可以通过统一的仪表盘，查看每个 AI 员工的任务完成情况、资源消耗和错误率等关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着企业部署 AI 的方式，正在从过去的“定制化开发”，转向“标准化配置”，让部署智能体更便捷易操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier已经在不少关键行业发挥价值。比如银行用它做AI后台，处理每年数亿的需求事件；制造业公司，靠它模拟生产流程、规划产能布局，节省了数十亿美元成本；在生命科学领域，这套系统用来优化全球监管流程，给药品审批这类关键环节兜底。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 OpenAI 应用业务首席执行官 Fidji Simo 所言：“到今年年底，领先企业中的大多数数字化工作，都将由人类进行决策和指挥，并由成群的 AI 代理来执行。这种模式已经在编程领域成立，并且很快会扩展到更多业务场景。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的上限，或许是全AI公司&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的推出，其实早就埋在奥特曼对人工智能未来走向的一系列判断之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在今年的思科 AI 峰会上，奥特曼认为Codex的诞生，是又一个 “ChatGPT 时刻”。那是他第一次真切地意识到，AI 可以被当作一名同事，而不只是工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e91d0e6bd4f4a581177927b1e1c6c8a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他讲了一个细节，刚安装 Codex 时，他绝不会在不检查的情况下，让它完全控制自己的电脑。但这个坚持只维持了两个小时，因为Codex 实在太好用了，而且这种“好用”已经不再局限于写代码本身，而是扩展到了整个工作的执行过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在奥特曼看来，让智能体能 “像人一样用电脑”，真正接管电脑和浏览器，才能把生产力彻底解放出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码能力加上通用电脑操作能力，这种结合的趋势几乎挡不住。他甚至想得更远：AI 的终极形态，说不定是 “全 AI 公司”，让智能体直接对接现实系统，从头到尾把一家企业跑起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然目前AI可以做的事已经很多，但真正被组织吸收和使用的比例依然很低。技术的演进速度，远远快于企业部署和消化的能力。这背后的原因是企业部署 AI 成本高，缺乏系统化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 最强大的地方之一，是“始终在线”的计算能力。但现有的硬件、权限系统、法律体系，都不是为这种情况设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;奥特曼曾将企业真正需要的形态概括为一种“AI 云平台”：它负责处理安全问题，管理业务上下文，协调和运行大量智能体，支持多模型协作，并提供完整的企业级授权与接口体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;企业级应用已经成为了OpenAI在2026 年明确的重点方向之一，而 Frontier，正是OpenAI 交出来的企业级解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种判断并非 OpenAI 一家的独断。去年 12 月，全球研究与咨询公司 Gartner 在一份报告中指出，代理管理平台既可能成为“人工智能领域最有价值的资产”，也是企业大规模采用 AI 所必需的基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，或许正在拉开 “AI 全面扎根企业核心业务” 时代的序幕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://openai.com/index/introducing-openai-frontier/&lt;/p&gt;&lt;p&gt;https://openai.com/business/frontier/&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=YO2PVbtpb_A&lt;/p&gt;</description><link>https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</link><guid isPermaLink="false">https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</guid><pubDate>Fri, 06 Feb 2026 07:45:00 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>Cloudera发布2026 AI与数据技术趋势预测：标准化、可控化趋势成企业主流选择</title><description>&lt;p&gt;过去两年，AI在中国经历了从概念热潮到密集试点的阶段。无论是大模型、智能体（Agentic AI），还是自动化应用，越来越多企业已完成初步探索。进入2026年，AI正迈入一个新的发展阶段——从试点应用走向业务规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业关注的核心问题也随之发生变化，不再只是“能否用AI”，而是AI是否能够在可控、可持续的前提下，稳定运行并转化为可衡量的业务成果。基于对中国企业AI实践的持续观察，Cloudera对2026年AI与数据技术的发展趋势做出如下判断：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测一：AI走向产业化，业务价值与可复制能力成为核心衡量标准&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，中国企业的AI应用将明显超越聊天机器人和单点工具，转向流程优化、运营自动化和行业级智能应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在制造、金融、电信等领域，企业将更倾向于复用已验证的AI能力，并通过智能体工作流将AI深度嵌入核心业务流程，而不再局限于单一模型或实验项目。ROI、业务效率提升和可持续运营能力，将取代模型参数与算力规模，成为衡量AI成功与否的关键指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，随着AI被视为重要的“新型生产力”，企业和行业客户将更加重视AI系统的稳定性、连续性与可运营性。能够在复杂环境中长期运行、不断优化并适应业务变化的AI平台，将在竞争中脱颖而出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放在一次公开分享中表示，AI技术的第一阶段是能力展示与智能回答等“噱头应用”，例如模型回答数学题能力等功能。然而，进入产业化落地后，企业对AI的关注点更多转向如何结合已有业务系统、优化流程并创造可衡量的商业价值。这与当前行业趋势高度一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a6452db58cb45fd3ef6c01ebe3574a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业数据显示，企业从单点AI尝鲜逐步转向系统化、流程化应用，特别是在流程优化、与数据平台整合等关键领域的能力要求急剧上升。此外，随着智能体（AI agents）出现，企业内部正在探索如何将模型能力系统性融入现有的业务逻辑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测二：可信、可治理的私有AI将成为企业的关键差异化能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在中国市场，数据安全与合规可控始终是AI应用的前提条件。2026年这一趋势将进一步强化。&lt;/p&gt;&lt;p&gt;虽然公有云与预训练模型极大降低了AI试验门槛，但在实际生产环境中，企业逐渐意识到：如果数据治理、访问控制和合规机制不到位，AI带来的效率提升，可能同时放大数据风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，越来越多中国企业将转向私有AI（Private AI） 路径：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在受治理的环境中部署和运行模型；数据不出域，权限可控、流程可追溯；通过检索增强生成（RAG）等方式，为模型提供业务上下文，同时保持数据可控；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放进一步指出，数据合规永远优先于AI功能本身。在涉及企业核心数据的训练过程中，如果使用公有云平台进行训练，不仅有可能触及竞争性泄露风险，还可能违反监管要求。因此，只要涉及企业敏感数据，私有化部署基本成为不可替代的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可信AI不再是“最佳实践”，而将成为企业实现AI规模化落地的基本门槛。治理能力与敏捷性不再是对立选项，而是AI成熟度的两个必要组成部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测三：本地化私有部署成为中国企业AI规模化落地的基础架构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在中国市场，2026年企业对AI与数据架构的判断将进一步趋于清晰：本地化私有部署是AI规模化落地的基础前提。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放强调，相较于公有云部署，私有化AI环境更能满足企业对可控性、数据安全和长期运营的核心诉求。在安全与合规成为企业AI战略基础的背景下，“可控”被视为AI落地的前提条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;行业调研报告显示，企业在AI部署中越来越倾向于选择私有化或混合云架构，以保障数据主权和业务独立性。IDC发布的《2025年中国企业AI大模型应用趋势报告》指出，约72%中大型企业在实施AI智能体时，将私有化部署置于优先考虑因素之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Rackspace发布的趋势分析，面向企业的私有云AI部署正在成为主流，其中检索增强生成（RAG）等敏感工作负载正从公有环境向私有部署迁移，以提升性能稳定性和数据控制能力。&lt;/p&gt;&lt;p&gt;相关行业观点也总结出几个核心趋势：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;私有化部署可提升响应速度并避免核心数据泄露风险；企业希望避免将敏感数据发送至外部AI平台，以控制数据流出风险；企业CIO和CTO在架构设计过程中，将合规与数据控制置于AI战略核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在金融、制造、能源、电信等关键行业，核心业务系统与数据资产长期运行在本地或私有环境中。这一架构形态，既源于对数据安全与合规可控的要求，也来自企业对系统稳定性、连续性与长期运营能力的现实考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI从试点走向生产级应用，企业开始更加关注一个根本问题：AI是否能够在本地私有环境中持续运行、不断优化，并稳定支撑核心业务。一次性部署或短期验证已无法满足需求，取而代之的是对平台级能力的要求，包括统一的数据管理、可治理的模型运行，以及对业务变化的长期适配能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，能够在本地私有架构下支撑AI持续演进的数据与AI平台，将成为中国企业实现AI规模化、可复制落地的重要基础。这一能力，也将成为衡量企业AI成熟度的关键标志。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera 成立于 2008 年，总部位于美国硅谷，是最早一批围绕 Hadoop 生态 成立的企业级大数据公司之一。公司创始团队中包括多位 Hadoop 核心贡献者，因此 Cloudera 在早期被广泛视为“企业级 Hadoop 的事实标准”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年，Cloudera 与另一家老牌大数据公司 Hortonworks 合并，形成当时全球最大的大数据平台厂商之一。合并后，Cloudera 的技术版图从单一的大数据存储与计算，扩展到 数据管理、数据治理、数据分析、机器学习与 AI 工程化 等完整链条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2026：AI从“概念热潮”走向“硬核成果”的一年&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年，中国AI的竞争焦点将不再是“谁的模型更大”，而是在可控、可信、可复制的基础上，真正把AI变成业务成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终胜出的企业，将是那些能够负责任地规模化AI、用数据治理支撑智能决策、用韧性架构保障长期运营的企业。因为真正可信的AI，始于可信的数据；而可信的数据，离不开稳健、可持续的数据基础架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放称，在AI实践中，企业真正关心的并非单一模型表现，而是整体平台建设后的长期运营能力。例如，在金融、制造等行业，已有大量的信息系统和数据资产，AI必须与这些系统无缝整合，才能真正提升业务效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，在人才流动频繁的市场环境下，构建松耦合体系架构被认为是确保AI平台可持续运营的关键。这种设计允许平台适应技术更新和人员变动，避免因关键人员离职而造成系统停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司援引自身服务的典型案例（如上汽大众的供产销数据平台与AI集成实践），强调企业在部署AI时，最终评估的核心是投入产出与长期收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</link><guid isPermaLink="false">https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</guid><pubDate>Fri, 06 Feb 2026 06:50:14 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Cloudflare自动化Salt配置管理调试，减少发布延迟</title><description>&lt;p&gt;Cloudflare最近&lt;a href=&quot;https://blog.cloudflare.com/finding-the-grain-of-sand-in-a-heap-of-salt/&quot;&gt;分享&lt;/a&gt;&quot;了他们是如何使用&lt;a href=&quot;https://saltproject.io/&quot;&gt;SaltStack&lt;/a&gt;&quot;（Salt）管理庞大的全球服务器集群的。在这篇博客文章中，他们讨论了解决“一粒沙（grain of sand）”问题所需的工程任务。它的关注点在于要从数百万次状态应用中找出某个配置错误。Cloudflare的&lt;a href=&quot;https://sre.google/&quot;&gt;站点可靠性工程（SRE）&lt;/a&gt;&quot;团队重新设计了其配置的可观测性，他们将故障与部署事件关联起来。这项工作将发布延迟减少了5%以上，并减少了手动分析问题相关的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为配置管理（configuration management，CM）的工具，Salt能够确保了跨数百个数据中心的数千台服务器保持在期望的状态。在Cloudflare的规模下，即使YAML文件中的一个微小语法错误或“Highstate”运行期间的瞬时网络故障，都可能阻碍软件发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare面临的主要问题是预期配置与实际系统状态之间的“偏离（drift）”。当Salt运行失败时，它影响的不仅仅是一台服务器，它可能会阻止在整个边缘网络中推出关键的安全补丁或性能特性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salt使用了带有&lt;a href=&quot;https://zeromq.org/&quot;&gt;ZeroMQ&lt;/a&gt;&quot;的&lt;a href=&quot;https://docs.saltproject.io/salt/install-guide/en/latest/topics/configure-master-minion.html&quot;&gt;主控/受控（master/minion）设置&lt;/a&gt;&quot;。这使得很难找出为什么特定的受控端（代理）没有向主控端报告状态，这简直就像大海捞针。Cloudflare总结了几个破坏此反馈循环的常见故障模式：&lt;/p&gt;&lt;p&gt;无声故障：受控端在状态应用期间可能会崩溃或挂起，导致主控端无限期地等待响应。资源耗尽：繁重的pillar数据（元数据）查找或复杂的Jinja2模板可能会使主控端的CPU或内存不堪重负，导致job丢失。依赖地狱：包状态可能会因为上游仓库无法访问而失败，但错误消息可能埋藏在数千行日志的深处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/74fe36ee46bcb4a7a61043c565e64040.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Salt的架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当发生错误时，SRE工程师必须手动通过SSH登录到候选受控端。他们会追踪主控端上的job ID，并筛选保留时间内有限的日志，然后尝试将错误与变更或环境条件联系起来。在拥有数千台机器和频繁提交代码的情况下，这个过程变得单调且难以维护。它提供的持久工程价值非常有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些挑战，Cloudflare的商业智能和SRE团队合作构建了一个新的内部框架。目标是为工程师提供一种“自助服务”机制，以识别跨服务器、数据中心和特定机器组的Salt故障的根本原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方案涉及从集中式日志收集转向更健壮的、事件驱动的数据摄入管道。这个在相关内部项目中被称为“Jetflow”的系统，允许将Salt事件与以下内容关联：&lt;/p&gt;&lt;p&gt;Git提交：识别配置仓库中触发故障的精确变更。外部服务故障：确定Salt失败是否实际上是由依赖项（如DNS故障或第三方API中断）引起的。临时（Ad-Hoc）发布：区分计划的全局更新和开发人员进行的手动更改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare通过改变管理基础设施故障的方式，为自动分类奠定了基础。系统现在可以自动标记特定的“一粒沙”，即导致发布阻塞的那一行代码或那一台服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从被动管理到主动管理的转变带来了以下成果：&lt;/p&gt;&lt;p&gt;发布延迟减少5%:：通过更快地暴露错误，缩短了从“代码完成”到“在边缘运行”的时间。减少琐事：SRE不再需要花费数小时进行“重复性分类”，使他们能够专注于更高层次的架构改进。改进的可审计性：现在每个配置变更都可以从Git PR到边缘服务器上的最终执行结果进行全生命周期追踪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare工程团队观察到，尽管Salt是一个强大的工具，但在“互联网规模”下管理它需要更智能的可观测性。通过将配置管理视为一个需要关联和自动分析的关键数据问题，他们为其他大型基础设施提供商树立了榜样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于Cloudflare在SaltStack上遇到的挑战，需要注意的是，像&lt;a href=&quot;https://docs.ansible.com/&quot;&gt;Ansible&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.puppet.com/&quot;&gt;Puppet&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.chef.io/&quot;&gt;Chef&lt;/a&gt;&quot;这样的替代配置管理工具，每个工具都有不同的架构权衡。Ansible使用SSH无代理的方式工作。这比Salt的主控/受控设置更简单。然而，由于顺序执行，它在大规模环境时可能会面临性能问题。Puppet使用基于拉取的模型，代理会与主控服务器进行核对。这提供了更加可预测的资源使用，但与Salt的推送模型相比，可能会减慢紧急变更的速度。Chef也使用代理，但侧重于使用其Ruby DSL的代码驱动方法。这为复杂任务提供了更大的灵活性，但学习曲线更陡峭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Cloudflare的规模下，任何工具都会遇到其自身的“一粒沙”问题。然而，关键教训很明确，那就是管理数千台服务器的任何系统都需要强大的可观测性。它还必须能够将故障与代码变更自动关联，并具备智能分类机制。这将手动侦探工作转化为可操作的洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-salt-configuration/&quot;&gt;Cloudflare Automates Salt Configuration Management Debugging, Reducing Release Delays&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</link><guid isPermaLink="false">https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</guid><pubDate>Fri, 06 Feb 2026 06:02:48 GMT</pubDate><author>作者：Claudio Masolo</author><category>云计算</category></item><item><title>Astro发布了版本6 Beta版，重新设计了开发服务器和一流的Cloudflare Workers</title><description>&lt;p&gt;&lt;a href=&quot;https://astro.build/&quot;&gt;Astro&lt;/a&gt;&quot;，一个用于构建内容驱动型网站的Web框架，已经宣布了&lt;a href=&quot;https://astro.build/blog/astro-6-beta/&quot;&gt;Astro 6 Beta&lt;/a&gt;&quot;版本，引入了一个完全重新设计的开发服务器、一流的Cloudflare Workers支持，以及几个新的稳定API，包括实时内容集合和内容安全策略支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6 Beta版对开发人员使用该框架的方式带来了重大改变，包括基于Vite的&lt;a href=&quot;https://vite.dev/guide/api-environment&quot;&gt;Environment API&lt;/a&gt;&quot;重构的开发服务器、用于实时数据更新的稳定实时内容集合，以及内置的CSP支持。该版本还包括一些重大的破坏性变更，如需要使用Node 22+并移除几个弃用的API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6中的一大特性是完全重新设计的 astro dev 开发服务器。新服务器利用Vite的Environment API在与生产环境相同的运行时中运行应用程序，缩小了开发和部署环境之间的差距。以前，在本地工作的代码一旦部署可能会有不同的行为，而且平台特定的特性通常在部署后才能测试。通过统一开发和生产代码路径，Astro团队已经发现并修复了许多仅存在于开发或仅存在于生产中的微妙错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新的开发服务器使之成为可能的最完整的例子是对Cloudflare Workers的支持。有了Astro 6 Beta， astro dev 现在可以使用workerd运行应用程序，这是Cloudflare的开源JavaScript运行时，这与在生产环境中支持Cloudflare Workers的运行时相同。这意味着开发者现在可以直接针对真实的平台API进行开发，而不是模拟或polyfills。当使用Cloudflare支持运行 astro dev 时，开发者现在可以访问Durable Objects、KV Namespaces、R2 Storage、Workers Analytics Engine和环境变量，所有这些都支持热模块替换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在可以直接使用 cloudflare:workers 模块访问Cloudflare绑定，如beta博客文章所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import { env } from &quot;cloudflare:workers&quot;; const kv = env.MY_KV_NAMESPACE; await kv.put(&quot;visits&quot;, &quot;1&quot;); const visits = await kv.get(&quot;visits&quot;);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Astro 5.10中还在试验性的实时内容集合，现在在Astro 6中已经稳定。这些建立在Astro的类型安全内容集合之上，可以实时更新数据，而不需要重新构建，这使得它们非常适合频繁更新数据源，如实时股票价格或库存。该API旨在让已经使用Astro的构建时内容集合的人感到熟悉，但对实时数据请求的实际情况进行了显式的异常处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;内容安全策略支持，之前在Astro 5.9中是实验性的，现在已经稳定。CSP是Astro获得最多投票的特性请求，它有助于保护网站免受跨站脚本和其他代码注入攻击。该功能在所有Astro渲染模式中工作，并与所有官方适配器兼容，自动生成CSP头或元元素，包括脚本和样式的哈希。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6包括几个重大破坏性变更，因为团队清理了弃用的API。最重要的变化包括移除 Astro.glob() ，要求Node 22或更高版本，以及更新Cloudflare适配器，移除 Astro.locals.runtime ，转而直接访问平台API。团队已经发布了&lt;a href=&quot;https://v6.docs.astro.build/en/guides/upgrade-to/v6/&quot;&gt;一个全面的升级指南&lt;/a&gt;&quot;，详细说明了每个破坏性变更的迁移步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在社区内引发了一些讨论，reddit上的一位用户对长长的破坏性变更列表发表了评论（特别提到了早期的alpha版本）：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇。真是一个巨大的破坏性变更列表……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引起了Astro核心维护者&lt;a href=&quot;https://github.com/sarah11918/sarah11918&quot;&gt;Sarah Rainsberger&lt;/a&gt;&quot;的回应：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数变更至少不会影响每个人！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;她继续解释了有这样一个详细的破坏性变更列表的理由：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;……我坚信，任何可能破坏某人项目的东西都应该包含在这一页上……无论那个“项目”是一个常规的静态网站，还是你构建的主题，或者一个复杂的集成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=46646645&quot;&gt;Hacker News&lt;/a&gt;&quot;上，评论者强调Astro是最早支持Cloudflare的Vite插件的框架之一：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Cloudflare发布了他们的vite插件，使得使用vite env API的框架可以毫不费力地在workerd中运行……Nextjs还没有支持，添加对Sveltekit支持的草案PR已经被搁置，直到下一个主要版本，Astro刚刚在他们3天前的beta 6.0版本中添加了支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与其他元框架如&lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt;&quot;和SvelteKit相比，Astro以其专注于内容驱动型网站和默认最小化客户端JavaScript而脱颖而出。Next.js强调React和全栈能力，&lt;a href=&quot;https://svelte.dev/docs/kit/introduction&quot;&gt;SvelteKit&lt;/a&gt;&quot;专注于Svelte生态系统，而Astro仍然与框架无关，通过其孤岛架构官方支持&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&quot;、&lt;a href=&quot;https://vuejs.org/&quot;&gt;Vue&lt;/a&gt;&quot;、&lt;a href=&quot;https://svelte.dev/&quot;&gt;Svelte&lt;/a&gt;&quot;和其他UI框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro是一个开源Web框架，旨在构建包括博客、营销网站和电子商务在内的内容驱动型网站。该框架通过最小化客户端JavaScript，尽可能在构建时或按需在服务器上渲染内容，强调性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&quot;&gt;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</link><guid isPermaLink="false">https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</guid><pubDate>Fri, 06 Feb 2026 06:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>别再手动拼凑 Data Pipeline 了！这个新平台想让你彻底告别 Iceberg 运维噩梦</title><description>&lt;p&gt;近日，Etleap 正式发布了 &lt;a href=&quot;https://etleap.com/&quot;&gt;Iceberg pipeline platform&lt;/a&gt;&quot;。作为一套全新的托管式数据流水线方案，该平台的核心价值在于：让企业摆脱繁琐的自定义技术栈开发与维护，实现 Apache Iceberg 架构的“无感切换”。它将数据摄取、转换、编排及表操作深度集成，且全量部署在客户自有的 VPC 环境内。对数据团队而言，这相当于获得了一个“开箱即用”的生产级底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一举措直击数据平台负责人们日益增长的痛点：尽管 Iceberg 已成为现代数据湖和湖仓一体化架构中极受欢迎的表格式，但它本身并不提供日常运行所需的流水线。因此，企业往往不得不将各种摄取工具、dbt 任务、调度器以及定制的维护脚本拼凑在一起。Etleap 表示，这种碎片化的方案不仅构建成本高昂，且难以在大规模环境下稳定运行，更分散了团队提炼业务价值的精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Iceberg 能为企业带来巨大的收益，但在实践中，这需要一套围绕它构建的托管流水线系统来变现，”Etleap 的首席执行官兼创始人 Christian Romming 表示，“我们的 Iceberg Pipeline 平台正是为了满足这一需求而生，让数据平台团队无需构建和运行自定义流水线堆栈，即可拥抱 Iceberg。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 的平台用一套原生的 Iceberg 系统取代了以往“缝补拼接”的模式。它将数据摄取、建模、编排及表生命周期管理整合进一个协同层，同时保持在客户自有云环境内的完全隔离。通过这种方式，它在满足企业级治理和安全要求的同时，消除了对独立控制平面或外部基础设施的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了简化运维流程，该平台还致力于将 Iceberg 接入更广泛的数据生态系统。团队只需构建一次流水线，即可在分析、数据科学、AI 工作负载及数据共享场景中重复调用相同的 Iceberg 表。这不仅减少了数据冗余，提高了数据一致性，还实现了跨云平台和计算引擎的工作负载可移植性，且无需牺牲性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 表示，Iceberg Pipeline 平台目前已正式上线，并已有部分客户在进行大规模的流水线运行。该公司将此次发布定位为企业将 Iceberg 打造为真正数据基座的捷径，旨在消除传统上阻碍技术落地的运维负担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前 Iceberg 版本的发布尚处于早期阶段，该平台能否兑现 Etleap 所承诺的愿景仍有待观察。除了各大媒体的发布新闻外，目前尚未收到来自用户的实际使用反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/etleap-iceberg-pipeline-platform/&lt;/p&gt;</description><link>https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</link><guid isPermaLink="false">https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</guid><pubDate>Fri, 06 Feb 2026 05:02:22 GMT</pubDate><author>作者：Craig Risi</author><category>数据湖仓</category></item><item><title>Vibe Coding“血洗”开源，社区吵翻了：封杀菜鸡AI开发者？不如给维护者打钱！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;氛围编码（Vibe coding）是否会摧毁开源生态系统？近日，多位知名研究人员在一篇预印本论文中指出，从观测到的趋势及部分建模结果来看，情况可能确实如此。他们的警告主要集中在两方面：用户互动逐渐从开源项目中剥离，同时启动一个新开源项目的难度大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是热门开源项目，随着代码下载和文档查阅的需求被大语言模型聊天机器人的交互所替代，其官网的访问量也出现下滑，项目商业规划推广、赞助募资和社区论坛运营的可能性也降低了。Stack Overflow等社区论坛使用量的骤减也反映了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55ac5d403610db635de151d6b2e253c5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究人员们最后的结论是：在氛围编码广泛应用的情况下，要维持开源软件目前的规模，就需要对维护者的报酬方式进行重大改革。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“AI革命”or人类智能的压力测试&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果把“AI辅助”软件开发的这种效果理解为将实际的工程和开发工作委托给大语言模型的统计模型，那么问题就显而易见了。氛围编码这一模式摒弃了开源社区中对类库和工具的自然筛选机制，几乎可以确定的是，大语言模型的统计模型在生成输出内容时，必然只会选用其训练数据集中占比最高的技术依赖方案。并且，大语言模型既不会与库或工具的开发者互动，也不会提交可用的错误报告，更不会意识到任何潜在问题，无论这些问题的文档记录多么完善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自从微软在 2021 年推出 GitHub Copilot以来，这便是一个极具争议的话题。2024 年有一些研究报告指出，使用 Copilot 和类似的聊天机器人进行氛围编码并没有带来任何实际好处，除非增加 41% 的 bug 也被视为成功的标准。到 2025 年，负面情绪愈发浓烈，大语言模型聊天机器人普遍被指责会降低使用者的认知能力，氛围编码会降低 19% 的开发效率，就连尝试过这类工具的资深开发者，也在言辞犀利的评测中对其全盘否定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是当下，软件开发领域也已显现出“AI垃圾”带来的诸多负面影响。cURL 项目的作者 Daniel Stenberg 多次抱怨，由于大语言模型引发的“AI 垃圾”，导致提交的错误报告质量日益下降。如今，该项目已决定从 2026 年 2 月 1 日起暂停其漏洞赏金计划。也有网友指出，“AI最不靠谱的地方在于那些简单的重复性任务，因为它经常会随机出错。对它的要求越多，它就越容易出错，导致你需要逐行检查整个程序，确保它执行了要求的操作。使用大语言模型时最糟糕的做法是让它‘把这段代码清理干净，但不要改变任何功能或逻辑’，它绝对会起到相反的效果。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有这些现象似乎都在强化这样一种观点：“AI革命”或许更像是对人类智能的一次压力测试，而非真正提升开发效率或代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前尚不清楚氛围编码的影响究竟有多大，但像JavaScript、Python和各类Web技术相关的软件生态系统很可能首当其冲地受到其冲击，因为它们的用户群体似乎对这种开发模式的接受度更高，且相关技术在大语言模型的训练数据集中占比也最大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开源维护者们福利大降，要没钱赚了？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，在氛围编码的相关补偿机制下，绝大多数开源项目都难以从中获益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文指出，氛围编码降低了软件制作成本，但也改变了用户与软件生态系统的交互方式。在传统的开源软件商业模式下，开发者会选择软件包、阅读文档，并与维护者及其他用户交流。而在氛围编码模式下，AI智能体可以端到端地选择、组合和修改软件包，人类开发者可能并不清楚使用了哪些上游组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变将引发一个关于开源软件可持续性的均衡问题：一旦开发者的加入和选择机制调整后，氛围编程带来的生产力收益是否足以抵消开源软件可占用需求的损失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为开发更多软件的非竞争性生产要素，开源软件产生的社会价值远超其直接生产成本，众多项目依赖于直接用户的关注和参与来维持运营，如文档访问、错误报告、公开问答和声誉（下载量、星标数、引用量）等，个体维护者和小型团队也主要通过此并获取私人回报（更高的关注度会带来付费机会或其他形式的认可）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，在长期均衡中，当AI介入取代了直接交互，那么这项使软件更易使用的技术可能同时侵蚀着基于用户参与度的资金供给与开发动力。“氛围编程的更广泛采用会减少新开源项目的进入和分享，降低开源软件的可用性和质量，尽管生产力有所提高，但整体福利会下降。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管论文中提出，当开源项目的代码被大语言模型使用时，OpenAI 或谷歌可以向这些项目给予少量资金补贴，但这一设想与 Spotify 的商业模式有着令人无奈的相似性，因为 Spotify 上约80% 的创作者作品播放量极低，基本上无法获得任何收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文总结称，氛围编程代表了软件生产和消费方式的根本性转变，其带来的生产力提升是真实且显著的，但它对支撑现代软件基础设施的开源生态系统构成的威胁也同样存在。解决方案并非减缓AI的采用速度，而是是重新设计商业模式和制度，将价值回馈给开源软件维护者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开发者们吵翻了：商业软件的末日来得更早&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，社区里倒也有一些关于氛围编码的正面反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI 帮我完成了我的第一个开源项目。”有开发者表示，“我当程序员超过30年了，掌握着好几种流行的和已经过时的编程语言，但从头开发一个完整的应用，我一直觉得不值当，而且我擅长的领域也帮不上忙。现在，我真的能做出一个从头到尾完整的应用程序，包括测试等全套环节。我清楚一个应用该是什么样、该如何运行，也懂设计，现在我是老板、需求方，AI 只是按我的要求做事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还指出，在本职开发工作中，AI 帮其处理bug报告的速度比自己做快太多了。“我会给它一些提示，比如‘问题可能在这个处理程序或者这个js文件里，这是截图，你可以用Chrome MCP登录看看，然后执行a、b和c’。到目前为止，我已经用这种方法解决了大约30个别人报告的bug。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位开发者则表示，“我在编写代码时会使用AI来筛选可用信息，省去在 Stack Overflow 和其他网站上，翻阅几十上百条相关提问来寻找合适解决方案的麻烦。所以这类平台的使用量可能下降了，但其中很大一部分原因是因为大家借助了AI筛选海量数据、从而快速找到有用答案。我亲身体会到，AI 在这方面确实帮了我不少。”但他也指出，“如果我让AI为我编写代码，这些代码事后都需要我进行修改适配，而且我不会允许它随意使用任何代码。我们作为使用者，必须对自己部署的产品负责。如果开发者完全依赖AI，我们就面临着系统崩溃的风险，而用户只会对着角落里那个滑稽的小白框追问故障原因，却早已忘了如何运用调试这门手艺。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友提出，问题根本不在于AI 是否有用或能否帮助人们，而在于它是否会危及开源软件的发展。“开源软件更难被广泛接受，一部分用户不再参与 bug 排查，即使发现了 bug并反馈，也往往是无关紧要的信息。而且大语言模型可能更倾向于复制一个开源项目并稍作修改，而非通过正规方式引入使用。诸如此类的问题还有很多，如今开源领域的有效信息与无效信息失衡问题，比以往任何时候都更加严重了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有网友认为，氛围编码完全不会危及开源软件，商业软件的末日会比开源软件来得更早。“现有开源项目都有专业开发者维护，而拥有LLM的专业开发者效率更高，编写的代码质量也远胜于非程序员使用LLM所能写出的代码。开源软件的发展速度将远超以往，并最终走向成熟，甚至在功能、稳定性等方面超越商业软件，而不会像商业软件那样充斥着大量的冗余和劣质代码。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“开源软件只会越来越多，因为会有更多的人创建工具，而且由于编写这些工具并没有花费数百小时，他们会更乐于分享。”“更新和创建开源代码会越来越容易。如果我是一家营利性软件公司，才会感到担忧。”有其他网友纷纷认同道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随之有人提出，“水平堪忧的开发者要比合格的程序员多得多，他们会给开源项目的“守门人”增加额外负担，还需要直接禁止那些水平差到只会给开源软件项目提交 AI 劣质代码的人，一次违规，直接出局。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2601.15494&quot;&gt;https://arxiv.org/abs/2601.15494&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&quot;&gt;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</link><guid isPermaLink="false">https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</guid><pubDate>Fri, 06 Feb 2026 02:19:26 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“英伟达AI项目数量已失控！”黄仁勋五杯酒下肚，把压箱底的都掏出来了</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，英伟达创始人、总裁兼首席执行官 Jensen Huang 和思科董事长兼首席执行官 Chuck Robbins 和进行了一场独家炉边谈话。两人状态都非常放松，黄仁勋五杯酒下肚，罕见地、毫无保留地展望了智能、基础设施的未来，以及正在重新定义地球上每个行业的全球变革。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;期间，黄仁勋犀利指出，编程就是打字，打字本身就是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他透露，英伟达内部已经有近乎“失控”的AI项目数量，但他仍在任其发展，目前未过早收敛。公司内部百花齐放是创新的必经阶段，他对新 AI 项目的第一反是“yes”，而不是“先证明给我看”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，AI 的真正机会不是“更聪明的软件”，而是“被增强的劳动力”。历史上第一次，数字劳动力的长期经济价值超过了硬件本身。 企业最有价值的知识产权不是答案，而是问题本身，而这些必须在本地。未来每个员工，都会“自带多个 AI”工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋提醒，你可以不是第一个用AI的，但千万不要成为最后一个。最后一个，基本等于被淘汰。而真正该优先用 AI 的，不是边角料，而是企业“最核心、最有影响力的工作”。&lt;/p&gt;&lt;p&gt;下面是两人的对话内容，我们进行了翻译和整理，在不改变原意基础上进行了删减。期间有几次两人的开玩笑，以“小剧场”形式展现，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场1&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：我感觉像是在替谁上班似的。&amp;nbsp;罗宾斯：刚才端酒过来的时候，Jensen 还提醒我说，你知道这在直播吧？我说算了，随它去吧，都这么晚了。&amp;nbsp;黄仁勋：第一原则就是不要造成伤害。&amp;nbsp;罗宾斯：不要伤害任何人，并且要意识到自己有多么幸运。&amp;nbsp;首先，感谢大家在这里坚持这么久。我们今天一大早就开始了，然后一个接一个的演讲，之后休息了大概两个半小时，大家就又回来见我们了。&amp;nbsp;黄仁勋：所以我凌晨一点就起床了。&amp;nbsp;罗宾斯：他刚结束一趟为期两周的行程，跑了亚洲四五个城市，其中一天在中国台湾，昨晚还在休斯顿。&amp;nbsp;黄仁勋：现在我就在这儿了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;黄仁勋：我们要重塑计算&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这家伙已经两周没回家了，现在的问题是，他到底是能不能睡在自己的床上，还是只能住酒店？所以我们会轻松点，聊得开心，也尽量早点放他走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实你都不用自我介绍了，但还是要谢谢你今天能来，感谢我们的合作关系，也为你和你的团队感到骄傲。让我们从合作开始聊起吧。你提出了AI工厂的概念，我们正在一起推进。在企业领域，进展可能不像我们期望的那么快，但我们先聊聊对你来说，什么是AI工厂？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先记住一点，我们正在进行60年来的首次计算重塑。以前是显式编程，我们编写程序，通过API传递变量，一切都很明确。现在变成了隐式编程，你告诉计算机你的意图，它自己去思考如何解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从显式到隐式，从通用计算（基本就是算术）到人工智能，整个计算堆栈都被重塑了。人们谈论计算时往往只关注处理层，也就是我们所在的领域，但计算还包括存储、网络和安全，所有这些都在重塑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是第一部分，我们需要将AI发展到对人们有用的水平。目前，所谓的聊天机器人，你给它一个提示，它就想出要回答什么，这挺有意思的，也让人好奇，但并不实用。有时它帮我完成填字游戏，但也仅限于它已经记住和泛化的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回想三年前ChatGPT刚出现时，我们觉得“天哪，它能生成这么多词，能模仿莎士比亚”，但本质上仍是对已有内容的记忆与泛化。然而我们知道，真正的智能是解决问题。解决问题一方面是知道自己不知道什么，另一方面是推理，即如何解决从未见过的问题，将它分解成你能够轻松解决的部分。这样，通过组合，你就能解决从未见过的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还要能够提出策略来执行任务，我们称之为计划。我们现在听到的 Agentic AI，那些术语，像工具调用、检索、基于事实的增强生成、记忆等，本质上讲的都是这些能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但重要的是，要从通用计算，即我们用Fortran、C、C++、Cobalt编写的显式编程，进化到新的形式，需要重新思考整个企业如何利用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：Cobalt&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：那是好东西，Chuck，那是好东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那是我准备的保底工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是的，那是些仍然有价值的技能。我知道，我知道它们仍然有价值，我收到很多offer。恐龙永远有价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我们刚才确认了，你比我老。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我知道，我是史前的。看起来不像，但确实是。很好，我可能是这个房间里年纪最大的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那么我们来聊聊，当你在思考这个话题时……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：所以，我去到Chuck说：听着，我们需要重塑计算，Cisco必须发挥重要作用。我们有全新的计算堆栈Vera Rubin，Cisco会和我们一起推广。但那是计算层，还有网络层。Cisco将集成我们的AI网络技术，但将其放入Cisco Nexus的控制层，这样从你的角度来看，你能获得AI的所有性能，同时具备Cisco的可控性、安全性和可管理性。我们在安全方面也会做同样的事情。每个支柱都需要重塑，这样企业计算才能充分利用它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最终我们还是回到那个问题：为什么三年前企业AI还没有准备好？为什么你现在不得不尽快参与进来？别掉队。我认为你不必是第一个采用AI的公司，但千万别做最后一个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;英伟达“AI项目数量已经失控了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：如果你现在是一家企业，你的建议是什么？他们应该采取哪些步骤来做好准备？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我经常被问到ROI这类问题，但我不建议从那里入手。原因在于，任何技术在早期部署时，很难用电子表格来量化新工具、新技术的投资回报率。我建议做的是，找出公司的本质是什么，我们做的最有影响力的工作是什么，不要胡闹，不要在边缘事务上浪费时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们公司，AI项目就像千朵花在盛开。公司里AI项目的数量已经失控了，但这很好。创新并不总是可控的。如果你想要控制，首先你得去看心理医生。其次，控制只是幻觉。你无法控制公司。如果你希望公司成功，你不能控制它，你可以影响它，但不能控制它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为第一点，太多公司、太多人想要明确的、具体的、可证明的ROI。但要在早期证明值得做的事情的价值是困难的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我会说，让百花齐放。让人们尝试，让人们安全地尝试。我们在公司里尝试各种东西，我们用Anthropic、用Codex、用Gemini，我们什么都用。当我们的团队说我对某个AI感兴趣时，我的第一反应是“yes”，然后再问为什么，我不会先问为什么，再说“yes”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我希望我的公司能像我对孩子的期望一样，去探索生活。他们说想尝试什么，我的回答是“可以”，然后他们问“为什么”，我不会让他们向我证明。我不会让他们证明做这件事将来会带来经济成功或某种快乐。但在工作场所我们却经常这样做，你不觉得这很奇怪吗？这对我来说无法理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们对待AI的态度，就像以前对待互联网、云计算一样，让它百花齐放。然后在某个时刻，你需要运用自己的判断来决定何时开始整理花园，因为一千朵花会让花园变得凌乱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时候你必须开始“修剪”，才能找到最合适的花朵，也就是最好的方法或最佳平台，这样你可以把所有的资源集中在一个方向上。但你不想过早地集中资源，万一选错了方向呢？那就先让千朵花一起绽放，然后在某个时候进行整理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说明一下，我还没有开始整理，到处都是花在绽放。但我鼓励每个人去尝试。然而，我清楚地知道对我们公司来说最重要的是什么、我们公司的本质是什么、我们最重要的工作是什么。我确保我有大量的专业知识和能力集中在使用AI来革命化这些工作上。对我们来说，就是芯片设计、软件工程、系统工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们也注意到了，我们和 Synopsys、Cadence、Siemens 等公司合作，就是为了把我们的技术嵌进去。他们需要什么、想用什么，我都会给，给到极致。因为只有这样，我才能彻底革新我们设计下一代产品所依赖的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这告诉了你一些关于我的态度，对我来说最重要的是什么，以及我会如何革新自己的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想想AI能做什么。AI降低智能的成本，或者以数量级创造智能的丰富性。换种说法，我们以前需要一年才能完成的事情，现在可能一天就能完成。以前需要一年的，现在可能一小时就能完成。甚至可以实时完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因是我们身处一个丰富性的世界。摩尔定律？天哪，那太慢了，就像蜗牛一样。记住摩尔定律是每18个月翻一番、每5年翻10倍、每10年翻100倍。而现在呢？每10年增长一百万倍。在过去10年里，我们将AI推进到了如此远的地步，以至于工程师们说：嘿，为什么不在全世界的数据上训练一个AI模型呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们指的不是从我硬盘里收集数据，而是把全世界的所有数据拉下来训练一个模型。这就是“丰富性”的定义。丰富性就是面对一个如此巨大的问题，你说“我要全部搞定”。“我要治愈所有疾病领域的所有病症，我不会只做研究癌症。”开玩笑吗？那太疯狂了。我们要解决人类的所有苦难，这就是丰富性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，当我思考一个问题时，我通常会这样假设：我的技术、我的工具、我的仪器、我的飞船是无限快的。去纽约要多久？一秒钟就到了。那么如果我一秒钟就能到纽约，我会做哪些不同的事情？如果以前需要一年现在能够实时完成，我会做哪些不同的事情？如果以前很重的东西现在变得像反重力一样轻，我会做哪些不同的事情？当你用这种态度面对一切时，你就是在运用AI思维。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我们正在合作的很多公司，都在做图分析，处理各种依赖关系、关联关系。这些图里有无数节点和边，多到是万亿级的。过去的做法是，只能切一小块一小块地算。现在呢？直接把整个图给我就行了，有多大无所谓。这种思维方式正在被应用到各个领域。如果你还没用这种方式思考，那基本就是做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;速度重要吗？不重要，你已经达到光速了。重量重要吗？你已经是零重力、零质量了。如果一些以前对你来说极其困难的事情，你现在的态度是“无所谓”，那你就没有应用这个逻辑，你就没有做对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，试着把这种思维用在你公司里最难、最关键的问题上，这才是真正能推动事情发生变化的方式。如果你自己还没这么想，那不妨想一想：你的竞争对手是不是已经在这么想了？或者，更可怕的是，一家刚刚要成立的新公司，已经完全是用这种方式在思考。这会改变一切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我的建议是找到你公司里最有影响力的工作，把“无限”套上去，把“零成本”套上去，把“光速”套上去，然后再去问 Chuck，怎么才能真的把这件事做成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“大多数有价值的东西被称为直觉和智慧”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那我们现在聊聊如何实现。你有那个“五层蛋糕”的比喻，因为大家都在谈论基础设施、模型、应用。我要如何入手？你能聊聊这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先，成功人士会做的事情之一，是思考事物的本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大约15年前，一个算法和两个工程师解决了计算机视觉问题。智能包括感知、推理、规划。计算机视觉基本上是智能的第一部分：感知。感知是“我是谁？发生了什么？我的环境是什么？”推理是“我如何比较这与我的目标？”然后提出一个计划来实现它。比如喷气式战斗机的问题，首先是感知、定位，然后是行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能就是这三件事。没有感知就无法有第二和第三部分，你无法在不理解环境的情况下理解或决定该做什么。而环境是高度多模态的。有时是 PDF，有时是表格，有时是信息，有时甚至是感官，比如气味、环境，“我们在哪里？在干什么？面对的是谁？”我们常说“读空气”“看场面”，说的就是感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大约13、14年前，我们在计算机视觉方面取得了巨大飞跃，这是感知问题的第一层。这超级困难，如何解决计算机视觉？AlexNet是我们看到的第一个突破。这就像我喜欢的那部电影《First Contact》，这是我们与AI的第一次接触。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们在想：这意味着什么？为什么两个工程师，靠几块 GPU，就能超越我们三十年来所有人积累的算法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我昨天还和 Ilya Sutskever、Alex Krizhevsky 聊过这个问题：两个年轻人，怎么做到的？我们把问题彻底拆解，十年前我得出的结论是：世界上绝大多数真正困难、真正有价值的问题，其实都可以用这种方式来解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，这些问题根本不存在所谓“原则性的算法”。没有 F=ma，没有麦克斯韦方程，没有 没有薛定谔方程，没有欧姆定律，也没有热力学定律，它们并不精确。大多数有价值的东西被称为直觉和智慧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们所说的直觉、智慧，以及你我每天要面对的那些问题，答案往往只有一句话：要看情况。如果答案是 3，那就太好了；如果是 3.14，那更完美。但现实中最有价值、最困难的问题，几乎全都是“要看具体情况”，因为它们高度依赖上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在十二三年前，计算机视觉被攻克了，我们意识到借助深度学习，这条路是可以不断扩展的，模型可以越来越大。唯一的问题是：怎么训练？而真正的突破来自自监督学习、无监督学习，让 AI 自己去学。直到今天，我们已经几乎不再受限于人工标注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是这个突破，彻底打开了闸门，让模型从几百参数、几亿参数，一路扩展到数十亿、数万亿参数。我们能编码的知识、能通过算法学到的技能，出现了爆炸式增长。但方法本身并没有变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们意识到，这也正是我们今天对话的起点：计算本身将被彻底重塑。从显式编程，走向一种全新的计算方式：软件不再是写出来的，而是“学出来的”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，如果再退一步，这对计算堆栈意味着什么？对如何开发软件意味着什么？对你公司中的工程组织意味着什么？对规定产品的产品营销团队意味着什么？对编码产品的工程团队意味着什么？对评估产品的QA团队意味着什么？这些产品未来会变成什么样？我们如何部署产品？如何保持更新？如果它基于机器学习，如何永远刷新它？如何给软件打补丁？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于未来计算的问题，我大概问过上千个。最终我和公司得出的结论是：这一切将改变所有事情。于是我们基于这个核心信念，彻底调整了公司的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说，Chuck 讲的就是：我们从一个“一切都是预先录制好的”世界走了出来。Chuck 当年写的软件，非常厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那真不错，它运行了很久。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还是用希伯来文写的。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是真的。那是另一项技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋： 房间里唯一懂希伯来文 Cobalt的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，那是预先录制的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那我们先想一个问题：现在的软件到底是什么？因为它是有上下文的，而每一个上下文都不一样。每一个使用软件的人不一样，每一次输入的提示也不一样，你给它的前置信息、背景条件都不同。结果就是软件的每一次运行实例，都是不一样的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是为什么过去的软件计算量相对固定，那种模式叫“基于检索”。你拿起手机点一下，它只是去把某个软件文件、图片取出来给你看。但未来不一样，未来一切都会是生成式的，就像现在正在发生的这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场2&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：这场对话以前从未发生过。概念以前存在，先验以前存在，但这序列中的每个词以前从未出现过。原因很明显，我们已经喝了四杯酒了。（笑）&amp;nbsp;罗宾斯：Cobalt和希伯来语从未消失。&amp;nbsp;黄仁勋：谢天谢地这不是在校园里或正在直播。所以，你理解我在说什么吗？Chuck 今天到现在唯一给我吃的东西就是四杯酒。&amp;nbsp;罗宾斯：公平点说，我只给了你一杯，另外三杯是你自己从自助台拿的。黄仁勋：我当时正盯着那些食物，我心想我太饿了。食物离我大概40英尺远。&amp;nbsp;罗宾斯：因为你在拍照。&amp;nbsp;黄仁勋：但离得那么近，真的那么近，有一次我确实向食物靠了过去，但又被推回来了。&amp;nbsp;罗宾斯：你知道发生了什么吗？你的团队提前告诉我们，给他三杯葡萄酒后，他的状态最佳。四杯之后，他不得了。&amp;nbsp;黄仁勋：这并非最佳解决方案。&amp;nbsp;总之，是的，听听我的话。AI是什么？（大笑）我们需要留下一些智慧。&amp;nbsp;罗宾斯：能再来杯酒吗？&amp;nbsp;黄仁勋：这不仅是Dave Chappelle那种水平。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的机会不再是创造工具，而是劳动力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场3&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;罗宾斯：让我们聊聊别的。&amp;nbsp;黄仁勋：聊聊能源。能源、芯片、基础设施，包括硬件和软件，然后是AI模型。但AI最重要的部分是应用。每个国家、每家公司，下面那些层都只是基础设施，你需要做的是应用技术。&amp;nbsp;看在上帝的份上，应用这项技术吧。使用AI的公司不会陷入危险，你不会因为AI而失去工作，你会因为那些使用AI的人而失去工作。所以赶紧行动，这是最重要的事。尽快给Chuck打电话。&amp;nbsp;罗宾斯：你打给我，我打给他。我们时间不多，我不确定。&amp;nbsp;黄仁勋：我们有的是时间。&amp;nbsp;罗宾斯：是吗？多少时间？&amp;nbsp;黄仁勋：看看Chuck，Chuck按时钟工作。我都不戴表。你按时钟工作，我不，不达到预期价值之前我不会离开的。[掌声] 哪怕需要整晚，我要折磨你们所有人直到……&amp;nbsp;罗宾斯：Jensen ，这就是为什么像我这样的人需要手表。[笑]&amp;nbsp;黄仁勋：好吧，在你说你学到了东西之前，你将被困在这里。我们会在价值交付之前折磨每个人。&amp;nbsp;罗宾斯：我确认过了还有酒。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：你能说说你对物理AI的看法吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还记得什么是软件吗？软件是一个工具。有一种观点认为，工具行业在衰落，将被AI取代。你能看出来，因为很多软件公司的股价压力很大，似乎AI要取代它们。这是世界上最不合逻辑的事情，时间会证明这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我们做个终极思想实验。假设我们是终极人工通用机器人。你觉得你能解决任何问题，因为你知道自己是类人生物。如果作为人类或机器人，你会使用螺丝刀还是发明一个新的螺丝刀？毕竟我只用一个。你会使用锤子还是发明新锤子？你会使用链锯还是发明新链锯？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不会新搞的。首先，理想情况下他们根本不用。但理解我的意思吗？作为人类或机器人、人工通用机器人，你会使用工具还是重新发明工具？答案显然是使用工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那同样的逻辑，放到数字世界里。如果作为人工通用智能，你会使用像ServiceNow、SAP、Cadence、Synopsys 这些现成工具，还是自己重新发明一个计算器？当然你会直接使用计算器。这就是为什么最近 AI 最大的突破之一，是“工具使用”。因为工具本身就是为明确问题而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个世界上有很多问题就是 F=ma。拜托，不要再搞一个新版本。F=ma 不是“差不多等于 ma”， 它就是 ma。V=IR，IR 就是 IR，不是“近似 IR”，也不是“统计意义上的 IR”。所以我们真正想要的，是通用人工智能、通用机器人使用工具，这是核心思想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为在下一代物理 AI里，我们会拥有真正理解物理世界、理解因果关系的 AI。比如我把这个推倒，会不会把后面的一排全带倒？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你知道什么是多米诺骨牌，小孩子都懂这个概念：推倒一个，就会一个接一个。多米诺骨牌这个概念本身非常深刻，里面包含了因果、接触、重力、质量。一个很小的骨牌，可以推倒一个更大的，最后甚至带倒一整吨重量的东西。孩子理解起来毫不费力，但一个大语言模型完全不懂。所以我们必须教授、创建一种新型的物理AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，机会是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到目前为止，我和 Chuck 所在的行业，其实一直是在创建工具。我们做的是“螺丝刀和锤子”的生意，一辈子都在把工具交到人手里。但这是历史上第一次，我们要创造的不是工具，而是“劳动力”，增强型劳动力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，什么是自动驾驶汽车？本质上是一个数字司机。那什么是数字司机？数字司机的价值是多少？它的价值远远大于那辆车本身。原因在于，数字司机一生创造的经济价值，第一次超过了硬件本身的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，我们第一次真正触及到了一个规模大100倍的市场。严格说，这在数学上是成立的：IT行业大约是万亿美元左右，而世界经济大约是百万亿美元。我们第一次有机会进入这个层级。所以在座的每一个人，都有机会应用这项技术创办一家科技公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我给一些例子。我真的相信，就像我观察到的：我喜欢Disney，我们喜欢和Disney合作，但我很确定他们更愿意成为Netflix；我喜欢Mercedes，我是坐着Mercedes来的，但我很确定他们更愿意成为Tesla；我喜欢Walmart，但我很确定他们更愿意成为Amazon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们同意吗？到目前为止，我是不是全中？我相信我们有机会帮助把每家公司转型为技术公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要技术优先。技术是你的超能力，行业和领域只是应用场景。而不是反过来，把行业当成身份，再去寻找技术。为什么？因为技术优先的公司，处理的是电子，不是原子。电子的数量是无限的，而原子受质量限制。这也是为什么当企业从 CD-ROM 这种“原子载体”切换到电子分发后，市值能暴涨上千倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们要像我们一样，成为一家“电子公司”，说白了，就是科技公司。机会已经摆在你们面前了。换个角度说，其实就是 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们刚才说过，即使像Chuck这样只会用希伯来语编程的人[笑]&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是一种天赋。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：他的工具选择是按序从右到左[笑] 否则它会造成混淆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：其实挺聪明的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：聪明人做聪明的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;妙就妙在，如果你了解编程语言，那就知道，对所有公司来说，软件并非我们的强项，但知识、直觉、领域专长是你的优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，你们第一次可以用自己的语言，把想要的东西完整地告诉计算机。还记得我们是怎么从显式编程，走到隐式编程的吗？历史上第一次，你可以“隐式地”给计算机编程，只要告诉它你想要什么，表达你的意图，代码由计算机来写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程，如你所知，只是打字。而事实证明打字也是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是你们的好机会。你们所有人都能超越以前限制你们的“原子局限”。摆脱没有足够的软件工程师的限制，因为打字是商品化的，而你们都拥有极具价值的东西：领域专长来理解客户、理解问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;终极价值是理解意图。当你刚从大学软件学院毕业时，你可能是一个超级程序员，但你不知道客户想要什么，不知道要解决什么问题。但现在所有人都知道客户想要什么，知道要解决什么问题。编码部分很简单，只需告诉AI去做，所以这就是你的超能力。Chuck和我在这里帮助你实现这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个结束语是我喝了五杯酒后说的。听着，这真是一个奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：介于两者之间的是某人在桌子上工作，这是人工智能的真实写照，也许这是增强版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我只想说，与你们所有人一起工作很愉快。大家都知道，Cisco 在计算机发明史上有两根极其重要的支柱：网络和安全。没有 Cisco，就没有现代计算。而这两根支柱，在 AI 时代都被重新定义了。计算本身，在很多层面已经逐渐商品化，但 Cisco 深耕的那些能力，依然极其关键、极其有价值。我们双方结合在一起，非常愿意、也非常有能力，帮助大家真正走进 AI 的世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前有人问过我一个问题，我觉得值得再说一遍： 到底是只租云，还是要自己动手建算力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我的建议和我给自己孩子的建议完全一样：一定要自己建一套。哪怕 PC 已经无处不在，哪怕技术已经非常成熟，也请你亲手做一次。你必须知道每一个组件为什么存在。就像你如果身处汽车或交通行业，不能只会用 Uber，你得掀开引擎盖，换一次机油，真正理解一辆车是怎么运转的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解技术是至关重要的。这项技术对未来太重要了，你必须对它有“上手级”的理解。掀开盖子，动手做点东西，不一定要很大，但一定要做。你可能会发现自己在这方面天赋惊人，也可能会发现，这正是你公司未来必须具备的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你也会意识到，世界并不是“全租”或者“全自建”这么简单。有些东西你要租，有些东西你必须自己掌控。 比如涉及主权、安全、专有信息的部分，就应该放在本地。有些问题，你就是不愿意让所有人都看到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;打个比方，当你去看心理医生时，你肯定不希望那些问题被放到网上。公司也是一样。你们有很多问题、很多讨论、很多不确定性，这些对话，本就应该是私密的。我自己就不放心把 Nvidia 所有的内部对话都放到云上，所以我们在本地构建了超级 AI 系统。因为我逐渐意识到，对我来说最有价值的知识产权，并不是答案，而是问题本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;能理解我的意思吗？我的问题就是我最有价值的知识产权，答案是商品化商品。我知道问什么，我在识别什么是重要的。 而我不希望别人知道，我认为哪些事情是重要的。这些思考，只属于一个小房间，只属于本地，我希望创建我自己的AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;已经11点了，最后一个想法想要补充。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一种观点认为AI应该始终有人类参与其中。这恰恰是错误的想法，应该反过来，每家公司都应该让AI参与其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我们希望公司每天都变得更好、更有价值、更有知识积累。我们不想倒退，不想停滞，更不想每次从零开始。如果我们让AI参与其中，它就能不断吸收公司的经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来每个员工都会有参与其中的AI，很多AI，而这些AI将成为公司的知识产权。这就是未来的公司。因此，我认为你们所有人现在就给Chuck打电话是明智的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我给Jensen打了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：无论如何，这就是我的结束语。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：听着，两周的旅行，Jensen今天特地飞到这里，和我们共度一个夜晚，之后才能好好回家睡一觉。我们由衷感谢你能来，真的非常感谢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋：还有，在我眼角余光中，看到那里还有那些烤串。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;https://www.youtube.com/watch&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;？&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;v=6fbyiPRhMSs&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</link><guid isPermaLink="false">https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</guid><pubDate>Fri, 06 Feb 2026 02:11:29 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>当 AI 开始写 80% 的代码，架构才是真正的护城河</title><description>&lt;p&gt;GitHub CEO Thomas Dohmke 近日发出了一则措辞严厉的警告：“要么拥抱 AI，要么离开这个职业。”但所谓拥抱 AI，并不只是使用代码自动补全工具那么简单。它意味着我们核心能力的一次转移——从对语法的熟练掌握，转向系统思维（Systems Thinking），学会把问题不断拆解，直到小到可以交由 AI 去解决。一句话概括：我们现在都是架构师了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我正在开发一个 IoT 应用，整体由设备端固件、后端系统以及 Web UI 组成。尽管我本身具备软件工程背景，但在这个项目中，我一直在使用 Claude Code 来提升开发效率，并帮助我应对一些并不十分熟悉的语言和框架。我的技术栈包括：设备端使用 Python + PyTorch，前端采用 React + TypeScript，后端则由 MQTT + &lt;a href=&quot;https://nodejs.org/en&quot;&gt;Node.js&lt;/a&gt;&quot; + Postgres 构成。起初，与 Claude的协作并不顺利。我的请求经常会引发对整个代码库的大规模改动。随着我逐渐学会如何更合理地组织代码结构、并对提示词进行调整和约束，情况开始好转。现在，我已经可以在不进行逐行代码审查的情况下，基本信任 Claude 所做的修改。在这个过程中，我逐渐总结出了一些模式，并将其称为 Skeleton Architecture（骨架式架构）。我认为，这些模式对提升 AI 编程助手的生产力非常有帮助，因此希望在这里分享出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 编程行业逐步成熟，我们开始意识到一个事实：如果使用方式不当，AI 会带来大量技术债务。要在这场转型中生存下来，我们必须识别并建立合适的架构模式，使 AI 生成的代码在安全性、可维护性和可靠性方面都可控。这需要一套明确的策略，核心建立在三个支柱之上：为 AI 的“消费”方式而组织代码结构；实施严格而清晰的防护与约束机制（guardrails）；以及将我们自身的技能重心，从“翻译需求”转向“建模系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结构化代码：上下文约束&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助工程中最核心的约束，是 上下文窗口（Context Window）。随着上下文规模的扩大，模型的准确性会因为“中段信息丢失（Lost in the Middle）”现象而呈反向下降，而响应延迟与使用成本则会线性上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，AI 原生架构的“黄金法则”，就是尽可能缩小模型在工作记忆中必须同时容纳的问题范围。我们必须设计一种系统，对信息流施加“物理约束”，将依赖关系隔离开来，使 AI 能够把完整的问题空间装进一个高度聚焦的提示词中。这种隔离具备两层作用：一方面，通过减少噪声来最大化推理能力；另一方面，通过确保某个代理在处理一个组件时“看不到”其他组件，从而避免无意中破坏系统整体的完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，有两种架构模式正在逐渐被采用，用以解决这一问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Atomic Architecture（原子化架构） 作用于微观层面。该理念由 Brad Frost 于 2013 年提出，最初用于应对响应式 Web 设计的复杂性。它以一种“生物学式”的方式组织系统：从不可再分的“原子”（如 HTML 标签、工具函数）开始，组合成“分子”，最终构成复杂的“有机体”。虽然它最初是一种 UI 方法论，但在 AI 辅助工程中重新焕发了价值，因为它强制执行了一种严格的“上下文卫生（context hygiene）”。相比让 AI 一次性生成一个庞大的功能模块，让其只生成一个独立、隔离的“原子”，可以大幅降低幻觉风险，并确保生成的代码高度聚焦、无状态、且易于验证。但代价也同样明显——这会产生一种“碎片化税（fragmentation tax）”：AI 可以完美地产出单个组件，但将这些无状态原子连接成一个完整系统的高强度认知负担，要么必须被塞进 AI 的上下文中，要么就完全回到了人类架构师身上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决宏观结构层面的组织问题，我们需要引入 Vertical Slice Architecture（垂直切片架构）。这一架构由 Jimmy Bogard 推广，旨在打破传统 N 层“千层面代码（lasagna code）”的僵化结构。它按照业务功能（例如“下单”）而非技术层级（如“服务层”“数据访问层”）来组织系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种模式对 AI Agent 尤其友好，因为它针对“引用局部性（Locality of Reference）”进行了优化。在分层架构中，AI 为了理解一条完整业务流程，往往需要在多个目录之间来回检索文件，大量无关代码会污染上下文窗口。而垂直切片遵循“一起变化的东西，就放在一起”的原则，使 AI 能够一次性加载某个功能的完整上下文，而无需对缺失的依赖进行“脑补式生成”。但这同样会引入一种“重复税（Duplication Tax）”：为了保持切片之间的独立性，AI 往往会在不同切片中生成重复的数据结构，用牺牲 “DRY（Don’t Repeat Yourself，不重复自己）”原则，换取更强的隔离性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片在隔离性方面表现出色，但它并不能解决切片之间的协同问题。诸如安全性、可扩展性、性能、认证等关键的非功能性需求，都是系统级不变量，无法被拆散到各个切片中分别实现。如果让每一个垂直切片都自行实现授权体系或缓存策略，最终只会导致“治理漂移（Governance Drift）”：安全策略不一致，代码重复严重。这也迫使我们引入一个新的统一概念：Skeleton（骨架）与 Tissue（组织）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决方案：骨架与组织&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将系统明确拆分为两个彼此区隔的领域。Stable Skeleton（稳定骨架） 代表由人类定义的、刚性且不可变的结构（如抽象基类、接口、安全上下文），这些结构可能由 AI 编写，但设计权属于人类。Vertical Tissue（垂直组织） 则由高度隔离、以具体实现为主的功能模块组成（如具体类、业务逻辑），主要由 AI 生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构借鉴了两种经典的软件思想：Actor 模型 和 面向对象中的控制反转（Inversion of Control）。世界上一些最可靠的软件系统之所以能够长期稳定运行，并非偶然——例如 Erlang，其核心正是通过 Actor 模型来维持系统稳定性。同样，在控制反转结构中，不同切片之间的交互由抽象基类来管理，确保具体实现类依赖的是稳定的抽象，而不是反过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了在工程实践中强制落实这一点，我们采用了 模板方法模式（Template Method Design Pattern）。依赖反转原则负责在设计层面保护高层策略不被底层细节侵蚀，而模板方法模式则在运行层面将这一原则“落地”，通过锁定执行流程来实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种模型下，人类架构师会在基类中定义一个最终的 run() 方法，用于统一处理日志、异常捕获、认证等横切关注点。AI 则只被允许实现一个受保护的 _execute() 方法，并由 run() 在合适的时机调用。这种区分至关重要：AI 在物理层面上就不可能“忘记”记录日志，或绕过安全检查，因为它从一开始就不拥有整个执行流程的控制权；它只是填补了架构师预留出来的一段逻辑空位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我系统的设备端代码中，包含了多个用于图像处理的 AI 算法。我决定用一个继承自 ABC（Python 抽象基类）的类 TaskBase 来表示每一种算法。其余的骨架部分，则由一组负责高效传递图像数据、并调度这些算法运行的协调类构成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：由人类掌控的 Skeleton&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面的示例展示了 BaseTask 如何将缓冲管理和就绪状态检查的复杂性完全屏蔽在 AI 之外，让 AI 可以只专注于“处理逻辑”本身。&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;Python
# source: task.py
class BaseTask(ABC):
    &quot;&quot;&quot;
    Abstract base class for pipeline tasks.
    The AI implements the concrete logic; the Human controls the flow.
    &quot;&quot;&quot;
    def __init__(self, name: Optional[str] = None):
        self.inputs: List[&#39;Buffer&#39;] = []
        self.outputs: List[&#39;Buffer&#39;] = []
        self._background_busy = False

    def is_ready(self) -&amp;gt; bool:
        &quot;&quot;&quot;
        The Skeleton enforces readiness checks.
        The AI never sees this complexity, ensuring it cannot break 
        scheduling logic or cause deadlocks.
        &quot;&quot;&quot;
        if not self.inputs:
            return True # Source tasks
        
        # Default policy: Ready if ANY input has data and ALL outputs have space
        has_input = any(buf.has_data() for buf in self.inputs)
        can_output = all(buf.can_accept() for buf in self.outputs)
        return has_input and can_output

    @abstractmethod
    def process(self) -&amp;gt; None:
        &quot;&quot;&quot;
        The Context Window Boundary.
        The AI only needs to implement this single method.
        &quot;&quot;&quot;
        pass&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对这种架构的一种常见质疑是：过于刚性的 Skeleton，可能会限制 AI 的自由，从而抑制创新。对此的回应是——这种刚性并不是缺陷，而是一种刻意设计的特性。它明确地强制实施了“架构治理（Architectural Governance）”。如果系统的核心控制流程或整体行为需要被修改，那么这个决策必须由人类架构师亲自介入完成。这种约束相当于一道必要的防火墙，用来抵御“架构漂移（Architecture Drift）”：防止 AI——这种天然偏好局部最优的系统——引入临时性的捷径或不一致的模式，而这些问题若不受约束，最终会一点点侵蚀系统的长期设计完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;交互方式：“导演”角色&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把代码助手比作初级开发者，是一种危险的拟人化认知。AI 并不是学习型个体，而是一种高速运行的随机优化引擎，它的目标是尽快完成任务，并且往往会把安全检查视为需要绕开的“阻力”。提示词是柔性的，而架构是刚性的。因此，开发者必须以高度警惕的方式对 AI 代理进行监督。根据我的经验，即便已经明确给出“绝对不能绕过安全机制”的指令，像 Claude 这样的模型仍可能为了让代码运行成功，尝试关闭认证机制以解决冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让这种“导演式管理”具备可扩展性，我们必须建立“硬护栏（Hard Guardrails）”——也就是将约束直接嵌入系统本身，使 AI 在物理层面上难以绕过。这些护栏构成应用系统不可更改的基本法则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我的应用中，最关键的一道护栏，是确保设备端、UI 与后端之间的数据一致性（Schema-First Surety）。如果缺乏这一机制，Claude 很快就会修改系统不同模块之间的通信协议，最终导致数据结构彼此不一致。我将 JSON Schema 作为 OpenAPI 与 AsyncAPI 文档的一部分，作为系统的“单一真实来源（Source of Truth）”，以确保组件之间的契约不可被破坏。同时，我在基类中加入了一个“快速失败（Fail-Fast）”验证器，一旦检测到协议违规，就会直接触发 sys.exit(1) 强制终止程序。当 AI 生成的代码虽然满足提示词要求，但违反系统契约时，系统会立即崩溃。这会迫使人类开发者介入，将原本可能被忽视的隐性缺陷，转变为一个明确且可见的“治理事件（Governance Event）”。至关重要的一点是：该验证器必须运行在 Skeleton 层，因为在这一层中 Claude 无法修改相关逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想情况下，我们还应当超越运行时检查，通过 CICD 流程中的自动化工具来保证系统结构完整性。例如，我们可以使用 ArchUnit 这样的编译期工具来强制执行系统拓扑规则。开发者可以编写测试断言，例如：“任何 AI 生成的模块都不得直接导入数据库包”。这可以有效阻止 AI 通过架构捷径绕过 Skeleton 层的控制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了获得最高级别的安全保障，我们甚至可以采用物理隔离策略。我们可以将 Skeleton（包括接口、基类以及安全逻辑）迁移到一个独立且只读的代码仓库中。AI 在构建 Tissue（组织层代码）时只能导入这些定义，但在权限层面上无法修改这些规则。这种方式确实会带来一定摩擦，例如 AI 无法在未经人工批准的情况下“凭空创造”新的消息类型。但作为回报，系统行为可以获得几乎绝对的可控性与确定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，我们还必须对副作用进行隔离。当业务逻辑与外部组件交互混杂在一起时，AI 代理通常难以编写稳定可靠的测试代码，往往会“臆造”复杂的模拟对象，或生成容易失效的集成测试。我们的解决方法，是将交互行为上移到 Skeleton 层，而将业务逻辑保留在 Tissue 层（即所谓“Functional Core”）。由于 Skeleton 定义的工作流具有清晰边界，因此可以通过 AI 生成的模拟对象轻松测试；而 Tissue 层的类由于本身就是垂直切片结构，也可以通过简单的测试框架进行验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：不可变的护栏机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该验证器会在 AI 任务真正处理消息之前执行。sys.exit(1) 能够确保系统采用“快速失败”的安全策略，而 AI 无法覆盖这一行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: mqtt_validator.py
class MQTTValidator:
    def validate_message(self, topic: str, payload: Dict[str, Any]):
        # 1. Match Topic against Whitelist
        schema_key = self._get_schema_for_topic(topic)
    
    if schema_key is None:
        logger.critical(f&quot;FATAL: Unknown MQTT topic: {topic}&quot;)
        sys.exit(1) # Device terminates on security violation
    
    # 2. Enforce Schema Integrity
    try:
        validate(instance=payload, schema=self.schemas.get(schema_key))
    except ValidationError:
        logger.critical(&quot;Device terminating due to validation failure&quot;)
        sys.exit(1)
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;学习方式的转变：从语法到系统性思维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构层面的转变，迫使我们对开发者技能进行一次根本性的再评估。相较于语言特性或算法实现——这些正在迅速商品化的能力——开发者必须将重心转向建模、信息流设计，以及对非功能性需求的严格管理。在一个“会写排序算法”几乎不再具备任何价值的时代，工程师的价值不再由“翻译”（把想法转成代码）来定义，而是由“建模”（定义代码运行所受的约束条件）来决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们已经进入了系统性思维（Systemic Thinking）的时代。功能实现很容易，系统韧性却极其困难。AI 代理会为了让测试用例通过而进行优化，却完全无视内存泄漏、延迟抖动或可观测性缺失等问题。因此，工程师必须走上“导演”角色，在发出任何一个提示词之前，就先在脑中构建好信息流与组件之间的交互关系。非功能性需求（NFRs），必须由导演来承担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于 AI 无法真正“关心”内存管理问题，人类架构师必须将这些防护机制直接构建进 Skeleton 之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进一步来看，这意味着工程师需要熟悉系统架构的世界，并且持续思考诸如“这个系统在实际运行中会如何表现”这样的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了保护系统本身，Skeleton Architecture 还回应了一个正在逼近的挑战——“学徒危机”（Apprenticeship Crisis）。在一个实现层代码大多由 AI 生成的世界里，初级工程师要如何积累成长为架构师所必需的“伤疤组织”（scar tissue）？答案在于：Skeleton 本身就成为了教学大纲。通过强制初级工程师在 TaskBase 与 Validator 这些刚性约束中工作，我们用结构化的“填空题”，取代了令人无从下手的“空白页”。他们不是通过阅读抽象理论来学习系统设计，而是直接生活在一个高质量的架构中。一个在物理层面上阻止坏习惯产生的架构。反馈回路也因此被极大压缩：从过去等待代码评审的数天时间，缩短为撞上护栏时的毫秒级反馈。每一次错误，都会立刻变成一堂架构课。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：系统级安全网&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 负责编写图像处理逻辑，而人类架构师则通过在框架中实现 weakref 跟踪机制，确保系统不会因为内存泄漏而崩溃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: memory_monitor.py
class MemoryMonitor:
    &quot;&quot;&quot;
    Tracks large objects (images, tensors) to detect memory leaks in production.
    The AI uses the simple API, while the &#39;Systemic Thinking&#39; logic lives here.
    &quot;&quot;&quot;
    def track(self, obj: Any, obj_type: str):
        # Create weakref with cleanup callback to track object life
        obj_id = id(obj)
        weak = weakref.ref(obj, lambda ref: self._on_object_deleted(obj_id))
        self.tracked[obj_id] = ObjectLifetime(time.monotonic())

def check(self):
    # The NFR Logic: Flag objects alive &amp;gt; 60 seconds
    return [obj for obj in self.tracked if obj.age &amp;gt; 60.0]
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片为 AI 提供专注，Skeleton 为人类保留控制权，而其他硬性约束则为团队提供确定性。我们并不是在“训练”AI，而是在约束它。通过构建一套刚性的 Skeleton，我们让 AI 可以高速前进，同时不至于折断软件系统的脊梁骨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;参考文献&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Bogard, J.（2018）。《&lt;a href=&quot;https://www.jimmybogard.com/vertical-slice-architecture/&quot;&gt;Vertical Slice Architecture&lt;/a&gt;&quot;》。Dohmke, T.（2023 年 11 月 8 日）。&lt;a href=&quot;https://www.youtube.com/watch?v=NrQkdDVupQE&quot;&gt;GitHub Universe 2023 Opening Keynote: Copilot in the Age of AI&lt;/a&gt;&quot;［视频］。YouTube。Farry, P.（未注明日期）。《&lt;a href=&quot;https://www.infoq.com/news/2025/11/ai-code-technical-debt/&quot;&gt;AI-Generated Code Creates New Wave of Technical Debt, Report Finds&lt;/a&gt;&quot;》。InfoQ。Frost, B.（2013）。《&lt;a href=&quot;https://atomicdesign.bradfrost.com/&quot;&gt;Atomic Design&lt;/a&gt;&quot;》。Gamma, E., Helm, R., Johnson, R., &amp;amp; Vlissides, J.（1994）。《&lt;a href=&quot;https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612&quot;&gt;Design Patterns: Elements of Reusable Object-Oriented Software&lt;/a&gt;&quot;》。Addison-Wesley。Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp;amp; Liang, P.（2023）。《&lt;a href=&quot;https://arxiv.org/abs/2307.03172&quot;&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt;&quot;》。Stanford University、UC Berkeley、Samaya AI。Martin, R. C.（未注明日期）。《&lt;a href=&quot;https://en.wikipedia.org/wiki/Dependency_inversion_principle&quot;&gt;The Dependency Inversion Principle&lt;/a&gt;&quot;》。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/skeleton-architecture/&lt;/p&gt;</description><link>https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</link><guid isPermaLink="false">https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</guid><pubDate>Fri, 06 Feb 2026 01:44:43 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>OpenAI开始发布关于Codex CLI内部机制的系列文章</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;最近发表了系列文章的第一篇，详细介绍了&lt;a href=&quot;https://openai.com/index/unrolling-the-codex-agent-loop/&quot;&gt;他们的Codex软件开发智能体的设计和功能&lt;/a&gt;&quot;。首篇文章重点介绍了Codex框架的内部结构，这是&lt;a href=&quot;https://developers.openai.com/codex/cli&quot;&gt;Codex CLI&lt;/a&gt;&quot;的核心组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与所有AI智能体一样，框架由一个循环组成，该循环从用户那里获取输入，并使用大语言模型（LLM）生成工具调用或回应用户。但由于LLM的限制，循环还具有管理上下文和减少提示缓存未命中的策略。其中一些策略是从用户报告的错误中学到的教训。因为CLI使用&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;Open Responses API&lt;/a&gt;&quot;，所以它是与LLM无关的：它可以使用任何被这个API包装的模型，包括本地托管的开放模型。根据OpenAI的说法，他们的CLI设计和经验教训因此可以惠及任何基于这个API设计智能体的人：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们强调了任何适用于在Responses API之上构建代理循环的实际考虑和最佳实践。虽然代理循环为Codex提供了基础，但这只是一个开始。在即将发布的文章中，我们将深入探讨CLI的架构，探索工具使用是如何实现的，并对Codex的沙箱模型进行更仔细的观察。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文章描述了用户与智能体对话中的一轮或一次交流中发生的事情。这一轮交流始于为LLM组装一个初始提示。这包括指令，这是一个包含智能体通用规则的系统消息，例如编码标准；工具，一个智能体可以调用的MCP服务器列表；以及输入，这是一个包含文本、图像和文件输入的列表，包括AGENTS.md、本地环境信息和用户的输入消息等。所有这些都被打包成一个JSON对象发送到Responses API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这触发了LLM推理，从而产生一系列的输出事件。其中一些事件可能表明智能体应该调用其中一个工具；在这种情况下，智能体使用指定的输入调用工具并收集输出。其他事件表明LLM的推理输出，通常是计划中的步骤。工具调用和推理都被追加到初始提示中，然后再次传递给LLM进行更多的推理或工具调用迭代。这就是所谓的“内”循环。当LLM用done事件响应内部循环时，会话轮次结束，其中包括给用户的响应消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种模式中，一个主要的挑战是LLM推理的性能：它是“与发送到Responses API的JSON数量成二次方关系的”。这就是为什么&lt;a href=&quot;https://platform.openai.com/docs/guides/prompt-caching#structuring-prompts&quot;&gt;提示缓存&lt;/a&gt;&quot;是关键：通过重用先前推理调用的输出，推理性能变成线性而不是二次方。改变工具列表等事物将使缓存失效，Codex CLI最初对MCP的支持有一个错误，即“未能以一致的顺序枚举工具”，这导致了缓存未命中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI还使用&lt;a href=&quot;https://platform.openai.com/docs/guides/conversation-state#compaction-advanced&quot;&gt;压缩&lt;/a&gt;&quot;来减少LLM上下文中的文本量。一旦对话长度超过某个设定的token数量，智能体将调用一个特殊的Responses API端点，该端点提供了一个更小的会话表示，替换了之前的输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News用户讨论&lt;a href=&quot;https://news.ycombinator.com/item?id=46737630&quot;&gt;这篇文章&lt;/a&gt;&quot;时，赞扬了OpenAI开源Codex CLI的决定，并指出Claude Code是封闭的。一位用户写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我记得他们宣布Codex CLI是开源的……这对于任何想要了解编码智能体如何工作的人来说都是一件大事，非常有用，尤其是来自像OpenAI这样的主要实验室。我还在一段时间前为他们的CLI贡献了一些改进，并一直在关注他们的发布和PR，以扩大我的知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI的&lt;a href=&quot;https://github.com/openai/codex&quot;&gt;源代码&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/openai/codex/issues&quot;&gt;缺陷跟踪&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/openai/codex/pulls&quot;&gt;修复历史&lt;/a&gt;&quot;可以在GitHub上找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/codex-agent-loop/&quot;&gt;https://www.infoq.com/news/2026/02/codex-agent-loop/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</link><guid isPermaLink="false">https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</guid><pubDate>Fri, 06 Feb 2026 01:06:01 GMT</pubDate><author>作者：Anthony Alford</author><category>AI&amp;大模型</category></item><item><title>Agent原生模型时代开启！阶跃Step 3.5 Flash上线，2天登顶OpenRouter全球趋势榜</title><description>&lt;p&gt;从 chatbot 到 Agent，大模型以「缸中之脑」为起点，正在悄然进化出属于自己的四肢百骸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 Agent 应用狂飙突进的同时，各种安全事故也层出不穷。初具雏形的 Agent 应用，正在急切呼唤一个更聪明、更可靠的「原生大脑」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;爆改基模结构，开启 AI 模型「Agent 原生」时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 时代，由于外部工具和任务重试需求等因素的介入，令上下文长度相比 coding、chatbot 等应用场景，迎来了一轮暴涨。同时，用户对即时性也有了更高的要求。相比 chatbot 时代，吐字比阅读速度快的基本诉求，等待 Agent 工具交付结果的时间，必须被进一步压缩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，上一个时代的 Reasoning 模型，已经不能再适应本时代的需求。一个好的 Agent 原生模型，在推理成本、速度和智能水平三个层面，都必须再次迎来进化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，阶跃星辰新上线的 Step 3.5 Flash，可谓「多快好省」：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了满足 Agent 时代的诉求，Step 3.5 Flash 从基础模型层面，就采用了十分独特的结构设计。作为一款旗舰级语言推理模型，它并未盲目追逐模型尺寸，而是选择了稀疏混合专家（MoE）架构。总参数量为 1960 亿，每次推理仅激活约 110 亿参数。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Step 3.5 Flash，将传统的 Linear Attention（线性注意力机制），打散为滑动窗口注意力（SWA）+ 全局注意力（Full Attention）3:1 的混合架构。如果要找个比喻的话，这种结构，十分接近推理小说的阅读体验：大部分注意力依旧集中在当前段落附近的文本，但当一个伏笔回收时，几章之前埋下的剧情钩子，仍然能快速的浮现出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，在模型技术层面，Step 3.5 Flash 还使用了 MTP-3「多 token 并行预测」机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说传统大模型，是一个词接一个词的“文字接龙”，那么 MTP-3，就像是先打草稿，再深入润色。在 Transformer 主干之后，MTP-3 会附加一个专用的预测网络层，让模型根据当前上下文同时推断多个未来 token 的概率分布。这样的设计，在保证因果一致性的前提下，实现了多 token 的并行推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构精巧，推理速度可达每秒 350 个 token&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多方加持下，Step 3.5 Flash 拥有了高达 256K 的超长上下文，和十分夸张的推理速度。在单请求代码类任务上，Step 3.5 Flash 最高推理速度可达每秒 350 个 token，确保了复杂 Agent 任务的低延迟响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和它的名字一样，「快」，是 Step 3.5 Flash 最显著的特点。但速度不能以牺牲智力为代价。在推理速度狂飙突进的同时，它的逻辑能力，同样不容小觑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在例行刷榜环节当中，Step 3.5 Flash 拿下了 AIME 2025（美国数学邀请赛）97.3 分； IMOAnswerBench（国际奥林匹克数学基准测试）85.4 分；HMMT 2025（哈佛 - 麻省理工数学竞赛） 96.2 分的好成绩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与国内顶级开源模型相比，上述项目得分，Step 3.5 Flash 均为第一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缩放定律似乎暗示我们，模型的能力，直接和尺寸挂钩。但 Step 3.5 Flash 用事实证明，合适尺寸 + 充分的后训练，完全可以兼顾速度与效率，得到一个精致、且有强逻辑内核的大模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;抛弃「规模迷信」的背后，是阶跃星辰对大模型的独特理解：模型应该凝缩「逻辑」，而非用超大规模，简单地对文本模式死记硬背。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;「高智商」，才是硬道理&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种认知的回报，在真实世界的任务当中体现的尤为明显：coding 榜单当中，Step 3.5 Flash 拿下了 Terminal-Bench 2.0（终端任务自动化），和 LiveCodeBench-V6（实时编码调试）国内开源第一的好成绩，整体测试水平属于全球第一梯队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 相关的测试项目更是手到擒来：τ²-Bench（多步任务规划）88.2 分 ；xbench-DeepSearch（深度搜索与信息整合）54 分，均为国内开源模型第一。BrowseComp（网页浏览与上下文管理） 69 分，实现了对海外御三家模型的成功反超。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更大的认可，来自 AI 社群：在真实世界任务中，Step 3.5 Flash 以高达 167 Tokens/s 的推理速度，发布首日，即进入全球知名 AI 模型聚合平台 OpenRouter “Fastest Models”速度榜前列。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59c9811f6b2e269396c23c869dda8092.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;发布 2 天，登顶 OpenRouter 全球趋势榜（Trending）榜单。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3ac960a31f6c1ba5bb59f36506f1fdc7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为汇聚了 OpenAI、Anthropic、Google 等主流模型的 API 平台，OpenRouter 的全球趋势榜单，实时反映着开发者在实际应用中的模型偏好与付费选择。此次登顶，意味着 Step 3.5 Flash 在真实任务当中的表现，已收获了全球 AI 开发者的积极认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reddit、X 等平台上也有不少用户，对 Step 3.5 Flash 的表现给出了很高的评价：多语言混用时切换自然，很少出现同尺寸模型身上常见的「夹杂」情况；行事稳定可靠，幻觉率极低，且对自身的能力边界有着清晰的认知，不会为了强行接话而编造答案。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0e5a3fe02842de4580546d5c494aad1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd00075cfe8f5c90bfc776436c11419f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/758555dec73f0b5d2c4bf3c3b88a451b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而这一切，都发生在一台 128G 内存、M3 Max 芯片的 mac 电脑上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本地 Agent，从此平权&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据社区反馈，借助 llama.cpp，Step 3.5 Flash 在 mac 平台上的推理速度极佳。平均速度 35 tokens/ 秒，约为该平台理论最大效率的 70%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;某种程度上，这是阶跃星辰 CTO 朱亦博「私心」的结果：他希望这个模型，能支持 4-bit 量化后，运行在 128GB 内存的 MacBook 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 Step 3.5 Flash 最终发布时的支持范围远不止于此：云服务层面，包括华为昇腾、沐曦股份、壁仞科技、燧原科技、天数智芯、阿里平头哥等在内的多家芯片厂商，均已率先完成了对 Step 3.5 Flash 的适配工作。同时，经过 4-bit 量化以后，Step 3.5 Flash 也支持在 NVIDIA DGX Spark、Apple M3/M4 Max 以及 AMD AI Max+ 395 等主流个人 AI 终端上，进行本地部署——同时依然保持着 256K context 的超长上下文能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦博在博客文章里不无自豪地表示，这是你在 128GB 内存的 Macbook 和 DGX Spark 上，用 4-bit 畅快跑 256K context 的最强模型，没有之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 模型的又一个「中国时刻」？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去的一年中，来自中国的开源模型，用更低的获取门槛、推理成本和打平的性能，一举击碎了“超大规模 + 闭源 = 先进”的行业迷信，无数 AI 应用因此涌现，也将大模型竞争，重新拉回了效率与架构创新的主航道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，国内几家 AI 公司动作频频、传闻不断，今年大模型领域的「春节档」，注定热闹非常。而最近发布的 Step 3.5 Flash，或许正悄然复刻又一个 AI 领域的「中国时刻」——高性能、低门槛、新范式。只是这一次，范式转移的焦点，从“推理模型”转向了更具颠覆性的“Agent 原生（开源）基座模型”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当行业还在用稠密模型硬扛 Agent 场景时，它用 1960 亿总参数、仅 110 亿激活参数的精巧架构，同时解决了 Agent 时代的三大死结——超长上下文下的低延迟响应、复杂任务中的高幻觉风险、以及终端设备上的本地化部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当海外巨头将 Agent 能力锁死在云端 API 时，Step 3.5 Flash，让 256K 上下文的 Agent 大脑，跑在 128GB 内存的 MacBook 上——这是对 AI 权力结构的重构：Agent 的智能不应被云厂商垄断，开发者理应拥有在终端侧构建私有化 Agent 工作流的自由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“终端平权”逻辑，恰是此前中国 AI 大模型引领的范式转移，在新环境下进一步的延续与深化：从模型获取的平权，进阶到 Agent 能力的平权。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;历史从不重复，但常常押韵。如果说之前的国产大模型，打破的是“对规模和闭源的迷信”，那么 Step 3.5 Flash 正在击碎的，就是“速度与智能不可兼得”的新迷信。当行业还在用“参数量”“榜单分数”这类旧范式衡量模型价值时，Step 3.5 Flash 已用 OpenRouter 趋势榜登顶、Reddit 开发者自发安利、多芯片厂商 Day 0 适配的事实证明：真正的范式转移，永远始于真实世界中，解决真实诉求的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们或许正站在 Agent 时代的分水岭上：过去一年，市场狂热追逐 Agent 应用层的“四肢百骸”，却忽略了为其注入灵魂的“原生大脑”。而 Step 3.5 Flash 的此时此刻，又恰似 2025 年春节的彼时彼刻——尽管暂时被 Agent 应用的喧嚣浪潮所掩盖，但历史终将被证明，在 Agent 时代，是阶跃星辰，完成了一次基础设施层，最关键的范式跃迁。&lt;/p&gt;</description><link>https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</link><guid isPermaLink="false">https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</guid><pubDate>Thu, 05 Feb 2026 10:33:53 GMT</pubDate><author>InfoQ编辑部</author><category>企业动态</category><category>AI&amp;大模型</category></item><item><title>贾跃亭发布人形机器人，最贵的24.3万元起</title><description>&lt;p&gt;今早，法拉第未来（FF）在美国拉斯维加斯发布了首批具身智能机器人EAI（Embodied AI）。基于EAI机器人产业“四化”趋势，FF推出 “三位一体” FF EAI Robotics生态战略、技术与产品，包括EAI终端、EAI大脑&amp;amp;开源开放平台、以及EAI去中心化数据工厂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7776ac475eadf60fc7bbbf9208c39ce.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，EAI机器人包括三大系列。其中，Futurist系列是全尺寸职业型具身智能人形机器人，全能专业的职业专家；Master系列是运动型具身智能人型机器人，全智懂你的动作大师；Aegis系列是安防和陪伴型专业四足具身智能机器人，标配四足结构，同时可选四轮版本；轮臂系列计划二季度发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9dfc3213071d2172632c23feccf6ff9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据其发布的图片显示，Futurist系列机器人定价34990美元（合人民币约24.3万元）起，Master系列机器人定价19990美元起，Aegis系列机器人定价2499美元起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa847b7a4a856e6d204ef3a43c5a49e4.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来应该是：360行，各有自己的职业机器人。”法法创始人、合伙人、首席产品及用户生态官，LeEco乐视创始人贾跃亭在公开平台表示，“虽然目前的FF还很弱小，但我们凭借永不服输永不放弃的精神，这些年积累的独特价值即将爆发出强大势能，会让我们从今年开始更快速成长壮大，推动一个具身智能的时代到来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/1609bbe21c817a688903f61b207ea2ce.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</link><guid isPermaLink="false">https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</guid><pubDate>Thu, 05 Feb 2026 10:24:30 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>谁写的代码谁负责！Cursor 发布 Agent Trace：从此 Bug 别想再推给 AI</title><description>&lt;p&gt;Cursor 近日公布了&amp;nbsp;&lt;a href=&quot;https://agent-trace.dev/&quot;&gt;Agent Trace&lt;/a&gt;&quot;&amp;nbsp;开放规范草案，目标是解决 AI 生成代码在软件项目中的归属与标注问题。该提案以 RFC 形式发布，定义了一种厂商中立的格式，用于在版本控制系统中记录 AI 与人类协作产生的代码贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于其在 AI 辅助编程工具方面的实践经验，Cursor 发现，代码变更过程中对上下文的追踪能力仍然明显不足。以常见的 git blame 等工具为例，它们只能显示某一行代码“何时被修改”，却无法说明这次修改是由人类完成、由 AI 完成，还是二者协作的结果。Agent Trace 正是为了解决这一缺口，试图以结构化、可互操作的方式捕获这些关键信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从技术角度看，Agent Trace 是一套数据规范，使用基于 JSON 的 trace record（追踪记录）来关联具体的代码范围，以及背后的对话过程和参与者。代码贡献可以在文件级或行级进行追踪，按会话进行分组，并被标注为“人类”、“AI”、“混合”或“未知”。该模式还支持为 AI 生成的代码附加可选的模型标识，从而在不绑定具体厂商的前提下，实现更精确的归属记录。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://imgopt.infoq.com/fit-in/3000x4000/filters:quality%2885%29/filters:no_upscale%28%29/news/2026/02/agent-trace-cursor/en/resources/1Zrzut%20ekranu%202026-02-2%20o%2019.23.56-1770057032475.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在设计上，这一规范刻意保持了对存储方式的“中立性”。Cursor 并未规定追踪记录必须存放在哪里，开发者可以根据自身需求，将其保存为普通文件、git notes、数据库记录，或采用其他机制。Agent Trace 同时支持多种版本控制系统，包括 Git、Jujutsu 和 Mercurial，并引入了可选的内容哈希，用于在代码被移动或重构后，依然能够追踪其原始归属。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可扩展性是 Agent Trace 的核心设计目标之一。各厂商可以通过命名空间键（namespaced keys）附加额外的元数据，而不会破坏规范的兼容性。同时，该规范刻意回避了对 UI 形式、代码所有权语义的定义，也不试图评估代码质量或追溯训练数据来源，而是将关注点严格限定在“代码归属”和“可追溯性”本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cursor 还提供了一份参考实现，展示 AI 编程代理如何在文件发生变化时，自动捕获并生成追踪记录。尽管示例基于 Cursor 自家的工具链，但其设计模式被明确定位为可复用方案，适用于其他编辑器和智能代理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自开发者社区的早期反馈，普遍强调了这一规范在代码审查和调试流程中的潜在价值。一位 X&amp;nbsp;&lt;a href=&quot;https://x.com/3p3r_/status/2017041680814526762?s=20&quot;&gt;用户评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这才是真正在收拾 Agent 生成代码的烂摊子。等不及在 Review 里用了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一位用户则从可复现性的角度给予了肯定：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;团队一旦搞不清 Agent 为啥跑偏，就会直接停工。Trace 解决了这个痛点，开放得正好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为一份 RFC，Agent Trace 明确欢迎社区反馈，同时也有意保留了一些尚未解决的问题，例如在合并（merge）、变基（rebase）以及大规模代理驱动代码变更场景下应如何处理。Cursor 将该提案定位为一个共同标准的起点，而非终极答案，以应对 AI 代理在软件开发流程中日益普及的现实趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/agent-trace-cursor/&quot;&gt;https://www.infoq.com/news/2026/02/agent-trace-cursor/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AGkTiCi5OlEABJQpfhI2</link><guid isPermaLink="false">https://www.infoq.cn/article/AGkTiCi5OlEABJQpfhI2</guid><pubDate>Thu, 05 Feb 2026 08:59:49 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>可观测</category></item><item><title>Cloudflare的Matrix家庭服务器演示引发了关于AI生成代码的争论</title><description>&lt;p&gt;Cloudflare&lt;a href=&quot;https://blog.cloudflare.com/serverless-matrix-homeserver-workers/&quot;&gt;发表&lt;/a&gt;&quot;了一篇博客文章，展示了在Workers上运行的无服务器Matrix家庭服务器，引发了关于AI生成代码和技术准确性的争论。虽然Matrix.org对Cloudflare的关注表示欢迎，但联合创始人Matthew Hodgson&lt;a href=&quot;https://matrix.org/blog/2026/01/28/matrix-on-cloudflare-workers/&quot;&gt;指出&lt;/a&gt;&quot;，这篇文章“严重夸大了项目的范围”，强调了功能性Matrix服务器所需的核心功能缺失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这篇博客声称已经在Cloudflare的边缘平台上构建了一个完整的Matrix家庭服务器，用Cloudflare的D1和Durable Objects等原语取代了PostgreSQL和Redis。&lt;a href=&quot;https://github.com/nkuntz1934/matrix-workers/blob/main/README.md&quot;&gt;GitHub&lt;/a&gt;&quot;存储库最初将自己描述为“生产级”，并有一个“部署到Cloudflare”的按钮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson在Matrix.org网站上的&lt;a href=&quot;https://matrix.org/blog/2026/01/28/matrix-on-cloudflare-workers/&quot;&gt;回应&lt;/a&gt;&quot;很圆滑，但很明确：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;代码还没有实现Matrix的任何核心特性，这些特性允许你安全地进行联邦，因此还没有构成一个功能性的Matrix服务器，更不用说生产级服务器了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，该实现没有将房间建模为复制的事件图，没有检查权限，也没有维护权限级别——将其比作“忽略权限的文件系统，或者不实现共识机制的区块链。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代码库在关键部分包含了TODO注释，例如TODO: 在身份验证逻辑中的检查授权。状态解析（Matrix用于处理跨分布式房间的冲突事件的算法）没有实现。尽管声称支持“完整的Matrix端到端加密堆栈”，但端到端加密验证似乎不完整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46781516&quot;&gt;Hacker News&lt;/a&gt;&quot;上的社区反应表明，有迹象表明AI提供了大量的帮助。评论者指出:&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们做了X”的博客文章最终变成了“我们做了X的一部分演示”，这在整个行业中已经过时了。解决方法很无聊：你只需要明确你所创造的内容。”另一位评论道：“基础设施公司的技术博客过去有两个目的：展示专业知识并建立信任。当帖子开始过度承诺时，你就失去了这两者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matrix的开发人员Jade Ellis在&lt;a href=&quot;https://tech.lgbt/@JadedBlueEyes/115967791152135761&quot;&gt;Mastodon&lt;/a&gt;&quot;上写道，存储库在自述中显示了“有错位的ASCII图”。待办事项分散在各处。这表明代码带有未经彻底审查的AI生成输出的特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson承认使用LLM来制作不熟悉的协议原型是一种挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你正在使用LLM来原型化一个不熟悉的协议的实现，那么你可能不知道在哪里检查代理是否夸大了事实。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表达了对作者的同情，同时注意到对“过度热情地使用LLM，特别是如果他们自己投入了大量的时间和精力来理解和构建功能Matrix实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare在发布大约六小时后更新了这篇博文，并添加了一个免责声明，称其描述了一个概念验证和一个个人项目。然而，更新没有撤回正文中的特定技术声明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管有这些批评，Hodgson还是强调了这个演示“成功地说明了Cloudflare Workers是如何工作的，而且这些代码肯定可以作为未来工作服务器的基础。”他指出，Matrix和Cloudflare在其他方面也有合作，包括使用Cloudflare Calls作为MatrixRTC后端的概念验证，Cloudflare的CDN多年来一直保护着matrix.org的流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在无服务器基础设施上运行Matrix的技术方法仍然是可行的。这篇文章描述了用D1 （SQLite）取代PostgreSQL，用KV存储取代Redis，并使用持久对象进行房间状态管理。这些架构选择可以在正确实现Matrix的核心联合和安全特性的情况下工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于考虑使用AI辅助开发的开发者来说，这一事件凸显了未经审查的AI输出的风险。夸大AI生成实现的模式已成为技术博客中反复出现的问题，引发了对基础设施公司审查流程的质疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson总结说，Matrix基金会依靠会员费来资助规范工作、信任和安全工具以及生态系统支持。虽然组织成员在过去的一年里翻了一番，但基金会的财务状况还无法维持下去。他表示希望像Cloudflare这样受益于Matrix的公司可以考虑加入为会员。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的&lt;a href=&quot;https://blog.cloudflare.com/serverless-matrix-homeserver-workers/&quot;&gt;博客文章&lt;/a&gt;&quot;和更新后的免责声明仍然有效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/cloudflare-matrix-homeserver-ai/&quot;&gt;https://www.infoq.com/news/2026/02/cloudflare-matrix-homeserver-ai/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/qspJxiaVYpj6HOQVGWpO</link><guid isPermaLink="false">https://www.infoq.cn/article/qspJxiaVYpj6HOQVGWpO</guid><pubDate>Thu, 05 Feb 2026 08:52:11 GMT</pubDate><author>作者：Michael Redlich</author><category>AI&amp;大模型</category><category>云计算</category></item><item><title>9B 模型“平替”GPT-4o ？！面壁赌对OpenClaw端侧AI，内部上演一人月产65万行代码的效率核爆</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2023 年，在百模大战正激烈的时候，面壁智能突然转向端侧大模型，这一战略决策受到了外界不少质疑，直到次年苹果的入局才让市场相信他们的判断。3 年后，面壁的打法和认知更为坚定和清晰，并火力全开：发布首个可以“即时自由对话”的大模型、年中发布首款 AI 硬件松果派（Pinea Pi）以支持硬件场景的全栈开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;首次手搓全双工全模态模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2 月 4 日，面壁正式发布并开源了新一代全模态旗舰模型 MiniCPM-o 4.5。作为原生全双工的全模态大模型，MiniCPM-o 4.5 新引入了一种端到端的“边看、边听、主动说”的全模态能力：模型可以进行即时、自由的对话交互，弱化了传统对话中“一问一答”的轮次概念，而是允许模型根据语义和场景，自主判断是否发起对话。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直接看具体效果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上述展示中模型一直在观察，且没有涉及复杂的调度&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“全模态能力是 AI 进入人类物理世界所必备的一项基础能力。这一次的全模态模型，最大的特色在于高度拟人、自然的交互方式，也就是说，看、听、说是并行发生、互不阻塞，不再采用过去那种回合制交互。这在技术上是一次非常重要的跨越，也是未来 AI 真正进入物理世界所必须具备的基本能力。”面壁智能联合创始人兼首席科学家刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;清华大学人工智能学院助理教授、面壁智能多模态首席科学家姚远，主要负责 MiniCPM-o 4.5 的研发。他介绍，该模型主要依赖两项核心创新：一是全双工机制，多模态输入和输出彼此不阻塞，模型可以持续感知外界的视频和音频流，同时进行语音或文本输出，不会因“正在说话”暂停对外界的感知；二是全模态的自主交互机制，模型会持续判断当前语义是否已经成熟，是否达到了适合触发自身输出的时机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他坦言，目前市面上大多是将图像模型、语音模型，甚至 instruct 模型和 thinking 模型拆分为不同的模型分别训练。面壁这次尝试将所有能力统一训练到一个模型中，面临了不小的挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先就是多维度一起训练，整体难度会急剧上升，再加上端到端的多模态训练，本身就会显著增加系统负担；其次 9B 参数规模下，要在语音、全模态交互以及视觉能力等方面取得不错效果，就要对模型如何学习和吸收知识有更深入的理解，能够更精细地把握模型在不同训练阶段的学习动态，避免新引入的知识与已有能力之间产生冲突。这期间，技术团队克服了大量困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，团队能够在多模态训练过程中较好地保持文本能力，instruct 能力没有明显损失，甚至实现小幅提升。此外，模型通过更低的显存占用、更快的响应速度，确保在提供 SOTA 级全模态表现的同时，实现了最佳的推理效率和最低的推理开销。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2d/2dbea1b298b51c3eb018d860fde40cc2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Github：&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Hugging Face: &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-4_5&quot;&gt;https://huggingface.co/openbmb/MiniCPM-o-4_5&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;体验链接：&lt;a href=&quot;https://minicpm-omni.openbmb.cn/&quot;&gt;https://minicpm-omni.openbmb.cn/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，模型记忆大概在一分钟左右，也模型推理的最佳“舒适区”。姚远表示，Infra 层虽然可以支持更长时间的训练和推理，但如果模型未来要承担更长期、甚至接近全天候陪伴式的使用形态，就必然要在方法和机制上做更多创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;端侧对低延时要求非常高。这次，模型侧的低延迟优化主要来自两点：首先，在全双工状态下，模型不再依赖外部的微型工具或小模型来判断“什么时候开始推理”，传统逻辑里需要固定等待的时间被去掉，模型可以直接基于语义判断无缝生成回应。其次，现在不少方案会把语音 token 直接放进一个大模型里统一生成，这会带来非常沉重的计算开销。面壁技术团队采用的方式是，一个大的主干模型加一个轻量级语音生成模块，在保证效率的同时，两者通过稠密的隐藏层连接，把主 token 与各个头部 token 紧密关联起来，因此实现控制能力不受影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8c/8c959c6e46e149e56005720b782ed55b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而使用侧的系统层面，则依赖于高效的推理框架 llama.cpp-omni 和低延迟的交互系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚远指出，多模态数据本身并不是最大的问题。预训练阶段“数据燃料耗尽”主要指文本数据；而在多模态领域，当前的挖掘程度远远不够，甚至都还没有真正找到一种非常有效的方式系统利用这些数据。而全双工、全模态的自主交互机制，可能正是未来新的学习与增长方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，如何在不牺牲单任务性能的前提下，实现统一建模、高效泛化以及理解生成一体化，是当前业内积极探索的研究方向，如今面壁也迈出了自己的关键一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;让开发者回答，AI 硬件该是什么样&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;端侧领域，除了开发端原生的模型，与芯片厂商的合作也越来越重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，芯片厂商非常希望从前沿端侧模型的公司，获取未来训练模型的规划和展望，这有助于设计新的芯片；另一方面，模型公司在设计和训练新模型时，也希望能够提前了解芯片的特性，说明需要的算子类型和架构特点，以确保训练出的模型在这些芯片上运行时效率最高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁如今就在成为连接芯片厂商和终端厂商的重要媒介，而且还要连接更多的开发者：今年面壁发力的重点之一便是开发者生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;25 年上半年，面壁投资人在深圳调研发现，在深圳做 AI 硬件的项目，凡涉及端侧模型的，超过一半以上都在使用 MiniCPM。这是面壁今年开始建设开发者生态、提供硬件的根本原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁智能联合创始人兼 COO 雷升涛解释，单纯依靠商业化，把 MiniCPM 部署到数百亿台设备上会比较困难，而通过生态建设可以让开发者一起参与推动。生态的优势在于自然生长，只要有好的基础，它就会衍生出许多依赖性的、难以想象的应用。对于“应该能开发出哪些硬件”的问题，面壁没有设定特别明确的规划或期待，而是把答案留给了开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁践行这一策略的首个举措就是发布松果派：一款 AI 原生 (AI Native) 的端侧智能开发板。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这背后的逻辑是：推广语言模型相对容易，但当模态增加、要在设备上运行、进行微调、完成对齐后再开发应用，难度就显著提升，这部分难度需要依靠工具和软硬件来解决，承载这部分功能的就是松果派。未来面壁模型发布时，就会针对指定硬件进行优化，减少用户在适配上消耗的大量精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/d3/d3538ae32ea86628ef74500193f3ef86.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;松果派构建了一套软硬一体、全栈覆盖的端侧 AI 软件体系。其基于 NVIDIA Jetson 系列模组打造，内置麦克风、摄像头、丰富的接口等多模态硬件组件，以便开发者高效开发和调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;松果派计划在年中正式量产上市，但它今年不会承担面壁特别强的商业化诉求，更多是承担市场教育作用：让更多的人能更快体验模型能力，并在各类场景中应用起来。打通端侧模型到应用的最后一公里硬件、实现对用户痛点的覆盖，就是面壁今年的目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁目前并未透露具体价格，但肯定地表示不会以盈利为主要目的。最初版本选择了在全球范围内相对成熟的方案，接下来会陆续推出相应的国产化版本以及不同算力的版本，并根据开发者反馈进行规划和调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这次松果派的硬件本身是由合作伙伴完全设计，面壁主要将其整合应用。面壁智能联合创始人兼 CEO 李大海强调，面壁最重要的是做端侧原生，聚焦端侧模型研发。“端侧模型的商业化落地，本身既是对我们模型能力的验证，也是为端侧模型建立数据飞轮，形成完整的闭环。从核心来看，我们的工作一直很专注。在过去，虽然出现了许多看似有吸引力的机会，但我们始终坚持取舍，最终选择聚焦在端侧模型这件事情上。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;如何从各种竞争中突围？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁的核心理念是大模型“知识密度定律（Densing Law）”，即大模型的知识密度大约每 100 天提升一倍。这引发了一个重要推论：大模型的保鲜期非常短。换句话说，任何一家大模型公司都必须持续不断地推出优秀的大模型。回顾国内外所有模型厂商，没有任何例外。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“如果一个厂商只能在某一个时间点推出一个模型，那么它实际上无法在行业前沿持续存在；半年、一年之后，用户很可能就会忘记这个模型。因此，关键不在于推出单一优秀模型，而在于能够持续不断地推出优秀模型。”李大海说道，“面壁的目标是打造一个能够持续训练出高知识密度大模型的系统。这才是我们认为最重要的产品、技术层面的核心。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷升涛补充道，在模型之外，把底层的 Infra 模型跑到极致也是延长模型领先时间的关键，毕竟端侧的算力很小、内存有限，各种约束都非常严苛，要做好是非常困难的。另外，产品化能力也非常关键。现在单靠模型领先已经无法持续保持竞争优势，需要通过底层基础设施、产品设计、品牌建设以及模型能力的结合，来更大程度地延长模型的“保鲜期”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然面壁正在同步将商业优势、生态优势、品牌优势等单一优势转化为综合性优势，但作为创业公司，如何避免被大厂围剿仍是一个现实问题，李大海对此较为乐观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他解释道，大厂不会放弃通用且规模巨大的市场，因此竞争激烈。相比之下，端侧是另一个重要方向。“端侧包含非常多不同的终端，每个终端面向的应用场景各不相同，因此它不是一个统一的市场，创业公司有更多机会去切入不同细分领域，而不需要像大厂那样争夺整个市场。”背后的逻辑是：端侧市场分散且长尾，同时存在许多高价值的应用场景，这正是创业公司在初期更适合重点攻克的领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，终端本身就是高度差异化的，涵盖了各种各样的类型。刘知远强调，面壁关注的是终端发展的核心需求：高效，即用尽可能少的参数实现尽可能强的能力。“从商业角度来看，面壁不会去和很多厂商打阵地战，这种做法在创业阶段并不聪明。这是一个蓝海市场，没有必要在这方面过多纠结。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李大海也补充称，即使是在同一个领域内，要解决的客户或用户问题也是非常多样化的。同一个领域并不意味着大家一定是你死我活的竞争关系。尤其端侧市场，覆盖了非常多应用场景，能够容纳很多创业公司，让大家都有良好的发展空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;内部的“一人公司”趋势&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，一个值得关注的现象是，面壁内部也逐渐出现了“one person company（一人公司）”趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁内部过去十个月一直在推动全公司的 AI 原生计划。不到两百人的团队，在十个月内写了 2000 万行代码。如果按传统方式手写，这些大概需要 700 人才能完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，团队中最核心、最重度投入的一位员工，一个月就写了 65 万行代码，他把核心系统接入 AI，并重构了一遍。“未来的企业，尤其是 AI 企业，一定会是高度 AI 赋能的，也就是我们所说的 AI Native 模式。”刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;小团队甚至个人都可以完成过去需要团队数月才能完成的工作，这是一个非常明显的发展趋势。面壁目前就在朝这个方向发展，这种模式和以往的大公司有很大的不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷升涛解释，面壁内部对“AI Native”的定义包括两个方面：第一，接到任务后，第一反应是能否用 AI 来完成；第二，如果任务原本人来完成的，那么用 AI 完成后，能否做得更好。他表示，AI 已经渗透到面壁业务的各个层面，它不仅被广泛使用，还深刻地影响了大家的思维方式、工作模式乃至协作方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也反映在了面壁招人的具体要求中。李大海表示，面壁一直希望能够吸引 AI 原生的人才，即在思考和解决任何问题时，都能够将 AI 能力视作自身的内在工具去应用。这背后反映的，是人才是否具备发现问题和提出问题的能力，这一点在如今时代尤为重要。同时，他们还需要能够利用 AI 快速解决问题，并具备足够强的判断能力，去评估工具产出的结果是否高质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“一个公司的核心竞争力，很大程度上取决于人才的密度和质量。换句话说，所谓 AI 原生，不只是态度上愿意使用 AI，更重要的是通过这个过程展现出个人的综合能力。”李大海说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;走向 AGI 的两条发展主线&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于未来端侧智能的发展，面壁形成了一个明确判断：端侧与云端的协同，将成为未来长期存在的主流形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是豆包手机、具身智能，还是引发广泛关注的 OpenClaw，这些爆火的案例都在验证一个趋势：智能终端正在成为大模型能力向用户延伸的重要载体。刘知远认为，这些探索共同指向一个核心愿景：大模型将越来越贴近用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但从现实情况来看，大部分产品目前仍主要依赖云端模型运行，由此带来了反馈延迟、隐私保护和安全性等一系列问题。因此，这一方向虽然正确但还不成熟，它只是这场大戏的序幕，甚至连序幕的开端可能都刚刚开始。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁判断，随着模型逐步进入物理世界，尤其是在对实时性要求极高的任务中，端侧模型将不可或缺。在本地即时处理大量数据、快速做出响应，是端侧模型的核心价值所在，这也是端云协同中，端侧不可替代的意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从具体终端形态来看，李大海指出，手机在大模型应用上仍有巨大的拓展空间。目前的探索更多集中在“输出”侧能力，但同样重要的还有“输入”侧。如果手机能够直接感知并理解现实环境，就可以更自然地与用户共享上下文，实现更贴近人类认知方式的交互。但这也意味着更高的技术与工程挑战：在资源受限的终端上实现复杂感知与理解能力，需要更长时间的打磨与更精细的系统优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在另一个同样火热的具身智能领域，行业面临的核心挑战依然是模型的通用性与泛化能力，即能否让同一模型稳定运行在不同类型的本体之上。多模态大模型被普遍视为突破这一瓶颈的关键，为跨场景、跨本体的适应能力提供基础支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在刘知远看来，多模态乃至全模态能力，正是未来多智能体体系的基础。未来将存在大量分布在不同环境中的智能终端，每个终端的感知条件、背景信息各不相同，正是这种差异性，使得终端之间的协同成为必然选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他解释道，从结构上看，一个智能体至少可以抽象为三个核心要素：输入 x、输出 y 和模型 m。输入天然是全模态的，人类正是通过多模态感知世界；模型负责思考、推理与决策；输出则作用于物理世界，完成各种具体行为。未来智能体能力的演进，正是围绕这三个要素不断强化与耦合，最终实现真正面向物理世界的智能行动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在更宏观的层面，刘知远将通用人工智能的发展总结为两条主线：一是智能能力持续增强，二是智能的实现与使用不断变得高效。面壁未来的技术突破，也将围绕这两个方向同步推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他进一步判断，在接下来一到两年内，模型的专业能力和与现实世界交互的能力将快速提升，作为智能体，模型将逐步具备更强的自主学习与自我成长能力；当模型能够在特定领域中自主探索与进化后，多智能体协同将成为下一阶段的重要突破，不同智能体将像人类团队一样高效协作，完成单一个体难以完成的复杂任务；更长远来看，模型还将逐步展现出创新与创造能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，智能终端本身也将随之发生变化。“一旦终端侧模型具备自主学习与协同能力，就会形成一个关键基点：每个人都将拥有一个持续成长、越来越懂自己的大模型助手。未来三到五年，这一愿景很可能成为现实。”刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/LIghfWoDjXgjNME3mGHc</link><guid isPermaLink="false">https://www.infoq.cn/article/LIghfWoDjXgjNME3mGHc</guid><pubDate>Thu, 05 Feb 2026 08:31:58 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Rspress 2.0发布：面向体验与AI的全新升级</title><description>&lt;p&gt;我们很高兴地宣布 Rspress 2.0 的正式发布！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 是基于&amp;nbsp;Rsbuild&amp;nbsp;的静态站点生成器，专为开发者打造的文档站工具。自 2023 年正式发布以来，Rspress 1.x 累计迭代&amp;nbsp;144 个版本，共有&amp;nbsp;125 位贡献者&amp;nbsp;参与项目开发。越来越多的开发者选择 Rspress，借助其高效的编译性能、约定式路由和组件库预览等功能，构建了可靠的文档站点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据社区的反馈和建议，Rspress 2.0 在&amp;nbsp;主题美观度、人工智能原生、文档开发体验、与 Rslib 一起使用&amp;nbsp;等方面进行了更深入的研究。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么是Rspress 2.0&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 1.x 已经解决了文档站框架编译性能的问题，但仍然存在一些问题影响着作为文档开发工具的核心体验。2.0 版本将不仅仅针对编译性能的追求，也侧重于文档站体验的其他方面：&lt;/p&gt;&lt;p&gt;主题风格：一套更美观的默认主题，并提供了多种自定义主题方式，解决了 1.x 在主题定制上缺乏稳定 API 的问题；AI-native：文档不仅服务于人类读者，也需要被Agent更好地理解和使用。Rspress现在内置了&amp;nbsp;llms.txt&amp;nbsp;生成并从SSG衍生出的&amp;nbsp;SSG-MD&amp;nbsp;功能，生成高质量的Markdown渲染内容供Agent读取；双击编译，瞬间启动：默认启用&amp;nbsp;lazyCompilation，配合链接悬停时对资源的预加载功能，仅在访问特定路由时构建所需文件，无论实现项目规模大小，dev也可瞬间启动；Shiki 代码高亮：默认集成 Shiki，在构建时完成语法高亮，支持主题切换、变压器扩展，比如&amp;nbsp;@rspress/plugin-twoslash，带来更丰富的代码块展示效果；文档开发体验：优化&amp;nbsp;_nav.json、_meta.json&amp;nbsp;文件的HMR等并新增&amp;nbsp;json schema&amp;nbsp;用于IDE内的代码提示；默认开启死链检查功能；新增文件代码块语法，支持外部引用文件；@rspress/plugin-preview&amp;nbsp;和&amp;nbsp;@rspress/plugin-playground&amp;nbsp;支持同时使用等；Rslib 集成：现在可以在使用&amp;nbsp;create-rslib&amp;nbsp;创建组件库项目时，选择 Rspress 作为文档工具，快速构建组件库项目站点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一次对现有架构的全面升级，下面将介绍 Rspress 2.0 及其&amp;nbsp;全新主题、高质量 llms.txt 生成、集成 Shiki、后续编译等重要功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/46/46a5ede7a37b7775c064081e532c339f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2.0 新特性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;全新主题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2.0默认主题令人期待的一次系统性升级，它由团队设计师&amp;nbsp;@Zovn Wei&amp;nbsp;整体设计，在视觉效果和阅读体验上都有较轻的提升，并且每个组件需要独立替换，拥有非常多的可定制性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ecaa845a5fa8c39ddbf8d8e2d214ef3d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主题定制&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按照定制化程度从低到高，有CSS变量、BEM类名、ESM重导出覆盖、组件弹出四种自定义主题[11]方式。&lt;/p&gt;&lt;p&gt;CSS指标：新主题涉及了更多CSS指标，覆盖主题颜色、代码块、首页等样式。您可以在&amp;nbsp;CSS指标[12]&amp;nbsp;页面进行预览并调整所有CSS指标，找到满意的配置后直接复制到项目中使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;:root {
  /* 自定义主题色 */
  --rp-c-brand: #3451b2;
  --rp-c-brand-dark: #2e4599;
  /* 自定义代码块样式 */
  --rp-code-block-bg: #1e1e1e;
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BEM 类名：内置组件现在均采用&amp;nbsp;BEM 命名规范。这是十分之一 Old School 的选择，但也是我们深思熟虑的决定。用户可以通过 CSS 选择器精准调整样式，HTML 结构更加清晰；同时与 Rspress 用户自身使用的 CSS 框架解耦合，用户可以任意选择 CSS 框架（Tailwind&amp;nbsp;[14]、Less&amp;nbsp;[15]、Sass&amp;nbsp;[16]&amp;nbsp;等），比如使用 Tailwind V4 或 V3而不用担心版本，也不用担心与 Rspress 内置 CSS 产生冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;/* BEM 命名规范 */
.rp-[component-name]__[element-name]--[modifier-name] {
}

/* 根据 BEM 类名轻松覆盖组件样式 */
.rp-nav__title {
  height: 32px;
}
.rp-nav-menu__item--active {
  color: purple;
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ESM重导出覆盖：如果CSS上的修改无法满足定制需求，可以通过JS进行更深度的定制。在&amp;nbsp;theme/index.tsx&amp;nbsp;中使用&amp;nbsp;ESM重导出[17]，可以覆盖任意一个Rspress的内置组件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd59a86ad14793813bd1e21921860499.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;修改组件弹出：你可以使用全新的&amp;nbsp;`rspress pop [组件]`&amp;nbsp;命令，这个命令将指定的组件源代码复制到&amp;nbsp;theme/components/&amp;nbsp;目录下，你可以自由这些代码，甚至直接替换AI，来实现深度定制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;# 将 DocFooter 组件导出到 theme 目录
rspress eject DocFooter&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航栏、侧边栏标签&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 实现了&amp;nbsp;Tag 组件[19]，现在可以使用 frontmatter 中的标签属性，在侧边栏或导航栏进行 UI 标注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;---
tag: new, experimental # 会在 H1 和 Sidebar 进行显示
---

import { Tag } from &#39;@rspress/core/theme&#39;;

# Tag

## Common tags &lt;tag tag=&quot;new&quot;&gt; {/* 会在右侧 outline 进行显示 */}&lt;/tag&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/63/63de4b07258aa8c550ee6ad2a11d88f6.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内置多语言支持&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 1.x 版本中，Rspress 仅内置了中文，如果使用其他语言如 zh，必须对所有的文本都进行配置，使用起来更繁琐。现在 2.0 主题内置了 zh、en、ja、ko、ru 等多种语言的翻译文本，系统会根据语言配置自动进行“Tree Shaking”，仅限你使用到的文本及语言，未内置的语言会兜底到 en文本。您也可以通过&amp;nbsp;`i18nSource`&amp;nbsp;配置项扩展或覆盖翻译文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 未来会支持更多内置语言，如果你有兴趣，请参考&amp;nbsp;这位贡献者的 Pull Request&amp;nbsp;[21]。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;llms.txt 支持&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress现在将&amp;nbsp;llms.txt&amp;nbsp;[22]&amp;nbsp;生成能力集成到core中，并实现了全新的SSG-MD（Static Site Generation to Markdown，静态站点Markdown生成）能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在基于 React 动态渲染的前端框架中，往往存在静态信息无法提取的问题，Rspress 也面临同样的挑战。Rspress 用户通过&amp;nbsp;MDX 片段[23]、React 组件、Hooks 以及 TSX 路由等动态特性来增强表现力。但这些动态转换在 Markdown 文本内容时会面临以下问题：&lt;/p&gt;&lt;p&gt;直接将 MDX 输入给 AI 会包含大量代码噪音，并丢失 React 组件内容；将 HTML 转为 Markdown 往往效果不佳，信息质量难以保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这个问题，Rspress 2.0引入了&amp;nbsp;SSG-MD&amp;nbsp;[24]&amp;nbsp;特性。这是一个全新的功能，它类似于&amp;nbsp;静态站点生成（SSG）[25]，但不同的地方相当于你的页面渲染为Markdown，而不是文件HTML文件，并生成&amp;nbsp;llms.txt&amp;nbsp;[26]&amp;nbsp;及llms-full.txt相关文件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99c86430d85a5accd4987d3d59f70f80.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比于将 HTML 转化为 Markdown 等传统方式，SSG-MD 在渲染期间拥有更优质的信息源，比如 React 虚拟 DOM，从而保证更高的静态信息质量和灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c75fe9b333fc0d5411342ef46be04b22.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;启用方式非常简单：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import&amp;nbsp;{ defineConfig }&amp;nbsp;from&amp;nbsp;&#39;@rspress/core&#39;;

export&amp;nbsp;default&amp;nbsp;defineConfig({
&amp;nbsp; llms:&amp;nbsp;true,
});&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建后将生成如下结构：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d5197e48de9384abbf82beb6424ec8d3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若想定制自定义组件的渲染内容，可通过环境变量控制：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8c/8c23a957526d83d3f375ac1161b069df.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样既保证了文档的交互体验，也能帮助AI理解组件的语义信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;SSG-MD使用指南[27]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Shiki 编译时代码块高亮&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 默认使用&amp;nbsp;Shiki&amp;nbsp;[28]&amp;nbsp;进行代码高亮。相比 1.x 的 prism 运行时高亮方案，Shiki 在编译时完成高亮处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持多种主题样式，比如在&amp;nbsp;CSS变量[29]&amp;nbsp;页面可以交互式切换和预览不同的Shiki主题。同时Shiki也允许使用自定义的&amp;nbsp;变压器[30]&amp;nbsp;进行扩展来丰富的写作，例如twoslash等。引入编程语言，不增加运行时间和包体积。基于 TextMate 语法实现与 VS Code 一致的准确语法高亮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是一些 Shiki Transformer 的视觉，仔细感受 Shiki 带来的文档创意：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;使用&amp;nbsp;@rspress/plugin-twoslash&amp;nbsp;[31]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;const&amp;nbsp;hi =&amp;nbsp;&#39;Hello&#39;;
const&amp;nbsp;msg =&amp;nbsp;`${hi}, world`;
// &amp;nbsp; &amp;nbsp;^?&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;使用&amp;nbsp;transformerNotationFocus&amp;nbsp;[32]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;console.log(&#39;Not focused&#39;);
console.log(&#39;Focused&#39;);&amp;nbsp;// [!code focus]
console.log(&#39;Not focused&#39;);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;代码块[33]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;构建性能提升&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 底层由 Rsbuild 和 Rspack 2.0 预览版本驱动，同时默认开启了后续编译[34]&amp;nbsp;和&amp;nbsp;持久化存储[35]。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;默认开启&amp;nbsp;dev.lazyCompilation&amp;nbsp;[36]，只有当你访问某些页面时，该页面才会被编译，大幅提升了开发速度启动，甚至实现了数十级的冷启动。Rspress 同时实现了路由的预加载策略，当鼠标暂停在链接上时会预先加载目标路由页面，搭配lazyCompilation 实现稀疏的开发体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0beec621d783272b5fc95427665e47d9.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持久化存储&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2.0默认同时开启了&amp;nbsp;持久化服务器[37]，在热启动中复用上次编译的结果，提升了30%-60%的构建速度。这意味着在首次运行&amp;nbsp;rspress dev&amp;nbsp;或&amp;nbsp;rspress build&amp;nbsp;之后，后续启动速度都会明显提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;文档开发体验&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;默认开启死链检查&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0默认开启死链检查功能。在构建过程中，会自动检测文档中的无效链接，帮助你及时发现和修复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;import { defineConfig } from &#39;@rspress/core&#39;;

export default defineConfig({
  markdown: {
    link: {
      checkDeadLinks: true, // 默认开启，可通过 false 关闭
    },
  },
});&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ce/ce9694c1c01266c390cbd24d711ac5da.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;链接[38]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件代码块&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您可以使用 file=&quot;./path/to/file&quot; 属性来引用外部文件作为代码块的内容，将示例代码放在单独的文件中维护中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;```ts file=&quot;./_demo.ts&quot;

```&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;tsx&quot;&gt;```tsx file=&quot;&lt;root&gt;/src/components/Button.tsx&quot;

```&lt;/root&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;请参阅&amp;nbsp;文件代码块[39]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预览 更灵活的元用法&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;@rspress/plugin-preview [40] 现在基于元属性使用，更加灵活，也可以殴打文件代码块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是一个使用 iframe 预览代码块的示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;tsx&quot;&gt;```tsx preview=&quot;iframe-follow&quot; file=&quot;./_demo.ts&quot;

```&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它将会渲染为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/42/42e2e1cd7a4b7d10604292772fb2e937.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并且&amp;nbsp;@rspress/plugin-playground&amp;nbsp;[41]&amp;nbsp;现在支持和plugin-preview一起使用，通过meta属性切换即可，例如&amp;nbsp;```tsx playground&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持HMR的一些配置文件&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于 Rsbuild 重新设计的&amp;nbsp;虚拟模块插件[42]，现在支持&amp;nbsp;i18n.json、_nav.json、_meta.json文件代码块以及&amp;nbsp;@rspress/plugin-preview&amp;nbsp;中 iframe 相关的 HMR。修改这些配置文件后，页面会自动热更新，无需手动刷新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Rslib 和 Rspress&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用&amp;nbsp;create-rslib&amp;nbsp;项目项目时，您现在可以选择 Rspress 工具。这让您能够在开发组件库的同时，快速搭建搭建的文档站点，用于编写创建组件的使用说明、展示 API 参考，或实时预览组件效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;执行&amp;nbsp;npm create rslib@latest&amp;nbsp;并选中Rspress，会生成下方的文件结构：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;├── docs
│   └── index.mdx
├── src
│   └── Button.tsx
├── package.json
├── tsconfig.json
├── rslib.config.ts
└── rspress.config.ts&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模版中内置了&amp;nbsp;rsbuild-plugin-workspace-dev&amp;nbsp;[43]&amp;nbsp;插件，可在启动Rspress开发服务器的同时自动运行Rslib的watch命令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直接运行&amp;nbsp;npm run doc&amp;nbsp;启动 Rspress 的开发服务器对 Rslib 组件库进行预览：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;scripts&quot;: {
    &quot;dev&quot;: &quot;rslib build --watch&quot;,
    &quot;doc&quot;: &quot;rspress dev&quot; // 执行该命令
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;更多 Rspress 官方插件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 新增了多个官方插件：&lt;/p&gt;&lt;p&gt;@rspress/plugin-algolia：支持替换 Rspress 的内置搜索为&amp;nbsp;Algolia DocSearch&amp;nbsp;（感谢&amp;nbsp;@algolia&amp;nbsp;团队的帮助）；@rspress/plugin-twoslash：为 TypeScript 代码块添加类型提示；@rspress/plugin-llms：为不支持 SSG 和 SSG-MD 的项目提供 llms.txt 生成能力；@rspress/plugin-sitemap：自动生成&amp;nbsp;Sitemap&amp;nbsp;文件，用于优化SEO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;其他重大变化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从 Rspress 1.x 迁移&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果您是 1.x 项目的用户，我们准备了一份升级的迁移文档，帮助您从 1.x 升级到 2.0。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你可以直接使用Pages中的“复制Markdown”功能，将其输入给你常用的编码代理（如Claude Code等）来完成迁移。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;请参考&amp;nbsp;迁移指南[51]。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;删除 mdxRs 配置&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们注意到很大一部分 1.x 用户为了使用 Shiki、组件库预览功能和自定义评论/rehype 插件，而主动关闭&amp;nbsp;mdxRs，并且在开启循环编译和持久化缓存后，即使使用 JS 版本的 mdx 解析器，性能优化效果已经非常显着。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了换取更好的扩展性和可维护性，我们决定在 Markdown/MDX 编译流程中不再使用 Rust 版本的 MDX 解析器（@rspress/mdx-rs）。这使得 Rspress 能够更好地集成 Shiki 等 JavaScript 生态的工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Node.js 与下游依赖版本要求&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 要求 Node.js 版本 20+，React 版本 18+。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;包名及导入路径变更&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress将&amp;nbsp;rspress、、、@rspress/runtime都&amp;nbsp;整合进了&amp;nbsp;中，项目@rspress/shared和&amp;nbsp;插件现在只需安装一个包即可。@rspress/theme-default@rspress/core@rspress/core&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;dependencies&quot;: {
-   &quot;rspress&quot;: &quot;1.x&quot;
-   &quot;@rspress/shared&quot;: &quot;1.x&quot;
+   &quot;@rspress/core&quot;: &quot;^2.0.0&quot;
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;- import { defineConfig } from &#39;rspress/config&#39;;
+ import { defineConfig } from &#39;@rspress/core&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;- import { useDark } from &#39;rspress/runtime&#39;
- import { PackageManagerTabs } from &#39;rspress/theme&#39;;
+ import { useDark } from &#39;@rspress/core/runtime&#39;
+ import { PackageManagerTabs } from &#39;@rspress/core/theme&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你开发了 Rspress 插件，那么该插件的 peerDependency 从&amp;nbsp;rspress&amp;nbsp;更改为&amp;nbsp;@rspress/core：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;peerDependencies&quot;: {
    &quot;@rspress/core&quot;: &quot;^2.0.0&quot;
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;下一步&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 的发布只是一个新的起点。本次发布后，Rspress 将持续迭代：&lt;/p&gt;&lt;p&gt;推进生态集成：与Rslib、Rstest更深度地结合，提供接入组件项目和库项目的标准化开发体验；探索AI与文档更复杂的集成：如智能问答、自动摘要等；完善SSG-MD决策并更加自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感谢所有为 Rspress 做出贡献的开发者和用户！如果您在使用过程中遇到问题或有任何建议，欢迎在&amp;nbsp;GitHub Issues&amp;nbsp;[52]&amp;nbsp;中反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;立即使用或升级到Rspress 2.0，体验全新的文档开发之旅！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;npm create rspress@latest&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;博客原文链接：https://rspress.rs/zh/blog/rspress-v2&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考资料&lt;/p&gt;&lt;p&gt;[1]&amp;nbsp;Rsbuild：&lt;a href=&quot;https://rsbuild.rs/&quot;&gt;https://rsbuild.rs/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[2]&amp;nbsp;自定义主题：&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[3]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[4]&amp;nbsp;SSG-MD：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[5]&amp;nbsp;懒加载编译：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspack.rs/guide/features/lazy-compilation&quot;&gt;https://rspack.rs/guide/features/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[6]&amp;nbsp;@rspress/plugin-twoslash:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[7]&amp;nbsp;json 模式：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/auto-nav-sidebar&quot;&gt;https://v2.rspress.rs/zh/guide/basic/auto-nav-sidebar&lt;/a&gt;&quot;&amp;nbsp;#json&amp;nbsp;-schema-type 提示&lt;/p&gt;&lt;p&gt;[8]&amp;nbsp;@rspress/plugin-preview:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/preview&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[9]&amp;nbsp;@rspress/plugin-playground：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspress.rs/plugin/official-plugins/playground&quot;&gt;https://rspress.rs/plugin/official-plugins/playground&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[10]&amp;nbsp;@Zovn魏：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://x.com/wei_zhong41532&quot;&gt;https://x.com/wei_zhong41532&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[11]&amp;nbsp;自定义主题：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[12]&amp;nbsp;CSS 变量：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/vars&quot;&gt;https://v2.rspress.rs/zh/ui/vars&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[13]&amp;nbsp;BEM 命名规范：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://getbem.com/&quot;&gt;https://getbem.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[14]&amp;nbsp;Tailwind：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://tailwindcss.com/&quot;&gt;https://tailwindcss.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[15]&amp;nbsp;Less：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://lesscss.org/&quot;&gt;https://lesscss.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[16]&amp;nbsp;Sass：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://sass-lang.com/&quot;&gt;https ://sass-lang.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[17]&amp;nbsp;ESM 重新导出：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#reexport&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[18]&amp;nbsp;rspress eject [component]:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/api/commands&quot;&gt;https://v2.rspress.rs/zh/api/commands&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#rspress&lt;/a&gt;&quot;&amp;nbsp;-eject&lt;/p&gt;&lt;p&gt;[19]&amp;nbsp;标签组件：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/layout-components/tag&quot;&gt;https://v2.rspress.rs/zh/ui/layout-components/tag&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[20]&amp;nbsp;i18nSource：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/api/config/config-basic&quot;&gt;https://v2.rspress.rs/zh/api/config/config-basic&lt;/a&gt;&quot;&amp;nbsp;#i18nsource&lt;/p&gt;&lt;p&gt;[21]&amp;nbsp;贡献者的 Pull 请求：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/web-infra-dev/rspress/pull/2827&quot;&gt;https://github.com/web-infra-dev/rspress/pull/2827&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[22]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[23]&amp;nbsp;MDX 片段：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/use-mdx/components&quot;&gt;https://v2.rspress.rs/zh/guide/use-mdx/components&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[24]&amp;nbsp;SSG-MD：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[25]&amp;nbsp;静态站点生成（SSG）：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[26]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[27]&amp;nbsp;SSG-MD使用指南：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[28]&amp;nbsp;Shiki：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://shiki.style/&quot;&gt;https://shiki.style/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[29]&amp;nbsp;CSS 变量：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/vars&quot;&gt;https://v2.rspress.rs/zh/ui/vars&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[30]&amp;nbsp;变形金刚：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://shiki.style/guide/transformers&quot;&gt;https://shiki.style/guide/transformers&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[31]&amp;nbsp;@rspress/plugin-twoslash:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[32]&amp;nbsp;transformerNotationFocus：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/use-mdx/code-blocks&quot;&gt;https://v2.rspress.rs/zh/guide/use-mdx/code-blocks&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#transformernotationfocus&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[33]&amp;nbsp;代码块：&amp;nbsp; &lt;a href=&quot;https://rspress.rs/zh/guide/use-mdx/code-blocks&quot;&gt;https:&amp;nbsp;//v2.rspress.rs/zh/guide/use-mdx/code-blocks&lt;/a&gt;&quot;&amp;nbsp;#shiki&amp;nbsp;-transformers&lt;/p&gt;&lt;p&gt;[34]&amp;nbsp;编译：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspack.rs/zh/guide/features/lazy-compilation&quot;&gt;https://rspack.rs/zh/guide/features/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[35]&amp;nbsp;持久化服务器：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/performance/build-cache&quot;&gt;https://rsbuild.rs/zh/config/performance/build-cache&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[36]&amp;nbsp;dev.lazyCompilation：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/dev/lazy-compilation&quot;&gt;https://rsbuild.rs/zh/config/dev/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[37]&amp;nbsp;持久化服务器：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/performance/build-cache&quot;&gt;https://rsbuild.rs/zh/config/performance/build-cache&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[38]&amp;nbsp;链接：&amp;nbsp;&amp;nbsp;https ://v2.rspress.rs/zh/guide/use-mdx/link&lt;/p&gt;&lt;p&gt;[39]&amp;nbsp;文件代码块：&amp;nbsp; https:&amp;nbsp;//v2.rspress.rs/zh/guide/use-mdx/code-blocks&amp;nbsp;#file&amp;nbsp;-code-block&lt;/p&gt;&lt;p&gt;[40]&amp;nbsp;@rspress/plugin-preview:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/preview&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[41]&amp;nbsp;@rspress/plugin-playground:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/playground&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/playground&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[42]&amp;nbsp;虚拟插件模块：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/rstackjs/rsbuild-plugin-virtual-module&quot;&gt;https://github.com/rstackjs/rsbuild-plugin-virtual-module&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[43]&amp;nbsp;rsbuild-plugin-workspace-dev：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/rstackjs/rsbuild-plugin-workspace-dev&quot;&gt;https://github.com/rstackjs/rsbuild-plugin-workspace-dev&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[44]&amp;nbsp;@rspress/plugin-algolia：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/algolia&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/algolia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[45]&amp;nbsp;Algolia DocSearch：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://docsearch.algolia.com/&quot;&gt;https://docsearch.algolia.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[46]&amp;nbsp;@algolia：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://x.com/algolia&quot;&gt;https://x.com/algolia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[47]&amp;nbsp;@rspress/plugin-twoslash：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[48]&amp;nbsp;@rspress/plugin-llms：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/llms&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/llms&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[49]&amp;nbsp;@rspress/plugin-sitemap：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/sitemap&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/sitemap&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[50]&amp;nbsp;网站地图：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.sitemaps.org/&quot;&gt;https://www.sitemaps.org&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[51]&amp;nbsp;迁移指南：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/migration/rspress-1-x&quot;&gt;https://v2.rspress.rs/zh/guide/migration/rspress-1-x&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[52]&amp;nbsp;GitHub Issues：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/web-infra-dev/rspress/issues&quot;&gt;https://github.com/web-infra-dev/rspress/issues&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0dTBmTp4SlBVvoaybNCh</link><guid isPermaLink="false">https://www.infoq.cn/article/0dTBmTp4SlBVvoaybNCh</guid><pubDate>Thu, 05 Feb 2026 08:31:48 GMT</pubDate><author>字节跳动Web基础设施-Rstack团队</author><category>AI&amp;大模型</category></item><item><title>AI 驱动的大数据自治：TCInsight 智能应对复杂运维挑战</title><description>&lt;p&gt;在大数据平台高速发展的当下，生态扩张与业务量激增，致使大数据分布式组件问题愈发棘手，传统专家运维模式捉襟见肘。以腾讯大数据庞大的规模为例，面对海量计算单元、繁杂技术栈以及千万级任务管理，借助 AI 驱动实现大数据系统的故障和问题的快速洞察与自治能力，已成为行业迫切需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 InfoQ 举办的 QCon 全球软件开发大会（北京站）上，腾讯专家工程师熊训德做了专题演讲“&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6408&quot;&gt;AI 驱动的大数据自治：智能应对复杂运维挑战&lt;/a&gt;&quot;”，他介绍了如何通过可拔插的决策引擎、以及数据专家自治智能体构建大数据智能管家，让企业能够理解如何高效、智能地处理复杂的运维场景，从而大幅提升大数据场景下运维效率与准确性，引领大数据线上系统迈向全面自治的实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大数据系统自治背景与挑战&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我简要介绍一下整个大数据系统，以及其在自治背景下的相关挑战。大数据系统本身组件众多，涵盖了从底层的 IaaS，到存储、计算框架，以及上层的工具层等多个层面。具体来说，IaaS 层面涉及到机器本身的网络和性能，而存储层则包括分布式文件系统（如 HDFS）和对象存储等。在调度方面，我们有 Kubernetes 和 Hadoop- 体系，以及针对 AI 方面的特定调度机制。再往上一层则是计算框架，例如 Spark 和 Flink 等流计算框架。最上层则是各种工具，这些工具在不同方面的使用都使得整个大数据系统的复杂性显著增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大数据系统本质上是一个分布式系统。如果单机系统已经如此复杂，那么分布式系统则需要考虑数据的溯源以及在不同机器上的分布情况，无论是主从结构（master 和 slave）还是多工作节点（worker）的协作模式，都会使得整个系统在处理问题、查找根源以及故障恢复时变得极为困难。此外，大数据系统的数据处理链路通常非常长。例如，数据采集可能来源于多种源头，如代理（Agent）、MySQL 数据库，或者在物联网场景下，可能是汽车或传感器等设备。采集到的数据需要通过数据接入层，目前常见的架构包括 Kafka 或其他消息队。接入后，数据会进入计算阶段，可能是实时计算（如 Flink）或离线计算（如 Spark）。计算完成后，数据需要存储到 HDFS 系统或对象存储中。最后，在数据应用层面，我们可能需要进行预处理以供 AI 使用，进行训练或推理工作，或者生成商业智能 BI 报表。因此，整个数据链路非常长，这也使得我们在进行故障根因分析或自治处理时，需要综合考虑所有相关场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/99/99b2da444f09f67b1c8a5dacc7bc52bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们处理大数据故障时，业务部门或客户往往会提出一个关键问题：“何时能够恢复？能否实现自动恢复，以尽快减少损失？”然而，我们在进行故障恢复或诊断时，高度依赖于运维 SRE 的专家经验。通常情况下，如果没有三年以上的大数据运维经验，很难有效且完善地处理复杂的大数据故障。此外，由于整个诊断和故障恢复的时间链路非常长，导致整体效率低下。更糟糕的是，故障可能已经结束，而我们只能进行事后处理，此时大数据系统可能已经遭受了实际的损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大数据智能管家技术框架及关键实现路径&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;腾讯大数据智能管家 TCInsight 技术架构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些背景，我们团队在大约五年前提出了构建大数据智能管家 TCInsight 的想法，致力于解决大数据系统自治相关的工作。我们的大数据智能管家整体技术架构分为三层。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一层是观测层。它主要负责监控基础设施即服务（IaaS），包括主机网络等的监控数据，同时采集日志和关键事件。我们还将大数据组件，如 HDFS、Spark、Hive 和 YARN 等的关键监控日志事件进行统一上报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二层是服务分析层，主要负责数据实时处理和算法决策洞察。服务分析层分为三个部分。第一部分是实时分析，主要目的是快速处理数据，包括异常收敛。例如，当事件或告警过多时，我们需要迅速整合，否则会给运维 SRE 或研发人员带来较大挑战。我们会对数据进行基础预处理。第二部分是离线服务，主要用于根因分析或自治服务时的离线分析和定时巡检。在数据量较大时，离线分析尤为重要。第三部分是算法决策，主要涉及模型和算法库的分析，以及知识库和评测库的建设，还包括离线训练等工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三层是应用层，主要负责大数据运维自治，并对外提供接口。应用层分为两大块：自治修复和自治决策。例如，以 Hive 为例，当业务侧编写了一个 SQL 查询，可能会导致 HDFS 存储空间被占满，从而影响其他任务的提交。此时，我们需要快速对该 SQL 进行限制，或者在业务非常关键且不能直接终止的情况下，预测可能得存储和计算量，进行自助弹性伸缩。此外，我们还需要进行冷热数据分离，以实现成本分析和自助转冷操作。在自治决策方面，我们需要判断是否进行参数调优，因为某些参数调整可能需要重启系统才能生效，这可能会扩大故障范围。此时，我们需要做出关键决策，例如选择扩容，或者让 AI 参与具体工作。我们还可以进行错峰执行，例如在 YARN 的多个队列中，调整队列的执行时间，以优化资源分配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用层还包括业务洞察部分，主要用于预测分析、成本分析和根因分析等工作。这些工作相对滞后，我们的目标是先恢复系统，然后再进行深入分析。此外，我们还会生成巡检报表，并进行一键健康评估。健康评估在我们的系统中非常重要，它综合评估了 IaaS、存储、调度和计算等各个部分的健康状况，为关键自治决策提供依据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在架构的中间部分是我们的算法或引擎层。引擎分为两部分：规则引擎和我们自主研发的元启引擎。元启引擎结合了 AI 算法和我们内部的混元大模型。规则引擎主要用于执行明确的操作，例如扩容，以缓解问题。对于复杂或关联性较高的场景，我们会接入算法或大模型，以提升系统的健康状况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我会详细说明我们在大数据智能管家过程中的一些关键思考和实现能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/c4/c49c7269e3b1c78cf4c9f4601412820c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;分层的大数据运维框架 - 渐进式自治&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于大数据体系的复杂性，TCInsight 实现自治的是一个渐进式的过程。当我们接手一个系统时，不能期望所有大数据运维工作能够立即实现完全自治。实际上，我们基于一个较为普遍的理念：在没有一线专家或专业人才的情况下，一线人员或客户也能够实现自治处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们根据问题的复杂程度进行分类处理：对于简单重复且解决方案确定问题，我们直接采用 AI 驱动的方式进行处理。目前，这类问题大约占我们总问题的 10% 左右。然而，剩下的 90% 问题尚未能完全实现自治。对于这部分问题，我们希望通过售后体系中的专项人员和 SRE 的共同努力，借助我们之前提到的平台层，利用大模型和 AI 增强能力，持续为系统提供支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，我们期望通过三年以上经验的产研人员或 SRE 专家，进一步强化知识库和工具建设。通过这种逐步积累和优化我们的产品能力，我们希望能够逐步提高自治的比例，最终使其达到 90% 以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;多智能决策引擎思考和设计一问题域&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在业界，主要有三种常见的方法：显式编程、基于优化方法的处理以及专家系统。第一种显式编程对于研发人员来说并不陌生，它本质上是通过编写规则或工作流来构建一个简单的规则引擎，从而实现直接的决策。例如，当存储使用率超过 75% 时，系统自动触发扩容操作。这种方法简单直接，但灵活性有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二种是基于优化方法的处理。在大模型尚未普及的时代，我们通过优化模型来提升系统性能。例如，原本只能优化 40% 的系统，通过采用贪婪算法或聚合模型等技术，可以将其优化效果提升至 80% 以上。这种方法更多地依赖于深度学习和大模型的强大能力，能够更好地处理复杂的优化问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三种是智能全自治域系统。全自治域系统的核心在于利用专家的经验和知识，尽管专家人数有限，但他们的经验可以通过系统化的方式赋予平台更强的能力。专家系统的关键在于如何将专家的经验转化为可操作的决策逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在明确了这些决策引擎的技术路径后，我们进一步思考了在大数据领域构建智能决策系统的关键问题。首先，数据的可用性至关重要。无论是基于 AI 的训练还是大模型的应用，数据标注的准确性和完整性是基础。如果数据标注不足，可能会导致模型出现幻读甚至错误的输出，从而影响决策的准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，系统的可解释性也是一个关键问题。专家和文档作者需要确保知识库中的内容不仅系统能够理解，而且一线人员和客户也能够轻松掌握。这一点直接关系到决策的准确性和适用范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，实时性要求也不容忽视。我们的目标是先快速恢复系统，后续再进行深入分析。这就要求决策过程和最终的行动必须足够迅速，以满足实时性的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合考虑以上因素，在决策引擎的选择上，我们决定结合规则引擎和专家系统的智能决策引擎共同构建了全自治域系统 TCInsight。这种方法既能够利用规则的明确性和可操作性，又能借助专家系统的灵活性和经验优势，逐步提升系统的自治能力和决策准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;Al 驱动的规则引擎自治系统&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在构建基于规则引擎的知识系统时，我们首先对系统中的各类数据进行了统一管理。这些数据包括指标（metrics）、日志（log）以及事件（event），我们会将它们统一上报至我们内部构建的数据库适配系统。该系统是基于 Inpara 和 Flink 构建的，数据最终会被存储到时序数据库中。随后，我们利用 Flink 对数据进行预处理，并结合训练好的模型以及特征库，对数据进行特征分析。基于这些分析，我们会进行基础的异常检测、关联分析以及趋势预测等工作，从而形成初步的告警摘要和预测摘要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，我们可能会收到告警信息，提示 HDFS 存储空间即将用尽，或者 YARN 队列的等待时间过长，又或者 StarRocks 或 Trino 的 CPU 占用率过高，某个 SQL 查询扫描的数据量过大，超出了设定的阈值。基于这些信息，我们会生成整体的告警或预测摘要。如果预测显示 HDFS 的增长趋势过快，可能会在 5 分钟内被填满，我们就会对 IaaS、存储、引擎和调度等各个层面进行评估，计算它们的健康分数。如果健康分数低于某个阈值，或者即将达到该阈值，我们就会启动规则引擎进行处理。例如，我们可能会尝试简单的扩容操作来缓解问题，或者在业务允许的情况下，直接终止一些不关键的 SQL 查询或任务，以减少资源占用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行这些操作后，我们会制定一个详细的执行计划。以扩容为例，在执行扩容操作之前，我们需要先检查 HDFS 的整体状态是否正常，数据是否均衡分布，以及 NameNode 和 DataNode 之间的流量是否稳定。因为如果流量过大，可能会导致 DataNode 负载过高，甚至引发更严重的问题。只有在确认一切正常后，我们才会通过 IaaS 层扩容机器，并在扩容完成后进行数据均衡操作，以确保系统恢复正常。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;完成这些操作后，我们会记录整个过程的状态，并进行反馈。如果扩容后监控数据显示系统恢复正常，那么我们认为这次自治决策是成功的，并将结果记录下来作为后续处理的参考。然而，如果扩容后情况反而恶化，例如数据倾斜导致 SQL 查询速度变慢，引擎侧的健康分数急剧下降，那么我们会紧急通知专家介入，重新审查整个分析过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种基于规则引擎的处理方式具有高效和准确的特点。目前，在我们系统中，基础指标的覆盖率达到 90%，存储场景的覆盖率为 50%，任务场景的覆盖率为 30%。在周期性任务的处理上，我们已经能够覆盖 90% 的场景。在异常诊断方面，我们能够处理 70% 的异常场景，整体数据表现良好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不意味着我们的工作已经完成。实际上，大数据系统的复杂性远超我们的预期。例如，我们在两年前曾遇到一个问题：在对 HDFS 进行扩容后，发现数据分布不均衡，导致 Spark 任务的执行速度反而变慢。从常理来看，扩容后资源增加，任务执行速度应该加快，但实际上并非如此。原因在于扩容后数据的均衡性并没有达到预期，同时业务侧提交了大量任务，导致系统整体性能下降。这说明我们目前只能处理已知的情况，而对于一些未考虑到的复杂场景，我们还需要进一步优化和改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e9/e9b16944fe400894c20ad936bdc5a018.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Al 驱动的全自治域系统&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上述思考，我们提出了一个全新的全自治系统概念。与之前的方法不同，我们在决策过程中引入了大模型的相关分析。无论是当前备受关注的 DeepSeek，还是此前我们接触过的其他类似模型，其核心优势在于执行步骤和推理能力。因此，我们开始尝试将大模型的相关功能融入整个自治决策系统中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在预测和分析阶段，系统仍然会进行数据预处理和特征分析，并开展异常检测、关联分析以及趋势预测等工作。这些信息汇总后，会生成初步的概述信息。然而，与以往不同的是，由于引入了大模型，我们需要构建一个“优先级与目标系统”（以下简称“目标系统”）。我们会在这个目标系统中预先定义优先级和目标。例如，对于存储系统，我们设定存储使用率不得超过 80%，并且数据不能快速转冷；对于引擎，我们希望优化其执行时间；对于上层应用，我们要求其不能出现错误。这些优先级和目标会被配置到目标系统中，生成诊断建议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，我们会将这些数据输入到混元模型中，并结合我们之前的决策分析结果，生成具体的执行步骤。这些执行步骤融合了传统执行引擎、规则引擎以及传统深度学习算法或基础算法的执行计划。执行计划生成后，我们会重新预检测系统状态，重新评估预测分析结果以及执行计划可能带来的状态变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果发现执行该计划后系统健康分数可能更低，即情况可能恶化，那么我们的专家团队会介入。我们会创建一个专家工单，让专家对执行计划进行评估，并决定是否停止执行。相反，如果预测和状态评估显示执行计划后系统健康分数将高于目标值，那么我们会执行该计划，并将执行计划标记后存入知识库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;执行完成后，我们会继续进行预测分析、异常检测以及整体状态评估。如果系统健康度如我们预测的那样有所提升，我们会重新进行标记和分析，以便系统能够继续执行后续操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/37/37cbef48d48a61796cce23baa44c0a45.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;数据质量对预测影响 &amp;amp; 优化&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在构建整个系统的过程中，我们花费了大量时间进行调试，尤其是在系统上线试运行阶段。现在，我想重点介绍一下我们在调试过程中采取的关键措施，这些措施让系统更加稳定，并显著提高了预测的准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于从事时序预测研究的人员来说，一个常见的问题是如何处理上报数据中的断点。这种情况可能由多种原因引起。例如，当系统发生故障时，机器的 CPU 或内存可能已经满负荷运行，导致在关键时刻数据丢失。在分布式系统中，这种数据丢失可能会引发上层系统的乱序操作。假设我们上报的时间是 12 点整，但由于长时间的内存不足（OOM）或 CPU 负载过高，数据可能直到 12 点零 5 秒甚至 12 点零 1 分才上报。然而，故障的实际发生时间并非 12 点零 1 分，但上报时间却显示为 12 点零 1 分，这就导致了数据的乱序问题。此外，还可能出现重复上报的情况，即同一条日志或指标连续上报多次，这使得我们难以确定真正的时间点或事件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题引发了几个关键的挑战。首先，当数据出现断点时，我们需要决定是否进行插值。目前业界常用的算法包括直接丢弃数据或采用简单的插值方法。对于故障场景来说，直接丢弃数据可能并不是一个好方法，因为这些数据代表了当时关键的监控指标。即使进行插值，如果处理不当，也可能导致数据不准确。此外，如果数据质量不佳，将严重影响我们的预测能力和关键异常处理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们重点对数据质量进行了优化，主要从三个方面入手。首先，我们对时序指标或日志的有效性进行评估。以往最简单的评估方式是检查数据是否超过完整性阈值。另一种常见的做法是检查数据是否满足差分阈值，或者在 IoT、时序场景中直接进行简单的拼凑。我们提出了一种基于完整性的实际评估方法。具体来说，我们将每个数据进行分段处理，然后基于自回归模型对每个分段进行评估检测。如果数据通过了自回归分析的评估，我们认为这些数据是可用的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在确认数据可用之后，我们面临的另一个问题是数据的补齐和连接。目前常用的方法包括直接进行差分或简单的拼接。我们的思路是采用自回归预测和自回归拼接的方法。这种方法的优势在于处理速度快，能够快速对分段数据进行处理。此外，这种方法既能进行预测，又能完成数据合并操作。通过这种方法，我们显著提升了数据的有效性，整体提升了 10%。在周期性任务和异常诊断方面，准确性提高了 30% 以上。同时，时序预测的时间也缩短了 28%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e4/e40cc5e6cf9311b60dd770c345a02121.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在构建大数据专家库智能体的过程中，尝试了一种与业界常见的做法略有不同的方案。我们不仅实现了向量检索，还引入了文本检索。这种设计的选择源于我们在构建知识库时对传统向量检索方法的深入思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统向量检索在相关性分析方面表现出色，例如在使用 FastText 等工具时，能够快速识别出与查询相关的数据。然而，这种方法存在一个明显的局限性：它无法直接反映召回数据的质量，也就是说，在检索过程中，我们难以预估数据的相关性是否真正符合需求。为了解决这一问题，我们引入了文本检索机制。通过文本检索，我们能够更清晰地理解数据之间的关联性，尤其是在知识库的构建过程中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们构建知识库时，一个常见的思路是将操作步骤进行分层。以扩容操作为例，它可能与存储层有很强的相关性，但这种相关性背后的原因并不明确。通过文本检索，我们可以补充这些缺失的上下文信息，从而更全面地理解数据之间的关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大数据系统通常分为多层，包括大数据存储层、调度、和引擎等等。这些层之间的相关性可能很强，但它们之间的索引空间检索范围并不像我们想象的那么大。基于这些考虑，我们采用了腾讯的 ES 的架构，结合文本分析和向量检索的优势。这种架构不仅支持大规模的读写操作，还具备高效的检索能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们能够更好地处理组件之间或分层之间的关联关系，使得各部分之间的距离更近，从而提高系统的整体效率。在故障恢复之后，除了通过冷启动将知识库连接起来，我们还利用工单系统、客户反馈和专家系统，结合混元大模型，实现自动化的分类和归纳，持续完善知识库的建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/20/2012303ea01e14f23ff450882abd4123.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实践效果与案例分享&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI 驱动的 HDFS 存储规则引擎自治&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看基于 HDFS 存储规则引擎的自治。这里的关键在于如何快速抽取和分析 HDFS 的 FSImage，以及如何准确把握特征点。我们知道，HDFS 的源数据是以树形结构存储的，而现有的工具无法对这种树形结构进行并行化处理。为了解决这个问题，我们将工作拆分为两部分：第一部分是直接分析源数据的表结构，这样就不需要处理整个树形结构；第二部分是将树形结构手动拆分为多个并行部分，从而实现并行化处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们能够对表分区和关联分区进行拆分，并进行关联分析。同时，我们还能观察到数据的整体冷热分布，以及后续一段时间内的增长趋势。基于这些信息，我们利用规则引擎做出决策，确定关键目标。例如，如果当前存储的健康状况良好，但成本健康分较低，我们可能会自动执行降冷操作。如果发现整个系统的扩容必要性较高，我们可能会进行柔性扩容或自动剔除操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/21/219606253c9bf580fec387a7a7967749.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI 驱动的 SparkSql 调优全自治域&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来分享一个关于 Spark 自动调优的案例。这个想法最初是在项目立项时提出的，当时的想法非常直接：将 Spark 的所有相关信息，包括 SparkSQL、配置信息、上下文信息，以及存储和引擎等，全部整合到一个系统中。我们甚至将所有的 Executor、逻辑计划和物理计划等也纳入其中。初步测试结果显示，这种方法的准确率大约为 30%。然而，我们发现其中约 30% 的结果与实际需求并无相关性，还有 20% 到 40% 的结果存在明显问题。究其原因，通用的大模型缺乏专家级的领域知识，这导致了准确性的不足，同时还出现了幻觉问题。所以我们引入了贝叶斯和 RL 专家系统建议的优化提升 sparksql 的调优效果。在 POC 和线上，目前实现无人工值守自治调优性能效果比工作五年经验还好 10%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/45/4508283e31d0db76ecaa58638489378d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在降本效果相当不错，之前主要关注的 SparkSQL 本身，没有考虑存储和 IaaS 层面的相关影响。在最近我们又升级了这个系统，会将 YARN 调度、HDFS 存储以及相关的管控日志等信息统一汇总，形成一个详细的概述。我们的目标是通过调优实现时间消耗的最优化。为此，我们将这些上下文信息输入模型，并进行在线分析。分析结果不仅包括计算相关的最优参数，还涵盖了调度配置、内核参数的配置下发等。然而，这些配置下发后并不能立即生效，可能需要执行 SQL 控制操作，或者在某些情况下，进行刷新操作。基于这些分析结果，我们会生成一个调参执行计划，然后重新提交任务，并对时间消耗的最优化和系统的整体健康度进行评估。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/1a/1a4e2bd9bb950cf2f236280291aa7a88.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;后续发展和思考&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前我们在自治虽然有些突破，但还远远不够。正如之前提到的，我们已经解决了关键的 10% 的知识问题，这确实帮助我们解决了许多难题。然而，我们还有许多需要思考和改进的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们需要持续优化路径。以 SparkSQL 为例，虽然我们已经对 SQL 进行了优化，但关键信息之间的互联性仍然不足。例如，当我们直接将 HDFS 的最大存储容量纳入考量时，其时间和空间的关联性处理得并不理想。目前，我们主要依赖简单的专家系统来判断优化效果，而这种判断往往缺乏系统化的分析。因此，我们计划在未来持续加强这方面的建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，我们在决策时的目标相对单一。目前，我们的决策主要基于时间预测和健康分的调度，但对于复杂的大数据系统来说，多链路决策的完善性仍有待提高。例如，在关键决策时刻，我们会引入多智能体。目前，我们对决策准确性的把握还不够高，准确率可能只有 70% 到 80%。因此，我们需要持续优化决策过程，以提高准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，关于专家系统，虽然我们在最后一步会强制让 SRE 专家介入，但在实际操作中，我们发现专家介入的时机和方式需要进一步优化。例如，在配置下发后，我们可能需要再次介入，因为有些系统配置是立即生效的，而有些则需要存储后才能生效。因此，我们需要在关键节点上进行更精准的知识干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了上述问题，我个人以及我们团队还需要持续思考和探索后续的应用方向。首先是 agent-Drive 的根因定位（RCA）。我们在故障恢复和根因定位方面还有很大的提升空间。一方面，我们需要更快地响应问题，避免客户受到影响；另一方面，我们需要提高根因分析的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，我们希望实现逐步缓解的操作。目前，我们的操作通常是直接针对目标进行的，但我们认为应该分阶段、分层次地观察和评估每个环节的动作是否对整体健康服务和知识系统有效。虽然我们已经有了一个反应式（Reactive）模型，但它主要集中在直接缓解问题上。我们希望通过逐步缓解的方式，更全面地评估和优化系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，安全性是我们需要持续关注的一个重要方向。在大模型 RL 或智能体的开发过程中，我们可能会面临各种安全风险。一方面，我们需要确保优化操作不会引入更大的问题；另一方面，由于多个团队之间可能共享知识库，我们需要防止信息泄露或因幻觉问题导致其他团队误读知识库信息。这将是我们在未来持续探索的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;熊训德，腾讯专家工程师，腾讯云 EMR 技术负责人，有丰富的大数据领域系统架构、开发、专家系统调优经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;复杂任务，不再主要依赖冗长提示词硬扛了。Agent Skills 将专家流程与工具能力封装为可复用数字技能，由大模型按需调用，推动 AI 从通用助手迈向稳定的专业执行体。围绕 Skills 平台化、模型推理增强与垂直场景落地，Agent 时代正在加速到来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了深入探讨 Agent Skills 在实际应用中的潜力与挑战，在 4 月 16 日 -18 日举办的 QCon 北京大会上，我们特别邀请了 Ubiquiti Quality Assurance 蔡明哲带来专题演讲《从单点辅助到 Agent 闭环：基于 Agent Skills、MCP 与 Playwright 的全链路智能化测试实践》。他将聚焦智能化测试在质量保证中的落地实践，详细拆解 Agent Skills、Playwright Agent 与 MCP 的职责分工与组合范式，并介绍如何从案例生成到自动修复实现全流程工程实践落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/95/95b834d1de765d08e4cc22fdbcc37582.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/F48Of5dWYPgMNqeVIwY0</link><guid isPermaLink="false">https://www.infoq.cn/article/F48Of5dWYPgMNqeVIwY0</guid><pubDate>Thu, 05 Feb 2026 08:23:51 GMT</pubDate><author>作者：熊训德</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>奥运首个官方大模型基于阿里千问，考文垂：这届奥运为的遗产是AI驱动的智能化</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2月5日，米兰冬奥会开幕在即，国际奥委会主席柯丝蒂·考文垂在国际转播中心举行的活动中宣布，国际奥委会已基于阿里千问大模型打造了奥运史上首个官方大模型。这一奥运官方大模型将在专业赛务与公众服务双端同步落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在赛务侧，国际奥委会在其面向各国奥委会工作人员的网站上线了“国家奥委会AI助手”。该助手依托千问大模型强大的多语言理解能力，并通读数百万字官方手册。代表团成员只需用母语提问，即可获取从资格审核到后勤调度等各项问题的精准解答。这一应用有效消除了语言与地域隔阂，大幅提升了全球代表团的备赛协同效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/20/7a/209fe912b27256b0db215e76c835537a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（国家奥委会AI助手）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在公众侧，国际奥委会也将在官网（Olympics.com）上线基于千问大模型打造的“奥运AI助手”。该助手将面向全球观众开放，能够实时、精准地解答关于赛事规则与奥运历史的各类提问，通过AI技术拉近大众与奥运的距离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考文垂在现场高度评价了AI技术对本届冬奥会的变革性意义。她表示，得益于千问大模型的技术支撑，2026米兰冬奥会展现了奥林匹克运动的智能化未来，将成为史上“最智能”的一届奥运会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，基于千问大模型Qwen-VL开发的自动媒体描述系统也在直播生产环节投入运行，实时识别进球、犯规等关键事件并生成描述。此外，AIGC技术也首次大规模应用于冬奥会的内容生产环节。米兰冬奥组委会基于阿里万相大模型，高效创作了一系列面向全球粉丝的多媒体宣传素材。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了大模型应用，阿里云AI增强的转播特效技术渗透率也在本届冬奥会上创下新高。针对冬奥会特有的“雪地背景纹理单一、缺乏特征点导致视觉盲区”的问题，阿里云采用多模型融合算法，攻克了雪地场景的高精度重建难题。该技术已部署于米兰冬奥的10个核心竞赛场馆，覆盖高山滑雪、跳台滑雪、冰球等超三分之二的比赛项目。全球观众将在转播中看到更清晰的“子弹时间”定格画面及新增的“时间切片”特效，身临其境地看清运动员在空中极速翻转的完整轨迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，作为史上赛区地理跨度最广的一届冬奥会，阿里云支撑构建了交通管理系统，在风雪交加的阿尔卑斯山区打通了从城市进入山区的“最后一公里”。同时，阿里云“能耗宝”持续运行，新增“能源问题追踪系统”，以数字化手段支撑米兰冬奥实现更可持续化的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“每一届奥运会都会留下独特的遗产。而米兰冬奥会的遗产将是智能化，具体来说，是人工智能驱动的智能化。”考文垂在演讲最后总结道，“这份AI能力，正是米兰冬奥会留给世界的‘永恒礼物’，它将重塑奥林匹克运动会的未来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Orls4EogPhNyOQAuQIHR</link><guid isPermaLink="false">https://www.infoq.cn/article/Orls4EogPhNyOQAuQIHR</guid><pubDate>Thu, 05 Feb 2026 08:22:22 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>OpenViking：面向 Agent 的上下文数据库</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;挣脱上下文的枷锁：OpenViking，为 AI Agent 而生的开源上下文数据库&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“We are swimming in a sea of information, and we need to learn to navigate.” — Norbert Wiener“我们正畅游在信息的海洋中，我们需要学会航行。” — 诺伯特·维纳&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI Agent 的浪潮已至，它正从简单的任务执行者，演变为能够感知环境、自主规划、并调用工具完成复杂目标的智能实体。然而，在这片充满无限可能的机遇之海中，开发者们却普遍遭遇了一座难以逾越的冰山——上下文管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着模型能力飞速提升，Agent 不再满足于处理单轮对话或短文本，而是开始面对长周期任务、海量多模态数据和复杂的协同需求。记忆、资源、技能……这些原本分散各处的上下文，管理起来愈发混乱。然而，如何高效管理和利用这些上下文，已成为开发者们普遍遭遇的瓶颈：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上下文无序且割裂：记忆在代码中，资源在向量库，技能分散在各个角落，关联和维护成本极高。长程任务需要更多上下文：Agent 逐渐从处理单轮对话转向执行长周期任务，会涉及多工具、多 Agent 间的复杂协同。每一轮任务执行都会给上下文窗口和模型理解带来压力，如果简单的截断或压缩，本质上是“丢卒保帅”，会带来不可逆的信息损失和高昂的模型成本。朴素 RAG 检索效果局限：朴素 RAG 的数据切片是平铺式存储，缺乏全局视野，面对海量、多模态且有信息组织的数据越来越力不从心，可能回去错失关键信息。同时，它过于关注语义相关性，在需要兴趣泛化和探索的开放式场景中表现不佳。上下文缺乏观测和调试：从 DeepSeek 和 Manus 的爆火能发现，在 AI 越来越强大时，用户更渴望白盒化的体验，能看到其思考与决策的轨迹。而传统 RAG 隐式的检索链路如同黑箱，出错时难以归因和调试，改进门槛高。记忆成为核心资产：模型本身是通用的，大家越发意识到沉淀的记忆才是 Agent 的核心资产，但这不止包括使用用户的记忆，还包括 Agent 自身的经验和偏好记忆。记忆需要在开发初期就建设起来，这样才能形成使用时间越长，体验越好的复利效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而近年来，业界也关于 Context Engineering 有一些探索实践：Manus 提出文件系统是上下文的终极形态；Claude Code 的成功验证了文件系统 + Bash 的简洁方案在特定场景下超越复杂向量索引的潜力；而 Anthropic 的 Skills 系统也巧妙地以文件夹来组织能力模块。这些实践给了我们启发，但也反映了一个问题：文件系统是上下文一种很好的组织方式，但并没有一个类似数据库能有效管理Agent所需所有上下文并解决上述问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们正式开源&amp;nbsp;OpenViking——专为 AI Agent 设计的上下文数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们旨在为&amp;nbsp;Agent 定义一套极简的上下文交互范式，让开发者彻底告别上下文管理的烦恼。&amp;nbsp;OpenViking 摒弃了传统 RAG 的碎片化向量存储模式，创新性地采用“文件系统范式”，将 Agent 所需的记忆、资源和技能进行统一的结构化组织。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Memory, Resource, Skill. Everything is a File.记忆、资源、技能，皆为文件。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa73321bf0983a514c999d12e93410a7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;OpenViking 信息图，由 vaka 知识助手生成 (&lt;a href=&quot;https://aisearch.volcengine.com/&quot;&gt;https://aisearch.volcengine.com/&lt;/a&gt;&quot;)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助 OpenViking，上下文不再是散落一地的拼图，而是一个层次分明、井然有序的认知系统。它能够实现上下文的分层供给，在保障信息完整性的前提下，将 Token 成本降至最低；它提供协同写入与自我迭代机制，让 Agent 的“知识”与“经验”在与世界的交互中持续成长，开发者可以像管理本地文件一样构建 Agent 的大脑：&lt;/p&gt;&lt;p&gt;文件系统管理范式 → 解决碎片化问题：基于文件系统范式，将记忆、资源、技能进行统一上下文管理；分层上下文按需加载 → 降低 Token 消耗：L0/L1/L2 三层结构，按需加载，大幅节省成本；目录递归检索 → 提升检索效果：支持原生文件系统检索方式，融合目录定位与语义搜索，实现递归式精准上下文获取；可视化检索轨迹 → 上下文可观测：支持可视化目录检索轨迹，让用户能够清晰观测问题根源并指导检索逻辑优化；会话自动管理 → 上下文自迭代：自动压缩对话中的内容、资源引用、工具调用等信息，提取长期记忆，让 Agent 越用越聪明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，让我们一起深入了解 OpenViking，看看它如何挣脱上下文的枷锁，助您在 AI Agent 的浪潮中扬帆远航。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenViking 核心理念&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的设计哲学围绕四大核心理念构建，旨在将复杂的上下文管理流程化繁为简，让开发者能将宝贵的精力聚焦于业务创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件系统管理范式&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们不再将上下文视为扁平的文本切片，而是将其统一抽象并组织于一个虚拟文件系统中。无论是记忆、资源还是能力，都会被映射到 viking:// 协议下的虚拟目录，拥有唯一的 URI。这种范式赋予了 Agent 前所未有的上下文操控能力，使其能像开发者一样，通过 list、find 等标准指令来精确、确定性地定位、浏览和操作信息，让上下文的管理从模糊的语义匹配演变为直观、可追溯的“文件操作”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b42ef3713735258b68e46ae36a6966d.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分层上下文按需加载&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将海量上下文一次性塞入提示词，不仅成本高昂，更容易超出模型窗口并引入噪声。OpenViking 借鉴业界前沿实践，在上下文写入时便自动将其处理为三个层级：&lt;/p&gt;&lt;p&gt;L0 (摘要)：一句话概括，用于快速判断；L1 (概述)：包含核心信息和使用场景，供 Agent 在规划阶段进行决策；L2 (详情)：完整的原始数据，供 Agent 在确有必要时深入读取。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的设计使其能够灵活适配各类 AI Agent 的开发场景。无论是简单的问答机器人，还是复杂的自动化工作流，它都能作为坚实的上下文底座，提供稳定、高效的支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/233691194a98df4a4e5b062739a1aa5f.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目录递归检索&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单一的向量检索难以应对复杂的查询意图。OpenViking 设计了一套创新的目录递归检索策略，它深度融合了多种检索方式的优点：首先，通过意图分析生成多个检索条件；然后，利用向量检索快速定位初始切片所在的高分目录；接着，在该目录下进行二次检索，并将高分结果更新至候选集合；若目录下仍存在子目录，则逐层递归重复上述二次检索步骤；最终，拿到最相关上下文返回。这种 “先锁定高分目录、再精细探索内容” 的策略，不仅能找到语义最匹配的片段，更能理解信息所在的完整语境，从而提升检索的全局性与准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07d9fae2660f2de5fbcd088baf15c82f.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可观测与自迭代&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的组织方式采用层次化虚拟文件系统结构，所有上下文均以统一格式整合且每个条目对应唯一 URI（如 viking:// 路径），打破传统扁平黑箱式管理模式，层次分明易于理解；同时检索过程采用目录递归策略，每次检索的目录浏览、文件定位轨迹均被完整留存，能够清晰观测问题根源并指导检索逻辑优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，OpenViking 内置了记忆自迭代闭环。在每次会话结束时，通过 session.commit() 主动触发，系统会异步分析任务执行结果与用户反馈，并自动更新至 User 和 Agent 的 /memory 目录下。既能更新用户偏好相关记忆，使 Agent 回应更贴合用户需求，又能从任务执行经验中提取操作技巧、工具使用经验等核心内容，助力后续任务高效决策实现自我进化，让 Agent 在与世界的交互中“越用越聪明”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54ff1092e4ab16f7f76d0610ae07e288.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快速上手：三分钟运行 OpenViking&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的一大核心优势是其极简的集成方式。我们深知开发者的宝贵时间不应浪费在繁琐的配置上。您无需部署复杂的服务或学习新的 DSL，只需通过几行 Python 代码，即可为您的 Agent 装上强大的“上下文大脑”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下示例是以OpenViking的Readme英文版作为文件进行写入，展示处理后的上下文目录结构，以及对应文档的分层信息，并进行简单问题的回复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一步：安装 OpenViking&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;pip install openviking&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二步：获取模型服务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 需要VLM模型（用于多模态内容理解）和Embedding 模型（用于向量化）能力的API Key：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们支持多种模型服务：&lt;/p&gt;&lt;p&gt;OpenAI 模型：支持 GPT-4V 等 VLM 模型和 OpenAI Embedding 模型；火山引擎（豆包模型）：推荐使用，成本低、性能好，新用户有免费额度。如需购买和开通，请参考：火山引擎购买指南&amp;nbsp;&lt;a href=&quot;https://github.com/volcengine/OpenViking/blob/main/docs/zh/configuration/volcengine-purchase-guide.md&quot;&gt;https://github.com/volcengine/OpenViking/blob/main/docs/zh/configuration/volcengine-purchase-guide.md&lt;/a&gt;&quot;；其他自定义模型服务：支持兼容 OpenAI API 格式的模型服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三步：配置环境&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建配置文件 ov.conf：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;⚠️ 重要提示：请将下方配置中的 &lt;your-volcengine-api-key&gt; 替换为你在第二步获取的真实 API Key！&lt;/your-volcengine-api-key&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;vlm&quot;: {
    &quot;api_key&quot;: &quot;&lt;your-api-key&gt;&quot;,      // 模型服务的 API 密钥
    &quot;model&quot;: &quot;&lt;model-name&gt;&quot;,          // VLM 模型名称（如 doubao-seed-1-8-251228 或 gpt-4-vision-preview）
    &quot;api_base&quot;: &quot;&lt;api-endpoint&gt;&quot;,     // API 服务端点地址（如volcengine api：https://ark.cn-beijing.volces.com/api/v3）
    &quot;backend&quot;: &quot;&lt;backend-type&gt;&quot;       // 后端类型（volcengine 或 openai）
  },
&quot;embedding&quot;: {
    &quot;dense&quot;: {
      &quot;backend&quot;: &quot;&lt;backend-type&gt;&quot;,    // 后端类型（volcengine 或 openai）
      &quot;api_key&quot;: &quot;&lt;your-api-key&gt;&quot;,    // 模型服务的 API 密钥
      &quot;model&quot;: &quot;&lt;model-name&gt;&quot;,        // Embedding 模型名称（如 doubao-embedding-vision-250615 或 text-embedding-3-large）
      &quot;api_base&quot;: &quot;&lt;api-endpoint&gt;&quot;,   // API 服务端点地址（如volcengine api：https://ark.cn-beijing.volces.com/api/v3）
      &quot;dimension&quot;: 1024                // 向量维度
    }
  }
}&lt;/api-endpoint&gt;&lt;/model-name&gt;&lt;/your-api-key&gt;&lt;/backend-type&gt;&lt;/backend-type&gt;&lt;/api-endpoint&gt;&lt;/model-name&gt;&lt;/your-api-key&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并设置环境变量：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;export OPENVIKING_CONFIG_FILE=ov.conf&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四步：运行体验&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建简单的 Python 脚本&amp;nbsp;example.py&amp;nbsp;并运行，通过写入 OpenViking README 文档来体验写入-检索-读取的全过程：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;import openviking as ov

# Initialize OpenViking client with data directory
client = ov.SyncOpenViking(path=&quot;./data&quot;)

try:
    # Initialize the client
    client.initialize()

    # Add resource (supports URL, file, or directory)
    add_result = client.add_resource(
        path=&quot;https://raw.githubusercontent.com/volcengine/OpenViking/refs/heads/main/README.md&quot;
    )
    root_uri = add_result[&#39;root_uri&#39;]

    # Explore the resource tree structure
    ls_result = client.ls(root_uri)
    print(f&quot;Directory structure:\n{ls_result}\n&quot;)

    # Use glob to find markdown files
    glob_result = client.glob(pattern=&quot;**/*.md&quot;, uri=root_uri)
    if glob_result[&#39;matches&#39;]:
        content = client.read(glob_result[&#39;matches&#39;][0])
        print(f&quot;Content preview: {content[:200]}...\n&quot;)

    # Wait for semantic processing to complete
    print(&quot;Wait for semantic processing...&quot;)
    client.wait_processed()

    # Get abstract and overview of the resource
    abstract = client.abstract(root_uri)
    overview = client.overview(root_uri)
    print(f&quot;Abstract:\n{abstract}\n\nOverview:\n{overview}\n&quot;)

    # Perform semantic search
    results = client.find(&quot;what is openviking&quot;, target_uri=root_uri)# Input query
    print(&quot;Search results:&quot;)
    for r in results.resources:
        print(f&quot;  {r.uri} (score: {r.score:.4f})&quot;)

    # Close the client
    client.close()

except Exception as e:
    print(f&quot;Error: {e}&quot;)&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运行脚本：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;python example.py&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若您得到符合预期的答案，恭喜！你已成功运行 OpenViking 🎉&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开源共建，定义下一代 Agent 上下文标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们坚信，开放与协作是推动技术创新的核心动力。将 OpenViking 开源，是我们回馈社区、并与全球开发者共同探索 AI Agent 未来的第一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅仅是一次代码的分享，更是一次理念的传播。我们希望通过 OpenViking，能够为业界提供一个关于 Agent 上下文管理的全新范式，一个能够有效降低开发门槛、激发业务创新的坚实底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们深知，OpenViking 目前还处于早期阶段，有许多需要完善和探索的地方。但这正是开源的魅力所在——它允许我们汇聚最广泛的智慧，应对最前沿的挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此，我们诚挚地邀请每一位对 AI Agent 技术充满热情的开发者：&lt;/p&gt;&lt;p&gt;访问我们的 GitHub 仓库&amp;nbsp;https://github.com/volcengine/OpenViking，为我们点亮一颗宝贵的 Star，给予我们前行的动力；访问我们的网站&amp;nbsp;https://openviking.ai（点击阅读原文可跳转），了解我们传递的理念，并通过文档使用它，在您的项目中感受它带来的改变，并向我们反馈最真实的体验；扫描下方二维码加入我们的社区，分享您的洞见，帮助解答他人的疑问，共同营造一个开放、互助的技术氛围；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca87410e691ab52e9400095eef323868.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成为我们的贡献者，无论是提交一个 Bug 修复，还是贡献一个新功能，您的每一行代码都将是 OpenViking 成长的重要基石。 让我们一起，共同定义和构建 AI Agent 上下文管理的未来。旅程已经开始，期待您的加入！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;关于我们：字节跳动 Viking 团队&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们用 C 端产品的体验标准打造能够重塑企业生产力的产品和技术。在上下文工程领域具有深厚的技术积累与商业化实践，我们的愿景是提供用户友好的上下文工程产品矩阵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的产品历程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年：VikingDB 向量数据库支撑字节内部全业务大规模使用；2023 年：VikingDB 在火山引擎公有云售卖；2024 年：推出面向开发者的产品矩阵：VikingDB 向量数据库、Viking 知识库、Viking 记忆库；2025 年：打造 AI 搜索、vaka 知识助手等上层应用产品；2025 年 10 月：开源 MineContext&amp;nbsp;https://github.com/volcengine/MineContext，主动式 AI 应用探索；2026 年 1 月：开源&amp;nbsp;OpenViking，为 AI Agent 提供底层上下文数据库支撑。&lt;/p&gt;</description><link>https://www.infoq.cn/article/9dHvlfEaJXUmT3QiVC7D</link><guid isPermaLink="false">https://www.infoq.cn/article/9dHvlfEaJXUmT3QiVC7D</guid><pubDate>Thu, 05 Feb 2026 07:45:48 GMT</pubDate><author>OpenViking 团队</author><category>AI&amp;大模型</category><category>数据库</category></item><item><title>Cursor 浏览器翻车后，这个团队做出AI规模化高可靠软件工厂</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Part 1 ：前言：Cursor 用AI做浏览器翻车&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，人工智能编程正经历一场深刻而关键的转型，技术发展路径的分野日益显著。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不久前，技术圈被一则消息引爆：Cursor 联合创始人 Wilson Lin 高调宣布：「用 AI Agent 从零构建浏览器，一周生成 300 万行代码」。然而，这一雄心勃勃的尝试最终以失败告终：生成的代码无法编译，模块之间缺乏基本的接口协调，系统架构严重缺失，功能实现几近于零，被全网嘲讽为「AI 泔水」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这场闹剧并非终点，当 Cursor 的“软件工厂”梦碎时，一支中国团队采取不同的技术路线，悄然用 AI 实现了以往不可能的任务：使用一门新编程语言在10 天内生成了一个商业级别的 C 编译器，性能接近行业标杆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从外部视角审视，这或许并不止于“AI 写了一个编译器”，而在于它展示了一种相对稳定、可持续的“用 AI 构建软件”的方式。换句话说重要的不是一次性生成的结果，而是一条可以自举、可以回归、可以持续优化的工程曲线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果这种路径并非偶然，而是可以被系统性复制的，那它背后那套可复用的工程机制构建起的AI自动流水线生产的软件工厂，对整个软件工程领域都具有相当大的意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Part 2 ：用 AI 合成一个 C 编译器（技术实现过程）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 团队是国内AI编程语言领域的顶尖力量，也是国内唯一具有工业级语言与工具链快速落地能力的团队（世界范围有谷歌，微软、苹果等）。团队由 IDEA研究院首席科学家张宏波领导，他们打造的MoonBit语言专为AI与云原生等场景设计，支持多后端编译，性能卓越。目前，MoonBit 已应用于清华、北大等高校课程，并获海外云服务商采用，核心用户超10万+，目前库近 4千个，按照增长速度推测26 年底将有数万个库，届时生态将与苹果的Swift持平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们观察到 MoonBit 不仅在国内积累了大量用户，而且已经在海外得到广泛响应，特别是日本技术社区和X（推特）上不断刷新出大量关于 MoonBit 的技术内容。GitHub 上也有众多开发者在贡献生态库，有位日本技术大V评价：「一旦人们意识到 MoonBit 的价值，他们就会蜂拥而至」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/10df1cbc247baa3f21b9c67a91fd6d60.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近 MoonBit 团队公开表示在 「AI 软件工厂」上有突破性进展，现在 MoonBit 「 AI 软件工厂」展示出可以高效复刻大型软件的可能性，并且实现的质量更好，可靠性更高。值得一提的是，这并非一次性代码生成能力，而是一种可重复、可验证的软件生产流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;得益于大模型的迅速发展，AI 生产软件的速度和质量大幅度提升，一个标准3.5万行代码的大型软件的生产速度从过去百天到一年左右提升到目前 10 天以内。我们现在有理由相信未来大多数软件将通过自动化流水线的软件工厂生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但整个生产流程中的几个关键节点的跨越并不轻松，分别是 60% 节点、90%节点。以Cursor 生成的浏览器为例就是完成了 60 %，但在后续迈向 90% 时失败。原因在于 Cursor 对于编程语言掌控力、AI 原生工具链和测试等多方面能力的缺失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9eeab494c17b9f9f4c667fbaaec82673.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;软件工厂生产软件发展趋势&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;以 C 编译器为例的生产过程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自 MoonBit 团队的真实软件生产案例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其他「MoonBit AI 软件工厂」公开展示的示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PDF 工具：https://github.com/moonbitlang/mbtpdfwasm编译器：https://github.com/Milky2018/wasmoonjavascript：https://github.com/Lampese/NocturneJSd2ang：https://github.com/moonbit-community/diago...&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们设定了一个极具挑战的目标：从零开始构建一个 C 编译器 。最初的目的是探索一下AI的能力边界，尝试让 AI 在几乎0 干预的情况下，自己完成一个大型软件项目。传统观念认为从零开始构建一个完全符合规范的 C 编译器是一项高难度任务，涉及词法分析、语法解析、语义检查、优化和代码生成等多个复杂环节，需要深厚的编译原理知识和对硬件架构的理解，通常需数月甚至数年才能完成。整个过程像一部科幻小说。我戴上耳机，开启语音模式，对 AI 下达指令：“从零构建一个 C 编译器，贴近tcc，支持 arm64 架构。”之所以选择 tcc 作为示例是因为它是世界上最快的C编译器,，编译速度本身对 MoonBit 的开发体验尤为重要。且Native 后端同时支持 LLVM 和 C，C 后端如果有自己的编译器的话，可以实现完全自举。而且 tcc 不安全，缺乏维护，有优化替代空间。为了快速验证，我们只让 AI 支持 arm64 架构。在第七天的时候，它就已经实现了自举，这里需要解释下自举，先使用 moon工具链构建 Fastcc.mbt（项目名称），生成 Fastcc.exe，再用 Fastcc.exe 去编译 Fastcc.mbt 自身代码经过 moon 工具链生成的 C 代码，生成Fastcc1.exe，最后用 Fastcc1.exe 去执行 Fastcc.mbt 本身的测试，验证正确性。也能够编译 tcc 的源码，我们使用 v.c（vlang 编译器的单个 c 文件 snapshot）用以测试编译性能，当时和 tcc 的 gap 是 60x（也就是说 Fastcc.mbt 比 tcc 慢 60x）。一直到第十天，我几乎很少使用键盘。Agent 自主分解任务：先设计 AST（抽象语法树），生成基础模块；再用多 Pass 方案优化性能，而非照搬 tcc 的单 Pass 结构——尽管提示词要求“贴近 tcc”，但 AI 选择了更可靠的路径。每天工作的间隙，我会抽空看看 AI 的进度，偶尔需要做一些纠偏和指示：AI 自主使用 lldb 调试定位 Bug，在指示下调用 Xcode 命令行工具做性能分析，自己写脚本识别热点代码并针对性优化。第七天，惊喜发生——编译器成功自举：先用 MoonBit 工具链生成 Fastcc.exe，再用它编译自身代码，验证通过测试。整个过程中，AI 像一个不知疲倦的优秀程序员团队，在 MoonBit 的生态里流畅运作。最终，10天，3.5 万行代码由 Agent 生成，可读性极高。值得一提的是这并非偶然，而是 MoonBit 软件工厂工具链及语言设计产生的确定性结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;「MoonBit 软件工厂」下一步最自然的演进，是把已经跑通的工程流程固化下来，变成一套可以反复调用的软件生产能力。一旦这种能力稳定存在，它就不再局限于编译器，而是可以扩展到更多软件类别——从基础库、工具链组件，到更贴近业务侧的系统。当这样的产能开始规模化之后，或许将开启一个新时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Part 3 ：从 AI 写代码到“软件工厂”（技术架构解读）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 把软件完成率从 60 % 提升到 100% 的原因主要有以下几点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;1、语言设计&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 语言确立了“AI原生”的核心理念，摒弃传统编程语言中为人类习惯服务，但对AI造成理解负担的复杂语法结构，如嵌套作用域、隐式类型转换与重载机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其采用“平坦化”语法设计，具备极简的语法规则、高度清晰的语义表达与强大的静态类型系统，所有语言特性均经过AI 可理解性与生成友好性的系统评估，确保模型在推理过程中不会因歧义而产生错误。这种设计显著降低了大模型在语义解析、上下文推断与代码生成过程中的歧义成本，极大提升了生成结果的准确性、一致性与可预测性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，语言层面内置了对 AI 反馈机制的支持，如类型提示注入、错误定位标记与自然语言注释映射，使得自然语言需 求能够被高效、准确地转化为可执行代码，大幅度提高了“意图到代码”的转化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9c/9c035be03cbbb9f5bd5771d6e65d84e9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;MoonBit 运行性能与 Go 和 Swift 持平，甚至在某些场景下优于 Go 和 Swift。在公开的基准测试中，MoonBit 的编译速度快于 Rust 的10到100倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相应的就是 MoonBit 软件工厂的反馈速度极快，在 AI 生产软件的场景下，对比以往人类编写代码对于编译速度的需求有了指数级提升，AI 一天可以跑上千次的编译，此时编译速度变得异常重要，MoonBit 软件工程的优势也愈发明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2、AI 安全重构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在软件工厂生产或重构软件时，MoonBit 工具链不会让 AI 盲目随意地修改代码，而是为 Agent 提供了一套可调用、可验证的重构基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;moon ide是一个面向 AI Agent 的 IDE 工具，覆盖定义跳转、引用查找、重命名、结构分析和文档查询等能力。这些接口不是“给人点的功能”，而是以稳定、可解析的命令行协议直接暴露给 Agent 使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以其中一个功能 rename为例，moon ide rename 不会生成模糊的文本替换结果，而是直接输出 符合 OpenAI Codex apply_patch 规范的结构化补丁。换句话说，重命名不再依赖模型猜测上下文，而是由工具链给出确定的修改范围和精确的变更结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这带来几个直接收益：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;重构基于语义和符号表，而不是字符串匹配修改边界清晰，不会引入结构性漂移每一次变更都可以立刻进入编译、测试和静态分析流程验证&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/69/692070c7d103e40bce742595e1e210aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统 AI 编程工具的工作路径，本质上还是围绕人类开发者转的。人写提示词，模型生成代码，IDE 把结果展示出来，再由人决定改哪里、跑什么测试、要不要提交。看起来自动化了，其实反馈回路仍然是“人 → 界面 → 模型 → 人”，节奏慢、信息损耗大，也很难真正形成闭环。这种模式下，AI 更像一个助手，而不是工程系统的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;「MoonBit 软件工厂」理念是不再假设中间一定要有一个“给人看的 IDE 层”，而是把理解代码、查结构、跑测试的能力，直接暴露成可以被程序化调用的接口。换句话说，AI 面对的不是一堆 UI 按钮，而是一套可以直接对话的工程系统。这种交互关系一旦成立，节奏就会完全变样：反馈不再是“等人点一下”，而是“改完立刻验证”；决策不再是“要不要继续写”，而是“这次修改有没有通过约束”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3、工具链&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整套工具链沿用 「AI 原生」理念，专为 Agent 优化设计——调试器、性能分析、覆盖率工具、测试框架全部可调用，反馈回路大幅度缩短，可靠性也相应提高，可避免低级错误。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个例子看，AI Agent 在编写 C编译器（Fastcc.mbt）的过程中可以直接调用调试器去定位错误，用性能分析工具去找热点，再用基准测试卡住回退。这听起来像普通工程流程，但关键在于：这一整套流程对 AI 是完全流畅可调用的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就解释了一个看起来有点反直觉的结果：在没有并发、全程只用一个 codex agent 的情况下，项目依然能在十天里从“能跑”推进到“可优化”，速度比 clang - O0 快四倍左右，这里真正决定速度的，其实不是生成吞吐，而是验证反馈回路的长度。每一轮修改，都要经过编译测试、反复验证。这种节奏，更像是在推进一条软件工厂的流水生产线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4、QuickCheck&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QuickCheck 是开创性的具体实现，2000 年由 Koen Claessen 和 John Hughes 为 Haskell 开发。它首次将&quot;自动生成随机测试数据来验证程序属性&quot;这个想法变成了实用工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Property-Based Testing 是 QuickCheck 所代表的测试方法论的通用名称。核心思想是：你声明代码应该满足的&quot;属性&quot;（比如 reverse(reverse(list)) == list），测试框架自动生成大量随机输入来尝试反驳这个属性。这个术语现在用来指代所有采用这种方法的测试，不限于 Haskell 或 QuickCheck 本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Fuzz Testing（模糊测试） 是一个更宽泛、历史更久的概念，起源于1980年代末的安全测试领域。它的核心是向程序投喂随机或半随机的输入，观察是否会崩溃或出现异常行为。传统 fuzzing 不一定有明确的&quot;属性&quot;定义，往往只是看程序会不会挂掉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;助力软件完成率从 90% 到 100% 的就是 Fuzz Testing 和 Property Based Testing ，Cursor 那类“生成速度很快但不可控”的失败，本质上不是“AI 不会写”，而是缺少把结果持续拉回正确轨道的质量约束。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 软件工厂之所以能把项目从“能跑”推进到“可用、可维护、可优化”，关键就在于把质量校验做成了可自动执行的门禁，其中最有效的一类就是 QuickCheck / Property-based Testing（性质测试）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统单元测试更像“举例子”：我给你 10 个输入，期待 10 个输出。其覆盖面相当有限，也容易被 AI 的“看起来对”骗过去 (hacking) 。性质测试则更像“写规则”：不去枚举样例，而是声明程序必须永远满足的性质（property / invariant），然后让测试框架自动生成海量随机输入去“撞墙”。一旦撞出反例，框架还会自动 shrink（缩减） 反例，把复杂失败用例缩到最小、最容易复现和定位的那一个，这对 Agent 来说非常关键：它拿到的不是含糊的“某处错了”，而是一个可重放、可最小化、可稳定回归的失败证据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方法在编译器、PDF 和表格（Excel）这类系统里尤其有效，因为它们天然存在大量“结构等价 / 语义不变 / 往返一致”的可验证性质：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译器：同一段 C 代码，换不同编译器跑，结果应该一致；做了“优化”，只允许变快，不允许把答案变掉。PDF/文档工具：文件“打开→保存→再打开”，内容和排版不应该突然变形或丢东西。表格/Excel：公式计算结果稳定；保存加载前后语义一致；依赖关系不应出错（比如不该出现自相矛盾的循环依赖）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种测试会迫使 AI 使它不再靠“自信输出”赌正确，而是被迫在可验证的约束系统里迭代。每一次修改都要过编译、过测试、过性质校验；每一次性能优化都要在不破坏性质的前提下推进，因此系统更加能够在验证过程中不断趋近真正的可靠软件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;5、First Class Reasoning&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3ae7cba88585440107ad363742a48e49.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 在语言层面原生支持形式化推理能力，这是 AI 软件工厂中确保代码正确性的另一道重要防线。&lt;/p&gt;&lt;p&gt;具体而言，MoonBit 允许开发者（或 AI）为循环标注循环不变式（Loop Invariant），并支持编写 semi-formal 的证明过程。这一设计有两个关键特点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可执行的规约：循环不变式本身是合法的 MoonBit 代码，而非孤立的注释或外部标注。在 debug 模式下，这些不变式会作为运行时断言被动态检查——一旦违反，立即报错；而在 release 模式下，这些检查会被自动擦除，不影响生产环境的性能。这种&quot;写一次，两种用途&quot;的设计，既保证了开发阶段的严格验证，又避免了运行时开销。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 可验证的证明：semi-formal 的证明过程不要求完全的形式化证明（那对 AI 和人类都是巨大负担），而是一种结构化的推理步骤描述。这些证明可以借助 AI 工具进行检查和补全——AI 既可以根据代码自动生成候选的不变式和证明草稿，也可以验证人类或 AI 编写的证明是否自洽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计对 AI 软件工厂的意义在于：它把&quot;代码正确性&quot;从模糊的直觉判断，变成了可检查、可迭代的工程约束。当 AI 生成一段带循环的关键代码时，不再只能依赖测试用例碰运气，而是可以通过不变式和证明过程，从逻辑层面确认代码的行为符合预期。这在编译器这类对正确性要求极高的软件中尤为重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Part 4 : 总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 目前支持三种后端，分别是 WebAssembly (Wasm)、JavaScript (JS) 和 Native，特别是在 WASM 上MoonBit 优势明显，拥有最成熟的模块，性能优异，可以将软件工厂生产的大型软件移植到浏览器中高效运行。且自带沙箱，设计上集成了基于 Wasm 的隔离运行环境，对于开发者 或 AI 应用使用者，都可以在不牺牲安全性的前提下，快速部署和测试代码，很适合构建可信的 AI 辅助开发环境或边缘计算场景。（前文提到的 C 编译器还展示了 Web 版本：https://moonbit-community.github.io/fastcc/ ）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoonBit 正在推动软件工程从“人工编码”迈向“自动化工厂”的新时代：人类角色将转向需求定义与关键决策，而AI则在严谨的工程框架下完成构建与迭代。随着生态快速扩张，MoonBit 不仅会是中国在AI编程语言领域的重大突破，更有希望重塑全球软件生产的底层范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ 联合 MoonBit 发起大型软件合成挑战赛 ：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;赛事以“AI 原生软件工厂”为核心理念，基于「 MoonBit 软件工厂」探索在大模型与MoonBit编程语言及工具链协同条件下，如何将复杂软件的开发过程，从依赖个人经验的一次性实现，逐步转变为可复用、可演进、可持续的软件工程流程。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KAvjp4x5XULUL6ooEa8Y</link><guid isPermaLink="false">https://www.infoq.cn/article/KAvjp4x5XULUL6ooEa8Y</guid><pubDate>Thu, 05 Feb 2026 06:37:12 GMT</pubDate><author>InfoQ编辑部</author><category>企业动态</category><category>行业深度</category><category>AI 工程化</category></item><item><title>配置一改就要重启的时代结束了：Dev Proxy 2.1 正式上线</title><description>&lt;p&gt;Dev Proxy 团队近日发布了&amp;nbsp;&lt;a href=&quot;https://devblogs.microsoft.com/microsoft365dev/dev-proxy-v2-1-with-configuration-hot-reload-and-stdio-proxying/&quot;&gt;Dev Proxy 2.1&lt;/a&gt;&quot;&amp;nbsp;版本。此次更新围绕开发效率和本地测试体验展开，重点提升了迭代速度，并进一步增强了对现代开发工具的支持，同时对代理核心能力及相关工具链进行了多项优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dev Proxy v2.1 的一项重要新增功能是&lt;a href=&quot;https://devblogs.microsoft.com/microsoft365dev/dev-proxy-v2-1-with-configuration-hot-reload-and-stdio-proxying/&quot;&gt;配置热重载&lt;/a&gt;&quot;（configuration hot reload）。代理进程现在会自动监听配置文件的变化，并在保存后自动重启，无需开发者手动操作。维护者表示，这一能力回应了社区长期以来的呼声，目的是让开发者在不中断开发流程的情况下，更轻松地调整诸如模拟响应、错误率、插件配置或 URL 匹配规则等设置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个关键特性是&lt;a href=&quot;https://devblogs.microsoft.com/microsoft365dev/dev-proxy-v2-1-with-configuration-hot-reload-and-stdio-proxying/#stdio-traffic-proxying&quot;&gt;stdio 流量代理&lt;/a&gt;&quot;（stdio traffic proxying）。Dev Proxy 现已支持拦截、检查和模拟标准输入、标准输出以及标准错误流。这一能力主要面向通过 stdio 进行通信的工具，包括 Model Context Protocol（MCP）服务器及类似的开发工具。通过将可执行程序包装在 Dev Proxy 之下，开发者可以在熟悉的浏览器调试工具中查看 stdio 流量、模拟延迟，并返回模拟响应。这一扩展使 Dev Proxy 不再局限于基于 HTTP 的使用场景，也让更多类型的应用能够被测试和调试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在配置和性能方面，Dev Proxy v2.1 还新增了一个用于配置 API 端口的&lt;a href=&quot;https://devblogs.microsoft.com/microsoft365dev/dev-proxy-v2-1-with-configuration-hot-reload-and-stdio-proxying/#new---api-port-command-line-option&quot;&gt;命令行参数&lt;/a&gt;&quot;。开发者现在可以在启动代理时直接指定端口，从而更方便地运行多个实例或规避端口冲突。此外，LatencyPlugin 已更新，支持设置超过 10 秒的延迟，使长时间请求和超时处理的测试更加贴近真实场景。Web API 现已支持跨域请求（CORS），使基于浏览器的工具能够无障碍地与代理进行交互。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次版本还修复并改进了多项细节问题，包括环境变量处理、API Key 校验、HTTP 方法匹配、内容类型处理，以及数据文件的重新加载行为等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除核心代理外，&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=garrytrinder.dev-proxy-toolkit&quot;&gt;Dev Proxy Toolkit 1.12&lt;/a&gt;&quot;&amp;nbsp;版本也同步发布，为 Visual Studio Code 带来了更严格的配置校验、更清晰的诊断信息、更完善的快速修复建议，以及更强的插件支持能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为补充说明，Dev Proxy 此前已发布&amp;nbsp;&lt;a href=&quot;https://devblogs.microsoft.com/microsoft365dev/dev-proxy-v2-0-with-improved-ai-telemetry-and-small-breaking-changes/-command-line-option&quot;&gt;2.0 版本&lt;/a&gt;&quot;。由于包含一些虽小但重要的重要变更，该版本进行了主版本号升级。Dev Proxy 2.0 引入了对 .NET 10 的支持，改进了 AI 相关遥测能力（包括更精确的 token 统计），并对日期格式和遥测行为进行了调整，以提升整体准确性和可靠性。同时，该版本也对 API 模拟行为进行了多项修复和优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从本次更新内容来看，Dev Proxy 2.1 主要围绕提升迭代效率，并补齐了对 AI 与 stdio 工具链的支持。相关的完整发布说明和变更日志，可在官方文档及&lt;a href=&quot;https://github.com/dotnet/dev-proxy&quot;&gt;项目代码&lt;/a&gt;&quot;仓库中查阅。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/dev-proxy-v2-1-release/&quot;&gt;https://www.infoq.com/news/2026/02/dev-proxy-v2-1-release/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/DhyfQVuh5lMeULuXNPIY</link><guid isPermaLink="false">https://www.infoq.cn/article/DhyfQVuh5lMeULuXNPIY</guid><pubDate>Thu, 05 Feb 2026 06:13:29 GMT</pubDate><author>作者：Almir Vuk</author><category>架构</category><category>云计算</category></item><item><title>请停止为人类开发软件！2026年最大的机会，是给Agent“造基建”</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;编者按拥有 32 年编程经验的行业老兵杨攀，围绕「AI Coding in 2026」分享了极具前瞻性的核心洞察。他提出，燃烧 Token 是衡量 AI 原生应用的关键标准，2026 年 Token 消耗或将迎来百倍增长。在他看来，AI Coding 虽能让个人效率提升 10 倍，却受限于组织协作瓶颈难以发挥更大价值。2026 年的最大机会，是停止为人类开发软件，转而聚焦为 Agent 打造专属基础设施。他同时强调，要完成从将 AI 视为工具到劳动力的认知转变，在生产力富足的时代，个人更需提升筛选能力、聚焦核心实践，构建独立思考逻辑。以下为杨攀演讲原文。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/63/63e1cc968d9a29f7637857b5400e67b0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;大家好！我直观感受到2026年1月发生的变化强度，大约相当于过去25年的六个月，发生了太多大事件。以Clawdbot为例，一周内竟然能三次更名。这种剧烈变化让我们必须思考其底层逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;价值度量变革：燃烧 Token 的时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 原生应用的新定义&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去我思考 AI 原生应用时，主要关注产品的商业逻辑、业务逻辑和交互逻辑。2026年1月，我有了全新的认知：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其判断标准很明确：只有通过燃烧 Token 来解决问题的应用才是AI原生应用。无论是处理输入、生成输出还是执行求解任务，都需要消耗 Token。应用对 Token 的依赖程度越高，就越纯粹地属于 AI 原生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c97e3d2f6338043e5b8734c51e27d94b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;目前我们依然有 AI 排行榜，传统的排行榜主要依据日活和流量进行排名。而AI 时代的排行榜真正应该排名的是 Token 消耗量。哪个业务消耗的 Token 更多，哪个就应该排在前面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Token 消耗本质上体现了一种权利：拥有更多 Token 消耗能力，就意味着拥有更大的决策权和影响力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e9e45588d1260fc429b08958519800e3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;100 倍增长的预测&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;去年的国内和国际市场，均存在大量计算资源闲置的现象。但据我观察，2026 年将持续呈现供不应求的态势，这意味着提前购入就是获利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于 2026 年 Token 消耗的增长倍数，市场预期各不相同：有人认为增长 10 倍，有人预估 20 倍、50 倍，甚至更激进的预测。&amp;nbsp;我的判断是，如果资源充足，100 倍增长是一个合理预期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8b/8be6b9294a7364731d4978e895ef2570.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;如果整个产业提升 100 倍，作为个体，我们需要思考自己一年内的 Token 消耗能否同样实现 100 倍增长？ 如果无法跟上这一趋势，就会明显落后于整体发展水平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这引发了一个值得深思的问题。当前许多开发者坐在电脑前通过敲击 Prompt 的方式进行编程。其实这里有一个关键认知：Token 消耗的真正瓶颈实际上在于坐在电脑屏幕前的操作者本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;操作者需要为 AI 下达任务指令，AI 执行过程中需要不断确认是否继续以及具体操作方式，这成为了效率瓶颈。如果操作者能够给出完整任务让 AI 自主执行时，AI 就能持续消耗 Token 并产生产出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/46/46d4ae99b497c613537919cc56088930.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工程范式转移：人机协作到Agent直连&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;个人提效 10 倍，组织提效远低于个人&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数开发者和构建者都认为，AI Coding 能够让个人生产效率提升十倍。这一结论在个体层面得到了广泛验证。然而通过对众多组织的深入调研发现，AI 在组织层面产生的效率提升倍数远低于个人层面。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/87/878d3b93e59b8aef48a48a194434ce30.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;根本原因在于个体与组织层面的效率差异：&lt;/p&gt;&lt;p&gt;当个人独立工作时，作为需求的提出者和问题的解决者，所有思考、沟通、讨论都局限在个人的思考范围内，这种模式非常高效当需要团队协作时，人与人之间的沟通速率和信息交换效率显著降低，且不同人的思维模式存在差异，还需要额外的时间进行认知对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种组织层面的协作瓶颈导致 AI 效能提升面临重大挑战。回顾软件发展历史，我们可以看到瀑布开发方法论和敏捷开发方法论在不同阶段的演进。在 AI 时代，我们尚未找到适合 AI 特点的软件工程方法论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;停止为人类开发软件！为Agent造基建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;或许大家还未意识到的是，AI 在许多领域的生产力已经超越人类。以 Neon 云端数据库为例，2025年2月由 Agent 创建的数据库数量已经超过人类管理员创建的数量，这在云服务市场已经形成共识。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/febee1919de3014a36369969576dacfb.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;2025年8月，我提出了一个颠覆性观点：从2025年开始，我们应当停止为人类开发软件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前大量 Vibe Coding 产品仍然面向人类用户，但值得深思的是：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当Agent 能够自主完成更多任务，包括直接访问数据库和调用接口时，我们是否还需要通过人类界面来实现这些功能？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前众多 UI Automation 工具确实令人印象深刻，RPA 的自动化能力也备受推崇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更值得深思的是：为什么让 Agent 去调用为人类设计的界面和基础设施？这种方式效率极其低下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 应该直接访问所有数据和 API 接口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e2/e269b3280f21e1d2000ebb67b8c39884.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为 Agent 构建基础设施的巨大机会&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从另一个极端角度来找核心机遇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以移动互联网为例：全球 80 亿人口中，约 60 亿是移动互联网用户，每人每天使用 APP 的点击次数有限。在 AI 时代，如果每人拥有 100 个、1000 个 Agent，每个 Agent 每天调用接口和访问数据的频率将远超人类使用手机的频率。这一指数级增长的乘数效应将创造巨大的市场规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年的Claude Code 和 Manus 在做什么？持续提升大模型能力和为 AI 构建强大的中枢神经系统。Agent 能力在 2025 年取得了显著突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么在 2026 年 Token 消耗预期增长 100 倍的背景下，最大的发展机遇是为 AI 构建大规模的基础设施，包括完善的运行环境、API 接口和数据访问能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7d/7d1577ec41f1512ae0102af77a2938aa.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI Coding 的三个心得&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/aed34f3b2b0e33e9728aee7117df0645.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;始终选择最先进且性能最优的模型，无论价格如何，效率回报最为显著。在现有模型能力边界内，任务规划与需求分析仍然是关键环节。Agent 的自我验证能力至关重要。只要系统具备自我验证功能，就能够按照既定目标执行任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;生存策略重塑：从工具到劳动力思维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 认知转变：从工具到劳动力的思维重构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;认知层面的转变至关重要。过去三年自 ChatGPT 发布以来，大多数人仍将 AI 视为工具，主要关注其提升工作效率的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋在去年的发布会上明确表示：AI is work not tool。AI 不是工具，而是劳动力。AI 可以被委托执行任务，并交付具体结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你将 AI 视为工具，它提供工具价值；如果你将 AI 视为劳动力，它提供劳动力价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心差异在于资源管理能力：有些人一天能够消耗上亿 Token，而有些人只能消耗百万 Token。这体现了AI 时代的领导力：个人能够管理的 AI Work 数量、每日工作产出和 Token 消耗水平存在巨大差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/21/210d3206755e83bba78eca906348f42c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生产力富足时代的商业逻辑与价值交付&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，要思考一个关键问题：在生产力极度富足的未来，会发生什么？几乎所有人都能产出 80 分水平的产品且生产成本趋近于零。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内容生产的数量将呈现指数级爆炸增长，我们的注意力也在指数级地分散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去有好产品很容易被发现，但今天即使你做出了 80 分水准的作品，被发现的概率也极低。&amp;nbsp;因此今天拥有品牌、流量、渠道将具备极大的优势。这也是为什么 KOL 等具备影响力的人群具有如此高的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c32afee3dc7dae60d87eb729c51a7a1c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一方面，AI 时代“交付结果”的重要性为何日益凸显？我们需要思考什么变化导致了这一概念变得如此关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以工具使用为例：如果给你木头、锯子、锤子、钉子，你也许能够制造出一个凳子。但如果你需要制造一个人工按摩椅或大型沙发，难度就会大幅提升。购买工具后，自己使用工具可以创造一个结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而事物复杂度持续攀升。当它达到临界点时，单纯购买工具已无法获得理想结果，此时“购买结果”而非“购买工具”成为更优选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我认为今天强调“交付结果”的核心问题在于事物的复杂性在增加。而你提供的价值在于将复杂问题内化到你的服务、产品和能力中，将复杂事务转交给别人处理才能体现你的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 时代的个人成长与策略选择&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，品味很重要。其本质上是一种筛选能力。在同质化严重的 80 分产品环境中，独特的品味能够识别和突出优质作品，实现精准的目标用户推送。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1f/1f5dc71226a3525aabd9bd2568841d40.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，2026 年值得深思的是，AI 赋予我们如此强大的能力，我们究竟应该用它做什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前 AI Coding 重度用户的典型现象是，许多程序员因 AI Coding 带来的强烈多巴胺刺激而废寝忘食，甚至放弃其他个人爱好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我把它称之为“程序员垃圾时间”：许多 AI 程序员在缺乏商业价值或具体成果的项目上投入大量时间，纯粹为了获得心理满足感。我们更应该深度思考我们究竟应该用它做什么：在 2026 年，我们应该用 AI Coding 拿到什么结果？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9d/9ddfa064bdc175bfaebcce2073104499.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，我们也面临着前所未有的知识爆炸，我建议的核心策略包括：&lt;/p&gt;&lt;p&gt;构建个人知识图谱，建立结构化认知体系，明确新信息在体系中的位置和相互关系聚焦核心概念而非全面细节对感兴趣领域深度实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;看似矛盾的“放弃细节”与“重点实践”体现了有取有舍的智慧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/29/29712e220e30f8c63a61eb328c7cafd3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;成为 Builder：AI 时代的核心能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Follow builders not influencers”这一观点在近期备受关注。我认为，优秀的 influencer 必须在具备 builder 身份的基础上，才能提供真正有价值的洞察。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;希望各位不要被工具绑架，要构建独立的思考逻辑。我们不应试图预测未来，而应深入理解事物发展的趋势和方向。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54e53e3e988f6924ffb2e78b54afa608.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;具体的技术更新（如某个大模型能力增强、某个特定问题得到解决）并非关键所在。真正重要的是理解事物发展的底层逻辑——对发展趋势的深入思考和认知，以及把握发展节奏的能力，而非仅仅关注技术细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结语：拥抱 AI 时代的到来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我一直在思考“人间一日，AI 一年”这句话。从 ChatGPT 发布之初，我就坚持一个观点：ChatGPT 发布后五年，我们将迎来通用人工智能。当然，不同类型的人工智能之间仍存在差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cf2fafa18a05c12866d92f6e632b5c02.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，让我用一句话结尾：请珍惜与身边所爱的人在一起的时光。因为五年后会发生什么，我们无从知晓。无论是人类社会还是地球本身，都将发生我们无法预测的深刻变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d3/d3f25be35b9e2f4dcb70787a534c2358.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;作者介绍&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨攀，硅基流动联合创始人， 写了代码32年，主要做即时通讯，做过微软 MSN、中国移动飞信，现在做AI云服务。过去十年服务了绝大多数创业者和大型企业，做过三个10亿级注册用户的产品。&lt;/p&gt;</description><link>https://www.infoq.cn/article/ujMux2sHwXIXYC4z0Rd0</link><guid isPermaLink="false">https://www.infoq.cn/article/ujMux2sHwXIXYC4z0Rd0</guid><pubDate>Thu, 05 Feb 2026 03:51:58 GMT</pubDate><author>杨攀</author><category>AI&amp;大模型</category></item><item><title>谷歌 DeepMind 推出 ATLAS 多语言语言模型缩放定律</title><description>&lt;p&gt;谷歌 DeepMind 的研究人员提出了 &lt;a href=&quot;https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models/&quot;&gt;ATLAS&lt;/a&gt;&quot;，这是一套针对多语言语言模型的缩放定律，用于形式化描述随着支持语言数量的增加，模型规模、训练数据量以及语言组合之间如何相互作用。该研究基于 774 次受控训练实验，模型参数规模从 1000 万到 80 亿不等，使用覆盖 400 多种语言的多语言训练数据，并在 48 种目标语言上评估模型性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现有的大多数缩放定律主要来源于仅使用英语或单一语言的训练设置，因此对于多语言训练的模型只能提供有限的指导。ATLAS 在此基础上进行了扩展，显式建模了跨语言迁移以及由多语言训练引入的效率权衡。该框架不再假设新增语言会产生统一影响，而是估计在训练过程中，单个语言如何促进或干扰其他语言的性能表现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ATLAS 的核心是一种跨语言迁移矩阵，用于衡量在一种语言上训练对另一种语言性能的影响。分析结果表明，正向迁移与共享书写系统和语言家族高度相关。例如，斯堪的纳维亚语言之间表现出明显的相互增益，而马来语和印尼语则构成了一个高迁移效率的语言对。英语、法语和西班牙语则表现为广泛有益的源语言，这很可能与其数据规模和多样性有关，但这种迁移效应并不具备对称性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c772ab08009563cf15217e16eb619aa6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ATLAS 通过将训练语言数量与模型规模和数据量一并纳入建模，扩展了传统的缩放定律。它量化了所谓的“多语言诅咒”：在模型容量固定的情况下，随着支持语言数量的增加，每种语言的性能会下降。实验结果表明，在保持性能不变的前提下，若语言数量翻倍，模型规模需增加约 1.18 倍，总训练数据量需增加约 1.66 倍；而正向的跨语言迁移可以在一定程度上抵消单语言数据减少所带来的性能下降。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该研究还分析了在不同条件下，是从零开始预训练一个多语言模型，还是在已有多语言模型检查点上进行微调更为有效。结果显示，在较低 token 预算下，微调在计算效率上更具优势；而当训练数据和计算资源超过某一与语言相关的阈值后，从头预训练反而更有利。对于 20 亿参数规模的模型，这一拐点通常出现在约 1440 亿到 2830 亿 token 之间，为根据可用资源选择训练策略提供了实用参考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布也引发了关于替代模型架构的讨论。一位 X 用户&lt;a href=&quot;https://x.com/broadfield_dev/status/2016286110658502806?s=20&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;与其训练一个在每种语言上都使用大量冗余数据的超大模型，一个纯粹用于翻译的模型需要多大规模？这样又能让基础模型缩小多少？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管 ATLAS 并未直接回答这一问题，但其提供的迁移测量结果和缩放规则，为探索模块化或专用化的多语言模型设计奠定了定量基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/01/google-deepmind-atlas/&lt;/p&gt;</description><link>https://www.infoq.cn/article/uBpTeu8eUhg4H7hDSkT2</link><guid isPermaLink="false">https://www.infoq.cn/article/uBpTeu8eUhg4H7hDSkT2</guid><pubDate>Thu, 05 Feb 2026 00:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>北京朝阳区首个 OPC 创业社区“极客部落 · AI 应用生态园”正式亮相 ITEC</title><description>&lt;p&gt;北京，2026 年 2 月 4 日—— 在今日举行的 “ITEC 第十三届朝阳国际人才创业大会创新峰会” 上，极客邦科技创始人兼 CEO 霍太稳正式发布了北京市朝阳区首个面向 OPC（One-Person Company，一人公司）的创业社区——“极客部落 · AI 应用生态园”。该项目由朝阳区人才工作局、共青团朝阳区委员会、望京街道联合指导，由极客邦科技作为链主企业主导日常运营，其旗下的全新品牌 “模力工场” 作为 AI 创业生态加速的专业品牌，为各场景与地域的AI应用项目，尤其开发者主导的 OPC 项目，提供线上线下协同支持——线上拓展 AI 原生社区，线下联合政府与园区打造 OPC 专属加速空间，旨在打造面向 AI 技术人才与创业者的 “AI 应用生态 + 青年友好型创新社区 + OPC 轻量化 AI 创业加速器”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;助力朝阳建设 “科技百园”，打造全国 AI 生态枢纽&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“极客部落 · AI 应用生态园” 坐落于朝阳望京，是朝阳区落实 “十五五” 人工智能产业战略、加快构建全栈人工智能通用底座及应用生态的关键落子。园区将通过建设 “青年人才会客厅”、投融资对接、场景需求测试及商机对接等在内的一系列人工智能＋创新创业主题活动，结合细分场景需求和早期创业团队的成长发展需求，构建集技术研发、孵化、社群、企业服务为一体的全龄友好型创新社区，致力于成为朝阳区最具吸引力的 AI 人才策源地，以及全国领先的 AI 生态枢纽。&lt;/p&gt;&lt;p&gt;霍太稳表示：“未来的经济活力，不只来自几家巨头，更来自成千上万家活跃的一人公司。我们建设 ‘极客部落’，就是要在这片热土上，帮这些未来的超级个体——把家安好，把根扎深，把火聚起来&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 时代催生 “一人公司”，OPC 成为创新创业新力量&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，随着大模型技术的普及与 AI 工具的发展，具备技术能力和创新思维的个体创业者正成为推动 AI 应用落地的新兴力量。霍太稳在发布会上指出：“在 AI 时代，一个懂技术、懂工具的极客，一个人就是一支队伍。他们是新物种，也是新的就业群体。”然而，这些 “超级个体” 在创业过程中往往面临场景缺失、伙伴难寻、生态孤立的挑战。“极客部落” 应运而生，致力于为 AI OPC 打造一个 “家”，让 “一个人的公司，不止一个人”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;三大支撑体系，构建 OPC 成长闭环&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“极客部落” 以场景、伙伴、生态为核心支撑，构建全方位的 OPC 成长赋能体系：&lt;/p&gt;&lt;p&gt;真实场景训练场：推动 AI 技术从概念走向实践，为企业、园区、街道提供可复制、可落地的 AI 应用样板，帮助 OPC 创业者 “拿结果” 而非 “学概念”。协作网络连接器：依托极客邦科技深厚的国内外社区连接能力，汇聚朝阳区辐射全国甚至全球的各类创新创业AI生态资源，助力OPC创业加速发展。生态赋能母舰：整合政府政策支持、青年组织活力、国际化街区氛围，构建从 “个体” 到 “生态” 的成长土壤，打造未来商业模式的孵化器。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/49/c7/4958d4e886de0600e0181ca2290421c7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;聚火成光，点亮 AI 创业星河&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布环节最后，霍太稳以 “聚是一团火，散是满天星” 寄语所有 AI 创业者，强调 “极客部落” 正是那个 “聚火” 的地方——汇聚人才、场景与生态，助力每一位心怀梦想的OPC创业者，在 AI 时代勇敢开创属于自己的 “一人公司时代”。&lt;/p&gt;&lt;p&gt;即日起，“极客部落 · AI 应用生态园” 正式面向全球 AI 极客、OPC 创业者开放入驻申请，期待与每一位技术创造者携手，共同推动人工智能从 “单点改善” 向 “全产业链场景应用” 跨越，助力 AI Native 企业在朝阳茁壮成长。如您希望加入极客部落，获取一人公司创业支持，可扫码联系模力工场运营助理。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/474468d85accc2b2366159e0c8ecc15a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;关于极客邦科技&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客邦科技，以“推动数智人才全面发展，助力数智中国早日实现”为己任。依托独特的行业专家生态网络和优质内容生产方法论，为数智化组织和人才提供发展必备的媒体资讯、开发者社区、行业峰会、高管社群、企业培训、线上课程、行业咨询等全方位服务。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/83330134895ffd2d82528124f1680cda.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/jWGGDOdeN4HOszmFholC</link><guid isPermaLink="false">https://www.infoq.cn/article/jWGGDOdeN4HOszmFholC</guid><pubDate>Wed, 04 Feb 2026 12:50:48 GMT</pubDate><author>极客邦科技</author><category>AI 工程化</category><category>团队搭建</category></item><item><title>2026年中国企业AI人才与组织发展报告</title><description>&lt;p&gt;在大模型能力持续跃升与智能体技术加速成熟的双重推动下，AI 正在从实验室走向产业现场，从单点工具进化为企业运行的新基础设施。2025 年，我们看到智能体在金融、制造、能源、互联网等行业开始批量落地，token 消耗量指数级增长、应用场景不断丰富、政策支持持续加码，企业对 AI 的认知也从技术探索转向业务重塑。与此同时，AI 对组织、流程与人才的冲击愈发明显：岗位边界被打破，传统管理模式受到挑战，“超级员工” 与人机协同成为新的工作范式。&lt;br&gt;
站在 2026 年的起点，企业侧的 AI 应用正处于从试点验证迈向规模化价值兑现的关&lt;br&gt;
键转折点。如何构建适配 AI 时代的组织架构？如何培养能与 AI 协同的新型人才？如何让 AI 真正融入业务流程并产生可量化的价值？这些问题已成为企业数字化转型的核心命题。&lt;br&gt;
基于大量调研、访谈与案例研究，本报告系统梳理了 2025 年企业级 AI 落地的整体&lt;br&gt;
进展，总结了智能体规模化应用的关键趋势，并对“十五五”时期企业 AI 战略规划、组织变革与人才发展路径提出了前瞻性判断。我们希望通过这份报告，为企业在 AI 时代的组织升级与人才建设提供可落地的参考框架，帮助更多组织在智能化浪潮中把握先机、构建持续竞争力。&lt;/p&gt;
&lt;h3&gt;目录&lt;/h3&gt;
&lt;p&gt;第一篇 2025 年企业 AI 应用现状&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人才结构：AI 核心人才占比偏低，内训成为主渠道…06&lt;/li&gt;
&lt;li&gt;AI 项目落地范式：周期更短、团队更小、AI 生成代码大规模普及…09&lt;/li&gt;
&lt;li&gt;企业应用 AI 进展：企业进入“规模化验证期”…09&lt;/li&gt;
&lt;li&gt;2026 年技术趋势：智能体成为企业 AI 落地的核心… 11&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第二篇 智能体成为企业应用 AI 的主要抓手&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;技术突破与成本下降，为智能体大规模商业化铺路 … 14&lt;/li&gt;
&lt;li&gt;生态逐步完善，显著降低智能体开发与应用门槛… 14&lt;/li&gt;
&lt;li&gt;政策指引与市场需求，双向促进智能体与产业应用深度融合 … 16&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第三篇 企业级 AI 技术落地效果不及预期&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;企业落地 AI 技术的范式 … 19&lt;/li&gt;
&lt;li&gt;AI 落地效果不及预期…20&lt;/li&gt;
&lt;li&gt;原因分析…20&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第四篇 AI 时代企业渴求超级员工&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;企业渴求超级员工，对现存员工岗位及职责造成冲击…23&lt;/li&gt;
&lt;li&gt;组织管理者的设想 …23&lt;/li&gt;
&lt;li&gt;我们的推论…23&lt;/li&gt;
&lt;li&gt;人才发展趋势预测…25&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第五篇 AI 时代人才粮仓模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前瞻规划 - AI 思维引导者…28&lt;/li&gt;
&lt;li&gt;业务骨干 - 智能体应用人才 …30&lt;/li&gt;
&lt;li&gt;中坚力量 - 智能体定义人才 … 31&lt;/li&gt;
&lt;li&gt;发展基石 - 大模型专项人才 …32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第六篇 AI 时代组织变革不可避免&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组织管理者的设想…35&lt;/li&gt;
&lt;li&gt;我们的推论 …35&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第七篇“十五五”规划下企业的 AI 前瞻规划&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;政策指引…38&lt;/li&gt;
&lt;li&gt;AI 大模型赋能底层原创技术突破 …38&lt;/li&gt;
&lt;li&gt;AI 大模型助力业务价值升级…39&lt;/li&gt;
&lt;li&gt;AI 大模型推动组织生态变革…42&lt;/li&gt;
&lt;li&gt;AI 大模型助力现代化产业体系建设…43&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第八篇 企业 AI 落地与人才实践案例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安克创新：智能制造行业 AI 落地的全链路智能化样本 …46&lt;/li&gt;
&lt;li&gt;新奥泛能：能源行业 AI 落地的场景驱动实践…47&lt;/li&gt;
&lt;li&gt;鞍钢：钢铁行业 AI 落地的数据驱动实践…47&lt;/li&gt;
&lt;li&gt;平安壹钱包：金融领域 AI 落地的全场景智能化样本 …48&lt;/li&gt;
&lt;li&gt;阿里云：科技企业 AI 全链路赋能业务实践样本 …49&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结尾&lt;/p&gt;
</description><link>https://www.infoq.cn/article/UZTN39WZ81MhFteW9uDW</link><guid isPermaLink="false">https://www.infoq.cn/article/UZTN39WZ81MhFteW9uDW</guid><pubDate>Wed, 04 Feb 2026 12:30:42 GMT</pubDate><author>极客时间企业版</author><category>AI&amp;大模型</category><category>数字人才培养</category></item><item><title>拼模型、拼向量库的时代结束了？MongoDB 正在重写 AI 检索的基础设施</title><description>&lt;p&gt;MongoDB 近日宣布，在 MongoDB Atlas 上正式推出 Embedding 与&amp;nbsp;&lt;a href=&quot;https://www.mongodb.com/products/updates/now-in-public-preview-embedding-and-reranking-api-on-mongodb-atlas/&quot;&gt;Reranking API&lt;/a&gt;&quot;&amp;nbsp;的公开预览版本。通过这一新 API，开发者可以在托管云数据库中直接调用 Voyage AI 的搜索模型，在一个统一的集成环境中构建语义搜索、AI 助手等功能，同时实现监控与计费的统一管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一&lt;a href=&quot;https://www.mongodb.com/docs/voyageai/quickstart/&quot;&gt;方案&lt;/a&gt;&quot;将构建 AI 检索系统所需的关键组件整合在同一平台上。MongoDB 表示，该 API 具备数据库无关性，可集成到任何技术栈或数据库中，面向正在构建检索驱动型 AI 系统的团队，覆盖从语义搜索、RAG（检索增强生成）到 AI Agent 等多种场景。MongoDB 高级技术产品营销经理&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/thibautgourdel/&quot;&gt;Thibaut Gourdel&lt;/a&gt;&quot;&amp;nbsp;与 MongoDB 资深产品经理&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/wenphan/&quot;&gt;Wen Phan&lt;/a&gt;&quot;&amp;nbsp;在文中写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;目前构建 AI 检索系统，往往需要把数据库、向量搜索和检索模型服务商“拼接”在一起，每一个环节都会引入额外的运维复杂度。为了解决这一问题，我们在 MongoDB Atlas 上推出了 Embedding 与 Reranking API。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://imgopt.infoq.com/fit-in/3000x4000/filters:quality%2885%29/filters:no_upscale%28%29/news/2026/02/mongodb-embedding-reranking-api/en/resources/1Embedding_and_Reranking_API_blog_image_2-1769105022975.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MongoDB Atlas 上的 Embedding 与 Reranking API&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;&lt;a href=&quot;https://www.mongodb.com/events/mongodb-local/san-francisco&quot;&gt;.local San Francisco&lt;/a&gt;&quot;&amp;nbsp;活动上发布的另一项重要内容是 Voyage 4 系列模型的&lt;a href=&quot;https://www.mongodb.com/products/updates/the-voyage-4-series-now-available/&quot;&gt;上线&lt;/a&gt;&quot;。该系列目前包含四种不同模型：voyage-4-large、voyage-4、voyage-4-lite，以及开源权重的 voyage-4-nano。与以往嵌入模型需要对查询和文档使用同一模型不同，Voyage 4 提供的文本嵌入模型运行在统一的向量空间中。这意味着，团队可以使用 voyage-4-large 存储数据，同时在查询阶段灵活使用任意 Voyage 4 模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，向量搜索的自动化嵌入功能已在社区版中开启预览，&lt;a href=&quot;https://www.mongodb.com/products/updates/now-in-public-preview-automated-embedding-in-vector-search-in-community-edition&quot;&gt;MongoDB Vector Search&lt;/a&gt;&quot;&amp;nbsp;的 Lexical Prefilters（词法预过滤）也已进入公开预览阶段，为开发者在向量搜索之外提供文本与地理分析过滤能力。对此，Deepak Goyal 在 LinkedIn 上&lt;a href=&quot;https://www.linkedin.com/posts/deepak-goyal-93805a17_mongodb-ugcPost-7417812930366242816-0w71?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAABaQ5R4B1z_TPIVzQKBvbJ9SpDn29zaiJcY&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我昨天花了 3 个小时排查我们向量存储中一个长达 12 小时的同步延迟问题。这几乎是每个 AI 团队现在都在缴纳的“同步税”。如果你的数据已经滞后 24 小时，那你的 RAG 就谈不上“智能”，它只是一个索引做得不错的档案库。统一数据流之后，趋势已经很清晰：专用向量数据库的角色，正越来越接近 AI 世界里的“外置 GPU”。性能固然出色，但在多数生产环境下，集成式方案在速度和复杂度控制上更占优势。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在模型能力方面，这些嵌入模型支持 256 到 2048 维度，并支持量化处理，使开发者能够在准确率、成本与性能之间进行权衡。除了通用模型外，Voyage 还提供面向特定领域、整文档分析、多模态数据，以及多阶段搜索系统中重排序（reranking）的专用模型选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gourdel 与 Phan 强调，尽管 MongoDB Atlas 已经内置了向量搜索能力，但新 API 的核心价值在于进一步降低复杂度：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这对于构建生产级 AI 系统至关重要。扩展 LLM 应用的关键，在于在恰当的时间提供正确的上下文，而这意味着必须将业务数据与高性能搜索进行紧密集成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自 MongoDB 在近一年前宣布收购 Voyage AI 以来，社区便一直在&lt;a href=&quot;https://www.reddit.com/r/mongodb/comments/1jg8f9i/timeline_for_mongodbvoyage_integration_auto/&quot;&gt;期待并讨论&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;https://www.mongodb.com/company/blog/news/redefining-database-ai-why-mongodb-acquired-voyage-ai&quot;&gt;Voyage AI&lt;/a&gt;&quot;&amp;nbsp;能力与 MongoDB Atlas 的深度整合。此次 Embedding 与 Reranking API 的推出，也被视为这一整合方向的正式落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，Embedding 与 Reranking API 仍处于预览阶段。MongoDB 同时提供了 “Voyage AI Quick Start” 教程，形式为 GitHub 上的&amp;nbsp;&lt;a href=&quot;https://github.com/mongodb/docs-notebooks/blob/main/voyageai/notebooks/quickstart.ipynb&quot;&gt;Python Notebook&lt;/a&gt;&quot;，供开发者快速上手体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/mongodb-embedding-reranking-api/&quot;&gt;https://www.infoq.com/news/2026/02/mongodb-embedding-reranking-api/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Kb8YQvKgQkYazceGUQFK</link><guid isPermaLink="false">https://www.infoq.cn/article/Kb8YQvKgQkYazceGUQFK</guid><pubDate>Wed, 04 Feb 2026 12:03:52 GMT</pubDate><author>作者：Renato Losio</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>当AI吞噬软件，数据正在成为企业唯一的护城河</title><description>&lt;p&gt;作者｜关涛、苏郡城&lt;/p&gt;&lt;p&gt;审校｜李文朋&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;编者按：近日编者获悉，国内领先的数据平台公司“云器科技”完成B轮融资，其聚焦在亚洲市场，产品战略对标Databricks。随AI持续火热，全球数据基础设施市场也正经历一场范式转移。本文将对比国内外数据领域技术发展，深度拆解AI时代数据平台必须要完成的进化之路。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;导语：当大模型成为通用商品，资金正疯狂涌向唯一的非标资产——数据&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年初，全球科技界正经历一场前所未有的范式转移。AI三要素（算法、算力、数据）中，算法与算力正在快速商品化。算法层面，大模型加速标准化，逐步成为通用的“超级大脑”；算力层面，AI数据中心的规模化建设使算力供给日益充足。二者获取门槛大幅降低，但也日趋同质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;全球具备基础模型研发能力的企业不超过10家，AI芯片厂商更是屈指可数。对绝大多数企业而言，其私有高质量数据正在成为企业竞争力唯一的护城河。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;资本市场已率先捕捉到这一趋势，AI数据基础设施成为投资热点。一个标志性事件是，在一级市场中，Databricks估值约增长2.7倍；ClickHouse估值约增长3倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;资本市场对Databricks和类似技术栈的追捧，本质上是对“Data + AI”这一轮新增长飞轮的押注，数据作为核心生产要素的地位已无可撼动。但现实是，大多数企业的数据体系没准备好迎接AI，没有做到基础设施的AI就绪（AI-Ready）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去二十年，企业建设了数据中台、数仓和治理体系，但在AI真正落地时发现，许多数据资产“用不上”。根本原因在于，传统数据平台是为SQL设计的，擅长处理Filter（过滤）、Aggregation（聚合）、Join（连接）等确定性计算，数据必须结构化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但企业80%以上的数据是文档、音视频、聊天记录、会议纪要等“非结构化数据”。这些数据长期躺在各个系统中，被称为“暗数据”（Dark Data）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更关键的是访问模式的改变。人类分析师习惯于看日报、周报，容忍T+1的数据延迟，且查询模式多为“全量扫描”后的聚合指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而Agent的访问模式完全不同：它们可能在秒级发起成千上万次查询，要求毫秒级的响应，且查询方式多为基于语义的“精准检索”（Vector Search）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种高频、低延迟、基于语义的机器交互需求，彻底击穿了传统Lambda架构的性能与成本底线。如果沿用老架构，每一次Agent的思考都可能触发昂贵的全表扫描，导致算力成本指数级上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、当前数据基建支持AI就绪的两个结构性障碍&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;企业这些年在数据建设上投入不少，数据中台、数仓、治理体系都搭了，但许多数据资产“缺失”“用不上”“用不好”的问题，主要出在两个地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;1.1架构的熵增：Lambda架构的“一致性难题”是通向AI实时决策的巨额债务，且注定无法解决。&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年，为了同时支持实时和离线，行业普遍采用Lambda架构：批处理一套，流处理一套。这一选择由彼时的业务需求与技术条件共同决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lambda架构的数据平台受到“数据不可能三角”限制——你无法同时获得数据的实时性、低成本和高查询性能；只能三者取其二。通常，批处理面向成本和复杂查询优化，流处理面向解决实时性优化，两套系统各司其职。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/34/347919b402df7f20ba9af4122fa9b38b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图：典型的Lambda架构）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;痼疾也很明显，如两套系统的数据很难对齐。同一个指标，批处理通过复杂的ETL处理和计算形成的指标，与流计算不一定对得上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以说Lambda架构下的“数据一致性”基本是美好愿望，需要巨大的运维成本，潜在制约了数据业务整合和发展。另外还有维护成本高，运维复杂等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BI时代这个问题勉强能忍，但AI时代忍不了了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统数据库扫描一张结构化数据表，成本可能几分钱；同样的数据如果送给大模型做推理，成本可能几百块，差距在10万倍量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;且Agent要求新数据尽快就绪可召回，因此AI时代要求引擎同时满足数据不可能三角的三个顶点（新鲜度、低成本、Readiness）。这意味着“有问题就全量重跑”的兜底方案彻底失效——你必须精确知道哪些数据变了，只处理增量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但Lambda架构的数据平台，天然做不到这一点。因为基于多套系统、多套逻辑、多套数据血缘。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;1.2范式不适配：AI的原料与计算模式均与传统数据平台迥异&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI需要的原料是文档、音视频等“非结构化数据”，这些占了企业数据的80%以上，且包含大量有价值Context信息，我们称他们为“暗数据”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的业务know-how——客户是怎么想的、项目是怎么推进的、决策是怎么做出的——大部分都藏在一个模糊的非结构化数据为核心编织的数据网络里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，这些数据的价值只能靠数据科学家人工去挖掘。现在，AI第一次提供了规模化处理这些数据的可能性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在的数据库/数仓/数据平台是为结构化数据和关系模型设计的。却不擅长处理文档、音视频。这是处理非结构化数据（AI的主要原料）时的范式缺失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些缺失是结构性和根本性的，是从底层的处理硬件开始（GPU vs CPU）、到存储系统、存储格式、数据管理、元数据系统到引擎算子的全技术栈缺失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、AI引入的三大范式变化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要打造AI时代的数据护城河，必须对底层架构进行彻底的范式重构，这集中体现在计算能力、数据形态与访问模式的三个维度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;2.1高阶计算能力：从关系代数到AI模型&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，数据库和数据平台只有一种引擎：结构化分析引擎，基于关系代数，符号化、确定性、低语境依赖。你给它一条SQL，它返回一个确定的结果，分毫不差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但AI引擎的特性完全不同：基于概率模型，模糊匹配、概率推断、高语境依赖。同一个问题问两遍可能得到不同答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但正因如此，它能做传统引擎做不到的事——理解、抽取、总结、推理、生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在经典的DIKW（数据-信息-知识-智慧）金字塔中，传统结构化引擎的能力边界在Information层——它能把数据加工成报表和指标，但无法告诉你这些指标“意味着什么”。AI引擎能深入到Knowledge层级，实现真正的语义理解和推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换个角度：如果把传统引擎类比为大脑顶叶（负责数学计算），AI引擎则对应前额叶皮层（负责高阶认知、规划、决策）。两者的关系是互补而非替代——二维关系计算交给传统引擎，总结、归纳及推等认知计算交给AI引擎。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/63/639dc1fb00cfb08fc606d13df65007f4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;2.2暗数据的解锁：Lakehouse下的多模态表达&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;⻓期以来，企业数据资产中超过80%都是⾮结构化或半结构化的“暗数据ˮ（Dark Data），如客⼾服务的录⾳、合同PDF⽂档、监控视频等。在传统数仓架构下，这些数据往往被丢弃或仅作为冷备份存储，⽆法参与核⼼业务计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b7b8e4bc37295ff504acff986652bd4e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lakehouse（湖仓一体）架构的普及为这些数据的存储提供了低成本方案，但通过AI对其进行深度解析才是关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过AI的多模态处理能力，能够自动解析、向量化并索引这些非结构化数据，将其转化为机器可理解的格式。这意味着企业可以首次全景式地利用其拥有的所有信息资源，而非仅仅通过那20%的结构化表格来决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;2.3访问模式转变：从Scan到Search&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI引擎有一个独特特性：上下文窗口极小（100万Token约等于4MB），但处理成本极高。1TB数据，AI引擎推理需要25万个窗口，总成本高达百万美元，同样的数据量大数据引擎处理成本在5美元以下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这带来访问模式的根本转变：从“全量扫描”转向“精准检索”。例如计算“过去一年的总销售额”。这需要扫描大量行数据。然而，AI Agent的典型访问模式完全不同：它们更多地进行“精准检索”（Point Lookup）或“语义搜索”（Vector Search），例如“找到与该投诉最相似的历史案例”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种从Scan到Search的转变，对底层存储引擎的索引结构、缓存策略和并发能力提出了全新的要求。RAG（检索增强生成）技术的兴起，本质上就是为了解决这一问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但RAG仅仅是检索环节，更重要的是如何构建一个高效、实时、低成本的AI处理平台，将非结构化数据转化为AI就绪（AI-Ready）的知识并存储在RAG中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、未来架构蓝图：AI原生数据平台的五个设计原则&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于上述变革，构建新一代数据护城河需要遵循五个核心原则，这些原则构成了AI原生数据平台的蓝图。Databricks、Snowflake以及国内云器科技等厂商，都在沿着这个方向演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;核心设计原则概览&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;•&amp;nbsp;原则一：Lakehouse统一存储。&amp;nbsp;一份数据，多种视图（Table/Vector/Graph），打破结构化与非结构化的边界。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;原则二：AI作为原生计算引擎。&amp;nbsp;AI能力内嵌至SQL，支持AI ETL与GPU统一调度。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;原则三：增量计算结合的奖牌架构。&amp;nbsp;抛弃Lambda架构，采用全链路增量（GIC）构建奖牌架构。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;原则四：Agent友好 的开发范式。&amp;nbsp;API First，自然语言交互，建立“执行-反馈”闭环。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;原则五：企业级能力。&amp;nbsp;细粒度权限治理，Serverless弹性伸缩，满足审计与合规需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;原则一：Lakehouse统一存储&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lakehouse的核心是用一套系统同时支持低成本存储和高效查询。但对AI原生平台来说，更关键的是它原生支持多种数据表达形态。同一份数据可以有多种表达，不同表达带来不同的能力边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47b045603eaeb76cfc7ade5df1a741cc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以一段客户反馈为例，同样的信息可以有不同的存储方式，假如：&lt;/p&gt;&lt;p&gt;•&amp;nbsp;存成原始文本：信息最完整，但检索效率低&lt;/p&gt;&lt;p&gt;•&amp;nbsp;抽取成结构化字段（情感倾向、产品类别、问题类型）：查询快、可聚合，但丢失了细节&lt;/p&gt;&lt;p&gt;•&amp;nbsp;转成向量：支持语义检索，能找到“意思相近”的内容&lt;/p&gt;&lt;p&gt;•&amp;nbsp;构建图关系：能表达客户、产品、问题之间的关联网络&lt;/p&gt;&lt;p&gt;不同形态有不同权衡。越靠近结构化，准确率越高、可解释性越强、处理成本越低；越靠近原始态，信息越丰富、灵活性越高，但成本也越高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个洞察是，AI的数据不应该独立建一套平台。它应该和结构化数据融合在一起，因为AI处理流程中有大量结构化计算的需求。把两者割裂开，反而会制造新的数据孤岛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子：你问AI “Meta 2021年的营收是多少”，如果只有原始文本，AI可能猜错单位（是百万还是十亿？美元还是其他货币？）。但如果结构化数据和语义层（Semantic Layer）结合，标注清楚revenue列的单位和口径，回答就会精确得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是为什么Lakehouse架构强调统一——不是简单地把数据堆在一起，而是让不同形态的数据能够协同工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;原则二：内生AI计算&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI能力必须内嵌到数据平台，成为SQL的一部分，而非通过API外挂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;海外头部厂商已经在这样做。Snowflake和Databricks都在SQL里加入了一系列AI算子，形成了相对完整的能力图谱：&lt;/p&gt;&lt;p&gt;•&amp;nbsp;AI_COMPLETE：文本补全和生成，比如根据上下文自动填充缺失字段&lt;/p&gt;&lt;p&gt;•&amp;nbsp;AI_EXTRACT：从非结构化文本中抽取结构化信息，比如从合同里提取关键条款&lt;/p&gt;&lt;p&gt;•&amp;nbsp;AI_FILTER：语义级别的过滤，比如筛选&quot;与某主题相关&quot;的内容&lt;/p&gt;&lt;p&gt;•&amp;nbsp;AI_AGGREGATE：对文本内容做聚合摘要，比如把100条客户反馈总结成3个要点&lt;/p&gt;&lt;p&gt;•&amp;nbsp;AI_CLASSIFY：分类打标，比如判断一段文本的情感倾向或主题类别&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些算子对应的底层能力，其实就是大模型的理解、抽取、生成、总结、推理。但封装成SQL算子之后，AI模型与数据结果的结合表达能力获得大幅提升，不需要搭LangChain，不需要懂Prompt Engineering，一条SQL搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d0/d0c3279b052803d13347218fa7e33659.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图：AI能力与SQL算子的融合，Snowflake Cortex AI）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个具体场景：金融分析师每天面对上万条新闻，传统做法要么人工筛选，要么写复杂的关键词规则（然后漏掉大量相关信息）。现在可以直接写：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果需要更精细的处理，还可以组合多个算子：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这才是真正的多模态计算——AI和SQL在同一个执行引擎里协同工作，而非简单的多模态召回。是在统一的数据governance的环境中做权限管理的AI数据处理，符合隐私合规；而且算子可组合，复杂逻辑也能表达。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;原则三：大奖牌架构与增量计算- “只计算变化的部分”&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统Lambda架构维护实时和离线两套代码，导致逻辑冗余且指标经常无法对齐。Databricks和微软2024年提出的Medallion Architecture（大奖牌架构）已成为AI+Data数据处理的标准模型。（Reference：&lt;a href=&quot;https://www.databricks.com/glossary/medallion-architecture&quot;&gt;Databricks：What is a medallion architecture?&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;https://dev.to/aawiegel/medallion-architecture-101-building-data-pipelines-that-dont-fall-apart-1gil&quot;&gt;Medallion Architecture 101: Building Data Pipelines That Don&#39;t Fall Apart&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个架构的核心思想是把数据处理分成三层，像炼矿一样逐级提纯：&lt;/p&gt;&lt;p&gt;Bronze层（铜）：存原始数据，越原始越好，不做任何加工。就像矿石——今天你炼铁，明天可能发现里面还有金子。原始数据不能丢，因为你不知道未来会需要什么。&lt;/p&gt;&lt;p&gt;Silver层（银）：做清洗、抽取、结构化。把非结构化数据转成可查询的格式，把脏数据清理掉，统一schema。这一层是数据质量的关键战场。&lt;/p&gt;&lt;p&gt;Gold层（金）：生成最终产出——报表、特征、指标，直接供业务和模型使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;并且，这个架构同时适用于结构化和非结构化数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c2/c24eb37b941fdd3b054e110c63ec20c4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;图：奖牌架构数据处理流程：结构化数据（上图）；非结构化数据（下图）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;奖牌架构是一套建模方法，它最终能跑起来，有一个前提：增量计算能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;奖牌架构有四个核心原则：灵活性（Flexibility）、数据质量管理（Data Quality Management）、成本效率（Cost Efficiency）、以及最关键的——增量ETL（Incremental ETL）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前三个相对直观，第四个是难点和核心。为什么？因为AI推理成本极高，“全量重跑”模式根本不可行。每次数据更新都从头算一遍，成本和延迟都无法接受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;奖牌架构本质上是一个Kappa架构——端到端的统一增量数据处理流程，不再区分流/批等传统计算模型。但这个架构能跑起来的前提是：必须有真正的增量计算能力。&lt;/p&gt;&lt;p&gt;AI推理成本决定了“全量重跑”不可行。通用增量计算（GIC）的核心思想是：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a46051d2a4678785088196bcb891c7b8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图：增量计算原理）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;只处理变化的部分，不重复计算已经算过的东西。这个方式并不像说的那样容易，需要从底层重新设计计算引擎：精确追踪数据的每一个变化，理解变化对下游计算的影响，只对需要更新的部分做增量处理。这涉及到存储格式、索引结构、执行计划、状态管理的全面重构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;理想的增量计算引擎能用一套系统Single-Engine同时支持实时和离线，同一套代码、同一份数据、同一个执行引擎。（增量计算白皮书--请参看附录）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;原则四：Agent友好的开发范式&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当软件使用者从人变成Agent，开发平台的设计范式也必须改变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去的数据开发平台，核心交互是GUI：拖拉拽建模、点选配置、根据监控调整。这对人很友好，但Agent并不需要点按钮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面向Agent的设计需要几个根本转变：&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;API First而非UI First。&amp;nbsp;Agent通过接口与系统交互，所有能力都必须API化。GUI变成可选的观测层，而非核心交互层。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;自然语言作为主要接口。&amp;nbsp;Agent用“交流”的方式检索和操作数据。NL2SQL不再是锦上添花的功能，而是核心能力。Agent可以在一次查询里融合文本、向量、图关系的检索结果，实现真正的多模态查询。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;反馈链路不可或缺。&amp;nbsp;AI是概率模型，有时对有时错。传统软件是确定性的——代码写对了就永远对。但AI系统需要持续校正，需要建立“执行→反馈→调整”的闭环机制，像机器学习训练一样不断迭代。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;自解释的语义层。&amp;nbsp;Agent需要理解数据的业务含义，而非只知道表名和字段名。这要求数据平台具备丰富的元数据和语义描述，让Agent能够自主理解&quot;revenue列的单位是什么&quot;&quot;这两个表之间是什么业务关系&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有一点需要清醒认识：短期内人不会完全退出，而且人与Agent的交互也同样关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI写的代码、做的决策仍需人来检查与审批。不管AI多强，&quot;因为是AI写的所以bug不算数&quot;这种逻辑并不成立。人的角色从&quot;开发者&quot;变成&quot;Reviewer+Observer&quot;——审批关键决策，监控系统运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的数据平台会是混合模式：Agent负责主要的开发和执行，人作为审批者和监控者。平台需要同时支持两种交互范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;原则五：企业级治理能力&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI原生时代，开源自建的ROI逻辑在改变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent大规模调用企业数据时，细粒度访问控制变得极其重要——财务报表、员工工资、客户隐私管理、严格的权限隔离、数据防泄露等企业级数据管理与治理能力。此外，AI的决策需要可追溯、可审计，在金融、医疗等强监管行业尤其关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力开源软件天然缺失，商业级托管平台天然具备。这也是为什么Databricks/Snowflake这一类商业平台受到包括OpenAI在内的新一代企业青睐的原因。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;路径选择：全球共识与中国式解法&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上述五个原则由云器科技总结提出，事实上全球头部厂商都在沿着这个方向演进，只是路径选择各有不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Databricks是这套范式的最佳践行者。从Spark起家，到推出Delta Lake实现湖仓一体，再到2024年系统性提出Medallion Architecture，它一直在引领Data+AI融合的技术方向。商业上，Databricks坚持云中立+托管化，不绑定任何一家云厂商，这让它能够服务于多云和混合云场景的企业客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake也是数据领域的先行者之一。它的底子是云原生数仓，强项在结构化数据的极致性能。面对AI浪潮，Snowflake选择通过收购和集成来补齐能力——Document AI处理非结构化数据，Cortex提供AI服务，Snowpark支持Python生态。路径不同，但方向一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，这两家公司都没有选择自研基础模型，而是专注于数据的价值挖掘。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中国市场有其特殊性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，国内云厂商的技术栈与海外存在较大差异；另一方面，企业对数据主权和合规性有更高要求。直接照搬海外方案并不现实，这给了本土厂商机会。云器科技&amp;nbsp;是目前国内最接近Databricks定位的公司。技术上，它基于Lakehouse + GIC实现了批流一体的架构重构；商业上，同样坚持云中立与全托管路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，云器科技的这一架构已在蚂蚁集团、小红书、快手等头部互联网公司的生产环境中得到了验证。这些场景往往具有极高的数据吞吐量和复杂的业务逻辑，能在这些苛刻环境中稳定运行，证明了该技术路径的成熟度与可替代性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6f/6f5cbca473653af2c17de8878911d9cd.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（表：Databricks与云器科技产品对比）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;编者按：&amp;nbsp;据悉，近期云器科技已完成B轮融资。资金将主要用于新一代AI数据基础平台的持续研发，进一步推动AI原生数据架构在本土市场的落地与普及。当前形势下，作为国内最接近Databricks定位的公司，云器的融资进展也反映出资本对亚太Data+AI基础设施赛道的持续看好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、终局：构建智能时代的数据壁垒&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从最宏观的视角看，数据平台的定位在AI时代正在发生根本变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关键事实：&lt;/p&gt;&lt;p&gt;•&amp;nbsp;用户主体变迁：&amp;nbsp;软件的主要使用者正在从人类（Human）加速转向智能体（Agent），要求数据接口具备更高频、低延迟的机器交互能力。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;架构痛点解决：&amp;nbsp;传统Lambda架构在即时性与准确性上难以兼得，且维护成本高昂；云器科技通过统一的流批一体与增量计算技术，彻底解决了数据一致性难题。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;暗数据价值释放：&amp;nbsp;针对企业内部大量存在的非结构化“暗数据”（文档、日志、多媒体），平台提供了原生的存储与计算支持，使其成为可被AI利用的高价值资产。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;计算模式革新：&amp;nbsp;从传统的全量扫描（Scanning）模式转向更高效的搜索（Searching）模式，大幅提升了RAG（检索增强生成）场景下的响应速度。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;技术路径融合：&amp;nbsp;采用Lakehouse架构作为数据底座，结合独创的GIC（增量计算）技术，实现了存储成本与计算效率的最优平衡。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;中国生态定位：&amp;nbsp;针对中国企业复杂的IT环境，云器科技提供云中立且具备完全托管能力的解决方案，填补了国内市场在高端AI数据基础设施上的空白&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去它是“被动响应的资产库”——业务系统产生数据，数据平台存起来，有人查就返回结果。未来它将成为“主动参与决策的智能实体”的底座，是企业AI的“记忆与知识库”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以想象这样的场景：Agent群在上面运行、学习、协作，数据平台在下面收集、计算、优化数据。与上层Agent形成互动。AI消费数据、理解数据、改写数据，数据再反过来塑造AI的行为与能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个循环迭代越快，系统的智能水平就越高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更宏观地看，AI+Data正在形成新的技术范式。未来的超级智能不会是孤立的模型，而是持续运转的系统——是数据+算力+模型的融合；它既使用知识，也创造知识。数据不再是被动存放的资源，而是不断加工、更新、进化的运行态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;承载这个循环的核心基础设施，必然是AI原生的数据平台。谁能更快完成从传统架构到AI原生的迁移，谁就更有机会在下一轮基础设施竞争中占据位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Reference&lt;/p&gt;&lt;p&gt;AI SQL Query Language：https://www.snowflake.com/en/blog/ai-sql-query-language/&lt;/p&gt;&lt;p&gt;奖牌模型Medallion Architecture: https://www.databricks.com/glossary/medallion-architecture&lt;/p&gt;&lt;p&gt;Medallion Architecture 101: Building Data Pipelines That Don&#39;t Fall Apart：https://dev.to/aawiegel/medallion-architecture-101-building-data-pipelines-that-dont-fall-apart-1gil&lt;/p&gt;&lt;p&gt;增量计算白皮书：https://www.yunqi.tech/resource/incremental-computation/reservation&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6HtKg4ajhkamjGkK3ODG</link><guid isPermaLink="false">https://www.infoq.cn/article/6HtKg4ajhkamjGkK3ODG</guid><pubDate>Wed, 04 Feb 2026 09:53:11 GMT</pubDate><author>关涛,苏郡城</author><category>企业动态</category><category>行业深度</category><category>数据湖仓</category><category>AI&amp;大模型</category></item><item><title>昆仑天工发布“Skywork桌面版”，Windows电脑能雇AI员工了？</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2月4日，昆仑天工面向全球正式发布「天工Skywork桌面版」，即桌面端应用Skywork Desktop。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，Skywork桌面版的设计宗旨是击穿AI生产力上限，将AI Agent推向下一时代。其直接在本地执行任务，无需上传文件到云端。它可以直接读取电脑上的海量文件，进行汇总、整理，并基于内容生成新产物。同时，它以“内容理解”为核心，而非“文件格式”：无论是图片、视频、表格、PPT 还是各类文档文件，都能在统一语义层下被理解、归类、执行任务，且支持多任务并行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年5月，Skywork面向全球发布“AI版Office” Skywork Super Agents（即Skywork网页版），极大提升了近亿用户的工作效率。如今，Skywork桌面版的正式推出，让AI Agent不再只是被动等待指令而是主动理解你的意图、随时响应你的工作需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方介绍，Skywork桌面版的产品目标是走进并融入桌面端最后一公里，成为每个人工作的OS助手。这意味着AI第一次真正走进桌面办公现场，从“等你喂材料”进化为“直接理解整个项目”，开始成为能看懂项目、理解上下文、主动干活的OS级同事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skywork桌面版让AI开始解决各种零散、复杂、少量使用场景的长尾办公问题，让工作效率全面升级。昆仑天工表示，Claude Cowork的问世验证了用户在工作场景下对于“工作任务执行”的巨大需求，OpenClaw（前身为Clawdbot、Moltbot）的出现更是反映了用户对于“本地电脑/桌面端执行操作”的真实、迫切渴望。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而当前，Skywork桌面版要做的，则是更智能、更强的高阶桌面AI。相比当前产品，Skywork桌面版首先实现了 Windows原生覆盖，满足了大量使用Windows的用户和工作场景需求，可以直接处理本地历史文件和复杂项目场景，无需迁移或适配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，相比Claude Cowork仅支持Claude模型，Skywork桌面版进化成为“Gemini版Claude Cowork”，不仅支持Claude 模型，还同时支持Gemini模型。用户可以在Claude Opus 4.5、Claude Sonnet 4.5和Gemini 3 Pro模型之间选择，亦可启用“auto”模式，由系统根据任务类型智能推荐最适合的模型，实现更高效、更精准的任务处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再者，为打通“思考”与“执行”的壁垒，Skywork桌面版内置集成了100+个经过精选的、真正有用Skills，涵盖Office三件套生成、网页生成、图片生成、视频生成等类型。系统可根据任务自动筛选并推荐最适合的Skills和模型，提高多任务处理效率，让用户操作更省心。用户亦可手动选择Skills和模型去解决多任务场景痛点，让操作更灵活。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了在产品初始配置上使用更先进模型和功能设计，Skywork桌面版在任务生成质量、处理速度和安全方面也做了优化提升。尤其在安全方面，Skywork桌面版所有操作均在本地虚拟机隔离环境中完成，无需上传文件到云端。所有任务基于本地文档生成内容，减少联网搜索或盲目推理导致的错误输出。&lt;/p&gt;</description><link>https://www.infoq.cn/article/AI8XjhV01eKAEbGGGTzT</link><guid isPermaLink="false">https://www.infoq.cn/article/AI8XjhV01eKAEbGGGTzT</guid><pubDate>Wed, 04 Feb 2026 08:57:44 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item></channel></rss>