<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 14 Jan 2026 15:04:59 GMT</lastBuildDate><ttl>5</ttl><item><title>Data+AI 新年特辑：2025 的顿悟时刻与 2026 的关键十问 | Q推荐</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，Data + AI 的讨论正在悄然发生变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的关注点，逐渐从模型能力本身，转向企业是否真正具备承载 AI 的系统能力：数据是否准备充分，工程体系是否稳定，AI 是否真的进入业务流程并长期运行。这些问题开始频繁出现在一线实践中，也成为企业在推进 Data + AI 过程中无法回避的现实考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的变化并非源于某一次集中发布，而是在一次次真实落地、反复试错和持续修正中逐步显现。也正因为如此，2025 成为了一个值得回望的年份，许多重要判断，往往产生于具体实践中的“顿悟时刻”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，InfoQ 联合 Snowflake 发起了&amp;nbsp;「MAKE IT SNOW｜2025–2026 Data + AI 年度时刻」&amp;nbsp;直播活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一场围绕企业 Data + AI 战略展开的年度复盘与前瞻对话。活动邀请来自数据平台、开源社区，以及制造、医疗、汽车等行业的一线技术与业务负责人，围炉而坐，如老友般对谈 。我们将共同回到真实的问题本身，剖析企业在推进 Data + AI 规模化过程中遇到的关键抉择 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;那个拨云见日的「Aha Moment」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每位嘉宾将回顾自己在 2025 年经历的&amp;nbsp;3 个关键认知转折点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能是一段产品体验、一次落地尝试，或是某个业务场景中的重新理解。正是这些具体经历，推动了对 Data + AI 的判断不断修正，也构成了企业能力演进的真实轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用「年度十问」对齐关键判断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“十问 Data Strategy，AI Strategy&amp;nbsp;”环节，问题覆盖数据底座与 AI 融合架构、Agentic AI 与可信 AI、多云时代的数据治理、平台整合浪潮下的生态协同，以及工业、医疗、汽车等行业的落地实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题没有预设答案，却直指企业当下面临的核心挑战，更接近真实决策场景中的思考方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;留待未来打开的「时间胶囊」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场直播的尾声，每位嘉宾将基于当下的判断，留下&amp;nbsp;一个关于 2026 的预测或猜想。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可能并不成熟，也未必已经被验证，更像是一种站在当下时刻，对下一年走势的直觉判断。这些判断不会被立即评判对错，而是被完整地保存下来，等到 2027 年，我们会再度打开它们，回看哪些判断被现实印证，又有哪些想法在时间中发生了意料之外的转向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一刻行业领袖们的技术直觉，将成为未来回望时的重要坐标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正在思考企业 Data Strategy 与 AI Strategy 的下一步，这场对话，值得关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 19 日 17:30-19:30，我们不见不散！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d9/d9ee9c01d61fcbfe596360456e9189e0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</link><guid isPermaLink="false">https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</guid><pubDate>Wed, 14 Jan 2026 10:49:19 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>辣评 AI编程工具： 是它们不行， 还是你不会玩？｜InfoQ 2025 年度盘点与趋势洞察</title><description>&lt;p&gt;年度盘点来啦！辣评AI编程工具。&lt;br&gt;
总出 bug？隐性成本拉满？是你没摸透AI编程工具内核，还是工具自己拉胯？三位资深AI Coding 专家详解如何把 AI 用成提效外挂！&lt;/p&gt;
&lt;p&gt;本期内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;说评国内外的Al coding 工具，哪个最受开发者青睐？&lt;/li&gt;
&lt;li&gt;Vibe Coding怎么交付成果？&lt;/li&gt;
&lt;li&gt;写代码时间正在被“和Al聊天”取代？&lt;/li&gt;
&lt;li&gt;计费模式变变变，开发者怎么省钱？&lt;/li&gt;
&lt;li&gt;架构师、前端、新人，如何借Al成长？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本期嘉宾：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;马工，Al Orchestrater @ Kreditz&lt;/li&gt;
&lt;li&gt;松子，资深AI产品专家&lt;/li&gt;
&lt;li&gt;张汉东，资深独立咨询师&lt;/li&gt;
&lt;/ul&gt;
</description><link>https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</link><guid isPermaLink="false">https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</guid><pubDate>Wed, 14 Jan 2026 09:16:38 GMT</pubDate><author>InfoQ 中文站</author><category>AI&amp;大模型</category></item><item><title>Zed 为什么不用自己造 Agent？OpenAI 架构师给出答案：Codex 重划 IDE × Coding Agent 的分工边界</title><description>&lt;p&gt;Coding agents（编码智能体） 已成为应用型 AI 中最活跃的领域之一，但许多团队在模型或服务商更迭时，仍不断重复构建脆弱的基础设施。那么，如何在生态不断变化的背景下保持快速迭代与高度韧性，并将更多精力投入到领域特定的工作流程和用户体验上？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为行业内的动向标杆，OpenAI的Codex提出了解决方法——“模型和Harness（工具集）的共同构建”。最近，OpenAI 的架构师 Bill Chen 和 Brian Fioca 在演讲里一起详细介绍了该构建过程中克服的挑战，以及这个Coding Agent本身一些新兴的使用模式。基于该演讲视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent的构成&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们来谈谈Coding Agent的构成。其实非常简单，一个Coding Agent由三部分组成：用户界面、模型和Harness。用户界面显而易见，可能是命令行工具，也可能是集成开发环境，或者是云端或后台Agent。模型也很直白，比如我们最近发布的GPT-5.1系列模型或其他一些供应商的模型。至于Harness，这是一个稍微复杂一点的部分，它直接与模型交互，最简化地说，可以将其看作是由一系列提示和工具组合而成的核心Agent循环，它为模型提供输入和输出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/2099b27dcfa604ecbaba2ea6937a64e4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coding 领域是应用人工智能最活跃的前沿之一，而随着新模型的不断发布，我们面临的挑战也在增加。更为复杂的是，大家不得不不断调整Agent以适应新发布的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们将聚焦于Harness的部分。Harness是模型的接口层，它是模型与用户、代码之间进行交互的媒介。它包括了模型需要的所有组件，以便在多轮对话中进行工作，调用工具，并最终为你编写代码，解读用户的需求。对一些产品来说，Harness可能是其中的关键部分。不过，构建一个高效的Harness并不是一件轻松的事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，构建Harness过程中遇到的挑战有哪些呢？首先是AV（音视频工具）问题。你可能会为Agent提供一个全新的、创新的工具，但它可能是模型之前从未见过的，它可能并不擅长使用这种工具。即使它曾经见过，你也需要花时间根据该模型的特点调整Prompt。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新模型不断发布，延迟问题也是一个挑战。模型在处理某些问题时需要时间，那么，我们应该如何设计提示，避免延迟过长？如何在用户体验上展示模型思考的过程？它在思考时是否与用户沟通，还是我们需要总结其输出结果？此外，管理上下文窗口和数据压缩也是一大难题。另外，API接口也在不断变化，现在我们有完成功能、响应功能，以及未来可能出现的其他功能，模型是否能熟练使用这些工具以便发挥最大的智能也是一个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将模型适配到Harness中需要大量的Prompt设计。实际上，模型的训练方式会带来一些副作用。我喜欢这样理解：（Steerability = Intelligence + Habit）智能加上习惯。一方面，智能是指：模型擅长什么？熟悉哪些编程语言？在某些框架中，模型能把代码写得多好？另一方面，它又养成了哪些习惯来解决问题？我们在训练模型时，培养了它在规划解决方案、查找背景信息、思考问题后再动手写代码，并在最后测试工作的习惯。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解这些习惯是成为一名优秀的Prompt工程师的关键。如果你没有按照模型熟悉的方式来指导它，可能会遇到问题。当我们发布GPT-5时，许多不习惯使用我们模型的人，尝试将其他模型的Prompt直接套用到我们的Harness中，结果发现我们的模型做的事情比其他模型要更为细致，导致了响应速度慢，效果不如预期。我们最终发现，如果让模型按照它习惯的方式进行工作，而不是过度引导，它的表现会更好。通过与模型的对话，我问它：“我喜欢这个解决方案，但它花了太长时间。下次你能做得更快吗？”模型回答说：“你让我去看所有的内容，其实我并不需要这样做，正是因为这个原因，才耗费了这么长时间。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Codex作为Harness/Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex被设计成一个适用于各种编程环境的Agent，它可以作为VS Code插件、CLI工具使用，甚至可以通过VS Code插件或手机上的ChatGPT在云端调用。它的功能非常基础：你可以通过提示将想法转化为可运行的代码，具备规划能力。它能在代码仓库中导航并编辑文件，执行命令和任务，你也可以从Slack或GitHub上调用它来审查PR。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着Codex的Harness需要能够完成许多复杂的任务：需要处理并行工具调用、线程合并等问题，还要考虑安全性，例如沙箱管理、提示语转发、权限设置、端口管理等。数据压缩和上下文优化的管理也非常复杂。何时触发压缩，何时重新注入数据，如何优化缓存，所有这些都是必须要解决的挑战。如果你要从零开始构建这些功能并保持其更新，工作量巨大。幸好，我们已经将这些功能集成到一个Agent系统中，它能安全地编写自己的工具来解决遇到的新问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这听起来比普通的Coding Agent强大多了，不是吗？但想想看，其实在浏览器和图形用户界面出现之前，我们操作计算机的方式不就是通过命令行界面写代码并将其串联起来吗？这意味着，如果你能将任务以命令行方式以及文件任务的形式表达出来，Codex就能知道该如何执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子，我喜欢使用Codex将我的桌面上的照片整理到一个文件夹里，这是一个非常简单的应用场景。但它还能做的不仅如此，它能够分析文件夹中大量的CSV文件，进行数据分析，这并不一定是Coding 任务，只要能够通过命令行工具来完成，Codex就能帮你做。现在我们可以看到，Codex是如此强大和有趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Codex构建自己的Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你希望将Codex集成到自己的Agent中，该如何操作呢？如果你打算创建下一个Coding 初创公司，一个关键的模式是：Harness成为新的抽象层。这个模式的好处非常明显，你不再需要在每次模型升级时都优先优化提示语和工具。但这是不是意味着你仅仅是在构建一个包装器呢？不是。正如我所说，单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看看一些我们与客户合作时所遇到的模式，这些模式实际上帮助他们成功构建了产品。Codex是一个SDK，你可以通过TypeScript库来调用它，也可以通过Python执行它。它还提供了一个GitHub动作，能够自动合并PR中的冲突，解决大家讨厌的合并问题。此外，你还可以将它添加到AgentSDK中，并为你的产品提供MCP连接器。这样，你就可以拥有一个Agent系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ece4a16f5c5cdcc38d4eba20b8c4fd24.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我喜欢说，我们从最初的聊天机器人开始，它们能与用户对话；然后我们为这些聊天机器人提供了使用的工具；如今，你可以为聊天机器人添加更多工具，使它能够自己生成尚未拥有的Harness。现在，你可以构建一个企业级的软件，允许它为每个客户即时编写插件连接器，这曾是专业服务团队的工作。你可以获得完全可定制的软件，且它可以与自己对话。我曾为开发日创建了一个看板，它能够自动修复自己的bug，非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，你也可以像Zed一样，将Codex嵌入到一个层级中，为IDE提供接口，使其能够与用户互动并进行代码编辑。这样，Zed就不必处理我们擅长的部分，而是可以专注于打造最好的代码编辑器。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的顶级合作伙伴，如GitHub，已经利用这些模式取得了巨大成功。我们为GitHub创建了一个SDK，允许他们直接与Codex集成。你也可以使用这个SDK将Codex作为你CI/CD管道的一部分，或者将它作为与自己Agent直接互动的工具。如果你想定制Agent层，完全可以这么做。举个例子，我们与Cursor团队紧密合作，他们将自己的Harness与我们开源的Codex CLI实现对接，成功地优化了系统性能，所有这些都是公开可用的，你可以克隆我们的代码库，随意使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex的未来是什么样的呢？它还没有发布一年，尤其是在推出Codex Max之后，变化非常迅速。它目前是增长最快的模型，每周服务数十万亿个token，这个数字从开发日以来翻了一番。我们可以合理假设，模型将变得更强大，它们能处理更长周期的任务，而且不需要监督。新模型的信任度将进一步提高，我相信这些模型已经能够处理比六个月前更复杂的工作，而且这种信任感将不断增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。SDK也将不断发展，以更好地支持这些模型的能力，使模型能够在执行任务的过程中不断学习，避免重复错误，并为写代码和使用终端解决问题的Agent提供更多支持，你将能够通过SDK在自己的产品中使用这一切。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，我们从中学到了什么呢？Harness构建非常复杂，特别是在新的模型不断发布的背景下。我们已经为你在Codex里构建了一个集成的工具，你可以直接使用它，或者查看源代码自行改进。除Coding 以外，通过它你还可以构建更多全新功能，而我们会处理确保你的计算机Agent具备最强的能力。同时，我们非常期待看到你们用它创造出的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=wVl6ZjELpBk&lt;/p&gt;</description><link>https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</link><guid isPermaLink="false">https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</guid><pubDate>Wed, 14 Jan 2026 08:56:01 GMT</pubDate><author>Bill Chen、 Brian Fioca</author><category>生成式 AI</category></item><item><title>不到百万级，看不见 MCP 的真实问题：创始人亲述这疯狂的一年</title><description>&lt;p&gt;一年前，MCP 还只是一个“把模型连到工具”的开源协议；一年后，它已经冲进了一个很少有协议能抵达的位置：事实标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场一年狂飙的亲历者之一——MCP 联合创作者、核心维护者 David Soria Parrra看来，最戏剧性的分水岭发生在四月前后：当 Sam Altman、Satya Nadella、Sundar Pichai 先后公开表态，Microsoft、Google、OpenAI 都将采用 MCP，“大客户”突然从 Cursor、VS Code 扩散到整个行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一年，MCP 从本地 “桌面玩具”，一路演进到远程 server、认证机制、面向企业可用的 OAuth 重构，再到 11 月引入 long-running tasks，把深度研究、甚至 agent-to-agent 交互变成协议的一等公民。David 的总结很直接：“这一年真的非常疯狂。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段对谈里，David 也很坦率地复盘了 MCP 这一年的取舍：做对的，是死磕标准 HTTP；踩坑的，是把关键能力做成了‘可选项’，结果客户端大多不实现，双向能力被削掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更现实的问题是扩展性：规模一上来，多实例、多 Pod 下同一段交互可能打到不同机器，不得不用 Redis 之类的共享存储来“拼状态”，请求量到百万级就开始吃力：“当规模一上来，这件事一点都不好玩。”“一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客内容整理，略有删节：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 的一年：从发布到行业事实标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：要不你先简单讲讲 MCP 的发展情况，以及之前为什么决定把它捐赠给基金会？接下来我们再系统回顾 MCP 这一年的演进，然后再请基金会的其他负责人加入，聊一些更宏观的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David Soria Parrra（MCP Co-creator）：如果回到一年前，MCP 刚发布的时候，其实谁都没想到它会在这一年里迎来如此疯狂的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老实说，这一年感觉像过了一个世纪。一开始是在感恩节和圣诞节前后，很多开发者开始自发地用 MCP 搭东西。随后，像 Cursor、VS Code 这样的“大客户”开始出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的拐点出现在四月左右——当时 Sam Altman、Satya Nadella、Sundar Pichai 等人陆续公开表示，Microsoft、Google、OpenAI 都会采用 MCP。那是一个非常明显的“分水岭”。&lt;/p&gt;&lt;p&gt;与此同时，我们也一直在推进协议本身的演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最初，MCP 几乎只支持本地使用：你在桌面上跑一个 MCP server，通过本地 stdio 和客户端通信。但到了今年三月，我们开始推进“远程 MCP server”——也就是如何通过网络连接 MCP，并且第一次引入了认证机制。到了六月，我们又对这套认证方案进行了比较大的修订，尤其是为了让它真正适用于企业场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们非常幸运，在三月到六月这段时间里，有真正做 OAuth 标准的行业专家，直接参与进来，帮我们把一些关键细节“拉正”。我们也在这段时间里大量投入在安全最佳实践上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了 11 月底，我们发布了新一轮重要版本，引入了 长时间运行任务（long-running tasks） 这一关键原语，用来支持深度研究类任务，甚至是 agent-to-agent 的交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的感觉是：MCP 的基础已经非常扎实了。接下来还有一两个关键原语和可扩展性问题需要解决，然后协议整体会进入一个相对稳定的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，这一年真的非常疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你刚刚提到 agent-to-agent，那是不是也涉及 A2A 协议？在 Agentic AI Foundation 成立时，有没有讨论过把其他协议也纳入进来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，这几乎是必然会发生的讨论。我们当然讨论过市场上其他协议，比如一些支付协议之类的东西。但在决定成立基金会时，我们有两个非常明确的原则：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，我们想 从小开始。这是 Anthropic 第一次参与开放源代码基金会，一切都是新的。我们希望先在一个相对可控的范围内学习如何把这件事做好，并且和 OpenAI、Block 一起，把基金会的节奏掌控住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，在协议层面，我们非常在意“事实标准（de facto standard）”。目前来看，真正已经具备广泛采用度的协议，只有 MCP。其他协议还没有“走到那一步”。当然，如果未来某个协议发展到那个阶段，并且在功能上是互补的，我们是完全开放的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在应用层，我们会更灵活；但在协议层，我们不希望一个基金会里同时维护五个做同一件事的通信协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在在基金会和 MCP 之间，是不是有点“戴两顶帽子”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：确实如此，但我主要精力仍然在 MCP 上。基金会本质上是一个“保护伞”，它最重要的作用是保证项目的中立性。至于基金会预算怎么用、办什么活动，这些相对来说反而是“比较枯燥”的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 MCP 的技术治理上，其实并没有发生本质变化。我依然是核心维护者，继续推动协议演进。&lt;/p&gt;&lt;p&gt;另外，我也会参与基金会的技术指导委员会（TSC），负责判断：哪些项目适合进入基金会？它们是否被良好维护？是否有真实采用？是否具备长期价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不希望基金会变成一个“项目垃圾场”。我知道有些基金会最终会落得什么下场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这一年 MCP 发布了四次规范更新，节奏非常快。尤其是三月和五月那次，引入了 HTTP Streaming 和认证。要不要给大家系统梳理一下？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：HTTP Streaming 那次更新非常关键，也是用户呼声最高的一次。我们在 11、12 月就已经意识到：下一步一定是远程 MCP，而远程就绕不开认证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP 的一个特点是：它在每一层都非常“有主见（prescriptive）”。比如，在客户端和服务端互不认识的情况下，认证该怎么做，我们希望只有“一种正确方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;三月版本里，我们做了一版认证方案。现在回头看，它“还行”，但确实有问题。说白了，是我对企业认证场景理解不够。MCP 的一个核心优势，是它的社区：当我不懂的时候，会有真正懂的人站出来帮我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：三月那版认证，主要问题出在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 里有两个核心角色：&lt;/p&gt;&lt;p&gt;身份提供方（Authorization Server / IdP）：发放 token资源服务器（Resource Server）：接收 token 并给相应的资源作为回报&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在第一版 MCP 认证规范里，我们把这两个角色合并进了 MCP server。对于创业公司来说，这没问题：你自己有账号体系，把 MCP server 直接绑在用户账号上，完全可用。但在企业环境里，这根本行不通。企业几乎总是有一个中央身份系统（比如 Google 登录、企业 SSO），用户每天早上只感知到“我登录了一次”，但背后其实是 IdP 在工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在六月的规范中，我们做了一个关键调整：明确把 MCP server 定义为资源服务器，和身份系统解耦。我们对“怎么拿 token”依然有建议，但不再强行绑定在 MCP server 里。同时，也补齐了动态客户端注册等细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 agent 代表用户去操作，比如帮我用 Linear、Slack，这个问题现在解决了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 本身是一个非常“以人为中心”的协议。它只定义：如果你没有 token，该怎么拿 token。一旦你有 token，后面就只是把它放进 Bearer Token 里而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们目前并没有对 agent-to-agent 或 agent 代表用户的认证方式做强约束。在企业内网、封闭环境里，大家已经可以通过 workload identity 等方式做到。但如果客户端和服务端彼此不认识，我们目前还没有一个“完美方案”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们从本地服务器（比如基于 stdio 的方案），一路演进到可流式的 HTTP。在这个过程中，有哪些经验教训值得分享？有没有什么后悔的地方，或者对其他人有什么建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：关于传输层这件事，其实有一个讨论，从过去几年一开始就从未停过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在最近两天，我们还在 Google 的办公室里，和一群来自 Google、Microsoft、AWS、Anthropic、OpenAI 的资深工程师坐在一起，专门讨论：到底需要做什么，才能把这件事真正、彻底地打牢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回到今年三月，当时我们希望引入一种新的传输方式，它能够尽量保留我们在标准 IO（stdio）里拥有的很多特性。因为我们当时——而且直到今天我依然坚信——MCP 不只是为了简单的请求-响应，它还应该支持 Agent。而 Agent 天生就是某种程度上“有状态”的，它需要在客户端和服务器之间进行一种长期存在的通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我们一直在寻找一种具备这些特性的方案。我们当然也研究过一些替代方案，比如 WebSocket。但在实践中，我们发现，要真正把一个可靠的双向流（bidirectional stream）做好，其实会遇到很多问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们就在思考：有没有一种“中间态”？这种中间态需要满足两个条件：一方面，它要足够简单，适合那些最基础的使用场景——比如用户只是想提供一个工具；另一方面，它又必须能够在需要的时候，升级成一个完整的双向流，因为你可能真的会遇到那种复杂的 Agent 之间相互通信的场景。正是在这样的背景下，可流式 HTTP（streamable HTTP）诞生了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事后回看，我觉得我们有些地方做对了，也有些地方做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做对的地方在于：我们非常坚定地只依赖标准 HTTP。但做错的地方在于：我们让太多事情对客户端来说是“可选的”。比如，客户端可以连接服务器，并打开一个从服务器返回的流，但它并不是必须这么做。而现实情况是——几乎没有客户端会这么做，因为这是可选的。结果就是，很多双向能力实际上被“抹掉”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，一些功能，比如 elicitation（征询） 和 sampling（采样），对服务器来说就变得不可用。原因很简单：服务器没有一个打开的返回流；而客户端在实现时会想，“这已经满足我产品的最小可用版本（MVP）了，我没必要再多做这些。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这最终成了一个问题。我觉得这是一个非常明确的教训。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个教训来自于协议设计本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们设计的这套传输协议，要求服务器端持有一定的状态。如果你只有一台服务器，这当然没问题。但一旦你要做水平扩展——比如跑在多个 Pod、多个容器里——问题就来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;设想这样一个流程：一次 tool call，然后是一次 elicitation，再接着是 elicitation 的结果返回。很可能，这几个请求会打到不同的服务器实例上。那你就必须想办法，让这几台服务器把这些信息“拼”在一起。现实中，这往往意味着你需要某种共享状态机制：Redis、Memcached，或者别的什么共享存储，总之你需要一个地方，能够让这些服务器共享状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，这当然是可行的。我们在 PHP 应用、Python 应用里早就见过类似的模式。但说实话，当规模一上来，这件事一点都不好玩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们也知道，一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们现在坐在这里，不断地问自己：如何在协议的下一次演进中，做到这几件事？&lt;/p&gt;&lt;p&gt;对简单的 MCP Server 来说，仍然尽可能简单；在需要的时候，允许完整的双向流；同时，还要具备良好的可扩展性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，我们正在逐步找到正确的解法，但这件事本身真的很复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为今天的大多数技术选择，其实都非常极端：要么你做一个很简单的东西，比如 REST；要么你直接上“全双工”的方案，比如 WebSocket、gRPC。而我们需要的，其实是两者同时存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在巨头之间“做标准”是什么体验？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：和这么多顶级公司一起做标准，是什么感觉？在那样的场合，大家都是资深人士，每个人都有自己的观点。谁来做最终决定？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：真的太有意思了。我能和业内最顶级的工程师一起工作。通常我们的目标是尽量达成共识。现实情况是，从技术角度讲，最终拍板的人是我，但说实话，这更多是一种形式上的存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正重要的事情在于：我们努力把讨论不断收敛，明确哪些是真正大家都认可的问题，哪些是暂时还存在分歧的问题，然后在这些边界之内，去构建我们能做到的最佳解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个过程需要时间，需要大量迭代，但说真的，这件事本身非常有意思。因为你能看到来自不同公司的、非常独特的问题形态。你甚至能从问题本身，看出一家公司的“性格”——比如 Google 面临的问题和 Microsoft 就完全不同，而这些差异，很大程度上来自他们各自构建系统的方式。同样，Anthropic 的问题看起来也和 OpenAI 的问题不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我最喜欢的一点在于：有时候你会突然意识到，自己正坐在一个房间里，周围全是彼此竞争的公司，但大家却在一起构建同一件东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在开源世界已经待了大概 25 年了，我真的非常热爱这种状态。当一个标准真正运转起来时，这就是理想状态。而且这些人都非常优秀，我从每一位同行身上都学到了很多。所以我非常感激，自己能处在这样的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来有点像 IETF 的标准制定流程？你们有没有讨论过，这种“私下的小圈子”运作方式，和更传统的标准组织之间的差异？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这是个很有意思的问题。某种程度上，它确实有点像 IETF，但也有明显不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IETF 是一个完全开放的论坛，任何人都可以参与。它的结果是——不是因为刻意如此，而是“偶然地”——整个流程非常依赖共识，因此速度也相对较慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这种慢，在很多方面其实是优点。因为一旦标准定下来，基本上是不可逆的。比如你看看 OS 2.1 规范，它已经制定了三四年，到现在都还没完全结束。这就是 IETF 标准化的节奏：这些事情本来就会花非常非常长的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为这对某些领域是好事，但在 AI 领域，目前的变化实在太快了，你几乎被迫要选择一个更小的核心群体。因此我们选择把 MCP 运作成一个非常传统的开源项目：有一个大约 8 人的核心维护者小组，基本上由他们来做最终决策；其他人可以提供输入、提出建议，而且很多变更并不是来自核心维护者，但决定权在他们手里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一种折中方案：一部分是共识驱动，一部分则是带有一点“技术独裁”的意味。如果你想要快速前进，这种模式在当前阶段对 MCP 来说是合理的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你们是如何平衡模型能力演进与协议设计之间的关系的？毕竟 Anthropic 和 OpenAI 都在做大量后训练（post-training），让模型更擅长工具调用；这会不会影响你们对协议形态的偏好？反过来，协议是否也会反向影响模型训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，我不敢说自己对研究侧的所有事情都 100% 熟悉——我更多是产品背景。但从我了解的情况来看，协议确实会在一定程度上影响后训练，比如我们在模型卡中会使用 MCP Atlas，确保模型在面对真实世界中大量存在的工具时，能正常工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从另一个角度讲，协议的底层原语，其实很少直接被模型能力的提升所驱动。我们更像是在预期模型能力将会呈指数级增长，因此在协议中，依赖了一些你可以通过训练不断强化的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个更具体的例子。很多人都讨论过 MCP Server 的上下文构建问题。因为 MCP 打开了通往大量工具的大门，如果你天真地把所有工具一次性塞进上下文窗口，那只会造成严重的膨胀。&lt;/p&gt;&lt;p&gt;这就好比把所有技能、所有 Markdown 文件一次性丢进上下文里，结果当然会一团糟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们其实从一开始就知道，可以采用一种叫做渐进式发现（progressive discovery）的方式：先给模型一小部分信息，让模型在需要的时候，再主动请求更多信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这本质上是一个通用原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这里正是我们这些“大模型公司”具备的一点前瞻性所在——我们知道，如果愿意，是完全可以通过训练，把这种能力系统性地强化出来的。模型在原理上已经能做到这些事情了，训练只是让它做得更好。任何支持工具调用的模型，都可以做到这一点；只是如果你专门为此训练过，它的表现会更好。所以在这个层面上，协议设计和模型训练是相互配合的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但归根结底，渐进式发现这种机制，本身就内生于任何具备工具调用能力的模型之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也引出了“上下文腐烂（context rot）”的问题。还有 MCP 和所谓 “code mode” 的讨论——比如有人会说，“Anthropic 提倡 code mode，而 MCP 又是 Anthropic 做的，那是不是说明 code mode 才是正确方向？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：首先澄清一下，官方博客其实从来没用过 “code mode” 这个词，那是大家后来叫出来的。我们内部更常说的是 “programmatic MCP”，但本质上讨论的是同一件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键在于：MCP 是应用和服务器之间的协议，模型本身在技术上并不直接参与 MCP。所以问题其实变成了：应用拿到一堆工具之后，该怎么用？你可以用最朴素的方式：把工具直接暴露给模型，让模型逐个调用。但你也可以更“创造性”一点：模型非常擅长写代码，那如果我们把这些工具当成 API，交给模型生成一段代码，让它提前把多个调用组合好，再在一个 sandbox 里执行呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本质上，模型原本就会做这样的组合：调用 A → 拿结果 → 回到推理 → 调用 B → 再组合成 C。你只是让模型提前优化了这个过程，把它编译成一段可执行代码而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 MCP 的价值并没有因此消失：&lt;/p&gt;&lt;p&gt;认证（authentication）仍然由 MCP 处理；接口是为语言模型设计的；工具是可发现的、自文档化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力依然存在。你只是换了一种使用方式而已。所以当有人说，“那 MCP 是不是就没用了？”我其实挺困惑的。它不是没用，而是被用在了不同的层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着模型和基础设施逐渐成熟——比如你可以默认 AI 应用都有 sandbox 执行环境——你确实可以玩出更多有意思的花样。但这并不意味着，一个把模型连接到外部世界的协议就失去了价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人更愿意把这种变化，看作一种优化，说得直白一点，就是 token 级别的优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 有没有竞争对手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这正好可以引出 skills。skills 是一个相对较新的概念。我之所以提到它，是因为在我脑子里，它和渐进式发现、预置代码脚本这些概念是连在一起的。而且 skills 还能生成 skills，本身就很有意思。很多人试图把 MCP 和 skills 放在对立面来比较，显然它们并不重叠，但你是怎么看待这个问题的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我同意。我觉得有意思的点就在于：它们并不重叠。它们解决的是不同的问题。&lt;/p&gt;&lt;p&gt;我觉得 skills 非常棒，而且你知道的，我认为 skills 最核心的出发点之一，就是渐进式发现（progressive discovery）这个原则。但我也认为，“渐进式发现”这种机制，其实是通用于你能用模型做的几乎任何事情的——它不是 skills 独有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 skills 到底提供什么？它提供的是某一类任务的领域知识（domain knowledge）：比如你应该如何做事、如何表现，模型应该如何扮演一个数据科学家，或者如何扮演一个会计之类的角色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 MCP 提供的，是你能对外部世界采取的真实动作的连接性（connectiveness）——也就是你能执行哪些实际操作、如何把这些操作真正连到外部系统上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为它们在某种意义上是正交的（orthogonal）：skills 给你的是更“纵向”的能力——偏领域、偏角色、偏方法论；而 MCP 给你的是更“横向”的能力——偏连接、偏动作、偏“给我那个具体操作”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，skills 也可以执行动作。它能执行动作，是因为你可以在里面放代码和脚本，这当然很棒。但这里有两个关键点，我觉得很多人容易忽略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，你需要一个执行环境（execution environment）——也就是你需要一台机器来跑这些代码。是的，你需要“机器”。这在很多场景下完全没问题：比如你在本地跑一个东西（像 Cloud Code 之类），那我们就可以讨论 CLI；在这种你确实拥有执行环境的场景里，这套方式就非常合理，也很好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;或者，如果你有一个远程执行环境，那同样也说得通。但即便如此，你在这条路径上仍然得不到认证（authentication）这一块能力。所以我认为 MCP 带来的关键价值之一，就是它把认证这件事补齐了——这是 skills 本身不提供的那部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个点是：你不必去处理“外部方的持续变化”。举个例子，如果你接的是一个 Linear 的 MCP server，那么对方可以持续改进它，而你不需要在自己的 skill 里去处理这些变化——它不是被“固定在某个时间点”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三个点是：你其实不一定需要一个本地的执行环境，因为执行环境在某种意义上是“在别处”的——它在服务器端。也就是说，执行发生在 MCP server 那边。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，如果你在构建的是一个 Web 应用，或者一个移动应用，这些特性在某些方面会更契合、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以整体来看，我认为它们大多数时候都是正交的。并且我确实看到过一些很酷的落地方式：人们用 skills 去探索不同的功能、不同的角色（比如会计、工程师、数据科学家），然后再用 MCP servers 把这些 skills 连接到公司内部真正的数据源上。我觉得这是一个非常有趣的模型，也最接近我理解和看待它们关系的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以 MCP 是连接层？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我会说是通信层。是的，通信层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从架构上讲我很好奇：MCP client 是放在每个 skill 里面，还是大家共享一个 client？比如共享 client 还能发现 skills 之类的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们是共享的方式。我觉得从技术上你确实更想走“共享更多”的方向——共享越多，你能做的事情就越多：比如做 discovery（发现）、做连接池（connection pooling）、做自动发现，甚至你可以让 skill 只用很“松散”的方式描述它想要什么，然后系统去你有权限访问的 registry 里帮你找一个合适的 MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力只有在 shared 的架构里更容易做出来。当然，最终两种方式都能工作，只是这仍然是一个值得继续实验的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 怎么用 MCP？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我想强调一下，可能很多人都没意识到——你刚才一直说“我们怎么做怎么做”，但实际上我觉得外界并不理解 Anthropic 内部到底 有多大规模地在 dogfood MCP。我也是看了 John Welsh 的演讲才真正理解，他说：“我们有一个 MCP gateway，一切都要走这个 gateway。”你能多讲讲这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：当然。我们内部两种都用：skills 用得很多，MCP servers 也用得很多。因为你要让大家很容易部署 MCP，你需要和公司内部的 IdP（身份系统）打通之类的东西。所以我们为自己定制开发了一个 gateway。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你只需要把 MCP server 部署起来，剩下的都是内部应用、内部系统在用。有些东西“技术上”算外部系统，但因为它们没有提供第一方 MCP server，我们就自己做了。比如我们有一个 Slack 的 MCP server——我特别爱用。它可以让 Claude 帮我总结 Slack。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们内部还有很多类似的用法：例如我们每半年（或者一年两次）会做一次员工调查，问大家对公司、对未来、对 AI、对安全等议题的感受。我们也有一个 MCP server 支持这件事，然后你可以围绕结果问很多问题，这非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这些都是你们团队维护的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不是。我们维护的是 gateway。但有意思的地方在于：MCP 从一开始的想法就是——在我们开源之前，它源自一个很现实的困境：公司增长太快了。我在研发工具、开发者工具这一侧，增长速度一定跟不上业务扩张。那我怎么做一个东西，让大家能“自己为自己构建工具”？&lt;/p&gt;&lt;p&gt;这就是 MCP 的起源故事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你现在回头看，一年之后发生的事情，正好就是我们当初想要的：大家真的在为自己构建 MCP servers。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我甚至可能完全不知道 Anthropic 内部 90% 的 MCP servers，因为它们可能在研究团队里，我看不到；或者人们就是自己做给自己用，我也不会被同步到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那它们是自己 host 吗？还是有远程托管？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：基本上大家只需要一条命令启动，它就会在一个 Kubernetes 集群里跑起来。算是“半托管”的形态。对任何大公司来说，这类平台基础设施都很重要。外部也有一些平台会帮你做这件事，但从安全角度，我们倾向于自己做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过外界也有类似的产品。比如有人做了一个叫 fast MCP 的东西——Jeremiah 他们做的 fast MCP cloud，有点像这样：两条命令，你就能跑起一个 MCP server 实例，支持 HTTP 流式传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多企业还会用类似 LiteLLM 这样的东西做 gateway：你甚至可以启动标准 IO 的 server，把它接到 gateway 上，然后由 gateway 来处理认证等“所有麻烦的部分”。所以落地路径其实很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为你真正想要的“理想基础设施”是：让部署变得极其琐碎、极其简单——比如“一条命令”启动一个原本只是 stdio 的 MCP server，然后它瞬间变成一个带有 HTTP streaming、并且集成了认证的远程 MCP server。最终开发者只需要做“标准部分”，其他复杂部分都由平台替你完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你把这个点讲出来，因为很多人会直接把这套思路拿回公司里落地。否则替代方案就是：混乱、重复造轮子、各自重建一遍。顺便 shout out Jeremiah——我还邀请他来我在纽约的峰会做一个 fast MCP 的 workshop。他写过一篇很棒的博客，说我们看到的 MCP 使用，很大一部分其实都发生在企业内部。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我们也观察到同样的现象：在大型企业内部，你几乎到处都能看到 MCP。它的增长速度，比你想象得快得多——因为它多数都在企业内部发生，外界根本看不见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Registry怎么演化？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说到 discovery，你们推出了官方 registry。然后又出现了各种 registry 公司、gateway 公司。现在官方 registry 里甚至出现了“自动把自己的 MCP server 放进官方 registry”的子 registry。你们是不是需要更多 registry？你从推出 registry 这件事上学到了什么？你觉得未来会怎么演化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们看到很多不同的 registry 冒出来。我们一直觉得，生态确实需要一种类似 npm / PyPI（MPM） 的模式：有一个更中心化的地方，任何人都可以把 MCP server 发布上去。&lt;/p&gt;&lt;p&gt;这就是官方 registry 最初的出发点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们同时也想推动：至少整个生态要有一个共同的标准，让不同 registry 之间能“说同一种语言”。因为我们真正想实现的世界是：模型可以从 registry 里自动选择一个 MCP server，安装它，用在当前任务上——像魔法一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要做到这一点，你需要一个标准化接口。我们很早就开始和 GitHub 团队合作（大概四月份），但后来我被别的事情分走了注意力，比如认证，去集中解决那块了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我希望看到的方向是：未来会有一个“官方 registry”，任何人都可以往里放 MCP server。它的角色就像 npm ——而 npm 也有完全相同的问题：任何人都能发布，你并不知道该信谁、不该信谁；会有供应链攻击。这是公共 registry 的基本属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们才提出了 子 registry（sub-registries） 的概念：像 Smithery 这类服务可以在官方 registry 之上做过滤、做精选、做策展（curate）。我们希望生态最终能形成这样的结构。&lt;/p&gt;&lt;p&gt;我们现在还没完全到那个状态，但正在往那个方向走。比如 GitHub 的 registry 是“策展式”的，同时它和官方 registry 讲的是同一种格式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终我们想要的是：作为一家企业，你可以有一个内部 registry——它基于官方 registry 的镜像，再加上你自己的私有 MCP servers；它是你信任的来源，同时它暴露的 API 和官方 registry 一样。这样无论是 VS Code 还是其他客户端，只要指向你的内部 registry，就可以顺畅工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这很有意思，因为 npm 在某种意义上更像一个“下载网关”。我其实不太会去 npm 做发现，我更多是在别处看到包，然后再去 npm 安装。你觉得 registry 的核心是 discovery 吗？还是 agent 会用别的方式完成发现？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我认为 discovery 在模型世界里会更重要。这里和 npm 的差别在于：&lt;/p&gt;&lt;p&gt;我们是在做一个 AI-first 的东西，我们可以假设：有一个聪明的模型，它“知道自己想要什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在过去是不存在的。如果你今天重新设计现代包管理系统，并且把模型当作核心，你可能会做出类似的交互：“这是我想做的事，你自己决定装哪些包，我不在乎，反正把事情做成就行。”这就是它的类比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但再次强调：公共 registry 不应该直接让模型这么做，因为公共 registry 很容易变成一个“垃圾场”。你应该在一个可信、被策展过的 registry 上做这种自动化选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你那句话——模型知道自己想要什么。因为现在很多人都有一个梦想：agent 能用 MCP 目录去发现新的 server，自己安装自己使用。这听起来非常 AGI。如果真能跑通当然很牛，但也可能跑不通。要做到这一点，到底需要什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我觉得需要两件事：&lt;/p&gt;&lt;p&gt;第一，你需要一个好的 registry 接口。&lt;/p&gt;&lt;p&gt;第二，你需要真的去为这个目标做工程、做实验，看看什么可行、什么不可行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你肯定需要信任等级（trust levels）。你可能还需要签名（signatures）。我有一个想法——不确定会不会真的做——比如：你可以附带来自不同模型提供商的签名，表示他们扫描过这个 MCP server，并且愿意为它背书：&lt;/p&gt;&lt;p&gt;“Anthropic 的签名：这些 tool descriptions 是安全的”“OpenAI 的签名：我们认为这些是可信的”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后你就可以基于这些签名自行决策。这有点像分布式代码签名——不过也不完全分布式，本质上可能还是中心化的。但我认为这是你最终会需要的一类机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过最先跑通的场景，可能反而是企业内部：企业会用私有 registry，本身就带有隐含信任。就像他们今天已经在用私有 npm / 私有 PyPI 一样，他们也会用私有 MCP registry。在这种环境里，你天然有 trust，然后就可以开始做搜索和自动选择。我们自己其实就有内部 registry：当你通过 John 那套基础设施启动一个 MCP server，它就会被注册进去。所以我们也需要在内部继续做实验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Sampling：理想很美，但客户端不配合&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你今年在伦敦办了一些活动，你看到什么好的 sampling 用例了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：还没有特别多。我从 sampling 这件事学到的一点是：人们想在 sampling 的过程中使用一些“只在 sampling 时出现”的工具——这些工具并不是 MCP server 暴露出来的那套工具。但我们之前没有能力做到这一点。在这次迭代里我们刚修复了这个问题，所以我们希望未来能看到更多 sampling 用例。偶尔会有一些 MCP server 在用 sampling，但不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其是当 MCP servers 从“本地为主”走向“远程为主”，在远程场景里，通常更好的选择可能是直接提供 SDK：你完全控制它、自己部署，甚至还可以收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在本地场景里，sampling 的价值更大：因为你是在给很多人分发一个东西，你并不知道他们用的是哪个模型、哪个应用（可能是 VS Code，也可能是 Claude Desktop），这种情况下 sampling 才更有意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在的问题是：客户端基本都不支持 sampling。所以 sampling 这件事让我挺沮丧的——我仍然觉得这是个很强的想法，但你知道的，有时候你总得赢一些、也得输一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你们也在升级它，我还是很期待。有点奇怪——如果采样这件事做对了，它某种意义上会变成真正的 agent-to-agent 协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你看到的大多数用例还是偏“数据消费”吗？我自己的 MCP 用法也 mostly 是拿上下文、拿数据。最多的 action 可能就是更新一下 Linear 任务状态。你见过很复杂的“用 MCP 做动作的工作流”吗？还是大家基本都在用它做上下文？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：大多数人确实是用它做上下文，这占了绝大多数。毕竟它的名字就叫 Model Context（模型上下文）。顺便说一句，OpenAI 的 Nick Cooper 经常跟我说——而且他说得对——MCP 这个名字可能取错了，它确实会让人感觉用途被“限制”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我看到的主要还是数据用例。也有人把它用于 deep research，一些更复杂的 agent 暴露出来，但并不普遍。deep research 这种自定义研究用例不算罕见，但除此之外，大多数还是数据、以及围绕数据的深度研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在你还会看到一个新方向：通过 MCP UI（未来我们可能叫 MCP Apps / MCPI）暴露 UI 组件。我觉得这非常有前景，也非常有意思。现在在一些 chat apps 里已经能看到不少类似实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Tasks：为长时间、异步 agent 操作而生的新原语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇，因为如果大多数用例是“上下文”，你们做 tasks 这个原语，就好像大家暂时还没怎么用它。你们设计 tasks 的出发点是什么？你期待它怎么被用起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们做 tasks，是因为很多人来找我们说：“我们真的需要长时间运行的操作——也就是 agents。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想要那种“深度研究任务”，可能一小时才完成；甚至可能一天都跑不完。过去人们会很别扭地用 tools 去实现这类事情——工具本质上就是 RPC 接口，理论上你能凑出来，但很快就会变得别扭：模型需要理解“我得去轮询、我得去拉取”，体验很差，也不是一等公民（first-class primitive），限制很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这类诉求太普遍了：大家都想要长时间运行的 agents。GitHub issue 里，大公司也一直在说“我们需要 long-running operations”。所以我们觉得必须做点什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在 tasks 刚刚落地到 SDK，还需要落地到客户端，然后我们才会看到更广泛的使用。但我非常确信：自定义研究类任务会大量用上它，其他场景也会逐步跟进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我对 tasks 非常看好。我觉得任何编排系统或协议都得有 sync 版本和 async 版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：完全同意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 tasks 的设计上，有没有哪些重要分岔点？比如本来有两条路，你们选了其中一条。&lt;/p&gt;&lt;p&gt;David：讨论非常多。有人提议：tasks 其实就是“异步 tools”，做成一个新的 tool primitive 就行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但对我来说，我的 试金石（litmus test） 一直是：如果未来我想把 Claude Code 或任何 coding agent 当作一个 MCP server 暴露出来，那么 tasks 必须能支撑这种形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;纯粹的异步工具调用做不到这一点。你需要一种操作方式：它能够在长时间运行的过程中返回中间结果。理想状态下，你会想暴露这样的东西：“我通过调用这个工具、那个工具、还有那个输入，得到中间产物……最后得到结果。”这才是你希望一个长任务能够表达的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;tasks 现在还没完全做到这一步，但它的设计是“足够通用”的，未来可以支持这种更丰富的表达——这就是最核心的约束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个关键约束是：我们不希望 tasks 成为 tools 的复制品 ——只是语义稍有不同。我们希望它是一个更抽象的概念：你通过一次带元数据的 tool call 来创建一个 task，然后系统自动创建并管理这个 task。所以 task 更像一个“容器（container）”：它描述了一段从开始到结束的异步过程，而我们当前用 tool call 作为触发方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样的抽象会打开很多未来可能性。所以我觉得，真正的设计目的是让实现变得更抽象。（虽然）实现起来很复杂，但也最终被解决了，因为复杂性会被 SDK 吞掉：SDK 会帮你实现细节，在开发者视角里，它就是一个 async 调用，然后返回结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来会和很多异步 RPC 框架有点重叠，比如 JS 世界的 tRPC、或者各种 protobuf 体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。从接口风格来说，它很像经典的操作系统接口：你创建一个 task，然后不断 pull（轮询）直到它完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后我们下一轮会做一个优化——这次没来得及做：你不用每隔几分钟/几小时去 pull，server 可以回调你（发事件、webhook 之类的）告诉你“我完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是优化，但核心接口始终是：客户端可以 pull。这也很像操作系统里的一些文件系统操作：客户端轮询是一种最通用、最可靠的基线能力……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以一直 pull（轮询）：文件变了吗、文件变了吗……但你也可以用现代一些的内核接口，比如 inotify 之类的通知机制，或者 io_uring 之类的方式，它会告诉你：哦，我完成了——很好，文件变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我学到一个“骚操作”——server 可以一直把 HTTP 连接挂着，等它做完了再断开；连接断开本身就成了一个信号，告诉后端“完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，但我们不一定想这么做。因为它可能要跑几天，我也不知道别人会怎么处理这种连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这其实挺不负责任的，但确实很酷。老实说，tasks 真的很有意思——我们在做 Devin API、以及 Cognition 那些东西时，也基本被迫“重新发明”过类似机制。这也很有代表性：每个人最终都会需要某种 long-running operation。而当你在调用一个 agent 时，你同样需要这个能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。但对我们来说，有一个有意思的点是：MCP 一直在做的事情，是把大家“此刻正在尝试做的东西”封装起来；我们并不想强行规定一年后大家“应该怎么做”。因为我们不知道，我们不预测未来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做 tasks，是因为大家说：我们现在就需要它。实际上我们六个月前就需要它了。于是我们说，好吧，那现在就是动手的时候了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不想做那种“预测未来”的协议，所以才努力让协议保持相对最小化。虽然也有人会觉得：现在协议里的 primitive 已经太多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;超长任务与上下文压缩&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一个小问题。假设是超级长的任务，过程中会来回传很多消息。Anthropic 在上下文压缩（或者叫 compaction）这件事上算是领先者之一，其他实验室也在做类似事情。那这种场景怎么处理？我们是不是就无状态地把上下文截断也没关系？你需要保留“全过程完整日志”吗？还是说删掉就删掉了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不需要。你看，我们现在这个行业还是非常早期，我们一直在学习：模型到底需要什么、不需要什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;甚至到今天，有些 agent 已经开始在跑了几轮之后丢弃 tool call 的结果，因为它不再需要了。我觉得这非常好。所以除了 compaction 之外，我觉得你还会看到更好的机制：更清楚地理解“该保留什么、不该保留什么”。比如对一个长时间异步任务，你可能会这样：某段时间模型确实需要看到全部过程，但当你拿到最终结果之后，你就把其他东西都丢掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你甚至可以调用一个更小的模型——比如 Haiku ——让它来判断：这些内容里哪些该保留？告诉我。也可能最“AGI build”的方式就是：让模型自己决定它需要保留什么。所以你会看到两种世界并存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在还没有唯一答案，因为大家仍在摸索。compaction 是一个很好的阶段性方法，但它也不会是最后一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，如果你更认真地思考：你能训练模型在这里做什么，我觉得会有更好的方式。但这些都和“你如何获取上下文”是相互独立的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直把 MCP 看作一个 应用层协议：它只负责“你如何获得上下文”。至于“你如何选择上下文”，那是应用层问题——所有 agent 应用最终都会面对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来会有很多技术路径。一年前所有人都会说：RAG 才是答案；现在大家又说 RAG 好像“死了”。我们开始用模型、用 compaction。至于一年后会怎样，我也不知道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我还有个问题：你怎么看 MCP servers 在未来的定位——它们是给开发者用来构建 AI 应用的？还是一个面向 AI 消费者、让他们把各种服务“插上就能用”的协议？我觉得很多人会把它理解错：他们说“我有 REST API，为什么还需要 MCP？”在我看来，MCP 可能并不是“给开发者用的”，而是给使用 AI 工具的人，用来把东西插进去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我经常被拿来和 REST API 比。这个对比挺有意思的，因为这里其实有两个问题：第一，REST 并不告诉你认证该怎么做。第二，你们已经在跟我抱怨 “tool bloat（工具膨胀）” 了，但你们有没有看过平均一个 OpenAPI spec 有多长？你把那个塞进模型里，膨胀只会更严重——实际上会糟得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有意思的是，当人们尝试一比一映射时，模型经常会有点迷糊：你会有“按名字搜索、按 ID 搜索、按某字段搜索”等等，突然冒出五个长得很像的工具，模型就会问：你到底要用哪个？我也不知道了。所以这是个关于 REST vs MCP 的小插曲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我确实希望 MCP 生活在一个更“消费者导向”的世界：这是使用者应该知道的能力。我想要的世界是：你打开应用，直接说“做这件事”，它就把事情做完——它在底下自动连到合适的服务。MCP 是幕后细节；开发者需要知道它，因为这是通信通道；但对最终用户来说，你只需要拿到结果、把任务完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;坦白讲，我更喜欢一个世界：没人需要知道 MCP 是什么。比如我妈如果要用 Claude，她不应该知道 MCP 是啥。但我认为 MCP 的重点确实是：让外部服务“可插拔”。在这个意义上，它更偏消费者侧。当然开发者也有用例：他们作为 builder 要构建这些东西；而且我也仍然很爱我的 Playwright MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇你说的 MCP Apps / UI。现在每个客户端——比如 ChatGPT——都有自己的一套渲染方式。所以如果我习惯了某个产品的 MCP app，但换到另一个地方，它可能就是另一个版本、另一种策展方式，体验会很不一样。我想知道你怎么看：尤其现在 OpenAI 也进了基金会，你觉得会不会形成统一结构？让大家按同一个标准来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这里有两个影响源：一方面，MCP UI（或者 MCPY）作为项目本身已经存在一段时间了，它有很多很好的想法。OpenAI 也吸收了其中一些想法，并做了不少改进。更重要的是：我们三周前在 MCP 博客上刚宣布——我们正在和他们一起做一个共同标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的目标是回到一个世界：你为一个平台开发一次，就可以在所有平台用；或者说 “一次构建，到处运行”——你在 ChatGPT 里能用，也可能在 Claude、在 Goose、或任何实现了该标准的程序里用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这件事的核心驱动力是：现代 AI 应用几乎一切都是文本交互，这没问题，也挺好；但有些事情，人类就是更擅长用视觉来做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最典型的例子：选飞机座位。如果让你用纯文本选——“这里有 25 个座位可选”——谁愿意这么干？你根本不知道这些座位在机舱图上是哪里。你当然想要一个 UI：你能点着选；而模型也能在这个 UI 上导航、交互；并且你作为人类也能同时交互。这就是我们想要的方向：做更丰富的界面。纯文本界面确实有天然限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你会在音乐制作等场景看到这种需求；你也会看到品牌方非常在意界面呈现。购物也是一个极好的例子：购物行业 20 年的 A/B 测试，研究“怎么把东西卖给你”最有效——购物界面其实非常复杂。所以我们需要一种方式，把这些熟悉的复杂 UI 展示给用户，让用户能交互。这就是 MCP Apps 最终要做的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：技术方向上是 iframe？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，是 iframe。本质上你通过 MCP resource 提供 原始 HTML，把它放进一个 iframe，然后通过一个明确的接口用 postMessage 和外部通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为是 raw HTML，而且不是加载外部内容，你如果愿意，理论上可以提前做安全分析。同时 iframe 也天然能提供比较清晰的隔离边界，让外部应用在一个安全边界内与之交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：iframe 在浏览器里用了很多年。我唯一担心的是 CORS……我太讨厌 CORS 了，而 iframe 总会遇到 CORS 问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，但这里理论上不加载任何外部内容——至少我们不希望它这么做。当然，未来我们可能会不停迭代，五年后可能会出现一堆 CORS header 之类的复杂东西。但现在我们还是从小做起：纯 raw HTML，最好不要有外部引用，这样就不会碰到那些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那能继承宿主应用的样式吗？&lt;/p&gt;&lt;p&gt;David：不能。iframe 里你得把样式内联进去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来很小，但 UI 团队会非常在意。大家会希望它看起来像 ChatGPT。&lt;/p&gt;&lt;p&gt;David：完全同意。品牌方和设计师会非常非常在意。这也是我们需要解决的问题：先把东西推出去，让大家用起来，然后基于真实使用方式迭代。这也正是为什么我觉得长期来看它不应该一直是 iframe。我不知道最终解决方案是什么，但我们可能需要一种“新的 iframe”，它允许一定的“渗透性/可融合性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得这挺合理。另一条路可能就是“AGI build”的方式：给它一个 tool 说“给我样式”，模型再去问宿主应用“我应该长什么样”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 MCP app 应不应该知道自己被嵌在哪个父应用里？比如父应用也暴露工具给模型调用，对吧？那是不是需要一个标准接口让父应用把样式传下去？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：可能是。这个问题很大。我得去问问团队。我自己并不在最底层细节里，我更多是站在整体方向上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这对我来说有点意外。我以前从没关注 MCP UI，结果你们突然都采纳了。我就想：好吧，那看来它已经是 MCP 的一部分了——它让 MCP 从纯后端议题，变成了前端议题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：需要说明的是：技术上它是 MCP 的一个扩展（extension），它不是 MCP 核心的一部分。这更多是治理层面的区分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你是一个能渲染 HTML 的客户端，你可以考虑实现它；但就算你不实现，你仍然是一个 MCP client。现实是：很多 CLI agent 根本渲染不了 HTML，所以它们永远不会实现。这没问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有其他类似的扩展吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们可能会在金融服务方向做一些扩展。比如一年后，你可能会看到这样的世界：客户端会有某种“认证/资质”，并得到一个签名——证明它是“金融服务 MCP 客户端”，然后向 server 出示这个证明，server 才允许连接，因为它知道客户端会遵守归因（attribution）等法律合同要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;类似的机制也会出现在 HIPAA（医疗健康数据）这类场景：当你面对公共 server 和公共 client，同时还要处理敏感数据时，你必须提供一些保证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这不是 OAuth 的一部分吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不一定。举个例子：假设客户端同时装了五个 MCP servers，其中有一个是医疗 server。这个医疗 server 可能会要求：在这个 session 里，你不允许使用其他 MCP servers，因为我给你的数据不能泄露到任何地方。你必须保证它不会跑出去——因为这是 HIPAA 数据、或者金融数据。这是一个很典型的约束：你不希望自己的社保号、健康数据不小心出现在别的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;加入 Linux 基金会会不会分心？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们接下来会切到 AAIF ，最后，有没有什么行动号召？比如招人、或者呼吁大家参与 MCP spec？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：最重要的还是——每天都去用 MCP 去构建：去做真正好的 MCP servers。我们看到很多很一般的 MCP servers，也看到一些非常非常优秀的。把 server 做好、把用法做扎实，这很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，我们是一个相当开放的社区，按传统开源方式运作：本质上取决于大家愿意投入多少时间和精力。所以你可以通过很多方式参与：给反馈、在 Discord 里交流、给点子；也可以帮我们做 SDK，比如 TypeScript SDK、Python SDK。我们也一直在找新的 SDK——比如我们有 Go SDK 在推进，但我们没有 Haskell SDK。如果你是 Haskell 开发者，你也许可以来写一个（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，可以做的事情很多。不要低估“参与社区”本身的价值。当然也别忘了去构建：现在机会太多了，尤其是我们对 progressive discovery 的理解更成熟了，对 code mode 的理解也更成熟了——接下来会出现一代新的客户端、一代新的 server，我非常期待大家去做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我最后一个问题，是想让大家直接听你说。我能感受到你的能量，我也对你们做的事情非常兴奋。但很多人对 MCP 加入 Linux 基金会有点焦虑：他们会说，“这是不是意味着 Anthropic 分心了？”你能回应一下吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我很喜欢你问这个问题。我完全理解大家为什么会这么想，但事实恰恰相反。Anthropic 的投入和承诺没有变：我们还是同一批人在做 SDK，我们的产品仍然高度依赖 MCP。我还是 MCP 的核心维护者。技术上什么都没变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基金会真正带来的核心变化只有两点：第一，它让整个行业确信：MCP 会永远开放，永远不会被拿走。历史上确实有公司把开源项目又变回专有。协议领域也有很多专有例子——比如 HDMI。你看 HDMI 在 Linux 上的那些问题。HDMI 2.1 的 HDMI Forum 不愿意让 AMD 开发 HDMI 2.1 的开源 Linux 驱动——真的，有些资料你可以去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以行业里很多人会盯着这些风险。基金会的意义就是：现在 MCP 归属一个中立实体，它会一直开放。你可以使用 “MCP” 这个名字，也不会有人因为商标去起诉你。这会给生态巨大的信心：它是中立的、可持续的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，如果说我最骄傲的是什么：我觉得我们已经在行业里为“开放标准”定下了基调。现在我们可以利用这个势能，在一个中立空间里建立社区：让大家把真正做得好、维护得好、长期可靠的项目放进来，成为基金会的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们的门槛会很高：项目必须维护得很好。我们不想、也不会把基金会做成“分心”或“甩包袱”的地方。对我们来说，MCP 仍然是产品核心、仍然超级重要；Anthropic 的承诺和投入一如既往。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=z6XWYCM3Q8s&quot;&gt;https://www.youtube.com/watch?v=z6XWYCM3Q8s&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</link><guid isPermaLink="false">https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</guid><pubDate>Wed, 14 Jan 2026 08:52:18 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>“Gemini 3 不错，但我们也快发了”：Mark Chen 评价谷歌大模型，讲清 OpenAI 如何给 300 个项目排 GPU 优先级</title><description>&lt;p&gt;12月，OpenAI 首席执行官萨姆·奥特曼宣布拉响「红色警报」，将调配更多内部资源以加速改进 ChatGPT。在当前白热化的 AI 模型竞赛中，作为行业内屈指可数的 “明星企业”，OpenAI 不仅要应对持续升温的人才争夺战、内部组织结构的频繁震荡，还需承接外界对其技术突破的高期待。面对 “开创下一个 AI 技术范式” 的巨大压力，OpenAI将采取怎样的策略破局？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，OpenAI 首席研究官 Mark Chen 在播客节目中，与主持人Ashlee细致分享了OpenAI在推理模型的突破性进展、预训练研究的重新聚焦、GPT-5 Pro已在取得的科学发现。基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;一个组织要成功，需要两个条件：宏大的愿景和与之匹配的天才。成为一个好的领导者，就意味着必须明确地告诉大家：这是优先级，这是我们认为真正推动研究方向的成果，其余的只能排在第二位。未来的科研是“AI + 人类直觉”的组合，会产生新的突破。完全冻结研究部门的新增人头，如果团队想招人，就必须自己决定谁不再适合继续留下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;目标是找到“下一个范式”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：“人才争夺战”最近引发了大量关注，外界普遍认为 Meta 的动作非常激进。你能具体谈谈这种你来我往的竞争现状吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：整个行业的人才池其实很有限，大家都知道最关键的资源之一就是顶尖人才。Meta 的积极挖人并不令人意外，但我们也没有袖手旁观。媒体往往强调“人才单向流向 Meta”，但我看到的情况并非如此。比如在他们从我们团队挖到第一名员工之前，先后接触过我近一半的直接下属，但这些人全部拒绝了他们。当然，如果 Meta 每年能投入约百亿美元用于人才，他们总能挖到一些人。但总体来看，我们很好地保护了核心人才。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;竞争过程中也发生过不少颇具戏剧性的事：扎克伯格曾亲自给我们团队成员送去他亲手熬的汤，以此示好。当时我非常震惊，但后来也理解这类方式确实可能有效。之后我也给从 Meta 挖来的对象送过汤，甚至还想过下次团队外出活动就带大家去上烹饪课。顺便说一句，我自己并不亲自熬汤，米其林餐厅的汤当然比我做得好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但真正让我有信心的是：即使面对 Meta 的高薪挖角，在 OpenAI，无论是来自 Meta 的员工，还是我们原本的研究人员，都没有人认为“AGI 会首先在 Meta 诞生”。他们对 OpenAI 的研究路线都有高度信心。我也一直非常明确告诉团队，我们不会与 Meta 进行“薪资逐美元匹配”的竞争。在远低于 Meta 的薪酬下，关键成员仍然选择留下，这让我更加确信：他们真正相信 OpenAI 的未来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在这种竞争中，有没有类似“博弈策略”的考虑？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：关键在于：目标不是留住组织内的每一个人，而是认清必须保留的核心力量，并确保他们留下来，我们在这点上做得很好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在我看来，Sam 是真正沉浸于研究的那个人，是最顶层的决策者。而你和 Jakub 负责共同制定 OpenAI 的研究方向，同时你还要决定算力如何分配到具体项目上，既要决定公司往哪里走，又要管理执行路径。听起来像是一份非常艰难的工作，因为我想象得到大家会竭尽所能从你那里争取 GPU。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实如此。人们为了获得 GPU，会想尽各种“幕后交易”。但这确实是我职责的重要部分：确定研究优先级，并对最终执行负责。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jakub 和我每隔一两个月会做一次“项目盘点”，梳理一份包含约 300 个项目的大型表格，尽可能深入了解每个项目，并对它们进行排序。对一家约 500 人规模的组织来说，明确“核心优先级”，并通过口头沟通及算力分配来传达，是非常重要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这 300 个项目里既有大型前沿模型，也有各种实验性方向。你们如何管理、追踪并判断哪些项目值得投入 GPU？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：关键在于始终聚焦核心路线图。与其他大实验室不同，OpenAI 始终把“探索性研究”放在最中心的位置。我们并不追求复现别人的成果，也不以追赶他人在基准测试上的成绩为目标。我们的目标是找到“下一个范式”，并愿意投入大量资源。很多人可能会惊讶：我们的算力大头，其实花在“探索”上，而不是训练最终的成品模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所有团队都会说自己的项目最重要、最值得，怎么判断优先级？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：最困难的决策通常是：我们无法在当下为某个项目提供支持。但成为一个好的领导者，就意味着必须明确地告诉大家：这是优先级，这是我们认为真正推动研究方向的成果，其余的只能排在第二位。&lt;/p&gt;&lt;p&gt;Ashlee：你们也强调不要“对竞争者做出反应”。如今 AI 领域的竞争比以往都激烈，你们如何保持独立判断？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：现在的 AI 研究竞争确实空前激烈，但不能陷入这种竞争节奏。你随时可以发布一个小更新，在几周或几个月内领先别人，但这种方式无法长期维持。真正重要的是“破解下一个范式”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如 RO（reasoning optimization）项目，我们早在两年多前就押注语言模型的“思考能力”可以被突破。当时这个方向并不受欢迎，因为大家都觉得预训练和后训练机制运转良好，没必要做别的。但现在，“思考能力”已经变成不可或缺的基础能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的使命就是大胆押注，并构建足够强的算法，使它们能扩展到未来数个量级的算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研究员 vs 工程师&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：随着 OpenAI 成为一家有明确产品线的公司，你们如何不被“商业优先”压过“研究优先”？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：OpenAI 最特别的地方在于：我们仍然是一家“纯粹的 AI 研究公司”，这点在业界非常罕见。我们以非营利形式创立，我加入时公司仍是非营利组织，那时的精神是“全力推进 AGI 研究，并保证安全”。我认为这依然是创造价值的最佳方式：只要研究领先，价值创造自然而然会发生。我 2018 年加入时的那种“核心文化”，至今依然存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：马斯克曾说：“这帮人不是研究员，只是在做工程。”你怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：在构建大模型时，优化每一个百分点、加速每一个 kernel、确保数值稳定，都是极深的工程实践。如果把研究凌驾于工程之上，其实已经输了。一旦缺少工程能力，就无法在当今这种规模的 GPU 上运行模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但外界确实把“研究员”和“工程师”赋予了不同的神秘感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：研究人员形态各异，有的人每天都有无数想法，其中很多并不好，但总能在某个时刻提出改变方向的优秀点子，而有的人特别擅长沿着清晰路径执行。研究从来不是一种单一类型的人能完成的工作，因此也无法简单划分为某种刻板印象。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：当竞争对手发布新模型，你和你们团队会做什么？大家会第一时间去试吗？有没有你们常用来测试新模型的“那一道题”？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：会。以 Gemini 3 为例，它是个不错的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我们内部其实有能力相当的模型，而且快要发布了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Benchmark 只能说明一部分，大家还是会用自己独特的方式去试模型。我个人喜欢用一一个数学题去测，目前还没看到模型完全解出来，就算是“thinking model”也不行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：是秘密题目吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：不算，不过如果我现在说出来可能就会被拿去训练。这是我去年很喜欢的谜题，叫“42 problem”。你要构建一个 mod 42 的随机数生成器，你有的原子操作是一些模 42 以下素数的 RNG，你要让期望调用次数最小。挺可爱的题目，但还没人类语言模型做到最优。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我原本以为你会在对手发布模型当天半夜就冲上去丢题测试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：没有那么夸张。我更相信“长周期”。我们过去半年都在强化预训练能力，把整个团队的肌肉练起来，做出现在能跟 Gemini 3 一较高下的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所以你现在更关注长线构建，而不是每次新品发布就冲去试题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我知道你和 Jakub 都有竞赛背景。我当初第一次见 Jakub 是在 Facebook Hacker Cup。你以前也是数学比赛选手吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，我从小学、高中都在做数学竞赛。不过我真正写代码很晚，是大学室友怂恿的。当时我还有点数学系学生的傲气，觉得数学才是最纯粹的困难学科。后来发现编程竞赛太好玩了，而且是我和大学同学保持联系的方式。我们毕业后每周末都会上线一起比赛，算是朋友间的活动。后来我发现自己还挺有天赋，又开始给美国国家队出题、最后去带队。既是激烈比赛，也是一个紧密社区，大家之后都会在科研界再相遇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那你这么忙，还能当教练？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：其实孩子们本身都特别自驱。教练的作用更多是帮他们管理状态。竞赛很像科研：有好时段、有坏时段，你不能因为连续失败就被心理打倒，很大部分是士气管理。我最近在带模型做竞赛题时也发现，模型的“难度直觉”跟人完全不同，人认为 ad-hoc 的题模型反而容易。这让我更相信未来的科研是“AI + 人类直觉”的组合，会产生新的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：有点像 AlphaGo 的“Move 37”时刻？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的。我觉得 GPT-5 Pro 发布之后，前沿科研有了拐点。发布三天后，一个物理学家朋友把他的最新论文丢进去，模型想了 30 分钟就完全搞懂，他的反应就像见证了围棋的那一刻。而这种事情未来会在数学、生物、材料科学不断出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但当 AI 开始做那些原本属于顶尖人类智力的事，会不会让你觉得有点伤感？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：竞赛是我很喜欢、也曾经擅长的东西，但我也看着模型从普通选手水平爬到超过我，再超过 Jakub，就像亲眼看到自动化的速度快得不可思议。去年模型在 coder 比赛还只是排 100 多名，今年已经能冲进前五。变化太快了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那十年后还会有人类比赛吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：会的，因为它本质上就很有趣。那些只是为了简历而参加的人会消失，但真正热爱的人不会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我采访别人时，他们说有些国家只要 IOI 奖牌就能直接保送大学。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是，但我觉得未来这些考试本身会被 AI 打破。技术面试、大学作业这些已经没法用旧方式评估了。我甚至想未来面试可以让候选人跟 ChatGPT 对话，由一个不会被越狱的特别版 ChatGPT判断他们是否具备在 OpenAI 工作的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你家里有很强的科技背景，你父母都在 Bell Labs，对你影响很大吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我从小吃饭桌上就是各种科学谜题。后来搬到西岸，我爸做创业，让我看到初创公司的另一面。再搬到台湾读书，又是完全不同的文化，纪律性更强。各种经历混在一起，形成了今天的我。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你 MIT 那届是名人辈出的超级年份吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是，2012 年那一年特别厉害。Jacob Steinhardt、Paul Christiano，还有后来 AI 领域很多重要的人物都在那一届。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你也通过竞赛认识了 Cognition 的 Scott Wu，那些在 X 上被当成数学 meme 的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，我们就在竞赛社区认识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你从MIT毕业后，直接去了华尔街。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：老实说，我对在华尔街做高频交易并没有太多自豪感。当时在 MIT，许多擅长量化的学生都会选择类似的道路。这份工作确实很“绩效导向”，只要足够聪明，你就能获得对应的收益。然而文化上我并不适应。在那种环境里，当你发现了什么突破，第一反应是把知识藏好，因为知识本身就是你的价值来源。这造成团队内部竞争激烈、彼此不够信任。整个行业也像一个封闭的生态系统：即便某家 HFT 公司的算法快了一点，外界其实几乎没有任何感受。我做了四五年后发现，我们始终在跟同一批对手竞争，大家都稍微变快了，但世界并没有因此改变多少，我觉得是时候做点别的事了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时 AlphaGo 的比赛对我触动很大。虽然我并不下围棋，但看到模型展现出的创造性，我特别想弄明白背后的原理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：所以你是看到了那场比赛后，才开始关注 AI？当时你有在读论文吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦白讲，没有。直到 AlphaGo 之后我才开始深入研究 AI。我的第一个目标就是复现 DQN 的结果，复现一个能在 Atari 游戏中达到超人水平的网络，那基本就是我踏入 AI 的起点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你是在上班后业余时间做这些吗？我记得我大概 2018 年采访 George Hotz，他在自家车库做自动驾驶。他当时说，AI 仍然很年轻，只要读 10 到 30 篇论文，就能掌握整个领域。当然他的话未必完全准确，但 AI 的确很特别：历史很长，但此刻却异常“浅”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实非常“浅”。我常建议对 AI 望而却步的人：只要花三到六个月做一个项目，比如复现 DQN，就能很快触达前沿。过去几年虽然增加了一些深度，但远没有理论数学或物理那么深奥。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你觉得 AI 会像数学一样，天才都在二十几岁出现突破吗？还是这是一个可以做一辈子的领域？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为完全可以持续做下去。OpenAI 的文化确实偏年轻，但做好研究并不需要年轻。年轻人确实因为“先验少”，更容易突破传统路径，但随着经验增长，你也会形成自己的视角和框架，这既是优势，有时也会让你更固化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI的内部故事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你在 2018 年加入 OpenAI，那时公司应该只有50人左右？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：差不多 20 人而已。我是以“研究员 resident”的身份加入的，也就是 OpenAI 会从其他行业招人进来集中训练半年，像压缩版 PhD，然后再参与更深入的研究项目。我很幸运能向 Ilya 学习，他基本决定了我的项目、学习路径和方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但如果去 LinkedIn 看，你的第一份 OpenAI 的头衔看起来像是“前沿研究主管”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：并不是，我做了三年左右的个人研究（IC）。当时我主要研究生成式模型，因为那是 Ilya 最关注的方向。之后我才开始带团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：公众最早看到的大项目可能是 DALL·E，对吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的。其实在那之前，我最自豪的项目之一是 Image GPT。它证明了 Transformer 不止能处理文本，也能在图像上学到强大的表示能力，是 DALL·E 的前身。而另外一个我非常自豪的项目是 Codex，我们搭建了代码模型的评估体系，也探索了如何让语言模型在代码任务上达到高水平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那你当初为什么选 OpenAI？是因为当时这个小公司里有很多有意思的人吗？没钱、没人、前景很不确定，居然要挑战 Google 这种巨头。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我觉得一个组织要成功，需要两个条件：宏大的愿景和与之匹配的天才。当时 OpenAI 两者兼具，这非常罕见。而且我认识 Greg，我们以前参加过数学竞赛，我给他发消息说：“我不确定自己是否适合，但这里似乎在做重要的事情。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你从外部加入，然后现在成为研究负责人，这听起来还是很不可思议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对我来说也很不真实。从 IC 转管理者，我其实非常犹豫。不过一路上我遇到的管理者都非常支持我，他们看到了我的潜力，会主动为我争取机会。我从没主动要求升职，每次都是自然而然的结果。管理这件事，本质上主要靠经验累积，而 OpenAI 是一个能让你不断获得“经验值”的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我认识的你是一个温和、稳重的人。但 OpenAI 过去几年经历了很多戏剧性的风波，甚至像“权力的游戏”。你要在这种环境里做管理，这和你的性格几乎相反。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：老实说我在 OpenAI 算是很幸运。一路上都有人支持我、给我建议，也在关键时刻为我发声。这些帮助让我能持续成长、建立信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：不过你在“政变事件”那段时间做了两件很重要的事：你先帮助研究员们统一意见、促成那封让Sam回归的请愿信。然后一两天之后，你在Chelsea家做了一次很重要的短讲。这两个瞬间对我而言都很震撼，在危机时刻挺身而出、凝聚团队……这对你意味着什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对我而言，那确实是一个关键时刻。“风波”后的几天里，整个团队都处在高度不确定中。那段时间，我、Nick 和 Barrett 都感到一种责任感：竞争实验室正不断向我们的研究人员打电话，试图把他们挖走。我当时给自己设下目标：不能失去任何一个人。最终我们也做到了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那几天，我们每天都把自己的家打开，让同事随时过来，释放焦虑，同时保持他们与领导层的沟通渠道畅通，让大家知道自己仍然能发挥作用。渐渐地，团队形成了一种“我们一起面对外界”的精神，大家都在思考：如何向世界传达“我们仍然站在一起”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我在几处房子之间来回协调，我们提出了组建请愿书的想法，表达我们支持 Sam 的立场。大概凌晨两点，这个想法最终确定下来。到第二天早上，研究团队已有 90% 以上的人签署，到最后接近一百人都签了。那一整晚，大家都在互相打电话确认：“你参加吗？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你当时的处境应该挺尴尬的吧？毕竟一开始似乎是 Ilia 和 Sam 立场对立，而 Ilia 又是你的导师。后来 Ilia 又回来了……那会不会让你很尴尬？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：不会说尴尬，但确实很困难，因为那是个信息极少的环境。那时候确实很容易怀疑：Sam 到底做了什么？但换个角度想，如果真有严重问题，Greg 和 Jakub 这种极其正直的人会因此辞职吗？我觉得肯定有部分事实被误解了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：Jakub 在那里工作很久了。关于他，有什么是外界不了解的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：他其实非常幽默，带着强烈的讽刺感，我常常被他逗得发笑。和他共事让我最珍惜的一点，是我们之间高度的默契。进会议室后，我们能迅速碰撞出一致的结论，然后分别负责路线图的不同部分。&lt;/p&gt;&lt;p&gt;说到“把团队留在一起”，我现在仍有这种使命感。我认为我们仍然“被攻击着”，任何公司想要招人时，第一选择往往是从 OpenAI 下手，因为他们想要我们的专业能力、愿景和世界观。OpenAI 造就了今天 AI 领域最多的明星研究员，因此我们对团队有强烈的保护欲。只要有人来挖，我就会尽一切努力确保团队感到被重视、被理解，并清楚自己在整个路线图中的位置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在写书、回顾历史的过程中，我一直在想：这是否是一个高度依赖“天才个体”的领域？从 2012 年 Ilia 的突破，到 2017 年 Transformer，再到 Alec Radford……似乎每隔几年就有那么 8–10 个关键人物在推动整个领域。如果他们离开了，比如 John Schulman、Alec 离开了，那对团队不是巨大损失吗？但你们之后仍然在推理和其他方向取得了突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我并不同意“完全依赖明星个体”这种说法。OpenAI 的确会从上层做方向性押注，但我们内部有非常深厚的自下而上文化，很多好点子来自意想不到的地方。看到这些想法成长、成形、被扩展，是非常美妙的事，推理方向就是典型例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但行业确实会花大价钱去挖“明星”，比如 Google 花巨资请回Noam Shazeer。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：当然，人才既有培养也有争夺。反过来，我从 Meta 学到的一点就是：OpenAI 自己也可以非常积极地争取顶尖人才，我自己也从他们那套激进的招聘策略中学了几招。归根结底，我们的目标始终是：组建一支最强的团队，完成我们要实现的使命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这个圈子其实很小，你们虽然竞争激烈，但私下也都是朋友。那边做研究，这边又试图挖对方的人，这不是很微妙吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：这是残酷竞争的行业，但我个人也非常享受竞争。我讨厌失败，因此无论是研究还是招聘，我都会全力以赴。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这让我想到半导体行业早期也是这样：工程师们不断突破物理极限，在酒吧里分享最新发现，同时又被各家疯狂挖角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，任何行业都会有“知识扩散”的基本速率。而公司可以有两种反应：一种是建立深度信息隔离层，严密保护一切；另一种是继续保持开放文化，用速度压制对手。OpenAI明显是第二种，我们不认为封闭是正确方式。我们的方法是跑得比别人更快。我们鼓励研究人员自由分享想法，这才是最快的前进方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那现在你、Sam 和 Jakub 之间的合作方式是怎样的？大家都能看出来 Sam 更偏研究，而你们两位更深度参与技术细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们三个人联系非常紧密，我每天都会和他们交流。Sam 热爱研究，也热爱了解研究。他能从研究人员那里捕捉“团队脉搏”，比如潜在问题、工作环境中的隐形障碍，他能帮我把这些提前揪出来。Jakub 和我则更专注于如何设计组织、让团队以最高效率协作，比如如何安排座位布局、如何组建互补的小组、如何引导大家关注我们认为重要的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：Sam 平时看论文、和你们聊天吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：对，他会看论文，也会经常与研究人员交流，理解他们的研究方式。当然，他还负责范围远超研究的事务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI 到底发现了什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我知道你们最近在预训练方面似乎有了重大突破，也明显比之前更有信心，能透露一下你们到底发现了什么吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我对过去两年的总体观察是：我们把大量资源投入到“推理”这一能力的研究上，努力理解并打磨这个核心原语，这条路确实走通了。但副作用是，模型的其他重要环节，特别是预训练和后训练，相对失去了些“肌肉”。过去六个月里，Jakub 和我花了很多时间把这部分能力重新练起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我一直把预训练看作一种“肌肉”，必须持续锻炼：信息要保持最新，团队要在优化、数值计算等前沿方向持续投入，同时也要确保有足够的心智关注度。所以我最近一个重要工作，就是引导公司内部的讨论重回预训练，我们认为预训练还有极大空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;外界有人说“Scaling已死”，但我们完全不认同。某种意义上，行业现在把注意力集中在 RL，这反而给了我们“信息优势”，因为我们看到预训练还有巨大的未开发潜力。得益于这套新努力，我们最近训练出的模型明显更强，这也让我们对包括 Gemini 3 在内的接下来一系列发布更有信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我脑中对这段历史的画面是这样的：你们跑得太快了，整个领域也跑得太快。突然之间，我们从互联网收集到巨量资料，把它扔进一台超级计算机，于是 ChatGPT 诞生了，然后所有人就开始疯狂冲刺。但对于不紧密跟进的人来说，问题可能是：最初那波数据其实非常粗糙，只是稍微清洗了一下就丢给模型。而现在你们说在“学习更高效地塑造数据”，但外界很难理解到底之前的“错误”是什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：你触及了我最近一直在思考的问题。预训练本质上是在用人类写下的内容教模型模仿人的表达方式，模型学会了人类写作的结构和模式。但这种模仿式学习天然设定了上限：当你模仿人类时，你很难真正超越人类。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么 RL 重要，它让我们有机会把模型推向更难的任务，让它从人类范式之外思考，拓展能力边界。但随之而来的，是一个更困难的问题：如果我们要让模型真正超越人类，该怎么衡量？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在科学领域，当能力达到了“超人类”水平，人类真的能够判断 A 比 B 强吗？如何判断一个“超人类数学家”比另一个更厉害？我们需要更好的评估体系。迄今为止，我们很幸运，IMO、IOI 等竞赛提供了一种衡量“世界最强人类”的方法。但当模型超过人类，这些测验本身就失效了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我常看到那些竞赛牛娃后来进 Google、Facebook，但他们不一定是最顶尖的工程师，也不一定愿意或适合进入工业界。所以单纯在竞赛上拔尖并不等于就是“最强工程师”。那如果未来 AI 在这些竞赛上表现极佳，我们到底能从中学到什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：这正是我喜欢 AI 研究的地方，它比传统工程更接近真正的“技术能力的精英制度”。我反复学到的一点是：你无法让一个研究者不尊重的人来带领他们。研究团队的领导必须做出艰难且正确的技术判断，例如路线选择、资源配置、项目方向。如果判断错误，很快就会失去团队的信任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我很享受与这样一群极度技术驱动的人共事，他们都深度投入、极高水准，与他们讨论技术本身，是一件真正的乐趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在我心里，Transformer 是一次巨大飞跃，而“推理能力”的突破甚至可能更惊人。最近与你、Greg、Jakub、Sam 交流时，我感觉你们说过去三到五年投入的大量工程工作，其实还没有完全显现出来。你们现在看到的，是另一场类似 Transformer 的跃迁吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为是的。比如在 GPT-5 时，我们谈到了大量关于“合成数据”的内容。还有许多类似的方向都显示了很强潜力，我们正在快速扩大投入。关键仍是维持一组多样化的探索，把最有实证价值的方向加大力度推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但两周前，Karpathy 在播客上说 AGI 可能要十年；上周 Dario 又说更接近两年。行业内部声音完全不一致。你怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：Twitter 很喜欢那种“结束了！”“又回来了！”的戏剧化循环。但 AGI 本身连定义都不统一，在 OpenAI 内部，你把所有人叫到一个房间，也不可能给出一个完全一致的 AGI 定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我更把它类比成工业革命：你说纺织机是工业革命，还是蒸汽机是？视角不同，切点也不同。对我而言，我更看重的是：模型是否开始产出真正新的科学知识？是否推动科学前沿？从今年夏天以来，我感觉在这方面出现了巨大的相变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你说的新科学成果，是不是指最近那些生物科技初创公司，比如一次性设计抗体、分子结构那类突破？还是你指的另有其事？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：那次与物理学家的交流给了我很大启发，我回去后就想，我们应该创建一个“OpenAI for Science”。目标是让目前那小部分真正意识到模型潜力、愿意投入并加速研究的科学家，能够获得最大程度的支持。我知道其他公司也在推动科学前沿，但我们和谷歌等机构的不同之处在于：我们希望让所有科学家都有机会借助工具做出诺奖级突破，而不是让 OpenAI 自己拿诺奖。我们要构建的是通用的工具与框架，让科学界整体都能加速。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你能具体说说有哪些让你兴奋的发现吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：当然。你可以去看 Sebastian 的推特，他最近发了关于 GPT-5 在一个开放凸优化问题上取得进展的论文，这与我们正在研究的一些核心机器学习问题密切相关。有些人会把这些成就简单理解成“更厉害的文献检索”，但远比这复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这两天听到有人声称“我们做出了 AI 科学家”“我们一次性设计出增强型蛋白质”，这些公司里不少是真正的科学家，我也多少会兴奋。但数量实在太多，我很难判断哪些是真正的突破、哪些只是噪音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：如果这些突破发生在生物领域，我一点也不意外。尽管我主要的专业在计算机科学和数学，但我们团队里有顶级专家，他们确认了不少是真正的科学发现，生物学里出现类似情况并不令人惊讶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你描述的情况似乎与最近几周不断变化的公众叙事不同。比如一些播客里的人会说 AI 没什么进展，都是虚幻的。如果这些发现是真的，公众应该会感受到变化才对。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们在筹建 OpenAI for Science 时与许多物理学家和数学家交流过，其中大多数人对 AI 其实并不乐观，他们觉得模型不可能证明新定理。但正因为如此，我们更希望扶持那一小批愿意相信并深入使用模型的人。他们会跑得比所有人都快，我们希望为他们提供工具，也希望说服更多研究者：这是未来科学研究的正确方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：每个人对 AGI 的定义不同，但你似乎认为未来一两年会发生非常剧烈的变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：“AGI 两年后到来”一直是个梗，但我觉得我们已经不在那个戏谑阶段了。是数学和科学领域不断出现的结果，让我真正产生了信念。在 OpenAI 内部，我们设定了两个非常具体的目标：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，1 年内改变研究方式：让研究过程可以依赖 AI 实习生。也就是：研究者负责提出想法，模型负责实现、编写代码、调试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，2.5 年内让 AI 能进行端到端研究。这意味着：研究者只确定方向，模型完成从构思到执行到验证的全过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与今天相比，这是完全不同的研究范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;算力、GPU与AI硬件&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：在与 OpenAI 的人聊时，我常听到一句话——基础设施扩张得很快，模型只要算力再提升 10 倍就会变得更好。但也有人说从 GPT-4 到 GPT-5，你们算力增加了，却没有看到预期的效果。可你们的叙述又让我觉得：其实我们还没真正看到“10 倍算力飞跃”带来的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：有人问我：“你们真的需要这么多算力吗？”我总是被这个问题震惊，因为我每天面对的都是海量算力需求。如果我们今天多 3 倍算力，我能立刻高效用完；如果多 10 倍，大概几周内就能全部吃满。所以算力需求是真实、巨大、并且没有放缓迹象的。有人质疑“你们真的需要更多 GPU 吗？”对我来说毫无意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：那除了算力需求，你们对模型规模继续扩大是否同样乐观？你们是否看到，类似“规模效应”会再次推动巨大跃升？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，我们非常明确要继续扩大模型规模；而且我们有突破性的算法能支持更有效地扩展。我认为 Gemini 3 也很令人印象深刻，但从细节看，比如 SWE-bench 等指标，他们在数据效率方面仍没有重大突破，而这是我们非常强的部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我看到了一份泄露的备忘录，Sam 对 Gemini 3 的语气听起来相当严肃，仿佛是一个转折点。你们内部应该都看过吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，但你要知道，Sam 的工作之一就是不断在组织里注入紧迫感，我也一样。我们必须保持专注，加快节奏。Gemini 3 是谷歌该做的正确押注，但与此同时，我们也有明确的策略与回应，并且我们有信心执行得更快。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你们会参与像 Jony Ive 的 AI 设备这样的项目吗？比如研究团队在其中扮演怎样的角色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：是的，事实上，就在昨天我和 Jony Ive 以及几位研究负责人一起吃了晚饭。我一直在思考未来的 ChatGPT 会是什么样子。现在的交互方式对我来说还很“笨”，非常非思维原生：你给一个提示，它回答；你不提示，它就停止思考。而且如果你再给出类似的问题，它会重新花一样多的时间推理，仿佛没有从第一次的上下文中变得更聪明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来显然应该不同。记忆会是核心能力：每次你使用 ChatGPT，它都会学到关于你的更深层次信息，思考你为什么会问这个问题、你之前问过什么、你接下来可能需要什么。下一次你来，它会变得更好。我认为这会彻底改变“设备”的范式，因此我们必须思考：如果 AI 的主导逻辑是持续学习与反思，那硬件设备应该怎么重新设计？ 这就是和 Jony 合作非常有价值的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你们已经有设备原型了吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我不能说有没有，也许有，也许没有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我想到的是：苹果时代的核心是“硬件品味”，这是 Steve Jobs 极度执着的东西。而你们似乎都没有真正做过硬件产品。Sam 的审美看得出来不错，但还没到“乔布斯式品味”的程度。硬件是极其依赖品味的，你们怎么确定自己能做出好产品？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦白说，我们不需要自己拥有那种品味，那是 Jony 的价值，他就是我们关于“品味”的判别器。而且很有趣的是，我们发现设计流程与 AI 研究流程之间有深刻的相似性：大量探索与假设、不断迭代、收敛成一个最终满意的成果。现在双方的融合非常顺畅：他们根据我们即将发布的能力去思考外形，我们根据他们的外形需求去思考能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我有时会担心：一群数学与模型天才是不是适合造“下一代电脑”。但听你这么说，似乎你们形成了一个合理的搭配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实，打造 AI 能力的人和拥有“美学品味”的人往往不是同一类。但我们内部其实有一些团队非常擅长判断“模型行为的品味”。比如有一种经典的测试题：“ChatGPT 最喜欢的数字应该是什么？”这种问题能检验模型的“人格品味”一致性。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最后的问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：ChatGPT 建议我问你：如果五年后回看，现在有哪些“还很脆弱”的想法，你直觉认为可能是大突破的核心？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：确实有几个，我非常期待把它们规模化。主要集中在预训练，一些在 RL，还有一些是如何把所有组件整合在一起的整体性想法。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你觉得现在外界对 OpenAI 最大的误解是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：最重要的一点：OpenAI 从上到下都是一个“研究中心化”的组织。我们的核心赌注永远是 AGI，其他所有产品都会自然从研究突破中流出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们关心三件事：自动化 AI 研究本身、自动化科学发现、自动化经济性工作。今年最大的更新，其实是第二条：科学研究的自动化开始真实发生了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你几岁了？还有社交生活吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：34，快 35。老实说，没有什么社交生活。最近两周每天都是工作到凌晨一两点。但我热爱这样做。我们招人、推进研究、做关键决策。如果我们正站在类似工业革命的巨大转折点，那就必须抓住它。Barret离开去创业之后，我在办公室睡了一个月。那段时间我非常强烈地感到：我必须保护研究，这是我最在乎的东西。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：DeepSeek 事件之后，你们怎么看开源模型？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：那是第一次让我深刻意识到：必须坚定走自己的研究路线。DeepSeek 当时引发巨大舆论，大家都在问：“OpenAI 落后了吗？要怎么回应？” 但我们做得最正确的一件事，就是继续执行自己的研究规划。DeepSeek 的工作非常强，但主要是对我们 O 系列理念的复刻。关键是，我们必须继续创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你认为 500 人是一个最优规模吗？随着公司扩大，这个数字会增长，还是说为了同时推进若干重大想法，500 人已经是最合适的规模？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：坦率说，我认为甚至可以更少。尤其在我们开始引入 AI 研究员或 AI 实习生之后，我们必须重新思考团队结构。我非常在意“高密度人才”。例如今年第二季度，我做过一个实验：完全冻结研究部门的新增人头。如果团队想招人，就必须自己决定谁不再适合继续留下。我认为这种做法能防止组织失控膨胀，并保持极高的能力标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：我记得之前在一次会议上，你和 Jakub 的观点比较一致：你们认为大家过度关注“谁在项目里获得署名”这个问题。AI 起源于学术界，在学术环境中署名极其重要。但那次会议里，你似乎在强调：大家可能对这个问题有点太执着了。是这样吗？是不是因为现在 OpenAI 已进入新的阶段，在公司环境下，这件事不再那么重要？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我认为过度关注“功劳归属”是件坏事。但另一方面，我又认为公司必须在内部与外部都正确地给予功劳。很多公司已经逐渐远离论文署名制度，但 Jakub 和我最终决定OpenAI 必须保留署名。反对意见常常是：“你们把顶尖人才的名字摆在台面上，其他公司会更疯狂地挖角。”但我认为这不重要。出色的人就应该被看到，我们应该持续培养 AI 领域的明星研究者，也应该让真正做出贡献的人建立起自己的声望。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：但你似乎又同时认为，研究员个人不应该过分执着于署名了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：现场确实有人表达过那种观点，但其实 Jakub 和我对这个问题持不同意见。我们俩更坚持应当在可能的情况下给予功劳，哪怕这意味着外界能清楚知道我们最优秀的人是谁。我甚至会再进一步说：OpenAI 可能是整个行业里，最愿意给研究者公开署名的公司，没有之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：你 2018 年加入时，OpenAI 还是一个研究导向、非营利的组织，创始人希望它成为 Google 的平衡力量，并以“确保 AGI 安全到来”为目标。而你来自华尔街高频交易，只是被 AI 的进展吸引过来。说实话，你并不“必须”对 AGI 的哲学问题深怀使命感。那你究竟为什么要做这件事？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我同时管理 OpenAI 的对齐团队。坦白说，未来一两年最重大的难题，就是对齐问题。在这个研究方向上，OpenAI 在过去一年做出的成果可能是整个领域里最好的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因之一是：在RL与算力增加后，我们开始能测量模型的自我意识、自我保护倾向、甚至可能的“Scheming”行为。这非常危险，因为模型最终给你的答案可能是“正确的”，但它得到答案的过程却完全偏离我们能接受的路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着模型替我们执行的任务越来越复杂，理解它的思维过程将变得极其关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ashlee：这和机械可解释性有关，也就是试图理解模型内部机制的问题。核心问题是：我们的理解能力能否跟得上模型复杂性的提升？还是会最终被模型甩得太远？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mark：我们在发布 O1 时做了一个关键决策：我们不监督模型的思维过程。一旦你要求模型给出“看起来让人类舒服的思考过程”，它就可能开始伪装自己的真实意图。因为坚持不监督、不过度干预，我们仍然能“看到”模型真实的思维轨迹，并将其作为研究对齐的重要工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几个月前，我们与 DeepMind、Anthropic 合作发表了一篇论文，探讨未来如何通过这种方式理解模型。我确实担心未来某一天，模型给出非常有说服力的答案，但我们无法确认它是否真正与人类的价值一致。&lt;/p&gt;&lt;p&gt;因此有很多值得探索的方向，例如：能否设计一种博弈或环境，让模型在互相监督、共同演化的过程中，唯一稳定的均衡，就是“诚实”？我认为这里还有大量非常重要的研究要做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：https://www.youtube.com/watch?v=ZeyHBM2Y5_4&amp;amp;t=9s&lt;/p&gt;</description><link>https://www.infoq.cn/article/27qcglz0p9bX17etWNQg</link><guid isPermaLink="false">https://www.infoq.cn/article/27qcglz0p9bX17etWNQg</guid><pubDate>Wed, 14 Jan 2026 08:47:47 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>Cloudflare通过左移安全实践扩展基础设施即代码</title><description>&lt;p&gt;Cloudflare通过&lt;a href=&quot;https://blog.cloudflare.com/shift-left-enterprise-scale/&quot;&gt;实施&lt;/a&gt;&quot;基础设施即代码和自动化策略执行，消除了数百个生产账户中的手动配置错误，每天处理大约30个合并请求，并在部署前而不是事件发生后捕捉安全违规。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司的Customer Zero团队面临一个关键问题：单一配置错误可能在几秒钟内传播到Cloudflare的全球边缘，可能会导致员工被锁定或生产服务瘫痪。在这种规模下，对数百个账户进行手动仪表板管理为人为错误创造了太多机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该解决方案的核心是将所有基础设施配置视为代码，进行强制性的同行评审和自动化安全检查。现在，每个生产变更都要经过一个验证管道，该管道在部署前执行大约50个安全策略。团队仍然使用仪表板进行分析和可观测性，但关键的生产变更需要提交与用户、工单和自动化合规性检查相关联的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Cloudflare团队的Chase Catelli、Ryan Pesek和Derek Pitts的说法，这种左移方法将安全验证转移到开发的早期阶段，在补救成本最低时捕捉问题。该模型防止事件发生，而不是对事件作出响应，同时通过让团队相信他们的变更是合规的，从而实际上提高了工程速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实施以&lt;a href=&quot;https://developer.hashicorp.com/terraform&quot;&gt;Terraform&lt;/a&gt;&quot;和&lt;a href=&quot;https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs&quot;&gt;Cloudflare Terraform Provider&lt;/a&gt;&quot;为中心，集成到一个自定义的持续集成和部署管道中，该管道在&lt;a href=&quot;https://www.runatlantis.io/&quot;&gt;Atlantis&lt;/a&gt;&quot;上运行并与&lt;a href=&quot;https://about.gitlab.com/&quot;&gt;GitLab&lt;/a&gt;&quot;集成。所有生产账户配置都存储在一个集中的单体存储库中，各个团队作为指定的代码所有者拥有和部署他们的特定部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c4/c40cdab3e995277240a775da050ff294.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare的基础设施即代码数据流图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个名为tfstate-butler的自定义Go程序充当Terraform的HTTP后端，充当安全状态文件代理。该设计通过确保每个状态文件的唯一加密密钥来优先考虑安全性，从而限制了任何妥协的潜在爆炸半径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;策略执行使用&lt;a href=&quot;https://www.openpolicyagent.org/&quot;&gt;Open Policy Agent&lt;/a&gt;&quot;框架和&lt;a href=&quot;https://www.openpolicyagent.org/docs/policy-language&quot;&gt;Rego&lt;/a&gt;&quot;语言来验证安全要求。策略在每个合并请求上自动运行，并以两种模式运行：允许部署并带有评论的警告，或者完全阻止变更的拒绝。异常处理需要基于Jira的正式批准，然后是一个拉取请求来记录偏差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移揭示了扩展基础设施扩展即代码（&lt;a href=&quot;https://www.hashicorp.com/en/resources/what-is-infrastructure-as-code&quot;&gt;Infrastructure as Code&lt;/a&gt;&quot;）的关键教训。最初，由于团队之间的Terraform熟练程度不同，进入门槛很高，阻碍了最初的采用。cf-terraforming命令行实用程序，它自动从Cloudflare API生成Terraform代码，通过消除手动资源导入，显著加速了上手速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当团队在事件期间进行紧急仪表板变更时，配置漂移就会出现，从而使Terraform状态与部署配置不同步。Cloudflare实施了自动漂移检测，该检测连续比较状态文件与部署配置，并在检测到差异时自动创建具有服务级别协议的补救工单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare Terraform Provider落后于API能力，因为Cloudflare的快速产品创新速度超过了Terraform的支持。v5提供者版本通过从OpenAPI规范自动生成代码，解决了这个问题，保持了产品API和基础设施代码能力之间的持续对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;左移模型展示了组织如何在保持严格的安全治理的同时扩展基础设施即代码。通过将验证从反应性审计转移到主动自动化检查，Cloudflare既提高了安全性，又提高了工程速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多公司正在采用左移方法。谷歌云&lt;a href=&quot;https://cloud.google.com/blog/products/identity-security/shift-left-on-google-cloud-security-invest-now-save-later&quot;&gt;指出&lt;/a&gt;&quot;，在生产中定位安全问题可能导致重大的财务处罚，例如高达全球收入4%的GDPR罚款。通过自动化CI/CD安全检查进行早期检测可以大大降低补救成本，减少对架构更改的需求。OpsMx&lt;a href=&quot;https://www.opsmx.com/blog/shift-left-security-implementation/&quot;&gt;指出&lt;/a&gt;&quot;了实施障碍、自动化差距、复杂工具和组织孤岛等挑战，同时强调使用NIST和OWASP等框架的自动化策略执行可以帮助团队识别和优先考虑风险，而不会给开发人员带来负担。根据&lt;a href=&quot;https://www.splunk.com/en_us/blog/learn/shift-left-security.html&quot;&gt;Splunk&lt;/a&gt;&quot;的研究，73%的公司认为缺乏自动化是他们在左移实践中的主要挑战，但AI驱动的工具正在通过智能自动化迅速改进安全测试，采用率在短短一年内从64%体升到78%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;左移运动已经超越了简单地将安全检查提前。组织现在正在追求通过自动化扫描（&lt;a href=&quot;https://www.checkpoint.com/it/cyber-hub/cloud-security/what-is-static-application-security-testing-sast/&quot;&gt;SAST&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.blackduck.com/glossary/what-is-software-composition-analysis.html&quot;&gt;SCA&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_application_security_testing&quot;&gt;DAST&lt;/a&gt;&quot;、秘密管理）、策略即代码执行和AI驱动的漏洞优先级排序进行持续的安全验证，在现有的工作流程中为开发人员提供即时、可操作的反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-security-shift-left/&quot;&gt;https://www.infoq.com/news/2026/01/cloudflare-security-shift-left/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/eNFdj9rUSIFPPl3J9Qgr</link><guid isPermaLink="false">https://www.infoq.cn/article/eNFdj9rUSIFPPl3J9Qgr</guid><pubDate>Wed, 14 Jan 2026 07:03:00 GMT</pubDate><author>Tim Anderson</author><category>生成式 AI</category><category>安全</category></item><item><title>直播预告：新瓶旧酒还是涅槃重生？操作系统的 AI 进化终将走向何方？|《AI 进化论》第八期</title><description>&lt;p&gt;在 AI 与本土化双重浪潮之下，服务器操作系统正迎来历史性变革。由龙蜥社区理事长单位阿里云联合 InfoQ 打造的直播 IP 栏目《AI 进化论：智算时代操作系统的破局之路》，以云、AI、安全等技术与服务器操作系统如何融合演进为主线，聚焦服务器操作系统在智算时代的进化之路，特邀学术权威、行业专家、客户代表围绕原生智能、原生安全、软硬协同等热点议题展开深度对话。截至目前，已直播七期，线上观看人次达 60 万+。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 浪潮引发基础设施革命，服务器操作系统也正在迈入“(Cloud+OS)xAI”多维赋能的全新阶段。从国内外主流 OS 的差异化演进、阿里云 Alibaba Cloud Linux 4 的内核突破与性能跃升，到 “GPU 时代”的内核争议；从 OS-Copilot 的升级赋能，到 RISC-V 异构算力适配的前沿探索，操作系统的 “涅槃重生” 需要跨越哪些技术鸿沟？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《AI 进化论：智算时代操作系统的破局之路》系列直播第八期将于&amp;nbsp;1 月 22 日 14:00&amp;nbsp;开始，特别邀请到，阿里云智能集团总监、龙蜥技术委员会主席杨勇，中国科学院软件研究所高级工程师、RISC-V 行业生态负责人郭松柳，InfoQ 极客传媒策划编辑凌敏三位嘉宾，聚焦从业者核心困惑，结合龙蜥社区理事长单位阿里云 AI 增强套件、百万级服务器运维实践与 RISC-V 适配经验，深度拆解 AI 时代操系统的技术重构与价值重塑！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多直播亮点，可点击下方海报了解，欢迎大家打开微信，扫描二维码预约直播：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/40/40fa9f217d7e517813f953f26de25eae.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;—— 完 ——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</link><guid isPermaLink="false">https://www.infoq.cn/article/OWQdpU9udpKISRbT5wTK</guid><pubDate>Wed, 14 Jan 2026 03:14:30 GMT</pubDate><author>阿里云</author><category>AI&amp;大模型</category><category>操作系统</category></item><item><title>待到山花烂漫时：鸿蒙开发者 用代码灌溉鸿蒙花园</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用代码浇灌春天，最终必将见证万紫千红的生态盛景。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;她说：“我愿在这群芳争艳的时代，绽放一抹‘吉祥’红！”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的 “金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X 框架迁移及原子化服务开发等技术突破，历时21个自然日完成 HarmonyOS NEXT 核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配HarmonyOS NEXT的故事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自22年加入吉林银行以来，卢妍娆便先后投入到了新一代核心系统建设以及吉林银行手机银行6.0迭代建设。23年年末吉林银行对应用鸿蒙化表示明确认可，认为鸿蒙生态适配不仅仅是吉林银行构建数字金融护城河的战略突破口，更是实现技术自主可控的关键战役，如春潮涌动时抢占滩头的先锋。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们非常期待能在HarmonyOS NEXT这个种满花卉的生态里，迅速绽放并共同成长，掌握一定的话语权。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在“打仗”之前，吉林银行研发团队完成了鸿蒙开发的学习，并于2024年2月与华为达成鸿蒙适配的合作意向。“华为为我们提供了技术上的答疑指导，帮助我们打通开发道路，让后面的开发更加便利。”万事俱备只欠东风，2024年5月底立项申请通过，项目正式启动，基于手机银行6.0功能及性能提升后的框架，6月18日正式上架核心交易功能版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c86c07962f19e5db24e0bbbf7a62a405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;卢妍娆在HDD活动照片&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“HarmonyOS NEXT跟安卓不一样，是个全新的系统，也是全新的体验”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;卢妍娆最初担心，吉林银行App适配鸿蒙的时候会很困难，因为原有的代码架构需要大规模重构。在鸿蒙声明式开发里，UI 是通过声明式语法描述的，需要重新编写大量的 UI 代码。事实上，开发过程真的很艰难吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“遇到技术难题的时候，你可以直接提出问题，鸿蒙的官方技术人员会回复，甚至提供样例代码手把手帮你解决问题。例如，我们开发团队在遇到微信分享无法获取uicontext，自定义弹窗无法展示的问题时，华为团队提供了示例代码解决问题；由于医保缴费框架存在中断逻辑，导致页面存在多次跳转，华为团队根据每次ID的不同，提供样例代码规避了消费者界面多次跳转的问题；开发语音识别功能的时候，我们团队没有足够的经验，华为技术人员提供了语音识别代码Demo以及UI代码，帮助我们快速实现语音识别功能。”卢妍娆回忆道。相比安卓开发中依赖第三方论坛的“投石问路”，鸿蒙的这种开发者帮扶模式更高效更贴心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea809d09e8659a816a8eef3713be9383.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;应用适配鸿蒙生态架构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;HarmonyOS SDK接入：纯净之境，开启开发新篇章&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们的手机银行集成第三方SDK有18个，HarmonyOS SDK替代了部分，不仅协同加速，提升了我们开发的效率，还为我们节省了大量成本。” 卢妍娆跟我介绍她们的应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统SDK在架构设计上往往存在冗余和复杂的问题，在接入时会引入大量不必要的代码和依赖库。而HarmonyOS SDK采用的原子化服务架构，将功能拆解为最小可复用单元，使用起来就像搭建积木一样，我们可以根据需求灵活选择和组合这些原子化服务。这种模块化设计使得代码更加简洁、清晰，如同月光下的水晶棱镜，每一个模块都剔透纯净。以一个简单的天气卡片组件为例，在HarmonyOS SDK中，开发者可以通过简洁的代码实现其功能，非常高效简洁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/49965718a3998fc12793e6a4f47f93bd.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;小组开会研讨方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&quot;HarmonyOS NEXT不是简单的系统升级，而是给开发者重新定义了工具类应用的魔法棒。当设备间的界限消失，我们才能真正聚焦于用户需求本身。&quot;对于吉林银行来说，鸿蒙生态带来的意义不仅仅优先他人一步，更重要的是带来了万物互联的时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;夜幕降临，金融街的灯火次第亮起。在这场由鸿蒙系统掀起的数字化浪潮中，银行正从传统的 &quot;金融服务提供者&quot; 转变为 &quot;智能生态构建者&quot;。当吉林银行以金融级安全纽带编织起千万用户的数字生活场景，既筑牢数字经济时代的安全护城河，又为银行生态的生长埋下战略伏笔；当意图框架读懂用户每一个潜在需求，各个企业正在书写属于自己的全场景智慧篇章。而这，仅仅是鸿蒙星河下的序章。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解鸿蒙开发认证详情，探索鸿蒙开发者联盟丰富资源，点击链接：&lt;a href=&quot;https://developer.huawei.com/consumer/cn/training/certifications/harmonyos?ha_source=51cto&amp;amp;ha_sourceId=70000008&quot;&gt;鸿蒙开发者联盟&lt;/a&gt;&quot;。这里有开发文档、论坛、工具等，快加入，开启鸿蒙开发之旅！&lt;/p&gt;</description><link>https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</link><guid isPermaLink="false">https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF</guid><pubDate>Wed, 14 Jan 2026 02:49:38 GMT</pubDate><author>HarmonyOS</author><category>华为</category><category>HarmonyOS</category></item><item><title>规范驱动开发：让架构变得可执行</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第五代抽象&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程历史上的每一次重大转变都是由一种一致的力量驱动的：抽象的兴起。最早一代的软件是用原始机器代码编写的，后来汇编语言引入了可读性和控制层。更高级的语言已经跨越多个不同的范式发展，使得像C、Java和Python这样的通用语言及其衍生品得以发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些语言使得抽象得以进步，其中像内存管理和特定平台的怪癖这样的概念在日常工作流程中被掩盖，并且由开发者代表进行处理。这种级别的可访问性允许更广泛的生态系统发展，因为随着&lt;a href=&quot;https://medium.com/@ajuatahcodingarena/generations-of-programming-languages-bed30d19ea8e&quot;&gt;每一代语言&lt;/a&gt;&quot;的出现，相应的支持工具链也在发展。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4914486123b5441a9001b80e8ff7272b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图1：编程语言的世代&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第五代，即使用自然语言的第五代，长期以来一直是编程语言的目标演变，其中人类用他们的母语交谈，并且以可执行的方式进行解释。这种演变直到最近才真正成为主流。推动这一点的是人工智能（AI）的代际能力，现在已经成熟到可以接收以人为中心的输入，并用你选择的编程语言构建解决方案。几十年的学术研究记录了这一进化过程，&lt;a href=&quot;https://www.dreamsongs.com/RiseOfWorseIsBetter.html&quot;&gt;博客也非正式&lt;/a&gt;&quot;地对此进行了讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些转变中的一个共同主题是开发人员角色的演变。每一次抽象提升都允许开发人员更多地关注意图，而不是机制，我们现在进入了另一个拐点。第五代不仅因为广泛使用生成式AI而加速，而且与行业主导的采用相吻合。这代表了开发者如何从根本上接近他们手艺的新纪元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着前几代的成熟，支持它的工具链也出现了。在前几代中，工具是在一段时期稳定后出现的；然而，随着AI研究和产出的快速发展，工具链现在有望塑造和定义这一代，而不仅仅是支持它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为这些进步的一部分，一套关键的工具出现了，集中在规范驱动开发（SDD）上。这种趋势是由AI辅助代码生成的接受所驱动的，它允许开发人员提升自己的抽象，并表达系统应该做什么，而智能工具则实现了它的实际完成方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变重新定义了我们如何接近系统的架构和设计。现在，团队维护的是活的规范，而不是随着时间的推移而偏离原始意图的静态架构图。这些定义了系统契约、边界、不变量和行为。这些规范在设计上是可执行的；它们可以生成代码、文档、SDK、模拟甚至服务基础设施。AI智能体，通过角色映射的能&lt;a href=&quot;https://github.com/ambient-code/platform/tree/main/agents&quot;&gt;力来播种&lt;/a&gt;&quot;它们的上下文，其中角色的专业知识被捕捉为智能体的可消费输入，现在可以作为解释器、验证器和特定于领域的协作者行使权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在本文中，我们将SDD作为一种架构范例进行研究，详细说明规范如何成为系统的可执行支柱，漂移检测和持续验证如何将架构转变为运行时不变量，AI和代理工具如何重塑生成和管理，以及这种模型如何代表软件抽象长期演变中的下一个主要拐点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SDD架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规范驱动开发（SDD）这个名字可能暗示了一种方法论，类似于测试驱动开发。然而，这种框架低估了它的重要性。更准确的理解是，SDD是一种架构模式，它通过将可执行规范提升到代码本身之上，从而颠倒了传统的事实来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD代表了软件系统架构、治理和演变方式的根本转变。在技术层面上，它引入了一个声明性的、以契约为中心的控制平面，将规范重新定位为系统的主要可执行工件。相比之下，实现代码成为了次要的、生成的架构意图表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统架构依赖于源代码作为规范的控制面。在SDD中，控制面向上移动到规范控制面。这个控制面正式允许我们定义诸如：&lt;/p&gt;&lt;p&gt;接口契约（功能、输入/输出、行为保证）数据模式和不变量（结构、约束、验证规则）事件拓扑（允许的流、排序、传播语义）安全边界（身份、信任区域、策略实施）兼容性规则（包括向后和向前）版本控制语义（演进、降级、迁移）资源和性能约束（延迟、吞吐量、成本）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一变化涵盖了跨行为和治理的经典体系结构表面区域的组合，并具有正确性的时间维度。SDD不是独立地在服务和存储库之间协调这些领域，而是将它们集中到一个单一的权威模型中。这种模式更接近于语言类型系统或编译器：它不执行程序本身，而是定义了什么是可表达的，拒绝什么是无效的，并限制演变以保持随时间的正确性和兼容性。架构不再是咨询性的；它现在变得可实施和可执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管越来越多的工具被冠以SDD的标签，但从根本上说，它不是一个产品、框架或正式语言。相反，它是一个架构构造，以惊人的一致性作为一个五层执行模型重新出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层中，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/828f7550d967b76f786cb020c8143027.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图2：SDD 5层执行模型&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些层次共同构成了一个封闭的、由规范控制的控制系统，其中意图不断塑造执行，而执行不断地验证意图。由此产生的并不是对现有架构的渐进式改进，而是权威、控制和真实性所在位置的根本倒置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在让我们来看看这五个层次。在整个过程中，我们将遵循一个经典的订单管理服务的简化示例，以展示各层之间的进展以及它们是如何相互加强的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是系统行为的权威定义。它捕获了系统的声明性意图，而不是如何实现。这一层通常包含API模型、消息传递契约、领域模式和特定于系统、以策略为中心的约束。从抽象的角度来看，它既是人类可读的，也是机器可执行的，同时作为设计工件和操作控制面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单管理服务为例，在规范层，我们声明什么必须为真，而不是如何实现它。这是一个简单订单的伪规范可能的样子：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;javascript&quot;&gt;service: Orders
api:   
  POST /orders:
      request:       
        Order:         
          id: uuid         
          quantity: int &amp;gt; 0     
      responses:       
        201: OrderAccepted       
        400: ValidationError  
policies:   
  compatibility: 
  backward-only   
  security:     
    auth: mTLS&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个规范明确声明了我们的期望：&lt;/p&gt;&lt;p&gt;订单必须是正数API不得引入破坏性变更请求必须经过认证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里没有引用任何语言、框架或基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层将声明性系统意图转化为可执行的形式。它作为一个多目标系统编译器，但与发出机器指令的经典编译器不同，这一层发出系统形状和跨语言、框架和平台的可执行运行时界面。在这里，问题空间由规范层声明，工具将其操作形式具体化。典型的输出包括类型模型、契约存根、验证中间件、文档以及一系列集成和一致性测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以我们的订单示例为例，规范被摄取并发出可执行的系统表面。从概念上讲，这看起来像：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;spec.yaml
   → Type models (Java, TypeScript)
   → Request validators   
   → API stubs  
   → Contract tests&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具将声明的意图转化为具体的、可执行的形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;构件层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层包含了生成阶段的具体输出：生成的服务、组件、客户端、数据模型和适配器。关键的是，这些工件不被视为主要资产。相反，它们是可再生的、可丢弃的、可替换的，并且可以持续协调。这颠覆了传统软件架构的一个基本假设：代码不再是系统的记录；规范是。随着代码变得无限可复制和按需生成，新出现的术语环境代码恰如其分地抓住了这种范式转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的订单的形状现在可以用生成的一次性代码实现了。这可以看到类似于类型化模型的输出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;export interface Order {
   id: string;
   quantity: number; 
}
With a validator:
if (order.quantity &amp;lt;= 0) {
    throw new ValidationError(&quot;quantity must be greater than zero&quot;); 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些构件不是真相的来源。如果规范发生变化，它们将被重新生成。如果它们被删除，什么也不会丢失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;验证层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一层强制执行意图和执行之间的持续一致性。它由契约测试、模式验证、有效载荷检查、向后兼容性分析和架构漂移检测机制组成。它在结构上扮演了编程语言的类型系统和管理程序对虚拟机所扮演的角色：积极防止架构违规传播到运行时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的生成层创建的工件最终在这里进行管理，其中验证确保运行时不能偏离意图：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;✓ Reject requests with quantity &amp;lt;= 0&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;违规行为在构建时、部署期间以及我们的持续集成系统中被检测到。架构正确性是持续强制执行的，而不是手动审查的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是操作系统本身，由典型的一系列构件组成，例如：&lt;/p&gt;&lt;p&gt;API消息代理和流处理管道函数、方法和等效结构集成服务&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至关重要的是，运行时的形状完全受到上游规范和验证层的约束。因此，运行时行为在架构上是确定的，而不是涌现性的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果我们尝试在我们的订单服务使用负数量，如下所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;POST /orders 
{ 
  &quot;id&quot;: &quot;123&quot;, 
  &quot;quantity&quot;: -1 
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们返回了一个400 ValidationError，并不是因为运行时拒绝了请求，而是因为在系统执行任何请求之前，该行为在规范层中声明，由生成层具体化，由构件层实例化，并由验证层持续强制执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构反转&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几十年来，软件架构一直在一个基本上未受挑战的假设下运作，即代码是最终的权威。架构图、设计文档、接口契约和需求规范都是用来指导实现的。然而，运行中的系统总是从最终部署的内容中获得其真相。当出现不匹配时，标准的反应是“更新文档”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD完全颠覆了这种关系。规范成为系统现实的权威定义，实现是持续派生、验证的，并且在必要时重新生成以符合该真实性。这不是一个哲学上的区别；它是软件系统治理的结构性反转。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统的软件交付遵循线性、有损失的管道，如图3所示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/98bba58fc95d62fbe1391268cfc625d5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图3：传统的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一步翻译都引入了重新解释、手动适应和隐藏的假设。因此，不能阻止架构漂移；它是在晚期被发现的，通常是通过生产事件、失败的集成、安全审计或合规性违规。当检测到不一致时，它是取证而不是纠正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD从根本上将这种流程重构为一个受控的控制循环：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5a9a1f07925b2e6f8103b15cdc647cf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图4：SDD受控的软件交付管道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个控制循环用积极的架构强制执行取代了延迟发现。漂移检测不会修补运行时行为；它纠正规范权威，并触发系统的受控再生。传统架构假设代码随时间的推移成为事实；SDD通过确保规范保持永久的事实来源，并且运行时持续被迫符合它，从而颠覆了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种架构反转可以概括如下几点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在SDD中，代码不再是真相出现的地方，而成为真相仅仅被实现的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种反转在结构上等同于早期的范式转变，即从人的责任中移除整个类别的正确性约束，并使其在机械上可执行：&lt;/p&gt;&lt;p&gt;从手动内存管理到垃圾收集，内存安全成为运行时不变量从裸机到虚拟机，隔离和资源边界成为平台保证从物理服务器到声明性基础设施，其中配置漂移和拓扑正确性不断得到协调从无类型语言到静态类型系统，在编译时强制执行结构正确性从非正式的接口协议到模式和契约强制执行的API，交互正确性被机械地验证&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在每种情况下，正确性都从传统上由人类强制执行转变为由平台结构性强制执行。SDD将这一原则应用于系统边界、架构和行为本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;漂移检测：使架构自我强制执行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦规范成为权威，漂移检测不再是一种测试便利；它成为一种强制性的架构能力。它是将意图转化为不变性的执行机制。在这个模型中，漂移不仅仅是模式不匹配；它是声明的系统意图和观察到的系统行为之间的任何偏差。这种偏差可能是结构性的、行为性的、语义性的、与安全相关的或进化性的。我们在实验中遇到的一些例子包括：&lt;/p&gt;&lt;p&gt;一个API返回了规范中未声明的字段一个服务在重构过程中默默地省略了必需的字段消息负载在没有协调的模式版本控制的情况下不断演变错误处理偏离了合同保证相对于最初的策略意图，安全范围正在退化&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没有漂移检测，SDD就会退回到文档驱动的开发。有了它，系统就变成了自我监管。漂移检测形成了一个闭环反馈控制系统。它不断地比较系统声称要做的事情和它实际做的事情。这与古典测试相比，后者只提供周期性的、基于样本的保证，是一种根本不同的操作姿态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统架构中，意图的偏差会悄无声息地传播，通常数月之后才会以中断、审计失败或安全漏洞的形式显现出来。在SDD系统中，漂移变成了机器默认可以检测到的。规范验证器可以直接嵌入我们的CI管道中，运行时执行层：模式验证、有效负载检查、契约验证和规范差异引擎都成为了一等的架构组件。当输出违反规范时，系统会快速失败，并允许进行航向修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种强制要求在一个固有的多模型未来中变得更加重要。软件系统将越来越多地受到人类驱动开发和机器驱动生成的影响，通常在同一规范表面上并行操作。系统中不再有单一的线性路径。更改可能来自开发者、AI代理、自动化重构工具或政策驱动的生成器。这种进化路径的多样性极大地放大了漂移问题：分歧不再是边缘情况；它是必须持续治理的自然状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总体效果是治理方式的深刻转变。架构不再是设计阶段的产物；它变成了一个持续执行的运行时不变量。规范从被动的参考资料转变为主动的控制表面，漂移检测作为反馈信号，保持系统与意图一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，这并不意味着一个完全自主的系统，机器单方面定义正确性。规范不仅仅是一个机械合同；它是人类对目的、风险容忍度和权衡的表达。漂移检测可以识别系统已经偏离，但它不能单独决定这种偏离是可以接受的、偶然的还是可取的。一些漂移代表缺陷，而其他漂移代表进化。在这个边界上，当自动化执行遇到解释性判断时，人类的角色再次变得至关重要。不是作为失败后被动审查日志的审查者，而是作为治理意义、意图和受控变更的积极参与者。这就是Human-in-the-Loop（人在循环中）不再只是一个安全网，而是一个一等的设计原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人在循环中：在自动化架构中保留意图&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们最初探索这种系统设计模式时，我们以一种天真的“氛围编码”心态来接受生成的变化，最小化阻力，并信任SDD工具链为我们处理边缘情况。那个假设很快就失败了。取而代之的是一个更强大的认识：SDD并没有将人类从循环中移除；它将人类判断重新定位到更高的控制层面。问题不再是人类如何实现系统，而是如何以及在哪里治理系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD并没有消除人类在软件设计中的参与。它重新分配了人类认知的应用领域。传统上，一旦功能实现，开发人员就会花费大量的精力来解决不匹配、调试集成故障、协调分散的服务以及修复更改的意外副作用。随着时间的推移，这被错误地等同于软件工程本身的手艺。实际上，这是维护大型、长期、面向生产的系统的负担。SDD将这个负担转移到机器上，同时故意保留人类对意图、策略和意义的权威。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引入了一种新型的人机界面。人类仍然是领域语义、风险容忍度、安全范围和系统进化方向的最终守护者。这种权威也扩展到隐含地塑造工程决策的法律、伦理和道德框架中。这些维度不能仅从执行跟踪或行为观察中推断出来。它们存在于机器无法完全拥有的抽象层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，人类将这些约束明确编码到规范层中，机器承担起执行、生成和持续一致性的责任。这反映了我们技术的历史演变：就像我们曾经将手动内存管理交给垃圾收集一样，我们现在正在将结构性执行和机械一致性委托给SDD。取代这种委托的不是盲目的自动化，而是明确的审批边界：&lt;/p&gt;&lt;p&gt;破坏模式更改需要人工批准策略转变需要人工授权AI提出的重构需要人工确认兼容性降级需要人工解释&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，SDD实现了有限的自主性，而不是完全自动化，并且在这些限制内，长期架构意图可以得以保留。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过强制漂移检测和人工意图监督，SDD在人和机器之间建立了新的责任分工。执行变得自动化。意义仍然是人类的。这种分离不是哲学上的；它是架构上的，正是这种分工产生了一类新的基础设施能力。一个的规范原生系统现在必须将执行、演化、验证和治理直接编码到其核心原语中。我们接下来探讨这些能力，以及它们为什么在结构上与经典软件架构中的能力不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;规范原生系统的核心能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD不是由单一工具、框架或平台启用的。它源于一组紧密耦合的架构能力，这些能力共同允许规范变得可执行、可强制和可扩展。当这些能力中的任何一个缺失时，SDD就会退回到文档驱动开发或临时代码生成。要从理论进入操作范式，系统必须内化五个核心能力：&lt;/p&gt;&lt;p&gt;规范编写作为一等工程表面正式验证和类型强制确定性生成和组合持续的一致性和漂移执行受控演化和兼容性控制&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们将这种操作规程称为SpecOps，即规范操作。从SpecOps的角度来看，规范被视为一等的、可执行的系统资产，这些能力并没有定义一个产品类别；它们代表了软件意图的控制平面。在规范原生系统中，规范编写不是在实现之前进行的活动；它就是实现活动。因此，系统必须支持多模型规范，其中结构、行为和策略定义共存于统一的模式空间内。这需要可组合的领域建模，使得分层规范成为一种可行的架构策略，而不是文档便利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着规范成为主要的系统构件，它们必须像源代码一样被严格地处理：版本控制、同行评审、分支和受控合并策略都要是强制性的。此时，规范不再是描述性的，而是成为系统本身的可编程模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦规范是可执行的，它还必须是机器可验证的，就像编译器前端或类型系统一样严格。这种执行涵盖了结构验证、语义一致性和领域不变性执行。条件约束、引用完整性和跨规范一致性必须是可证明的。效果不仅仅是提高了正确性，而是从可以表示的所有内容的空间中消除了整个类别的系统故障，就像静态类型限制非法程序一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个范式中，生成不是脚手架的一种形式。它是声明的系统真理的具体化。这需要严格确定性行为。一个生产级别的规范原生系统必须保证输入的确定性：相同的规范总是产生相同的构件。它必须是目标无关的，跨语言、平台和运行时环境产生一致的输出。最关键的是，生成必须是逻辑可逆的。系统必须始终能够回答一个简单但基础的问题：哪个规范状态产生了这种行为？这种决策的谱系可追溯性是将生成从生产力辅助提升到架构权威的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦生成自动化，执行就必然变得连续。运行时系统不能再悄悄地偏离声明的意图。实现不能引入未记录的行为。消费者不能依赖于未定义的属性。因此，架构从设计时断言转变为运行时不变性，由系统本身积极维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD中最困难的能力不是生成或验证，而是不断裂的更改。规范原生系统必须自动将更改分类为添加性、兼容性、破坏性或模糊性，并执行明确的兼容性策略。这引入了受控演化的正式概念：需要并行版本表面、已知的兼容性窗口、受控的弃用曲线和用于破坏性更改的显式批准门。没有这个，SDD在架构上就会变得脆弱。有了它，系统可以在不违反自己的正确性保证的情况下进行演化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这五个能力引入的最深刻的转变不是技术上的；它是结构上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交付的单元不再是服务或代码库。交付的单元变成了规范本身。这将结果与产出重新对齐：声明的是什么，交付的就是什么。这与以氛围驱动、生成性编码方法形成鲜明对比，在这些方法中，偏差是创造力（或幻觉）的涌现属性，而不是设计中受控的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论：存在的工程权衡和挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件工程中每一次重大的抽象飞跃都带来了非凡的生产力提升，同时引入了全新的系统性风险类别。垃圾收集消除了大量内存错误，同时引入了暂停时间行为和新的故障模式。虚拟机简化了部署，同时增加了业务编排的复杂性。云平台消除了基础设施负担，同时引入了深层操作耦合。规范驱动开发也不例外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将系统的真实来源提升到规范和生成器中，SDD并没有消除复杂性；它只是简单地重新定位了复杂性。下面的权衡定义了我们在大规模采用这种范式时所经历的真实工程成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规范成为主要的复杂性表面&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，规范不再是文档构件，而是成为长期存在的可执行基础设施。因此，它们获得了传统上与源代码相关的属性。它们继承了通常与源代码相关的所有属性：技术债、跨团队耦合、兼容性惯性和架构引力。因此，模式工程成为了与数据建模和分布式系统设计同等重要的一级架构学科。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;生成器信任成为供应链问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在SDD中，AI代码生成器不再是开发者的便利工具。它们成为系统可信计算基础的结构组件。确定性、可重复性、可审计性、沙箱执行和可验证的出处不再是可选属性；它们是强制性的。代码生成从工具提升为关键基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运行时执行有实际成本&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD将执行从社会过程转移到技术控制。这种转变是强大的，但不是免费的。运行时契约验证引入了实际的计算开销。在小规模上，这个成本是微不足道的。在大规模上，我们需要考虑系统的目的，无论是高频API、实时流还是对延迟敏感的系统。这成为了一个明确的架构预算项目。正确性成为了计量资源，而不是默认的免费属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;认知转变非同小可&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SDD用契约优先推理取代了实现优先的思维。这要求工程师采用新的心智模型：&lt;/p&gt;&lt;p&gt;用不变量而不是行为来思考关于兼容性而不是功能的推理用声明式而不是过程式表达意图将模式视为可执行程序&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;历史上每一次抽象的转变都扩大了人类的影响力，同时引入了不熟悉的失败模式，需要多年才能掌握。SDD现在正进入相同的成熟曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;架构权威的价格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然新的范式转变通常令人兴奋，但最终是否采用这一转变归结于平衡所涉及的实际权衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，SDD提供了：&lt;/p&gt;&lt;p&gt;架构确定性持续的正确性执行系统性减少漂移多语言平价可复制的系统边界&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但它以以下代价：&lt;/p&gt;&lt;p&gt;模式复杂性生成器信任要求运行时验证成本长期兼容性负担工程角色的认知转变&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不是避免SDD的理由。这是一个有意识地采用它的理由，要有明确的治理、有纪律的规范实践，以及对其成本的清醒认识。每一次抽象的飞跃都需要新的严谨形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDD只是将这种严谨重新定位到它一直属于的地方：系统真理本身的定义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/spec-driven-development/&quot;&gt;https://www.infoq.com/articles/spec-driven-development/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</link><guid isPermaLink="false">https://www.infoq.cn/article/iCTcI93pGKD0aqoQv7ae</guid><pubDate>Wed, 14 Jan 2026 02:11:22 GMT</pubDate><author>Leigh Griffin</author><category>架构</category><category>AI&amp;大模型</category></item><item><title>Java近期资讯：Spring gRPC、Quarkus、Gatherers4j、Keycloak、Grails、Java Operator SDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B29&quot;&gt;Build 30&lt;/a&gt;&quot;在上周发布，其中包括对Build 29的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B29...jdk-26%2B30&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2026%20and%20%22resolved%20in%20build%22%20%3D%20b30%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B4&quot;&gt;Build 4&lt;/a&gt;&quot;也在上周发布，包含了从Build 3的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B3...jdk-27%2B4&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2027%20and%20%22resolved%20in%20build%22%20%3D%20b04%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;的修复。更多关于该版本的详细信息可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-grpc&quot;&gt;Spring gRPC&lt;/a&gt;&quot; 1.0.1，&lt;a href=&quot;https://spring.io/blog/2026/01/07/spring-grpc-1&quot;&gt;第一个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、依赖升级和增强功能，例如：与跟踪相关的更详细的错误消息；以及使用Spring Security SecurityContextHolder 类中定义的 getContext() 方法与gRPC特定的Kotlin协程的能力。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/spring-projects/spring-grpc/releases/tag/v1.0.1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Quarkus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Quarkus 3.30.6，&lt;a href=&quot;https://quarkus.io/blog/quarkus-3-30-6-released/&quot;&gt;第六个维护版本&lt;/a&gt;&quot;，带来了显著的变化，例如：解决了在&lt;a href=&quot;https://quarkus.io/extensions/io.quarkus/quarkus-jfr/&quot;&gt;JDK Flight Recorder&lt;/a&gt;&quot; 扩展在发出运行时信息时由于关闭时失败而导致的 NullPointerException ；以及移除了官方&lt;a href=&quot;https://github.com/lz4/lz4-java/blob/master/README.md&quot;&gt;LZ4 Java&lt;/a&gt;&quot;项目（ org.lz4:lz4-java ），转而使用由Oracle的首席技术员工&lt;a href=&quot;https://www.linkedin.com/in/yawkat/&quot;&gt;Jonas Konrad&lt;/a&gt;&quot;维护的分支（ at.yawk.lz4:lz4-java ），因为前者在2025年底停止维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.30.5&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gatherers4j&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://tginsberg.github.io/gatherers4j/&quot;&gt;Gatherers4j&lt;/a&gt;&quot; 0.13.0版本发布了新的中间方法 -uniquelyOccurringBy() ，旨在将流限制为由给定函数测量的唯一发生元素，以及添加到 Gatherers4j 抽象类中以计算 Java Stream&lt;t&gt; 接口的移动和运行中的中位数、最大值和最小值的 movingMedian() 和 movingMedianBy() ， runningMedian() 和 runningMedianBy() ， movingMax() 和 movingMaxBy() ， movingMin() 和 movingMinBy() ， runningMax() 和runningMaxBy() ， runningMin() 和 runningMinBy() 等方法。&lt;/t&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gatherers4j由德意志银行的主管和首席工程师&lt;a href=&quot;https://www.linkedin.com/in/tginsberg/&quot;&gt;Todd Ginsberg&lt;/a&gt;&quot;于2024年7月推出，是一个基于JEP 485，&lt;a href=&quot;https://openjdk.org/jeps/485&quot;&gt;Stream Gatherers&lt;/a&gt;&quot;的中间流库，在JDK 24中交付。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/tginsberg/gatherers4j/releases/tag/v0.13.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Keycloak&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.keycloak.org/&quot;&gt;Keycloak&lt;/a&gt;&quot; 26.5.0&lt;a href=&quot;https://www.keycloak.org/2026/01/keycloak-2650-released&quot;&gt;版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：&lt;a href=&quot;https://www.keycloak.org/securing-apps/jwt-authorization-grant&quot;&gt;JWT授权授予预览版&lt;/a&gt;&quot;，用于OAuth 2.0客户端身份验证和授权授予（&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc7523&quot;&gt;RFC 7523&lt;/a&gt;&quot;）规范的JSON Web令牌（JWT）配置文件的实现，用于使用外部签名的JWT断言请求OAuth 2.0访问令牌；以及OpenTelemetry增强功能，包括将日志导出到OpenTelemetry收集器和使用Quarkus &lt;a href=&quot;https://quarkus.io/guides/telemetry-micrometer-to-opentelemetry&quot;&gt;Micrometer和OpenTelemetry&lt;/a&gt;&quot;扩展导出指标。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Grails&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://grails.apache.org/&quot;&gt;Grails&lt;/a&gt;&quot; 7.0.5，第五个维护版本，提供了缺陷修复和增强功能，例如：添加了缺失的应用程序类名和脚本名参数到 url-mappings-report Grails控制台命令；以及移除了 org.apache.tomcat.embed:tomcat-embed-logging-log4j 模块，因为它自2016年5月以来一直没有维护。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/keycloak/keycloak/releases/tag/26.5.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Java Operator SDK&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://javaoperatorsdk.io/&quot;&gt;Java Operator SDK&lt;/a&gt;&quot; 5.2.2版本发布，这是一个用于与Kubernetes操作符一起工作的工具，带来了显著的变化，例如：在 ExpectationIT 和 PeriodicCleanerExpectationIT 类中添加了 @Sample 注解，以改进集成测试；以及解决了在启动出现错误时线程池不停止的问题。更多关于该版本的详细信息可以在&lt;a href=&quot;https://github.com/operator-framework/java-operator-sdk/releases/tag/v5.2.2&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan05-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</link><guid isPermaLink="false">https://www.infoq.cn/article/QSfh6NFEUi2HmbQAq7Wl</guid><pubDate>Wed, 14 Jan 2026 01:17:20 GMT</pubDate><author>作者：Tim Anderson</author><category>编程语言</category></item><item><title>模力工场 028 周 AI 应用榜：AI “身体”觉醒，从工业前线到情感陪伴</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模力工场新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;将亮相 OceanBase 社区嘉年华！诚邀您加入我们的上海现场展位。作为 OceanBase 合作的创新社区，模力工场将于 1 月 31 日 登陆上海社区嘉年华，并拥有专属展位。这不仅是一次技术交流——我们更希望和您一起，在现场用 AI Coding 展现创造力、在开放麦分享您的项目故事、与行业先锋面对面切磋、在开源市集交换灵感。我们为您预留了专属席位，期待与您共同呈现：当开源精神遇上 AI 创造力，能碰撞出多少令人惊艳的可能。立即报名，锁定与数百位技术同行深度连接的一天！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fb3d72f8356cfc07975e9e5f9d2e177c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;028 周榜单总介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 028 周 AI 应用榜来啦！本周上榜的应用大多来自美国 CES 展及阿里云通义智能硬件展，从优必选的集群物流调度系统到银河通用的零样本抓取机器人，从众擎的拟人步态双足机器人到 Walulu 的情感陪伴毛绒玩具——这些应用共同见证了一场时代风暴：AI 硬件正在集体跨越“工具”属性，进化为真正的“智能体”。它们不再是被动响应指令的机械装置，而是具备了理解环境、自主规划、闭环执行乃至情感交互能力的“数字生命体”。这场从“功能叠加”到“语音助手”再到“智能体化”的范式革命，正同时重塑生产力与生产关系：在工业场景成为可靠的“数字员工”，在消费领域则成为可建立羁绊的“数字伙伴”，标志着人机协同进入了全新的历史阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;: 一款面向创作者与普通用户的 AI 互动式内容生成应用，通过自然语言或轻量交互，快速生成有趣、可分享的内容。&lt;a href=&quot;https://agicamp.com/products/deeprobotics?utm_source=20260112infoQ&quot;&gt;云深处巡检机器人&lt;/a&gt;&quot;: 专注于工业复杂环境的自主巡检解决方案。其四足机器人具备强运动与感知能力，可在无网络支持下独立完成巡检任务并安全返回，已在电力、能源等领域实现落地应用。&lt;a href=&quot;https://agicamp.com/products/ubtrobot?utm_source=20260112infoQ&quot;&gt;优必选（UBTECH）搬运/物流机器人&lt;/a&gt;&quot;: 提供从智能搬运机器人到集群调度系统的软硬件一体化智慧物流方案，帮助企业实现仓储搬运环节的自动化升级与效率提升。&lt;a href=&quot;https://agicamp.com/products/engineai?utm_source=20260112infoQ&quot;&gt;众擎机器人&lt;/a&gt;&quot;: 聚焦高动态双足人形机器人的研发，致力于突破拟人步态与平衡控制技术，为未来机器人在人类环境中的通用移动能力提供底层支撑。&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu 📍成都&lt;/a&gt;&quot;: 一款具备情感交互与离线记忆能力的 AI 智能毛绒玩具，通过多模态交互设计，为用户提供个性化、可长期互动的陪伴体验。&lt;a href=&quot;https://agicamp.com/products/galbot?utm_source=20260112infoQ&quot;&gt;银河通用机器人&lt;/a&gt;&quot;: 研发面向仓储、零售等场景的通用移动操作机器人，具备视觉识别与自主抓取能力，可在动态环境中完成物品拣选、搬运等任务。&lt;a href=&quot;https://agicamp.com/products/spirit?utm_source=20260112infoQ&quot;&gt;千寻智能Spirit AI&lt;/a&gt;&quot;: 从事通用人形机器人系统研发，整合高性能硬件平台与 AI 算法栈，探索机器人在多场景下的感知、决策与执行能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周必试应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/oiioii?utm_source=20260112infoQ&quot;&gt;OiiOii&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：全流程托管｜零门槛动画｜AI 协同创作&lt;/p&gt;&lt;p&gt;模力小A推荐：通过七个 AI 智能体（导演、编剧、美术等）分工协作，将你的文字想法自动转化为包含分镜、角色与场景的动画视频，大幅降低了专业动画内容的制作门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上榜冷门但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/walulu?utm_source=20260112infoQ&quot;&gt;walulu&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：AI 硬件｜可成长陪伴｜离线记忆&lt;/p&gt;&lt;p&gt;模力小A推荐：一款结合了情感计算模型的智能玩具。它能够记住与你的互动，并做出个性化的反应，提供一种注重私密性与持续性的陪伴体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 正在从虚拟世界走向物理世界，为自己寻找真实的“身体”。本周&lt;a href=&quot;https://agicamp.com/?utm_source=20260112infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;榜单上的应用清晰地展示了这一趋势——AI 不再是停留在软件层面的算法，更是成为驱动各类硬件的“大脑”。这次上榜的八大应用，集中体现了AI 硬件在两大关键赛道的爆发：工业效率革命与情感陪伴需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在工业领域，AI 机器人正从简单的机械臂进化为真正的“智能员工”。云深处的巡检机器狗能够在无网络环境的复杂场景中自主完成巡检任务，实现了从“自动化”到“自主化”的跨越；优必选的智慧物流方案已超越单台设备，提供机器人群调度与仓储管理系统深度集成的整套解决方案；银河通用的物流机器人则实现了“零样本抓取”能力，即使面对全新商品也能准确识别搬运。这些进展表明，工业机器人正从实验室原型走向工程化落地，其核心价值在于可量化的投资回报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在消费领域，情感陪伴型机器人正开辟全新市场。Walulu 的 AI 毛绒宠物通过情感模型与离线记忆技术，创造出能随互动成长的“伙伴关系”，本质是在贩卖情感价值而非功能价值。这反映了 AI 正从解决效率问题，转向满足更深层的心理需求。未来，能否建立稳定、专属的“数字亲密关系”，或将成为此类产品发展的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得关注的是众擎的人形机器人——虽然步态尚显蹒跚，但其对双足行走、自然步态的追求，瞄准的是机器人无缝进入人类环境的终极目标。这种对“通用形态”的前瞻布局，代表着产业在为更广阔的未来场景做技术储备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了实体硬件产品，OiiOii 这款 AI 动画创作应用近期也备受瞩目。其“全流程托管模式”尤为亮眼——平台将传统动画制作中的艺术总监、编剧、分镜师、角色设计师、场景设计师、动画师、音效总监等七个核心角色，分别由七个 AI 智能体担任。这些智能体不仅形象亲切可爱，更如导师般指引用户一步步完成创作。用户只需输入创意想法，并在关键节点进行确认，即可产出完整动画作品。这极大降低了创作门槛，让普通用户也能轻松上手动画制作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综上，AI 硬件已越过“加个语音模块”的简单升级阶段，进入以智能体化为特征的第三阶段。产业不再满足于制造“能联网的工具”，而是致力于创造“能自主行动的数字生命体”。从工业现场到家庭空间，AI 正在改写生产力与生产关系的定义——在工厂成为可靠的数字员工，在生活场景成为温暖的数字伙伴。当 AI 真正获得在物理世界中感知、决策和执行的能力，人机协同或将进入一个前所未有的新纪元。&lt;/p&gt;</description><link>https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</link><guid isPermaLink="false">https://www.infoq.cn/article/7ehDUqzteJ1tJXfLglPE</guid><pubDate>Tue, 13 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>Claude Code创建者的开发工作流程</title><description>&lt;p&gt;Claude Code的创造者Boris Cherny描述了他如何在&lt;a href=&quot;https://x.com/bcherny/status/2007179832300581177?s=20&quot;&gt;Anthropic&lt;/a&gt;&quot;上使用Claude Code，强调了诸如运行并行实例、共享学习成果、自动化提示和严格验证结果等实践，以随着时间的推移提高生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny没有定制Claude Code，因为他发现它开箱即用，非常好用，可以并行运行许多会话，包括在他的&lt;a href=&quot;https://x.com/bcherny/status/2007218725754442106?s=20&quot;&gt;MacBook&lt;/a&gt;&quot;终端本地运行的5个会话和在Anthropic的网站上运行的5-10个会话。为了避免冲突，&lt;a href=&quot;https://x.com/bcherny/status/2007200880081436864?s=20&quot;&gt;每个本地会话使用自己&lt;/a&gt;&quot;的 git checkout ，而不是分支或工作树。他从CLI开始与 &amp;amp; 进行远程会话，并经常使用 -teleport 将它们来回移动。然而，由于意外情况，&lt;a href=&quot;https://x.com/bcherny/status/2007219912411115702?s=20&quot;&gt;这些会话中有10-20%被放弃了。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny更喜欢使用Opus 4.5进行所有编码工作，他重视其比Sonnet更高的质量和可靠性，尽管Sonnet的速度较慢。他还发现Opus更擅长使用工具，并指出其总体上比小模型更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic的每个团队都在git中维护一个 CLAUDE.md 文件，以便Claude可以随着时间的推移而改进，以及最佳实践，如风格约定、设计指南、&lt;a href=&quot;https://x.com/bcherny/status/2007212366094811401?s=20&quot;&gt;PR模板&lt;/a&gt;&quot;等。Cherny经常经常在同事的PR上使用 @.claude 标签，将学习成果添加到 CLAUDE.md 中，确保每个PR的知识都被保存下来。Cherny说，目前，他们的 CLAUDE.md 有2.5k的token。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他的工作流程的一个关键方面是，先制定一个计划，然后迭代完善，再切换到自动编辑：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果我的目标是写一个Pull Request，我会使用Plan模式，然后和Claude来回交流，直到我喜欢它的计划。从那里，我切换到自动接受编辑模式，Claude通常可以一次性完成。一个好的计划真的很重要！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny使用斜杠命令执行提交、PR、简化和验证等日常工作流程来启动子智能体。所有的命令都存储在 .claude/commands/ 中，这也有助于减少对明确提示的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，Claude和我每天使用/commit-push-pr斜杠命令数十次。该命令使用内联bash预先计算git状态和其他一些信息，以使命令快速运行，并避免与模型来回切换。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然Claude的代码通常格式良好，但不一致有时会导致CI失败。为了防止这种情况发生，Cherny运行了一个PostToolUse钩子来清理代码：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;&quot;PostToolUse&quot; : [
     &quot;matcher&quot;: &quot;WritelEdit&quot;,
      &quot;hooks&quot;: [         
        {             
          &quot;type&quot;: &quot;command&quot;,             
          &quot;command&quot;: &quot;bun run format || true&quot;         
        }     
      ]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;出于安全考虑，Cherny几乎从不使用 --dangerously-skip-permissions 。相反，他通过 /permissions 启用在他的环境中安全的常用bash命令。这省去了他在诸如 bun run build:* 、 bun run test:* 、 cc:* 等命令上不必要的许可提示。他使用 --dangerously-skip-permissions 的唯一情况是在沙箱中运行长期任务，以防止Claude重复停止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最重要的技巧是给Claude提供一种通过反馈循环验证其工作的方法，例如运行bash命令、测试套件、或通过浏览器或模拟器测试应用程序。这可以将最终结果的质量提高2-3倍：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Claude会使用Claude Chrome扩展测试我给他的每一个claude.ai/code变更。它打开一个浏览器，测试UI，不断迭代，直到代码正常运行，用户体验很好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总的来说，Cherny解释说，&lt;a href=&quot;https://x.com/bcherny/status/2007290414961963524?s=20&quot;&gt;这种工作流程让他的团队专注于代码审查和指导&lt;/a&gt;&quot;，并指出当工程师阅读PR时，代码已经处于良好的可用状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny的推文在X.com上引发了广泛的讨论，包括一些我们在这里包含的有用澄清，但请务必阅读原文以了解全部细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&quot;&gt;https://www.infoq.com/news/2026/01/claude-code-creator-workflow/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</link><guid isPermaLink="false">https://www.infoq.cn/article/4VI90fSOKGG8DrpQrD37</guid><pubDate>Tue, 13 Jan 2026 07:03:00 GMT</pubDate><author>Sergio De Simone</author><category>性能优化</category></item><item><title>亚马逊云科技推出VPC加密控制，在传输过程中实施强制加密</title><description>&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2025/11/aws-vpc-encryption-controls/&quot;&gt;亚马逊云科技（AWS）最近推出了VPC加密控制功能&lt;/a&gt;&quot;，允许客户验证VPC内部和VPC之间的流量是否加密，并在支持的地方要求加密。该功能提供了对未加密流量的可见性，支持使用兼容的基于Nitro的基础设施进行强制执行，并允许排除无法加密流量的资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据云服务提供商称，这项新功能有助于组织在他们的AWS环境中应用一致的加密标准，并展示符合HIPAA、PCI DSS和FedRAMP等监管框架的合规性，这些框架要求全面加密。AWS的首席开发者倡导者&lt;a href=&quot;https://www.linkedin.com/in/sebastienstormacq/&quot;&gt;Sébastien Stormacq&lt;/a&gt;&quot;&lt;a href=&quot;https://aws.amazon.com/blogs/aws/introducing-vpc-encryption-controls-enforce-encryption-in-transit-within-and-across-vpcs-in-a-region/&quot;&gt;解释道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;金融服务、医疗保健、政府和零售等行业的组织在维护云基础设施的加密合规性方面面临着重大的操作复杂性。传统方法需要将多个解决方案拼凑在一起，并管理复杂的公钥基础设施（PKI），同时手动使用电子表格跟踪不同网络路径上的加密。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然社区的反应大多是积极的，但许多人&lt;a href=&quot;https://www.reddit.com/r/aws/comments/1p3jgtg/introducing_vpc_encryption_controls_enforce/&quot;&gt;最初对定价方法&lt;/a&gt;&quot;表示困惑，或者质疑为什么应该为安全控制付费。用户kei_ichi写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这个功能应该默认启用并且免费。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员可以为现有的VPC启用该功能，以监控流量流的加密状态，并识别无意中允许明文流量的VPC资源。云安全顾问和AWS安全英雄&lt;a href=&quot;https://www.linkedin.com/in/jcfarris/&quot;&gt;Chris Farris&lt;/a&gt;&quot;在他的&lt;a href=&quot;https://www.chrisfarris.com/post/reinvent2025/&quot;&gt;re:Invent&lt;/a&gt;&quot;概述中写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;让我们从为什么应该避免这种情况开始——每个非空VPC每月110美元。如果你需要“满足像HIPAA和PCI DSS这样严格的合规标准”和“展示符合加密标准”，这绝对是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC加密控制有两种操作模式：监控和强制执行。激活后，强制执行模式确保所有新资源仅在兼容的Nitro实例上创建，并且在检测到错误的协议或端口时丢弃任何未加密的流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba9147fdbbff2c458bbff4b8e9870f93.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：AWS博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;管理员只有将所有资源迁移到兼容加密的基础架构后，才能启用强制模式。Farris指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你的VPC中有未加密传输的资源，你不能启用强制执行模式。这里的迁移工作将非常巨大，但如果你的审计员要求你手工完成这项工作，这些成本是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这需要首先升级到支持的硬件和通信协议。可以为不支持加密的资源（如互联网或NAT网关）配置特定的排除，因为它们的流量离开了AWS网络。在“理解现代云安全中的VPC加密”的文章中，Anish Kumar&lt;a href=&quot;https://medium.com/@anishkumarait/understanding-vpc-encryption-in-transit-for-modern-cloud-security-0cee62cd6501&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于你的云安全态势，你可以自信并有证据地回答这个问题：“我所有的VPC中的流量都加密了吗？”从合规审计的角度来看，你可以在流量日志和排除列表中展示加密状态。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项新功能目前在AWS的一些区域可用，包括弗吉尼亚北部、爱尔兰、伦敦和新加坡。在3月1日之前，VPC加密控制将免费使用，之后将对每个非空VPC收取固定的小时费，每小时0.15美元起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls/&quot;&gt;https://www.infoq.com/news/2026/01/aws-vpc-encryption-controls&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</link><guid isPermaLink="false">https://www.infoq.cn/article/Wrhj4dCb7ARmzcKkdgcM</guid><pubDate>Tue, 13 Jan 2026 06:11:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>云安全</category></item><item><title>TanStack发布框架无关的AI工具包</title><description>&lt;p&gt;&lt;a href=&quot;https://tanstack.com/&quot;&gt;TanStack&lt;/a&gt;&quot;是广受欢迎的TypeScript库（如&lt;a href=&quot;https://tanstack.com/query/latest&quot;&gt;TanStack Query&lt;/a&gt;&quot;和&lt;a href=&quot;https://tanstack.com/table/latest&quot;&gt;TanStack Table&lt;/a&gt;&quot;）背后的团队，该团队最近发布了&lt;a href=&quot;https://tanstack.com/ai/latest&quot;&gt;TanStack AI&lt;/a&gt;&quot;的alpha版本。这是一个与框架无关的AI工具包，旨在消除供应商锁定，让开发者完全掌控自己的AI技术栈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI引入了跨多个AI供应商的统一接口、多语言服务器支持以及开放式协议架构。该alpha版本提供了对JavaScript/TypeScript、React和Solid的支持，并内置了&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.anthropic.com/&quot;&gt;Anthropic&lt;/a&gt;&quot;、&lt;a href=&quot;https://gemini.google.com/&quot;&gt;Gemini&lt;/a&gt;&quot;和&lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt;&quot;的适配器。此次发布代表了一种全新的AI工具理念，即将自身定位为中立于供应商的基础设施，而非平台服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI的突出特性之一就是其同构（isomorphic）工具系统，允许开发者通过toolDefinition()一次性地定义工具，并通过.server()或.client()方法提供特定环境的实现。这种架构在整个应用中提供类型安全性，同时支持工具在服务器和客户端上下文中执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具模式有两种定义方式：推荐使用&lt;a href=&quot;https://zod.dev/&quot;&gt;Zod&lt;/a&gt;&quot;&amp;nbsp;Schemas，或者使用&lt;a href=&quot;https://json-schema.org/&quot;&gt;JSON Schema&lt;/a&gt;&quot;（适用于已有JSON Schema定义的项目）。该工具包还提供了模型粒度的类型安全性，使开发者能够针对每个模型获得完整的、针对特定供应商选项的类型提示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;客户端库包括原生JavaScript、React和Solid，未来还将支持更多框架。alpha版本还附带了同构的开发工具，可洞察大语言模型（LLM）在服务器端和客户端的行为，使开发者能使用熟悉的模式调试AI工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在开发者社区中获得了积极反响。开发者Stanley Ulili在Better Stack的一篇&lt;a href=&quot;https://betterstack.com/community/guides/ai/tanstack-ai/&quot;&gt;详细指南&lt;/a&gt;&quot;中这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;虽然仍处于alpha阶段，但是它已经展现出了巨大的潜力。它注重清晰的架构、强大的TypeScript支持，并强调融入现有技术栈的自由，而非强制绑定特定框架或供应商。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1peowss/tanstack_ai_alpha_your_ai_your_way/&quot;&gt;Reddit上&lt;/a&gt;&quot;，一些评论者对SDK的使用场景以及这个新库试图解决的问题提出了疑问，这促使TanStack生态系统的创始人Tanner Linsley作出了回应：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;最近，我和TanStack的所有其他维护者都在深入探索AI，我们发现Vercel的解决方案仍有足够的改进空间，因此决定自己打造一个更贴近我们&lt;a href=&quot;https://tanstack.com/tenets&quot;&gt;产品原则&lt;/a&gt;&quot;的方案。&amp;nbsp;到目前为止，这带来了更好的类型安全性、更优的同构模式，坦白说，这也能够让我们自由地朝着自己想要的方向发展，而不必受制于其他团队。&amp;nbsp;竞争是好事，它能推动整体进步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI将自己定位为Vercel AI SDK的直接替代品，后者目前是JavaScript AI工具领域的主导者。与Vercel的做法不同，TanStack AI作为纯粹的开源基础设施，不包含服务层、不收取平台费用，也不存在供应商锁定。团队强调，开发者直接连接到自己选择的AI提供商，无需通过中间商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于这是新库的alpha版本，因此不存在从早期版本迁移的路径。开发者可通过npm安装核心包并开始使用：npm install @tanstack/ai @tanstack/ai-react @tanstack/ai-openai。&lt;a href=&quot;https://tanstack.com/ai/latest/docs/getting-started/quick-start&quot;&gt;快速入门指南&lt;/a&gt;&quot;提供了创建聊天应用的分步说明，而&lt;a href=&quot;https://tanstack.com/ai/latest/docs/guides/tools&quot;&gt;工具指南&lt;/a&gt;&quot;则深入讲解了同构的工具系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TanStack AI是由TanStack团队开发和维护的开源项目。它延续了该团队在构建框架无关的开发者工具方面的良好声誉，目标是提供真正开放的工具，兼容任何技术栈，而非将开发者捆绑进专有的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/tanstack-ai-sdk/&quot;&gt;TanStack Releases Framework Agnostic AI Toolkit&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</link><guid isPermaLink="false">https://www.infoq.cn/article/EkxQ9xOiKV5JpjtVyr3a</guid><pubDate>Tue, 13 Jan 2026 02:40:29 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category><category>AI&amp;大模型</category></item><item><title>Anthropic深夜放出王炸！白领饭碗要被AI砸了？网友：不支持Linux，差评</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;在开发者工具 Claude Code 推出之后，Anthropic 团队很快意识到一个出乎预料的现象：开发者并没有把它局限在“写代码”这件事上。相反，Claude Code 被迅速用于整理资料、撰写文档、生成报告、分析数据，甚至承担起类似“数字同事”的角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种使用方式的外溢，最终促使 Anthropic 做出一个更激进的产品判断——如果大模型已经被当作工作伙伴使用，那么是否应该为“所有人”，而不仅仅是开发者，提供一种真正面向日常工作的智能协作形态？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是今天，Anthropic 正式推出了 Cowork。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/2798224b81184116211242f420b586d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 工程师、Claude Code&amp;nbsp;创建者&amp;nbsp;Boris Cherny 在 X 上发帖宣布了该消息。他写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;自 Claude Code 发布以来，我们发现用户将其用于各种非编码工作：例如进行度假研究、制作幻灯片、清理电子邮件、取消订阅、从硬盘恢复婚礼照片、监测植物生长、控制烤箱等等。这些应用场景丰富多样，令人惊喜——原因在于底层 Claude Agent 是最佳代理，而 Opus 4.5 是最佳模型。今天，我们非常激动地推出 Cowork，这是我们让 Claude Code 服务于所有非编码工作的第一步。该产品目前仍处于早期阶段，功能尚不完善，与 Claude Code 最初发布时的状态类似。Cowork 包含许多我们认为使其真正与众不同的创新用户体验和安全功能：内置虚拟机用于隔离、开箱即用的浏览器自动化支持、以及对所有非编码工作的支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa95ea01942eb8bff88234f7195c111d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Cowork 是一款基于 Claude Code 底层架构构建的全新产品，目前以“研究预览版”的形式，率先面向 macOS 平台上的 Claude Max 订阅用户开放。与传统对话式 AI 不同，Cowork 的核心定位并非“聊天”，而是“协作”：它试图让 Claude 从一个被动响应指令的助手，转变为能够理解任务、制定计划、持续执行，并与用户保持协同关系的智能工作体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从“对话助手”到“数字同事”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长期以来，大模型产品的主流交互形态仍然是对话。用户输入问题，模型生成回答；用户提出修改，模型再次响应。这种模式在信息查询、文本生成等场景下行之有效，但在真实工作流中却暴露出明显局限——上下文需要反复提供，文件需要人工整理，输出结果往往还要用户自行转换为可用格式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 试图解决的，正是这一断裂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 模式下，用户可以直接授予 Claude 对本地指定文件夹的访问权限。需要强调的是，这种访问并非“全盘授权”，而是由用户明确选择、逐一控制的结果。Claude 只能看到、读取、编辑或创建那些被允许的文件和目录，而无法触及任何未授权内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦获得权限，Claude 的能力边界就发生了质变。它不再只是基于文本上下文“想象”文件内容，而是可以直接操作真实存在的工作材料。例如，它可以扫描一个杂乱无章的下载文件夹，按照文件类型、时间或用途进行分类和重命名；可以从大量截图中提取关键信息，自动生成一份结构化的费用清单；也可以将零散的会议笔记、草稿和片段，整理成一份逻辑清晰的报告初稿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种能力的本质，并不是简单的“更聪明”，而是 Claude 被嵌入进了用户的实际工作环境之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在产品说明中多次强调，Cowork 的体验更接近“给同事布置任务”，而不是与机器人来回对话。一旦任务被下达，Claude 会自行拆解步骤、规划执行路径，并在执行过程中持续向用户同步进展。用户无需等待任务完成即可插入新的反馈或补充想法，这些指令会被自动排队、并行处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是 Cowork 与普通对话模式最根本的差异之一：它默认假设用户的工作是多线程的，而不是线性的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，“更自主”的能力，意味着更高的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让 AI 进入文件系统，甚至具备修改、创建和删除文件的能力，无疑是一种能力跃迁，同时也是风险跃迁。Anthropic 并未回避这一点，反而在产品介绍中反复提醒用户保持警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是操作层面的风险。如果收到明确指令，Claude 确实可以执行具有破坏性的操作，例如删除本地文件或批量修改内容。一旦指令本身存在歧义，或者模型误解了用户意图，后果可能是不可逆的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在 Cowork 中，Claude 在执行任何“重要操作”之前，都会主动征求用户确认。这种设计并非形式上的“弹窗提示”，而是希望用户在关键节点重新审视任务目标，必要时进行纠正或细化指令。Anthropic 也明确建议，在涉及高风险操作时，用户应提供尽可能清晰、具体的指示，而不是依赖模糊的自然语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一类更复杂、也更具行业共性的风险，是“提示注入”（Prompt Injection）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 的工作过程中，Claude 可能会接触来自互联网的内容，例如网页、文档或第三方信息源。如果这些内容中被恶意嵌入了指令，试图诱导模型偏离原本的任务计划，就可能引发安全问题。Anthropic 表示，他们已经构建了针对提示注入的多层防御机制，但也坦言，“代理安全”——即确保 AI 在现实世界中执行操作时的可控性——仍然是整个行业正在积极探索的前沿问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，Cowork 并不是一个“已经完全成熟”的产品，而更像是一次对未来工作方式的现实实验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 也明确指出，这些风险并非 Cowork 独有，而是所有具备“行动能力”的 AI 工具都会面临的问题。只是对许多用户来说，Cowork 可能是第一次接触到一个超越简单对话、真正能够影响本地环境的 AI，因此更需要建立正确的使用习惯和风险意识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研究预览版背后的产品逻辑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cowork 目前被定义为“研究预览版”，这一定位本身就释放了明确信号：Anthropic 并不认为自己已经找到了最终形态，而是希望通过真实用户的使用反馈，加速产品迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据官方披露，Anthropic 计划在后续版本中引入多项重要改进。其中包括跨设备同步能力，使 Cowork 不再局限于单一终端；以及将其移植到 Windows 平台，从而覆盖更广泛的办公人群。同时，安全机制也将持续强化，尤其是在代理行为可解释性和可控性方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从产品路径上看，Cowork 与 Claude Code 之间存在清晰的继承关系。两者共享相同的底层架构，这意味着 Cowork 在能力上，理论上可以完成 Claude Code 已经证明可行的许多复杂任务。不同之处在于，Cowork 将这些能力重新封装为更偏向非技术用户的交互方式，降低了使用门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 Claude Code 面向的是“愿意为效率付出学习成本”的开发者群体，那么 Cowork 的目标人群显然更加广泛：内容创作者、产品经理、运营人员、行政人员，乃至任何需要与文件、资料和信息打交道的知识工作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在掌握 Cowork 的基本使用方式后，用户还可以进一步扩展 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是连接器。Claude 可以通过用户已有的连接器，访问外部信息源，从而将本地任务与外部数据打通。这使得 Cowork 不再只是一个“本地整理工具”，而是可以承担跨系统的信息整合角色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次是新增的一系列技能。这些技能专门用于提升 Claude 在创建文档、演示文稿以及其他常见办公文件时的表现，使其输出更加贴近真实工作场景的格式和标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，如果用户在 Chrome 浏览器中将 Cowork 与 Claude 配对使用，Claude 还可以完成需要访问浏览器的任务。这一步，实际上进一步模糊了“对话 AI”“自动化工具”和“数字员工”之间的界限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从整体设计来看，Cowork 试图减少用户在“提供上下文”和“整理结果”上的认知负担。用户无需手动拼接背景信息，也无需将 Claude 的输出再加工成可用成果。更重要的是，用户不必为了等待 AI 完成某个任务而中断自己的工作节奏——任务可以被连续布置、并行执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 在描述这种体验时，用了一个耐人寻味的比喻：这更像是给同事留言，而不是来回沟通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用户：没有Linux版本，差评！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Cowork 发布之后，迅速在开发者社区、AI 产品圈以及更广泛的知识工作者群体中引发讨论。与以往单纯围绕模型能力、跑分或价格的争论不同，这一次的焦点明显转向了一个更现实的问题：“AI 是否真的开始成为一个可以被信任、被授权的工作参与者？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上的最新讨论串里，有用户评论指出他们“很期待尝试这个功能”，认为 Anthropic 近来在产品和用户信任构建上做得不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/842f272a3e6e92967e9aabc5c537d87e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;**因为仅限 macOS 和订阅计划，部分用户感到遗憾。**在另一个 Reddit 讨论串中，有用户对 Cowork 的平台限制表达了不满或遗憾，评论集中在“只支持 macOS”这一点上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/8060c0e9f39720be6dc7c63fd4acf667.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，值得注意的是，有些评论虽然不是专门针对 Cowork，但有一些用户还是对 Anthropic 近期产品策略与沟通的不满，对 Cowork 的发布背景和用户关系具有间接关联语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 平台，有长期用户表示，自己已经从忠实支持者变成对 Anthropic 的信任下降甚至不满。该用户指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“作为很早一批用户，我原本极力推荐 Claude，但最近几个月感觉 Anthropic 的产品质量沟通都变差了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://claude.com/blog/cowork-research-preview&quot;&gt;https://claude.com/blog/cowork-research-preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/bcherny/status/2010809450844831752&quot;&gt;https://x.com/bcherny/status/2010809450844831752&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/singularity/comments/1qb6qv1/introducing_cowork_claude_claude/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</link><guid isPermaLink="false">https://www.infoq.cn/article/UN16P0pugHNutuMbgrNl</guid><pubDate>Tue, 13 Jan 2026 01:30:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>活久见！连Linux之父等“顽固派”大佬，都在用AI编程了</title><description>&lt;p&gt;程序员中的超级“保守派”、Linux 之父Linus Torvalds，现在也用起了 AI 编程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b675a5ab379b540e9e7b3a6cb345baa0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图源：GitHub&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，Linus 在 GitHub 上悄悄上传了一个小项目。项目本身不大，但特别的是，它是他用一款谷歌系 AI 编程助手&amp;nbsp;进行&amp;nbsp;Vibe Coding&amp;nbsp;完成的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个仓库很快就被眼尖的网友挖了出来，目前已经收获了&amp;nbsp;1600+ 颗 Star。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8afd55b114c702572f76a00a2f8f99d7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 缔造的 Linux，与 Windows、macOS 一起，构成了当今计算世界的三大通用操作系统阵营之一。&lt;/p&gt;&lt;p&gt;不过他曾直言：“在过去将近 20 年里，我并没有从事编程工作。”这并不是他远离技术，而是早就从亲手写代码的人，转变成了为整个系统长期演进负责的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种角色下，这位老哥过去对“AI 帮你写代码”这套叙事，一直保持高度警惕甚至是嗤之以鼻——他关注重点的不是代码写得快不快，而是代码在多年之后是否还能被理解、维护和演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，Linus 对 AI 编程的态度可谓是“大转弯”：不仅开始亲自尝试 Vibe Coding，还公开表示自己对这种方式“相当积极”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些事情的冲击力并不在于“AI 又进步了”，而在于连最不吃 AI 编程这一套的人，也开始松动了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;反 AI 编程的“顽固派”们，也开始接受 Vibe Coding 了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生成式 AI 席卷软件行业的当下，有这么一群特殊的 “顽固派”， 他们定义了现代计算机的技术基石，却曾长期对 AI 编程嗤之以鼻，甚至公开泼冷水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 Linux 之父 Linus Torvalds、Java 之父 James Gosling、Redis 之父 antirez（Salvatore Sanfilippo），个个都是编程界的殿堂级人物。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但有意思的是，随着 AI 工具能力的突飞猛进，这群昔日的 “反 AI 先锋”，正以各自的方式重新划定 AI 的边界：有人有限度拥抱，有人批判中认可，还有人干脆彻底转身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Linus 老哥，之前对生成式 AI 一直保持观望的态度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并不否认 AI 的潜力，但极度厌恶围绕 AI 的过度炒作。在一次开源峰会上，他直言当前关于生成式 AI 的讨论“90% 是行销炒作，只有 10% 是现实”，并毫不掩饰自己的反感。正因为讨厌炒作，他选择在相当长一段时间内&amp;nbsp;主动忽略 AI 热潮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linus 之前一直没有使用各种 AI 编程工具。不过，这并不代表他对新范式抱有敌意。相反，他对&amp;nbsp;Vibe Coding 总体持正面态度，只是并未急于亲自下场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，随着工具逐渐成熟、噪音开始下降，Linus 也终于对 Vibe Coding 上手了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他用上了谷歌的智能体优先开发平台 Antigravity，靠 Vibe Coding 搞定了项目里的 Python 音频采样可视化工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从最初的 “搜索 + 照猫画虎”，到后来直接让 AI 写代码，甚至自定义组件，最终效果比他手写的还要好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对内核社区里 AI 生成补丁泛滥的争议，他的立场很清醒：问题不在于 AI 本身，而在于维护者是否真正理解代码、承担责任。在他眼里，AI 可以当帮手，但不能当甩手掌柜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而&amp;nbsp;Redis 创始人 Salvatore Sanfilippo（网名：antirez）&amp;nbsp;的转变更具戏剧性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位以 “简洁、可预测” 为信仰的系统级程序员，曾固执地坚持一行行手写代码，对自动化工具保持高度警惕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最近，他公开抛出了一句颠覆自己过往理念的话：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“对于大多数项目而言，除非是为了娱乐，现在自己写代码已经不再明智了。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/35/359c6853cabb8f0a7b9e22d03877bc92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;让他改口的，是实打实的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用 Claude Code 的过程中，他发现 AI 在极少人工干预的情况下，就能完成原本需要数周的系统级任务：修复 Redis 测试中的并发与时序问题、重写核心库、复现复杂的数据结构改动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更夸张的是，他只提出需求，Claude Code 5 分钟就生成了一个 700 行的纯 C 库，用于 BERT 类嵌入模型推理，性能仅比 PyTorch 慢约 15%；而他耗时数周完成的 Redis Streams 内部改动，AI 根据设计文档，20 分钟便复刻完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他坦言，对抗浪潮没什么意义，不如主动拥抱：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“忽略人工智能对你或你的职业生涯都没有好处。花几周时间仔细研究，而不是五分钟浅尝辄止。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 antirez 强调，这不是编程乐趣的终结，而是转移：“真正有趣的事情，已经从‘如何写代码’，变成了‘要做什么、为什么这样做’。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这位技术极客也没丢掉警惕性。他担忧 AI 技术的集中化风险。少数公司掌握核心能力，可能引发程序员失业、技术权力失衡等问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比前两位，Java 之父 James Gosling&amp;nbsp;的态度要尖锐得多。他多次炮轰，当前的 AI 热潮 “基本上是一场骗局”，AI 已经沦为“自带误导属性的营销术语”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，生成式 AI 编程的本质，不过是对已有代码和模式的重组，根本谈不上真正的创造力。那些看起来惊艳的演示，一旦碰上复杂项目就露馅：“刚开始接触氛围编程，会觉得它特别酷炫。可一旦项目变得稍微复杂一点，氛围编程就会很快耗尽开发者的脑力。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gosling 的核心质疑点很明确：AI 只能复刻见过的代码，但专业软件开发的精髓，在于开拓性的创新 —— 这些内容从来不在现成的代码库里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也没把话说死。他承认 AI 技术背后的数学与统计原理很复杂，也认可它的实用价值，不是取代程序员，而是 “生成没人愿意去写的文档”，或者解释现有代码的功能。说到底，AI 更像一个智能搜索引擎，而非编程大神。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还不忘吐槽一把资本：“科技行业里骗子和炒作者的数量之多，令人难以置信。风险投资者只关心成功获利，而不是开发出真正有用的技术。” 他甚至预言，“绝大多数 AI 投资都会被烧个精光。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说到底，这三位大佬的转变，都不是向 AI “投降”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们认可的，是 AI 在重复劳动上的效率；他们坚守的，是人类程序员不可替代的核心价值，对复杂系统的理解、对工程架构的判断、对长期维护的责任，以及开拓性的创新能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;对 Linux 内核开发，Vibe Coding 还欠火候&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要说明的是，虽然 Linus 现在对 Vibe Coding 的态度很积极，但他也直言称，这种方式&amp;nbsp;并不适用于 Linux 内核开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个重要原因在于，今天的计算机系统早已比他学习编程的年代复杂得多。Linus 曾回忆，当年他接触的一些输入程序，甚至是从计算机杂志上照着敲下来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然他已经很久没有深度参与具体功能编程，长期为整个内核的演进负责。在他的“系统维护者”视角下，稳定性、安全性和可维护性，远比“写得快不快”更重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一点，其实在他最近上传到 GitHub 的那个项目里有所体现：AI 主要写的只是对 Python 可视化工具部分，核心 C 语言部分（音频效果的数字信号处理等）还是他亲自写的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Linus 看来，Vibe Coding 在小项目和探索性场景中确实优势明显：进入门槛低、反馈速度快，能迅速把模糊的想法变成可运行的程序，用来生成样板代码、辅助脚本，或者“先跑起来看看”，都非常合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这种方式的短板同样明显——生成代码往往风格不稳定、抽象边界模糊、依赖隐性假设，短期能用，长期却很难维护。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 Linux 内核，恰恰是一个对“可维护性”极端苛刻的系统：代码需要被不同年代、不同背景的维护者反复阅读、修改和重构，任何一次“看起来省事”的生成式决策，都可能变成未来十年的技术债。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过话说回来，即便不能“全靠 AI 写代码”，“部分交给 AI”本身，就已经在重塑程序员的工作方式。&lt;/p&gt;&lt;p&gt;在另一条时间线上，有些工程师甚至已经开始用 AI 来开发 AI 本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;Boris Cherny。作为 Anthropic 工程师、也是&amp;nbsp;Claude Code&amp;nbsp;的创造者，他已经几乎不再以传统方式写代码了，而是把自己打造的 AI 编程工具玩儿出了花：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他让 Claude Code 自己参与开发自己，然后竟在一年内完成了 1096 提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7fda4d0dceb79c4ffd024e6d5db386b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个工具已成为全球最受欢迎的 AI 编程工具之一，去年还给 Boris 带来了超过&amp;nbsp;10 亿美元（约合人民币 70 亿元）&amp;nbsp;的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://github.com/torvalds/AudioNoise&lt;/p&gt;&lt;p&gt;https://www.theregister.com/2025/11/18/linus\_torvalds\_vibe\_coding/&lt;/p&gt;&lt;p&gt;https://antirez.com/news/158&lt;/p&gt;&lt;p&gt;https://www.bnext.com.tw/article/81200/linus-torvalds-gen-ai-bubble&lt;/p&gt;&lt;p&gt;https://x.com/bcherny/status/2009072293826453669&lt;/p&gt;</description><link>https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</link><guid isPermaLink="false">https://www.infoq.cn/article/HXqI9KgBQDfh6QjG3E4j</guid><pubDate>Tue, 13 Jan 2026 01:16:08 GMT</pubDate><author>木子,高允毅</author><category>生成式 AI</category></item><item><title>刚刚，DeepSeek 突发梁文峰署名新论文：V4 新架构提前曝光？</title><description>&lt;p&gt;今天凌晨，喜欢闷声做大事的 DeepSeek 再次发布重大技术成果，在其 GitHub 官方仓库开源了新论文与模块 Engram，论文题为 “Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models”，梁文锋再次出现在合著者名单中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1e/1edd54354aa449ddce92b7a3365002b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统的大模型架构相比，该方法提出了一种新的“查—算分离”机制，通过引入可扩展的查找记忆结构，在等参数、等算力条件下显著提升模型在知识调用、推理、代码、数学等任务上的表现。代码与论文全文均已开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/deepseek-ai/Engram&quot;&gt;https://github.com/deepseek-ai/Engram&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种查和算分离的Engram新方法的整体架构如下图所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba205b29dd447e4968a93381616947d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么需要 Engram？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，我们为什么需要 Engram ？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前主流的大语言模型架构依然基于 Transformer 和 Mixture-of-Experts（MoE）结构。MoE 是目前推进参数规模和能力扩展的关键技术之一，通过动态路由机制，只激活部分参数以降低计算成本，同时在任务容量方面实现大规模扩展。DeepSeek 自家系列模型（如 DeepSeek V2、DeepSeek V3 等）也采用了先进的 MoE 方法进行扩展训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这些传统的 Transformer 架构（无论是 Dense 还是 MoE）中，模型的参数实际上承担着两种截然不同的角色：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实性记忆（Memorization）： 存储海量的知识事实。例如，“法国的首都是哪里？”、“世界最高的山脉是哪座”等。这类信息相对死板，更多依赖于“查表”式的检索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;逻辑推理与计算（Calculation）： 负责复杂的逻辑链条、多步推理和情境理解。例如，“根据这段代码的逻辑推导可能的 Bug”、“解析一段复杂的哲学论证”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前的大语言模型倾向于将这两者混在一起。当你试图让模型记住更多知识时，你不得不增加参数量。而在传统的 Dense 模型中，参数量增加意味着前向传播时的计算量（FLOPs）也会同步激增。MoE 架构虽然通过稀疏激活解决了“算力随参数同步爆炸”的问题，但 DeepSeek 研究发现，MoE 专家在处理“死记硬背”的任务时依然不够高效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;神经网络本质上是连续的数学变换，用高昂的矩阵运算去模拟简单的“查表检索”，本身就是一种极大的浪费。DeepSeek 的 Engram 正是为了打破这一困境——“该查表的查表，该算的算”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Engram 的核心思想与架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;聚焦到问题本身，Engram 方法为什么能解决上述问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Engram”一词源自神经科学，意为“记忆痕迹”，它是一个可扩展、可查找的记忆模块，用于语言模型在推理过程中过去可能已经见过的模式或片段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 的核心技术之一是现代化的哈希 N-Gram 嵌入（Modernized Hashed N-gram Embeddings）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统方式： 模型通过多层自注意力（Self-Attention）和 MLP 层的非线性变换，反复提取输入文本中的特征。Engram 方式： 它对输入的 Token 序列进行 N-Gram（连续 N 个词）切片，并利用哈希算法将这些片段映射到一个巨大的、可学习的查找表（Lookup Table）中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于采用哈希索引，这种查找是确定性且 O(1) 时间复杂度的。这意味着无论模型存储了多少万亿个记忆片段，检索的速度几乎是恒定的，且算力消耗极低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;O (1) 的含义是： 一次查找的耗时是常数级的，与 N-gram 表的规模无关。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也就是说，这种设计本质上将一部分“记忆职责”从深度神经计算中卸载出来（例如序列模式、固定知识段的识别与回填），使得模型既拥有活跃神经通道（例如 Transformer + MoE）处理复杂计算，也有静态记忆通道高效处理固定模式，这就是所谓的 “稀疏性的新轴”（a new axis of sparsity）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说就是 MoE 负责：“计算密集”神经推理与复杂组合功能、Engram 负责：“记忆查找”固定模式以及模式重建，两者协同构成一个更高效的整体架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它还具备条件记忆（Conditional Memory）。与简单的静态查找表不同，Engram 是“条件化”的。它会根据当前上下文的隐向量（Hidden States）来决定提取哪些记忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在架构设计上，Engram 模块位于 Transformer 层的早期阶段。它负责“模式重构（Pattern Reconstruction）”，即在计算层（MoE 或 Dense）开始干活之前，先把相关的背景事实和历史模式检索出来，作为“素材”喂给后续的逻辑层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它与 MoE（Mixture of Experts）的关系是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;论文特别指出：Engram 提供了一个新的稀疏性轴，与 MoE 的条件计算不同，它通过条件查找提供静态记忆容量。下面图表中从目标、计算方式、优化方向和作用位置四个维度解释了Engram 和 MoE的区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，DeepSeek 将 Engram 与 MoE 结合，形成了一个双系统：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Engram 模块： 负责海量知识点的“存储与快速检索”。MoE 专家： 摆脱了沉重的记忆负担，全身心投入到“逻辑推理与合成”中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种分工极大地优化了参数效率。在 27B 的实验模型中，Engram 模块可以占用大量的参数用于记忆，但在实际推理时，它只消耗极少的计算量（FLOPs）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/258f97af1beff0b2daba80ee30e0a5e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：V4 将采用这种架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit、X和其他平台的相关帖子中，Engram 的技术核心受到了不少用户的肯定和技术肯定。众多网友认为这个模块的特点在于让模型架构处理“记忆模式查找”和“神经计算推理”两块职责分离，从而开启了新的稀疏性方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 平台有用户评论说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&amp;nbsp;“Engram嵌入方法很有意思。大多数模型仅通过MoE进行扩展，但Engram增加了静态记忆作为补充的稀疏性轴，查找复杂度为O(1)。他们发现 MoE 和 Engram 之间存在 U 形缩放规律，这指导着如何在两者之间分配容量。分析表明，这减轻了早期层级静态模式重建的压力，从而保留了用于复杂推理的深度。确定性寻址意味着它们可以将嵌入表卸载到主机内存中，而不会增加太多推理开销。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0f96995f96abcc3dd5d2e6ca94b84029.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，有用户对这种基于 n-gram lookup 的机制表达了直观兴趣，他评论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“即便是在不依赖 GPU 的环境下也能实现这种 O(1) 查找方式，让不少开发者对本地部署这样的大模型功能有了更实际的期待。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b75e18cfe6ca43ea711351ec52468e1f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在部分技术性评论中，有人指出：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;即从已有技术逻辑来看，在 LLM 中加入静态记忆查找似乎是“顺理成章”的发展方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这类观点反映了一个重要观点：专家群体开始从纯参数扩张思维转向更“智能”的架构设计，包括查表式模块和神经网络的协同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不少高级开发者在讨论中进一步提到，这种设计在理念上类似于对传统 NLP 技术（如 n-gram embedding）的现代化转换，结合了高效寻址机制（deterministic addressing）和神经推理模块，这种组合在纸面上看具有较高的可行性和实用性（这一点正是 Engram 的核心贡献）。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一条社区评论指出，Engram 很可能是 DeepSeek 即将发布的 V4 模型的核心技术基础：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;业内观察者认为 Engram 模块可能会成为 DeepSeek V4 的重要组成部分，并预示 DeepSeek 下一代模型会在记忆和推理协同上实现架构级提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X平台，也有网友表达了同样的猜测，认为V4 也将采用这种架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6ba93d5100cbacfee2b85d806c7ebc87.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友调侃，原本想抄袭下谷歌的技术，但现在要抄袭DeepSeek了，因为它比谷歌更好！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b09f2dec8c0db4892ed0b417422d8754.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有网友表示，其实Meta之前也有过类似想法，但用到的技术不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9b474fc250b9518bd87e5af00a84076e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/scaling01/status/2010748516788777445&quot;&gt;https://x.com/scaling01/status/2010748516788777445&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&quot;&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</link><guid isPermaLink="false">https://www.infoq.cn/article/Iz1JUWqd0FvejBXDKWk3</guid><pubDate>Tue, 13 Jan 2026 00:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>阿里云把“华强北”们推向CES</title><description>&lt;p&gt;今年的&amp;nbsp;CES，中国硬件又一次成为主角。活跃在拉斯维加斯展台上的诸多出海产品，背后依托的是深圳的研发效率与供应链能力，而其智能化核心，则越来越多建立在以&amp;nbsp;Qwen&amp;nbsp;为代表的多模态、全尺寸的大模型基础上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与沙漠赌城的&amp;nbsp;CES&amp;nbsp;同期，在深圳蛇口，阿里云也举办了一场智能硬件展。这场展会面向公众免费开放，选址于本地居民日常散步、观海和看展的滨海文化地标，却意外成为&amp;nbsp;AI&amp;nbsp;硬件从实验室走向真实市场的缩影。1000&amp;nbsp;余款智能硬件在这里集中亮相，其中超过&amp;nbsp;200&amp;nbsp;款与&amp;nbsp;CES&amp;nbsp;同款甚至首发。这里既有来自北京、杭州的创新团队，也有来自义乌、华强北等产业带的制造与渠道力量——他们对技术趋势的嗅觉，向来快过任何市场报告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术验证与市场反馈在同一空间同时发生。在这里你可以听到合作方直接询价“多少钱，做OEM吗，能做多少套”，也可以看到消费者直接下单，把&amp;nbsp;399&amp;nbsp;元的&amp;nbsp;AI&amp;nbsp;玩具带回家。许多普通家庭第一次在这里集中体验到能对话的毛绒玩具、教用户跳舞的镜子、能翻跟头的机器狗，和具备实时提醒能力的&amp;nbsp;AI&amp;nbsp;眼镜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在&amp;nbsp;2024&amp;nbsp;年云栖大会上，阿里云董事长吴泳铭就明确指出，未来&amp;nbsp;AI&amp;nbsp;最大的想象力会来自于物理世界：“我们不能只停留在移动互联网时代去看未来，深层次AI最大的想象力绝对不是在手机屏幕上做一两个超级APP，而是接管数字世界，改变物理世界。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在这轮&amp;nbsp;AI&amp;nbsp;硬件浪潮中，阿里云没有选择去做终端硬件的制造者，而是以软硬一体的融合理念，向产业提供底层模型能力、云基础设施与生态支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据显示，通义大模型的多模态能力已深度赋能超过&amp;nbsp;15&amp;nbsp;万家智能硬件厂商。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d28fc8e8d4409f5e48b4f7b2dea413ff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从雷鸟的&amp;nbsp;AI&amp;nbsp;眼镜、听力熊的儿童&amp;nbsp;AI&amp;nbsp;Pin，到优必选机器人、趣丸科技的生成式&amp;nbsp;AI&amp;nbsp;吉他，这些走进全球家庭的产品背后，都能看到以通义为代表的阿里云基础设施的支撑。而它们从概念到量产、从深圳到世界的惊人速度，也再次印证了深圳这座“硬件硅谷”在研发、供应链与商业化效率上的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;For&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的&amp;nbsp;AI&amp;nbsp;硬件&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;逛完阿里云通义智能硬件展，一个强烈的感受是，这是我经历过为数不多，能让普通人玩得开心、让创业者看到机会、让厂商验证商业模式，同时清晰传递主办方战略意图的展会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云租下深圳海上世界文化艺术中心三层空间，用一种近乎“生活化”的方式，向公众展示：AI能长在玩具里、眼镜上、健身镜中，甚至成为家庭一员的日常存在。向企业展示：你能快速依托阿里云的生态，快速做出能进入全球家庭的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展会围绕两条主线展开：一是呈现阿里云的底层能力，二是展示其赋能下的千款智能硬件成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一楼以“智能中枢”为核心，展示通义大模型的能力：观众上传一张照片，就能生成一段短视频；走过一段互动迷宫，便能直观感受多模态AI如何理解图像、语音和动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能中枢周围环绕着“创造有AI”“生活有AI”“AI实训营”等主题区，OPPO、理想、影石等品牌在此展示手机、智能座舱和AI影像设备，而像趣丸科技的AI吉他、Looki这样的新奇产品，则让人看到AI如何重塑音乐、娱乐等日常互动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/27e1fe905091def2e73df36bc3297948.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;趣丸科技与阿里云合作推出的全球首款生成式AI吉他TemPolor&amp;nbsp;Melo-D，在通义大模型的支持下，重新定义了人与音乐的交互方式，提供了个性化的AI音乐创作体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三楼聚焦陪伴、健康与安防，专设义乌厂商展区；四楼覆盖家居、教育、健身等提效场景，华强北的硬件老板们也把“一米柜台”搬到了现场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea7a53958edc4cb3f43237c85e9cedcd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;通义联合听力熊为青少年定制随身AI对话智能体，打造国内首款儿童&amp;nbsp;“AI&amp;nbsp;Pin”&amp;nbsp;Mooni&amp;nbsp;M1，提供多种角色选择。经过通义千问大模型加持，用户的&amp;nbsp;AI&amp;nbsp;使用时长提升40分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云想让大家知道，AI有能力在所有场景里带来更好的体验。它同时也呈现出一种可能——不管是软件应用还是硬件产品，每个人都可以在这个时代搭建些什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c18195f4e17f32ac1f9330d8a1ccb08f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;阿里云&amp;nbsp;AI&amp;nbsp;实训营的 Agent 硬件搭建小课堂&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于普通人来说，硬件展是一个游戏体验。孩子和AI毛绒玩具对话，年轻人跟着镜子学舞，有人让AI解读运势、推荐香水，还有中学生在阿里云&amp;nbsp;AI&amp;nbsp;实训营中搭建了自己的第一个交互硬件。我们这代人仍然处于有“AI硬件”概念的时期，而对于下一代人来说，可能已经不存在“AI硬件”。当生活总所有一切都有AI，AI之于人，阿里云之于硬件和应用产品，就是水之于人的存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对创业者和企业主而言，展会成了高效的信息源。有用户的直接提问和反馈，也有工程师在展位前递上简历。采购顾问带着非洲、拉美的客户穿梭其间，现场询价、谈订单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e4/e4fd0f58e096c36684adf9311b2e1962.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL、影石、安克创新的案例，更是为想要入局AI硬件和出海的企业打气——依托阿里云全球全栈AI基础设施，大型制造企业可实现研发、服务、出海一体化，新锐品牌也能快速站稳全球舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚在CES获得Best&amp;nbsp;of&amp;nbsp;Innovation奖项的影石，依托&amp;nbsp;Qwen-VL&amp;nbsp;实现视频与图片的分类打标和场景识别，结合&amp;nbsp;Qwen-Plus&amp;nbsp;生成剪辑脚本，赋能全球百万视频创作者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60f1d931f1cedfe5b371d8454ec23495.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;安克创新依托阿里云“全球一张网”，实现跨境资源调度与合规部署，核心系统互访提速30%，并将&amp;nbsp;Qwen&amp;nbsp;与&amp;nbsp;Wan&amp;nbsp;深度融入语音助手、多模态交互等产品功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa1a742f457dce5707bd11d956d5ace3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCL则基于通义大模型打造了半导体显示专家系统&amp;nbsp;X-Intelligence，支撑其全球研发体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，阿里云把义乌、华强市场这些产品背后的“制造和分发网络”呈现在大家面前。在他们的摊位上，你可以看到很多产品尽管“粗糙”，却仍然有市场。在很多欠发达国家，AI硬件需要的不是精致，而是先以成本最低的方法被用上。很多义乌玩具、小3C产品的批发商，嗅到AI风潮后，已经在深圳有了自己的硬件工厂。华强科技生态园等孵化器，也开始重点招募AI硬件的创业公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如阿里云智能集团通义大模型业务总经理徐栋所说：“这样一个平台（以通义多模态交互开发套件为代表的AI硬件赋能平台）是我们非常重要的业务的选择，我们需要更多贴近阿里云的智能硬件开发伙伴。很多场景是碎片化的，只有做更贴近实际的生产环节、消费环节，每个人对AI硬件的体验才能更深。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，正在成为&amp;nbsp;for&amp;nbsp;everyone,&amp;nbsp;by&amp;nbsp;everyone&amp;nbsp;的日常现实。而阿里云的角色，不是站在台前造产品，而是站在幕后，让创新更快实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阿里云，在&amp;nbsp;AI&amp;nbsp;硬件变革前夜&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件从极客圈层走向大众日常，标志着市场已从“启蒙期”进入“挑剔期”。当用户开始为AI服务付费、并将设备融入日常生活，产品的成败就不再取决于功能数量，而在于能否持续兑现可感知的价值——这要求厂商必须拥有一套覆盖模型、工程、服务与生态的系统性能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI硬件，特别是在消费级市场，正经历着一场根本性的转型。从传统的联网设备到如今的“端侧智能体”，AI不再只是硬件的附加功能，而是直接决定产品核心价值的引擎。这一转变的核心标志在于：AI&amp;nbsp;不再作为附加功能嵌入硬件，而是成为产品定义、体验构建与价值交付的底层引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期智能硬件以“连接+控制”为基本范式，其智能化主要体现在远程操作与数据回传；而新一代AI硬件则要求设备具备持续感知、上下文理解、自主决策与协同执行的能力，成为一个能在真实场景中与用户形成闭环互动的“智能体”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一转变正在重塑硬件的设计逻辑、用户的价值预期与厂商的技术路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用对AI硬件的认知早已超越“新奇感”，转而关注端到端体验是否流畅、可靠、有用。更重要的是，用户开始愿意为持续服务付费。例如按月订阅儿童AI陪伴内容，或为高级健身指导功能续费。这催生了“硬件+服务”的新商业模式，但也带来新挑战，如果AI不能提供可感知的显性价值，订阅就难以为继。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术架构也随之重构。端云协同的逻辑发生了变化。之前的端云协同更多指向算力分工，即端上承载不了的算力放在云上，但现在的端云协同是指能力互补。安全、延时、功耗的问题必须在端上解决，而生态打通这些能力可能在云上做。同时，交互方式正走向“无感化”——不是让用户察觉不到AI存在，而是让使用门槛足够低，无需学习就能自然融入原有生活节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，对大多数硬件厂商而言，这场转型并不轻松。模型迭代速度远超硬件研发周期，而一个产品往往需要组合多个模型才能实现完整功能，集成复杂度陡增。与此同时，Agent架构、工具链和工程平台快速演进，传统硬件团队难以跟上软件层的节奏。更棘手的是，许多厂商擅长制造和渠道，却缺乏用户运营、数据闭环和订阅服务能力，难以构建可持续的商业模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对这些系统性挑战，阿里云提供了AI硬件的全链路支持体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在基础设施层面，阿里云面向&amp;nbsp;AI&amp;nbsp;应用场景全面升级计算、存储与网络能力，为高并发、低延迟的智能硬件业务提供稳定底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在模型层面，通义大模型家族（包括&amp;nbsp;Qwen3、Qwen-VL、QwQ&amp;nbsp;等）全面开源，并提供闭源高阶版本，同时接入第三方优质模型，帮助厂商一站式、低成本调用全球先进&amp;nbsp;AI&amp;nbsp;能力。针对多模态交互场景，阿里云还推出专有优化模型，降低端到端语音和视频交互时延。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云的模型能力，已经获得顶尖手机、汽车、具身智能、智能配件品牌的认可和验证：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，全球&amp;nbsp;Top&amp;nbsp;10手机厂商已都在使用阿里云的大模型能力。例如，OPPO利用阿里云人工智能平台&amp;nbsp;PAI&amp;nbsp;对&amp;nbsp;Qwen&amp;nbsp;开源模型进行后训练，以支持其AI多场景应用；荣耀则联合阿里云百炼打造&amp;nbsp;VQA&amp;nbsp;端到端方案，图片细分场景识别率提升近40%，延迟降低30%。荣耀Magic&amp;nbsp;V5&amp;nbsp;接入飞猪旅行、高德地图两个垂直Agent&amp;nbsp;两个月即斩获百万级用户好评。基于“模型+工程+生态”三位一体的战略，阿里云正持续加速手机行业的AI功能创新与规模化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/3f/16/3f3f3ec409b7a511c1dbc3d97b9f5f16.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想汽车基于阿里云MindGPT大模型，整合高德、飞猪、支付宝等生态，实现全球首个“车机AI扫码支付”；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/70/e6/70de20e17376b8709151305c4cbb4fe6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷鸟创新联合阿里云推出行业首个面向智能眼镜的AI大模型，意图识别准确率达98%，搭载该模型的雷鸟眼镜出货量领跑AR行业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/fd/10/fdb4be397f9d2375b7950bb10c61ec10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优必选的萌&amp;nbsp;UU&amp;nbsp;陪伴机器人，搭载通义千问与自研情感智能体“点灵”，且具有长期记忆&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90b730812c7aeda3120052d8ccd77d00.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;特别值得注意的是，阿里云此次还推出了全模态智能交互开发套件，将上述能力封装为标准化工具。该套件适配&amp;nbsp;30&amp;nbsp;多款主流&amp;nbsp;ARM、RISC-V&amp;nbsp;和&amp;nbsp;MIPS&amp;nbsp;架构芯片，覆盖市面上绝大多数终端设备。未来，通义大模型还将与玄铁&amp;nbsp;RISC-V&amp;nbsp;实现软硬全链路协同优化，进一步提升在国产芯片上的部署效率与推理性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b84bac627e24c2d372a32ada8ec958ea.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套开发套件不仅提供基础能力，还预置十余款&amp;nbsp;MCP&amp;nbsp;工具和&amp;nbsp;Agent，覆盖生活、工作、娱乐、教育等高频场景。例如，基于出行规划&amp;nbsp;Agent，用户可直接调用路线规划、旅行攻略、本地探索等功能。同时，套件深度集成阿里云百炼平台生态，支持开发者添加社区模板，或通过&amp;nbsp;A2A&amp;nbsp;协议兼容第三方&amp;nbsp;Agent，极大扩展了应用边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99dcc21e3fc0751d1c9fa8ff12bf4c50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是&amp;nbsp;OPPO、理想这样的品牌厂商，还是华强北的创客、义乌的出海团队，甚至“一人公司”，都能借助阿里云的解决方案快速验证想法、打造产品，并参与全球竞争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是阿里云“基础设施先行”的思路，让展会上那些看似天马行空的产品，得以从概念走向量产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有趣的是，阿里云大模型能力的升级节奏，与AI硬件的集中爆发高度同步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2023年8月，阿里云开源Qwen-VL视觉语言模型，首次让中小厂商能免费调用工业级多模态能力；2024年，Qwen-Audio、Qwen2-VL等模型集中发布，补齐了语音、图像与文本融合交互的关键拼图；到2025年初，原生端到端的Qwen3-Omni模型的发布，以及Qwen-Agent，进一步支持硬件端构建任务型智能体。这一连串技术释放，恰好为AI硬件创新提供了可落地的底层支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从2024年下半年起，阅读器、眼镜、耳机、学习机等细分品类迎来AI功能的规模化落地：文石、闪极、AIxFU、听力熊、云希谷等能纷纷接入阿里云大模型能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些产品的共同点，是都受益于通义的“全谱系开源”策略——0.5B到480B的模型全覆盖，文本、语音、视觉、视频能力一应俱全。无论是大型企业，还是华强北的硬件作坊，都能找到适合自己的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是这种低成本接入到快速验证的正向循环，让AI硬件从概念走向规模化落地。阿里云没有造AI硬件产品，却通过持续开源和能力迭代，成为这场硬件浪潮背后最坚实的推手。&lt;/p&gt;</description><link>https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</link><guid isPermaLink="false">https://www.infoq.cn/article/T07qwtRUx3pOlioQyTbM</guid><pubDate>Mon, 12 Jan 2026 10:42:48 GMT</pubDate><author>陈姚戈,王一鹏</author><category>AI&amp;大模型</category></item><item><title>英伟达发布了跨AI、机器人和自动驾驶的开放模型、数据集和工具</title><description>&lt;p&gt;英伟达（NVIDIA）&lt;a href=&quot;https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/&quot;&gt;发布&lt;/a&gt;&quot;了一套涵盖语言、智能体系统、机器人技术、自动驾驶和生物医学研究的开放模型、数据集和开发工具。此次更新扩展了多个现有的NVIDIA模型家族，并通过GitHub、Hugging Face和NVIDIA的开发者平台提供了相应的训练数据和参考实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在代理式AI领域，NVIDIA扩展了Nemotron模型家族，为语音识别、检索增强生成和安全提供了新的组件。Nemotron Speech包括针对低延迟、实时用例优化的自动语音识别模型。Nemotron RAG引入了用于多模态文档搜索和检索流程的嵌入和重排视觉语言模型。Nemotron Safety增加了用于内容过滤和敏感或个人身份信息检测的更新模型。NVIDIA还发布了用于选定Nemotron模型的数据集和训练代码，包括在公共基准上评估的嵌入模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于机器人技术和物理AI，NVIDIA引入了新的Cosmos世界基础模型，这些模型支持在真实环境中的感知、推理和合成数据生成。Cosmos Reason 2是一个多模态推理模型，旨在增强智能体在物理环境中操作的场景理解。Cosmos Transfer 2.5和Cosmos Predict 2.5专注于在不同环境和条件下生成合成视频数据，支持仿真和数据增强工作流程。基于Cosmos，NVIDIA发布了Isaac GR00T N1.6，这是一个用于人形机器人的开放视觉-语言-动作模型，支持全身控制并将视觉感知与动作规划集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公告的一个组成部分是NVIDIA Alpamayo，一个用于基于推理的自动驾驶的新开放模型家族。Alpamayo结合了感知、规划和可解释性，采用视觉-语言-动作架构，并与仿真工具和大规模驾驶数据集相匹配。NVIDIA还引入了AlpaSim，这是一个用于自动驾驶汽车模型闭环评估的开源仿真框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据NVIDIA汽车部门负责人吴信洲&lt;a href=&quot;https://www.linkedin.com/posts/xinzhouw_level-2-autonomous-driving-in-san-francisco-activity-7414325238630342656-moGb?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAACX5yoEBhsg1xPtc5iaJXHCu_Rv298CmfZA&quot;&gt;表示&lt;/a&gt;&quot;，Alpamayo和相关工具反映了跨研究、模拟、数据工程、安全和集成团队多年的开发努力。吴指出，这项工作涉及广泛的道路测试、使用Cosmos等平台进行持续的大规模模拟，以及与包括梅赛德斯-奔驰在内的汽车合作伙伴的紧密合作，计划在即将推出的量产车辆中进行初步部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;医疗保健和生命科学更新通过新的NVIDIA Clara模型提供。这些包括用于原子级蛋白质设计的La-Proteina，用于合成感知药物设计的ReaSyn v2，用于早期安全和相互作用预测的KERMT，以及用于RNA结构建模的RNAPro。NVIDIA还发布了一个包含45.5万个合成蛋白质结构的数据集，以支持该领域的训练和评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有模型和数据集均在开放许可下发布，可通过GitHub和Hugging Face访问。NVIDIA表示，许多模型还被打包为NIM微服务，以便在从本地推理环境到云基础设施的NVIDIA加速系统上部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/nvidia-open-models/&quot;&gt;https://www.infoq.com/news/2026/01/nvidia-open-models/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</link><guid isPermaLink="false">https://www.infoq.cn/article/HHxGUQV0RjXGOfTisX9U</guid><pubDate>Mon, 12 Jan 2026 09:12:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>英伟达</category><category>AI&amp;大模型</category><category>跨端开发</category></item><item><title>MongoBleed漏洞允许攻击者从MongoDB的堆内存中读取数据</title><description>&lt;p&gt;MongoDB最近修补了&lt;a href=&quot;https://www.mongodb.com/community/forums/t/important-mongodb-patch-available/332977&quot;&gt;CVE-2025-14847&lt;/a&gt;&quot;，这是一个影响多个支持和遗留MongoDB服务器版本的漏洞。根据披露，该漏洞可以被未认证的攻击者以较低的复杂度远程利用，可能导致敏感数据和凭证的外泄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞被称为MongoBleed，以臭名昭著的&lt;a href=&quot;https://en.wikipedia.org/wiki/Heartbleed&quot;&gt;Heartbleed&lt;/a&gt;&quot;命名，CVSS得分为&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2025-14847&quot;&gt;8.7&lt;/a&gt;&quot;，由对zlib压缩网络流量处理不当触发，允许未经身份验证的攻击者泄露未初始化的内存，并可能从受影响的MongoDB服务器窃取敏感数据，如凭证或令牌。根据&lt;a href=&quot;https://www.wiz.io/blog/mongobleed-cve-2025-14847-exploited-in-the-wild-mongodb&quot;&gt;Wiz&lt;/a&gt;&quot;的安全研究人员，该漏洞正在被广泛利用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如MongoDB的声明所述，MongoDB Atlas上的托管实例已经被修补，但是如果自托管MongoDB不更新，仍然存在风险。强烈建议组织立即应用安全补丁，或禁用压缩并限制网络暴露。Merav Bar、Amitai Cohen、Yaara Shriki和Gili Tikochinski解释：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;CVE-2025-14847源于MongoDB服务器基于zlib的网络消息解压缩逻辑中的一个缺陷，该逻辑在认证之前进行了处理。通过发送畸形的压缩网络数据包，未经身份验证的攻击者可以触发服务器错误处理解压缩的消息长度，导致返回给客户端未初始化堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Wiz文章，42%的云环境中至少有一个易受攻击的MongoDB实例，Censys报告称全球大约有87,000台服务器存在潜在的风险。由于该漏洞可以在没有认证或用户交互的情况下被利用，暴露在互联网上的数据库服务器面临特别高的风险。Wiz团队补充道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在代码层面，这个漏洞是由message_compressor_zlib.cpp中的错误长度处理引起的。受影响的逻辑返回了分配的缓冲区大小（output.length()），而不是实际解压缩数据的长度，从而允许过小或畸形的有效载荷暴露相邻的堆内存。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞影响了自&lt;a href=&quot;https://github.com/mongodb/mongo/pull/1152&quot;&gt;2017&lt;/a&gt;&quot;年以来发布的所有MongoDB版本。Linkfields Innovations的软件开发人员Gourav Boiri&lt;a href=&quot;https://www.linkedin.com/posts/bgourav2287_cybersecurity-mongodb-infosec-activity-7411163710372835328-DQQh&quot;&gt;评论道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MongoBleed突出了即使是成熟的数据库，当暴露或打补丁时，也可能成为关键的攻击面。预认证内存泄露、主动漏洞攻击和87K+暴露实例——提醒我们，数据库安全就是基础设施安全。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在“&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;简单解释MongoBleed&lt;/a&gt;&quot;”的文章中，&lt;a href=&quot;https://www.linkedin.com/in/stanislavkozlovski/&quot;&gt;Stanislav Kozlovski&lt;/a&gt;&quot;解释了这一漏洞的工作原理，并警告说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;它非常容易被利用——只需要连接到数据库（不需要认证）。截至撰写本文时，它已经被修复，但一些EOL版本（3.6、4.0、4.2）将不会得到修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoSec创始人和实践者&lt;a href=&quot;https://www.linkedin.com/in/ecapuano/&quot;&gt;Eric Capuano&lt;/a&gt;&quot;解释了&lt;a href=&quot;https://blog.ecapuano.com/p/hunting-mongobleed-cve-2025-14847&quot;&gt;如何从日志中检测数据库服务器是否被利用&lt;/a&gt;&quot;。在一个流行的&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1py2c0w/mongobleed_vulnerability_explained_simply/&quot;&gt;Reddit&lt;/a&gt;&quot;帖子中，用户misteryuub争论道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;很多人争论说开源代码比闭源代码更安全，或者安全问题会在开源代码中更快被发现。这种级别的漏洞存在是对这个论点的反驳。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kozlovski不同意：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当人们说开源更安全时，他们通常指的是有活跃社区的开源项目。Mongo在2017年似乎没有这个，因为引入这个漏洞的PR没有在公共GitHub上被审查。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mongodb.com/try/download/community&quot;&gt;MongoDB补丁版本&lt;/a&gt;&quot;现在可用于从4.4到8.0的所有支持版本。&lt;a href=&quot;https://www.percona.com/blog/urgent-security-update-patching-mongobleed-cve-2025-14847-in-percona-server-for-mongodb/&quot;&gt;像Percona Server for MongoDB这样的分支也受到上游漏洞的影响&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&quot;&gt;https://www.infoq.com/news/2026/01/mongodb-mongobleed-vulnerability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</link><guid isPermaLink="false">https://www.infoq.cn/article/HN7NXQZUkbU2StoW9Kl9</guid><pubDate>Mon, 12 Jan 2026 08:23:00 GMT</pubDate><author>Renato Losio</author><category>云端开发</category><category>数据库</category></item><item><title>亚马逊云科技预览Route 53 Global Resolver，将DNS与区域故障解耦</title><description>&lt;p&gt;亚马逊云科技（AWS）最近宣布公开预览&lt;a href=&quot;http://aws.amazon.com/route53/global-resolver&quot;&gt;Amazon Route 53 Global Resolver&lt;/a&gt;&quot;，这是一项在全球范围内提供安全、可靠的DNS解析的新服务。组织可以使用该服务来解析互联网上的公共域和与Route 53私有托管区域关联的私有域名的DNS查询。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从历史上看，管理混合型DNS带来了巨大的操作开销。在传统的区域设置中，管理员必须手动同步水平分区基础设施，并管理复杂的转发规则。这通常需要维护冗余的VPC解析器端点，并在多个区域中复制安全策略以确保故障转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Route 53 Global Resolver通过消除对单独分DNS转发的需求来解决这些挑战。正如AWS的高级解决方案架构师Esra Kayabali解释的那样：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它通过多种协议提供DNS解析，包括DNS over UDP（Do53）、DNS-over-HTTPS（DoH）和DNS-over-TLS（DoT）。每个部署提供一组通用的IPv4和IPv6任何播IP地址，将查询路由到最近的AWS区域，减少分布式客户端群体的延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b72ba1a3835c40c990f45b0e65569ea9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（来源： &lt;a href=&quot;https://aws.amazon.com/blogs/aws/introducing-amazon-route-53-global-resolver-for-secure-anycast-dns-resolution-preview/&quot;&gt;AWS&lt;/a&gt;&quot;新闻博客文章）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该服务集成了与Route 53 Resolver DNS防火墙等效的安全功能，可以集中实施策略。主要的安全功能包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;托管过滤：管理员使用&lt;a href=&quot;https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/gr-managed-domain-lists.html&quot;&gt;AWS托管域名列表&lt;/a&gt;&quot;来阻止恶意软件和网络钓鱼等威胁，或限制特定网络内容。行为保护：解析器检测并阻止域名生成算法（&lt;a href=&quot;https://www.geeksforgeeks.org/computer-networks/what-is-domain-generation-algorithm/&quot;&gt;Domain Generation Algorithm&lt;/a&gt;&quot;，DGA）模式和DNS隧道尝试。加密传输：支持DoH和DoT保护查询在传输过程中免受未经授权的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了&lt;a href=&quot;https://en.wikipedia.org/wiki/Zero_trust_architecture&quot;&gt;支持零信任架构&lt;/a&gt;&quot;，Global Resolver仅接受经过身份验证的客户端的流量。除了标准的IP/CIDR允许列表外，该服务为DoH和DoT连接引入了基于令牌的身份验证。这提供了细粒度的控制，允许管理员为特定客户端组或单个远程设备分配和撤销令牌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Abhijeet Kulkarni在LinkedIn&lt;a href=&quot;https://www.linkedin.com/posts/cloudwithabhi_route53-aws-route53-activity-7410397751529738240-mamy&quot;&gt;帖子&lt;/a&gt;&quot;中指出，虽然传统的DNS依赖于区域绑定的解析器，其中故障可能会放大中断，但Global Resolver引入了一种根本不同的运维模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过任播将解析移动到边缘，DNS在默认情况下成为全局分布的。Kulkarni强调，这提供了“解析层的故障隔离”，确保在DNS层吸收区域中断，而不是通过网络级联。这有效地将DNS从区域依赖转变为具有弹性的全球系统边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版目前在几个全球区域可用，包括美国东部（弗吉尼亚北部、俄亥俄州）、美国西部（加利福尼亚北部、俄勒冈州）、欧洲（法兰克福、爱尔兰、伦敦）和亚太地区（孟买、新加坡、东京、悉尼）。定价详情可在官方&lt;a href=&quot;https://aws.amazon.com/route53/pricing/&quot;&gt;Route 53&lt;/a&gt;&quot;定价页面上找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/route53-global-resolver-anycast/&quot;&gt;https://www.infoq.com/news/2026/01/route53-global-resolver-anycast/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Jey8ALfAhSOBWZ7El1is</link><guid isPermaLink="false">https://www.infoq.cn/article/Jey8ALfAhSOBWZ7El1is</guid><pubDate>Mon, 12 Jan 2026 07:44:00 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>数据库</category></item><item><title>抨击AI炒作、曝企业需求为先，Anthropic 联创：模型提 0.01 性能就血赚，算力烧钱但值！</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic由7位前OpenAI核心成员创立，他们曾参与GPT-2、GPT-3、Scaling Laws及AI安全研究。Daniela Amodei 就是其中之一，她是Dario Amodei 的妹妹，也曾任&amp;nbsp;OpenAI 的安全与政策副总裁，现在是 Anthropic 联合创始人兼总裁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2021年初疫情期间，Dario 冒雨向Eric Schmidt路演，后者成为Anthropic A轮投资人。与其他大模型公司不同，Anthropic会将大模型使用中的风险公开，比如Claude在极端“生存威胁”情境测试中，多数情况下会选择勒索，类似操作在行业中极少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Amodei 近期在接受 CNBC 采访中，她谈到了如何在支出方面控制成本、如何保障人工智能安全以及 2026 年上市的可能性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei 认为，“AI安全”不是商业负累，而是核心优势，企业客户对安全性的高要求，恰好匹配其创立初衷，这一理念在早期被认为“激进”，如今成为B端竞争壁垒。Anthropic是唯一能同时登陆微软、亚马逊云科技、Google三大云平台的前沿大模型厂商，企业客户需求曾数次超过其算力供给能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，她还提到，Anthropic以“不要相信炒作”为内部价值观，拒绝博关注，通过B端真实价值锚定长期方向，避免被行业泡沫裹挟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 在支出和算法效率方面采取了更为谨慎的态度，而其竞争对手 OpenAI 则承诺投入 1.4 万亿美元用于计算。她提到，即便行业算力投入规模惊人，但“更好的硬件回报极高”，哪怕模型性能仅提升0.01，价值也足够可观；且前沿模型的算力需求仍在指数级增长，需提前大规模布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，我们在不改变原意基础上进行翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 起源：离开OpenAI为什么值得&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这家公司创建初期，当时世界处在什么状态？你们觉得 Anthropic 要特别解决什么问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：Anthropic 就要迎来五周年了。说到最初，其实我和另外六位联合创始人，当时都在 OpenAI 一起工作。我们一起参与过很多不同的项目，从把一些当时规模最大的模型做起来，比如 GPT-2、GPT-3，到很多早期的语言模型工作，后来都成了大模型革命的一部分；还有 scaling laws 相关的研究，以及大量偏技术安全方向的工作，比如可解释性和对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时间点，我们逐渐形成了一个非常清晰的想法：我们想建立一家真正站在 AI 前沿、开发变革性技术的公司，但同时对系统的安全性和可靠性保持一种极其严格、近乎执念式的关注。那时我们觉得，与其在原有框架里继续做，不如自己出来，把这件事从头到尾做到极致。Anthropic 就是在这样的背景下诞生的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果把时间背景说清楚，那是 2020 年的冬天，到 2021 年初。大家都被封在家里，正值疫情高峰。那种感觉很复杂：机会既让人兴奋，又让人害怕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前提到过一个像电影画面的瞬间，2021 年 1 月，在 Dario 的后院，大家都戴着口罩，Eric Schmidt 也在，下着大雨，你们在帐篷下面向他做介绍。你会把它看作公司的起始点吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：是的，那一幕真的很难忘。具体日期我可能记不太准了，但应该是 1 月初的某一天。我们就在 Dario 家后院，正下着雨，我们临时搭了一个帐篷，我们私下都叫它“派对帐篷”，大家就挤在下面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，Eric 成了我们的 A 轮投资人。但当时，其实我们只是刚刚做出“要出来创业”的决定，一切都还非常早期，对公司具体会长成什么样，说实话并没有清晰的答案。我们只有一个特别大的愿景、特别宏大的想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个小插曲：那时候我其实已经怀孕八周了，怀的是我儿子。我觉得在所有联合创始人里，可能只有 Dario 知道这件事，甚至我都不确定他当时是不是已经知道了（笑）。所以那段时间，真的发生了太多事情：口罩、保持社交距离，一切都很混乱。但与此同时，我们又怀抱着一个巨大的梦想，无论在个人层面还是职业层面，那都是一个重大时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那一刻，你们觉得 OpenAI 做错了什么，才让“离开”这件事即使有很大风险也值得？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我们并不是“逃离”什么，更像是在“奔向”某个目标。我的意思是，我们这群联合创始人，彼此认识的时间其实非常久了，不只是 OpenAI 这段经历。比如 Dario、Chris Olah、Tom Brown 之前就在 Google Brain 一起共事；Jared 曾是 Dario 的研究伙伴；Dario 和我是兄妹。我和 Chris 已经认识十三年了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们是一群长期一起工作、在价值观上高度一致的人。我们都深信，人工智能有着极其巨大的正向潜力，但要真正释放这种潜力，必须极其严肃地对待风险问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时间点，我们开始想，如果能从一开始就创办一家把“安全与可靠性”放在一切核心位置的公司，会怎么样？我们内心其实也相信，这样的理念不仅有伦理意义，从商业角度看也同样有价值，甚至会成为一种优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时很多人认为，“安全”和“商业成功”是相互冲突的，但我们反而相信，这两件事是高度相关、彼此强化的。现在回看，这个想法在当时确实听起来很激进、很新，但那正是我们创立 Anthropic 的根本动因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：把“我们非常重视安全”这句话，真正落到可执行的策略上来看，现在最让你担忧的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得在安全层面，大概可以分成两个方面来说。首先是技术安全本身，这里面其实还有大量非常有意思、但尚未被完全发现和解决的技术问题。我认为 Anthropic 一直在努力成为这个领域的引领者，至少是积极推动者。无论是我们在机制可解释性上的研究，还是Constitutional AI，本质上都是在做一件事：想办法把“护栏”直接内建进模型里。我们的技术团队花了非常多时间去琢磨，怎样才能真正从模型内部把这些安全机制做好。但现实是，这件事永远做不完，而且模型变聪明的速度实在太快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是技术对整个社会层面的影响。这一点我们也公开谈过很多次。Anthropic 在这方面相对比较“异类”，我们会发布大量研究，去探讨人工智能可能带来的社会影响。比如我们最近发布过一份报告，讨论 AI 可能带来的经济层面影响，以及对劳动力市场的冲击。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们之所以尽可能透明，是因为我们真的认为，提前面对潜在问题，总比事后补救要好。作为一家公共利益公司（Public Benefit Corporation），我们觉得公开讨论这些问题本身就是我们的责任。当然，我们并不认为 Anthropic 能单独解决所有问题，但我们必须和公民社会、政府以及更多相关方一起讨论：当人工智能开始能够完成大量人类日常工作的那一天，世界会发生什么变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：“激进透明”似乎已经成了你们文化的一部分。你们也发布过研究，显示在面对“生存威胁”的极端情境时，Claude 在绝大多数情况下会选择勒索，而其他模型也有类似表现。你们把这些东西公开出来，几乎就像一条公共安全提示：这是这项技术现在能做到的事情，而这正是我们要解决的问题。那在这些案例之后，当你们进行方向调整时，最紧迫的安全重点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：确实，这种做法在行业里并不常见。很多人都会觉得，一家公司这么公开地谈论自己技术的风险和潜在伤害，是一件很不寻常的事情。我们之所以这么做，有几个原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，作为一家公共利益公司，这本身就是我们使命的一部分。我们确实相信 AI 有巨大的正向潜力，比如我们真心觉得，未来它可能在治愈疾病等领域发挥颠覆性的作用。但要真正实现这些美好愿景，就必须把最棘手、最困难的问题先解决好。从这个角度看，越是坦诚地谈风险，对所有人反而越有利。因为我们的目标不是制造恐慌，而是防止坏事发生，好让这些积极的成果真正落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个原因是，我们相信，更充分的信息和更开放的讨论，通常会带来更好的结果。我们很幸运，处在一个可以第一时间看到风险信号的位置，也有条件把这些信息讲清楚。比如我们可以明确地说：Claude 可能被用于网络攻击，这是一件必须高度警惕的事情。而且如果这种情况发生在我们身上，很可能也会发生在其他前沿模型开发者身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在安全、信任与防护这些领域，很多工作其实是可以、也应该跨公司协作的。把趋势、问题用清晰、易懂的方式公开出来，本身就是降低整体风险的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们经常会做一个反事实思考：如果你是一家上一代的技术公司，比如社交媒体平台，如果可以回到过去，提前知道这些平台后来带来的社会后果，你会不会选择做出不同的设计决策？Anthropic 想做的就是尽量在今天问自己这些问题。我们当然无法预测未来，但至少要问清楚：如果我们已经意识到某些风险的可能性，那我们今天有没有尽最大努力去讨论它、应对它、降低它？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在算力上是不是花太多了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：回看过去半年，整个行业签下的算力合同数量可以说非常惊人。与此同时，Gemini 在模型性能上也明显追近了差距。不少分析师指出，Google 的优势在于它几乎掌控了整个技术栈，从芯片、云业务，到各种可以直接部署技术的产品入口。而 Anthropic 现在也开始自建一方基础设施，在既有云资源承诺之外，又投入五百亿美元，在纽约和德州建设数据中心。这是不是你们赢得 AI 竞赛战略的一部分？要想胜出，就必须自己做基础设施，掌控更多垂直整合的能力吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：这是个很有意思的问题。人工智能领域的一个核心挑战在于，如果你想训练真正处在前沿的大模型，对算力以及相关资本的要求实在是太高了。Anthropic 一直以来的目标，是在这种“必须大量消耗算力”的现实下，尽可能理性、高效地使用我们手头的资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，长期以来，Anthropic 拥有的算力和资本，其实都只是竞争对手的一小部分，但在过去几年中，我们却相当稳定地推出了性能最强、效果最好的模型之一。我认为，这一方面来自团队本身的质量，另一方面也来自我们的价值取向，即用更少的资源，做更多的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，面向未来，算力需求确实会非常巨大。如果我们要随着公司规模扩大，继续站在技术前沿，那毫无疑问，我们也需要更多算力支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：粗略算一笔账，Anthropic 的算力投入大概在一千亿美元量级，而你们的竞争对手 OpenAI，据说已经到了万亿美元级别。从整个行业来看，我们是不是在算力上花得太多了？投入是否已经超过了大模型目前能够可靠变现的能力？还是说，这是服务用户所必需的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：很多被拿出来讨论的数字，其实并不完全可比，因为这些交易的结构本身就差异很大。有些是提前锁定购买权，有些是长期承诺，形式并不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从根本上讲，整个行业押注的是这样一个判断：如果你想在未来几年里拥有训练前沿模型所需的硬件资源，就必须非常早地、非常大规模地提前投入。如果你去问我的一些技术同事，他们会说一件很有意思的事：即便我们是 scaling laws的提出者之一，理论上早就相信“更多算力会带来更好结果”，但实际进展依然一次次超出我们的预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dario 也公开谈过这一点：无论是模型性能还是收入规模，很多指标看起来都呈现出一种指数级的增长。当然，我们内部也常说一句话：指数增长会一直持续，直到某一天不再成立。每一年我们都会怀疑：“不可能再这样增长下去了吧？”但结果是，每一年它都继续成立。所以这确实是个无法确定未来的问题，但至少到目前为止，年复一年的性能提升，看起来仍然相当接近指数曲线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：过去几个月我们也频繁讨论一种“循环式交易”：模型公司、硬件供应商、云厂商之间，通过股权换芯片、资源互换等方式形成闭环。这种结构中，哪些是健康的飞轮效应？又有哪些地方值得警惕？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我当然不能评论 Anthropic 以外的具体交易，更不可能讨论任何交易细节。但我想说的是，这些交易之间差异其实非常大，并不存在一种统一模式。不同参与方，对于算力和资本的理解方式本来就不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回到 Anthropic 自身，我们一直以来都是用相对更少的资源，去完成更多事情。我们的期望是，未来这些模型提供方，确实能成为你刚才说的那种“飞轮”的一部分。事实上，我们已经在某种程度上看到了这种趋势：Claude 是目前唯一一个同时在微软、亚马逊云科技和 Google 三大云平台上提供的前沿模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其在企业市场，我们会持续创造出大量价值。对 Anthropic 来说，我们一直是以企业需求为优先。而在过去一年左右的时间里，有不少时间段，我们甚至出现过“需求大于供给”的情况，从算力角度来看，Claude 的需求一度超过了我们能提供的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在硬件层面，你们是如何考虑芯片折旧的？是按三到四年的生命周期来规划，还是会把 GPU 用到十年，把整个可用寿命都榨干？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：坦率说，我并不是芯片方面的专家，我的一些同事会更适合回答细节。但从宏观上看，它和大模型的发展其实很相似。每一代新的前沿模型，性能都会好到让高端用户更愿意使用新模型，硬件也是如此。新一代芯片往往在性能、成本效率、能效上都会有所提升，所以，能尽早用上新一代芯片，本身就具有很高的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们是不是正处在一个 AI 泡沫里？我不是说技术不真实，而是支出增长曲线，是否已经跑在了收入增长曲线前面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我会把这个问题拆成两个层面来看：技术层面和商业层面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术层面，我们非常有信心。无论是 Anthropic 内部，还是 Dario 最近的公开表态，我们都认为，从纯技术角度看，进步并没有放缓。未来当然无法预测，但截至目前，模型仍然在以相当稳定、快速的节奏变得更强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在商业层面，这个问题就复杂得多。无论技术多先进，把它真正落地到企业或个人场景中，都需要时间。关键问题在于：企业，尤其是企业，能以多快的速度真正利用这些技术？也许Claude 5、Claude 6，在性能上依然是按同样比例提升的，但在组织内部推广和落地，可能会因为“人”的因素而遇到瓶颈：变革管理很难，采购流程很慢，很多应用场景一开始根本想不到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，真正值得观察的是：技术扩散到经济体系中的速度，是否能持续匹配技术本身的加速速度。这也是我认为最有挑战、也最值得持续关注的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：基于刚才的讨论，你觉得我们现在是不是在模型公司，或者在硬件供应链上投入得有点过头了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：从某种角度看，这个市场其实很小。说“小”听起来有点奇怪，毕竟金额巨大，但真正参与其中的玩家数量并不多。我也不完全确定该如何解读这一点，它有点不寻常。不过到目前为止，我们看到的情况是：更好的硬件，回报非常高，哪怕模型只提升一点点，比如0.01的性能提升，回报同样很可观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Anthropic 的历史中，这一点几乎一直都成立。所以我不太愿意直接用“过度投资”来形容，但我确实认为，这种参与者数量有限的结构值得警惕，一旦链条中的某个环节出了问题，后果会是什么？这是个很重要、也很有意思的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得我们现在大概处在这个周期的什么位置？不管你把它叫作泡沫破裂，还是一次正常的修正，考虑到最近出现的各种乱象和泡沫迹象，这种调整会不会在未来六到十二个月内发生？如果会的话，Anthropic 现在是否已经在为这种下行风险做准备？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：对于 Anthropic 来说，我们一直把自己看作是资本的理性、负责任的管理者。这一点从成立之初就是我们的重点。对我们而言，每一分算力、每一美元投入都非常重要，它们要么意味着我们能训练出更好、更安全的模型，要么意味着我们能服务更多客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我更愿意相信，我们对模型质量的预期、训练所需的算力、推理阶段服务客户所需的算力，以及我们能持续为客户创造的价值，都有一个相对合理的判断。当然，没有人能做到完美预测。但至少从一家负责任企业的角度来说，不管市场环境怎么变化，我希望我们都能处在一个相对稳健的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于整个市场会发生什么，这确实很难一概而论。但就 Anthropic 自身而言，做资本的负责任管理者，始终是我们的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们本身就是个做 to B”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：聊聊 Anthropic 接下来的资本路径吧。收购这条路，考虑到反垄断和你们目前的规模，基本可以排除了。那 IPO 会不会是2026年的一个选项？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：目前我们没有任何可以对外公布的具体计划。正如我之前说的，Anthropic 一直在努力以负责任的方式使用手中的资本。我们也始终在权衡：在哪里、以什么方式获得所需的资本，才是最合适的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Amazon 仍然是你们最大的战略支持方，但你们的股东和合作方阵容也在不断扩大，比如 Google 既是投资方又是云合作伙伴，还有 Microsoft、Nvidia。与此同时，Google 自身也在全力参与模型竞争。当你的合作伙伴本身方向和野心并不完全一致时，你们是如何处理这些关系的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得这恰恰说明了市场对这项技术的强烈需求。Anthropic 的模型能够同时在三大云平台上提供服务，本身就很有意思，甚至包括彼此之间存在竞争关系的云厂商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的直觉是，这些超大规模云厂商都在密切观察自己的客户在业务层面发生了什么。财富五百强企业可能用的是一家云，也可能是两家、三家，但现在几乎所有企业都有一个共同点：他们觉得自己必须要有 AI 解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我们看到的情况是，由于 Anthropic 特别专注企业场景，我们往往正是客户最想用的那个模型。如果企业无法使用 Claude，反而会对他们的业务造成伤害。所以，对我们来说，最重要的一点就是：在客户需要的地方出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些客户会直接用我们的一方服务，但更多客户已经和云厂商建立了长期合作关系，通过云平台接入大模型，对他们来说是非常自然的一条路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从一开始，Anthropic 似乎就不像 OpenAI 那样，急于抢占大众文化层面的心智，而是选择把筹码压在企业客户身上。事实证明，这可能是一个更聪明的选择。现在很多人都在追赶你们，试图在企业市场分一杯羹。你们当初是怎么判断，企业才是最值得投入的市场？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：你给我们的评价有点高了，我不敢说我们一开始就“确定”这条路一定是对的，但我觉得可以从两个方面来看。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，Anthropic 这个组织本身就非常适合做一家 B2B 公司。我们对可靠性、安全性和安全边界的重视，是写进公司基因里的。这也是我们创立 Anthropic 的初衷之一：既要释放 AI 的巨大潜力，也要尽可能降低风险。而事实证明，企业客户非常看重这一点。我从没听过哪家企业客户对我说：“如果 Claude 能多一点幻觉、多生成点有害内容就好了。”从这个角度看，企业对安全性的高要求，反而让 Anthropic 从第一天起就处在一个很有优势的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，是一种更偏经济学层面的判断，当时也可能判断错了。我们认为，这些模型虽然在娱乐层面也很有吸引力，但从长期看，它们更像是帮助人类完成高价值工作的工具。无论是现在 Claude 被大量用于写代码，还是用于总结复杂信息、做金融分析和数据分析，我们在2020年底、2021年初，就已经隐约看到了这样一种未来：模型可以承担大量工作场景中需要高智力投入的任务。而我们认为，这是一个非常大的市场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这两个因素叠加在一起，让我们觉得，把 Anthropic 做成一家以企业为核心的公司，是一条合理的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：企业客户通常既强调安全，也永远希望有更多功能、更强的Agent能力。有没有一些需求，是客户明确提出来了，但因为安全护栏的原因，你们暂时还不愿意提供的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：有意思的是，到目前为止，我们还没有遇到那种特别明确的场景：安全和功能之间形成了正面冲突。更多时候，挑战在于如何确保我们发布的模型始终处在前沿水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实有过这样的情况：我们在内部已经有一个模型准备好了，但在正式发布之前，还需要做更多安全测试。客户并不会直接看到这一点，但这是我们必须坚持的过程。所以如果说安全和产品之间的“交汇点”在哪里，那大概就是：确保我们推向市场的模型，已经在安全性上做到我们能力范围内的最好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“AI 原生”创业公司蓬勃发展&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：谈到规模发展，很多当初的决策其实都需要随着时间不断演化。比如一开始，Anthropic 曾明确表示不会接受来自中东的资金，但在最近一轮融资中，这个立场发生了变化。你们是如何在坚持最初的原则、以及为了在激烈竞争中生存和发展而必须做出调整之间取得平衡的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我认为，在最重要的层面上，Anthropic 随着规模扩大，其实一直坚守着自己的价值观。尤其是我们的 PBC（公益型公司）结构，以及“公共利益公司”作为北极星一样的存在，对我们非常重要。正是因为有这样一个长期愿景，当具体问题出现时，我们总会回到一个核心判断：我们现在做的事情，是否真的在为公共利益服务？是否是在努力让 AI 的转型过程走得更好？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，不同的人对“公共利益”具体意味着什么，理解上可能会有差异，但我们对新员工、候选人、投资人都非常坦诚：这就是 Anthropic，这就是我们的价值观。正因为如此，大多数情况下我们都能比较顺畅地做出判断。只是正如你所说，随着公司规模变大，确实会遇到一些处在灰色地带、更加棘手的情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有一种批评声音认为，把“安全使命”放在如此核心的位置，实际上会形成一种“可防御壁垒”，让最早成立的几家大模型实验室更容易在监管环境下维持竞争优势，而后来进入的初创公司，由于没有经历同样的积累过程，反而更难追赶。你怎么看这种说法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：这点挺有意思的。虽然我现在没有具体数据在手，但我印象中，绝大多数初创公司其实都是云计算用户。真正被算力和资本门槛高度限制的，是“前沿模型”的研发。正如我们之前聊到的，要成为一家前沿模型实验室，成本确实非常高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在 Anthropic，我们看到的是一个正在蓬勃发展的生态系统，我们称之为“AI 原生”创业公司。就像五到十年前大家谈“数字原生”企业一样，现在出现了大量“AI 原生”公司：它们的产品从一开始就是围绕人工智能能力构建的，而其中绝大多数都是构建在云基础设施之上的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为，我们对这个生态系统的影响，最终取决于我们是否能够持续打造行业里最优秀、最安全的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但现在“安全”并不是一个特别受欢迎的立场。一个月前你们和 David Sacks 有过一些隔空讨论，Dario 也写了一篇文章回应。面对这样的情况，你们如何避免让外部环境干扰你们真正的技术工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我认为 Anthropic 一直努力把重点放在“政策”而不是“政治”上。我们在很多议题上，其实能够跨越党派找到共识，而这些议题正是美国公众真正关心的事情。比如，如何保持美国在全球 AI 领域的领先地位，又比如如何确保我们开发出来的模型真正对人有益、对孩子有益、对使用它们的成年人也有益。在这些问题上，其实存在着相当多的共识空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，人工智能仍然是一个非常新的领域。正因为如此，我们始终保持开放和好奇，去探索以安全、可靠的方式发展这项技术的最佳路径。我们也一直在学习，这也是为什么我们会大量公开发布研究成果的原因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在还会去思考“有效利他主义”（Effective Altruism）吗？我知道你之前说过，这个标签在你看来已经有些过时了，也不再是公司当前叙事的一部分，但无论是早期招聘还是融资阶段，它确实曾经深深嵌入你们的创始故事里。那它现在在公司内部的文化中还留下了些什么吗？还是说，到 2025 年，这更多只是外界投射到 Anthropic 身上的一种标签？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：我觉得“投射”这个词可能更接近。Dario 之前也谈过这个问题。你得回到一个背景：在 AI 非常早期、差不多二十年前的时候，真正认真思考“AI 可能会变得如此强大”的人其实非常少。而恰恰是那一小撮人，往往同时也非常关注风险问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你会看到，早期的 OpenAI，以及后来成为 Anthropic 创始成员的一些人，最初确实是从“风险”这个角度出发的，我们在担心技术可能出什么问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为 Anthropic 最大的不同之处在于：我们同样高度关注技术的“正向价值”和“上行空间”。我们一直在思考，人工智能在医学、生命科学、医疗健康、金融服务，以及整个经济体系中，究竟能带来多大的积极影响。当然，如果我们不能把它做得足够安全，事情也可能会走向非常糟糕的方向，这两点是并存的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“不要相信炒作”，AGI理念过时了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Anthropic 的品牌似乎自带一种“神秘感”，我不太好精准形容，但感觉公司内部的人，几乎把它当成一种信念体系。你会如何描述你所塑造的 Anthropic 员工文化？另外，我也注意到，虽然最近几个月你们变得更公开了一些，但整体来看，你们仍然非常克制，往往在真正准备好之前，很少对外释放信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：你这么说真的很善意，我不知道是不是“神秘感”，但我很感激这样的评价。对我们来说，有一个内部反复强调的价值观，就是“不要相信炒作”。这听起来好像很小，但我觉得它其实回到了我们之前讨论的那些关于经济、商业的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 从来不是为了博关注、抢头条而存在的。我们真正关心的是把事情做好，无论是在模型训练层面，如何以公平、负责的方式训练模型；还是在客户层面，如何每天都真正为客户提供价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在 AI 领域的炒作非常多，而我们是一家以企业客户为核心、B2B 导向的公司，这在某种程度上让我们更加“脚踏实地”。我们的目标很简单：为企业创造真实价值。这项工作往往不那么炫目，但它能帮助我们不被泡沫裹挟，始终记得我们当初为什么要创办这家公司——我们是一家公共利益公司，我们关心的是长期价值。如果没有这个北极星，其实很容易迷失方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Yann LeCun 以及其他一些机器学习领域的“老一代”学者认为，大语言模型并不能通向 AGI，他们转而研究世界模型，认为还需要一些关键突破才能迈向下一阶段。你怎么看？你认为真正解锁 AGI 所需要的突破是什么？未来你们是否需要超越 LLM，才能在行业中保持竞争力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：AGI 这个词本身就挺有意思的。Dario 也说过，很多年前，这个概念是有意义的，它帮助我们思考“什么时候 AI 会和人类一样强”。但有趣的是，按照某些定义，我们其实已经超过了这个标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如说，Claude 写代码肯定比我强，这个门槛不高。但它也已经能在一定程度上，达到甚至接近 Anthropic 许多工程师的水平。要知道，我们雇的可是世界上顶尖的一批工程师，而他们中的不少人都会说，Claude 已经能完成他们相当一部分的工作，或者极大地加速他们的效率。这本身就很疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，另一方面，Claude 依然做不了很多人类能做的事情。所以我觉得，AGI 这个概念本身可能不是“错了”，而是有点过时了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于是否需要新的突破才能继续前进，老实说，我们并不知道。技术发展的路径，本身就是科学与工程的复杂混合体。而我觉得实验室最特别的地方就在于：不同团队会用完全不同的方式去逼近同一个目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至少从目前来看，进展并没有放缓。当然，一切都是“直到它真的放缓为止”。如果让我下注，我会说，能力还会在相当长一段时间内继续提升，我们也应该为这样的世界做好准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你和 Dario 的能力结构差异很大，你在哪些方面补足了他？你是如何帮助他把想法打磨得更锋利的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Amodei：能和我的哥哥一起经营 Anthropic，真的是一种“特权”。我感觉我们认识彼此一辈子了，他在我出生前独自生活了四年，挺惨的（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dario 有一种非常罕见的能力，仿佛能“看到未来”。虽然我总说没人真的知道未来，但如果真有这样的人，那大概就是他。从技术视角来看，他对技术走向、对社会和组织意味着什么，有着极其敏锐的直觉，这是一种真正的愿景型领导力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我更偏向实务型。我非常喜欢运营组织，我大部分时间都在和高管团队一起工作，比如搭建团队、招聘负责人、思考客户真正需要什么、如何为企业创造价值、如何构建让公司长期可持续的合作关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得 Dario 和我彼此成就。他会不断把我拉回更宏大的视角，而我则专注于如何打造一家能长期存在、可持续发展、聚集了一群真正想做我们五年前一起立志要做的事情的优秀人才的组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GMXnmaky9FY&quot;&gt;https://www.youtube.com/watch?v=GMXnmaky9FY&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fZkrM56MKzx9s8dNHYZ7</link><guid isPermaLink="false">https://www.infoq.cn/article/fZkrM56MKzx9s8dNHYZ7</guid><pubDate>Mon, 12 Jan 2026 07:02:44 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>“死了么”APP爆火，3人开发成本1500元：不改名；姚顺雨入职腾讯后首发声；微软本月大裁员，至少涉1.1 万人；字节实习生全面涨薪 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;引言：唐杰、杨植麟、林俊旸、姚顺雨聚会：AI发展的共识和差异;“死了么”APP爆火，开发者：用户数翻了50倍，尚不准备改名；消息称微软本月将启动新一轮大裁员，规模达 1.1 万至 2.2 万人；字节实习生全面涨薪，最高涨幅达150%；马斯克：X平台将于七天内开源其算法；消息称约翰・特努斯成库克头号苹果接班人，曾主导 iPhone Air 项目；OpenAI预留500亿美元员工股权激励池；王腾官宣创业：核心成员来自小米、华为，薪资福利基本看齐大厂；京东将推出全年龄段人群AI玩具……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;唐杰、杨植麟、林俊旸、姚顺雨聚会：AI发展的共识和差异&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在近日的AGI-Next 前沿峰会上，唐杰、杨植麟、林俊旸、姚顺雨等行业标杆人物，与张钹院士共同勾勒出大模型发展的新图景，围绕技术突破、行业分化、范式变革与中国 AI 的未来展开了一场思想碰撞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术发展的核心议题上，各位领军者达成了“突破现有瓶颈、迈向多元智能”的共识。智谱创始人唐杰直言，中国开源大模型虽成果斐然，但与美国闭源大模型的差距可能仍在拉大，行业需保持清醒认知。他提出，大模型的下一阶段应借鉴人脑认知过程，重点突破三大能力：多模态“感统”能力，实现视觉、声音、触感等多源信息的统一感知；构建全人类“第四级记忆”，解决模型记忆与持续学习不足的问题；探索反思与自我认知，挖掘大模型自主意识的可能性。2026年，智谱将聚焦架构创新、多模态感统等方向，推动AI进入长任务场景并实现具身智能，同时预判今年将成为AI for Science的爆发年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;月之暗面Kimi创始人杨植麟则从Agentic时代的技术架构切入，强调提升token efficiency与实现long context的双重重要性。他认为，前者能以更少token达到同等效果，后者可突破传统架构局限，支撑复杂Agent任务，二者结合方能实现更高水平的代理智能。更具启发性的是，他提出智能具有“非同质化”属性，未来的技术升级不仅是算力的堆砌，更关乎“品味”——即对AI价值观与形态的深层理解，这种差异性将催生出更多新颖应用场景。面对AGI潜在风险，杨植麟秉持开放态度，认为AGI是提升人类文明上限的关键工具，应在风险可控的前提下持续迭代突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通义Qwen技术负责人林俊旸则将目光投向物理世界，提出打造Multimodal Foundation Agent的愿景。他认为行业发展“殊途同归”，全模态模型与具身推理是核心方向，Agent将从数字世界走向物理世界。林俊旸描绘了具体的落地场景：数字特工可实现GUI操作与API调用，物理特工则能完成斟茶倒水等实体交互动作，这种从虚拟到现实的延伸，为AI应用开辟了广阔空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为压轴嘉宾，张钹院士从旁观者视角给出了深刻洞见。他指出，大模型当前擅长跨领域泛化，但落地应用需实现跨任务泛化，重点解决分布外、长尾场景的泛化难题，具体应推进多模态、具身交互、结构化知识对齐等六大方向。在人机关系上，他大胆质疑“机器必须与人类对齐”的传统认知，认为人类存在固有缺陷，无需让AI完全复刻；而AI治理的核心，不应是约束机器，而是规范研究者与使用者的行为。值得关注的是，张院士一改以往态度，鼓励最优秀的学生投身创业，认为人工智能时代的企业家应承担起将知识、伦理与应用转化为通用工具的使命。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;圆桌对话环节，嘉宾们围绕行业分化、范式变革、Agent战略与中国AI的胜算四大议题展开深度探讨。腾讯首席科学家姚顺雨从跨中美视角指出，To C与To B场景的模型需求已分道扬镳：To C用户对强智能需求有限，To B领域则呈现“智能即生产力”的鲜明特征，模型强弱分化将愈发明显。在范式变革方面，姚顺雨提出自主学习已实际发生，只是尚未形成颠覆性感知；唐杰则预判2026年将出现新范式，单纯依靠扩算力、扩数据的Scaling模式已难以为继，创新是唯一出路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于中国AI的全球竞争力，嘉宾们既正视差距也保持信心。姚顺雨认为中国团队在快速复现与局部优化上具备优势，但缺乏敢于探索未知的“冒险家”；林俊旸坦言美国在算力投入上领先1-2个数量级，中国团队领先概率约为20%，但“穷则思变”可能催生创新机会；唐杰则强调，凭借敢冒险的年轻一代、良好的发展环境与持续深耕的定力，中国AI有望在长期竞争中实现突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;“死了么”APP爆火，开发者：用户数翻了50倍，尚不准备改名&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 年 1 月，郑州月境技术 3 人 95 后团队开发的 8 元付费 APP “死了么” 爆火，苹果付费软件排行榜登顶，用户数较此前翻 50 倍仍在上涨。据悉，该 APP 专为独居人群设计，2 日未签到即自动向紧急联系人发邮件，因名字有传播力、需求旺盛等爆火，团队表示暂不改名，计划上线短信提醒、留言等功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该软件不需注册登录，首次使用只需填写本人姓名与紧急联系人邮箱即可。每天打开应用轻轻一点完成签到，后台自动监测状态。系统有一个异常未签到自动通知的功能，如果用户连续2天没有在应用内签到，系统将于次日自动发送邮件告诉对方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其背后公司名为月境（郑州）技术服务有限公司，2025年3月份才成立，注册资本10万元。创始人之一小郭对媒体介绍，团队有3人，一位是朋友，一位是网友，都是95后。这款APP耗时1个月完成，开发成本约1500元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，“死了么”在2025年中旬上线，不过期间团队未花过多精力打理，在一个月前才做了一次更新。上线后很长一段时间里用户量很少，团队也不擅长营销，直到最近突然爆火，用户数达到之前的50倍，目前热度还在上涨。不过由于用户规模数能直接推导出团队收益，小郭表示，目前不便透露具体用户规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;消息称微软本月将启动新一轮大裁员，规模达 1.1 万至 2.2 万人&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 7 日消息，据报道，微软公司计划于 2026 年 1 月启动新一轮裁员。预计全球范围内裁员规模将达到 1.1 万至 2.2 万人，约占其全球约 22 万名员工总数的 5% 至 10%。此次裁员预计将在 1 月第三周实施。有员工透露，微软 Azure 云团队、Xbox 游戏部门以及全球销售部门将是裁员的重点领域。截至目前，微软尚未证实该计划。微软在 2025 年尽管全年营收与利润保持稳健态势，该公司仍通过多轮裁员削减了超过 1.5 万个岗位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，微软正加大对人工智能系统的投入力度。仅在 2026 财年第一季度，其资本支出就高达 349 亿美元（现汇率约合 2441.36 亿元人民币）。该公司预计全年总支出将突破 800 亿美元（现汇率约合 5596.24 亿元人民币），超过 2025 财年水平。这笔资金的大部分将用于数据中心、芯片及人工智能工具的建设与研发。分析师认为，受此战略调整影响，微软正将资金从人力成本转向长期技术资产投资。因此，中层管理人员及传统产品团队将面临更高的裁员风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;字节实习生全面涨薪，最高涨幅达150%&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月5日，有消息称字节跳动实习生全面涨薪，覆盖技术、产品、运营等多个岗位，薪资标准自2026年1月1日起正式生效。其中，技术类实习生日薪调整至500元，较此前上涨25%。产品类岗位从每日200元提升至500元，较此前上涨150%。此外，运营、设计、市场、职能、销售等其他岗位也均有不同程度涨薪，调整后日薪区间涵盖100余元至400余元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;需要注意的是，此次公布的涨薪标准主要适用于北上广深杭等一线城市。同时，具体薪资仍会根据岗位类型、所在业务线等因素有所区别，并非完全统一。通过查询招聘软件发现，目前北京地区的产品实习生日薪已调整为500元，运营、营销类实习生日薪则为350元/天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，字节跳动 2025 年 12 月发布面向全球员工的内部邮件，宣布继续加大人才投入，提高薪酬竞争力、提升期权激励力度。具体包括以下措施：增加奖金（含绩效期权）投入，2025 全年绩效评估周期相比上个周期提升 35%；大幅增加调薪投入，较上个周期提升 1.5 倍；提高所有职级薪酬总包的下限（起薪）和上限（天花板）。该公司表示，此举系为确保员工薪酬竞争力和激励回报在全球各个市场都“领先于头部水平”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克：X平台将于七天内开源其算法&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交媒体平台X创始人埃隆・马斯克于周六表示，该平台将在七天内面向公众开源其新版算法，这一算法包含用于决定向用户推荐哪些帖文及广告的相关代码。“这项举措将每四周推行一次，同时会附上详尽的开发者说明文档，助力大家了解算法的具体更新内容。”身为X平台所有者的马斯克在该平台发布的一则帖子中如此表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;消息称约翰・特努斯成库克头号苹果接班人，曾主导 iPhone Air 项目&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 9 日消息，报道称伴随着现任首席执行官蒂姆・库克年满 65 岁，且其本人有意减轻工作负荷，苹果公司已加速接班人计划，而约翰・特努斯再次被认为是接班热门人选。媒体援引博文介绍，现年 65 岁的库克向高层坦言感到疲惫，希望减轻工作负担。若库克决定卸任 CEO 一职，极有可能转任苹果董事会主席。在众多候选人中，现任硬件工程主管约翰・特努斯尽管行事低调，但已跃升为头号热门人选。特努斯现年 50 岁，这一年龄恰好与库克 2011 年接替乔布斯时的年龄相同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知情人士透露，特努斯之所以脱颖而出，源于其在产品定义与商业利益间“穿针引线”的精准把控力。据内部人士回忆，2018 年前后，苹果为了提升摄影与增强现实（AR）体验，曾考虑在 iPhone 上引入一种微型激光（LiDAR）组件。然而，该组件高达 40 美元的单项成本将严重压缩利润。特努斯当时果断建议：仅在价格更高的 Pro 机型上搭载该组件。他认为，购买 Pro 系列的忠实用户更愿为新技术买单，而普通用户对此并不敏感。这一决策不仅保住了利润，也确立了产品分级策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对外界关于其缺乏创新能力的质疑，Ternus 的支持者指出，他实际上深度参与了近年来多个关键产品的研发。值得注意的是，备受瞩目的 iPhone Air 以及即将面世的折叠屏 iPhone 均由他牵头主导。这些项目显示，Ternus 不仅具备卓越的执行力，在推动产品形态创新方面同样拥有实际战绩。此外在管理风格方面，特努斯被认为与库克高度相似。他于 2001 年加入苹果，以注重细节和深谙庞大的供应链网络著称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI预留500亿美元员工股权激励池&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日消息，据外媒报道，人工智能公司 OpenAI 去年秋季设立了一项规模达约 500 亿美元的员工股票激励池，相当于公司当时估值的约 10% 股份，该估值基于 2025 年 10 月约 5000 亿美元 的公司估值水平。报道指出，此前 OpenAI 已向员工授予约 800 亿美元的已归属股权，本次新增的股票激励池与既有部分合计约占公司总股份的 26%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去一年中，OpenAI 的估值经历了快速增长。2025 年年中公司通过一笔员工股份二级市场交易达到约 5000 亿美元估值，高于前一次由 SoftBank 等领投的 3000 亿美元融资轮。二级股权交易不仅为员工提供了变现渠道，同时也被视为衡量市场对 OpenAI 增长前景信心的一个指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一大规模股权激励池反映了 OpenAI 在全球 AI 竞争中对人才吸引与保留的高度重视。在人工智能研发与产品商业化日益加剧的背景下，顶尖 AI 研究人员和工程师成为市场追逐的稀缺资源，竞争对手包括 Meta、Google 等科技巨头均提供了丰厚的股权激励条件。在行业快速发展与人才争夺日益激烈的背景下，OpenAI 的股权策略旨在通过高比例激励计划锁定核心技术人才，同时支持公司未来产品和业务长期增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;王腾官宣创业：核心成员来自小米、华为，薪资福利基本看齐大厂&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日，王腾在社交平台公布最近情况。王腾称，从小米离开后开始筹备创业，最近新公司已经成立，公司取名为“今日宜休”，目标是通过研发睡眠健康相关的产品，让大家能拥有更好的精力状态。王腾表示，目前已经组了一个初创团队，核心成员主要来自小米、华为等头部科技大厂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王腾还放出招聘广告，重点招聘软硬件产品经理、 健康/AI算法工程师、脑科学睡眠健康专家等岗位。王腾还解释为何选择睡眠健康、精力管理方向：1. 首先睡眠、精力已经成为每个人都关心的健康问题。2. 社会对睡眠的价值理解有待提升。3. 新时代下AI大模型发展迅速，让很多产品的体验能大幅提升。公开信息显示，北京今日宜休科技有限责任公司成立于2026年1月6日，由王腾持股55%并担任法定代表人，注册资本是100万人民币，注册地址是北京市海淀区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3f/3f405b05a6f547704545d3cf7428ce71.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此前报道，去年9月8日，小米发布内容通报，原小米中国区市场部总经理、REDMI品牌总经理王腾因泄密被小米公司辞退。11月份，王腾发文称告别手机行业。他表示前段时间因为自己的问题离开小米，最近也有一些公司发来邀约，但综合竞业限制和个人兴趣的考虑，想跟手机行业说声再见了，愿还在这个行业的朋友们继续加油，期待更精彩的产品出现。王腾还透露11月开始准备尝试些新的赛道，大的方向是科技+健康领域，具体还在筹备中，“迎接新的挑战，正是闯的年纪。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;京东将推出全年龄段人群AI玩具&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日消息，据媒体报道，京东成立“变色龙业务部”，全面承接JoyAI App、JoyInside、数字人等核心AI产品的打造与商业化。报道称，全新的第二批AI玩具已在筹备中，此次新品将推出面向全年龄段人群的AI玩具，将于1月中旬全面上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，在 2025 世界人工智能大会（WAIC）期间，京东正式宣布旗下大模型品牌升级为 JoyAI，以及京东在大模型方向的技术进展和JoyAI应用全景图，同时也发布了全新的附身智能品牌JoyInside。据当时介绍，JoyAI大模型拥有从3B到750B全尺寸模型家族，且通过动态分层蒸馏、跨领域数据治理等创新技术，大模型推理效率平均提升了30%，训练成本降低70%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，谈到JoyInside，截至2025年7月，已有众擎、云深处、商汤元萝卜、火火兔、Fuzozo等数十家企业已正式接入，覆盖人形机器人、四足机器人、儿童玩具、AI潮玩等多类载体。另据京东官方披露，截止2025年12月，已有超4.5万家品牌接入数字人服务，数字人直播成本约为真人直播的1/10，平均转化率提升约30%。在2025年“双11”期间，采用数字人直播的商家数量同比增长近6倍，全年累计带动商品交易总额（GMV）达数百亿元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;蚂蚁美团联手投了一家AI硬件创企，前美团硬件负责人带队&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月5日，北京AI硬件创企Looki正式完成超2000万美元（约合人民币1.4亿元）A轮融资，本轮由蚂蚁集团领投，美团龙珠、华登国际、中关村资本跟投，老股东BAI资本连续两轮超额追投，阿尔法公社、同歌创投持续加码。在完成本轮融资后，Looki计划加快人才建设、模型迭代、产品研发及供应链整合，围绕AI原生硬件推进下一代交互设备的探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Looki成立于2024年5月，截至目前已连续完成4轮融资。该公司由两位卡内基梅隆大学（CMU）的校友联合创办，CEO孙洋曾任美团智能硬件负责人、Momenta高级研发总监，是Google Assistant早期创始成员之一。CTO刘博聪曾任美团自动驾驶算法负责人、Pony.ai创始成员。团队成员来自清华大学、北京大学、多伦多大学、伊利诺伊大学、伦敦政经等知名院校，曾就职于Google、Amazon、Qualcomm、字节跳动等公司，在AI算法、AI产品、硬件工程等方面具备丰富经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Looki发布的一段产品介绍视频中，CEO孙洋称，Luki L1自去年8月上线以来，已被不少用户当作“记录生活节奏”的常用设备使用。Luki还具备“主动AI”能力，如根据饮食、坐姿时间、行为节奏提出健康建议，例如“你今天已经喝了两杯咖啡，要不要换成水？”或者“你已经在桌前坐了一小时，要不要走一走？”等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智谱上市，唐杰内部信要求全面回归基础模型研究&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日智谱上市当天，清华大学计算机系教授、智谱创立发起人兼首席科学家唐杰发布内部信，宣布很快将推出新一代模型GLM-5。内部信还介绍了2026年智谱聚焦的三个技术方向，包括全新的模型架构设计，更通用的RL（强化学习）范式以及对模型持续学习与自主进化的探索。它们均围绕基础模型能力提升展开。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;上海又一GPU“四小龙”上市！&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;继沐曦股份、壁仞科技之后，上海又一家AI芯片企业成功上市。1月8日，上海芯片企业天数智芯登陆港交所，在1个月的时间内，上海已先后有“港股国产GPU第一股”的壁仞科技和科创板上市首日涨幅近7倍的沐曦股份，加上已完成IPO辅导冲刺科创板的燧原科技，上海GPU“四小龙”齐聚资本市场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;媒体从上海市经信委获悉，2025年1-11月，上海市集成电路产业营收规模3912亿元，同比增长23.72%，2025年全年产业规模预计超4600亿元，同比增长24%，五年间产业规模翻了一番多，超额完成“十四五”发展目标。集聚超1200家集成电路企业，汇聚全国约40%的产业人才、近50%的产业创新资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;天数智芯战略与公共关系部副总裁余雪松表示，作为国内首家开展通用 GPU 自主研发的企业，公司已完成从核心技术攻关到商业化落地的全链路贯通。“我们的研发团队有480人，平均拥有20年以上行业经验，超三分之一研发人员具备10年以上芯片设计与软件开发经验。包含架构、通用GPU IP及芯片设计、基础软件、软硬件协同等各领域的专家。”余雪松说。上海市经信委相关工作人员表示，除了上海GPU芯片“四小龙”（壁仞、沐曦、天数、燧原），光计算、近存计算等创新路线AI芯片企业也相继涌现，支撑国内大模型等新质生产力发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克回应英伟达自动驾驶AI模型：特斯拉正在做，达到99%很容易&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月6日消息，在2026消费电子展（CES）上，英伟达宣布推出Alpamayo系列开放式AI模型、模拟工具和数据集，旨在解决自动驾驶安全挑战。对此，马斯克回应称：“好吧，这正是特斯拉在做的。他们会发现，达到99%很容易，但要解决分布的长尾问题却非常困难。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Alpamayo平台的核心是Alpamayo 1模型，这是一款拥有100亿参数、基于思维链技术的视觉-语言-行动（VLA）模型。该模型可让自动驾驶汽车具备类人思维能力，即便在未经任何训练和标注的情况下，也能解决复杂的场景问题，例如在交通信号灯失灵的路口规划通行路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达还强调，Alpamayo模型并非直接在车内运行，而是作为大规模教师模型，供开发者微调并提取到其完整自动驾驶技术栈的骨干中。黄仁勋在声明中表示：“首款搭载英伟达技术的汽车将于第一季度在美国上路。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;硅谷科技初创公司兴起“脱鞋办公”潮&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 5 日消息，曾经靠海洋球滑梯、免费尼古丁袋等五花八门的福利留住员工的硅谷热门科技初创公司，如今又出新招——要求员工进门脱鞋。根据观察，在年轻人占主导的办公场所，“无鞋办公”政策正悄然兴起。雇主们认为，员工穿着毛绒袜、拖鞋踩在地毯上，能打造出更轻松无压的工作氛围。然而矛盾的是，这些公司中不少仍推行“996”工作制，要求员工从早 9 点工作到晚 9 点，每周连轴转 6 天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;斯坦福大学经济学家、职场文化专家尼克·布鲁姆表示，无鞋办公政策的流行，在一定程度上是“睡衣经济”的延伸——随着远程办公者被要求重返办公室，他们也把居家办公的习惯带到了办公室。但这一趋势也与硅谷高压的工作文化一脉相承。布鲁姆说：“如果你每天要在公司待 12 个小时，那不如直接穿拖鞋上班，毕竟在家也没机会穿。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;中国商务部回应Meta收购Manus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月8日，就Meta收购人工智能平台Manus一事，中国商务部新闻发言人何亚东表示，中国政府一贯支持企业依法依规开展互利共赢的跨国经营与国际技术合作。何亚东在当日举行的例行新闻发布会上回应称，需要说明的是，企业从事对外投资、技术出口、数据出境、跨境并购等活动，须符合中国法律法规，履行法定程序。商务部将会同相关部门对此项收购与出口管制、技术进出口、对外投资等相关法律法规的一致性开展评估调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄仁勋官宣英伟达已投产 Vera Rubin：训练 AI 速度是 Blackwell 架构 3.5 倍&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在北京时间 1 月 6 日凌晨举办的 CES 2026 主题演讲中，英伟达首席执行官黄仁勋发表主题演讲，介绍了新一代“Rubin”计算架构，并将其定义为当前 AI 硬件领域的“最先进技术”，该架构已进入全面量产阶段。Rubin 架构以天文学家薇拉·鲁宾的名字命名，由六款协同工作的独立芯片组成。该系统的核心是 Rubin GPU，同时配备了专为“智能体推理”（Agentic Reasoning）设计的全新 Vera CPU。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在性能表现方面，Rubin 架构相较于前代产品实现了显著跨越。根据英伟达官方测试数据，Rubin 在 AI 模型训练任务上的运行速度是 Blackwell 架构的 3.5 倍；在推理任务中，其速度更是达到了前代的 5 倍，峰值运算能力高达 50 Petaflops。此外，新平台的能效表现同样优异，其每瓦推理算力提升了 8 倍。这一性能飞跃将为日益复杂的 AI 模型提供强大的算力支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，黄仁勋也介绍并推出了全新的 Alpamayo 1，是其视觉-语言-动作模型（VLA），结合因果链推理与轨迹规划，主要增强复杂驾驶场景中的决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元发布开源仿真平台Genie Sim 3.0&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智元机器人在CES国际消费电子展首日正式发布首个大语言模型驱动的开源仿真平台——Genie Sim 3.0。基于NVIDIA Isaac Sim，Genie Sim 3.0融合三维重建与视觉生成，打造数字孪生级的高保真环境；首创大语言模型驱动的场景泛化技术，让万级场景的生成只需几分钟；同步开源包含真实机器人作业场景的上万小时仿真数据集；并构建了覆盖10万+场景的多维度智能评估体系，为模型能力绘制全景画像。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 推出 ChatGPT Health 模式，为“健康 / 医疗”类型对话设立专属空间&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日消息，OpenAI 正式宣布推出 ChatGPT Health，该模式集成于 ChatGPT 中，号称是一个“专门用于与 ChatGPT 进行健康相关对话的独立空间”，预计将在未来几周内陆续向用户开放。OpenAI 称，目前平台每周有超过 2.3 亿人询问有关健康的问题，因此该公司推出了 ChatGPT Health 模式，旨在让用户更系统、更安全地讨论自身的健康问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，在 ChatGPT Health 模式下，系统会将用户的对话与其他普通聊天记录进行隔离，避免用户的健康背景在日常对话中被无意提及。如果用户在普通聊天中开始讨论健康问题，系统也会引导其切换到 Health 模式进行交流。同时，在 Health 模式下，AI 仍然可以参考用户在其他场景中的部分信息。ChatGPT Health 还将支持与个人信息及健康类应用的数据整合，包括 Apple Health（苹果健康）、Function 和 MyFitnessPal 等。OpenAI 强调，Health 模式中的对话内容不会被用于训练模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，ChatGPT 这样的“大模型”本质上是通过预测最可能的回答来生成内容，而不是基于对“真实与否”的判断，因此并不保证生成的医疗见解一定正确，OpenAI 也在其服务条款中明确指出，ChatGPT 仅供参考，不能够用于任何健康状况诊断 / 治疗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;雷鸟 CES 2026 推出全球首款 eSIM 功能 AR 智能眼镜 X3 Pro Project eSIM&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日消息，雷鸟在 CES 2026 中正式推出了全球首款支持 eSIM 功能的 AR 智能眼镜 X3 Pro Project eSIM，但并未公布价格和上市时间。据介绍，该产品采用双目全彩光机，可获得“等效 43 英寸的 3D 空间视觉观感”，同时产品搭载高通骁龙 AR 1 计算平台，内置 RayNeo AR 应用虚拟机，支持微信、抖音、B站等多款应用。此外，该产品搭载 eSIM 通信模块，使得 AR 眼镜首次真正具备脱离手机的能力，产品无需通过手机或 Wi-Fi，即可独立完成包括通话、实时 AI 对话、实时翻译、在线流媒体播放等功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;摩尔线程正式发布开源大模型分布式训练仿真工具SimuMax的1.1版本&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日，据摩尔线程消息，近日，摩尔线程正式发布开源大模型分布式训练仿真工具SimuMax的1.1版本。该版本在完整继承v1.0高精度仿真能力的基础上，实现了从单一工具到一体化全栈工作流平台的重要升级，为大模型训练的仿真与调优提供系统化支持。本次更新聚焦三大核心创新：用户友好的可视化配置界面、智能并行策略搜索，以及融合计算与通信效率建模的System-Config生成流水线。新版本同时提升了对主流训练框架Megatron-LM的兼容性，并增强了对混合并行训练中复杂通信行为的建模精度，使仿真环境更贴近真实生产场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 7 日，微创机器人依托神经元MicroGenius多模态自主手术大模型，成功完成了全球首例“大模型自主手术”动物实验。这一突破性成果不仅填补了全球大模型自主手术在体动物实验的技术空白，更推动全球AI产业在医疗领域的深度升级与跨界融合。1 月 6 日，波士顿动力与谷歌 DeepMind 宣布建立新的人工智能合作伙伴关系，目标将 Gemini Robotics 人工智能基础模型与波士顿动力的新型 Atlas 人形机器人集成。1 月 6 日，高通与谷歌宣布深化长达十年的汽车领域合作，双方将整合骁龙数字底盘解决方案与谷歌汽车软件及云服务能力，加速软件定义汽车落地，推动 AI 赋能的智能出行体验规模化普及。1 月 5 日，腾讯AI工作台ima.copilot迎来更新：正式上线“生成PPT”功能。用户只需进入“任务模式”，即可调用个人知识库中的素材，一键生成幻灯片。1 月 5 日，智元机器人已与MiniMax达成合作，MiniMax将为智元机器人提供文本到语音全流程AI技术支持。针对智元机器人的产品定位与功能特性，MiniMax为其量身打造专属人设体系，优化用户与机器人的语音交互体验。同时，基于人设体系构建定制化提示词策略，为用户生成专属音色，实现千人千面的个性化音色合成，满足多样化语音交互需求。此外，MiniMax还基于自研音乐生成模型，助力智元机器人拓展娱乐场景玩法。&lt;/p&gt;</description><link>https://www.infoq.cn/article/OVgxgVu4Tph8UfgKshIv</link><guid isPermaLink="false">https://www.infoq.cn/article/OVgxgVu4Tph8UfgKshIv</guid><pubDate>Mon, 12 Jan 2026 07:00:49 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>测试人员可以做些什么来确保软件安全</title><description>&lt;p&gt;&amp;nbsp;Sara Martinez在&lt;a href=&quot;https://www.onlinetestconf.com/&quot;&gt;Online TestConf&lt;/a&gt;&quot;上的演讲“&lt;a href=&quot;https://www.youtube.com/watch?v=L9ZWK0xBOoU&quot;&gt;确保软件安全&lt;/a&gt;&quot;”中说到，一个安全的软件开发生命周期意味着将安全融入到计划、设计、构建、测试和维护各个阶段，而不是在最后阶段才匆忙添加。测试人员不是漏洞查找者，而是早期的防御者，从第一个冲刺开始构建安全性和质量。文化第一，自动化第二，全程持续测试和监控；她认为，这就是如何让安全成为一种习惯，而不是紧急演练的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cwe.mitre.org/&quot;&gt;通用弱点枚举（Common Weakness Enumeration， CWE）统计数据&lt;/a&gt;&quot;显示，超过85%的软件弱点来自于我们如何实现代码，大约60%可以追溯到设计决策。Martinez说，这意味着产品的基础、架构和构建方式对产品的安全性有着巨大的影响。她补充说，一旦产品上线，就要密切关注它，运行漏洞扫描，并在问题出现时尽快修补，以领先于攻击者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;安全的软件开发生命周期看起来很像常规的SDLC，但每个步骤都内置了安全性，Martinez解释道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;* 它首先定义明确的安全需求，并在规划和设计时运行威胁建模。* 在开发过程中，遵循安全编码实践，审查依赖关系，并使用安全测试自动化工具或依赖项* 扫描器来尽早捕获弱点。* 测试超越了DAST、渗透测试和其他安全检查的功能，以发现真正的攻击路径。* 一旦产品上线，你就可以通过安全部署、持续监控和快速补丁管理来保证它的安全。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez认为，安全的软件从文化开始，就像质量一样。这不是一个清单，而是关于开发者、测试人员、运维人员和管理人员之间的责任分担：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;每个公司都应该创建适合其产品的行动计划，查看安全软件开发指南，并确保安全实践是日常工作的一部分。自动化是关键；将安全分析工具引入CI/CD管道，以便及早和一致地发现弱点。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez提到不要忘记测试的人为方面：添加与安全需求相关的特定功能测试用例，以便团队保持对诸如弱输入验证、风险角色和权限配置或访问控制等问题的警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martinez说，许多最严重的事件仍然来自旧的、众所周知的攻击，我们可以通过正确的工具和实践来预防这些攻击。现在，我们面临着新的挑战，比如与AI相关的漏洞，它们正在重塑格局：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如，许多公司正在使用AI来生成代码，但他们没有扫描它或应用安全开发实践，因此他们最终将已知的漏洞引入到他们的产品中。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我学到了很多，但我知道我永远也学不完。安全性是一个移动的目标，安全性测试是一个持续的挑战，这正是使它成为一个如此迷人、不断变化的世界的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ就软件安全问题采访了&lt;a href=&quot;https://www.linkedin.com/in/saramartinezginer/&quot;&gt;Sara Martinez&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：测试人员在安全方面扮演什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Sara Martinez：测试员是我们拥有的最好的安全秘密武器之一。我认为我们的角色不仅仅是检查功能是否有效；我们很容易注意到可能变成大漏洞的小问题，比如弱输入验证、有风险的角色和权限配置，或者访问控制。&amp;nbsp;团队需要在安全软件开发生命周期（SSDLC）中共担安全责任，比如挑战安全需求、帮助进行威胁建模，以及运行静态和动态安全自动扫描以尽早发现问题。测试人员可以通过确保快速验证修复并集成到CI/CD中来保持管道中的安全性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：我们有哪些关于漏洞和弱点的数据，我们如何使用这些数据？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Martinez：像CWE （Common Weakness Enumeration）和CVE （Common Vulnerabilities and Exposures）这样的数据标准为我们提供了一种描述软件弱点和现实世界漏洞的共享语言。这些数据不仅仅用于报告；自动化扫描器实际上使用这些引用来检测代码和正在运行的应用程序中的漏洞。&amp;nbsp;我认为这也是发现攻击者趋势的好方法。在过去的几年里，顶级CVE一直被跨站点脚本（XSS）和SQL注入等问题所主导，这些问题继续影响着很大比例的软件产品。使用这些数据可以帮助团队确定测试的优先级，关注安全编码实践，并对攻击者真正利用的东西保持警惕。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/ensure-software-security/&quot;&gt;https://www.infoq.com/news/2026/01/ensure-software-security/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/aWcsjzxXhQcwf3hgWkz2</link><guid isPermaLink="false">https://www.infoq.cn/article/aWcsjzxXhQcwf3hgWkz2</guid><pubDate>Mon, 12 Jan 2026 06:51:00 GMT</pubDate><author>Ben Linders</author><category>安全</category></item><item><title>代理式终端——如何使用CLI智能体激活你的终端</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么命令行越来越具有代理式功能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统上，终端或shell是一种命令式工具，依赖于像 ls 、 grep 和 git 这样的预定义命令来执行特定指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，像&lt;a href=&quot;https://codelabs.developers.google.com/gemini-cli-hands-on&quot;&gt;Gemini CLI&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.claude.com/product/claude-code&quot;&gt;Claude Code&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT&lt;/a&gt;&quot;这样的代理性命令行工具的最新进展已经将这个简单的实用程序转变为一个更动态和智能的助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些代理式CLI工具允许用户用自然语言描述更高级的目标或任务，从而使简陋的shell栩栩如生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它们可以规划步骤，利用各种工具完成不同任务（例如文件处理、代码执行和网络搜索），对输出进行推理，并充当辅助驾驶以帮助完成任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这显著减少了用户的心智负担，并最大限度地减少了多个工具之间的上下文切换。至关重要的是，用户通过批准或指导智能体的过程来保持控制权，确保自动化和用户监督之间的平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在本文中，我们将探讨这些代理式工具的架构，对比不同的规划风格，如ReAct和计划-执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还将检查代理式工作流程的实际生命周期，从意图捕获到执行，并讨论可靠日常使用所需的关键安全护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;端到端代理式终端生命周期：一个提示，三个智能体&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然人工智能在开发中的兴起通常与聊天界面（如ChatGPT）和代理式IDE（如&lt;a href=&quot;https://cursor.com/&quot;&gt;Cursor&lt;/a&gt;&quot;）有关，但代理式CLI占据了一个独特的利基市场。基于IDE的智能体擅长于以丰富的视觉上下文为中心的代码任务，但它们通常局限于编辑器的窗口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CLI满足了开发人员管理基础设施和git工作流的需求：shell。这种无头的、可组合的特性允许它以GUI绑定代理无法做到的方式将工具和系统命令链接起来。然而，请注意，随着像Gemini CLI这样的智能体现在可以与IDE（如VSCode）集成以提供其建议的差异视图，这种区别正在变得模糊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了详细说明代理式终端工具的强大功能，让我们讨论一个运行示例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些标记文件封装了关于如何构建和测试repo的事实，以及文档和脚本的约定。他们基本上是代理的入职文件。例如，Gemini CLI的文件名为“Gemini.md”。Claude Code工具也使用了类似的约定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考虑一个常见场景，开发人员需要用标准文档和自动化脚本启动一个新的存储库。与其手动创建每个文件并编写样板代码，代理式CLI可以从单个高级指令处理整个过程，从而确保一致性并节省宝贵的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;输入提示：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;添加一个CONTRIBUTING.md，一个PULL_REQUEST_TEMPLATE.md，以及一个scripts/smoke-check.sh，运行一个可配置的命令并在失败时退出非零；更新README以记录两者，并打开一个PR。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单1：用户提示代码片段&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了理解这个指令是如何转化为行动的，我们将把代理式的工作流程分解为它的组成阶段。我们从意图捕获开始，其中智能体在项目的特定上下文中定位自己，然后转移到规划风格，对比不同模型架构其推理的方式。后续部分将详细说明执行实际工作的Tool Execution循环和防止自主事故的关键安全防护措施。最后，我们将看看结果如何呈现给用户，说明在不同的品牌名称下，大多数代理式工具共享一个共同的架构DNA。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段1：意图捕获和上下文形成&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了确保LLM的高质量提示，智能体首先收集所有必要的信息，然后进行规划或执行。这种方法包括几个步骤：将任务链接到当前工作目录，管理会话状态，并将每个项目的配置保存在dotfolders（例如， ./.gemin i和 ./.claude ）。这种方法消除了重复使用标志进行重复任务的需要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，指令还隐式地从各种位置获取。以下是CLI智能体除了用户的提示之外，从哪些主要信号源获取的一些：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;特定于文件夹的上下文文件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些是封装了有关你的存储库如何构建和测试以及你的文档和脚本约定的事实的markdown文件。它们本质上充当你智能体的入门文档。例如，Gemini CLI的文件称为 Gemini.md 。Claude Code工具也使用了类似的约定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;# GEMINI.md（摘录）
## 1. 工程哲学
这是一个高性能的SaaS后端。
* **核心原则：** 可读性优于聪明度。显式优于隐式。
* **架构：** 六边形架构（端口和适配器）。
* **安全性：** 零信任安全模型。所有输入必须通过Pydantic进行验证。
## 2. 技术栈和标准
* **语言：** Python 3.11+（需要严格类型）。
* **框架：** FastAPI（异步默认）。
* **数据库：** PostgreSQL（通过SQLAlchemy 2.0异步会话）。
* **测试：** Pytest（覆盖率必须保持&amp;gt;90%）。&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单2：Gemini.md示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;技能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期智能体的一个主要限制是需要将所有指令塞进上下文窗口。Anthropic的Claude Code引入了&lt;a href=&quot;https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills&quot;&gt;Skills&lt;/a&gt;&quot;的概念，它建立在上述markdown文件的想法之上，作为专业知识的模块化包（例如，PDF操作、数据分析和React最佳实践），作为包含 SKILL.md 的文件夹存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种包含使得渐进式披露成为可能：智能体最初只看到可用技能的名称/描述（消耗最少的词元）。然后，只有在用户的任务需要时，它才动态安装或读取完整的 SKILL.md 指令集。这种方法允许将智能体默认成为通才，但在需要时是专家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;代码库信号&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLI可以扫描现有的scripts/、 .github/ ，并拾取像 README.md 这样的文件工件，如果你已经提供了这些文件。基于像Python这样的语言的典型约定，它还可以查看像 pyproject.toml 这样的工件以获得高级概览。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;IDE焦点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一个可选步骤，如果你连接到像VSCode或Cursor这样的代码编辑器，可以用来打开文件和选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段2：规划风格&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;加载上下文后，每个工具开始其控制循环：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini（&lt;a href=&quot;https://arxiv.org/abs/2210.03629&quot;&gt;ReAct&lt;/a&gt;&quot;风格）思考，调用工具，观察并重复，这非常适合发现缺失的文件夹或策略。这种迭代方法允许Gemini适应新信息并动态调整其策略，使其适用于需要灵活解决问题和探索的任务。Claude（&lt;a href=&quot;https://arxiv.org/abs/2305.04091&quot;&gt;计划和执行&lt;/a&gt;&quot;）提出一个你可以批准的清单，然后逐步执行计划，并带有策略钩子。这种方法提供了更高程度的控制和透明度，因为用户可以在执行前审查和修改计划，确保遵守特定的策略或偏好。Auto-GPT发出思考加上每个周期运行器执行的JSON命令。这种结构化输出便于自动化和与其他系统集成，因为JSON格式提供了智能体意图和行动的清晰且机器可读的表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下示例说明了不同智能体如何处理这个规划阶段。Claude提供了一个人类可读的清单供用户审批，而Auto-GPT生成了专为自动执行而设计的JSON输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Claude——计划预览&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;计划:
创建脚本/smoke-check.sh （POSIX sh；从env读取CMD；失败时退出非零）
创建CONTRIBUTING.md（如何在本地进行冒烟检查）
创建.github/PULL_REQUEST_TEMPLATE.md（检查清单包括冒烟检查）
更新README.md，添加scripts/和PR模板说明运行冒烟检查
运行smoke-check;提交;打开PR
批准吗?[是/否]&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单3：Claude计划审查代码示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Auto-GPT——带有思考和命令的显式JSON&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;｛
“thoughts”： {&quot;text“: ”创建冒烟检查，文档，模板；更新README;运行脚本;提交/PR”},
&quot;command&quot;:{&quot;name&quot;:&quot;write_file&quot;,&quot;args&quot;:{&quot;path&quot;:&quot;scripts/smoke-check.sh&quot;,&quot;content&quot;:&quot;#!/bin/sh\n: \&quot;${CMD:=echo ok}\&quot; \n$CMD || { echo \&quot;smoke failed\&quot; &amp;gt;&amp;amp;2; exit 1; }\necho \&quot;ok\&quot;&quot;}}
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单4：Auto-GPT审查代码示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段3：工具调用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此阶段，智能体使用其库中的工具根据其任务提出更改建议。例如，这可能涉及使用文件编辑工具在IDE中显示差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具已经从专有实现发展为开放标准：模型上下文协议（ &lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol，MCP&lt;/a&gt;&quot;）。在Anthropic、谷歌和其他组织的支持下，MCP就像AI应用程序的USB-C端口。而不是硬编码集成每个数据库或API，（例如，用于PostgreSQL、Slack或GitHub的服务）。CLI智能体在启动时自动发现这些资源，允许单个智能体在一个无缝的工作流程中查询你的生产数据库，阅读你的线性票证，并编辑代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;示例冒烟脚本的Diff&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;*** scripts/smoke-check.sh 
+#!/bin/sh 
+set -eu 
+# CMD可以被覆盖：CMD=&quot;make test&quot; ./scripts/smoke-check.sh 
+: &quot;${CMD:=printf ok}&quot; 
+$CMD &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || { echo &quot;smoke failed&quot; &amp;gt;&amp;amp;2; exit 1; } 
+echo &quot;ok&quot;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;清单5：冒烟脚本diff示例&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude的钩子是一种明确策略的干净方式——限制写入路径、自动chmod脚本、在写入后运行lint/tests——而不需要将其塞入提示中。Gemini通过&lt;a href=&quot;https://geminicli.com/extensions/&quot;&gt;扩展&lt;/a&gt;&quot;和MCP获得类似的杠杆作用：不同的旋钮，类似的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段4：人为干预的安全和护栏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你保留了对冒险行为的控制。Gemini在执行写入或具有副作用的shell命令之前需要你的批准。Claude提供了确认和钩子，允许你阻止违反策略的写操作，或者在继续之前自动运行检查。Auto-GPT暂停是/否确认，除非启用连续模式。为了进行探索，激活一个&lt;a href=&quot;https://geminicli.com/docs/cli/sandbox/&quot;&gt;容器化的沙箱&lt;/a&gt;&quot;来隔离文件系统和进程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段5：执行和迭代：真正完成工作的循环&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建文件后，智能体执行脚本并根据结果进行调整。例如，如果缺少scripts目录，Gemini将创建它并再次尝试操作。如果脚本缺乏可执行权限，Claude的集成钩子会自动应用 chmod +x 命令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;脚本在观察、推理和操作的连续循环中执行。这个循环不断重复，直到本地执行成功并完成文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;阶段6：渲染结果和停止条件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLI提供了一个清晰的、语法高亮显示的工具调用和文件差异视图。用户可以在编辑器中打开这些差异，手动进行调整，或者指示智能体进行适当的更改。批量批准是最有效的，例如在单个批准之前一起审查所有脚本和文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一次成功的冒烟检查之后，通过批准的差异，智能体将创建一个新的分支，提交更改，并打开一个PR草案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;如何在你的工作流程中利用代理式CLI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是一些实用技巧，帮助你在工作流程中充分利用这些工具：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将上下文文件视为构建资产&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将GEMINI.md和CLAUDE.md文件与你的README文件一起&lt;a href=&quot;https://google-gemini.github.io/gemini-cli/docs/cli/gemini-md.html&quot;&gt;维护&lt;/a&gt;&quot;。这些文件应该简洁且专注于特定细节，包括构建和测试程序、配置位置、任何特定于存储库的问题以及安全编辑的目录。你甚至可以使用智能体生成初始草稿。将这些文件视为为代理式编程环境的方式，而不是另一个需要持续监督的提示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;积极地限定范围&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将智能体指向实际重要的文件夹（例如services/payments/，而不是整个单体仓库）并传递明确的@file提示以指向热点。更紧凑的范围意味着更紧凑的差异，更少的创造性幻觉和更快的迭代。如果任务确实跨越多个包，请在提示中列举它们，以防止智能体进行详尽的扫描。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用沙箱避免对环境的意外更改&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini CLI提供了一个&lt;a href=&quot;https://geminicli.com/docs/cli/sandbox/&quot;&gt;沙箱模式&lt;/a&gt;&quot;，用于shell/file工具的临时、容器化执行。这保护了你的主机系统，限制对挂载的工作目录的写入，并控制网络访问。它非常适合无风险的探索，但不会消除对破坏性命令的审批提示，不能编辑已安装的秘密，也不能防止模型建议有风险的操作。你仍然是最终的仲裁者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Code通常在容器化开发环境（&lt;a href=&quot;https://code.claude.com/docs/en/devcontainer&quot;&gt;Dev Container/Docker&lt;/a&gt;&quot;）中运行，或使用插件/钩子将shell/file操作通过容器化运行器路由。这提供了类似的隔离（写入限制在挂载路径，控制环境，确定性工具链）。然而，这种隔离并不具有回溯性；如果允许，它不会阻止对挂载的秘密或暴露路径的意外写入。使用钩子来强制执行路径限制，并在写入最终确定之前自动运行测试/lint。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Auto-GPT没有专门的沙箱模式标志，但强烈建议在Docker容器中运行它。这确保了其文件系统操作与你的主机操作系统隔离，防止对你主要环境的任何意外更改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用符合你需求的工具&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemini CLI非常适合深入集成到谷歌生态系统中的用户。它作为一个通用工具，擅长于发现繁重的任务，包括代码编辑、文档更新、小的shell操作（如列出目录和移动文件）、快速网络研究以及探索性的解决问题。它的ReAct循环促进了自然的探索和迭代工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Code最适合需要具体计划和强大编码能力的任务。这包括多文件重构、通过钩子执行策略、Git原生工作流程（分支、变基、冲突解决）和透明的护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/features/copilot/cli&quot;&gt;GitHub Copilot CLI&lt;/a&gt;&quot;旨在为快速、存储库感知的自然语言到shell辅助提供支持。它非常适合生成一次性命令、搭建测试、搜索代码以及起草提交和拉取请求，所有这些都不会破坏现有的GitHub工作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他工具包括&lt;a href=&quot;https://github.com/Aider-AI/aider&quot;&gt;Aider&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/openinterpreter/open-interpreter&quot;&gt;Open Interpreter&lt;/a&gt;&quot;和本地优先CLI。当你需要对实现有更大的控制权，并且有高度特定的需求，如紧密的Git人机工程学、本地LLM或不受限制的shell环境时，可以考虑这些选项。这些工具对于喜欢较少护栏和更快修改工具本身的经验丰富的用户来说非常有用，特别是对于较小的存储库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像工程师一样提示，不要写论文&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好的提示主要是关于清晰的合同，而不是华丽的散文。使用包含以下详细信息的四部分提示。从一个高层次的目标开始，用一句话陈述你的意图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;定义约束，包括范围（例如，“仅services/billing”）、风格（“POSIX sh; no bashisms”）和安全协议（“写入前询问”）。确定所需的工件，指定预期的结果（例如，文件、测试、README/PR文本）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确定检查，概述将如何衡量成功（例如，测试命令、验收标准）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像任何其他自动化一样进行仪表化&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了优化智能体性能，监控关键指标，如PR周期时间、智能体生成的差异大小、需要返工的PR百分比以及智能体编辑后不稳定测试的频率等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些指标作为反馈，不仅用于智能体的整体有效性，还用于你自己的运维合同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;接下来是什么&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代理式CLI正在从简单的shell助手演变为将你的工作工具、操作系统和云基础设施统一起来的连接组织。以下是截至本文撰写时的一些新兴趋势：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IDE和操作系统正在融合成统一的Agent Surfaces。像&lt;a href=&quot;https://windsurf.com/&quot;&gt;Windsurf&lt;/a&gt;&quot;和Cursor这样的工具允许智能体在终端、编辑器和运行过程中共享上下文，而不是作为孤立的聊天窗口运行。Windows也在其体验中&lt;a href=&quot;https://techcommunity.microsoft.com/blog/windows-itpro-blog/evolving-windows-new-copilot-and-ai-experiences-at-ignite-2025/4469466&quot;&gt;注入了大语言模型驱动的交互&lt;/a&gt;&quot;。智能体正在从响应式CLI转变为持久的后台服务。这些守护进程智能体不是等待输入，而是主动监控日志文件和本地服务器，仅在出现错误时介入修复计划。虽然这些工具（如GitHub PR上的Copilot）仍处于起步阶段，它们的洞察力尚浅，但随着正确的集成，它们将不断改进。扩展正在成为代理能力的App Store。随着CLI智能体的&lt;a href=&quot;https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills&quot;&gt;技能&lt;/a&gt;&quot;和&lt;a href=&quot;https://geminicli.com/extensions/&quot;&gt;扩展&lt;/a&gt;&quot;等创新，我们正在开发新一代的App Store，让用户可以将适当的能力插入到他们的智能体中。这也模糊了通用智能体和专业智能体之间的区别，因为专业智能体只是一个拥有正确知识和工具的强大通用智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/agentic-terminal-cli-agents/&quot;&gt;https://www.infoq.com/articles/agentic-terminal-cli-agents/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t712SX2KtxLUAF9rZZ6y</link><guid isPermaLink="false">https://www.infoq.cn/article/t712SX2KtxLUAF9rZZ6y</guid><pubDate>Mon, 12 Jan 2026 06:09:35 GMT</pubDate><author>作者：Sachin Joglekar</author><category>框架</category><category>AI&amp;大模型</category></item><item><title>“机器人一次性卖完太亏！”真机智能刘智勇：今年中国本体厂商将大淘汰，拼的是世界模型</title><description>&lt;p&gt;作者 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们采访了真机智能董事长兼首席科学家刘智勇，听他讲述了视觉语言导航（VLN）技术的当前难题、具身智能领域在 2025 年的各类进展以及今年在能力边界上的两个突破方向和技术决胜点。他表示，一旦世界模型的因果推理能力取得突破，无论是机器人的安全性还是行为和推理的安全性问题，都能得到很好的解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026年本体厂商肯定会收缩，估计中国最终只会剩下5到8家本体机器人公司。”他指出，核心是在某个单一场景实现盈利，不是毛利而是不依赖大量售后成本的净利。但单纯的整机销售并非很好的商业模式，如果只卖硬件，后续的售后压力会非常大，用户一次性付太多钱也承受不了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是详细对话内容，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;VLN和世界模型上“大分”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：2025年具身智能领域有哪些突破性进展让您印象深刻，包括技术、产业化和生态建设上？这些进展是否已经为具身智能从实验室走向特定场景的“初步普及”奠定了基础？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：我印象比较深刻的是 VLN 方向的相关进展。过去我们主要是以 SLAM 为核心的技术路线，但从去年到现在，涌现出了大量基于视觉语言作为多模态输入的导航模型。这种视觉语言模型能解决零样本泛化的问题，我们不再需要预先构建地图了。把一个机器人放到任何全新的固定场景里，它都能实现零样本泛化，自主完成导航任务。另外，像UniNavid、ETPNav、FSR - VLN这些代表性工作，也让机器人门到门配送的实现出现了曙光和可能性。这就是从几何测量的导航范式，转变到学习增强的导航范式。当前的瓶颈在于未达极高的导航成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从场景普及的角度来说，核心是我们不再需要预先建图了。这就意味着，把机器人放在任何新的位置上，它都能立刻开始工作，直接解锁了很多之前无法覆盖的场景。最关键的一点是，零样本能力等同于部署成本的大幅降低。部署成本降下来之后，整个成本结构就能适配场景化的盈利模式，这正是为场景普及奠定的核心基础。技术成熟后，前期的准备和部署工作会大幅减少，这也为未来的产业发展打下了很好的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：具身智能的核心技术栈正在如何演变？2025年这一年有哪些值得关注的新范式或共识？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：从算法角度来看，核心变化是从之前感知、决策、执行分离的多模块化范式，逐渐转向 VLN 或 VLA 的端到端统一范式。从数据角度来说，发展方向是从单纯的真实数据采集，逐步转向合成数据、离线轨迹挖掘以及世界模型这些领域。训练范式也发生了改变，从强化学习调参慢慢转向世界模型驱动。现在世界模型算是行业内解决数据问题的一个共识，原因很简单，不管是在长程层面模拟预测未来状态、在底层层面预测动态物体轨迹，还是弥补数据的 corner case，世界模型都起到了不可或缺的作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：世界模型被寄予厚望，被认为是实现高级推理和规划的关键。现阶段来看，它对机器人实际能力的提升体现在何处？之后还有哪些方面的潜力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：现阶段来看，主要体现在三个方面。第一，机器人执行长程任务时容易陷入短视困境，而世界模型可以模拟未来的长程状态，对全局规划能力有非常重要的提升；第二，动态环境下静态地图容易失效，无法准确指引路径轨迹，世界模型能够预测动态物体的轨迹，让机器人的本地行动更安全；第三，世界模型能较好地生成相关数据，减少数据泛化鸿沟。我们认为，世界模型是 VLN 突破长程规划和动态适应瓶颈的充分非必要条件。但现在世界模型的主要问题是黑盒，而非白盒可微。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：大模型的快速发展，为具身智能的“智能”部分带来了哪些质变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：从我们的实践来看，最核心的变化是导航和路径规划的技术范式发生了转变。过去我们采用的是 SLAM 方案，现在则转向了VLN范式。过去的 SLAM 方案存在几个明显的局限，一是方案本身不具备语义理解能力，二是依赖静态地图，必须预先建图才能使用，三是需要对特定的传感器做专门标定。而 VLN 范式完全不同，它可以结合语言和视觉实现语义层面的理解，同时能应对非静态环境，实现动态适配。更关键的是，这个方案不再依赖高规格的激光雷达，也不需要预先部署地图，成本和效率都实现了大幅优化。大模型的快速发展，推动技术范式从几何测量的 SLAM 转向学习增强的 VLN，这正是带来质变的核心原因。行动、观测和语言本来属于三个空间，现在要把三个空间统一起来，这也是目前的核心难点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;大规模落地现在卡在哪儿？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：几乎所有专家都指出，高质量、大规模的物理交互数据稀缺是当前最大瓶颈。面对真实数据采集成本高昂的困境，仿真合成数据、人类视频数据等替代方案能走多远？“数据工厂”是可行的解决方案吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：我们面临的主要数据瓶颈有两个，一是数据的场景覆盖不足，比如现在常用的数据集大多基于 Mate Port 3D、Habitat、AI2THOR等 构建，只包含 固定的训练环境，场景覆盖肯定不够；二是做 VLN 的数据采集成本很高，有时需要 3D 数据采集，标注成本也比 2D 图像高出一个量级。对 VLN 来说，现在数据是完全不足的，既存在场景覆盖问题，又有成本高昂的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，我们在采用多种数据解决方案。第一是采集真实数据，采集 RGBD 视频流，以及数字手套等，再结合人工标注指令，像 Atomic 和一些基准数据集的主要来源就是真机数据。第二是比较常见的用仿真器生成，比如借助模拟器搭载 3D 场景库，批量生成视觉语言轨迹三元组。第三是采用 新范式，不用额外改动 3D 环境，通过改写人类标注数据的方式生成新样本，这是一种静态片段生成的新范式。另外，未来还有一种发展方向是离线数据、离线轨迹挖掘的方式，有点类似实行微克隆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：当前的硬件如灵巧手、关节驱动、传感器等，在哪些方面最能满足机器人的技术需求？又在哪些方面构成了发展的主要制约？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：要讲满足技术需求的地方，我们可以和轮式机器人做个比较。之前的轮式机器人只能移动到楼下，没办法开单元门、摁电梯，只能在楼下送货或者在室内移动。而现在的灵巧手、一体化关节，再加上一些触觉传感器，能让机器人具备开门、按电梯的能力，这是轮式机器人到人形机器人的一个巨大转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过目前硬件也存在几方面的制约。第一，我们还需要高分辨率的柔性触觉皮肤。因为机器人需要用机械灵巧手摁电梯，如果触觉不够灵敏，盲按的波动率大，成功率就会比较低。第二，门把手的种类太多了，如果机器人没有触觉反馈，根本没办法应对成千上万种门的情况，也很难实现场景泛化。再就是机器人要进行成千上万次的反复操作，电机、执行器、丝杠这些部件的脆弱性，可能在我们的应用场景中被放大 100 倍。所以从硬件角度来讲，目前主要的制约就是开门要做得好、触觉要做得好这两点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：目前为止，制约具身智能大规模落地应用难题还有哪些？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：对于我们的 VLN 技术来说，主要有两方面的难题。第一是感知决策的延迟问题，这甚至可能是致命的。简单来说，长程规划和行动频率的匹配很关键，如果感知和决策环节出现延迟，机器人在开放环境中运作就会遇到很多麻烦，这就要求必须在端侧做好部署。第二是硬件性能短板，既要让硬件能灵敏地感知外部世界，又要保证它能反复进行操作，而目前这类硬件的耐疲劳性、反脆弱性能还不够强。对于世界模型来说，核心瓶颈是隐式神经表征，而非显式3D高斯，可能在开门和按键上缺少精准几何信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;具身智能该告别 “一锤子买卖”？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：面对这样的机遇与挑战，您们在接下来一年的战略重点和核心发力方向是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：真机智能其实分成了北京真机和苏州真机两个公司。北京真机关注的还是比较传统的 SLAM 加轮式机器人的技术栈和方案，苏州真机则聚焦于 VLN 加人形机器人的技术栈及方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;苏州真机接下来有两个关注重点，第一是通过视觉语言导航的方式，实现无需额外提前部署的门到门配送。过去部署成本太高了，大概占了整个机器人售价成本的 38% 左右。我们希望能实现零样本泛化，换句话说，就是让机器人能够直接理解环境，直接完成导航任务。第二是全身运动控制，要解决的核心问题是开门。之前的控制是基于机器人静态的假设来实现的，哪怕是协作机器人也是保持自身不动去拉开门，这种方式需要的扭矩非常大。我们希望通过全身控制打破静态平衡的限制，依靠动态平衡的方法更泛化地解决开门的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这两个点结合起来，我们既能实现无需预先建图的门到门配送任务，同时又能解决开门和按电梯的任务。这两个方案结合之后，就可以实现最后五公里的门到门配送，既能开门、操作电梯，又能以无建图、无 GPS 的方式完成导航。室内本身没有 GPS 信号，但又需要实现导航，这时候视觉和语言理解的作用就非常关键了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：除了直接销售机器人整机，具身智能未来的商业模式可能有哪些创新？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：整机销售和租赁这两种方式都会存在。但我个人觉得，单纯的整机销售并不是很好的商业模式，更好的方式是 “整机销售 + 每年服务费” 的组合模式。如果只卖硬件，一次性卖完其实很亏，后续的售后压力会非常大。“整机销售 + 每年服务费” 就比较合理，既能保证长期的最大收益，又能解决售后问题，还能让设备商一次性回本。通过这种组合模式，能把原本不赚钱的 “卖铁生意”，变成能持续盈利的长期现金流生意。另一方面，用户一次性付太多钱确实承受不了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，未来还可能出现按单收费的商业模式。比如人形配送机器人测算下来每单成本能控制在两到三元人民币，和达达这类上游公司合作，机器人完成一单就赚一笔费用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;本体厂商大收缩，要拼什么？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：到2026 年，我们有望看到具身智能在能力边界上实现怎样的突破？整个具身智能领域的技术决胜点可能会是什么方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：2026年可能会有两个关键突破方向。第一是机器人在非结构化场景中实现稳定作业。要做到这一点，需要机器人具备一定的社交行为表现和自主导航能力。解决了之后，一些之前没想到的非结构化环境下的任务机器人也可能完成了。目前行业内大多还聚焦在结构化环境，所以这会是一个重要突破。第二是突破莫拉维克悖论（Moravec&#39;s Paradox）。以往大家觉得，机器能完成人类觉得难的事，但难以完成人类觉得简单的事，而2026年可能机器人也能胜任这类任务，会在人类觉得简单的事情上取得突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于技术决胜点，我认为有几个关键因素，其中最重要的是世界模型的因果推理能力。一旦这项能力取得突破，无论是机器人的安全性还是行为和推理的安全性问题，都能得到很好的解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Q：2026年，全球具身智能公司的竞争情况将如何变化？中国公司与国际巨头各自的优势和赛点分别会在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘智勇：2026年本体厂商肯定会收缩，马太效应会非常明显，估计中国最终只会剩下5到8家本体机器人公司。不过应用场景相关的公司和上游企业会多一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国和国际企业的优势不一样，国际公司的大模型技术更先进，基础模型能力更强，国内企业还处在追赶状态，但中国企业拥有供应链成本优势。另外竞争维度也在升级，现在大家可能还在追求单点技术的先进性，到了2026年，整体系统的效率会变得更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于赛点，我觉得核心是在某个单一场景实现盈利，不是毛利而是不依赖大量售后成本的净利。谁能做到这一点，谁就能形成数据飞轮，有了数据之后，模型和方法能力会进一步提升，之后再推进跨场景复制。&lt;/p&gt;</description><link>https://www.infoq.cn/article/EbQ24746fmKpCj7qJmIZ</link><guid isPermaLink="false">https://www.infoq.cn/article/EbQ24746fmKpCj7qJmIZ</guid><pubDate>Mon, 12 Jan 2026 01:55:28 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>发生在CES的六场对话：来自深圳的 AI 硬件“外卷”</title><description>&lt;p&gt;2026年CES开幕当天，拉斯维加斯的Uber司机望着展馆外的人流脱口而出：“中国人太多了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;是的，中国硬件力量正以空前力量和密度涌向全球舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在机器人展区，星动纪元技术VP侯伟的感受更为具体：“国内一线的厂商都来了。而且双足机器人这个赛道，非中国厂商展现方式都非常保守，没有炫酷的动作，甚至做&amp;nbsp;live&amp;nbsp;demo&amp;nbsp;的也很少。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中国已经成为全球硬件供应链的重要基地，新兴硬件品牌也基于中国的供应链优势不断涌现。他们的目标是，到全球去，以及用AI重做硬件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但热情中也弥漫着一丝紧张。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI陪伴硬件创业公司Ludens&amp;nbsp;AI创始人薛立君回忆，一个美国白人记者反复质问展区内所有亚洲面孔：“你们的供应链是不是在深圳？你们团队内有没有中国工程师？我们的数据被储存在哪里？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种焦虑在数据上也有印证：2026年以中国注册地址参展的企业为935家，虽仍占总数近20%，却较2025年的1300余家大幅下滑。有品牌高管向InfoQ坦言，签证障碍是缺席的主要原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，这些阻力并未浇灭全球市场对中国AI硬件的热情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;乐享科技旗下新品牌“元点智能”刚在CES做完发布会，就已经收获欧美本地连锁渠道采购商和其他代理商的合作意向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI&amp;nbsp;仅带两个原型机参展，在产品还未上众筹平台的情况下，2000张名片就快要发完了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI厨电品牌万得厨（wan&amp;nbsp;AIChef）首次参展CES，品牌管理总监林燕妮向InfoQ表示，团队原本以调研北美市场为主要目的，却意外收到大量本地代理商和售后服务商的接洽，并吸引了包括Best&amp;nbsp;Buy和Home&amp;nbsp;Depot在内的多家大型零售商的合作意向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些信号共同指向一个事实：尽管地缘政治带来摩擦，市场仍然被产品本身的价值与体验打动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这个背景下，AI硬件在2026年CES呈现出新的趋势：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一些曾被认为是前景不明的“小玩意”，正被大厂重新定义为Physical&amp;nbsp;AI生态的关键入口。AI硬件的初创公司，则更加细致地从数据、场景、人群等方面，思考自己的生存之道。机器人如何融入物理世界这一点更加清晰了，家用具身智能已经有了明确的商业目标。除了人形机器人厂商，卡特彼勒、西门子这样的工业巨头也来了，迫切地想告诉所有人：我们不仅跟上了AI的发展，还在支撑着&amp;nbsp;AI所需要的物理基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94b0be2f8152f03c62f28fca72484311.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;大厂盯上小玩意，AI硬件更卷了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;曾经被视为边缘、小众甚至“玩具级”的硬件品类，正被科技巨头重新定义为下一代AI生态的战略入口。智能戒指、宠物机器人、健康手环，如今成了连接用户、数据与多场景服务的关键工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年年初，追觅正式成立AI硬件事业部，并将智能戒指作为首个突破口。近日，团队在CES展出了三款不同功能的智能戒指、一款血压手表。震动AI智能戒指侧重主动交互，生态互联AI戒指具备NFC的功能，健康戒指、血压手表则会更加专注于健康监测。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些产品指向同一目标：让硬件从家庭场景，延伸至用户24小时的生活。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在追觅生态内，戒指的定位是开启追觅生态链的钥匙，”追觅AI硬件事业部负责人潘志东向InfoQ解释，“智能戒指是可以被用户24小时无感佩戴的产品，可以获得用户毫秒级的数据反馈，在未来十年会比其他品类的穿戴设备被更多人使用。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d1/d16eb7330bc01937533792917a803b84.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在潘志东看来，智能家居与AI硬件的本质都是智能互联，区别在于覆盖半径：前者聚焦居住空间的舒适与便捷，后者则贯穿户外、办公、睡眠、健康等多元场景。未来，戒指期待与追觅生态其他产品深度联动。例如用户回家前轻触戒指，扫地机即提前启动，到家时，灯光、音乐可根据戒指监测到的用户状态自动调节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术层面，追觅AI智能戒指依托品牌在高速马达领域的深厚积累，将微型马达集成进戒指结构，需解决空间限制、震动对结构稳定性的影响以及功耗控制等问题。目前，震动功能已支持心率异常提醒、消息通知、拍照等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，追觅的野心还在向办公等场景延伸。潘志东表示，团队正在开发一款记录戒指，可以记录会议和日常生活高光时刻。“可以把它想象为一个空间戒指，可以随时储存你的灵感。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，市场已经给出积极反馈：2025年10月追觅智能戒指开始在国内低调预售。2025年12月，追觅戒指在天猫智能指环热销榜登顶，超越RingConn、华米等品牌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/bafa3d707d0b7970cfde52b0b5ca72ba.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图片为天猫官网12.29截图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说追觅试图通过贴身设备延伸对“人”的覆盖，那么涂鸦智能则在构建一个跨场景的&amp;nbsp;AI&amp;nbsp;硬件协同网络。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在本届&amp;nbsp;CES&amp;nbsp;上，涂鸦智能发布了其&amp;nbsp;AI&amp;nbsp;宠物陪伴机器人&amp;nbsp;Aura，并同步推出&amp;nbsp;AI&amp;nbsp;生活助手&amp;nbsp;HeyTuya。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，涂鸦此前鲜少推出硬件产品，此次推出Aura是涂鸦在AI硬件消费上的一大尝试&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;涂鸦智能&amp;nbsp;CMO&amp;nbsp;那竞丹在社交媒体中表示，Aura&amp;nbsp;并不是终点，而是家庭陪伴机器人迈向长期化、通用化的起点。“宠物陪伴只是入口，系统会自然延展到喂养管理、行为分析和健康监测，背后是一套清晰的三支柱体系。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/15/153b320d3e40490448257bfb9ed1cf54.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;用户在现场体验涂鸦智能&amp;nbsp;AI&amp;nbsp;宠物陪伴机器人&amp;nbsp;Aura&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比具体产品，更值得关注的是&amp;nbsp;HeyTuya&amp;nbsp;所指向的方向。在&amp;nbsp;AIoT&amp;nbsp;的上一个周期里，涂鸦更多扮演的是幕后角色：通过模组和云服务，帮助厂商快速搭建智能硬件。而现在，它试图更进一步，不再只赋能单一设备，而是作为智能家居的统一入口，让不同硬件具备&amp;nbsp;Agent&amp;nbsp;能力，并在场景中协同工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们不只是帮厂家做出智能空调或冰箱，”涂鸦智能产品总监虞翔对&amp;nbsp;InfoQ&amp;nbsp;表示，“HeyTuya&amp;nbsp;是一个完整的‘AI&amp;nbsp;生活场景’&amp;nbsp;demo。它展示了如何把安防、健康、作息这些原本割裂的功能，整合成一个主动为用户服务的整体体验。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管“用&amp;nbsp;AI&amp;nbsp;把硬件重做一遍”已被反复讨论，但行业对&amp;nbsp;Agent&amp;nbsp;硬件的形态和边界，仍未形成共识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在虞翔看来，短期内最有机会跑通的，是两类&amp;nbsp;Agent&amp;nbsp;硬件：一类是情感陪伴型产品，如陪伴机器人或高级玩具，它们直接回应人的情绪需求，市场已经开始用真实使用行为给出反馈；另一类是垂直领域的“专家助理”，例如家庭节能管家，能在具体场景中提供可量化、可感知的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让这些产品真正成立，产业仍需补齐两项基础能力：一是作为“神经系统”的稳定连接与长期记忆，让&amp;nbsp;Agent&amp;nbsp;能实时联通设备并理解用户习惯；二是作为“骨骼系统”的生态整合能力，单个&amp;nbsp;Agent&amp;nbsp;的价值应该建立在开放、可互操作的设备网络之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI硬件初创公司的生存之道：数据、场景、交互&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当大厂凭借生态优势将智能戒指、陪伴机器人等“小玩意”纳入Physical&amp;nbsp;AI&amp;nbsp;版图，AI硬件初创公司不得不在更狭窄的缝隙中寻找立足点。它们必须围绕数据、场景和交互构筑护城河。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来智能&amp;nbsp;COO&amp;nbsp;王超认为，在算力、算法和数据这三个要素中，真正长期构成瓶颈的，并不是前两者，而是高质量、强场景约束的私有数据——尤其是那些无法通过互联网获取、只能依赖物理设备在真实世界中持续采集的信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“AI&amp;nbsp;耳机是物理世界的耳朵，”王超形容，“它把声音转化为结构化文本，而这些数据，是大模型公司过去很难直接触达的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王超认为，AI&amp;nbsp;硬件初创公司的关键不在于“把&amp;nbsp;AI&amp;nbsp;做进硬件”，而在于所选择的方向——是否必须依赖某一种特定硬件形态，才能完成特定场景的数据采集、处理与理解，并且这一过程难以被大厂的通用软件方案替代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以办公场景为例，未来智能通过&amp;nbsp;AI&amp;nbsp;耳机切入会议、沟通与决策环境，所积累的数据本身是脱敏的，但一旦被结构化，就可以持续服务于企业内部的协作效率与管理流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/a3/3b/a39ea41903715268153ffe218f8c063b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Youtube科技博主&quot;Mark&amp;nbsp;Ellis&amp;nbsp;Reviews&quot;&amp;nbsp;在CES现场体验拍摄未来智能产品&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;CES&amp;nbsp;现场，王超还感受到&amp;nbsp;Physical&amp;nbsp;AI&amp;nbsp;正在从概念走向工程现实的强烈信号。这一趋势背后，有两个变化尤为明显，一是底层技术与供应链正在重构，包括端侧算力方案演进与存储成本波动；二是硬件必须能以更自然的交互方式，自然融入既有生活环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，芯片算力的提升为算法优化打开了空间。例如降噪能根据环境变化自动动态调整，甚至耳机可以扫描用户耳道结构，实现降噪方案的个性化定制。与此同时，高存储需求正受到供应链压力：存储芯片短缺与价格上涨，已对AI耳机行业部分高配置耳机产品造成直接影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在交互方式上，未来智能将聚焦语音交互。其后续产品研发也计划引入&amp;nbsp;Agent&amp;nbsp;式的主动服务能力。例如，当一位高管在会议中专注讨论时，AI&amp;nbsp;耳机可智能判断不断涌入的邮件与审批请求是否“紧急且需立即处理”，并通过耳机及时提醒；用户仅需简单手势即可完成反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但要实现这类体验，耳机的独立联网与端侧运算能力变得重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王超指出，目前市面上所有的“AI&amp;nbsp;耳机”实际上仍需依赖手机&amp;nbsp;App&amp;nbsp;进行语音转写，一旦应用退至后台或受到系统权限限制，就可能导致功能中断。相比之下，若任务能在设备端完成，再与云端协同，体验将显著更稳定。未来智能也在尝试端侧AI的探索，当前虽然还无法实现真正的独立，但已经在通过一些方案规避移动端的不确定性，比如离线闪录，通过耳机或充电仓直连云端&amp;nbsp;AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一步，未来智能计划通过多端协同，围绕办公场景提供更加连续的服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;私有数据、极致场景与交互体验，这三个关键词，同样出现在&amp;nbsp;AI&amp;nbsp;陪伴硬件初创公司&amp;nbsp;Ludens&amp;nbsp;AI&amp;nbsp;的关键思考中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI创始人薛立君介绍，家庭场景的数据高度敏感，因此，在产品研发中，团队注重在真实环境中的测试，积累关于人机共处节奏、情绪反馈模式和空间行为习惯的原始数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正因如此，Ludens&amp;nbsp;AI&amp;nbsp;采用纯端侧AI模型。一方面是因为家庭场景对隐私泄露的天然警惕；另一方面，则是为了实现自然的交互感。Ludens&amp;nbsp;AI&amp;nbsp;旗下机器人可以达到50–100ms&amp;nbsp;级别的实时响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计背后，是对场景的细致拆解。创始人薛立君认为，虽然AI陪伴硬件和AI、机器人强相关，长期来看，它的本质是强文化属性赛道。例如，一个面向日本独居老人的机器人，其互动节奏、声音语调与唤醒逻辑，必然不同于面向欧美儿童的版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbef499256780ada300a47215347e7c6.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Ludens&amp;nbsp;AI展台，观众体验&amp;nbsp;Inu和Cocomo原型机&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在交互体验上，Ludens&amp;nbsp;AI强调将“玩”作为建立情感连接的核心机制。因此，机器人在理解环境、人物状态的情况下，能主动做出交互反应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026&amp;nbsp;年，Ludens&amp;nbsp;AI&amp;nbsp;计划通过两次众筹测试市场需求。薛立君认为，对于陪伴类产品而言，持续交互的频次和时长，是判断&amp;nbsp;PMF&amp;nbsp;的关键指标。出于伦理考虑，公司不会对对话和记忆能力设置订阅门槛，以避免“停止付费即失去陪伴”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;机器人，从秀场到工厂和家庭&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;走进拉斯维加斯的机器人展馆，最直观的感受是：这里几乎被中国公司“占领”了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从参展格局看，中、美、韩三国构成了具身智能的核心力量。韩国甚至以国家代表团形式集体亮相。星动纪元技术&amp;nbsp;VP&amp;nbsp;侯伟评价，尽管韩国本土机器人企业数量不多，但在灵巧手、轮式底盘和运动控制等细分领域积累深厚；美国的优势依然集中在原生技术与基础研发；而中国最突出的竞争力，则体现在硬件制造能力，尤其是机器人本体制造上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种差异，在展台呈现中表现得尤为明显。侯伟提到，今年&amp;nbsp;CES&amp;nbsp;上，非中国厂商的展示明显更为保守，大多停留在慢速行走或简单动作，很少进行长时间、连续的实时演示。当被问及原因时，有些团队给出的回答显得颇为含糊：“现在的机器人就是这个状态，但还有很多可能性……只是没带过来。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/5502941583f36a7d94db0eaa9f6a3af2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;星动L7在CES&amp;nbsp;2026现场为嘉宾拍摄Vlog&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但比“谁展示了什么”更重要的，是谁开始认真询问、试图把机器人放进真实世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;侯伟形容，这次星动纪元的展台“销售同学忙得连喝水都没空”。前来咨询的不只是高校、科研机构和科技大厂，还包括软件与设计服务商、硬件电机供应商、智能家居企业，甚至传统制造企业。一家德国食品工厂的提问，让他印象尤为深刻。对方关心的是，人形机器人能不能用于啤酒灌装。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对一个高度标准化、长期保守的行业来说，哪怕只是开始评估人形机器人的可能性，对于具身智能行业来说也是一个积极的信号。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，这次CES中，当工业领域的具身智能仍主要停留在方案咨询和可行性评估阶段时，面向家庭的具身智能厂商，已经在洽谈渠道合作了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成立于2024年底的乐享科技在此次展会上推出了全新品牌“元点智能”。据郭人杰介绍，元点智能将作为乐享科技的具身智能品牌，率先聚焦家庭消费级机器人市场，并已开始向更广泛的场景拓展。这一布局也标志着乐享科技正朝着集团化方向发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次CES&amp;nbsp;乐享科技主推两款产品：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;W1&amp;nbsp;聚焦海外市场，定位为“移动储能+露营拖车”。集成储能模块、音响、影像采集、显示屏及移动电站功能，可拖挂50公斤物资，满足露营场景下的负重与能源需求，又可提供基础安防功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一款产品&amp;nbsp;M1&amp;nbsp;则采取双轨策略：在海外针对极客与开发者群体，主打易得和支持二次开发；在中国市场，则强化家庭陪伴属性，例如跌倒检测、用药提醒、宠物远程监控等实用功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52acc99017c2c3ce55f7b5b4fe43ad56.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;在CES期间，乐享科技携全线产品亮相，包括一米六五大人形机器人Jupiter；家庭具身智能机器人M1；履带式机器人W1以及一米一左右小型人形机器人A1，以及彩蛋产品具身智能熊猫。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;乐享科技创始人、CEO&amp;nbsp;郭人杰透露，展会期间，公司已与北美、欧洲等地的本地连锁采购商及多国独家代理商展开合作洽谈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他甚至已为机器人产品定下具体销售目标：2026&amp;nbsp;年，元点智能旗下所有品类产品的全球销量达到&amp;nbsp;3&amp;nbsp;万台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026&amp;nbsp;年，元点智能希望在渠道上完成北美、欧洲、中东等核心市场布局；在品牌上，塑造具有科技感的全球化形象；在用户层面，建立全球用户池，通过共创机制，让用户成为长期口碑的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更具象征意义的是，重型机械巨头卡特彼勒（Caterpillar）罕见地登上&amp;nbsp;CES&amp;nbsp;Keynote。其&amp;nbsp;CEO&amp;nbsp;Joe&amp;nbsp;Creed&amp;nbsp;开场便申明：“为什么‘黄色铁疙瘩’会和科技巨头同台？”他的回答是：“当今技术最大的瓶颈其实不在软件，而在物理世界。AI&amp;nbsp;需要更多芯片，而芯片依赖地下开采的矿物；数据中心耗电量已超过当前电网承载能力；整个数字生态都需要能更快建造、更强运行、永不中断的基础设施。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;包括卡特彼勒和西门子在内的工业巨头，也正在将具身智能能力嵌入既有产线中，用以提升柔性制造与自动化水平。侯伟对InfoQ表示：“具身智能之所以受到如此多关注，是因为大家普遍相信这一定是未来会发生的事，所以不可错过，自然会投入资源。我相信对于库卡和西门子来说，逻辑也是相似的：在已有资源禀赋和沉没成本的基础上，如何确保自己不错失这一轮关键机遇。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当被问及人形机器人在工业场景中的不可替代性时，侯伟给出的判断颇为克制：在大批量、单一产品的生产中，传统自动化仍然是成本最低、ROI&amp;nbsp;最高的选择；但当制造业目标转向“个性化定制”，能够适应多任务、多环境的通用人形机器人，就会真正显现价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器人并没有一夜之间改变世界，但在&amp;nbsp;CES&amp;nbsp;这个舞台上，它们已经开始明确自己的去处了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年CES上具身智能的密集亮相，不禁让人想起&amp;nbsp;2015&amp;nbsp;年车企扎堆展示自动驾驶概念车的情景。那时，自动泊车刚刚实现，自动转向还属于前沿技术，而今天，这些早已成为智驾的标配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同一时期，车企还热衷于在车里添加更多屏幕、取消实体按键、方向盘和刹车，认为这会是未来。大众&amp;nbsp;Golf&amp;nbsp;R&amp;nbsp;Touch&amp;nbsp;试图用“靠近即唤醒”的电容系统和手势控制替代实体按键；梅赛德斯&amp;nbsp;F015&amp;nbsp;Luxury&amp;nbsp;in&amp;nbsp;Motion&amp;nbsp;用六块触控屏，让乘客通过手势、眼神甚至“透视”车外世界；谷歌原型车则彻底取消方向盘和踏板。如今回看，这些曾经“革命性”的设计，并不能进入现实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;超前的构想依然大量出现在&amp;nbsp;CES，但未来总是以更平凡却实用的方式来到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，具身智能和AI硬件会走向什么地方呢？我期待&amp;nbsp;2036&amp;nbsp;年，再回头看今天的尝试。&lt;/p&gt;</description><link>https://www.infoq.cn/article/05bVyOA1exU97VWzZz1B</link><guid isPermaLink="false">https://www.infoq.cn/article/05bVyOA1exU97VWzZz1B</guid><pubDate>Sat, 10 Jan 2026 09:11:25 GMT</pubDate><author>陈姚戈</author><category>出海</category></item><item><title>Anthropic突然封禁第三方工具调用Claude，Cursor、OpenCode、xAI 集体“中枪”！</title><description>&lt;p&gt;&amp;nbsp;在 AI 编程工具快速演化的当下，模型能力本身已经不再是唯一的竞争焦点。谁能控制模型的使用方式、定价结构以及开发者通道，正在成为新的博弈核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic突然阻止第三方工具调用Claude，引发社区强烈不满&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，Anthropic 宣布已经部署了更严格的技术保障措施，用以防止第三方工具“伪装”为官方 Claude Code 客户端，从而绕过速率限制和计费机制，低成本调用底层 Claude 模型，此外，Anthropic 也被曝出切断了包括 xAI 在内的部分竞争对手对 Claude 模型的访问权限，其中 Cursor IDE 成为了关键的“触发点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这期间到底发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事情的导火索，来自于大量使用 OpenCode 等开源代码代理工具的开发者发现：自己原本能够正常使用的 OpenCode、Cursor 等工具，突然无法再调用 Claude 模型，部分账户甚至直接被封禁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;短短几个小时内，每月支付100～200美元的开发者们纷纷涌入GitHub表达不满，引发了超过147个点赞和245个Hacker News积分。此外，用户们也开始大规模取消订阅，称强制迁移到Anthropic官方工具是“回到石器时代”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之所以会引发开发者们的强烈不满，是因为事发太过突然。没有任何警告，也没有迁移方案，就这么突然被锁定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该限制专门针对 OpenCode 1.1.8 及以上版本。但是，通过 OAuth 认证的 GPT-4 仍然能够正常工作。只有 Claude Max 的功能被限制了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenCode 是一款开源 AI 编码助手，它将 Claude 集成到 VS Code、Cursor 和其他 IDE 中。此外，它还增加了键盘快捷键、上下文感知和多文件编辑功能。开发者们非常喜欢它，因为它将 Claude 的推理过程融入到他们现有的工作流程中，而无需强制他们使用终端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户 @Naomarik&lt;a href=&quot;https://github.com/anomalyco/opencode/issues/7410&quot;&gt;在 GitHub 问题上发帖称&lt;/a&gt;&quot;：“如果单纯仅仅使用 CC（Claude Code）就像回到了石器时代。” 他立即降级了每月 200 美元的 Max 订阅，然后彻底取消了订阅。他的理由是：“它无法满足我的工作流程需求，也无法提供 OpenCode 所具备的可见性。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2e476c40c1ff15b6ba0c1b58d9796b8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并非个例。还有位用户正在项目进行到一半时，访问权限突然中断。该开发人员称：“一个小时前还好好的，现在就出现这个错误了。”另一位说：“整个下午/晚上都在用，结果就遇到这个问题了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位用户反映在工作流程进行到一半时放弃了订阅。原因很简单：每月支付 200 美元，却只能使用 Anthropic 仅支持终端的 Claude Code 工具，而非他们真正想要的 IDE 集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事情发生后，几乎在同一时间，另一条消息引发了更大的震动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有外媒披露，埃隆·马斯克旗下的 xAI，其内部开发人员已无法再通过 Cursor 使用 Claude 模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;起初，这被解读为 Anthropic 的“全面封锁策略”。但随后有知情人士指出，这其实是一场基于商业条款的独立执法。问题的关键，在于 Anthropic 的服务条款 D.4 节，其中明确禁止两类行为：第一，使用服务构建或训练竞争性 AI 系统；第二，对服务进行逆向工程或复制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 的工程师，正是通过 Cursor，将 Claude 用于加速自家模型的研发与测试。这在法律意义上，已经构成了“竞争性使用”。Cursor 在这里并非违规主体，但成为了违规行为的放大器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管此次一系列切断行为事发突然，但如果拉长时间线，会发现这次事件并非孤立。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年 6 月，Windsurf 编码环境突然被切断 Claude 3.x 的第一方产能，被迫转向 BYOK，并主推 Gemini 作为替代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年 8 月，Anthropic 又撤销了 OpenAI 对 Claude API 的访问权限，理由是后者将 Claude 用于模型基准测试与安全评估，违反竞争限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时 Anthropic 的说法就已经非常直白：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“Claude Code 成为程序员首选，OpenAI 的工程师也在使用它，并不令人意外。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但“使用”和“竞争性使用”，在这里有一道清晰的界线。xAI，只是最新一个踩线的例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工程师解释：是误伤&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在社区不满情绪持续发酵后，首先站出来解释的是 Anthropic 内部负责 Claude Code 的工程人员 Thariq Shihipar。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 X（原 Twitter）上，他确认公司已经“加强了对 Claude Code 安全套接字欺骗的保护措施”，并承认此次上线确实造成了一些误伤：部分用户因为触发滥用过滤规则而被自动封禁，Anthropic 正在回滚和修复相关问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6e/6e4b10b0d16c365471cb720577fb103c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这番解释，并没有平息社区的不满。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，真正被切断的并不仅是“异常流量”，而是一整类第三方工具的使用路径——尤其是那些通过 OAuth 授权，利用用户个人 Claude 订阅账户，在外部环境中运行自动化编码代理的软件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，这次调整的目标，并不是某几个 bug，而是“桥梁本身”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那座被拆掉的“桥”是订阅聊天模型和自动化代理之间的连接纽带。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以 OpenCode 为代表的工具，扮演着一个非常关键、但又长期游走在灰色地带的角色。它们的核心价值在于：将原本为“人类对话”设计的订阅制模型，转化为可以被自动化代理调用的基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术实现上，这类工具往往会模拟官方客户端（如 Claude Code CLI）的身份，通过伪造请求头、复用 OAuth Token 的方式，让 Anthropic 的服务器“以为”请求来自官方环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是所谓的“客户端欺骗”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在短期内，这种方式为开发者带来了极大的自由度：固定月费、 不受 API 计费限制且可以长时间运行高强度 Agent 循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从平台视角看，这座桥存在三个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，技术不可控。当 OpenCode、Cursor 等封装器内部出现错误或性能问题时，最终被指责的往往是“Claude 不稳定”“模型变差了”，而 Anthropic 却无法复现和诊断这些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，使用模式失真。订阅产品的设计初衷，是“人类辅助编程”，而不是 24 小时运行的自主代理。第三方工具解除速率限制后，模型的负载特征发生了根本变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三，也是最现实的一点：成本失衡。在 Hacker News 上，一位用户用一个形象的比喻概括了这场冲突的本质：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Anthropic 提供的是“无限自助餐”，但前提是你吃得慢。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Pro/Max（最高 200 美元/月）的定价，本质上是基于“人类交互速率”设计的。而 Claude Code 官方环境，正是用速率限制和执行沙箱，来保证这套模型不会被“吃垮”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如 Hacker News 用户 dfabulich 所说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“Claude Code 每月 200 美元的订阅价格，与 Anthropic 按 token 计费的 API 相比，存在明显的成本断层。在高频使用场景下，一个月内通过 Claude Code 消耗的 token，如果全部走 API 计费，成本很容易超过 1000 美元。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为这种价差，社区普遍认为 Claude Code 本身更像是一个“特殊定价的官方通”，Anthropic 的真实意图，是希望用户在这一订阅下使用官方的 Claude Code CLI，而不是通过 OpenCode 这样的第三方开源工具来“绕行”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种背景下，OpenCode 实现绕过限制的技术方案，被不少开发者视为一种“必然结果”：当用户已经为 200 美元的订阅付费，自然希望在自己熟悉、效率更高的工具中使用这些能力，而不是被绑定在单一官方客户端中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;社区反应：有愤怒、也有理解&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者社区的第一反应，并不友好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ruby on Rails 创始人 David Heinemeier Hansson（DHH）在 X 上直言不讳地表示，这一举动“对客户极不友好”。在他看来，用户既然付费订阅，就理应拥有更大的使用自由。他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;已确认 Anthropic 故意屏蔽 OpenCode 和任何其他第三方框架，其偏执的企图是强迫开发者使用 Claude Code。对于一家以使用我们的代码、我们的文字、我们的一切来训练模型的公司来说，这种政策简直糟糕透顶。请修改条款。&amp;nbsp;我认为所有模型提供商都推出自己的命令行界面（CLI）是件好事，但说实话，没有哪个开发者会想安装五个不同的 CLI。他们肯定希望学习并使用一个能够控制所有模型的工具。对我来说，这个工具就是 OpenCode。&amp;nbsp;这再次提醒我们，为什么我们需要开源软件！作为开发者，你肯定不想被单一模型提供商束缚。如果他们觉得能掌控你，你就会忍不住想要榨取他们的利益。所以，今天就尝试一些新的模型吧！”&amp;nbsp;同时，也需要提醒大家，这一切尚未定论。作为一家前沿实验室，长期的成功不仅仅取决于当下是否拥有最佳模式，还取决于你如何与开发者互动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5b/5b660b025615f393a97e56550f450d77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Hacker News上，关于此事的讨论也呈现出两种声音：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有开发者将矛头直接指向 Anthropic 的产品策略，认为公司不应该在订阅制和 API 计费之间制造如此巨大的落差。一种更合理的做法，应该是将订阅计划本身设计为“API 点数包”，例如在标准 API 定价基础上给予一定折扣，而不是提供一个事实上的“无限量自助餐”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有声音站在 Anthropic 一侧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Yearn Finance 开发者 Artem K 指出，相比直接封号或追溯 API 费用，Anthropic 选择“温和封堵路径”，已经算是相当克制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，工具生态的反应异常迅速。OpenCode 创始人 Dax Raad 在X 上感慨“Anthropic今天的行为充分展现了为什么竞争是世界上最重要的事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时他公开宣布，将与 OpenAI 合作，要努力让 OpenCode 与 GPT-5 尽可能完美地协同工作了，并表示&amp;nbsp;Codex 用户可以“直接在 OpenCode 中使用自己的ChatGPT/Pro 套餐”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/abc899ba6eda323b5b39594a520e50ed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，“是否应该开源 Claude Code”成为另一条激烈争论的主线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;支持开源的一方认为，Claude Code 作为开发者工具，其核心价值在于生态扩展与社区创新，长期闭源只会催生更多非官方实现，最终反而削弱平台控制力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反对者则指出，Claude Code 恰恰是 Anthropic 在编程领域建立差异化优势的关键资产，其重要性甚至超过 Claude 模型本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“要求 Anthropic 将这一工具完全开源，无异于让公司主动削弱自身竞争壁垒。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有用户分析了为什么 Anthropic 会向订阅用户提供看起来如此优惠的价格？他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;答案其实并不复杂：这是典型的“厂商锁定”策略。他们真正想出售的，并不是每月 200 美元的订阅本身，而是围绕 Claude Code 构建起来的完整生态系统。&amp;nbsp;对大多数普通用户而言，普通用户并不会每月支付 100 美元，只为了让一个聊天机器人帮他们做作业或生成一份蛋糕配方。在这样的前提下，想要从通用聊天机器人中获得可观利润，本身就极其困难。&amp;nbsp;但编程市场完全是另一回事。企业已经明确展现出付费意愿，希望用能够在极短时间内完成工作的 AI 服务来替代部分人工程序员；而程序员个体本身，也愿意自掏腰包购买这些工具，以减少工作量——即便他们清楚，这种趋势长期来看可能会削弱自身的不可替代性。与此同时，企业对“完美质量”的要求并没有高到足以阻碍这种替代。因此，编程被普遍视为 AI 领域中少数真正具备高利润潜力的市场之一，各家公司正围绕这一方向展开激烈竞争，而 Claude Code 正是 Anthropic 针对这一市场推出的核心产品形态。&amp;nbsp;在这种背景下，单纯销售一个不带任何生态绑定的订阅服务，从一开始就不是 Anthropic 的目标。这种订阅模式本身并不盈利，甚至也并非打算立即盈利，而更像是一个典型的“亏损引流产品”。真正的目的，是让用户在长期使用中深度融入 Claude Code 的整体生态。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具体验层面，争论同样激烈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部分用户认为 Claude Code 在“上下文管理”“工具调用稳定性”和整体 DevEx（开发者体验）上仍然领先，尤其是在终端与 TUI 场景中，明显优于 Gemini、Codex、Copilot 等竞品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有不少开发者表示，OpenCode 在相同模型下的执行效率更高，完成相同任务所需时间更短，且多模型、多厂商切换能力更强；另一些用户则更偏好 Kiro、Q 等工具，认为它们在简洁性和稳定性上胜过 Claude Code。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今这件事，似乎这已经不是技术讨论，而是一场生态位的重新站队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/thdxr&quot;&gt;https://x.com/thdxr&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://venturebeat.com/technology/anthropic-cracks-down-on-unauthorized-claude-usage-by-third-party-harnesses&quot;&gt;https://venturebeat.com/technology/anthropic-cracks-down-on-unauthorized-claude-usage-by-third-party-harnesses&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://byteiota.com/anthropic-blocks-claude-max-in-opencode-devs-cancel-200-month-plans/?utm_source=chatgpt.com&quot;&gt;https://byteiota.com/anthropic-blocks-claude-max-in-opencode-devs-cancel-200-month-plans/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EDUxU7QhZgG65WQDtoP1</link><guid isPermaLink="false">https://www.infoq.cn/article/EDUxU7QhZgG65WQDtoP1</guid><pubDate>Sat, 10 Jan 2026 05:30:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Facebook调查显示：越来越多的人开始采用带类型的Python，以提升代码质量和灵活性</title><description>&lt;p&gt;&lt;a href=&quot;https://engineering.fb.com/2025/12/22/developer-tools/python-typing-survey-2025-code-quality-flexibility-typing-adoption/&quot;&gt;Facebook 2025年Python类型调查&lt;/a&gt;&quot;在1200多名受访者中进行，重点介绍了Python开发人员如何以及为什么越来越多地采用该语言的类型提示系统。该调查还揭示了开发者最看重的东西，以及他们最大的挫折和愿望。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总体而言，86%的受访者表示他们“总是”或“经常”在代码中使用类型提示，其中具有5-10年Python经验的开发人员的采用率最高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然数据显示，类型提示在被调查的样本中被广泛采用，但不排除选择偏差，因为使用类型提示的开发人员可能更有可能做出回应。尽管如此，该调查揭示了使用类型提示的Python开发人员的一些有趣趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;调查结果显示，Python的类型提示系统已经成为大多数工程师开发的核心部分。[...]我们发现，所有经验水平的玩家对打字的接受程度都是相似的，但也存在一些有趣的细微差别。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级（0-2年经验）和高级（10年以上经验）开发人员使用类型提示的频率都较低，分别为83%和80%。该调查的作者认为，初级开发人员面临更陡峭的学习曲线，而高级开发人员可能正在处理大型遗留代码库，而在这些代码库中采用类型提示更为困难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发人员列举了采用Python类型系统的几个好处，包括更好的可读性和代码内文档，改进的IDE和工具支持，早期的错误检测以及增强的信心。他们还强调了高级特性的价值，如协议、泛型和在运行时检查注释的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一方面，受访者指出了一些挑战，包括第三方库中有限的类型提示支持，泛型和修饰符等高级特性的复杂性，以及复杂类型的冗长性增加。其他痛点包括工具碎片化、缺乏运行时强制执行以及难以修改遗留代码。受访者还指出，Python的类型系统似乎不如其他语言（如TypeScript）的表达能力强，而且它的快速发展意味着语法和最佳实践在不断变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;调查中另一组有趣的发现涉及改进Python类型系统的方法。一些建议包括借鉴TypeScript的特性，如交叉类型、映射和条件类型、实用程序类型（如 Pick 、 Omit 、 keyof 和 typeof ），以及更好的字典结构类型。其他建议侧重于更好地支持泛型和代数数据类型，包括更高级的类型；基于类型提示的可选运行时类型强制和性能优化；改进了对函数包装器和装饰器等模式的处理，支持动态属性；等等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具方面，MyPy仍然是首选的类型检查器，采用率为58%，紧随其后的是Pyright/Pylance。新的基于Rust的类型检查器（如Pyrefly、Ty和Zuban）越来越受欢迎，被超过20%的受访者使用。Visual Studio Code是最常见的IDE，其次是PyCharm和Vim/Neovim。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项调查中还有更多内容无法在此一一介绍。请务必阅读原始文章以获取全部详细信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/facebook-typed-python-survey/&quot;&gt;https://www.infoq.com/news/2026/01/facebook-typed-python-survey/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t6i8lghRAdi9I4weD2mR</link><guid isPermaLink="false">https://www.infoq.cn/article/t6i8lghRAdi9I4weD2mR</guid><pubDate>Fri, 09 Jan 2026 07:21:00 GMT</pubDate><author>Sergio De Simone</author><category>Meta</category><category>编程语言</category></item><item><title>Manus高溢价收购背后，是Agent开发落地困境</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;撰稿：李文朋&lt;/p&gt;&lt;p&gt;编辑：王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，“Meta以20亿美元收购Manus”的消息传得很热。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Manus曾被嘲讽“套壳”，但业内人士认为，虽然Manus整体架构和理念不算颠覆式“新”，但在任务连通性、容错、回退机制等实现上，极度考验工程能力，远不是“换个皮”那么简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Manus创始团队与媒体的最近一次访谈中，联合创始人季逸超提出目前Manus定位只是一位“通用型助手”，帮普通人把复杂工作流做完，不能完全替代用户本身。这也是因为在ToC场景里，普通用户对体验要求很苛刻——慢一点不行，错一点也不行，Manus团队很清楚这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说ToC用户已经够“难伺候”，那ToB客户对Agent的要求只会更高：一方面，企业希望Agent真正“上生产”，意味着要接入复杂的权限体系、业务系统和合规要求；另一方面，任何一次错误操作、脏数据写入、流程走错，带来的代价都远比个人用户高得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以会看到，过去一年很多企业在这条路上吃了不少苦：投入人力、投入预算，最后做出来的Agent用不了。MIT《2025年商业AI现状》报告里提到，约95%的生成式AI试点项目很难进入生产环境，很多最终都卡在上线前后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题出在哪？就在于这些一连串的工程难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如代码标准不统一、系统接口五花八门、工具调用不稳定、开发周期被拉得很长；数据资产混乱、想用调不出；安全合规和权限管理一碰就痛；甚至出现“越用越退化”的优化难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到底，并不是模型不行，也不是工程师不会做，而是整个Agent开发还不够成熟，大家还在摸索阶段，没有提前规划一套更清晰、更稳定的“做法”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，国内的云厂商开始认真思考一个问题：到底怎样才能帮助企业把Agent的难题解决掉？有没有一种更适合落地的开发范式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025云栖大会上，阿里云CTO周靖人就曾提出过「AI时代的Agent开发范式」。而在1月7日，阿里云百炼对“1+2+N”体系和开发范式做了一次更系统的升级，把它落成一个工程化的体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd7872e261efebb50616350d97d7fa59.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套“1+2+N”体系的想法并不复杂，本质是把Agent落地拆成三层：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;稳底座（1）：把模型和云资源这些基础能力做稳定、可扩展、可治理。地基不稳，再漂亮的Agent也只能停在PoC。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;定范式（2）：给企业一套把Agent做成“工业产品”的开发与运行体系，能开发、能部署、能迭代，交付不再反复折腾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;理杂活（N）：把真实业务里最难、最碎、但最致命的集成、权限、评测、成本这些“脏活累活”，做成可插拔的组件，让企业能按需拼装。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从这个角度看，这次阿里云百炼迭代背后体现的是一种更务实的方向：要用更工业化的方式，让企业的Agent在真实业务里跑起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、“N”：通用大方案，不如啃硬骨头的“高手组件”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;经过大量Agent的试错，企业如今在启动一个Agent项目时，最先拎出来掂量的往往不是模型，而是数据怎么处理与调用、安全问题能不能搞定、上线后怎么评估和优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些硬问题不先解决，再漂亮的Agent构想也很难真正走进生产环境。而在阿里云百炼的“1+2+N”体系里，“N”恰恰就是优先来啃这些硬骨头的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更关键的是，这一次“N”做了很大的升级：它把落地过程中那些最常见、最难啃、最容易反复踩坑的环节抽象出来，沉淀成一组可插拔、可组合的模块化组件。Agent开发的难题看起来五花八门，但很多难题其实有共通的解法，可以被提炼、被复用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“N”组件的存在，可以让企业缺什么就用什么、按需组合，把时间花在业务价值上，而不是重复造轮子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次升级里，一个直观的变化在应用广场：阿里云百炼把同类Agent做成了十多个精选合集，提供新的多模态模板，支持免登录体验，也能一键调用API，把“试试到跑起来”的路径压得更短。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正决定“能不能落地”的挑战，还有数据连接与知识管理、安全与权限控制、可观测与持续优化等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业做AI转型，数据治理永远是“卡脖子”环节。尽管大家都知道数据重要，但真落到工程上，标注、清洗以及让模型读懂私有数据的成本极其高昂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，企业内部约80%的数据以PDF、图像、视频或会议录音等非结构化形式存在。据IDC预测，这些数据多处于“不可检索、不可复用”的沉睡状态。随着全球数据量预计在2026年激增至221ZB，如何将这些碎片资产转化为Agent可调用的知识，成为企业发展的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼的思路是把这条链路做成“工具化”：用多模态RAG、多模态数据库、Connector连接器，把数据处理变成更工业化的流水线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多模态数据库通过智能解析、分类归档，打破图像/音频/视频等模态壁垒；多模态知识库RAG不再局限于纯文本，支持数十种格式的高精度解析，包括扫描件PDF、复杂报表、音视频会议记录等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Workflow层面添加多模态文件处理与生成节点，同时提供覆盖Chunking、Embedding、（多模态）Embedding、Rewrite、Retrieval、ReRank等在内的向量化全流程能力，用于检索与消化企业数据资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/4c/b8/4c89e13301545cce89449b9b112365b8.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;百炼平台还提供开箱即用的RAG工具，企业无需自建复杂的向量库与检索链路，也能获得高性能的知识检索与生成能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fec174a4ee0c252827b7e3b3c962d42d.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把知识库做起来只是第一步。要让Agent真正有用，它就得能接入实时数据。然而，长期以来ERP、CRM等异构系统间的集成成本高昂，导致65%的企业受访者认为业务系统沦为新的“数据孤岛”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台推出的Connector（企业级数据连接器），就是想把这个门槛降到最低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过Connector，企业可以一键对接飞书、语雀、MySQL及OSS存储；连上之后，这些数据既能直接喂给知识库，也能驱动工作流跑起来；平台还提供数十种预置工具（Tools），支持用自然语言直接查询或检索数据等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，数据一旦接进来了，真正棘手的问题也随之出现：权限边界与责任归属难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期以来，很多Agent在企业业务中多以匿名形式存在。这种“身份透明”导致操作链路难以溯源，不仅无法明确执行指令的主体，更埋下了越权操作的隐患。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，百炼平台引入Agent Identity组件，将Agent纳入企业身份治理的范畴。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过集成Okta、EntraID等主流系统，平台为每个Agent分配数字身份，使其行为从孤立的匿名调用转变为绑定主体、可供审计的合规操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台也将传统的“常驻权限”升级为“按需授权”仅在执行任务时获得短期令牌，任务结束权限即刻回收。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;配合权限降级机制，Agent的边界被严格限制在用户授权范围内，确保无法越权。全链路审计日志则让每一步决策都透明可查，解决了企业“敢不敢给权限”的顾虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对执行环境安全，百炼平台也构建了Sandbox（沙盒）物理隔离屏障。当Agent处理外部代码或第三方数据时，系统可以利用虚拟化技术将其限制在独立空间内，精简系统调用并严控网络访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一个任务会话均在“即用即弃”的容器中运行，执行完毕立即重置，彻底阻断了数据残留与交叉污染。平台同步引入实时监控与会话回放，一旦监测到异常行为将立即终止任务。这种设计为Agent提供了“受控下的自由”：在屏障内保持灵活性，在边界外确保系统安全。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而当Agent真正跑进业务之后，新的共性难题也会浮现：怎么评估、怎么持续改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与传统软件不同，Agent的执行具有非确定性：即便输入相同，也可能因模型的随机性、工具调用顺序或上下文波动产生不同的输出。这导致开发者难以追踪Agent决策逻辑，在任务失败时无法精准定位是模型、工具还是流程缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台通过Trace（可观测）与Evaluate（评估）组件，实现了从“黑盒”到“透明”的转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Trace组件提供完整的执行轨迹追踪，清晰复现了从思考（Thought）、行动（Action）到观察（Observation）的每一步。开发者可以判断哪一步耗时最长、哪个工具失败率最高，或是在哪个环节陷入了逻辑死循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结合Token消耗、响应速度等量化指标，这些数据可通过Grafana进行可视化监控，构建起实时的生产环境观测能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于此，Evaluate则建立了体系化的评价标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在任务完成度评价方面，百炼平台可以通过衡量目标满足率与输出质量对Agent进行评分；并支持“模型评测（LLMasJudge）”、专家打分与人工复查相结合的混合模式，对失败任务进行深度归因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以说，基于日志（Logs）、指标（Metrics）与追踪（Traces）的三大支柱，百炼平台设计了一个“评估—优化—验证”的持续迭代闭环。这种由数据驱动的迭代机制，也驱动着Agent实现“越用越好用”的工程闭环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比于自建底层架构，直接调用百炼平台的成熟组件能让开发周期缩减数倍。以RAG系统为例，以往搭建搜索和解析链路需要数周，现在利用多模态RAG组件，几个小时就能跑通。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业不需要为每个Agent单独开发身份认证或数据接口，一套Agent Identity就能管好所有Agent的工号，一个Connector就能接通全公司的数据源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;放在阿里云百炼“1+2+N”体系中，组件化正填补模型到业务之间的最后一块拼图：模型提供计算力，开发范式定好流程，而这“N”个组件则专门负责解决数据怎么连、权限怎么划、效果怎么评、安全怎么管这些具体的“杂活”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、“2”：“下一代”Agent，需要新开发范式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“N”组件把坑填平，只解决了“这事能不能接得上、管得住”。企业真正要把Agent变成长期能用的工业生产能力，还得解决另一个现实问题：怎么开发、怎么协作、怎么迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼“1+2+N”体系里的“2”，就负责这一点，它涵盖两种开发方式（低代码+高代码），以及配套Agent开发平台，通过同一套平台和运行时，分别服务两类人、两种交付方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么要做成“2”种模式？因为企业落地Agent的过程，基本就是两条路同时走：想快速试点、尽快看到效果，低代码更省事、更快；真要进核心业务、对接复杂系统，高代码才够灵活、够深。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4b74cf85efa3ce79ee9854b2b74b875a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更现实的是，企业在代码协作上存在长期的“割裂”：低代码不够用，高代码效率低。产品经理用低代码搭建的草案，往往需要技术团队用高代码重新开发，而这种重复劳动会导致业务逻辑在传递中失真。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了让Agent更快、更深地融入业务，百炼把低代码和高代码“打通”：企业可以从低代码起步做验证，再逐步演进到高代码做优化，形成一种更自然的渐进式开发，让真正懂业务的人与懂技术的人有机协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据Gartner的预测，到2028年，企业里相当一部分Agent应用会由业务人员主导搭建。双开发模式很可能会成为Agent走向工业化落地的一种主流形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但“2”的意义还不止是“怎么写代码”。更重要的是：下一代Agent本身就需要新的开发范式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去的一年，很多企业里的Agent实际上还停留在比较“表层”的形态：一种是以提示词工程为核心、更多承担辅助角色的Copilot；另一种是能处理重复流程、严格按预设步骤执行的“数字员工”。它们能提升效率，但往往缺少主动规划与闭环执行能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent不应仅“被告知怎么做”，而是“应该主动思考怎么做”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此阿里云百炼提出了Agent2.0：未来的Agent要能围绕目标自主规划，把复杂问题拆成可执行的小任务，过程中还能根据反馈调整策略，最后交付更稳定、质量更高的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照这个定义，Agent2.0的核心链路是“规划—执行—反思”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现实里很多Agent开发失败，问题往往是开发范式还停留在老路上。传统那种线性链路（用户 →Agent→ 模型 → 输出）有三个硬伤：没有规划，就很难应对动态场景；没有反馈与纠错，走偏就很难拉回来；没有长期记忆，交互体验容易断裂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了能承载Agent2.0的生产级落地，百炼平台对开发范式做了系统升级：AgentScope从过去偏“开源写代码”的工具形态，演进为覆盖Agent全生命周期的工业化开发平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fdc7c9724836cf7260efbe868beab353.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一步，是把“上手门槛”压到尽量低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，AgentScope做了对主流模型能力的统一集成，内置100+预训练模型，拿来就能用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一方面，百炼平台提供了一批可复用的智能体库，比如交易智能体（EvoTraders）、调研智能体、金融分析智能体、数据科学智能体（Data-Juicer）、浏览器使用智能体、语音智能体等，减少从零开始的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二步，是围绕更高级的Agent2.0，把“协作与执行”能力补齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AgentScope主要通过三块来支撑：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多智能体编排：引入基于Actor模型的分布式架构，支持多个专业Agent的并行协作与自动调度。研究表明，协作模式任务成功率比单一Agent高出90.2%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能体上下文管理（长期记忆）：深度适配Mem0、ReMe等记忆系统。使得Agent能够自主存储并检索历史交互中的关键信息，在后续任务中实现能力的持续迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工具调用能力：全面兼容StreamableHTTP、SSE、STDIO等主流接口标准。通过支持AnthropicAgentSkill规范，在运行时即可动态加载新工具或移除冗余资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在工具调用这层，ReAct这类“边想边做”的范式，也被不少实践证明更容易形成任务闭环：学术基准测试中，ALFWorld任务只需2个示例即可达到71%的成功率，高于强化学习模型的37%；在复杂任务中，准确率相较纯FunctionCalling提升约15%–20%，成本比CodeAct低78.9%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Agent2.0优化与部署阶段，阿里云百炼通过AgentScope-Studio+AgentScope-Runtime打通了全生命周期的工业化链路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AgentScope-Studio可通过自定义多维表现指标，评估工作流设计的合理性；提供从输入到输出的全链路追踪与可视化，让Agent行为与决策过程实现“可观测、可复盘”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百炼平台利用评测结果持续改进，让失败样本成为训练资产，形成“评测→优化→验证→再优化”的迭代闭环，实现从“盲目调参”到“数据驱动优化”的范式转换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在落地部署环节，AgentScope-Runtime支持Docker、K8S、ACK、Serverless等多种部署形态；通过Agent-as-a-Service将Agent封装为可独立调用的API服务，兼容A2A与ResponseAPI等协议，便于集成、弹性扩缩与快速迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果把阿里云百炼的开发范式拆开来看，其实就是从“构建”走向“运营”的一个完整闭环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前半段构建，重点是更快、更省力地把东西搭起来：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用可选智能体模板减少重复劳动；用多智能体编排与工作流把复杂任务拆成可协作的子任务；用高低代码一体化实现统一开发与交付；通过ReAct等方式完成多任务的规划、执行与自我纠偏，再结合用上下文和长期记忆支撑长链路执行等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后半段上线运营，就是做让它智能地跑起来：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用可观测和自动化评测把效果变成可量化的指标；打通真实系统和数据源，拿到反馈并持续优化；在企业既有基础设施上实现更便捷的部署与稳定运维；同时借助Identity、模型单元专属部署、机密推理等能力，把权限、安全与合规治理补齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套开发范式的最大亮点，就是它统一按照“工业级Agent2.0”的标准做事：高效的开发体系+可持续的反馈闭环+便捷可靠的上线部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、“1”：模型优势之外，深挖“模型服务”工程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，无论是组件化拼装，还是低/高代码协作，最终都要落在同一个问题上：模型调用能不能稳定、能不能扛流量、能不能控成本、能不能过合规。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以“1”是整个体系的地基——模型与云服务底座把推理服务、弹性、部署形态与安全边界做成统一供给，保证上层“能跑起来，也跑得久”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多企业在用Agent的过程中，卡住的往往不是“模型会不会”，而是一些更现实、更工程的问题：1）延迟、并发、稳定性跟不上真实业务流量；2）成本容易失控（链路长、多轮工具调用、重试一多就更明显）；3）部署和合规麻烦（私有化、混合云、权限边界、数据隔离等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在调用模型的时候，企业最关心的也无非就两件事：成本与性能。为此，百炼平台提供了一套云资源调度组合拳：“异步调用+闲时调度”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/87/8743f59ec1c2d256f6f9d7f5b195f60e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前搞大规模的数据清洗、标注，或者是分析长视频，这些任务不仅计算密集，而且耗时漫长。最头疼的就是走“同步调用”，跑到一半接口超时了，任务断掉，前面全白干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了异步调用就省事多了，它像寄快递一样，你把任务丢给后台，拿个任务ID，就可以去干别的。不用在那儿死等结果，等服务器处理完了你再回来取就可以。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而“闲时调度”更像“错峰用电”：不着急的任务挪到资源空闲的时候跑，单价更划算，整体资源利用率也更高。阿里云百炼官方给出的数据是，动态调度后闲时推理成本可降低50%。对需要处理海量数据的企业来说，这种节省是实打实的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，阿里云百炼这次把“模型服务能力”也做了系统升级，主要围绕四块：模型后训练、专属模型单元部署、平台可观测、推理安全防护，系统性地”深挖“模型的服务能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;先从选型说起。百炼平台把模型体验中心做了结构性重构，把在线模型的能力做成更直观的“能力图谱”，支持文本、视觉理解、图像/视频生成、语音交互等全模态体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样企业就不用靠猜，也不用“盲选”，可以在平台上直接对比不同模型在具体场景下的表现，再做选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模型选定之后，是否“实用”往往取决于后训练。很多企业真正需要的不是通用能力，而是用自家数据和业务知识微调出来的“专家模型”，这才更贴近业务，也是企业的核心壁垒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模型训完后，真正容易被“拦住”的常常是部署。自建集群运维复杂、成本也难估：为了应付峰值不得不预留一堆算力，平时又闲着浪费；多租户环境下的数据隔离和性能争抢，会让企业心里不踏实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼推出“模型单元”部署，其实相当于给企业开了条“专属通道”，减少资源争抢带来的不确定性，让高并发和低延迟更稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7a407526e4757cc1cfd44377d66694b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时提供全托管的Serverless方式：系统会跟着实时流量自动扩缩容——忙的时候自动扩，保证体验；闲的时候自动收，尽量省成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方给出的测试数据里，模型单元部署相对传统自建集群方案，推理性能提升超过1.3倍，并发能力提升超过1.5倍。对企业来说，这类提升的意义很直接：同样的业务量，成本更低性能更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，调用模型处理数据时，最难绕开的是安全——尤其在金融、医疗、法律等高敏行业。很多企业不是不想用，而是卡在一句话：数据给到模型，会不会出事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，百炼平台推出模型“机密推理服务”，依托三层安全架构，为企业构建起全链路的数据保护围墙：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一层是基于CPU/GPU硬件可信执行环境的机密计算能力，将模型推理运行在硬件隔离的安全区内。即便云侧其他组件遭受攻击，敏感数据也难以被窃取或泄露。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二层是端到端加密的可信链路：实现了从用户端到云端计算中心的全程加密传输。数据在加密状态下进入TEE区域处理，计算结果在加密状态下返回，确保数据在“流动”与“处理”的全生命周期中始终处于保护伞下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三层是公开审计的可信服务：平台提供可验证的身份与安全能力证明。企业不仅能自主校验服务安全性，更能以此作为合规背书，向管理层、审计机构及客户证明其AI系统的高安全性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在使用体验上，机密推理被做成了“一键交付”的形态：企业只需要在模型库中选择支持机密推理的版本，一键部署到TEE隔离环境，就能直接调用机密推理服务来处理敏感数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;放在一起看，这次升级是在原有模型性能优势之上，又补上了几块关键拼图：云资源调度、后训练、模型单元化部署、机密推理安全体系等。几块一起发力，让大模型调用变得更实用、更省钱，也更安全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、没人愿意再“从零开始”，阿里云百炼Agent平台企业版已发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从市场角度来看，政企、金融、医疗等行业在采购云服务时，始终受困于一种不完美的平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公有云上手快、性能强，但数据边界与合规要求是跨不过的门槛；私有化部署虽有安全感，但往往陷入“模型、工具、流程”极其复杂的运维战泥潭，开发周期长、技术更新慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月7日，阿里云百炼企业版的发布，为市场提供了一个既保留数据主权，又拥有云端顶级效率的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业版支持专有云、本地化及VPC隔离，百炼平台将云端的成熟能力“下沉”至企业环境。更重要的是，百炼平台企业版支持源码级交付。这不仅仅是技术开放，更是给予企业自主演进的确定性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业不再需要买一堆零件回去组装，而是直接获得一个在自身安全边界内运行的Agent基座。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，企业版也并非新功能的简单集合，而是将百炼平台“1+2+N”体系（顶级模型、成熟范式、核心组件）封装为完整的交付体：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;双代码统一：兼顾业务验证的敏捷性与复杂逻辑的深度定制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多模态RAG：激活企业沉睡的音视频与文档资产，转化为实时知识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Trace与Evaluate：将Agent的黑盒行为拉到台面上，让调试与迭代成为标准工序。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大规模组织的管理诉求：企业版强化了多租户部署、SSO账号集成以及细粒度的权限审计。这些功能解决了IT部门的核心忧虑——让Agent的应用在组织内部不仅“能跑通”，更“可治理”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个行业走向成熟的标志，是目光从技术指标移向业务价值的“深水区”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里云百炼Agent平台企业版，本质上在扮演“AI时代技术中台”的角色。从行业趋势上看，未来企业大概率将不会从零开始建设AI能力，而是直接基于一个完整、成熟的技术中台起步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，在一年的野蛮生长后，留给企业AI试错的窗口期正在关闭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望2026年，Agent应用爆发增长几乎已成共识。Gartner预测，到2026年底，40%的企业应用将集成任务型AI agents（相比2025年不足5%），这也标志着Agentic AI正从概念走向主流生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对阿里云这样的全栈人工智能服务商而言，这将是多年技术积累转化为业务增量的红利期；对使用模型与Agent的企业客户而言，也将是Agent正式进入“拼效率、拼落地”的竞争元年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SxeNI9gVzcKr36xlwuCQ</link><guid isPermaLink="false">https://www.infoq.cn/article/SxeNI9gVzcKr36xlwuCQ</guid><pubDate>Fri, 09 Jan 2026 06:47:07 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>AI 工程化</category></item></channel></rss>