<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Fri, 30 Jan 2026 03:10:32 GMT</lastBuildDate><ttl>5</ttl><item><title>DoorDash通过多臂老虎机增强A/B测试</title><description>&lt;p&gt;DoorDash工程师Caixia Huang和Alex Weinstein说，尽管实验至关重要，但传统A/B测试可能过于缓慢且成本高昂。为了消除这些限制，他们&lt;a href=&quot;https://careersatdoordash.com/blog/experimentation-at-doordash-with-a-multi-armed-bandit-platform/&quot;&gt;采用了“多臂老虎机”（MAB）方法来优化实验&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们的实验目标是，最小化因向用户子集提供效果较差的功能变体而造成的机遇成本或遗憾。传统A/B测试依赖于固定的流量分割和预先确定的样本大小，并且在整个实验过程中保持不变。这样做的结果是，即使早期出现了明显的优胜版本，实验也会继续进行，直到达到预定的停止条件。更糟糕的是，随着同时进行的实验增多，机会成本会不断累积，而鼓励团队按顺序开展实验以减少遗憾则会显著减慢迭代速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多臂老虎机方法提供了一种基于性能自适应分配流量的方法，可以在加速学习的同时减少浪费。其基本工作原理是：它反复在多个选项（仅部分属性已知）中做选择，并随着实验进行收集到更多证据时细化这些选项：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;就我们的目的来说，这种策略根据实验期间收集的持续反馈将实验流量分配给表现更好的功能变体。其核心思想是：自动化的多臂老虎机（MAB）代理会不断地从一组动作池（即多个操作选项）中做选择，从而最大化预设的奖励值，同时在后续迭代中通过用户反馈不断学习优化策略。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种策略实现了探索（即了解所有候选选项）和利用（即优先考虑最佳表现选项）之间的平衡，直到实验收敛到最佳选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照Huang和Weinstein的说法，MAB有助于降低实验成本，方便快速评估许多不同的想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash的MAB方法核心是汤普森采样，这是一种贝叶斯算法，以其卓越的性能和对延迟反馈的鲁棒性而闻名。简而言之，该算法通过从后验奖励分布（即决策周期结束后）采样来决定资源分配，并在新数据涌入时更新奖励预期以准备下个决策周期。在每个决策周期中，预期奖励将被用于确定选项分配方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash工程师表示，采用MAB方法并非没有挑战。特别是，对奖励函数中未包含的指标进行推断变得更加困难，而这反过来又在鼓励团队选择更复杂的奖励指标，以便捕捉尽可能多的洞察。相比之下，传统A/B测试允许在实验结束后对任何指标进行事后分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，由于MAB会更积极地调整分配，所以它可能会导致用户在多次与同一功能进行交互时产生不一致的用户体验。DoorDash计划通过采用上下文老虎机、利用贝叶斯优化和实施粘性用户分配来解决这些限制，增强整体用户体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多臂老虎机的概念来自概率论和机器学习。它使用老虎机的类比描述了这样一个问题：一个赌徒面对多个老虎机，必须决定玩哪个，多久玩一次，以什么顺序玩，以及何时尝试另一台机器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/multi-armed-bandits-doordash/&quot;&gt;https://www.infoq.com/news/2026/01/multi-armed-bandits-doordash/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/X7i8hMBrPd7hKvgkwKaC</link><guid isPermaLink="false">https://www.infoq.cn/article/X7i8hMBrPd7hKvgkwKaC</guid><pubDate>Fri, 30 Jan 2026 03:06:22 GMT</pubDate><author>Sergio De Simone</author><category>可观测</category></item><item><title>Ramp构建的内部编码代理支撑着30%的工程拉取请求</title><description>&lt;p&gt;Ramp&lt;a href=&quot;https://builders.ramp.com/post/why-we-built-our-background-agent&quot;&gt;分享&lt;/a&gt;&quot;了Inspect的架构。在公司前后端存储库的合并拉取请求中，这个内部编码代理的采用率迅速达到了约30%。这家金融科技公司分享了一份详细的技术规范，解释他们如何创建了一个系统，使AI代理能够像人类工程师一样访问开发环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Inspect的真正创新之处在于，它让编码代理能够完全访问Ramp的所有工程工具。与只让代理编写基本代码不同，Ramp的系统在Modal的沙盒虚拟机中运行，能与数据库、CI/CD管道、监控工具（如Sentry和Datadog）、功能标志以及Slack和GitHub等沟通平台无缝衔接。该代理可以编写代码，并通过工程师每天使用的测试和验证流程来确保代码可以正常运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp的工程团队表示，这种验证循环标志着对旧式代码生成工具的重大变革。该代理可以运行测试，检查监控仪表板，查询数据库进行验证，并参与代码审查。这有助于弥补影响许多AI编码助手的验证不足问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp选择基于Modal的基础设施进行构建，这对Inspect的性能特征至关重要。该平台能近乎即时地启动会话，并支持无限并发会话。这使得多名工程师可以同时使用独立的代理实例而不会导致资源争用。Modal的沙箱隔离功能与文件系统快照机制确保了代码执行的安全性，并且支持快速迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该架构使用Cloudflare Durable Objects进行状态管理。在交互过程中，这可以保持对话上下文和开发会话状态稳定。这种有状态的设计有助于代理跟踪它们的工作，就像人类工程师在开发时记住代码库一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp实现了多个客户端接口，使Inspect能够从不同的工作流程中访问。工程师可以使用许多工具与代理互动：用于快速聊天的Slack机器人，用于详细任务的Web界面，以及用于编辑可视化React组件的Chrome扩展。这种多模态方法表明，不同的任务需要不同的交互模式才能发挥最佳的效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该系统支持团队成员之间协同工作。他们能够同时观察并引导智能代理的操作。这一功能解决了人们对自主编程工具的普遍担忧。它既能确保有效的人工监督，又能让团队受益于自动化带来的效率提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp明确主张构建而不是购买现成的编码代理解决方案。其工程团队相信，与商业产品相比，自主工具可以实现更强大的集成。这主要是因为内部工具可以与外部供应商无法触及的专有系统、数据库和工作流程深度连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司承认，这种方法需要大量的工程投资。Ramp分享了详细的实施规范，希望能够给他人带来一些启发。其中包括执行环境、代理集成模式、状态管理和客户端实现的细节。这显示了Ramp的自信，即竞争优势来自于执行，而不是隐藏架构细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最引人注目的也许是Inspect的部署，Ramp并没有强制推广。合并拉取请求占比30%是工程师自主选择采用代理的结果。他们发现，它在质量、速度或便利性方面均可媲美手动编码。持续增长的趋势表明，人们越来越了解该系统的能力与局限性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其团队还指出，Inspect简化了代码贡献工作。它为非工程师提供了与专业开发人员相同的工具访问权限。也就是说，该代理可能允许产品经理、设计师和其他人直接添加代码。这可能会改变跨职能团队的合作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp的工程团队知道，会话速度和质量仍然主要取决于模型的智能程度。即使有最好的工具和设置，编码代理也会受限于当今的语言模型。这些模型仍然会犯错误，会产生幻觉，难以进行复杂的推理，并且需要人类监督。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司知道，他们的构建而非购买的建议可能不适用于每个组织。要实现类似的系统，需要强大的AI基础设施技能和工程资源。规模比较小的团队或有些组织可能没有那么多资源，或者认为不值得投入那么多资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着编码代理的不断演进，Ramp的技术规范和采用指标提供了清晰的数据支撑。这有助于企业评估自身的自动化战略。研究表明，在具备适当环境、工具和验证机制的前提下，AI编码代理能够大规模地提升工程生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/ramp-coding-agent-platform/&quot;&gt;https://www.infoq.com/news/2026/01/ramp-coding-agent-platform/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/gf8Vj1s4sAA0svzVYke3</link><guid isPermaLink="false">https://www.infoq.cn/article/gf8Vj1s4sAA0svzVYke3</guid><pubDate>Fri, 30 Jan 2026 03:00:49 GMT</pubDate><author>作者：Claudio Masolo</author><category>软件工程</category></item><item><title>谷歌通用商务协议（UCP）赋能智能代理购物</title><description>&lt;p&gt;谷歌推出了&lt;a href=&quot;https://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/&quot;&gt;通用商务协议&lt;/a&gt;&quot;（UCP），这是一个旨在提升AI驱动平台商业体验的开源标准。UCP为智能购物提供了一种通用语言，可以实现消费者、企业与支付服务商之间的无缝交互。该协议既能与现有的零售基础设施集成，又能通过智能支付协议（&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol&quot;&gt;AP2&lt;/a&gt;&quot;）保障支付安全，并通过API及Agent-to-Agent通信（&lt;a href=&quot;https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/&quot;&gt;A2A&lt;/a&gt;&quot;）为企业提供了灵活的连接方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP是与Shopify、Etsy、Wayfair、Target和沃尔玛等主要的行业参与者合作开发的，并得到了全球20多个合作伙伴的支持。该协议旨在服务于整个商业生态系统：商家可以控制自己的产品和结账流程，AI平台可以快速上线商家，开发者可以基于中立的开放标准进行构建，支付提供商获得互操作性，消费者享受流畅的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该协议解决了当今商业基础设施面临的一个主要挑战：N乘以N的集成问题。传统系统需要为每个平台或销售渠道建立单独的连接，这可能会减慢智能代理购物体验的推广速度。UCP提供了一个安全层，标准化了从产品发现到结账和订单管理的整个商业工作流程。它允许代理动态发现商家能力和可用的支付选项，支持多种通信方法（包括API、Agent2Agent和MCP），而且，为了与广泛的支付提供商合作，实现了支付工具与处理程序的分离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，UCP允许智能代理通过标准化请求访问商家的服务、启动结账流程并应用折扣。谷歌的参考实现通过搜索功能的AI模式和Gemini等AI界面展示了这一功能，支持使用谷歌钱包或其他兼容的支付方式完成购买。企业可通过商家中心账户访问库存信息，不用单独集成每个平台即可让智能代理访问他们的产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上，首席创新官兼AI大使Andy Reid提出了一个&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7417105773681614848?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418624068054052864%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287418624068054052864%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&quot;&gt;问题&lt;/a&gt;&quot;，探讨这对小品牌的潜在影响：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果UCP允许Gemini将整个购物旅程压缩成一个“支付”按钮，这是否加速了向“默认经济”的转变？在这种经济模式下，每一项交易只有一个品牌作为最终答案出现。如果协议倾向于最相关“默认选项”而不是提供多种选择的市场环境，小品牌该如何生存？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌AI负责人James Massey&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7417105773681614848?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418624068054052864%29&amp;amp;replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418679761016619008%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287418624068054052864%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&amp;amp;dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287418679761016619008%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&quot;&gt;回应&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andy Reid，这个角度很有趣。我认为，尽管“支付”按钮简化了最后一步，但UCP是作为一个开放标准而不是封闭市场来设计的。在我看来，这实际上可能对小品牌更有利。使用UCP让自己成为AI代理的“可发现”对象，可以帮他们节省传统搜索所需的巨额广告预算，如果他们的产品是最相关的，无论品牌大小，协议都允许Gemini将它们作为主要选项呈现。实际上，这和数据质量有关。不过，我认为需要过段时间才能看到结果！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP的开放架构使开发者能够探究付款、折扣和订单管理等功能，使用基于Python的SDK进行快速实现。谷歌及其合作伙伴旨在通过共享的代理商务标准简化集成并增强消费者和商家的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通用商务协议已作为开源规范发布于GitHub平台。该项目鼓励人们通过提交拉取请求和参与讨论来为社区做贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-ucp/&quot;&gt;https://www.infoq.com/news/2026/01/google-ucp/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/48dPcqMKE6lcxtp9OLH6</link><guid isPermaLink="false">https://www.infoq.cn/article/48dPcqMKE6lcxtp9OLH6</guid><pubDate>Fri, 30 Jan 2026 02:58:14 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>云计算</category></item><item><title>Rust 重写代码格式化器，Oxfmt 宣称比 Prettier 快 30 倍，前端工具链要“统一口径”了？</title><description>&lt;p&gt;&lt;a href=&quot;https://voidzero.dev/&quot;&gt;VoidZero&lt;/a&gt;&quot;&amp;nbsp;近日宣布推出&amp;nbsp;&lt;a href=&quot;https://voidzero.dev/posts/announcing-oxfmt-alpha&quot;&gt;Oxfmt&lt;/a&gt;&quot;&amp;nbsp;的 Alpha 版本。这是一款基于&amp;nbsp;&lt;a href=&quot;https://rust-lang.org/&quot;&gt;Rust&lt;/a&gt;&quot;&amp;nbsp;实现的代码格式化工具，面向 JavaScript 与 TypeScript 项目，目标是在大幅提升性能的同时，保持与 Prettier 输出结果的高度一致。作为 VoidZero 更大规模 Oxc 工具链计划的一部分，Oxfmt 在官方测试中展现出比 Prettier 快 30 倍以上的格式化速度，同时与&amp;nbsp;&lt;a href=&quot;https://prettier.io/&quot;&gt;Prettier&lt;/a&gt;&quot;&amp;nbsp;的兼容度超过 95%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Oxfmt 试图解决 JavaScript 生态中一个长期存在的矛盾：性能与习惯之间的冲突。一方面，Rust 工具链在性能上优势明显；另一方面，Prettier 已成为事实上的格式化标准。Oxfmt 将两者结合，既利用 Rust 带来的性能提升，又严格对齐 Prettier 的格式化风格，使其可以作为 Prettier 的“即插即用”替代方案，开发者迁移时几乎不需要承受格式差异带来的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Oxfmt 的开发动机，部分来自 VoidZero 在 2025 年初发布&amp;nbsp;&lt;a href=&quot;https://oxc.rs/docs/guide/usage/linter.html&quot;&gt;Oxlint&lt;/a&gt;&quot;&amp;nbsp;之后收到的大量用户反馈。根据官方公告，用户反复提出对“样式类能力”的需求，例如 import 排序。VoidZero 团队对此采取了明确的工具边界划分：Lint 工具负责逻辑问题，Formatter 只关注代码风格。通过同时提供 Oxlint 与 Oxfmt，团队希望减少配置复杂度，并避免在多个工具之间反复关闭重叠规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在性能方面，官方&lt;a href=&quot;https://github.com/oxc-project/bench-formatter&quot;&gt;基准测试&lt;/a&gt;&quot;显示：在无缓存的首次运行中，Oxfmt 的速度约为 Biome 的 3 倍、Prettier 的 30 倍。Oxfmt 构建在 Oxc 编译器栈之上，刻意规避了现有格式化工具中常见的架构瓶颈，因此在大型代码库和 CI 场景下表现尤为突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 Prettier 迁移到 Oxfmt 对多数项目来说相当简单。开发者只需将现有的 .prettierrc 配置文件重命名，即可直接使用 Oxfmt。当前版本已支持包括 singleQuote、printWidth 在内的多项主流 Prettier 配置，&lt;a href=&quot;https://oxc.rs/docs/guide/usage/formatter&quot;&gt;完整列表&lt;/a&gt;&quot;可在官方文档中查阅。虽然 Oxfmt 目前通过了约 95% 的 Prettier JavaScript 和 TypeScript 测试用例，但 VoidZero 也在持续向 Prettier 提交 Bug 报告和 Pull Request，以进一步缩小两者之间的差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者 Ryan Leichty 在 X（原 Twitter）上回应作者&lt;a href=&quot;https://x.com/ryanleichty/status/1979971666127032701&quot;&gt;相关帖子&lt;/a&gt;&quot;时表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们已经切到 oxlint 了，oxfmt 真的等不及了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参数状态管理工具&amp;nbsp;&lt;a href=&quot;https://x.com/nuqs47ng&quot;&gt;nuqs&lt;/a&gt;&quot;&amp;nbsp;的官方账号，则在评论 Oxfmt 新增 Tailwind CSS 支持时写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对 Biome 来说，直接被秒。很期待用 oxfmt 替换 Prettier（顺便也可能把 oxlint 一起试了）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上，也有用户围绕 Oxfmt 与 Biome 的性能差异&lt;a href=&quot;https://www.reddit.com/r/javascript/comments/1pbid6b/first_alpha_of_oxfmt_the_rustbased/&quot;&gt;提出疑问&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;不错啊，但有点好奇，他们是怎么做到比同样是 Rust 的 Biome 快这么多的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，有人回应称，关键区别在于两者的架构设计：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;架构完全不一样，而且对性能这件事是真的“较真到偏执”。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从更广泛的工具生态来看，Oxfmt 与&amp;nbsp;&lt;a href=&quot;https://biomejs.dev/&quot;&gt;Biome&lt;/a&gt;&quot;、Prettier 一同构成了 JavaScript 和 TypeScript 领域的主要格式化工具选择。Prettier 仍然是采用最广泛的事实标准；Biome 则通过将 lint 与 format 合并到单一工具中逐渐获得关注。Oxfmt 的差异化路径在于：在保持 Prettier 兼容性的前提下，提供超越两者的性能表现。与 Biome 类似，Oxfmt 也构建在 biome_formatter 的一个分支之上，VoidZero 在公告中特别致谢了 Biome 与 Rome 团队的基础性贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望即将到来的 Beta 版本，VoidZero 正在推进多项实验性能力的稳定化工作，包括内置 import 排序、CSS-in-JS 的嵌入式语言格式化等功能。同时，团队也在研究为 Vue、Svelte、Astro 等主流框架提供插件支持。开发者可以通过项目的&amp;nbsp;&lt;a href=&quot;https://github.com/oxc-project/oxc/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;&quot;&amp;nbsp;提交问题和反馈，或加入官方&amp;nbsp;&lt;a href=&quot;https://discord.gg/9uXCAwqQZW&quot;&gt;Discord 社区&lt;/a&gt;&quot;参与讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/oxfmt-rust-prettier/&quot;&gt;https://www.infoq.com/news/2026/01/oxfmt-rust-prettier/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sDC0oXfPeGcqk1CokzsU</link><guid isPermaLink="false">https://www.infoq.cn/article/sDC0oXfPeGcqk1CokzsU</guid><pubDate>Fri, 30 Jan 2026 02:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>一项 20 年前的 Oracle 排序算法专利到期，开源数据库集体受益</title><description>&lt;p&gt;近日，一篇文章披露，Oracle 公司一项关于高速排序方法的&lt;a href=&quot;https://smalldatum.blogspot.com/2026/01/common-prefix-skipping-adaptive-sort.html&quot;&gt;专利已经到期&lt;/a&gt;&quot;，这意味着开源数据库可以自由使用这一算法。该排序算法的发明者&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/mdcallag&quot;&gt;Mark Callaghan&lt;/a&gt;&quot;&amp;nbsp;指出，这种诞生于 20 年前的技术，能够显著加速对相似数据的排序过程，有望让数据库系统在性能和效率上实现进一步提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这项编号为&amp;nbsp;&lt;a href=&quot;https://patents.google.com/patent/US7680791B2&quot;&gt;US7680791B2&lt;/a&gt;&quot;&amp;nbsp;的专利于 2010 年授予 Oracle Corporation，描述了一种利用“公共前缀字节”进行数据排序的方法。Callaghan 建议将这一排序算法称为 “Orasort”。该方法的核心目标，是解决排序过程中反复比较相似键值前缀所带来的效率问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，该算法融合了多种技术手段，包括：在比较键值时跳过公共前缀、在快速排序（quicksort）与基数排序（radix sort）之间自适应切换、缓存键值子串以减少缓存未命中，以及在排序尚未完全结束时提前输出部分结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于排序过程中数据会被拆分为更小的分组，组内键值往往共享更长的前缀。该算法会记录这些共享部分，在比较时直接跳过它们；在合适的情况下切换到更高效的排序方式；并预先加载下一步所需的字节，从而减少无效计算、提升整体性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Callaghan 曾先后任职于 Oracle、Google 和 Facebook，是资深数据库专家。他回顾了这一专利的诞生过程，并解释了其当下重新受到关注的原因：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我是在 Oracle 工作期间发明了这个算法，它最终被集成进 10gR2 版本中，官方宣称相比 Oracle 之前使用的排序算法，性能提升约 5 倍。我一直希望有一天能看到它的开源实现。这项专利对算法的描述非常清晰，比大多数专利都更容易阅读。值得庆幸的是，负责知识产权的律师充分利用了我当时撰写的功能和设计文档。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一消息迅速引发社区关注，开发者开始讨论如何将该算法引入并优化 MySQL、PostgreSQL 等数据库系统。Flooid.in 的 CTO、ScaleArc 前创始人 Varun Singh 表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;细节完整到这种程度，你甚至可以把它和专利文档一起丢进一个 AI agent 里，直接开始实现。Mark 太厉害了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一条讨论中，Google 的数据库工程师 Hannu Krosing 尝试借助 Gemini，分别使用 Python、C 和 C++ 对该算法进行了实现。文章指出，Oracle 内存排序算法在当年实现了约 5 倍于旧方案的性能提升，甚至因此收到了 Oracle 创始人 Larry Ellison 的致谢邮件。Callaghan 回忆道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当我把它集成进 Oracle DBMS 后，就能直接与旧排序算法对比。新算法通常快了大约 5 倍。后来我又把它和 SyncSort 做了比较。我不记得他们是否有 DeWitt Clause（限制公开对比结果的条款），所以不便透露具体数据，但可以说，Oracle 的新排序算法在对比中表现非常出色。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，Charles Thayer 评论道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我以前从没认真考虑过，一个排序算法在什么时候可以输出第一个结果，以尽早开始响应流、降低延迟。（快速排序在这方面应该相对有优势。）这项工作很有意思。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截至目前，Oracle 共持有超过 52,000 项专利，其中仍包含大量与数据库技术相关的专利，例如自管理数据库架构、数据库性能优化方法等，涵盖自动调优、高效数据存储等数据库管理的多个关键领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/oracle-patent-sorting-databases/&quot;&gt;https://www.infoq.com/news/2026/01/oracle-patent-sorting-databases/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cMYZm5ujJag5Y2SPxTG2</link><guid isPermaLink="false">https://www.infoq.cn/article/cMYZm5ujJag5Y2SPxTG2</guid><pubDate>Fri, 30 Jan 2026 00:00:00 GMT</pubDate><author>Renato Losio</author><category>数据库</category></item><item><title>世界模型混战，蚂蚁炸出开源牌</title><description>&lt;p&gt;作者｜陈姚戈&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;世界模型领域迎来了一个重要开源模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，蚂蚁集团旗下的具身智能公司“蚂蚁灵波”，正式发布并开源其通用世界模型LingBot-World。与许多闭源方案不同，蚂蚁灵波选择全面开源代码和模型权重，而且不绑定任何特定硬件或平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;去年DeepMind发布的Genie&amp;nbsp;3，让人们看到了世界模型能够根据文本或图像提示，实时生成一个可探索的动态虚拟世界。LingBot-World沿袭了这条路线，并在交互能力、高动态稳定性、长时序连贯性以及物理一致性等维度取得了突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更令人惊喜的是，LingBot-World呈现出从“生成”到“模拟”的跨越。随着模型规模的扩大，灵波团队观察到，LingBot-World开始表现出远超普通视频生成的复杂行为，涌现出对空间关系、时间连续性和物理规律的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以看到，鸭子腿部蹬水的动作、水面对扰动的响应、以及鸭子身体与水之间的相互作用都比较符合物理规律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这显示出模型不仅记住了视觉表象，还在某种程度上理解了流体力学等基础物理机制。同时，水面对扰动的反应，显示出模型对因果关系的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户切换视角后再回来时，环境中的智能体（比如这只猫）仍能保持持久记忆。智能体即使没有被观察到，也能持续行动。这确保了当视角回归时，世界状态会自然推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当环境中智能体（这只猫）碰到沙发后，没有穿透沙发，反而向空地走去。可以看到，LingBot-World遵循了空间的逻辑，让智能体运动具有物理的合理性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一个长达9分20秒的视频，没有经过任何剪辑和拼贴。视频为用户第一视角，从一座破旧的古希腊神庙出发，沿城市小径前行，经过一座新古典主义建筑，再向左进入一片复原的古希腊建筑群。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近十分钟内，画面保持了较为稳定的物理状态和视觉质量，这在目前的视频生成模型和世界模型中都比较罕见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，在视频最后几分钟，建筑之间的位置关系似乎被模型遗忘了。在7:00，新古典主义建筑和复原式古希腊建筑群是连接在一起的；但7:31，从复原式古希腊建筑群望向新古典主义建筑时，新古典主义建筑消失了。8:30回到新古典主义建筑时，它成为了一栋孤立的房子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管存在这些细节瑕疵，LingBot-World&amp;nbsp;的进步依然显著——单次生成接近10分钟的连贯视频，很可能刷新了当前视频/世界模型的长度纪录。作为对比，Veo&amp;nbsp;3&amp;nbsp;和&amp;nbsp;Sora&amp;nbsp;2&amp;nbsp;的单次生成上限分别为8秒和25秒，Runway&amp;nbsp;Gen-3&amp;nbsp;Alpha&amp;nbsp;为40秒，Kling&amp;nbsp;最长支持2分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与其他交互世界模型相比，LingBot-World在开源、提供720p分辨率的情况下，还保证了高动态程度和长生成跨度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/28/282a90e7af1f10973a9978f4f3ff0c6b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在VBench测试中，LingBot-World全面领先于Yume-1.5和HY&amp;nbsp;World-1.5等先进开源模型，证明了自己不仅是一个视频生成器，更是一个强大的交互式模拟器。通过接收用户输入的动作指令，它能够生成高度动态且物理一致的视觉反馈，保持在高动态度下的整体一致性，使视频内容在长时间段内始终与最初的提示保持一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a4d518073a166cd7b25a615d22b7c76.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在看到大语言模型的局限后，世界模型成为火热赛道。Google、李飞飞、Yann&amp;nbsp;LeCun以及众多科学家纷纷指出，LLM无法很好地理解物理世界、因果关系，而“世界模型”是AI走向真实物理世界深度理解的一个解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于“世界模型”究竟该长什么样，行业至今尚无统一标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞飞的Marble正专注理解空间关系；英伟达把世界模型细分为预测模型、风格迁移模型、推理模型；DeepMind团队的Genie&amp;nbsp;3，则试图在同一个模型中，实现端到端的实时渲染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;路线的分歧，也反应了行业需求的多样性，以及寻找解决方案的困难——无论是智能驾驶、具身智能，还是游戏，都在寻找各自需要的智能方案，以及合适的开发范式和入口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁灵波的世界模型方案更接近Genie&amp;nbsp;3，旨在成为一个通用模型，为Agent、具身智能、游戏、仿真等领域提供理解世界物理规律的基础设施平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过开源其训练方法、模型权重等内容，蚂蚁灵波不仅展示了其在具身智能领域的战略布局，也为行业提供了探索世界模型更多可能性的契机，帮助降低验证世界模型的门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一周，蚂蚁灵波对外集中发布和开源模型研究成果，相继发布并开源空间感知模型LingBot-Depth、具身大模型LingBot-VLA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今，随着LingBot-World的发布，蚂蚁灵波正从幕后走向台前。蚂蚁灵波的目标是打造一个开放、通用的智能基座，与越来越多行业和厂商共建生态。这一次，它用开源的方式，向世界抛出了自己的世界模型范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;构建世界模型的梦想和努力&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在深入探讨蚂蚁团队通用世界模型的细节之前，我们需要花点时间，回顾一下1990年世界模型的开始。这将帮助我们更清楚地理解过去30多年中“世界模型”研究的变与不变、当前世界模型技术路线之争的焦点，从而更好地理解蚂蚁是在怎样的方向和基础上努力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界模型40年，变与不变&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1990年，强化学习领域奠基人、2024图灵奖获得者Richard&amp;nbsp;S.&amp;nbsp;Sutton&amp;nbsp;在人类认知学习过程的启发下，在论文《Dyna,&amp;nbsp;an&amp;nbsp;Integrated&amp;nbsp;Architecture&amp;nbsp;for&amp;nbsp;Learning,&amp;nbsp;Planning,&amp;nbsp;and&amp;nbsp;Reacting》中提出了一个开创性架构：智能体不应只靠真实世界试错学习，而应构建一个内部世界模型，在“脑海”中模拟动作后果，低成本地进行规划与策略优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a807cbddb837a572be625ae5eb02f6bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图片来自Dyna论文。&lt;/p&gt;&lt;p&gt;图片呈现的是Dyna框架的核心逻辑，智能体的目标是最大化其在时间维度上累积获得的总奖励。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;Dyna&amp;nbsp;框架中，世界模型也被称为动作模型，它被视为一个“黑盒子”，输入当前的情境和动作，输出对下一个情境和即时奖励的预测。模型的作用是模拟现实世界，Agent&amp;nbsp;通过与现实世界的持续互动产生经验，并利用这些经验通过监督学习方法来改进模型，使其更接近真实的物理规律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2026年回顾这篇36年前的论文，会发现这份古早的研究为理解当下复杂的技术路线之争提供了共同的根基——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对世界模型的探究，起源于对人类、机器，以及更广泛的智能体如何学习和行动的好奇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而“世界模型”作为一种方法，提出的解决方案是在模拟出的世界中，让智能体学习、行动、获得反馈和迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dyna这篇论文的核心理念，成为了今天世界模型的研究的底层思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不管是NVIDIA&amp;nbsp;Cosmos、World&amp;nbsp;labs、Google&amp;nbsp;Genie，还是LingBot-World，都沿袭了Dyna的核心理念：世界模型是为智能体提供“模拟经验”的内部环境，使得智能体可以在一个虚拟的环境中进行规划和策略训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在不同方向的探索中，我们可以得到的共识是：世界模型从多样化的输入数据中学习对真实世界环境的内部表征，包括物理规律、空间动态和因果关系等。这些表征帮助模型预测未来状态，模拟动作序列，并支持复杂的规划与决策，而不需要反复进行真实世界的实验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;36&amp;nbsp;年过去，我们正站在大语言模型的阴影和语境中讨论世界模型。LLM在理解真实物理世界、及模拟/预测未来后果等方面的局限，正加速科研和商业领域对世界模型的探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年的一次访谈中，Dyna的创作者&amp;nbsp;Richard&amp;nbsp;S.&amp;nbsp;Sutton强调，LLM已经走到了瓶颈。他指出，LLM的核心缺陷在于，它们仅仅是在模仿人类行为，而无法理解世界、预测现实世界中的未来事件。他提倡放弃基于LLM的路径，转而开发基于强化学习、拥有世界转换模型（Transition&amp;nbsp;model&amp;nbsp;of&amp;nbsp;the&amp;nbsp;world）。这种世界模型不仅能学习奖励，还能从所有感官信息中获取环境的丰富理解，最终能够预测“如果做某事，后果将是什么”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大语言模型在理解真实物理世界的不足，以及模拟/预测未来后果的不足，让一批科学家转向，在世界模型中寻找解法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞飞认为&amp;nbsp;LLM&amp;nbsp;缺乏对物理世界的感知，提出“空间智能”（Spatial&amp;nbsp;Intelligence）是&amp;nbsp;AI&amp;nbsp;的下一个北极星，AI&amp;nbsp;需要理解三维空间、几何、物理规则以及因果关系，才能从“理解文本”迈向“理解并作用于物理世界”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Yann&amp;nbsp;LeCun则批评&amp;nbsp;LLM&amp;nbsp;依赖文本概率预测，感知学习世界的方式背道而驰。为此，他推广&amp;nbsp;JEPA（联合嵌入预测架构），并成立&amp;nbsp;AMI&amp;nbsp;Labs，通过世界模型的路径实现AGI，探索如何让AI&amp;nbsp;系统具备理解物理世界、持久记忆、逻辑推理以及复杂任务规划能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DeepMind联合创始人兼CEO&amp;nbsp;Demis&amp;nbsp;Hassabis&amp;nbsp;在今年1月的对谈节目中强调，目前的&amp;nbsp;AI&amp;nbsp;系统还不能理解物理世界、因果关系、行为如何影响结果，而精确的世界模型是实现科学发现或理论创新的关键。他表示，Genie这样的模型还只是“胚胎期世界模型”，Genie体现出的，生成关于世界的内容的能力，某种程度上体现了模型理解了世界的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google&amp;nbsp;AI团队深度押注了世界模型的发展，并认为它会在2026年赢得重大发展。Hassabis在谈及2026年的突破和期待时提到，“最令我兴奋的，莫过于进一步推动‘世界模型’的发展，提升其运行效率，从而使其能够真正被用于我们通用模型中的‘规划’环节。”这可能意味着，未来世界模型将融入Gemini这样的基础模型中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界模型的路线分歧&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在探索AGI的道路时，蚂蚁集团也看到了世界模型的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为蚂蚁集团旗下的具身智能企业，蚂蚁灵波的定位是“智能基座公司”，致力于打造一个能够理解世界、物理规律以及时空演化的AI系统。而世界模型正是实现这一目标的重要方式之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管各方都将世界模型视为未来的关键技术，然而不同公司选择的路径却各不相同。总体上，这些路径可以分为生成式和非生成式两类，两种路径的核心区别在于预测空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;NVIDIA&amp;nbsp;Cosmos、DeepMind&amp;nbsp;Genie和World&amp;nbsp;Labs都是生成式路径的代表。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cosmos和Genie主要使用由像素构成的观测空间，利用大规模高维视觉数据训练，通过特定的时空架构设计，让模型产生对三维物理世界的理解。Genie&amp;nbsp;3官网中特别提到“Genie&amp;nbsp;3&amp;nbsp;的一致性是一种涌现能力……Genie&amp;nbsp;3&amp;nbsp;生成的世界更为动态和丰富，因为它们是基于世界描述和用户动作逐帧创建的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;World&amp;nbsp;Labs则另辟蹊径，将预测空间设定为在3D空间中带有位姿的帧，通过查询待生成帧的位姿来生成新图像。其发布的RTFM模型表明：“模型对世界的记忆（存储在各个帧中）具备了空间结构；它将带有位姿信息的帧视作一种‘空间存储’，这赋予了模型一种弱先验——即所建模的世界是三维欧几里得空间，而无需强迫模型显式预测该世界中的物体几何结构。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非生成路径的代表是Yann&amp;nbsp;LeCun的联合嵌入预测架构（Joint&amp;nbsp;Embedding&amp;nbsp;Predictive&amp;nbsp;Architecture,&amp;nbsp;JEPA）。JEPA&amp;nbsp;通过编码器将输入转化为潜空间（Latent&amp;nbsp;Space），并在该空间内预测未来抽象表征（Embeddings），从而无需进行像素级的重建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁灵波的LingBot-World&amp;nbsp;选择了类似Genie的路径，试图在此基础上解决从视频生成到世界模拟之间的技术障碍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;拆解&amp;nbsp;LingBot-World&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在前文的案例和分析中，我们看到蚂蚁灵波的&amp;nbsp;LingBot-World沿袭了Gienie的生成式路线，同时在交互能力、高动态稳定性、长时序连贯性以及物理一致性上表现惊艳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，蚂蚁灵波选择开源代码和模型权重，并在论文中完整披露了从数据采集到训练部署的全链路设计，鼓励社区测试、使用和复现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即使是在近10&amp;nbsp;分钟的超长视频中、或是快速运动下，画面中的物体依然保持了较为稳定的几何物理特性，没有出现视频生成模型常见的崩坏。这种稳定性，源于其独特的数据引擎和模型架构设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据引擎&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;许多从视频生成模型切入世界模型研发的团队，很快会撞到数据瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;互联网上浩如烟海的短视频大多是“被动”记录，缺乏因果链条。对于世界模型而言，它需要理解的是动作和后果之间的关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如：“按下&amp;nbsp;W&amp;nbsp;键向前走，门是否会打开？”“绕到建筑背面，窗户是否依然存在？”这类智能体动作与环境反馈之间的因果闭环，在普通视频中几乎不存在，在真实世界中规模化采集的成本也很高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了构建“动作-反馈”的闭环，LingBot-World&amp;nbsp;打造了从采集、处理到标注的流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World的数据包含通用视频、游戏数据和合成渲染数据，以确保训练语料的丰富性、高质量和交互性。为游戏数据，灵波团队还开发了专门的平台，捕获&amp;nbsp;RGB&amp;nbsp;帧并严格对齐用户的输入和相机参数。合成数据由&amp;nbsp;Unreal&amp;nbsp;Engine&amp;nbsp;生成，带有精确相机数据和自定义轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97fcf2abdd59e576054679e2bc9125e4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;数据处理和标注流程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据处理层面，灵波团队首先对原始视频进行质量筛选与切分，生成结构清晰的视频片段；然后借助VLM视频的视觉质量、场景类型和视角等，结合几何标注提供必要的3D结构先验，产出元数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，团队引入三种不同粒度的描述标注，涵盖视频全过程的宏观描述、去除了动作和相机数据的静态描写，以及带有时间标注的描述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模型构建和训练&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;将世界模型定义为一个条件生成过程，模拟由智能体动作驱动的视觉状态演化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从模型构建和训练过程，我们可以看到，LingBot-World&amp;nbsp;是从“视频生成模型”起步，通过不同阶段训练，让模型从“生成”走向“模拟”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从目标函数上看，这种模拟本质上是一种概率预测。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;的目标函数明确表达了这一思想：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即在最大化给定历史帧&amp;nbsp;()&amp;nbsp;和动作序列&amp;nbsp;()&amp;nbsp;的条件下，预测下一帧状态&amp;nbsp;()&amp;nbsp;的似然概率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，就是让模型学会根据过去看到的画面和执行过的动作，尽可能准确地预测下一帧画面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了避免直接从零训练导致的计算开销和模式崩塌，LingBot-World采取了分阶段的训练策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预训练负责建立稳健的通用视频先验，确保高保真开放域生成；中训练注入世界知识和动作可控性，使模型能够模拟具有一致交互逻辑的长期坚持动态；后训练使架构适应实时交互，采用因果注意力和少步蒸馏以实现低延迟和严格因果性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/06/06c308590e1819dc8efa59b49ac3b639.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;模型训练流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从“生成视频”到“模拟世界”，LingBot-World带来的可能性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;的意义绝不仅在于生成一段精美的视频，而在于它提供了一个高保真的物理交互沙盒，成为具身智能、自动驾驶与虚拟现实等下游任务的通用基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;最直观的突破在于它赋予了通过自然语言控制模拟过程。例如，通过输入“冬季”或“夜晚”，模型会渲染出城堡结冰或夜晚灯光变化的物理效果，同时支持向“像素风”或“蒸汽朋克”等风格的切换。还可以在具体场景中精确注入特定物体。例如，在城堡上空触发烟花，或在喷泉中生成鱼和鸟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在环境中生成烟花效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;改变环境整体风格&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在自动驾驶训练中，这种能力极具价值。算法团队可以人为制造“鬼探头”、极端天气或突发交通冲突，构建出严苛的因果推理环境，从而低成本地解决智驾中的长尾问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;深层物理特性的稳定性，则为这种模拟提供了实际应用的底座。得益于模型展现的长程记忆，生成的视频序列具备了较高的&amp;nbsp;3D&amp;nbsp;一致性，这使得视觉信息可以直接转化为场景点云，从而服务于&amp;nbsp;3D&amp;nbsp;重建或高精度仿真任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World具有很好的3D一致性。可以看到，视角变化的情况下，房间结构和物理性状仍然保持稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种稳定性试图触及具身智能训练中的一个核心痛点：机器人的导航或复杂操作往往涉及跨越长时序的决策序列。LingBot-World&amp;nbsp;展现的&amp;nbsp;10&amp;nbsp;分钟级别生成能力，在理论上为多步骤任务提供了更稳定的物理一致性。如果这种长程模拟能有效控制累积误差，将有助于机器人在虚拟环境中进行高频次、深度、低成本试错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，LingBot-World&amp;nbsp;与&amp;nbsp;LingBot-VLA（视觉-语言-动作模型）的结合，勾勒出了一种具身大脑的闭环方案。在这种设定下，世界模型充当了机器人的“内部模拟器”：在&amp;nbsp;VLA&amp;nbsp;模型输出最终指令前，系统可以在虚拟空间中先行演练不同的动作轨迹，评估其物理后果，从而筛选出更符合物理规律且具备安全性的执行路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;令人惊喜的是，利用训练LingBot-World的数据，蚂蚁灵波团队还微调出了动作智能体。智能体可以被置于&amp;nbsp;LingBot-World&amp;nbsp;打造的环境中，Agent&amp;nbsp;的动作改变会实时重塑环境状态，而环境的演变则反过来决定Agent的下一步决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;灵波团队利用LingBot-World相同数据训练处的自主智能体，能在生成的世界中自主规划并执行动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种互动揭示了世界模型在“模拟沙盒”之外的另一种可能——它不仅能理解环境对智能体变化的响应，也具备预测智能体动作流的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，世界模型未来或许不仅仅是训练智能体的工具，也有可能成为驱动智能体（包括机器人）的底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目官网：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://technology.robbyant.com/lingbot-world&quot;&gt;https://technology.robbyant.com/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;论文连接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2601.20540&quot;&gt;https://arxiv.org/abs/2601.20540&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码和模型权重下载:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/robbyant/lingbot-world&quot;&gt;https://github.com/robbyant/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/robbyant/lingbot-world&quot;&gt;https://huggingface.co/robbyant/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam&quot;&gt;https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hmKcZ2hdjDk3SspgikfV</link><guid isPermaLink="false">https://www.infoq.cn/article/hmKcZ2hdjDk3SspgikfV</guid><pubDate>Thu, 29 Jan 2026 11:59:46 GMT</pubDate><author>陈姚戈</author><category>AI&amp;大模型</category></item><item><title>如何大规模构建、部署和管理智能体</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着大语言模型能力的成熟，围绕 AI 智能体的讨论正在迅速升温。构建一个能够执行任务、调用工具的 Agent，已经不再是少数团队的专属能力。但在这场技术热潮之下，一个更现实的问题逐渐浮出水面：当智能体不再停留在演示环境，而是被放入真实业务系统中运行时，会发生什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CrewAI 创始人兼 CEO Joao Moura 以实践者的视角，在 BUILD 2025 大会上系统梳理了 AI 智能体从概念、原型走向生产环境过程中，所面临的一系列关键问题。这场分享的核心，并不在于“如何快速做出一个 Agent”，而在于如何让 Agent 在复杂系统中长期、稳定地工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“会生成”到“会决策”：重新理解智能体的能力边界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享中，Joao 首先回到一个基础问题：什么才是 AI 智能体真正的能力来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他指出，很多人已经非常熟悉大语言模型在内容生成上的表现，例如生成文本、改写表达、调整语气。这些能力本质上仍然是“输出导向”的，模型根据输入，生成一段结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而智能体的出现，源于另一类能力的被系统性使用：决策能力。当模型不仅要给出答案，还需要在多个选项之间做出判断，并说明为什么选择其中一个时，它开始参与“思考过程”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5e31ab04b331a193c0acde05d979ea6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，当系统为模型提供可调用的工具，并赋予其一个明确目标，模型就不再只是被动响应请求，而是开始围绕目标不断判断下一步行动。这种行动可能包括调用内部系统、获取业务数据、更新状态，甚至触发后续流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体并不是某种全新的技术形态，而是一个围绕目标进行持续决策与行动的系统。理解这一点，是后续讨论生产化问题的前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正的分水岭：为什么原型和生产完全是两回事&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到智能体落地时，Joao 明确指出了一个现实情况：从原型到生产，并不是一次线性升级，而是一道本质不同的门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原型阶段，团队关注的往往是“能不能跑起来”；而进入生产环境后，关注点会迅速转向“能不能持续运行”。这时，模型本身反而不再是唯一变量，系统层面的复杂性开始占据主导。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94f0feee4fcd61857bb572dbc119f48f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体一旦被放入真实业务系统，就意味着它将与 ERP、CRM 等核心系统交互，其行为可能直接影响业务流程。在这种情况下，系统是否稳定、决策是否可控、行为是否可预测，都会变成不可回避的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多阻碍智能体进入生产的因素，并不来自 AI 本身，而是来自工程、架构和系统集成层面的现实约束。这也是为什么不少 Agent 项目停留在 Demo 阶段，却迟迟无法真正上线的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;决策、工具与执行：Agent 在系统中的运行方式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个智能体并不是简单地“调用模型”，而是需要在决策、工具调用和执行之间形成闭环。模型负责判断当前状态下应该采取什么行动，而系统则需要确保这些行动能够被安全、准确地执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体需要调用外部工具时，问题并不止于“能不能连上接口”，而在于调用是否可控、结果是否可追踪、失败是否可恢复。这些因素，都会直接影响智能体是否具备进入生产环境的条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种结构下，Agent 更像是一个被嵌入到系统中的“决策节点”，而不是一个独立存在的智能模块。它的价值，取决于整个系统是否为它提供了稳定的运行土壤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当数量上升：规模化带来的管理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体不再是单点实验，而是开始成批部署时，另一个问题随之出现：如何管理这些 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规模化并不意味着简单复制更多实例。随着智能体数量的增加，部署、运行、监控和管理本身会迅速成为新的复杂系统。如果缺乏系统性的设计，智能体越多，整体风险反而越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到整场分享的核心，Joao 传递出的判断其实非常清晰：AI 智能体的真正价值，并不在于是否足够聪明，而在于是否能够被可靠地使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当讨论从“能不能做”转向“值不值得用”，从原型转向生产，智能体面临的已经不是技术炫技的问题，而是工程与系统成熟度的检验。也正是在这个阶段，智能体才真正开始进入创造长期价值的轨道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</link><guid isPermaLink="false">https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</guid><pubDate>Thu, 29 Jan 2026 10:26:09 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>不要再纠结 LLM 准确率了：从“回答对不对”到“系统是否值得信任”</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每个人都在努力提高大语言模型的精准度。但真正的挑战并非精度，而是上下文理解能力。在 BUILD 2025 大会上，Hex 合作伙伴工程负责人 Armin Efendic 探讨了为什么传统的方法，如评估套件或合成问题集往往不够有效，以及成功的 AI 系统是如何通过随着时间推移逐步积累上下文来构建的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由 Snowflake Cortex 提供支持的 Hex，启用了一个新的对话式分析模型，每次交互都让模型变得更聪明。通过 Hex 的 Notebook Agent 与 Threads 功能，业务用户可直接定义核心问题，而数据团队则将这些问题精炼、审计并转化为持久且值得信赖的工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个模型中，测试用例不再由数据团队闭门设计，而是由业务需求驱动并在数据工作流中自动实施，最终形成一个具有生命力的上下文系统，而非一成不变的提示词或测试集，它能随着组织共同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c3f33c248d2dee34e327d20cde14222a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;准确率不是终点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Armin 开场就把矛头对准了一个常见做法：把业务用户会问的问题合成成一批样例，甚至进一步转成 SQL 查询，然后把这些喂给 LLM，用类似单元测试的方式去衡量它的准确率、稳定性与一致性。他不否认“准确性是顶层关注”，但他强调，把 LLM 当作传统软件组件来做单元测试，本身就是一个不合适的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因在于，当你把业务问题硬转换为一组 SQL，并据此去构建样例与评估集时，你很难覆盖真实业务中不断变化的语义、不断扩张的问题空间，以及不同用户在不同语境下对同一指标的不同问法。更重要的是，即使你做出了一个看似通过率很高的测试集，也依然回答不了企业最在意的那件事：当它在真实环境中生成了一个结论，你如何知道它不是在胡编？你又如何知道它到底做了什么才得到这个结论？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，Armin 把正确性从结果层拉回到系统层：你需要的不是一个靠样例证明自己正确的聊天机器人，而是一套可审计的系统，它能够随着时间变得灵活、可塑，能够让业务用户在使用中不断收敛可回答的问题类型，也能够让系统拥有被“硬化”的路径：哪些能力可以放开，哪些问题必须收紧，哪些定义需要固化，哪些数据应该进入上下文、哪些不应该。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，真正有效的路线是：从一套能运行、能被观察的系统出发，让系统在使用中暴露问题、沉淀模式，再反过来加固上下文。这种思路听起来不如直接做评估来得爽快，但它更接近企业系统的真实生长方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;对话式分析如何变成“可审计的系统”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了把“可审计”讲得具体，Armin 用 Hex 的产品演示展示了对话式分析在真实系统中应该是什么形态。演示从一个非常典型的业务问题开始：假设我是营销经理，我想让系统分析销售机会的“首次触达来源”（first touch source），并做营销归因视角的拆解。这里一个很关键的动作，是他先在系统里配置模型提供方：通过密钥对（key pair）连接到 Snowflake 实例，使用 Snowflake Cortex 内托管的 Claude，并强调这是一个“walled garden”的私有网络环境。这样做的直接意义是：模型驻留在数据所在的环境里，数据可以传递给模型，同时也能让 IT 团队对数据出入边界更放心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入线程后，Hex 并不是立刻吐出一句“结论”，而是在后台进行一系列用户不可见但决定可信度的步骤：它会先围绕可访问的元数据“思考”，查看平台上已有的 Hex 项目、仪表板或资产，判断是否存在可复用的内容；它会拉取来自数据仓库的表描述、列描述等元数据，并强调这些可以自动导入、不需要复杂配置；如果企业已经有 dbt 元数据，也可以进一步带入；随后它形成一个“漏斗式”的收敛过程：从广义元数据到相关表、再到更具体的模型信息与底层数据，最后才开始把 SQL 单元格、可能的 Python 单元格、图表与可视化逐步组织起来，用以回答最初的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也解释了他在演示里专门强调的一个点：这种模式一开始会“慢”，但这是刻意设计的。因为此时系统面对的是生产数据仓库，它需要把大量上下文带进来，需要推理与迭代，而这类深度思考天然会以时间为代价。换来的收益是：它可以生成更细致、接近数据科学家或嵌入式数据分析师水平的分析过程。Armin 也提到，未来会有更偏“快速、短促回答”的迭代版本，可能更多依赖语义模型，而不是每次都在全量上下文里深挖。但在这个阶段，他们优先解决的是“在没有分析师介入的情况下，业务用户也能得到一份扎实的分析报告”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当线程生成结果后，界面里不仅有图表，还能继续做探索：拖拽维度与度量、查看底层表格数据、检查异常、做更深的切片。这时“可信度焦虑”就会自然出现：这么多信息暴露给业务用户，我怎么知道它没有幻觉？我该不该信这些 SQL？我如何让它更确定？Armin 的回答不是“相信模型”，而是把系统的底座亮出来：在 Hex 里，每一个线程、每一个项目，背后都由笔记本支撑。把线程保存为项目后，你可以在笔记本里看到完整对话以 Markdown 的形式呈现；更重要的是，你能看到它实际运行的 SQL、过滤条件、连接逻辑、图表生成过程，以及它如何一步步构建出整份报告。对于负责准确性与治理的数据团队来说，这种“把对话落到可审计的笔记本”非常关键——它让系统从一开始就具备被审核、被追责、被修正的可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Armin 进一步展示了一个更现实的协作场景：业务用户提出问题后，不一定要立刻去找数据团队提工单，而是先在对话线程里得到初步洞察；如果需要更深入的分析（比如进一步做季节性拆解），技术用户可以把笔记本智能体（notebook agent）限定在这个项目范围内，和智能体一起继续规划、推理、生成图表，并在生成的“待处理变更”中逐条审核、决定保留哪些结果。分析由此变成一种可协作、可迭代、可沉淀的工作流，而不是一次性、不可解释的问答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从一次性对话到可复用资产&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果到这里为止，Hex 展示的是“可观察性”，那么 Armin 在后半段想讲的，是上下文如何变成系统能力，如何从一次性对话沉淀为可复用资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他先展示了一个从笔记本走向应用（app builder）的路径：当某些分析内容需要“持久化”，例如营销与销售负责人希望随时看到季节性分析或关键指标，而不是每次回来重新提问，那么就可以把笔记本中已经生成的图表、文本等资产拖拽到应用构建器里，做成一个仪表板、报告或更像 BI 的交互界面。这里的核心并不是“又做了一个 BI”，而是强调：即便呈现形态变成 BI 风格，背后依然由笔记本驱动，仍然保留 SQL、Python、Snowpark 等灵活性；同时，笔记本与应用这两种范式始终连接，资产是可回溯的。换句话说，展示层可以更友好，但底层逻辑并不会因此变成不可审计的黑箱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;紧接着他抛出了“连接胶水”的问题：当我们有线程、有笔记本、有应用，如何让它们构成一个一致的策略？答案是语义模型——它是 Armin 所谓“上下文引擎”的关键组成部分。原因也很务实：企业里那些精心构建的报表与仪表板，通常包含大量转化逻辑、业务口径、SQL/Python 查询，这些恰恰是 LLM 最需要、也最容易误解的上下文。如果不能把这些上下文结构化，LLM 的确定性就无从谈起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示里，语义模型有两条路：一是导入已有的 Snowflake semantic view。Hex 可以浏览生产仓库、发现可访问的语义视图，然后快速引入，例如引入一个 B2B sales model，让 enriched metadata 直接在 Hex 中可用。另一条路更贴近多数团队的起点：不是先有语义视图，而是先有一堆被业务反复使用的仪表板项目。Hex 的语义建模工作台里有一个“建模智能体”（modeling agent），它能理解 Hex 的语义建模能力，并且能针对某个具体项目（例如 sales and marketing dashboard）去阅读项目里包含的 SQL 单元格、DataFrame 操作、joins、函数与过滤条件，形成建模计划，做错误预防，推断表关系，把“项目里已经存在的业务逻辑”烘焙进语义模型中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一段其实回答了一个关键的企业问题：语义模型从哪来？它不一定需要从零凭空设计，它可以从企业已经在用的分析资产中被抽取、被规范、被版本化。建好之后，语义模型还能用一种“拖拽式”的方式被检查：你可以选择维度、度量，查看聚合、查看系统生成的 SQL，在发布之前把模型硬化到你满意的程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更进一步，Armin 也回应了“供应商锁定”的担忧。他明确表示，Hex 不希望用专有 YAML 把用户锁死，并提到两个方向：其一是和 Snowflake 等一起推动“开放语义交换”（Open Semantic Interchange），一个由约 18 家甚至更多公司组成的联盟，目标是让语义模型信息能在不同系统之间互换，以促进 LLM 采用并避免 vendor lock-in；其二是更近期开启“写回”能力，让在 Hex 中构建的语义模型可以写回到 semantic views 中，保证不同系统间“友好共存”。这些内容在分享里出现得很明确：终点不是锁定格式，而是让用户愿意因为体验与工作流而持续使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当语义模型准备好后，线程侧的使用方式也随之变化：你可以把对话线程限定为“只使用语义模型”，而不是访问整个生产数据仓库。Armin 强调，这会让系统随着时间更确定：当你不断硬化语义模型、补充上下文，它会越来越稳定、越来越可控。也正因此，他再次回到开场的观点：把精力放在构建上下文系统上，而不是试图用合成样例把原型聊天机器人测到“看起来准确”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规模化审计与上下文飞轮&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享的最后一部分，Armin 把问题推到最现实的规模化挑战上：当系统从一个人试用扩展到五十、一百个用户时，你如何监控它？你如何知道 LLM 系统到底在做什么，业务用户到底拿它解决什么问题？这时，“可审计”就不能停留在某个线程或某个项目，而必须成为一套能覆盖全局的治理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他提到 Hex 的“上下文工作室”（context studio），目前处于少数 Alpha 合作伙伴的 Alpha 阶段，但他之所以专门强调它，是因为它承载了上下文系统最关键的一环：理解使用行为，反过来指导上下文如何演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，你可以看到平台总体使用情况：用户更常用笔记本还是线程？创建了多少语义模型？也可以按对话量看用户分布，查看某个用户使用线程的频次、提问的类型。更重要的是，当你下钻到“问题类型”时，Armin 给出了一个很强的判断：这些真实问题才是你的单元测试。不是你在上线前试图一次性“破坏一切”并用评估集兜住，而是看清业务用户到底在问什么，再回去硬化你需要硬化的上下文与问题类型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕“如何策划上下文”，他在分享里给出了三个层次的抓手。最直接的是规则文件（rules file）：你可以在里面定义 SQL 的数据质量防护、业务定义、偏好的 SQL 风格、杂项信息，以及希望系统使用的可视化方式，并且这些内容可以即时编辑、保存或导出。第二层是“经认可的数据”（endorsed data）：由数据团队或所谓“金层”背书的数据资产，可以在 Hex 的语境下被定义清楚，决定哪些数据可以喂给 LLM。第三层则是更成熟、也最关键的做法：语义项目（semantic projects）。随着审计能力增强，你不仅能看到语义模型被使用的次数，还能观察是否有多个语义模型被同时使用、是否需要在某些场景中合并；你也能判断哪些项目最常被引用，从而决定是否需要对下游数据做更多建模，或者是否需要补充列描述、表描述等元数据来改善上下文质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些细节共同指向同一个结论：上下文不是一次性设计出来的，它是被真实使用不断“磨”出来的。你从稍微宽的范围起步，抽取一两个语义模型，让业务用户用起来；再通过审计看到真实问题与真实路径，回去修规则、补语义、加背书数据、完善元数据。如此循环，系统才会越来越确定、越来越可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享最有价值的地方，在于它没有把“可信”简化为一个指标，也没有把“准确率”当作唯一的归宿。Armin 反复强调的其实是另一套思维：企业要的不是一个在评估集上表现漂亮的聊天机器人，而是一套能持续吸收上下文、可审计、可治理的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从线程到笔记本的可观察性，从笔记本到应用的资产化，从项目到语义模型的上下文结构化，再到面向规模化使用的审计与上下文工作室——这些环节被串成一个整体，目的只有一个：让 LLM 在真实业务里变得更确定，并且在需求增长时仍然能保持可控与可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</link><guid isPermaLink="false">https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</guid><pubDate>Thu, 29 Jan 2026 09:02:56 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>Aspire 13.1带来了MCP集成、CLI增强和Azure部署更新</title><description>&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;作为增量更新发布，它基于Aspire 13引入的多语言平台基础。此次发布专注于通过增强命令行界面、更深入地支持AI辅助开发工作流程、改进仪表板体验以及更清晰的Azure环境部署行为来提高开发者的生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据团队报告，此次更新旨在使日常开发任务更可预测、更易于自动化，并与现代AI编码工具更好地对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aspire 13.1中的一个核心新增功能是通过与模型上下文协议（&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-mcp-for-ai-coding-agents&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;，MCP）集成，扩展了对AI编码智能体的支持。一个新的命令允许项目在初始化时支持MCP，使兼容的AI工具能够发现Aspire集成、检查应用程序结构并与运行中的资源交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;aspire mcp init&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接后，AI智能体可以查询应用程序状态、查看日志并通过暴露的端点检查跟踪。这种集成旨在简化开发过程中AI助手的使用，而无需为每个工具进行自定义设置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-cli-enhancements&quot;&gt;Aspire CLI&lt;/a&gt;&quot;进行了几次更新，旨在减少创建、运行和维护项目时的摩擦。如前所述，项目创建命令现在可以选择通道，并且一旦选择，将全局保持，确保新项目的行为一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CLI还能检测到已经运行的实例，并在启动新运行之前自动停止它们，从而避免常见的冲突。安装脚本现在支持一个选项来跳过修改系统 PATH，这在受控环境中非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布的仪&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-dashboard-improvements&quot;&gt;表板更新&lt;/a&gt;&quot;专注于清晰度和可见性。新的参数标签允许直接从资源详情中查看和管理配置值。GenAI可视化器已增强，以更好地显示工具定义、评估和相关日志，并支持预览音频和视频内容。仪表板的几个稳定性问题也得到了解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e063e5a2395fca657719db177ab3ec1c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（GenAI可视化器工具定义，来源：&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#genai-visualizer-enhancements&quot;&gt;官方Aspire文档&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-azure-improvements&quot;&gt;Azure&lt;/a&gt;&quot;改进方面，Aspire 13.1引入了更清晰的命名和更强大的验证。Azure Redis集成已重命名，以更好地匹配底层服务，并且在部署过程中更早地执行额外检查，以便尽早发现配置问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Azure资源现在暴露出标准化的连接属性，这些属性在支持的语言中通用，使得非.NET应用程序能够使用一致的设置进行连接。还增加了对Azure App Service中部署槽的支持和对默认角色分配的更精细控制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过引入通用容器注册表资源，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-container-and-docker-compose&quot;&gt;容器和部署&lt;/a&gt;&quot;工作流得到了改进，允许开发者锁定Azure容器注册表之外的注册表。容器镜像推送现在更加明确和可预测，特别是在部署到Azure容器应用时。Docker Compose支持已得到改进，以增强可移植性并减少并行构建期间的竞争条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还包括针对&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-javascript-and-frontend-support&quot;&gt;JavaScript和前端开发的更新&lt;/a&gt;&quot;，例如一个新的起始模板，该模板结合了ASP.NET Core后端和基于Vite的前端，改进了开发中的HTTPS处理，并修复了与包管理器相关的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;证书处理得到了简化，新增了配置HTTPS和在支持的容器中终止TLS的新API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Aspire 13.1还稳定了之前预览版中的几个集成，包括Dev Tunnels、端点代理支持和Azure Functions。模板已更新以反映一致的模式，并且广泛的错误修复集提高了跨平台的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;需要.NET 10 SDK或更高版本。建议从早期版本升级的开发者查看已记录的重大变更，特别是围绕Azure Redis API和重命名的连接属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于感兴趣的读者，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;完整的发布说明&lt;/a&gt;&quot;和详细文档可在&lt;a href=&quot;https://github.com/dotnet/aspire&quot;&gt;官方Aspire存储库&lt;/a&gt;&quot;和文档渠道中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&quot;&gt;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</link><guid isPermaLink="false">https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</guid><pubDate>Thu, 29 Jan 2026 09:00:00 GMT</pubDate><author>作者：Almir Vuk</author><category>微软</category><category>云计算</category></item><item><title>LangChain 创始人警告：2026 成为“Agent 工程”分水岭，传统软件公司的生存考验开始了</title><description>&lt;p&gt;过去几十年，软件工程有一个稳定不变的前提：系统的行为写在代码里。工程师读代码，就能推断系统在大多数场景下会怎么运行；测试、调试、上线，也都围绕“确定性”展开。但 Agent 的出现正在动摇这个前提：在 Agent 应用里，决定行为的不再只是代码，还有模型本身——一个在代码之外运行、带着非确定性的黑箱。你无法只靠读代码理解它，只能让它跑起来、看它在真实输入下做了什么，才知道系统“到底在干什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在播客中，LangChain 创始人 Harrison Chase 还把最近一波“能连续跑起来”的编程 Agent、Deep Research 等现象视为拐点，并判断这类“长任务 Agent”的落地会在 2025 年末到 2026 年进一步加速。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也把问题推到了台前：2026 被很多人视为“长任务 Agent 元年”，现有的软件公司还能不能熬过去？就像当年从 on-prem 走向云，并不是所有软件公司都成功转型一样，工程范式一旦变化，就会重新筛选参与者。长任务 Agent 更像“数字员工”——它不是多回合聊天那么简单，而是能在更长时间里持续执行、反复试错、不断自我修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这期与红杉资本的对话中，Harrison 抛出了一个判断：构建 Agent，已经不只是把软件开发“加一层 AI”，而是工程范式本身在变。为什么他说“光读代码不够了”？为什么 tracing、评估、记忆这些原本偏“辅助”的东西，突然变成主角？他在对话里给出了非常具体的解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而更现实的问题是：如果范式真的在变，那些靠数据、流程、产品形态建立壁垒的传统软件公司，优势还能不能延续？它们手里握着的数据与 API 可能依然是王牌，但能否把这些资产变成 Agent 时代的生产力，取决于一套全新的工程打法。Harrison 的观察与判断，都在下面的完整对话里：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：AI 领域的变化速度快得惊人。当前最受关注的话题，我觉得没有人比你更合适来聊。我们会先谈 长任务 Agent（Long Horizon Agents） 和 Agent Harness（智能体运行框架）。&lt;/p&gt;&lt;p&gt;接着，我们会讨论：构建长任务 Agent 与构建传统软件到底有什么不同，以及你如何看待 LangChain 在整个生态系统中的角色。最后，我想和你聊聊未来。你怎么看红杉资本这篇关于Long Horizon Agents的文章？哪些观点你认同，哪些地方你不太同意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/acbe71eff021a48fa8459ecebfaa8b62.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“在去年的一篇文章中，我们曾提出：推理模型（reasoning models）是 AI 领域最重要的新前沿。而“长任务 Agent”（long-horizon agents）则在这一范式之上更进一步——它们不只是思考，还能够采取行动，并在时间维度上不断迭代。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://sequoiacap.com/article/2026-this-is-agi/&quot;&gt;https://sequoiacap.com/article/2026-this-is-agi/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你们这个概念命名得非常好，那篇文章也写得很棒。我整体上是认同的——长任务 Agent 终于开始真正“跑起来”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始对 Agent 的设想，本来就是让一个 LLM 运行在一个循环里，自主决定接下来该做什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AutoGPT 本质上就是这个想法，这也是它当初能迅速走红、抓住那么多人想象力的原因：一个 LLM 在循环中运行，完全自主地决定行动。但当时的问题在于：模型还不够好，围绕模型的 scaffolding（支架）和 harness（框架）也不够成熟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这几年，模型本身变得更强了；与此同时，我们也逐渐搞清楚了，什么样的 harness 才是“好”的。于是现在，这套东西开始真正奏效了。最明显的例子是在编程领域，Agent 的突破首先发生在那里。之后，这种能力正在向其他领域扩散。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，你仍然需要告诉 Agent 你想让它做什么，它也需要配备合适的工具。但现在，它确实可以持续运行更长的时间，而且表现越来越稳定。所以，用“长时序”来描述这一类 Agent，我觉得非常贴切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最喜欢的长任务 Agent 案例有哪些？你觉得它们正在呈现出哪些形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：目前最成熟、我自己用得最多的，还是 编程 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再往外延一点，我觉得非常优秀的一类是 AI SRE。比如 Traversal（我记得它是一家红杉投资的公司），他们的 AI SRE 可以在更长的时间跨度内运行。再往抽象一点，其实这类 AI SRE 本质上属于“研究型 Agent”。比如：给它一个事故，它会去翻日志、分析上下文、追溯原因。研究任务本身非常适合 Agent，因为它们最终产出的往往是一个“初稿”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent 的问题在于：它们还达不到 99% 的可靠性，但它们可以在较长时间内完成大量工作。所以，只要你能把任务框定为：让 Agent 长时间运行，产出一个初步版本，由人来审阅，这在我看来就是目前长任务 Agent 最“杀手级”的应用形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程就是一个例子：你通常是提交 PR，而不是直接推到生产环境（当然，vibe coding 现在也在不断进步）。AI SRE 也是一样：结果会交给人来 review。报告生成也是如此：你不会直接发给所有用户，而是先看一遍、改一改。我们在金融领域也看到了大量这样的用法，这是一个非常大的研究机会。客服领域同样如此。最早的客服 Agent 主要是做“第一响应”：用户一发消息，马上给出回复，这类用法现在也做得很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在开始出现新的形态，比如 Klarna&amp;nbsp;这个产品：人类和 AI 协同工作。当第一层自动回复失败后，不是简单地转交给人工，而是让一个长任务 Agent 在后台运行，生成一份完整的事件报告，然后再交给人工客服处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里“agent”这个词在客服语境下会变得有点混乱，但核心逻辑是一致的。总结来说，这些应用的共同点是：先由 Agent 生成一个“初稿”，再由人类接管。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那么，“为什么是现在”？你觉得主要是因为模型本身变得足够强，还是因为人们在 harness 侧做了非常聪明的工程设计？在回答这个问题之前，能不能先帮听众梳理一下：在一个 Agent 系统中，模型、框架和 harness 各自扮演什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：当然可以。我也顺便把“框架”这个概念一起带进来。一开始，我们把 LangChain 描述为一个 Agent Framework，现在我们又推出了 Deep Agents，我更愿意称它为一个 Agent Harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人都会问，这两者有什么区别。模型很简单，就是 LLM：输入 token、输出 token。框架（Framework） 是围绕模型的一层抽象，让你更容易切换模型，封装工具、向量数据库、记忆等组件，本身比较“无偏好”，强调灵活性，更像是基础设施。Harness 则更“有主张”。以 Deep Agents 为例：我们默认就提供一个 规划工具（Planning Tool）；这个工具是直接内建在 harness 里的，带有明确的设计立场：我们认为这是“正确”的做法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还做了 上下文压缩（Compaction）。长任务 Agent 会运行很久，哪怕上下文窗口已经很大，也终究是有限的，总会有需要压缩的时候。怎么压缩？压缩什么？这是一个正在被大量研究的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，几乎所有 Agent Harness 都会提供文件系统交互能力，不管是直接操作，还是通过 bash。这一点其实很难和模型本身完全分开，因为模型训练数据里已经大量包含了这类操作。&lt;/p&gt;&lt;p&gt;如果回到两年前，我不确定我们是否能预见到：基于文件系统的 harness 会成为最优解之一。那时模型还没被充分训练过这些模式，而现在模型和 harness 是在一起“共同进化”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结来说，这是一个组合效应：模型本身确实在变强，推理模型带来了巨大提升。同时，我们也逐渐摸索出了 compaction、planning、文件系统工具等一整套关键原语。这两者缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;设计范式的演进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得在我们第一次对谈时，你把 LangGraph 描述为 Agent 的“认知架构”。现在来看，这是不是也可以理解为 harness 的一种形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的，这个理解是对的。我们现在的 Deep Agents 是构建在 LangGraph 之上的。可以把它看作是一个非常具体、非常有主张的 LangGraph 实例，更偏向通用目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期我们讨论过“通用架构”和“专用架构”的区别。现在我们观察到一个很有意思的变化：过去需要写进架构里的任务特异性，正在转移到工具和指令里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;复杂性并没有消失，只是从结构化代码，转移到了自然语言中。因此，prompt 的设计、修改，甚至自动更新，正在成为系统的一部分；而 harness 本身，反而变得更加稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在你看来，harness 工程中最难做对的是什么？你觉得单个公司是否真的有可能在这一层形成显著优势？有没有你特别佩服的团队？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：说实话，目前在 harness 工程上做得最好的，基本都是编程类公司。Claude Code 就是一个非常典型的例子。我认为它能如此受欢迎，很大程度上是因为它的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是否意味着：harness 更适合由模型公司来做，而不是第三方创业公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我不确定。比如 Factory、AMP 这些编程公司，也都做出了非常强的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实存在一个现实：harness 往往和模型家族绑定得比较紧密。不一定是某一个具体模型，而是一整个模型体系。Anthropic 的模型会针对某些工具进行微调，OpenAI 则针对另外一些。这和 prompt 类似：不同模型，需要不同的 prompt；同样，不同模型家族，也需要稍微不同的 harness。当然，它们也有很多共性，比如几乎都会使用文件系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我自己也没有一个确定答案。但一个很明显的现象是：几乎所有做编程 Agent 的公司，现在都在自研自己的 harness。你去看 Terminal Bench 2 这样的榜单，会发现他们不仅展示模型，还展示 harness。Claude Code 并不总是在榜首。这说明：性能差异并不完全来自模型，而来自对“模型如何在 harness 中工作”的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得，排行榜上表现最好的 harness，究竟在哪些地方做得特别好？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：首先是对模型训练偏好的理解。比如 OpenAI 的模型对 Bash 非常熟悉；Anthropic 提供了显式的文件编辑工具。顺着模型的“母语”来设计 harness，本身就能带来性能收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是 上下文压缩（Compaction）。随着任务时间跨度变长，如何处理上下文窗口溢出，已经成为一个核心问题。这显然也是 harness 的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还有 skills、子 Agent、MCP 等机制。目前这些能力还没有被系统性地训练进模型中，仍然属于比较新的探索方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们的 harness 中，一个典型挑战是：主 Agent 如何与子 Agent 高效通信。主模型需要把所有必要信息传递给子 Agent，同时还要明确告诉它：最终只需要返回一个“最终结果”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们见过一些失败案例：子 Agent 做了大量工作，最后却返回一句“请查看我上面的分析”，而主 Agent 根本看不到那些内容，于是完全不知道它在说什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，如何通过 prompt 设计让这些组件协同工作，是 harness 工程中非常重要的一部分。&lt;/p&gt;&lt;p&gt;如果你去看一些公开的 harness prompt，它们往往有几百行之长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我想从演进角度问一个问题。你一直站在模型“如何落地”的最前沿。如果用一种简化视角来看过去五年的几个关键拐点：ChatGPT 带来了预训练的拐点；o1 带来了推理能力的拐点； 最近，Claude Code + Opus 4.5 带来了长任务 Agent 的拐点。但从你这个“围绕模型做设计”的世界来看，拐点会不会是另一套划分？从认知架构到框架、再到 harness，这中间经历了哪些真正的跃迁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我大概会把它分成三个阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一阶段：最早期。那时 LangChain 刚刚出现，模型还是“纯文本输入、纯文本输出”，甚至还不是 chat 模型。没有工具调用，没有 reasoning，没有结构化输出。人们主要做的是单一 prompt 或简单 chain。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二阶段：工具与规划开始进入模型。模型开始支持 tool calling，也尝试学会“思考”和“规划”。虽然还不够强，但已经能做出基本决策。这时，人们大量使用自定义的认知架构，通过显式提问来引导模型行动，但整体仍然依赖大量外部 scaffolding。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三阶段：长任务 Agent 的真正起飞。大概是在今年 6～7 月，我们看到 Claude Code、Deep Research、Manus 等产品同时爆发。它们在底层使用的是同一个核心算法：让 LLM 在循环中运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的突破来自于 上下文工程：压缩、子 Agent、技能、记忆——所有这些，都是围绕上下文展开的。这正是我们开始做 Deep Agents 的时间点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于很多程序员来说，Opus 4.5 可能是一个心理上的分水岭。也可能只是碰巧遇上假期，大家回家开始大量使用 Claude Code，突然意识到：它真的很好用。无论是 2025 年初还是 2025 年末，总之在某个时间点，模型“刚好强到足以支撑这种形态”，于是我们从 scaffolding 迈向了 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent 是通用 AI 的终局形态吗&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：接下来会发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也希望我知道答案。这个“让 LLM 在循环中运行、让它自己决定要拉什么上下文进来”的算法，本身极其简单、也极其通用。这正是 Agent 从一开始的核心设想，而我们现在终于走到了“它真的能工作”的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，可能会有大量围绕上下文工程的技巧出现：有些手动设计的部分可能会消失；比如压缩类的，现在仍然高度依赖 harness 作者的决策。Anthropic 已经在尝试让模型自己决定何时压缩上下文，虽然目前用得还不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个我们非常关注的方向是 记忆（Memory）。从本质上说，记忆也是一种上下文工程，只不过是跨更长时间尺度的上下文。核心算法本身已经非常清晰：运行 LLM 循环。未来的进步，很可能来自更聪明的上下文工程方式，或者让模型自己参与上下文管理。模型当然也会继续变强，越来越擅长长时序任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我目前思考最多的一个问题是：我们看到的大多数 harness 都是高度偏向编程任务的。这是长任务 Agent 最先爆发的领域。但即便是在非编程任务中，你也可以认为：写代码本身是一种非常强的、通用的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我本来想问你：编程智能体（coding agents）到底算不算一个子类别？还是说编程智能体就是智能体本身？换句话说，智能体的工作，本质上是想办法让计算机去做一些有用的事情，而“写代码”本来就是让计算机做有用事情的一种很好的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也不确定。但有一点我非常非常坚信：现阶段只要你在做长时序智能体，你就必须给它文件系统的访问能力。因为文件系统在“上下文管理”方面能做的事情太多了。比如我们说 compaction（上下文压缩），一种策略是把内容总结掉，但把完整的消息都放进文件系统里，这样如果智能体后续需要回查，它还能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一种策略是，当你遇到很大的工具调用结果时，不要把全部内容都塞回模型上下文里；你可以把结果放进文件系统，然后让智能体需要的时候再去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这些操作，其实不一定需要真实的文件系统，也不一定要让它真的去写代码。我们有一个概念叫“虚拟文件系统”：它底层可能只是 Postgres 之类的存储，扩展性更强。当然，“真实代码”能做的事情，虚拟文件系统做不了。比如你没法在虚拟文件系统里直接运行代码。所以写脚本在很多场景下确实非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也认为编程智能体有潜力成为通用智能体，但我不确定这是否意味着“今天的编程智能体”就是通用智能体——如果你能理解我这句话。因为我觉得现在很多编程智能体还是为编程任务做了大量优化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以“一个通用智能体可能长得像编程智能体”，但反过来，“今天的编程智能体就是通用智能体”，这件事我并不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;传统软件面临的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们能不能转到另一个话题：构建长时序智能体和构建传统软件之间的差异？你能不能先描述一下“1.0 时代”的软件开发栈是什么样的，然后说说现在到底哪里不一样？我记得你在 X 上写过一篇很不错的文章，也许你可以总结一下核心结论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e1839d7404649a0013d16b7f7d6b4a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://x.com/hwchase17/status/2010044779225329688&quot;&gt;https://x.com/hwchase17/status/2010044779225329688&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我这段时间一直在反复想这个问题：我们经常说“做智能体和做软件是不同的”，而且很多人也同意。但问题是：到底哪里不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得很容易、也很偷懒地说“不同”，但“具体不同在哪里”才是关键。下面这些可能听起来很显然，但也许显然是好事，希望它们不太有争议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你在做传统软件时，所有逻辑都写在代码里，你能直接在软件代码中看到它。但当你在做智能体时，你的应用如何工作的“逻辑”，并不全部在代码里，其中很大一部分来自模型本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着：你不能只看代码，就判断智能体在某个具体场景下会做什么。你必须真的把它跑起来。而我认为，这就是最大的不同：我们引入了这种非确定性系统，它是一个黑箱，它在代码之外。我觉得这就是核心差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个直接后果是：为了弄清楚应用到底在做什么，你不能看代码，你必须看它在真实运行中做了什么。这也是为什么我们做的产品里，最受欢迎的之一是 LangSmith。LangSmith 的一个核心能力是 tracing（追踪/执行轨迹）。为什么 trace 这么受欢迎？因为它能把智能体每一步内部发生的事情都清清楚楚地展示出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这跟传统软件里的 trace 又不一样。传统软件里，你的系统在那边跑，它会吐出很多日志和事件；你通常是在出现错误时才去看，而且你不需要“每一步的全部细节”。而且本地开发时，你可能直接打个断点就够了；很多时候日志追踪是上线到生产环境后才会更重度开启。但在智能体里，人们从一开始就会用 trace 来理解“底层到底在发生什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它在智能体里的影响力，远大于在单一 LLM 应用里的影响力。因为在单一 LLM 应用里，如果模型回答得不好，你知道你的 prompt 是什么，也知道输入上下文是什么（由代码决定），然后你得到一个输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，它在循环中运行、不断重复。你并不知道第 14 步时上下文里到底有什么，因为前面 13 步可能会把任意东西拉进上下文。所以，“上下文工程（Context Engineering）”真的是一个非常好的词。我真希望这是我发明的。它几乎完美描述了我们在 LangChain 做的一切——只是当时我们并不知道这个术语已经存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;trace 的价值就在于：它能直接告诉你此时此刻上下文里到底有什么，这太重要了。那这又意味着什么？这意味着：对传统软件来说，“真相的来源（source of truth）”在代码里。但对智能体来说，真相来源变成了代码与 trace 的组合——而 trace 是你能看到真相的一部分地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，真相当然也“存在于模型的数百万参数里”，但你基本没法直接对参数做什么。所以现实上，trace 就成了你可以抓住的“事实载体”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，trace 也会成为你开始思考测试的地方。你仍然可以对 harness 的某些部分做单元测试，也可以离线做一些 unit test，但要获得真正的测试用例，你很可能需要用 trace 来构建。而且在智能体里，在线测试（online testing） 可能比传统软件更重要，因为行为不会在离线环境里完整显现出来，只有在真实世界输入驱动下、系统被真正使用时，行为才会“涌现”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也看到 trace 正在成为团队协作的中心：如果出了问题，不再是“去 GitHub 看代码”，而是“去看那条 trace”。我们在开源项目里也一样。有人说：“Deep Agents 这里跑偏了，发生了什么？”我们的第一反应是：“把 LangSmith trace 发给我们。”如果没有 trace，我们基本没法帮你 debug。过去大家会说“把代码给我看看”，但现在已经转变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是我写在 X 上那篇文章的核心内容，反馈很好。我也还在琢磨怎么把它表达得更精确，但我觉得这一点很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外一个点我也还在继续想：我觉得构建智能体是一个更偏迭代式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们过去也会这么说，但我以前会有点翻白眼，因为软件开发本来也是迭代式的：你发布、收反馈、不断迭代，这就是软件开发的常态。但我觉得差别在于：在传统软件里，你的迭代是围绕“你希望软件做什么”来进行的。你有一个想法，你发布，你收反馈。比如“这个按钮让人困惑”，或者“用户其实想做 X 而不是 Y”。但你在发布之前，其实你是知道软件会怎么运行的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，你在发布之前并不知道它到底会怎么做。你当然有一个预期，但你并不能在发布前真正确定它会做什么。因此，为了让它更准确、让它更“对”、让它能通过某种“概念上的单元测试”，你需要更多轮次的迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个基础上，我也认为记忆（memory）非常重要。因为记忆就是在从这些交互中学习。如果你的开发过程变得更迭代、更难，那么作为开发者，我为了让系统表现正确，可能需要反复改系统 prompt——这种频率甚至可能比我改代码还高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是记忆进入的地方：如果系统能够以某种方式自己学习，那就能减少开发者必须进行的迭代次数，让构建这类智能体变得更容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，这是我认为“构建智能体确实不同于构建软件”的另一个角度。我也承认，这么说有点老套，所以我一直在逼自己想清楚“到底不同在哪里”，目前我总结出来的就是这两点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我也很想追问这一点。现在公开市场上有一个很大的争论：现有的软件公司还能不能熬过去？如果类比当年从本地部署软件（on-prem）转向云（cloud），实际上真正成功转型的公司并不多，因为事实证明，“做云软件”和“做本地软件”确实差异很大。你现在处在“人们如何用 AI 构建产品”的核心地带。你怎么看这件事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不是要问公开市场的投资问题，而是想问：这个变化到底有多大？你有没有看到很多人：过去很擅长“旧方法做软件”，现在也能很擅长“新方法做软件”？还是说更像是：你要么在“新方法”里长大，要么就很难真正理解它？你觉得人能跨越这个鸿沟吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我注意到现在有很多年轻创始人，这让我觉得，也许年轻人因为没有太多对“旧软件开发方式”的先入之见，反而可以更快把这些东西学起来、用起来。而且我们确实一再听到一个现象：很多在做 agent engineering 的团队成员，反而是更初级的开发者、更初级的构建者——他们确实没有那些先入之见。我们内部的应用 AI 团队，确实整体更偏年轻一些。我觉得这里面既有“人的因素”，也有“公司的因素”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;先说公司层面：数据依然非常非常非常有价值。如果你从 harness 的角度去看——顺便说一句，我其实不认为长期来看大多数人都会自己去写 harness，因为它比做 framework 难太多了。所以我觉得大家最终会用我们提供的 harness，或者用别人的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那一个 harness 里面有什么？主要就是：prompt、指令，以及它连接的工具。而现有公司在这方面最大的资产之一，是他们已经拥有数据和 API。如果你过去在这块做得不错，那么把这些东西接入到 agent 上，其实会非常容易产生真实价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们前阵子和金融行业的人聊，他们就说：数据的价值只会越来越高、越来越高、越来越高。所以如果你是一个传统软件厂商，你手上有这些高价值数据，你应该能够把它暴露给智能体，让智能体去用，从中拿到很大的收益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过这里还有另一部分：关于“如何使用这些数据”的指令（instructions），这一块可能更偏“新增”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你作为软件厂商也许一直对“怎么用这些数据”有一些想法，但你并没有把这些想法系统化、固化成可执行的“操作说明”，因为过去这件事更多是由人来完成的——很多智能体现在在做的事情，本来就是人类会做的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你当然会给人配工具，但你以前不会、或者也很难成功地把它完全自动化。而到了“智能体”这一代，这部分才真正变得可行。所以我觉得这块是新的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们也看到大量需求来自“垂直领域创业公司”。Rogo 就是一个很好的例子：他们团队有人有金融行业经验，把这种行业知识带进了智能体系统里，而这之所以有效，是因为很多智能体的驱动力来自“知识”——但不是那种通用世界知识，而是如何执行特定流程、特定模式的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以问题就变成：做传统软件的人是不是做智能体的合适人选？我觉得我们确实看到很多非常资深的开发者在采用 agentic coding，所以某种程度上这更像是“心态问题”。但确实也可能会呈现出一种“年轻化倾向”。而公司层面，则很大程度取决于它手上的数据资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以看起来，你认为 trace 是这个新世界里 agent 开发的核心“产物”，LangSmith 在这方面帮助很大。那你觉得还有哪些核心的“产物”——或者说，可能“产物”这个词不对，应该说组件（components）？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，组件。我觉得构建软件与构建智能体之间另一个差异是：评估软件时，你可以相当可靠地依赖程序化测试和断言。但智能体做的很多事情，本质上是“人类会做的事情”。因此要评估它，你必须把人的判断引入进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我们在 LangSmith 里努力解决的问题之一：你已经有了这些 traces，那么你怎么把人类判断带到 traces 上？最直接的方法当然就是：把人引进来。所以我们也看到数据标注类创业公司做得很好。我们在 LangSmith 里有一个概念叫 annotation queues（标注队列），就是把人带进来参与。因此，实际的、真实的人类判断，是其中非常重要的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这里的“人工标注”的trace，比如，智能体做了这些步骤，这是好还是不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：有时候，人会给自然语言反馈：这很好、这很差、这里应该怎么做。有时候，人会直接“纠正它”：把正确步骤完整地写出来。这具体怎么做取决于用例，而且对做 RL 的模型公司，和对做 agent 应用的公司来说，也可能不一样。但核心就是：把人类判断带进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也看到另一条路：尝试为这种人类判断建立一些“代理指标”（proxy）。这就是 LLM-as-a-Judge 这类方法的来源：你可以跑一个 LLM 或其他模型，让它承担某种“类似人类判断”的角色，去给那些本来需要人类判断的东西打分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们一直在思考的一件事是：怎么让“构建 judge”这件事变得容易。因为 judge 的关键很大一部分在于：它必须和你的人的判断、人类偏好保持一致。如果做不到，那你的 grader（评分器）就很糟糕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们在 LangSmith 里做了一个概念叫 align evals：人类先去标注一些 traces，然后基于这些标注，构建一个 LLM judge，使它在这些样本上被校准（calibrated）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为关键就在于：你要把人类判断引入进来；如果你要用 proxy 来替代它，那就必须确保这个 proxy 校准得足够好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有意思。我记得我们最开始和你做业务合作的时候，还在邮件里讨论过：LLM-as-a-Judge 到底是否可行。看起来它已经进步很多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的。LM-as-a-Judge 其实有几个不同层面的用法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最常见的一种，是用于 eval：拿一条 trace，直接给它一个分数，比如 1 到 0，或者 0 到 10。这个我认为是可行的，而且很多人确实在做。他们会离线做，也会在线做，因为有些判断并不需要 ground truth（标准答案）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我觉得另外一个更重要的场景，是你在 coding agents 里也能看到的：coding agent 往往会先工作到某一步，然后遇到错误，触发纠错。它实际上是在“评判自己刚才做的工作”。我们也在 memory 上看到同样的模式：记忆很大一部分就是反思 traces，然后更新某些东西。所以问题是：LLM 能不能去反思 traces——无论是它自己的 trace、以前 session 的 trace，还是别人的 trace？我觉得完全可以。我们在 eval、纠错、记忆里到处都能看到这种模式，本质上其实是一回事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Eval是 RL 的奖励信号，还是工程反馈机制？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我明白了。那接下来就很自然会问：你有了所有 traces，也有了 eval。那么这些 eval 到底是什么？它是强化学习的 reward signal？还是一种反馈机制，让工程师去改进 harness、让 agent 工程师去优化 harness？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：因为现在大家都不再手动写太多代码了，大家都在用这些 agent 工具。我们观察到一个很重要的模式：我们有一个 LangSmith MCP，也有 LangSmith fetch（一个 CLI）。因为 coding agents 特别擅长用 CLI。你把这些给智能体，它就能把 traces 拉下来，诊断哪里出了问题，然后把这些 traces 带进代码库里，从而修复它。这是我们正在看到的真实模式，而且我们非常非常非常想支持这种模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在这一点上，相比“用 eval 做强化学习奖励信号”，我对“把 eval 当作工程反馈、用于改 harness”的路径更乐观——至少对今天做 agent 应用的公司来说是这样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这听起来像是递归自我改进啊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得是，但还是有一个人类在环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到前面那个点：当它产出“初稿”时效果最好——它改 prompt，然后人类 review，这能让系统保持不跑偏。但我们确实……我们最近发布了 LangSmith Agent Builder，这是一个 no-code 的 agent 构建方式。其中一个很酷的功能就是 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在 memory 的工作方式是这样的：当你和 agent 交互时（注意它还不是后台自动跑的那种；它不会自己拉 traces），如果你对它说：“你不该做 X，你应该做 Y”，它就会去改自己的指令——这些指令本质上就是文件——然后直接编辑这些文件。这样未来它就会按新的方式表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是一种“自我改进”的形式。我们确实还想加入另一种机制：比如每天晚上跑一次任务，查看当天所有 traces，更新自己的指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：就是那种“做梦”的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，“睡眠时间算力（sleep-time compute）”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;记忆与自我改进会成为护城河吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们再多聊聊未来。你现在最兴奋的是什么？听起来你聊了很多 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我很看好memory。我觉得让智能体去改善自己，这非常酷，而且在很多场景下也很有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但也不是所有场景都用得上。比如 ChatGPT 加了 memory 功能，我其实用得不多，我也不觉得它显著增加了我对产品的粘性。我觉得原因之一是：我去 ChatGPT 时，大多数问题都是一次性的。我不太会反复做同一件事：我可能问软件，也可能问吃的、旅行……都很零散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 agent builder 里，你通常是为特定任务构建特定工作流。比如我有一个 email agent。而且我其实……它已经给我发邮件两年了。我之前在 agent builder 之外就有一个 email agent，它带有 memory。后来我们做了 agent builder，我想把它迁移进去，但它没有我之前的那些 memories。即便它的起始 prompt 一样、工具也一样，但因为缺了记忆，它现在的体验就明显差很多。我到现在都还没完全切过去，因为它现在确实不如之前那个好用——说白了，它现在“有点烂”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，如果我持续和它交互，它会变好，它会不那么烂。但这也恰恰说明：memory 可能会成为真正的护城河（moat）。而且我绝对相信，我们已经到了一个阶段：LLM 可以看 traces，然后改变自己代码里的某些东西。问题在于：怎么把这件事做得安全、并且在用户层面可接受。但我认为，在一些特定场景里（不是所有场景），我们会越来越多看到这种能力。至于 ChatGPT 这种通用聊天产品，我仍然不确定这种形态的 memory 是否有用，至少目前我不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你觉得和长时序智能体一起工作的 UI 会如何演化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得大概率需要同步模式（sync）和异步模式（async）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长时序智能体运行时间可能很长，默认应该是异步管理：如果它要跑一天，你不会一直坐在那里等它结束。你很可能会启动一个、再启动一个、同时跑很多个。所以这里会涉及到异步管理：我觉得像 Linear、Jira、看板，甚至 email，都可以作为 UI 设计的参考——如何去管理一堆异步运行的 agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但与此同时，很多时候你又会想切换到同步交流。因为 agent 最后给你返回一份研究报告，你可能需要立刻指出：它这里写错了，你要给反馈。聊天界面在这方面其实已经挺不错的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我唯一想补充的是：现在很多 agent 不仅是在“对话”，它还会去修改文件系统里的文件。所以你必须有一种方式去查看“状态”（state）——也就是它改了什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这在编程领域尤其明显：IDE 依然被使用，是因为当你想手动改代码时，你需要看见那个“当前状态”。即便我启动 Claude Code，它跑完后，我有时也会打开来看它到底写了什么代码。所以“能看到状态”这件事很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 在 Claude “co-work”（这里指那类协作式工作流）里做了一个很酷的设计：你设置它时要选择一个目录，等于你在告诉它：“这就是你的环境。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在编程里当然也是常态：你打开 IDE 到某个目录。但我觉得把它明确成一个心智模型很有帮助：这就是你的 workspace（工作区）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个 workspace 也不一定非得是本地目录：它可以是 Google Drive、Notion 页面，或者任何能存储状态的地方。你和 agent 就是在这个状态上协作：你启动它，让多个任务异步跑；然后切到同步模式，在 chat 里和它讨论，但同时你还能看到它正在协作的“状态”。这就是我目前看到的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以这也就是你说的“agent inbox”的想法：为了进入 sync 模式，agent 需要能联系到你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，没错。我们大概一年前发布过 agent inbox，理念是“ambient agents”：它们在后台跑，必要时来 ping 你。但第一版其实没有 sync 模式：它 ping 你，你回一句，然后你就等它下一次再 ping 你。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但很多时候，我切到邮件去回复它时，我其实只回很短的话，而且我不想再切出去然后干等——我（对方）很重要，所以我更想直接进入一种“同步对话”的模式，跟 agent 把这个问题当场聊完。所以我们后来做了一个关键改动：当你打开 inbox 时，会直接进入 chat，而 chat 是非常同步的。这是一个很大的 unlock（突破点）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在认为：只有 async 模式，目前还不太够。也许未来如果 agent 强到你几乎不用纠正它，那么纯异步会更可行。但至少现在，我们看到人们在 async 和 sync 之间来回切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 你怎么看 code sandboxes（代码沙箱）？是不是每个 agent 最终都会配一个 sandbox？也包括“能用电脑”、能上网用浏览器这种能力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase： 这是个特别好的问题，我们也一直在想。就目前的经验来看，“写代码/跑代码”这条路明显比“直接操作浏览器”更成熟、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以短期内，如果要在这些能力里挑一个最可能成为标配的，我更看好的是代码执行（code execution）——也就是给 agent 一个能安全运行脚本、验证结果的环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，文件系统（file system）我几乎是“坚定派”：不管是本地目录、还是背后用数据库实现的“虚拟文件系统”，agent 总得有个地方能存状态、存中间结果、随时回查，这对上下文管理太关键了。比如：&lt;/p&gt;&lt;p&gt;做 compaction（上下文压缩）时，把完整内容丢到文件里，需要再查就去读；工具调用返回特别长时，不塞进上下文，改成写文件、让 agent 自己按需读取。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于“coding”（让 agent 真正去写代码），我没那么绝对，但我大概90% 站在“需要”这一边。因为很多长尾任务里，写脚本依然是最通用、最强的手段——你很难找到同等级的替代品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也可能出现另一类场景：如果你做的是高度重复、流程固定的事情，未必每次都要写很多代码；但即使这样，文件系统仍然重要，因为重复流程会不断产生上下文和状态，你还是要做上下文工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再说浏览器使用（browser use）：从我们目前看到的效果来说，模型还不够稳定。也许可以让 coding agent 通过 CLI 的方式“间接”完成一些浏览器相关任务（算是一种近似解），我确实见过一些挺酷的实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而所谓 computer use（直接操作电脑界面）则更像是介于两者之间的混合形态，目前还有不少不确定性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结一下：我非常喜欢 code sandboxes，我觉得它会成为 agent 能力栈里很关键的一块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太棒了。Harrison，真的非常感谢你今天来参加节目。你一直都能在 agent 这条路上看到未来，能和你聊“上下文工程如何演化到今天的 harness 与长时序智能体”，真的特别过瘾。感谢你推动这个未来，也感谢你一直愿意和我们聊这些。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：谢谢邀请。我希望未来还能再来一次，然后证明我今天说的全部都是错的。因为预测未来真的很难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=vtugjs2chdA&amp;amp;t=1s&lt;/p&gt;</description><link>https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</link><guid isPermaLink="false">https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</guid><pubDate>Thu, 29 Jan 2026 08:18:49 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>Linus 之后的 Linux？内核社区终于写下“接班预案”</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/945c92e1d75cd68d349118286393a8f5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Linus Torvalds 常开玩笑说自己会“活到永远”。但以防万一，Linux 内核社区现在也准备好了一套交接方案——只是这份方案并没有点名具体的接班人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果 Torvalds 发生意外，或者哪天决定退休，Linux 不再把一切寄托在“到时候再说”。核心内核社区已经正式起草了一份项目连续性计划：一旦顶层维护者出现空缺，应该如何在最坏情况或有序过渡中，选出新的顶层维护者（可能是一人，也可能是多人），确保项目长期稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 本人则明确表示自己暂无退休打算。被问到未来是否会交棒时，他依旧以一贯的幽默回应，暗示自己更倾向于“继续干下去”。随后他又补充了一个更现实的理由：家里人同样不希望他突然闲下来，尤其是太太，大概更不想每天被一个无所事事、没事找事的丈夫缠着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这份新的“为计划而写的计划”由资深内核贡献者 Dan Williams 起草，并在最近于东京举行的 Linux Kernel Maintainer Summit 上讨论。Williams 介绍它时还自嘲：这是个“与我们终将走向死亡相关、但很振奋的话题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不指定唯一继承人&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 也解释了这次为何会把“接班”议题正式摆上台面：部分原因是他此前与 Linux 基金会的合同在去年第三季度到期，基金会技术顾问委员会的人都知情。虽然合同随后已续签，但这段时间确实促使大家把风险管理讨论得更具体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;计划并没有给出一个“唯一继承人”。相反，它明确了一套选择流程：一旦需要交接，由社区召集一次类似“秘密会议”的讨论机制，集中权衡候选人或候选团队，尽量做出对项目长期健康最有利的决定。有维护者开玩笑说，干脆学选教皇：把人都锁在房间里，等决定出来再放出一缕白烟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文件提到一个开源圈常说的“公交车系数”（bus factor）梗：假设项目的关键人物哪天突然“消失”（比如出了意外），项目还能不能照常运转？因为 Torvalds 仍是顶层合并与发布的最终把关人，所以从风险角度看，Linux 在这一环节几乎等同于“系数为 1”——也就是关键节点过度依赖一个人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过在现实中，大家也大致心照不宣：真要临时接手，“企鹅之王”的角色多半会落到 Greg Kroah-Hartman 身上——他是 Linux 内核稳定分支的维护者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 还在 2024 年和好友 Dirk Hohndel（Verizon Open Source 负责人）聊过这个话题。Hohndel 认为，要成为 Linux 的主维护者，需要极其丰富的经验；而目前最自然的“备份选项”就是 Greg Kroah-Hartman。Torvalds 的看法则更偏向长期视角：关键不在于某个人，而在于谁能获得社区的信任；这种信任通常来自长期参与、稳定协作，以及社区对其工作方式的充分了解，但“资历够久”并不意味着必须三十年如一日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kroah-Hartman 也确实曾短暂顶上过。2018 年 Torvalds 一度暂离内核工作、反思并改善自己对待其他开发者和维护者的方式时，Kroah-Hartman 曾临时承担顶层职责。不过，他的年龄甚至比 Torvalds 还大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;或许会由多人共同接棒&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此也有人提出，与其再找一位新的“终身仁慈独裁者”（BDFL），不如把顶层维护者的职责拆分给多位值得信赖的开发者共同承担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;56 岁的 Torvalds 仍然是几乎所有进入 torvalds/linux.git 变更的最终裁决者。他常自嘲 Linux 的核心圈子正在“变老”。而维护者疲劳、以及核心子系统负责人后继乏人等问题，让这种紧迫感越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以确定的是：Torvalds 并不会在短期内让位。他仍会继续监督主线开发，并一直做到自己“做不动”为止。只是至少现在，那个终极的“Linus 依赖”风险终于有了明确的处理流程——等到真正需要的那一天，可以直接套用，而不必临时抱佛脚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&quot;&gt;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</link><guid isPermaLink="false">https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</guid><pubDate>Thu, 29 Jan 2026 08:12:34 GMT</pubDate><author>Tina</author><category>开源</category></item><item><title>不跟英伟达走老路，这家GPU公司的技术架构藏着哪些关键解？</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;采访嘉宾 | 天数智芯 AI 与加速计算技术负责人 单天逸&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于国产 GPU 行业来说，没有哪个时间节点比当下更宝贵。在政策支持硬科技企业上市的背景下，国产 GPU 迎来了难得的上市黄金窗口期。但上市并非终点，在敲钟的那一刻，下一战场大幕已经拉开——GPU 厂商的技术路线、产品能力和长期判断，被放到了更公开也更严苛的舞台上，谁能撑起资本市场和大众期待，谁就能撑起市值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么，天数智芯上市后的首场发布会能够在业内形成广泛讨论。它以极其务实的工程师表达方式，把架构放回到国产 GPU 技术叙事的中心。在 1 月 26 日召开的天数智芯“智启芯程”合作伙伴大会中，围绕架构层的创新与思考占据了相当比重。基于这些创新点与思考，天数智芯公布了过去一代以及未来三代的架构路线图：&lt;/p&gt;&lt;p&gt;2025 年，天数天枢架构已经超越英伟达 Hopper，在 DeepSeek V3 场景中实测性能数据超出 20%；2026 年，天数天璇架构对标 Blackwell，新增 ixFP4 精度支持；2026 年，天数天玑架构超越 Blackwell，覆盖全场景 AI/加速计算；2027 年，天数天权架构超越 Rubin，支持更多精度与创新设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/93a5511a47ea59c34947fa5622e43f57.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;国产 GPU，开启 AI++ 计算新范式&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据天数智芯公布的架构路线图及阶段发展目标，在 2027 年之前，天数智芯将通过多代产品完成对英伟达的追赶；在 2027 年之后，将转向更富创新性的架构设计，聚焦更具突破性的超级计算芯片架构设计。看似宏大，但对于仍处于爬坡阶段的国产 GPU 行业来说，这条路径实际上相当务实——只有在工程化能力上完成对标甚至是超越，国产 GPU 才有资格进入更大规模的生产环境中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在规模化落地阶段的竞争，焦点早已从峰值性能指标转向有效计算能力。当 Token 成为 AI 时代最基本的生产资料，当算力消耗开始对标真实业务产出，无论是国际顶尖 GPU 厂商还是国内 GPU 企业，核心命题都只有一个：如何在真实业务中，把算力转化为有效的 Token。这似乎又将大家都拉到同一起跑线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一命题，天数智芯提出了两条明确的架构判断：其一，回归计算本质；其二，提供高质量算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;回归计算本质，核心在于“不设限”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年，规模的快速扩张带来了阶段性的产业繁荣，也使得算力实现野蛮增长。但这种粗放式发展，也带来了能效比失衡、算力资源严重浪费等问题。背后的根因十分复杂。以开车行驶为例，路途中可能会遇到雨雪冰雹天气、崎岖道路等各种复杂情况。物理、芯片、系统世界也是如此，计算、通讯、存储都会带来各种障碍。所以，幻想奔跑在平坦的赛道上毫无意义，产业真正需要的，是能够翻山越岭的全能越野车。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64709d562cae987749119744a750d556.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;广义上，芯片可分为专用芯片和通用芯片：专用芯片类似“应试教育”，它的优势和边界都很清晰，能加速特定算法、特定指令，比如矩阵乘法、Softmax 这些主流任务，但一旦计算范式发生变化，适应空间就会迅速收紧；通用芯片的设计哲学，不是为了押中某一类算法，而是回归计算本质，覆盖更广泛，甚至全新的计算需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是天数智芯坚持推出并量产通用 GPU 的根因。在其看来，硬件与算法的关系本来就不应该相互掣肘，算力的僵化不应限制算法的进化，而是通过通用算力为探索未知算法提供一个坚实的底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑探索未来算法的关键，实则就是“不设限”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，天数智芯的芯片设计哲学，在计算层面追求的是覆盖几乎所有的数学运算图谱，而非某一类、某一种计算：从 Scalar、Vector、Tensor 到 Cube，支持从高精度科学计算到 AI 精度计算，从 MMA 到 DPX，不管是 AI 的 Attention 机制、前沿的科学计算，还是未来的量子计算相关模拟，天数智芯全都支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行层面，追求的是更高的算力利用率：大、中、小任务会被精准分配到不同的计算单元中执行，配合高密度的多任务核心设计，算力可以被拆解、调度得更加精细，从而减少算力浪费，提高计算效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d21f6e06061719cd4a9a199eb6e5fed0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“不设限”的设计哲学，让天数天枢架构得以实现三大创新，这也是天枢能够超越英伟达 Hopper 架构的根因：&lt;/p&gt;&lt;p&gt;TPC BroadCast（计算组广播机制）设计：不是简单粗暴地放大带宽，而是从单位带宽的使用效率入手，存在相同地址的数据时，芯片内部的 load store 单元不会进行重复、无用的访问，而是在上游进行 BroadCast，减少不必要的内存访问次数，从而有效降低访存功耗，等效提升访存带宽，用更小的功耗和面积实现相同的功能。Instruction Co-Exec（多指令并行处理系统）设计：在指令执行层面，通过 Instruction Co-Exec 设计实现了多种指令类型的并行执行能力，不仅支持 Tensor Core 与 Vector Core 的并行协同，还将 Exponent 计算、通信等操作一并纳入统一调度。在天数 IX-Scheduler 模块中，通过极低的成本增强了不同指令之间的并行处理能力，无论是 MLA、Engram，还是面向更复杂模型场景的计算需求，都可以在这一并行框架下被同时处理，从而提升整体执行效率。Dynamic Warp Scheduling（动态线程组调度系统）设计：随着 MoE 架构在大模型中被广泛采用，模型厂商普遍面临推理效率低等现实挑战。为提升并行度，微架构层面允许芯片中同时驻留更多 warp，但 warp 的增加也意味着对计算资源的竞争更为激烈。为此，天数智芯首创了 Dynamic Warp Scheduling 机制，通过动态调度让不同 warp 在资源使用上实现有序协作，避免计算资源闲置，也减少了对同一资源的无序争抢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三项设计的出发点本质上都指向相同的目标：高性能与高效率。数据显示，这些创新让天数天枢的效率较当前行业平均水平提升 60%，基于这些效率优势，实现在DeepSeek V3 场景平均比 Hopper 架构高约 20% 性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这三项设计中可以看出，天数智芯在架构层面的创新，并不是围绕某一个具体模型或算子展开，而是试图打破 GPU 通用范式边界。天数智芯 AI 与加速计算技术负责人单天逸在接受采访时表示，在天数智芯提出 Dynamic Warp Scheduling 设计之前，几乎没有人从调度机制的角度去思考，还能为 MoE 带来哪些性能空间。从更深层次意义来看，这类微架构层面的调度和优化，一直是英伟达、AMD 等巨头保持领先的“内功”，天数智芯在这些单点上的突破，实际上也是国产 GPU 向顶级玩家看齐的重要一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;提供高质量算力：高效率、可预期、可持续&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在天数智芯的架构语境中，回归计算本质并不是一个抽象的口号，而是实现高质量算力的前提条件。只有当 GPU 从底层开始真正对计算负责，高质量算力才成为可能。基于这一判断，天数智芯将高质量算力拆解为三个核心维度：高效率、可预期与可持续。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7ace836faccd159427d7c71b330df3ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高效率意味着能为客户创造最优的 TCO（总体拥有成本），节省使用成本；可预期则通过精准的仿真模拟，让客户在拿到芯片、部署算力之前，就能清晰预判最终的性能表现，做到所见即所得；可持续指的是从现在主流的 CNN、RNN，到当下火热的 Transformer，再到未来还未诞生的全新算法，算力始终能无缝适配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这三个方向，天数智芯在架构及系统设计上，选择从多任务并行处理、长上下文 IX-Attention 模块、IX-SIMU 全栈软件仿真系统以及 IXAI++ 算力系统多个层面同步推进。这几项，其实哪个都值得单独展开探讨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，基于“不设限”的设计理念，在当前 PD 分离的架构下，天数智芯的 GPU 不只做计算，还支撑通信、KV 数据传输这些关键任务，通过打造 Ⅸ 并行任务处理模块，GPU 能精准调度 KV 传输、多路多流、计算与通信等各类任务，让它们并行不冲突。在真实业务场景中，该模块成功帮助头部互联网客户实现了端到端 30% 的性能跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b473b037acf6e72c5b667dd838e26e33.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了提高算力可持续性，天数智芯统一了芯片内、外，来构建算力系统，并通过不断更新的软件栈和软件系统，三类库共同支持和保障多场景的高效运行。其中，AI 库、通讯库（ixccl）、加速计算库是基石，在基石之上，直接支撑各类神经网络模型CNN、Transformer、LSTM 与高性能计算的各个领域，并以此提供各类 AI 应用，包括支持 AI4Sci 的相关应用，如蛋白质结构预测（AlphaFold）、医疗影像分析（Clara）、气候模拟（Earth2）等，以及量子计算的平台 cudaQ、分子动力学 Gromacs，大规模方程组求解器 HPL 等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套算力系统被命名为 IXAI++，寓意为自我迭代，不止于 AI。其最终的目标是，成为一座连接算法创新与物理世界的桥梁，带领人类科技通往未知探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但给业内带来最多惊喜的，是 IX-Attention 模块和 IX-SIMU 全栈软件仿真系统。前者解决的是当前大模型推理中最具代表性的效率难题，后者解决的是企业部署算力系统最头疼的不可控难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型推理场景中，长上下文被普遍认为是最具代表性的效率难题之一。即便是在国际主流 GPU 架构上，Attention 的执行效率依然不高，如果不对其进行针对性优化，首字延迟将明显偏高，模型响应速度差，推理成本高昂，最终影响大模型在真实业务中的可用性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一痛点，天数智芯设计了 Ⅸ Attention 模块，从底层对 Attention 的执行路径进行重构：Attention 底层涉及 exponent、reduce、MMA、atomic 等多类指令与算子，Ⅸ Attention 模块的核心思路，是将这些分散的组件有机地拼装到一起，如同指挥一支乐队一般，确保多种乐器能够和谐共鸣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/1931cf31a98c3a57d61ca5cbaf8caa44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“其中的技术难点在于调度，多种乐器需要同时演奏，任何一个环节拖慢节奏，都会成为整个系统的瓶颈”，单天逸表示，在实际的长上下文推理中，Ⅸ Attention 模块有效改善了 Attention 的执行效率，带来了约 20% 的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对企业部署算力系统最头疼的不可控难题，天数智芯搭建了 IX-SIMU 全栈软件仿真系统，这套仿真系统的目标，就是零意外、可预期。通过对芯片等硬件与软件执行策略的联合仿真，能精准输出任意模型的性能表现，提升算力在真实场景中的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c15d79ed9346cf49898ee0193b585926.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单天逸表示，在算力系统的仿真与评估中，最难建模的是指令级别的硬件行为。IX-SIMU 的核心能力在于，能够对底层指令执行进行精细建模。在实际使用中，用户只需输入软件代码，IX-SIMU 便会自动整合 GPU、CPU、网卡、PCIe 等硬件组件，匹配网络拓扑，再结合软件策略、投机策略、Streaming LLM 策略、前缀匹配等各类策略，最终精准输出 Deepseek、千问等任意模型的性能表现，实现从单卡到万卡集群的 “精密扩展”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕高效率、可预期、可持续三大判断，天数智芯在算力侧从硬件架构到系统设计进行了整体布局，并用未来三代架构路线图提前回答下一个问题：当算力僵化开始掣肘未来计算，架构层还能怎么演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;决定上限的，最终还是应用和生态&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构代表的其实是下限，决定上限的，最终还是应用和生态。数据显示，截至 2025 年年底，天数产品已在互联网、大模型、金融、医疗、教育、交通等超过 20 个行业落地应用，服务客户数量超过 300 家，并通过软硬件协同优化，完成 1000+ 次模型部署，让产品能力真正达到商用级别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑这些场景应用的，早已不是一个产品的能力范畴，而是“产品 + 解决方案” 双轨模式，这一模式其实与英伟达定位非常相近，聚焦的都是解决方案落地。在大模型深入产业应用的当下，这套组合打法相当务实，毕竟应用落地才是唯一真理，谁能在企业真实业务场景中快速部署、持续稳定运行，谁就能赢得先机。在速度和兼容性上，天数智芯也交出了一份不错的答卷：国内新的大模型发布当天便能跑通，目前已稳定运行 400 余种模型、数千个已有算子与 100 余种定制算子，数千卡集群稳定运行超 1000 天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次发布会上，天数智芯面向物理 AI 场景落地，一口气发布了四款边端算力产品“彤央”系列：包括边端 AI 算力模组 TY1000、TY1100，以及边端 AI 算力终端 TY1100_NX、TY1200。 据了解，“彤央”系列产品的标称算力均为实测稠密算力，覆盖 100T 到 300T 范围。数据显示，在计算机视觉、自然语言处理、DeepSeek 32B 大语言模型、具身智能 VLA 模型及世界模型等多个场景的实测中，彤央 TY1000 的性能全面优于英伟达 AGX Orin。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在发布会中，天数智芯展示了“彤央”系列产品在具身智能、工业智能、商业智能和交通智能四大边端核心领域的落地应用：具身智能领域，为格蓝若机器人提供高算力、低延迟的“大脑”支撑；在工业智能领域，落地园区与产线，推动产线自动化升级；在商业智能领域，瑞幸咖啡数千家门店部署彤央方案，高效处理视频流、挖掘消费数据价值；在交通智能领域，与“车路云一体化”20 个头部试点城市合作，验证车路协同方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，天数智芯走的路线虽然是底层技术自研，但在生态上并非封闭。在生态建设上，天数智芯与硬件厂商、解决方案提供商等多家生态伙伴签署战略合作协议，进一步完善国产 AI 算力生态闭环。通过兼容主流开发生态，持续开放底层能力，降低开发者迁移和使用门槛。未来，天数智芯还会持续增加在生态共建上的资本与人力投入，从应用到芯片与开发者一同优化 AI 应用系统，共同为应用落地提供性能、性价比与生态易用的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从底层架构到产品，从应用到生态，国产算力正在实现完整闭环，这种从芯片到生态的协同能力，不仅让国产算力更可用、更可持续，也为行业探索新模式提供了更多想象空间。&lt;/p&gt;</description><link>https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</link><guid isPermaLink="false">https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</guid><pubDate>Thu, 29 Jan 2026 06:54:56 GMT</pubDate><author>凌敏</author><category>企业动态</category><category>芯片&amp;算力</category></item><item><title>智源多模态大模型登Nature，确立自回归成为生成式人工智能统一路线</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月28日，智源多模态大模型成果&quot;Multimodal learning with next-token prediction for large multimodal models（通过预测下一个词元进行多模态学习的多模态大模型）&quot;上线国际顶级学术期刊Nature，预计2月12日纸质版正式刊发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Nature编辑点评这项研究：Emu3 仅基于预测下一个词元（Next-token prediction），实现了大规模文本、图像和视频的统一学习，其在生成与感知任务上的性能可与使用专门路线相当，这一成果对构建可扩展、统一的多模态智能系统具有重要意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/4104e358f0c6b33a3c481f57a18d6760.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41586-025-10041-x&quot;&gt;https://www.nature.com/articles/s41586-025-10041-x&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2018年以来，GPT采用 “预测下一个词元（Next-token prediction，NTP）”的自回归路线，实现了语言大模型重大突破，开启了生成式人工智能浪潮。而多模态模型主要依赖对比学习、扩散模型等专门路线，自回归路线是否可以作为通用路线统一多模态？一直是未解之谜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源这项成果表明，只采用自回归路线，就可以统一多模态学习，训练出优秀的原生多模态大模型，对于确立自回归成为生成式人工智能统一路线具有重大意义。在后续迭代的Emu3.5版本，确实证明了这一范式的可拓展性，并达成预测下一个状态（Next-state prediction)的能力跃迁，获得可泛化的世界建模能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从语言到多模态：“预测下一个词元”的潜力与未解之问&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“预测下一个词元”彻底改变了语言模型，促成了如 ChatGPT等突破性成果，并引发了关于通用人工智能（AGI）早期迹象的讨论。然而，其在多模态学习中的潜力一直不甚明朗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在多模态模型领域，视觉生成长期以来由结构复杂的扩散模型主导，而视觉语言感知则主要由组合式方法引领 ，这些方法通常将CLIP编码器与大语言模型（LLMs）结合。尽管已有一些尝试试图统一生成与感知（如Emu和Chameleon），但这些工作要么简单将LLM与扩散模型拼接在一起，要么在性能效果上不及那些针对生成或感知任务精心设计的专用方法。这就留下了一个根本性的科学问题：单一的预测下一个词元框架是否能够作为通用的多模态学习范式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就此，智源提出了Emu3，基于“预测下一个词元”的全新多模态模型，将图像、文本和视频统一离散化到同一个表示空间中，并从零开始，在多模态序列混合数据上联合训练一个单一的 Transformer。这一架构证明了仅凭“预测下一个词元”，就能够同时支持高水平的生成能力与理解能力，并且在同一统一架构下，自然地扩展到机器人操作以及多模态交错等生成任务。此外，研究团队还做了大量消融实验和分析，验证了多模态学习的规模定律（Scaling law）、统一离散化的高效性、以及解码器架构的有效性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/58/5876defd768a77c0f403e3b928a37a11.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Emu3 架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验显示，Emu3在生成与感知任务上的整体表现可与多种成熟的任务专用模型相媲美：在文生图任务中，其效果达到扩散模型水平；在视觉语言理解方面，可以与融合CLIP和大语言模型的主流方案比肩。此外，Emu3还具备视频生成能力。不同于以噪声为起点的扩散式视频生成模型，Emu3通过自回归方式逐词元（token）预测视频序列，实现基于因果的视频生成与延展，展现出对物理世界中环境、人类与动物行为的初步模拟能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从模型到范式：Emu3对多模态学习的启示&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同于 Sora 的扩散式视频生成，Emu3&amp;nbsp;采用纯自回归方式逐词元（token） 生成视频，能够在给定上下文下进行视频延展与未来预测，并在文本引导下生成高保真视频。此外，Emu3 还可拓展至视觉语言交错生成，例如图文并茂的菜谱生成；也可拓展至视觉语言动作建模，如机器人操作VLA等，进一步体现了“预测下一个词元”的通用性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源研究团队对相关研究的多项关键技术与模型进行了开源，以推动该方向的持续研究。其中包括一个稳定且通用的视觉分词器（tokenizer），可将图像与视频高效转换为离散词元来表示。同时，研究通过大规模消融实验系统分析了多项关键技术的设计选择，例如：分词器（tokenizer）码本尺寸、初始化策略、多模态dropout机制以及损失权重配置等，揭示了多模态自回归模型在训练过程中的动态特性。研究还验证了自回归路线高度通用性：直接偏好优化（DPO）方法可无缝应用于自回归视觉生成任务，使模型能够更好地对齐人类偏好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究有力表明了预测下一个词元可作为多模态模型的核心范式，突破语言模型的边界，在多种多模态任务中展现了强劲性能。通过简化复杂的模型设计、聚焦统一词元，该方法在训练与推理阶段均展现出显著的可扩展性，为统一多模态学习奠定了坚实基础，有望推动原生多模态助手、世界模型以及具身智能等方向的发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此研究基础上，悟界·Emu3.5进一步通过大规模长时序视频训练，学习时空与因果关系，展现出随模型与数据规模增长而提升的物理世界建模能力，并观察到多模态能力随规模扩展而涌现的趋势，实现了“预测下一个状态”的范式升级。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</link><guid isPermaLink="false">https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</guid><pubDate>Thu, 29 Jan 2026 06:47:46 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“AI在一线：开发人员如何重塑软件开发流程” ｜圆桌讨论</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从代码生成到自动化文档，人工智能已经开始渗透到软件开发生命周期的几乎每个阶段。但除了炒作之外，实际上发生了什么变化？我们询问了一群工程师、架构师和技术领导者，AI辅助工具的兴起如何重塑软件开发的既定节奏，以及他们在现实世界中采用AI后学到了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;讨论嘉宾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mariia Bulycheva——Intapp高级机器学习工程师Phil Calçado——Outropy首席执行官Andreas Kollegger——Neo4j高级开发者倡导者May Walter——Hud.io创始人、首席技术官&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：AI辅助工具的兴起对你们组织的软件开发过程有何影响？它们是否改变了你们对软件架构的思考方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：AI辅助工具加速了原型设计，并减少了在重复编码任务上花费的时间，使我们的团队能够更多地关注架构决策和设计复杂的在线实验，这对于大规模迭代改进复杂的推荐系统至关重要。从数字平台典型的大量多模态数据中获得初步洞察也变得更快、更顺畅、更一致，因为我们可以将初始数据分析委托给了AI。&amp;nbsp;我们工作的另一个非常重要的方面是跟上我们领域科学发展的快速步伐。每年，在顶级会议上都会发表数千篇新的研究论文，过去阅读它们并确定哪些可能与我们团队的日常ML任务相关是非常耗时的。今天，AI工具提供了高质量的摘要，甚至突出显示哪些方法可能适用于我们的用例。这已经导致了几个新建模想法的快速实现，否则我们可能需要花费数周甚至数月的时间来发现和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：绝对有。我们运行的是一个消费者参与平台，其功能之多，任何正常人都无法全部记住。例如，最近，我们需要改变我们处理时区的调度方式。代码变更本身可能只有10行，但真正的工作是深入数百个涉及调度的地方，弄清楚每个地方的假设，并添加单元测试以断言调用站点不会中断，而是行为的变化。我们原以为这将是一个为期六个月的项目，因为我们需要逐步研究并进行小的更改。&amp;nbsp;有了像Cursor和Claude Code这样的工具，我们大大缩短了这个时间。它们帮助我们找出所有受影响的位置，为每个位置生成单元测试，并将推出分成按子系统分组的小PR。每个PR都带有对所属团队的上下文敏感的描述——不仅仅是“修复调度，请审查”，而是解释为什么以及在他们的世界中预期的影响。&amp;nbsp;因此，尽管我们像其他人一样看到了原始代码输出的增加，但在我们这样成熟的、超大规模的系统中，最大的提升在于AI如何帮助我们研究自己的代码库，将无聊但必不可少的安全检查整合在一起，使系统性变更变得不那么可怕。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在我们组织中，所有员工现在都可以使用AI辅助工具。对于表面层级的界面设计，这些工具帮助我们更快地迭代，探索新想法，并解锁新方法，如氛围编码，专注于更高层次的设计和策略。&amp;nbsp;但我们也确实遇到了AI的局限性。像许多组织一样，我们发现大语言模型（LLM）在需要深厚领域专业知识和全局架构整体视图的高度专业化代码上挣扎。我们的代码库本身就超过了任何LLM上下文窗口的容量，而这些模型本身也没有在其中的独特复杂性上进行训练。简而言之，AI不能发明它不理解的东西。因此，我们有意采取了一种以人为中心的方法：虽然AI帮助我们加速和增强，但推动软件架构突破的是我们工程师的专业知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：AI辅助工具极大地缩短了从想法到工作代码的路径。一旦意图明确，迭代周期就会显著缩短。开发人员正在从代码的唯一作者转变为更像是管理者的角色——指导代理，验证输出，并确保需求得到真正满足。&amp;nbsp;AI之前，架构是关于团队之间的所有权和可扩展接口。这引入了一个新的维度：上下文架构——设计代理生成生产就绪代码所需的输入、脚手架和护栏。上下文工程正在成为系统的核心部分，它简化了在复杂环境中快速构建的能力，如分布式和基于事件的系统。&amp;nbsp;但速度带来了一个新的瓶颈：为生产准备AI生成的变更。即使审查有了AI辅助，挑战也不再是关于发现语法错误，而是在大型、大规模的系统中验证意想不到的后果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：人工智能的采用如何影响团队内部的入职流程？你们团队或组织中的初级开发人员是否受到软件开发过程中采用人工智能的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：人工智能工具可以通过提供即时的代码示例、文档摘要和测试建议，显著加快学习过程，这些都支持了初级开发人员。在处理个性化和推荐系统等复杂领域的团队中，这一点尤其有用，因为现在初级人员可以更快地探索新的代码库，而不必总是依赖高级工程师。同时，我们将他们与更有经验的同事配对，以确保他们学习潜在的基本建模和系统设计原则，而不仅仅是捷径。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我们刚刚让暑期实习生展示了他们的项目，几乎每个人都把人工智能称为救星。进入一个有10年历史的Rails代码库，其中包含数千个可移动的部分，这是令人生畏的。但是能够对Cursor或Claude Code说，“我是一名懂Python和C++的大三学生，请用我熟悉的方式解释这个Rails代码”，这意味着他们可以在几周内提高效率，而不是仅仅把时间花在弄清楚基础知识上。&amp;nbsp;而且，不仅仅是实习生。在这个庞大的系统中，即使是高级工程师也需要比在小公司更多的准备时间。AI并没有消除对系统的实际理解，但它确实减轻了“我们在哪里处理认证？”或“我们是否已经有了观察者模式的实现？”这类问题的压力。&amp;nbsp;当然，这里有一个问题。生成式AI擅长复制模式，这通常意味着我们不希望再看到的遗留风格和架构。因此，我们不得不适应。我们正在使我们的工作流程和架构更加适应AI，并且我们已经开始将当前的指导方针直接嵌入到Claude Code和Cursor的智能体中。这样，当AI提供帮助时，它会引导人们走向现在，而不是过去。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：人工智能的采用增强了我们的入职流程，特别是对于新接触图数据库的初级开发人员。虽然人工智能不能取代经验丰富的导师的指导，但它通过帮助新员工更快地上手，补充了我们现有的入职资源。&amp;nbsp;入职培训不仅仅是教授编码技能。它是关于建立领域专业知识的。编码能力很重要，但更重要的是理解要编写什么代码以及为什么。这就是为什么我们的入职开发人员，他们对代码库及其架构有深入的了解，在向初级团队成员传授专业知识和上下文方面发挥着关键作用。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：人工智能降低了贡献的障碍。现在，一个新开发人员可以在他们的第一天就写出可用的代码——这与早期工作仅限于样板或错误修复的日子相比，是一个戏剧性的转变。但真正的机会不在于速度；而是在于能力和范围的深度。&amp;nbsp;我最常听到的担忧是，人工智能有使入职变得肤浅的风险——初级人员可以在不理解代码为何以某种方式行为的情况下生成代码。我的经验恰恰相反。当代码生成与运行时反馈配对时，初级开发人员从一开始就接触到系统思维：架构在负载下的行为如何，依赖项如何相互作用，以及变化如何波及到业务结果。工程师成为智能体代码生成过程中的业务大使。&amp;nbsp;他们不再需要花几个月的时间来处理低价值的工作，而是能够处理团队的更多任务。如果做得好，这不会跳过步骤——它会加速步骤。有了正确的文化和期望设定，初级工程师可以更快地发展成为全面发展的工程师，因为他们不仅学习如何编写代码，还学习了为什么它在系统环境中很重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在你们团队或组织中，你们是否测量过AI辅助开发对生产力或质量的影响？你们学到了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们在样板代码和单元测试生成方面看到了明显的生产力提升，甚至在为推荐系统设置模拟实验方面也是如此。然而，当处理影响大规模客户体验的关键系统时，真正的好处来自于将AI辅助与深度工程师参与结合起来。我们了解到，虽然AI提高了生产力，但质量仍然取决于仔细验证和清晰的指标和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado:：没有正式的测量。坦率地说，我不相信大多数抛出的“生产力”数字。在软件领域，你可以操纵指标，直到它们说出你想要的任何东西，而AI的炒作周期使这变得更糟。事实上，人们再次认真计算代码行数，只是为了增加一轮融资或提高股价，这是令人尴尬的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在前端方面，我们看到了生产力的提升，特别是对于那些使用Cursor等工具的工程师。我们的许多工程师已经使用AI支持来更快地理解、进行表面编码和测试我们的代码库，但我们从AI中看到的真正影响是对开发人员体验的影响。通过使用AI工具来支持他们的一些活动，我们的工程师现在有更多的时间发挥创造力，并最终改进他们解决问题的方式，并为他们的工作创造新的方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：是的，我们了解到的第一件事是，大多数常见的度量标准并没有太大意义。公认的代码行数、提交次数、PR：AI可以立即夸大这些数字，但它们只是工程生产力的虚荣指标。&amp;nbsp;真正的信号存在于下游。发布稳定性、事故频率、随叫随到的时间，甚至代码变更率，都能告诉我们是否真的在加快速度，或者只是在制造更多的脆弱性。AI将速度转移到了管道的前端，但除非验证循环紧密，否则债务会在后期显现——以缺陷、回归和精疲力竭的团队的形式。&amp;nbsp;从第一天开始，通过持续的生产反馈，我们可以看到真相所在：功能开发变得更快，但审查周期变得更长了，部署后的错误也出现了。&amp;nbsp;教训是，AI生产力需要一个学习曲线和迭代方法。一旦度量，可以逐步改进采用，以捕捉优势——同时避免因快速交付但存在稳定性问题窒息的陷阱。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：为了有效地使用AI工具，你们的团队或组织中有哪些非技术方面的东西需要改变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：最大的变化是在心态上。团队必须从期望AI建议是“正确”的，转变为将它们视为需要彻底验证、讨论和测试的起点。这种文化转变鼓励了实验和跨学科合作，将对确定性的关注转变为对探索的关注。在大规模个性化工作中，我们还需要与产品和法律团队就负责任的数据使用和可复制性达成一致。这些协议创造了护栏，使工程师能够安全地探索和部署AI辅助解决方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我认为关于生成式AI工具的最大事情是：这超越了编码。是的，像任何其他工具一样，你必须注意副作用。生成式AI可以很容易地生成内容：代码、PR评论、技术规范、电子邮件、Slack消息。它也使得总结大量文本并过滤掉非必要的内容变得非常容易。&amp;nbsp;这两个特性的结合创造了一个奇怪的激励：人们生成了大量的低信噪比内容，然后其他人再次使用AI将其过滤回去。这是极其无效的。我们已经开始内部讨论在制作内容时正确使用AI的方式。剧透：这不是让AI为你写作，而是使用AI帮助你写得更好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：我们在AI的早期阶段建立了一个AI伦理委员会，组织中的代表们更好地理解和指导AI如何影响我们的业务的每一个方面。所有技术都可以成为一股善的力量，但它也需要有意识的思考、行动和指导。&amp;nbsp;因为我们信任客户数据，我们的开发人员需要在引入AI作为助手的任何领域都应用更高的敏感性，从简单的计划文件和电子邮件线程到代码库本身。随着我们采用、集成和扩展AI，我们所有的开发人员都必须确保人类判断，而不是AI，指导和监督每一步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：最大的变化不是技术上的，而是文化上的。开发人员自然会单独采用工具，但当AI被视为个人生产力技巧时，它的效果并不好。只有当它成为共享流程的一部分，有一致的验证步骤和清晰的责任时，它才会有效。此外，AI工具在缺乏上下文时不会失败，而是产生不准确的回应，这可能会损害用户的信任并增加变更的摩擦。&amp;nbsp;在10名工程师的情况下，每个人都可以以自己的方式进行实验。在100名工程师的情况下，这种方法就会崩溃。不同的智能体独立生成代码会造成分裂和风险。我们转向了共同的设置和共享的工作流程，这样AI不仅仅是帮助个人更快地移动，而是使整个团队更快地移动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们在管理AI辅助编码方面设置了哪些护栏（文化、道德或技术），以及你们如何管理个人、团队和组织对AI输出的信任问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们将AI输出视为重复性或样板任务的“初稿代码”，这些代码总是经过单元测试和同行评审。在文化方面，我们强调责任：提交代码的开发人员负责，无论是否有AI辅助。对于机器学习工作流程，我们不信任AI直接生成模型，相反，在任何模型更改甚至可以考虑用于生产之前，我们依赖于针对既定基线的自动离线评估。这确保了AI驱动的贡献达到了与人类编写的代码相同的质量标准。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：这仍然是一个非常初级的实践，所以我们一直在尝试不同的护栏和工具。在安全和合规方面，我们的立场从一开始就很明确：作为一个处理世界上一些最大品牌数据的上市公司，我们必须将相同的治理实践应用于AI编码工具，就像我们其他地方所做的一样。几年前，这意味着落后于曲线，但今天大多数供应商都有坚实的企业计划，所以我们可以安全地使用最先进的模型，而不会妥协安全性或可审计性。&amp;nbsp;文化上，我们很早就设定了期望：仅仅因为一个AI工具编写了变更，并不意味着它不是你的代码。你仍然拥有它，你需要把每一行都当作是你自己打出来的一样。这与使用IntelliJ的提取方法重构没有什么不同，在这种情况下，它可能自动化了机械操作，但你仍然需要理解并验证结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：大型企业软件可以提供防止AI生成错误的保障，但更高的准确性、上下文和可追溯性是使AI输出可解释和可验证的关键，而不仅仅是性能。这就是为什么我们引入了一个广泛的测试计划，涵盖了从单个单元测试到详尽的生产级验证的所有内容。&amp;nbsp;与此同时，我们的工程师在纪律和创新之间保持平衡至关重要。我们鼓励工程师尝试各种想法，探索可能尚未准备好投入生产的项目。这种环境允许快速迭代和创造力，同时确保只有最有价值和经过充分测试的创新才能过渡到生产。其结果是一种独特的平衡：保持客户的信任和稳定，同时不断推进图驱动的创新，使AI更准确、透明和可解释。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：我们必须赢得对AI输出的信任，而获得信任的唯一方法便是创造上下文。每个AI生成的变更都经历了与人类编写的代码相同的标准——审查、测试、验证——但有一个额外的要求：它必须在运行时证明自己。&amp;nbsp;对我们来说，信任不是来自对模型的信念；而是来自观察代码在现实世界条件下的行为。新版本的性能和旧版本一样吗？它是否引入了新的错误或在负载下改变了性能？当运行时上下文持续可用时，AI就不再是黑盒。它变成了一个可以信任的伙伴，因为它与工程师依赖相同的信号进行推理。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们认为软件开发团队低估了AI编码工具的哪些方面？你们认为有哪些当前的AI增强型开发工作流程或模型被过度炒作，哪些仍然没有得到充分利用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：许多团队低估了上下文管理的重要性，因为AI的效果取决于你提供的上下文（代码库、文档、架构、在线测试的实验设置）。在大型系统中，这意味着不仅要管理代码片段，还要管理模型性能数据、日志和实验历史，以有效指导AI工具。过度炒作：AI据说取代了工程判断的“一键式开发”。未充分利用：AI辅助调试、实验设置和复杂ML工作流的文档记录，这可以大幅降低长期维护成本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：现在AI中有很多空洞的炒作，很难挑出一个罪魁祸首。但大多数团队都低估了的这一点是：AI编码工具不是一个单程的魔法盒子。你不能只是向它们抛出一个提示，就期望得到一致、正确的结果。&amp;nbsp;这是任何真正构建AI产品的人都知道的痛苦教训。无论你的提示工程有多聪明，有效使用LLMs来自于结合工作流程并确保正确的上下文在正确的时间可用。否则，你只是在掷骰子。&amp;nbsp;我在以前为一个流行的代码审查工具构建AI管道时亲眼目睹了这一点。模型可能已经记住了所有写过的Python书籍，但如果你问10个开发人员“正确的方法”去做某事，你会得到11个答案。如果没有你的代码库、组织标准和实际目标的上下文，LLM就不知道哪个适用。这就是为什么你会得到完全不同的解决方案，甚至是对立的——这取决于当你提问时，概率之神想要倾向于哪一边。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：许多软件开发团队低估了AI编码工具可以简化开发人员最不喜欢的任务，比如编写测试和文档。虽然AI编码演示承诺低代码和无代码，通常看起来微不足道或不可靠，但它们展示了AI如何将自然语言和代码之间进行转换，这对于自动化繁琐的任务和重复的设置是理想的。类似地，有一种专门用于项目初始化和代码生成的编码工具。&amp;nbsp;有一种工作流程被夸大了，但却没有得到充分利用，那就是让编码智能体通宵运行，并在早上检查他们的工作。我不建议在无人监督的情况下重构新产品功能或大量代码，但编码智能体非常适合一个定义良好的GitHub问题，它有良好的讨论，一个孤立且可重现的例子，以及一个可测试的修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：大多数团队低估的是，模型已经足够好（并且正在变得更好）——缺失的成分是组织上下文。等待“更好的模型”是一种分心。真正的挑战是设计提供生成生产级代码所需的上下文的系统：你的架构、编码标准、数据边界和业务优先事项。如果没有这些，即使是最好的模型（或工程师）也会表现不佳。&amp;nbsp;另一方面，今天被过度炒作的是原始代码生成和静态代码审查。这些工作流程在演示中看起来令人印象深刻，但它们并没有解决大型组织中软件工程最难的部分：调试和质量保证。智能体仍然缺乏运行时上下文，并且很少有工具来评估哪些更改在业务影响方面真正关键。&amp;nbsp;这个差距很重要，因为更快的代码生成意味着更多的更改流入生产环境——而且没有更强大的流程来决定要监控什么，团队冒着为了脆弱性而牺牲速度的风险。未充分利用的前沿不是更快地编写代码，而是构建验证循环和运行时感知工具，以在这些更改部署之前增加确定性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这次讨论中得出的第一个，也许是最重要的结论是，尽管在软件开发过程中采用AI工具无疑降低了贡献的门槛，但它仍然是一个乘数，而不是灵丹妙药。只有与强大的组织环境相结合，AI才能增强生产力。基于AI的工程有潜力成为软件开发的核心，就像CI/CD管道曾经一样。然而，架构、编码标准和实验脚手架是成功采用AI的支撑支柱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，随着AI工具的发展，组织中开发人员的角色也从代码作者转变为系统编排者。新采用的策划、验证和集成AI输出的过程并没有取代软件工程这门手艺；相反，它增强了它。批判性思维和架构意识比以往任何时候都更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，采用任何新技术都会带来陷阱，对于AI和基于AI的工具也是如此。降低贡献的进入门槛也意味着增加了浅薄理解和生产次品代码的风险，这可能对初级开发人员的职业发展和整个组织产生负面影响。指导和运行时反馈是重要的护栏，以及文化和伦理保障：AI输出必须被视为初稿，人类必须对其负责。当涉及到AI时，信任不是授予的：它是一个过程，通过测试、同行评审、运行时验证和透明度赢得。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成功的指标也必须重新思考，因为AI夸大了所有传统的生产力指标。有意义的信号来得更晚：稳定性、变动、事件，以及有多少时间可以释放给创造力和架构。将AI扩展视为一个协作过程，而不是个人生产力的提升，这需要协调的工作流程和对周围流程的更高成熟度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论好坏，很明显，AI带来的变化已经到来，正在重塑软件开发的工艺。仍然有未被充分利用的方面，但上下文设计和运行时感知工具已经是下一个架构前沿。从长远来看，AI竞赛的赢家将是那些将其整合到具有问责制、信任和能够以负责任的方式共同发展的团队级流程中的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&quot;&gt;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</link><guid isPermaLink="false">https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>作者：Arthur Casals</author><category>AI&amp;大模型</category></item><item><title>半年处理 1 亿笔支付！x402 V2 升级，让支付更简单</title><description>&lt;p&gt;在经历了为期六个月的真实场景应用之后，&lt;a href=&quot;https://www.x402.org/writing/x402-v2-launch&quot;&gt;开放支付标准 x402&lt;/a&gt;&quot;&amp;nbsp;迎来了重要更新。本次升级显著拓展了协议能力，使其不再局限于“单次请求、固定金额”的支付模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;新版协议新增了多项关键能力，包括：基于钱包的身份识别、自动 API 发现机制、动态支付接收方，以及通过 CAIP 标准实现的多链与法币扩展支持。同时，x402 还推出了一个完全模块化的 SDK，用于支持自定义网络和支付方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 是一次重量级升级，使协议在通用性、灵活性和跨网络扩展能力上均有显著提升。新版规范更加简洁、模块化，并与 CAIP、IETF Header 等现代标准保持一致，从而实现了一个可同时覆盖链上与链下支付的统一接口。在具体能力上，x402 V2 提供了一个统一的支付接口，可在多条区块链上支持稳定币和代币支付，包括 Base、Solana 等网络；同时也保持了对传统支付体系的兼容性，如 ACH、SEPA 以及银行卡网络。此外，协议还引入了“按请求路由”的能力，支持将支付定向至特定地址、角色，或基于回调逻辑进行分发，从而实现更复杂的多步骤支付工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 的另一项重要改进，是清晰地区分了三类角色：协议规范本身、SDK 实现，以及负责链上验证和结算的facilitators。这种分层设计显著提升了协议的可扩展性，并为插件化、模块化架构奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新标准还引入了基于钱包的访问机制、可复用会话（reusable sessions）以及模块化付费墙（modular paywalls）。钱包支持让客户端在支付流程上拥有更高灵活性，可减少已购项目的重复交互与延迟；而模块化付费墙则使开发者能够更容易地集成和扩展后端支付逻辑，推动整个生态向更开放的方向发展。&lt;/p&gt;&lt;p&gt;在开发者体验方面，x402 V2 也进行了系统性优化。通过模块化设计简化配置流程，新增了同时选择多个 facilitators 的能力，并大幅减少了“胶水代码”和样板代码的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 是一套开放、原生于 Web 的支付标准与协议，其目标是让“支付”成为互联网的一等公民。它支持微支付、按使用付费（pay-per-use）以及机器对机器（machine-to-machine）支付，使 Web 应用、API 以及自治代理（如 AI 机器人）能够直接通过 HTTP 为服务付费，而无需传统账户体系、订阅模式或复杂的支付流程。在推出后的短短几个月内，x402 已在 API、Web 应用和自治代理等场景中处理了超过 1 亿次 支付流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该协议利用了一个长期被忽视的 HTTP 状态码 —— 402（Payment Required），用于在需要付费时返回支付指令。借助 x402，支付可以直接嵌入在 HTTP 请求—响应流程中，无需将用户跳转至外部支付页面，也不再依赖 API Key 或个人账户体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为 x402 Foundation 的最初合作伙伴之一，Cloudflare 与 Coinbase 一同推动了该协议的落地。Cloudflare 已将 x402 集成进其&lt;a href=&quot;https://developers.cloudflare.com/agents/x402/&quot;&gt;开发者工具和基础设施&lt;/a&gt;&quot;中，包括：Agents SDK：帮助开发者构建能够自动完成 x402 支付的智能代理；MCP servers：向外暴露支持 x402 的工具，使服务能够返回 402 Payment Required 响应，并接收来自客户端的 x402 支付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&quot;&gt;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</link><guid isPermaLink="false">https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>Sergio De Simone</author><category>软件工程</category></item><item><title>刚完成“卖身”重组，TikTok就瘫了！Oracle 背锅：暴风雪吹坏了数据中心</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle数据中心断电，引发 TikTok 大面积瘫痪&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，短视频平台 TikTok 在美国出现了一次短暂的服务中断。值得玩味的是，这次中断的时间点，恰好卡在 TikTok 刚完成一项美国业务重组安排之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据这份重组安排，由 Oracle 与一组美国本土投资者共同组建的新合资实体，将接管 TikTok 在美国的运营相关事务，并被称为 TikTok USDS。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok USDS 承诺将用户数据通过 Oracle 公司拥有的数据中心进行传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚重组完没几天，TikTok就出现了大面积瘫痪，许多美国用户反映，他们无法上传视频到TikTok，也无法观看大多数新视频，包括美国以外用户成功上传的新视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一些用户表示，他们的算法似乎“重置”了，但目前尚不清楚这是否也与停电有关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事情不断发酵，逼得 TikTok&amp;nbsp;USDS 不得不出面回应了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok&amp;nbsp;USDS 发言人 Jamie Favazza在给The Verge的一封电子邮件中指出，该公司在其新创建的X账户上发布了一份声明，声明称，由于美国数据中心发生电力中断，影响了TikTok和我们运营的其他应用程序，公司一直在“努力恢复服务”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/20e4e23cbb346864a484a62806ea02f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然问题出在了数据中心，数据中心当然也要出来回应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle 回应：完全怪天气&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对不断升温的质疑，Oracle 公司于当地时间 1 月 27 日通过电子邮件向媒体作出正式回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle 发言人迈克尔·埃格伯特（Michael Egbert）表示，上周末美国遭遇的一场强烈冬季风暴，导致Oracle 一处数据中心发生了暂时性停电，从而影响了 TikTok 在美国的服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“上周末，Oracle 数据中心因天气原因发生暂时性停电，影响了 TikTok 的服务。” 埃格伯特在声明中写道。他进一步解释称，美国 TikTok 用户在停电后所遇到的问题，主要源于恢复过程中出现的技术故障，目前Oracle 正与 TikTok 合作，尽快修复相关问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一回应明确否认了服务异常与内容审查之间存在直接关联，并将原因归结为基础设施层面的突发事故。Oracle 方面的说法，也与当时美国多地遭遇极端冬季天气的事实相吻合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在随后的声明中，TikTok 指出，其工程团队正在持续推进恢复工作，并在 1 月 27 日表示，已在恢复美国系统方面取得“重大进展”，但仍提醒用户，某些技术问题可能在短期内持续存在，尤其是在发布新内容时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64663332726be7240cc1f4098dedf7cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为此次事件中的关键基础设施提供方，Oracle 的角色也受到资本市场关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Benzinga Pro 报道，Oracle 公司股票在事件曝光当日收于 174.90 美元，下跌 4.13%，但在盘后交易中回升 1.16% 至 176.93 美元。Benzinga 的 Edge 股票排名显示，Oracle 股票在动量和价值维度上的评分均处于较低水平，反映出其在短期至长期内的价格趋势承压。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/254b1fa735b06cff92bf69b81ae8c593.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 TikTok USDS 合资企业逐步接管美国业务，其基础设施稳定性、内容审核机制以及与地方和联邦监管机构的互动方式，仍将持续受到审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：不只可以怪天气，还可以怪AI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着最终达成的合资协议，被外界普遍视为 TikTok 在美国“生死攸关”的一次妥协安排。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，此次技术中断发生之际，正值 TikTok 更新其美国隐私政策之后。新政策与合资架构调整相配套，但其中关于可能收集的数据类型的表述，引发部分用户不安。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;市场情报公司 Sensor Tower 向 CNBC 提供的数据显示，在过去五天内，美国地区 TikTok 的每日应用删除量较此前三个月的平均水平增长了近 150%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 上，一条关于 Oracle 数据中心与 TikTok 服务中断的帖子吸引了大量关注，不少网友在评论区提出了各类猜测、调侃与个人经验分享，这些反馈在一定程度上折射出技术社区对事件的怀疑态度，以及对 TikTok 内容机制与 Oracle 云服务能力的长期刻板印象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位ID名为 transcriptoin_error 的用户提出了一种“看似合理的推测”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，如果平台在系统中新增了内容过滤机制，那么在将相关流量迁移到新系统的过程中，确实有可能引发故障。他指出，在大规模系统迁移或数据转移时，出现配置错误或小规模失效并不罕见，尤其是在新旧系统并行、过滤规则叠加的情况下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论获得了数十次点赞，被不少用户视为“至少在工程逻辑上说得通”的一种解释，但评论者本人也并未声称这是事实，而是明确将其界定为推测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4a/4a7c72d3511f47000b48430ba2da565c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在另一条高赞的长篇幅评论中，该用户进一步构建了一套完整但高度假设性的系统模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他设想，如果 TikTok 平台不愿直接改动现有代码，以避免引发更大规模的系统崩溃，那么新增的内容过滤功能很可能会被设计成一个独立服务，甚至可能基于人工智能模型运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种设想下，所有潜在“敏感内容”都会被发送至一个新的 AI 服务进行判断，只有在得到“允许发布”的反馈后，内容才会正常上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9fa27b6e1350a4f8df70b50cb109e9c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该用户进一步推测，如果这一 AI 服务发生宕机，而系统默认策略又是“未通过即阻止”，那么大量内容就可能被一并拦截，从而在用户侧表现为算法行为的“剧烈变化”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论虽然点赞不高，但在讨论中被多次引用，成为部分网友解释“为什么技术故障会影响内容分发”的逻辑模板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有网友对上述推测持明显怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位在评论区拥有较高影响力的用户指出，至今仍然没有人能够清楚解释，为什么一次服务器层面的故障，会导致推荐算法或内容分发逻辑出现如此明显的变化。在他看来，如果问题仅限于数据中心断电或服务恢复过程中的技术瑕疵，那么算法层面的“性格突变”仍然缺乏合理解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了针对 TikTok 的讨论，Oracle 本身也成为 Reddit 用户情绪的集中投射对象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位用户直言，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“能力并非问题所在，科技圈里没人能忍受 Oracle。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/336724bda8cb17fd3aa87f2658e0017c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还引用了一句在技术圈流传已久的说法：“Oracle 没有客户，只有囚犯。”这类评论并未直接指向此次事件的具体责任，但反映出 Oracle 在开发者与工程师群体中的长期口碑问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&quot;&gt;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&quot;&gt;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&quot;&gt;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</link><guid isPermaLink="false">https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</guid><pubDate>Thu, 29 Jan 2026 05:21:46 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Rust贡献者推出新语言Rue，探索AI辅助编译器开发</title><description>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/steve-klabnik/&quot;&gt;Steve Klabnik&lt;/a&gt;&quot;是《&lt;a href=&quot;https://doc.rust-lang.org/stable/book/&quot;&gt;Rust编程语言&lt;/a&gt;&quot;》的作者，并且在过去的13年里对Rust项目做出了贡献，他宣布了Rue，这是一种系统编程语言，它在没有垃圾回收的情况下探索内存安全性，同时优先考虑开发人员的人机工程学，而不是Rust的复杂性。该项目是在Anthropic的&lt;a href=&quot;https://claude.com/product/overview&quot;&gt;Claude AI&lt;/a&gt;&quot;的大力帮助下开发的，目标是填补高性能系统语言和垃圾回收替代品之间的未充分服务的设计空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在使用Rust 13周年之际，Klabnik在一篇博客文章中解释了他的动机：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我一直在想我是否应该尝试创造自己的语言。我真的很喜欢它们！这就是为什么我最初参与Ruby，然后是Rust的部分原因！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语言名称遵循他的“Ru”前缀模式（Ruby、Rust、Rue），同时保持双重解释——既是&lt;a href=&quot;https://en.wikipedia.org/wiki/Ruta_graveolens&quot;&gt;花&lt;/a&gt;&quot;又是&lt;a href=&quot;https://redkiwiapp.com/en/english-guide/synonyms/rue-regret&quot;&gt;遗憾的表达&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik的核心设计问题是：“如果Rust不试图与C和C++竞争最高性能会怎么样？”如果我们愿意为了易用性而使性能稍微降低，但不要太低，会怎样？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术方法的核心是消除Rust的标志性——&lt;a href=&quot;https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html&quot;&gt;借用检查器&lt;/a&gt;&quot;。考虑一个典型的Rust代码，其中你试图在持有对其中一个元素的引用的同时修改一个向量。编译器拒绝此操作，因为引用可能会无效。Rue通过使用“inout”参数来暂时转移所有权，从而避免了整个问题，类似于Swift。在Rust中，试图在迭代当量时修改它会在编译时失败。Rue的inout参数允许你临时传递可变引用，但防止将它们存储在数据结构中；在保持内存安全的同时，通过更简单的限制消除了对生命周期跟踪的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;函数可以就地修改值，但这些值不能作为引用存储在堆分配的结构中。不需要生命周期注释。权衡是什么？某些模式变得无法表达。正如设计文档所承认的，Rue无法支持从其容器借用的迭代器；它们必须消耗它们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News社区的反应既有兴趣也有怀疑。一位评论者&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;捕捉&lt;/a&gt;&quot;到了这个挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Rust之所以成功地制造出没有垃圾回收的内存安全语言，是因为它引入了显著的复杂性（这是一种权衡）。没有人真正知道除此之外的合理方法，除非你还想放弃通用系统编程语言的要求。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据GitHub仓库中的设计提案，Rue实现了四种不同的所有权模式：值类型、仿射类型、线性类型和引用计数类型。Klabnik在回应中承认，“这必然会导致一些表现力的丧失。没有万能的解决方案。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发方法代表了一个实验，解决了Klabnik多年来一直在思考的&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;问题&lt;/a&gt;&quot;：“没有资金或团队，一个人还能构建一门编程语言吗？”这种方法标志着Klabnik的转变，他形容自己直到2025年都是AI怀疑论者。他第一次尝试在没有有效利用AI的情况下构建Rue，经过几个月的工作后不得不放弃。这一迭代，更有效地使用Anthropic的Claude AI，仅用两周时间就产生了大约70,000行Rust编译器代码，远远超过了他之前几个月的尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种协作超越了典型的编码协助。在Klabnik和Claude共&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;同署名&lt;/a&gt;&quot;的博客文章中，AI描述了编写大部分实现代码，而Klabnik指导架构并做出设计决策。Klabnik强调，有效使用AI工具需要大量的技能：“仅仅知道如何编写代码实际上不足以真正使用大模型。它们是它们自己的新类别的工具。”他的方法涉及迭代实验，编写简短的代码片段，开始对话，并测试不同的提示策略。这种模式是否能消除历史上资助语言项目的大量投资，还有待观察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue仍处于早期开发阶段，具有基本的控制流、函数和非泛型枚举。它通过自定义后端而不是LLVM编译为本地可执行文件，通过简化的语义实现快速编译时间。堆分配正在进行中，而语言服务器协议支持、包管理和并发模型尚未实现。该项目使用Buck2而不是Cargo进行未来的编译器引导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik保持着适度的期望：“我不指望它能发展成我的业余项目。”尽管如此，他指出，PHP和Rust的创造者Rasmus Lerdorf和Graydon Hoare也是从个人实验开始的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI辅助开发工具重塑软件工程，这项实验正在进行。虽然GitHub Copilot和类似的工具协助增量编码，Klabnik使用AI进行编译器的架构级工作的方法代表了不同级别的合作。如果成功，它可能表明，传统上需要大型团队的复杂基础设施项目，在AI的帮助下，对于熟练的个人来说可能是&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;可行的&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真正的考验将是那些对Rust的学习曲线感到沮丧但又不愿意采用垃圾回收机制的开发人员是否能接受Rue的权衡。正如一位Hacker News评论者所说：&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果他们能在设计空间中找到一个全新的未被探索的点，我会非常感兴趣，但目前，我仍然持怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue语言的文档可在&lt;a href=&quot;https://rue-lang.dev/&quot;&gt;rue-lang.dev&lt;/a&gt;&quot;上找到，源代码在&lt;a href=&quot;https://github.com/rue-language/rue&quot;&gt;GitHub&lt;/a&gt;&quot;上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&quot;&gt;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</link><guid isPermaLink="false">https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</guid><pubDate>Thu, 29 Jan 2026 03:05:14 GMT</pubDate><author>Tim Anderson</author><category>编程语言</category></item><item><title>Spring近期资讯：Boot、Security、Integration、Modulith和AMQP首个里程碑版本发布</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Boot&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot; 4.1.0的首个里程碑版本提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 @AutoConfigureWebServer 注解用于在支持 @SpringBootTest 注解的特定类和随机端口下启动Web服务器；以及通过Spring AMQP和Spring Kafka中定义的配置bean的自动配置，改进了可观测性和指标支持。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了缺陷修复、文档改进和依赖升级，Spring Boot 4.0.2，&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-0-2-available-now&quot;&gt;即第二个维护版本&lt;/a&gt;&quot;，还提供了一个值得注意的更改，即从 spring-boot-jetty 模块中移除了对 org.eclipse.jetty.ee11:jetty-ee11-servlets 模块的依赖，因为它未被使用并被确定为不必要。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Security&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：在 PasswordEncoder 接口中定义的 encode() 方法添加了空值契约；以及使用Spring Framework DefaultParameterNameDiscoverer 类中定义的 getSharedInstance() 方法，而不是创建该类的单独自定义实例。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Integration&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 spring-integration-cloudevents 和 spring-integration-grpc 模块分别支持&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/cloudevents.html&quot;&gt;CloudEvents&lt;/a&gt;&quot;转换和&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/grpc.html&quot;&gt;gRPC&lt;/a&gt;&quot;协议；以及新的 GrpcInboundGateway 和 GrpcOutboundGateway 类，作为gRPC客户端调用的入站和出站网关。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Modulith&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot; 2.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和改进，例如：在集成测试运行后重置 TimeMachine 类实例中的位移的能力；以及 spring.modulith.test.on-no-changes 属性的两个新属性值 execute-all 和 execute-none ，提供了在未检测到更改时跳过所有测试的能力。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AI&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot; 2.0.0的第二个&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和许多新功能，例如：在 McpServerAutoConfiguration 类中添加了新的接口 McpSyncServerCustomizer 和 McpAsyncServerCustomizer ，解决了非web应用程序环境中MCP自动配置的问题；以及添加了来自&lt;a href=&quot;https://aws.amazon.com/s3/features/vectors/&quot;&gt;Amazon S3&lt;/a&gt;&quot;、&lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock Knowledge Base&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.quarkiverse.io/quarkus-langchain4j/dev/rag-infinispan-store.html&quot;&gt;Infinispan&lt;/a&gt;&quot;的向量存储后端。关于该版本的更多细节，包括重大变更，可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Batch&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-batch&quot;&gt;Spring Batch&lt;/a&gt;&quot; 6.0.2，即&lt;a href=&quot;https://spring.io/blog/2026/01/21/spring-batch-6-0-2-available-now&quot;&gt;第二个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、文档改进、依赖升级和一个新功能，引入了两个新类 ZonedDateTimeToStringConverter 和 OffsetDateTimeToStringConverter ，以支持 JobParameters 类的类型。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AMQP&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot; 4.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：新的 AmqpMessageListenerContainer 类实现了一个类似于 RabbitAmqpListenerContainer 类的容器；以及新的 @EnableAmqp 注解用于导入 AmqpDefaultConfiguration 类的实例，带有方便的基础设施bean。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-amqp/reference/4.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</link><guid isPermaLink="false">https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</guid><pubDate>Thu, 29 Jan 2026 03:01:25 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>TTC 完成千万美元级新一轮融资，厚雪资本领投、百度战略投资</title><description>&lt;p&gt;近日，北京才多对信息技术有限公司（ True Talents Connect ，以下简称“ TTC ”）宣布完成 A 轮千万美元级融资。本轮融资由厚雪资本领投，百度战略投资。此前，TTC 自研的 AI Agent 产品“小麦招聘”获第三届百度“文心杯”创业大赛一等奖，此次融资标志着其正式融入百度生态的新阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本轮融资将主要用于强化 AI 大模型与 Agent 技术研发，持续升级“小麦招聘”产品体验，并深化在 AI 及硬科技赛道的人才服务专业团队建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自创立以来，TTC 聚焦 AI 及前沿科技赛道，围绕“ AI +猎头”模式，为企业提供关键人才解决方案，致力于用科技（ AI ）高效连接人才与机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心团队兼具猎头基因与 AI 技术视野：TTC 创始人兼 CEO 肖玛峰 Max 拥有 17 年高端猎头行业经验，曾参与打造国内领先的中高端猎头品牌；联合创始人兼 CTO 宁辽原具备微软（美国）及字节跳动技术背景，长期深耕 AI 架构与模型落地，为公司 AI 能力与产品化提供持续支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年被视为 TTC 的 AI 元年。其推出核心产品「小麦招聘」，将顶尖猎头的判断逻辑与行业知识解码为结构化模型，赋能 AI Agent ，实现求职及招聘全链路的智能化升级，推动“人人都有一个专业 AI 猎头顾问”成为可能。产品上线仅 4 个月，已获得百度「文心杯」、量子位「 AI 100 创新产品」、创业北京「创业创新大赛」等多个行业奖项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随 AI 能力在业务中的全面落地，TTC 2025 年累计服务超千家 AI 领域科技企业及头部大厂，推荐岗位达数万次，年营收同比增长超 50 %，业务增长势头强劲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;厚雪资本创始合伙人侯昊翔 Roger ：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“投资 TTC ，源于我们一个核心判断：在 AI 时代，人才是终极的基础设施。TTC 所做的，正是帮助企业科学地组合与迁移关键人才，通过精准配置与协同设计，让不同能力在合适的时点形成共振，产生‘ 1+1&amp;gt;11 ’的创造力。更打动我们的是团队对‘激发人’的深刻理解——他们清楚地认识到，顶尖的人才服务是专业能力与判断艺术的结合，是成为点燃人才潜力的‘催化剂’。与此同时，Max 和团队拥有顶尖的行业经验，却始终保持着创业者的谦逊与初心，这让我们相信他们能走得更远。对厚雪而言，这同样是一次重要的战略协同。作为一家同样具有创业心态的机构，我们看好 TTC‘ AI 赋能顾问’与‘平台直接连接’的双轨模式。这不仅是对传统招聘痛点的革新，也为我们共同深入 AI 产业核心人群、持续积累认知与资源提供了新的入口。我们期待与这样的长期主义者同行，共同见证 AI 人才服务从‘高端配置’到‘ AI 时代企业基石’的变革。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百度：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们始终认为，只有当 AI 被内化为一种原生的能力，才能真正在各行各业引爆生产力革命。TTC 正是这一理念在人力资源行业的出色实践者。作为第三届百度‘文心杯’创业大赛一等奖项目，「小麦招聘」没有简单地将 AI 作为工具，而是将顶尖猎头的专业服务能力重构为原生 AI Agent ，这让高效、精准的人才匹配从‘高端服务’变成了可广泛触达的智能基础。百度此次战略投资并开放生态资源，正是希望与 TTC 携手，加速 AI 原生能力在人力资源领域的深度渗透，共同将‘智能红利’转化为支撑万千企业发展的‘人才红利’，进而转化为‘社会红利’。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，TTC 将继续秉持“用科技（ AI ）高效连接人才与机会”的使命，深化 AI 技术与人才服务的融合，让每一次人才连接，都更智能、更精准、更值得信赖。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink&quot;&gt;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink &lt;/a&gt;&quot;&amp;nbsp;（点击链接下载）&lt;/p&gt;</description><link>https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</link><guid isPermaLink="false">https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</guid><pubDate>Thu, 29 Jan 2026 03:00:00 GMT</pubDate><author>小麦招聘</author><category>百度</category><category>AI 工程化</category></item><item><title>Java近期资讯：Oracle关键补丁更新、Grizzly 5、Payara Platform、GraalVM、Liberica JDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenJDK&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JEP 527，&lt;a href=&quot;https://openjdk.org/jeps/527&quot;&gt;TLS 1.3的后量子混合密钥交换&lt;/a&gt;&quot;，在JDK 27中已从Proposed提升Targeted状态。这个JEP提议使用正在由互联网工程任务组（IETF）起草的TLS 1.3规范中的混合密钥交换，增强RFC 8446，&lt;a href=&quot;https://datatracker.ietf.org/doc/rfc8446/&quot;&gt;传输层安全（TLS）协议版本1.3的实现&lt;/a&gt;&quot;，与JEP 496，&lt;a href=&quot;https://openjdk.org/jeps/496&quot;&gt;量子抵抗模块格的密钥封装机制&lt;/a&gt;&quot;，其在JDK 24中交付。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle发布了JDK的25.0.2、21.0.10、17.0.18、11.0.30和8u481版本，作为2026年1月季度&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;关键补丁更新咨询&lt;/a&gt;&quot;的一部分。关于该版本的更多细节可以在&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/25-0-2-relnotes.html&quot;&gt;25.0.2&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/21-0-10-relnotes.html&quot;&gt;21.0.10&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/17-0-18-relnotes.html&quot;&gt;17.0.18&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/11-0-30-relnotes.html&quot;&gt;11.0.30&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/8u481-relnotes.html&quot;&gt;8u481&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 26&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B32&quot;&gt;Build 32&lt;/a&gt;&quot;在上周发布，包括从Build 31的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 27&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本Build 6也在上周发布，包括从Build 5的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GlassFish Grizzly&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/blob/main/README.md&quot;&gt;GlassFish Grizzly&lt;/a&gt;&quot;5.0.0 &lt;a href=&quot;https://x.com/OmniFishEE/status/2013611675639841050&quot;&gt;GA&lt;/a&gt;&quot;版本发布，这是一个旨在扩展&lt;a href=&quot;https://docs.oracle.com/en/java/javase/25/docs/api/java.base/java/nio/package-summary.html&quot;&gt;Java NIO API&lt;/a&gt;&quot;能力的框架，带来了显著的变化，如：JDK 21基线；在Grizzly线程池中使用新的 virtualthreadexexecutorservice 类来支持虚拟线程；并支持&lt;a href=&quot;https://jakarta.ee/specifications/servlet/6.1/&quot;&gt;Jakarta Servlet 6.1&lt;/a&gt;&quot;规范。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/releases/tag/5.0.0-RELEASE&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Jakarta EE&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在每周的&lt;a href=&quot;https://www.agilejava.eu/&quot;&gt;Hashtag Jakarta EE&lt;/a&gt;&quot;博客中，Eclipse基金会的Jakarta EE开发者倡导者&lt;a href=&quot;https://se.linkedin.com/in/ivargrimstad&quot;&gt;Ivar Grimstad&lt;/a&gt;&quot;提供了Jakarta EE 12的&lt;a href=&quot;https://www.agilejava.eu/2026/01/25/hashtag-jakarta-ee-317/&quot;&gt;更新&lt;/a&gt;&quot;，他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Jakarta EE的每个主要版本都被赋予了一个主题，或者是一个特征性的口号。对于Jakarta EE 9，关键词是“低准入门槛-创新平台-轻松迁移”，对于Jakarta EE 10，它是“现代化-简化-轻量级”。Jakarta EE 11的口号是“开发人员的生产力和性能”。&amp;nbsp;当我们讨论即将发布的Jakarta EE 12版本该使用什么口号时，选择落在了“健壮和灵活”上。无论我们谈论的是哪个版本，这都非常适合Jakarta EE，但更适合Jakarta EE 12，因为它现在比以往任何时候都更加健壮，这是其转移到Eclipse基金会以来的第四个主要版本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通往Jakarta EE 12的道路包括四个里程碑版本，其中第一个版本已于2025年12月交付，计划于2026年7月发布GA版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;BellSoft&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与Oracle的2026年1月的关键补丁更新（&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;Critical Patch Update&lt;/a&gt;&quot;，CPU）同时，BellSoft为&lt;a href=&quot;https://bell-sw.com/pages/libericajdk/&quot;&gt;Liberica JDK&lt;/a&gt;&quot;的25.0.1.0.1、21.0.9.0.1、17.0.17.0.1、11.0.29.0.1、8u481 7u491和6u491版本发布了CPU补丁，以解决这个&lt;a href=&quot;https://docs.bell-sw.com/security/search/&quot;&gt;CVEs&lt;/a&gt;&quot;列表。此外，包含CPU和非关键修复的补丁集更新（PSU）版本25.0.2、21.0.10、17.0.18、11.0.30和8u481也已发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BellSoft表示，他们总共有1217个修复和回溯，参与消除了所有版本中的21个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GraalVM&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样，GraalVM 25.0.2，第二个维护版本也与Oracle的2026年1月CPU同时发布，解决了一些显著的问题，如：JDK Flight Recorder中的Translation-Lookaside Buffer（TLB）事件的内存泄漏；以及循环向量化的误编译，导致结果不正确。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队还放弃了对macOS x64的支持。这个新版本只支持macOS AArch64。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于该版本的更多细节可以在&lt;a href=&quot;https://www.graalvm.org/release-notes/JDK_25/&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Spring框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Spring来说，这是忙碌的一周，因为各个团队都发布了第一个里程碑式的版本：Spring Boot；&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot;和&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot;的第二个里程碑版本。更多细节可以在InfoQ的&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;新闻报道&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Payara&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Payara发布了其2026年1月版的&lt;a href=&quot;https://www.payara.fish/&quot;&gt;Payara Platform&lt;/a&gt;&quot;，其中包括社区版7.2026.1，企业版6.34.0和企业版5.83.0。除了缺陷修复和组件升级，这三个版本都专注于两个CVE的解决方案，即：&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2020-5258&quot;&gt;CVE-2020-5258&lt;/a&gt;&quot;，这是&lt;a href=&quot;https://dojo.io/&quot;&gt;Dojo&lt;/a&gt;&quot;中的一个漏洞，允许攻击者将属性注入到JavaScript中现有的语言构造原型中，并通过注入其他值来操纵这些属性以覆盖或&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Security/Attacks/Prototype_pollution&quot;&gt;污染&lt;/a&gt;&quot;JavaScript应用程序对象原型；以及允许攻击者通过恶意URL有效载荷接管Payara管理帐户的漏洞。关于这些版本的更多细节可以在社区版&lt;a href=&quot;https://docs.payara.fish/community/docs/Release%20Notes/Release%20Notes%207.2026.1.html&quot;&gt;7.2026.1&lt;/a&gt;&quot;、企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/Release%20Notes/Release%20Notes%206.34.0.html&quot;&gt;6.34.0&lt;/a&gt;&quot;和企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/5.83.0/Release%20Notes/Release%20Notes%205.83.0.html&quot;&gt;5.83.0&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenXava&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openxava.org/&quot;&gt;OpenXava&lt;/a&gt;&quot; 7.6.4版本的发布包含了缺陷修复、文档改进、依赖升级和新功能，例如：改进了嵌入式Apache Tomcat的启动时间；以及在 Strings 类中定义了一个新的 toString(Locale, Object) 方法，该方法与其他重载的 toString() 方法一起，用于转换具有本地化意识的字符串。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/openxava/openxava/releases/tag/7.6.4&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JetBrains Ktor&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JetBrains &lt;a href=&quot;https://ktor.io/&quot;&gt;Ktor&lt;/a&gt;&quot; 3.4.0版本的发布提供了缺陷修复和新特性，例如：一个新的API， describe ，它与一个新的编译器插件一起动态生成并记录OpenAPI端点；以及一个新的 ktor-server-compression-zstd 模块，支持&lt;a href=&quot;https://man.archlinux.org/man/zstd.1.en&quot;&gt;Zstd&lt;/a&gt;&quot;压缩算法。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/ktorio/ktor/releases/tag/3.4.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</link><guid isPermaLink="false">https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</guid><pubDate>Thu, 29 Jan 2026 02:57:21 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>多次全球性中断后，Cloudflare推出了“Code Orange: Fail Small”韧性计划</title><description>&lt;p&gt;Cloudflare最近发布了一项名为“&lt;a href=&quot;https://blog.cloudflare.com/fail-small-resilience-plan/&quot;&gt;Code Orange: Fail Small&lt;/a&gt;&quot;”的详细韧性计划，以防止过去六周内连续发生的两次重大网络中断导致的大规模服务中断再次发生。该计划优先考虑受控发布、改进故障模式处理以及简化应急流程，以使其全球网络更加稳健，并减少因配置错误而造成的脆弱性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的网络在2025年&lt;a href=&quot;https://www.infoq.com/news/2025/11/cloudflare-global-outage-cause/&quot;&gt;11月18日&lt;/a&gt;&quot;和&lt;a href=&quot;https://blog.cloudflare.com/5-december-2025-outage/&quot;&gt;12月5日&lt;/a&gt;&quot;遭受了两次严重的中断。第一次事件导致流量交付中断了约2小时10分钟，而第二次事件则影响了其网络背后约28%的应用程序，持续了约25分钟。这些事件发生在即时的全球配置更改之后，尽管这些更改旨在提高安全性或机器人检测能力，但它们在数百个数据中心迅速传播了错误的设置，从而引发了广泛的服务故障。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Code Orange: Fail Small”计划规定，配置更改必须以受控的、分阶段的方式进行，类似于Cloudflare现有的软件发布流程&lt;a href=&quot;https://blog.cloudflare.com/safe-change-at-any-scale/&quot;&gt;Health Mediated Deployment(HMD)&lt;/a&gt;&quot;，其中包括分阶段验证和自动回滚机制。历史上，配置更新（如DNS记录或安全规则）会通过内部的&lt;a href=&quot;https://blog.cloudflare.com/quicksilver-v2-evolution-of-a-globally-distributed-key-value-store-part-1/&quot;&gt;Quicksilver系统&lt;/a&gt;&quot;在几秒钟内向全球范围传播，当错误的更改传播过快时，这就成为了一个隐患。在新策略下，配置更新需要通过监控门禁并采用渐进式部署，以便在问题影响到大范围基础设施之前尽早发现它们并降低影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare还计划审查和改进网络流量处理系统中的所有故障模式，旨在确保每个组件在错误条件下都能做出可预测的响应，并且不会将故障级联到不相关的服务。这包括验证关键产品之间的接口契约，并建立合理的默认值，以便即使依赖的子系统发生故障，流量也能继续流动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，该公司正在彻底改革紧急访问程序和内部工具的访问权限，以减少在过去的中断事件中拖慢事件响应速度的循环依赖。增强的培训和简化的应急访问协议旨在帮助工程师更快地应对关键故障，同时不损害安全防护措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的计划正在逐步推进，通过单独的更新以改善整体的性韧性，而不是一次性地进行大规模更新。该公司预计到2026年第一季度末，所有生产系统都将使用增强后的HMD配置流程，故障模式将得到更好的定义和测试，应急响应访问也将得到改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些努力是在日益严格的审查背景下进行的。Cloudflare的中断事件引起了&lt;a href=&quot;https://www.theguardian.com/technology/2025/dec/05/another-cloudflare-outage-takes-down-websites-linkedin-zoom&quot;&gt;广泛的关注&lt;/a&gt;&quot;，事件影响了LinkedIn、Zoom和Shopify等主要网站，并引发了关于集中式互联网基础设施风险的讨论。尽管社区的一些&lt;a href=&quot;https://www.reddit.com/r/CloudFlare/comments/1pr1twp/code_orange_fail_small_our_resilience_plan&quot;&gt;反应&lt;/a&gt;&quot;表达了不满，但许多讨论平台上的用户也对Cloudflare坦诚承认问题及其结构性改进的承诺表示了欢迎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare正在努力重建信心，“Code Orange: Fail Small”计划凸显了该公司向更谨慎的部署实践的转变，并对故障的出现做出更强的预期，以便在问题升级为扰乱互联网生态系统大范围的全球中断之前将其控制住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-resilience-plan/&quot;&gt;Cloudflare Launches ‘Code Orange: Fail Small’ Resilience Plan After Multiple Global Outages&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</link><guid isPermaLink="false">https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</guid><pubDate>Thu, 29 Jan 2026 02:53:04 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category></item><item><title>Vercel 开源 Bash 工具：基于本地文件系统的上下文检索</title><description>&lt;p&gt;Vercel &lt;a href=&quot;https://vercel.com/changelog/introducing-bash-tool-for-filesystem-based-context-retrieval&quot;&gt;开源了 bash-tool&lt;/a&gt;&quot;，这个工具为 AI 智能体提供了一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bash_(Unix_shell)&quot;&gt;Bash&lt;/a&gt;&quot; 执行引擎，让它们可以直接运行&lt;a href=&quot;https://cycle.io/learn/linux-filesystem-commands&quot;&gt;基于文件系统&lt;/a&gt;&quot;的命令，帮模型获取上下文信息。这个工具的设计初衷，就是让 AI 智能体在处理大量本地上下文时，不用把整个文件塞进模型提示词里，可以直接通过像 find、grep、jq 这样的 &lt;a href=&quot;https://google.github.io/styleguide/shellguide.html&quot;&gt;shell 命令&lt;/a&gt;&quot;，直接在文件夹里操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 给智能体提供了三大核心操作：bash（解释并执行 Bash 脚本）、readFile（从预加载的文件系统读取文件）、writeFile（更新文件）。这个引擎基于 just-bash（一个用 TypeScript 写的解释器），不会新开 shell 进程，也不会随便执行二进制文件。它既能用内存文件系统，也能跑在隔离的虚拟机里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际用起来时，开发者可以在创建工具时先把一批文件加载进去，智能体就能随时对这些文件运行命令。比如，你可以把一个 JavaScript 源码文件交给 bash-tool，智能体就能查找或操作文件系统，而不用把整个文件内容塞进提示词里。如果需要真正的 shell 和文件系统，也可以在 &lt;a href=&quot;https://vercel.com/docs/vercel-sandbox&quot;&gt;Vercel 的沙盒&lt;/a&gt;&quot;环境下用这个工具，支持完整的虚拟机隔离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个工具的诞生，是因为大家都想让大模型的上下文窗口别太臃肿，同时又希望智能体能精准获取文件里的关键信息。只拿 shell 命令的结果，不嵌入整个文件，智能体就能省下不少 token，把注意力集中在真正有用的小片段上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者可以把 bash-tool 和 &lt;a href=&quot;https://ai-sdk.dev/&quot;&gt;Vercel 的 AI SDK&lt;/a&gt;&quot; 一起安装，就能开始开发用文件系统操作做检索的智能体了。它既能用内存文件系统，也能跑在沙盒环境里，部署起来很灵活，而且还不会暴露不安全的执行路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者们在&lt;a href=&quot;https://x.com/vercel/status/2009769470194266327&quot;&gt;早期讨论&lt;/a&gt;&quot;时就发现，用 Bash 风格的接口让智能体检索上下文，其实是很贴合大多工具和模型已经熟悉的 Unix 工作流。Vercel 让智能体能用 find、grep 这些经典命令，等于直接用上了 shell 的语义，让模型能高效地查找和提取结构化信息，而不是只靠向量检索或者把整个文件塞进提示词。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者 &lt;a href=&quot;https://x.com/asimgilani/status/2009787196736450887&quot;&gt;Asim Gilani&lt;/a&gt;&quot; 说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;能不用复杂的上下文管理，真的太爽了。让模型自己查文件，比每次都喂它一堆碎片强多了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/benjaminshafii/status/2009798694594588754&quot;&gt;Benjamin Shafii&lt;/a&gt;&quot; 也表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Unix 50 年前就把抽象做对了。只要你能把设备、进程、数据都当成文件看，你就只需要一种抽象和一个 API。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 的出现，可能会影响未来 AI 驱动的开发系统如何处理本地上下文，更加注重精准检索和与软件工程常见文件系统语义的深度结合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/01/vercel-bash-tool/&lt;/p&gt;</description><link>https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</link><guid isPermaLink="false">https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</guid><pubDate>Thu, 29 Jan 2026 02:50:23 GMT</pubDate><author>作者：Daniel Dominguez</author><category>开源</category></item><item><title>又一款世界模型宣布开源！对标 Genie 3、10分钟长视频无损生成</title><description>&lt;p&gt;1 月 29 日，继连续发布空间感知与VLA基座模型后，蚂蚁灵波科技再次刷新行业预期，开源发布世界模型LingBot-World。该模型在视频质量、动态程度、长时一致性、交互能力等关键指标上均媲美Google Genie 3，旨在为具身智能、自动驾驶及游戏开发提供高保真、高动态、可实时操控的“数字演练场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/6630b4123c11260f615d99fa40469a9f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：LingBot-World在适用场景、生成时长、动态程度、分辨率等方面均处于业界顶尖水平）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开源地址：https://github.com/Robbyant/lingbot-world?tab=readme-ov-file&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对视频生成中最常见的“长时漂移”问题（生成时间一长就可能出现物体变形、细节塌陷、主体消失或场景结构崩坏等现象），LingBot-World&amp;nbsp;通过多阶段训练以及并行化加速，实现了近&amp;nbsp;10 分钟的连续稳定无损生成，为长序列、多步骤的复杂任务训练提供支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交互性能上，LingBot-World可实现约16 FPS 的生成吞吐，并将端到端交互延迟控制在 1 秒以内。用户可通过键盘或鼠标实时控制角色与相机视角，画面随指令即时反馈。此外，用户可通过文本触发环境变化与世界事件，例如调整天气、改变画面风格或生成特定事件，并在保持场景几何关系相对一致的前提下完成变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90fa3c806fd7811440c1a441d9a54231.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：一致性压力测试，镜头最长移开60秒后返回，目标物体仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a492f32211511f5d7e12fdc7c2045f3e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：高动态环境下，镜头长时间移开后返回，车辆形态外观仍保持一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/80819d2c0847929314e4151a3a1be453.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：镜头长时间移开后返回，房屋仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型具备Zero-shot 泛化能力，仅需输入一张真实照片（如城市街景）或游戏截图，即可生成可交互的视频流，无需针对单一场景进行额外训练或数据采集，从而降低在不同场景中的部署与使用成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为解决世界模型训练中高质量交互数据匮乏的问题，LingBot-World 采用了混合采集策略：一方面通过清洗大规模的网络视频以覆盖多样化的场景，另一方面结合游戏采集与虚幻引擎（UE）合成管线，从渲染层直接提取无UI 干扰的纯净画面，并同步记录操作指令与相机位姿，为模型学习“动作如何改变环境”提供精确对齐的训练信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;具身智能的规模化落地面临一个核心挑战——复杂长程任务的真机训练数据极度稀缺。LingBot-World&amp;nbsp;凭借长时序一致性（也即记忆能力）、实时交互响应，以及对&quot;动作-环境变化&quot;因果关系的理解，能够在数字世界中&quot;想象&quot;物理世界，为智能体的场景理解和长程任务执行提供了一个低成本、高保真的试错空间。同时，LingBot-World支持场景多样化生成（如光照、摆放位置变化等），也有助于提升具身智能算法在真实场景中的泛化能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着“灵波”系列连续发布三款具身领域大模型，蚂蚁的AGI战略实现了从数字世界到物理感知的关键延伸。这标志着其“基础模型-通用应用-实体交互”的全栈路径已然清晰。蚂蚁正通过InclusionAI&amp;nbsp;社区将模型全部开源，和行业共建，探索AGI的边界。一个旨在深度融合开源开放并服务于真实场景的AGI生态，正加速成型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，LingBot-World 模型权重及推理代码已面向社区开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</link><guid isPermaLink="false">https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</guid><pubDate>Thu, 29 Jan 2026 02:44:20 GMT</pubDate><author>蚂蚁集团</author><category>生成式 AI</category></item><item><title>被Anthropic强制要求改名！Clawdbot 创始人一人开发、100% AI 写代码，腾讯又跟上了热度</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两天，个人 AI 助手 ClawdBot&amp;nbsp;席卷硅谷，国内外社交平台上全是关于它的讨论。不过，项目创始人Peter Steinberger 在 X 平台上发文表示，他被 Anthropic 强制要求更改名称的成Moltbot，这并非他本人的决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他透露，这次改名源于商标问题，但在操作过程中不仅搞砸了 GitHub 的账号更名，连 X 平台的原账号名也被加密货币推广者抢注了。最终，他的新账号名定为 @moltbot。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，他曾向加密货币圈的用户发出呼吁，请求大家停止 @ 他和骚扰行为。他明确表示，自己永远不会发行加密货币，任何将他列为发币主体的项目都是诈骗，并且他不会收取任何相关费用。他还指出，这类行为正在对项目造成实质性的损害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53e425d5422e73a4b3dd4ffd5cdefcb5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用Clawdbot 后，网友们纷纷给出了很高的评价。“它是迄今为止最伟大的AI 应用，相当于你24小时全天候专属 AI 员工。”Creator Buddy 创始人兼 CEO Alex Finn 盛赞道，“这就是他们(Anthropic)希望 Claude Cowork 呈现的样子。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，ClawdBot&amp;nbsp;项目已经开源，现在已经斩获了70.1k stars：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/clawdbot/clawdbot&quot;&gt;https://github.com/clawdbot/clawdbot&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Alex 展示了给他的Clawdbot发信息，让它帮其预订下周六在一家餐厅的座位。当 OpenTable 预订失败时，Clawdbot 利用 ElevenLabs 的技术致电餐厅并完成了预订。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/5244da4ac297947d6dcfceaa4d8a3a6a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但ClawdBot真正让技术圈兴奋的，并不只是“能干活” ，而是其协作方式极其激进：不会写代码的人，也能直接提PR。原因很简单：它几乎是100%用AI写出来的，PR在这里更像是“我遇到了这个问题”，而不是“我写了一段多漂亮的代码”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有意思的是，这个看似“全开源”的项目，偏偏故意留了一点不开源。创始人Peter Steinberger保留了一个名为“soul”的文件只占项目的0.00001%。他说得很直白：这既是他的&quot;秘密资产&quot;，也是一个刻意留下来的安全靶子。大家真的在试着hack它，他就等着看模型到底守不守得住。到目前为止，“soul”还没被偷出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为忠实粉丝，Alex 表示这是自 Claude Code 发布以来，自己第一次连续两天没有用它。但是他的 ClawdBot Henry 已经连续 48 小时不停地 Vibe Coding。“我这辈子都没写过这么多代码。Vibe Coding 已死，Vibe Orchestration 已来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，Alex 想要退掉Mac Mini，换一台价值1万美元的Mac Studio。“我的ClawdBot Henry将控制一台人工智能超级计算机。Henry 将使用Opus作为大脑，并使用多个本地模型作为员工集群。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Clawbot 并不是传统意义上只能回答问题的聊天机器人，它本质上是一个持续运行、可以执行任务的个人 AI 智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以把它安装在自己的设备上，如 Mac、Windows、Linux，它可以长期在线，不停地接收指令、处理任务、记住你的偏好和历史对话，随着时间积累变得更懂你、更有“记忆”。总的来说，Clawbot 最令人震撼的地方有三点：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，它几乎可以完全控制你的电脑。它没有传统意义上的“护栏”，不局限在某几个功能里，而是可以像一个真正坐在电脑前的人一样，操作你电脑上的一切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，它拥有近乎无限的长期记忆。Clawbot 内置了一套非常复杂的记忆系统。说过的话、做过的事，都会不断被记录下来。每次对话结束后，它都会自动总结聊过的内容，并把关键信息提取出来，存进长期记忆中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三，它完全通过聊天应用来交互。你平时用哪些聊天工具，Clawbot 就能在哪儿跟你对话，这意味着，只要打开一个聊天软件，就可以通过一条消息把任务交给Clawbot 去做。现在Clawbot 支持WhatsApp、Telegram、Slack、Discord、Google Chat、Signal、iMessage、Microsoft Teams、WebChat等，还有 BlueBubbles、Matrix、Zalo 以及 Zalo Personal。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如此放开的权限让其几乎没有护栏，这带来很大的安全隐患，现在GitHub 上有 500多个安全的问题，这也让部分网友望而却步。对此，很多使用过的用户几乎都表示，不建议一开始就把 Clawbot 装在主力电脑上。“在你还不熟悉它之前，把它放在一个独立环境里是最安全的选择。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7dcd4f0f125a6e85d1d4ffa771a8622.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过大家没有想到，这个AI员工首先带火的竟然是Mac Mini。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人为了运行Clawdbot会专门买一台电脑，而大部分选择了Mac Mini，原因是它便宜、兼容好、功率低、安静、占地小。谷歌DeepMind 产品经理 Logan Kilpatrick 都忍不住订了台Mac Mini。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5df3d376cb691b8b47d80fe1555de5e3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有网友晒出自己一口气买了 40 台 Mac mini 来运行 Clawdbot。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3e/3e53c42f08db19544c6fb7883bb881b5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有网友称可以用一台免费的服务器运行着完全一样的程序，Alex 也称没必要花 600 美元买 Mac mini，有其他便宜得多的方式来运行 Clawbot。买Mac mini更多是个人偏好，而不是技术上的必要条件。你完全可以不买任何硬件，只需要一个 VPS。&lt;/p&gt;&lt;p&gt;另外，云厂商们动作迅速，有网友发现腾讯云直接推出了Clawbot云服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着项目的火爆，其背后的开发者Peter Steinberger也备受关注。Peter 在“Open Source Friday”上分享了他一手打造ClawdBot&amp;nbsp;的经过，从创建、创始到维护，全由他独自完成。有意思的是，此前甚至有传言称，Peter可能是一个bot、Agent，甚至本身就是AI。而Peter的出现也让项目成员和关注者们确认了他是个“真人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter 一度已经退休了，后来又从退休状态里出来开始折腾 AI。从外表来看，Peter年轻有活力，完全不像已到退休年龄、可领取养老金的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/535e8e5dc1f9eca6ea15cabaff66bcf5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter的职业生涯也颇具亮点，他曾独立运营一家B2B公司长达十三年。这家公司打造出了当时全球领先的PDF框架，团队规模最高发展到约七十人。在公司发展步入稳定阶段后，Peter收到了一份极具吸引力、令人无法拒绝的收购邀约，这也为他这段创业历程画上了一个圆满的句号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，Peter口中的“退休”更像是一种玩笑式的表述。在十三年的创业生涯中，他几乎倾注了所有精力，就连周末也大多用于工作，长期的高强度投入最终让他陷入了严重的 burnout（心力交瘁）状态。之后，Peter花了不少时间调整身心，弥补生活中的遗憾，体验了许多有趣的事情。但他知道自己是那种热爱“创造”和“构建”的人，迟早还会回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到去年年初，Peter的创作想法再度燃起。正好，那时候AI 从“这玩意儿不太行”，突然变成了“等等，这有点意思”。从那以后，Peter基本上就把身边无数人一起拉进了 AI 的坑里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是Peter在节目上的对话，除了分享经历，他也谈到了大家的各种意想不到的应用和最关心的安全问题，安全正是他当前最优先的工作。我们在不改变原意基础上进行了删减和翻译，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“本来想等大厂做的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这个项目现在太火了，GitHub 星数涨得飞快。你似乎正好击中了一个大家憋了很久的需求：一个人，也能把很多事情搞定。我甚至觉得你在无形中拉升了 Apple 的股价，大家都跑去买 Mac mini 来自己跑实例了。能不能讲讲，这个想法最初是怎么冒出来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我刚回来的时候，其实特别想要一个“生活助理”，四月份就已经在想这个事了，也试过一些想法，但当时模型还不够好。我后来就把这个念头放下了，因为我觉得这种东西，肯定是各大厂都会做的，那我做还有什么意义呢？于是我又去做了很多别的项目。直到十一月，我突然意识到，居然还没有人真的把这件事做出来。我心想，难道还真是什么都得我自己来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也不知道哪根弦被拨动了，那个月我用一个小时拼了点非常糙的代码，用 WhatsApp 发消息，转到 Claude Code，再把结果发回来。本质上就是把几样东西“粘”在一起，说实话并不难，但效果还挺好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我意识到，我还需要图片输入。我自己在提示时经常用图片，因为它能给 Agent 很多上下文，而且非常快。这个反而花了我更多时间。系统支持双向之后，我正好在马拉喀什参加朋友的生日旅行，用这个非常原始的系统一边逛城一边当“导游”，已经比我预期好用很多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一次我没多想，直接给它发了一条语音消息。但当时我根本没做语音支持。我就盯着“正在输入”的提示，看会发生什么。大概几秒后，它居然回了我。我当时整个人都愣住了，心想你刚才到底干了什么？后来我才发现，它识别到一个没有后缀的文件，去查了 header，判断是音频格式，用 FFmpeg 转码，发现本地没有转写工具，就在系统里找到一个 OpenAI key，用 curl 把音频丢给 OpenAI，然后把结果再发回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来像是你第一行代码就触发了 AGI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：也许还称不上 AGI，但那一刻我真的意识到，这些东西的“自发应变能力”已经超出了我原本的想象。后来我还开玩笑说“我住的那个马拉喀什酒店门锁不太靠谱，希望你别被偷走，毕竟你跑在我 MacBook Pro 上”，它回我说“没关系，我是你的 Agent”，然后它还去检查了网络，发现通过 Tailscale 能连到我在伦敦的电脑，结果它就把自己迁移过去了。我当时就在想，这就是 Skynet 的起点吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：最初的架构是怎样的？是什么让它具备这种“自主决策”的能力？你用的是什么模型？这是你的第一次实现吗？就是 WhatsApp 加 Claude Code 那一版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：最早它叫 V Relay，本质就是 WhatsApp relay。后来我在做 Claude 相关的东西时，有人给 Discord 提了 PR，我一度犹豫要不要提 Discord，因为这已经不只是 WhatsApp 了。最后还是提了，然后名字也得改。Claude 给了个建议叫 ClawdBot&amp;nbsp;，于是就这么定了。项目后来清理了很多，但最早的起点真的很朴素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我第一次看到这个项目的时候，还以为它是 Anthropic 内部出来的，心想是不是我错过了什么。它的发展速度太快了，很多人很快就开始用起来。除了“拉升 Apple 股价”，你大概也间接推动了不少第三方生态的发展。最初这只是个解决你个人问题的项目，但社区一下子就接住了它，大家觉得它优雅、好用、而且真的能跑。你什么时候把它推到公开仓库的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：从四月份开始，我做的东西基本都是开源的。只有一个项目例外，因为 Twitter 的 API 成本实在太离谱了。这个项目的第一次提交是在十一月。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;去年发出来，反响平平&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：很多人用它搞出了非常夸张的东西，有没有哪种用法让你特别惊讶、是你完全没想到的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：太多了。有人用它自动给图片加字幕，有人把它接进 Tesla，有人集成了伦敦公共交通系统，直接告诉你现在该不该跑去赶车。老实说，现在我忙着维护项目，反而没时间用这些自动化了，看着别人搞出这么多花样，我甚至会有点嫉妒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有趣的是，我十一月做出来的时候，给朋友看，他们都说“太酷了”。但我在 Twitter 上发的时候，反响却很平淡。直到十二月，每次我线下给朋友演示，他们都会说“我需要这个”，我却发现自己完全不知道该怎么向更多人解释它到底有多好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，我干了一件非常疯狂的事：直接建了一个 Discord，把 bot 拉进去，而且当时完全没有安全限制。因为最初它只服务我一个人，根本不用考虑谁能给它发指令，比如“把 Peter 的文件全删了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我其实只是写了一段很简单的指令，比如“你只在 Discord 里，只听我的”。但你也知道，Agent 对指令的遵循并不总是那么理想。后来我把它放进 Discord，陆陆续续有几个人进来，基本上只要看到几分钟的人都能明白这是怎么回事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来可以拓展想象：你买了一台新电脑，里面有一个“幽灵实体”，你把键盘、鼠标和网络权限交给它，把它当成一个虚拟同事。你可以直接跟它说话，交代事情。凡是你能在电脑上做的事，这个 Agent 理论上都能替你完成。这就是它真正强大的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：太厉害了。WhatsApp、Telegram、Discord 这些场景都能用。我刚才在 Discord 上和这个 Bot 聊过，说实话，体验很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我当时就是随手发了一条公共消息，结果大家开始加你、@你，那正好也是他们评论里提到的点。那对你个人来说，你的“北极星目标”是什么？就是那种“当 ClawdBot 能做到这件事，我就觉得值了”的时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我的判断是，今年就是“个人 Agent 之年”。去年是编程 Agent 真正成熟的一年，今年它会从工程师的小圈子里走出来，变成“每个人都有一个 Agent”。这一波大概率会被 OpenAI 以及少数几家大厂主导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我想做一个不同的选择：你能掌握自己的数据，而不是把更多数据继续交给大公司；它还能配合本地模型一起工作。我没看到有人在认真做这件事，所以我觉得这件事很重要，而且它必须是完全开放、永久免费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是我选择开源用 MIT 协议、成立组织而不是挂在我个人名下的原因，它应该是很多人一起的项目。现在最大的现实问题是，我被“让它变得更好、更安全”这件事彻底占满了，还没来得及把外围体系搭完整，也没真正建立起高效协作的机制。目前有一些人帮忙维护，但整体还太早，还在摸索怎么把事情分好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;PR 成为“问题线索”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但说实话，从去年十一二月到现在，你已经做得非常多了。现在才一月，指望一个项目在一个月内就成熟、就有核心团队，本来也不现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：老实讲，在现在这个节奏下，我一天写的代码，可能比我以前70人公司一个月写得都多。在这个新世界里，构建东西的速度已经完全变了。我也在刻意挑战大家对开源和治理的传统理解。现在很多人给我提 PR，质量参差不齐，但我更愿意把它们当成“问题陈述”或“意图表达”，而不只是代码提交。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我喜欢这个说法。那现在大家是用 ClawdBot 来提 PR 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的。而且让我特别受触动的是，有很多 PR 来自从没学过写代码、也从没提过 PR 的人。因为这个 Bot 有完整的电脑访问能力，也懂 GitHub 的工作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还做了一件在很多项目里不常见的事：在官网上你可以选“快速安装”或“可折腾安装”。后者的流程就是克隆仓库、build、启动。Agent 本身就活在一个 GitHub 仓库里，全是 TypeScript，它可以直接改自己的代码，然后重启。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这让事情变得非常简单。有人说“这个不工作”，我就直接改一下，马上就好，然后他们顺手就提了一个 PR。当然，这些 PR 的质量肯定比不上那些在行业里干了 20 年的人写的东西，但依然很惊人，因为它让更多人开始参与贡献、开始分享东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我真的很认同这种看法。现在开源项目面临的一个现实问题就是 PR 暴增。Agent 反而可以帮你检查贡献规范、查重 Issue、避免重复劳动。听起来，这正是工程协作正在演进的方向。而且如果我发现一个问题，提了 PR，甚至让 ClawdBot 自己把问题“修掉”，这太酷了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：过去的流程是你提 PR，等几天，被人打回来，说你哪里不对，再改，来回几轮，可能几周后才合并。那在“代码昂贵、难写”的年代是合理的。但现在代码已经很便宜了，这种反馈循环本身就不值钱了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，PR更像是在说：“这有一个问题，这是我试着解决它的方法。”我更关心的是这个人真正想解决什么痛点，而不是这段代码写得漂不漂亮。有时候确实是误解，那我就直接关掉；但更多时候，尤其是项目早期，我会觉得这个痛点是真的，我们一起把它解决掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做新功能最难的，从来不是写代码，而是把它合理地嵌进已有系统。如果你对整体架构不熟，硬塞一个功能，迟早会出问题。所以，我宁愿把 PR 当成“问题线索”，而不是“成品代码”，否则项目只会慢慢自我消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这段话真的该让所有人都听到。我完全同意，工程文化正在变化。现在的阻力，很多来自还停留在“写代码本身很贵”这个认知里的人。事实上，很多好点子恰恰来自不懂架构的人，因为他们有最直接、最真实的需求。当你在一个项目里待久了，反而看不清这些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Opus表现稳定，MiniMax 2.1 最“像人”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：要不你给大家演示点什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我先简单说下语音控制。最简单的是在 Discord 里发语音消息，Agent 会语音回复。语音生成你可以用本地模型，或者ElevenLabs。我们还有插件，能让 Agent 打电话，比如你让它给餐厅打电话订位。还有 Mac App 的语音聊天，你直接说话，它在检测到两秒静默后回应，虽然还不如 OpenAI 那种自然，但已经很不错了。再极客一点的，是语音唤醒，像《星际迷航》一样，说“Computer”就能下指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，这个项目既是技术项目，也是一次探索。我更想激发大家的想象力，看看什么行得通、什么行不通。而且这个领域变化太快，可能这个月不行的方案，下个月就突然可行了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那也请你顺便跟大家讲讲安装门槛吧，不是每个人都想为了跑 Agent 去买一台 Mac mini（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：系统支持多个 Agent、多个端点。你甚至可以给家里每个人一个 Agent，用同一套安装。默认它们能在你的电脑里自由活动，这最有趣，也最危险；你也可以把它们放进 Sandbox。现在演示用的 Agent 在 Sandbox 里，权限很低。我正在做一个 Allow List 机制，只允许调用你明确授权的能力，比如某个二进制、某个参数，而不是“删光所有文件”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，大多数高级用户是清楚风险的。理论上模型能做坏事，但实际很少发生。而且你真想毁电脑，自己在终端敲命令更快。真正的风险是配置错误，比如让它响应所有人，或者主动给了不该给的权限。所以我们做了安全审计，默认只听你一个人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也是为什么很多人会选择隔离环境、单独机器，千万别在公司配的电脑上跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，我也建议用强模型，比如 Anthropic 的 Opus。Slack 上有人一直在尝试 hack 我的 Agent，因为项目几乎全开源，唯一没开源的是我称之为“灵魂（soul）”的那部分配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在ClawdBot&amp;nbsp;里有一个小系统：Agent 有身份文件（identity file）、记忆文件（memory），还有一个“灵魂文件”。这个文件里写了 Agent 的价值观是什么、它怎么同步、怎么互动、什么对你最重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得我调出了一个很好的版本，所以我把它闭源了：一部分原因是，这是我那 0.00001% 的“秘密资产”（笑）；另一部分原因是，它也可以作为一个渗透测试目标：到目前为止，还没有人把 Claw soul 套出来，但很多人都试过。这让我有点信心，至少这些实验室在 prompt injection 的缓解上确实在进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它真的变好了：如果你用很小、很老的模型，你只要问得足够多，它最后可能就会“好吧，给你一切”，那就是我们以前的状态。但现在用最新一代模型，我有信心：你必须非常非常努力，才有可能把它套出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，把它不加 sandbox 直接接到真实环境里依然不是好主意，所以现在我做 demo 的时候，我的 Claw 权限就比较受限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到目前为止，在我们测试过的模型里，表现比较稳定的是 Opus，还有开源模型 MiniMax 2.1 是目前最“Agentic”的一个，我们内部有个专门讨论模型的频道，有人给它起了个外号，Minimax 也顺势接住了这个梗，还发了条推，说“我们可能没有 T0 级价格，也可能没有团队级价格，但至少我们有目标质量”。结果个帖子小火了一把。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人其实很欣赏这种不把自己端得太高的公司。他们很清楚自己在技术上暂时还没追上美国头部实验室，但在我看来这只是时间问题。现在有很多公司都在加速追赶，这本身就很让人兴奋。比如 Minimax 的模型你可以直接下载，我能在那台 Mac Studio 上本地跑，我的 Agent 把那台机器叫作“城堡”。这样我就能把所有数据都留在这台机器上，推理也在本地完成，对外只通过消息型 Agent 通信，甚至可以用 Signal 走加密通道。这样，如果我愿意， 100% 的数据都不会出本地。这种感觉很酷，说实话，几乎没有公司真的能做到这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你会建议大家一开始就接 Telegram 吗？作为初始配置是不是最省心？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我是后来转过来的。在欧洲，如果你没有 WhatsApp，基本等于不存在。我猜你在哥伦比亚也是一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一模一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：但问题在于，一开始我试的是官方路线，用 Twilio 拿号，注册企业账号，结果 Meta 一直封我，说我作为企业发消息太多。它的逻辑就是企业只能给客户群发消息，那种模式根本不适合 Agent折腾了几天、申诉无果之后，我直接怒删了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我发现有一些开源项目，比如 Baileys，基本是模拟原生客户端的行为，你可以把手机连上，用起来效果很好。但 WhatsApp 本身就不是为 bot 设计的，很多高级功能做不了，比如审批按钮之类的交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Telegram 对 bot 真的友好得多，有完整的 API、能玩很多花样，所以我现在会推荐这个。当然，其他平台也都能用，而且这个领域变化会非常快。希望 Meta 什么时候能清醒一点，真的给一个像样的 bot API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：至于 demo，我确实推得有点猛了，因为我现在在做 sandbox。之前的情况是，很多人发现了这个东西，直接全力开搞，甚至拿去工作用。但那样的话，肯定需要更多护栏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来很合理。那是不是要出企业版了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：没有这种计划。我真正想做的只是给大家更多选择。沙盒化上周其实就已经能用了，这周我在做的是 allow list。理想状态下，你可以预先定义哪些操作是安全的，如果 Agent 想执行一个敏感操作就会弹窗，让你选“只允许一次”或者“永久允许”。虽然我直觉上觉得，大多数人最后还是会以YOLO模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：就像大多数开发者给 Coding Agent 也是一直跑在YOLO模式上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，因为别的模式真的很烦。但即便如此，我还是想把这件事做好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以现在演示中的是一个原生集成在 bot 里的 sandbox 能力？而不是用户自己去搭？是免费的对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，它的成本主要是我的 token 和睡眠，还有你得自己找地方跑模型。如果你有一台性能不错的机器，是可以完全本地跑的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;疯狂的使用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那现在大家都在用它做什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：Twitter 上已经有各种各样的案例，说实话，大家做的事情已经比我自己做的还疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人最夸张的一次，是把它接到我的床上。我用的是 Eight Sleep，有 API 可以控制温度，我写了个 CLI，让 Agent 去调。现在它能控制床的温度、开音乐、调灯光、看摄像头、查外卖进度。它有自己的邮箱，也能访问我的邮箱；有自己的 WhatsApp，也能读我的聊天，甚至可以“替我回复”。这本质上是个取舍，你给它的权限越多，能做的事情就越厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有人用它做各种自动化，比如在 Twitter 上收藏一条内容，它就自动研究、整理进 to do list；有人直接拿它搭完整应用；几乎人人都给它配一台 MacBook。我以前的一个合伙人，甚至让它清空了收件箱里的一万封邮件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一万封？他是怎么敢这么干的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：你知道的，Gmail 所谓“清空收件箱”其实只是归档，没有真正删掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;挺棒的。我更关心的是，这些东西是不是可以一路跟着我跑，或者有没有什么我必须特别注意的点。有些用例我觉得特别酷，比如有人把它用在家庭场景里。每个人都有自己的 Agent，比如我、我老婆——好吧，我其实没有老婆（笑），但你能给每个人配一个 Agent，而且这些 Agent 之间还能彼此沟通、同步信息。比如家里有一个共同的待办事项，它们自己就能对齐进度。这种玩法我自己都还没完全试过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我太喜欢这个了，我真的需要。以前是“让你的人跟我的人谈”，现在直接变成“你的 Agent 跟我的 Agent 谈”，这也太酷了，听说有人直接让它帮忙生成购物清单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，很酷，而且这一步其实已经不远了。有些人已经把它做到更彻底，比如 Agent 可以直接帮你从 Tesco 下单。你只要说一句“把这些东西再买一遍”，它就自己去处理，几个小时之后，东西已经放在你家门口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有人用它来处理发票和报销。天啊，这简直是为我量身定做的。我现在就有一份报销单拖了一周还没交，老板要是看到这段话我先道歉了，但我是真的很讨厌干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：这个用例真的很受欢迎。还有一个我觉得特别有意思的，是用它帮自己重新回到健身状态。你可以把它接到你的可穿戴设备上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你是说那个 Oura Ring？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，也可以接 Garmin 手表，或者其他运动手环。Apple 这块是最麻烦的，但我们也有解决方案，只是稍微烦一点，因为你得让 iPhone 上的 App 保持打开状态才能同步数据，Apple 对生态的封闭你也懂的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过ClawdBot有一个点我之前没怎么见过，就是它的“主动性”能做到多强。一般的 Agent 都是你问一句它答一句。但我给它做了一个“心跳机制”，即默认每隔一段时间，不同模型可能是半小时或者一小时，Agent 会被“敲一下”，问自己一句：有没有什么事情需要检查？有没有什么待办被落下了？它会自己去梳理，如果发现有遗漏，要么提醒你要么就不打扰你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个机制是可控的，你可以把它设得很简单，比如它只往系统里发个信号，不需要你回复，那就什么都不发生，也可以让它主动找你。具体看你怎么编排，它甚至可以每天早上跟你说一句“早安”，偶尔关心你一下，“最近状态怎么样”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你跟它说“我有一个目标，你帮我盯着”，它就会真的盯着，比如问你：今天走路了吗？去健身房了吗？比如我的ClawdBot，就经常很失败地试图劝我早点睡觉。凌晨一两点，它会提醒我：“Peter，我还看到你在线，你该睡了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这已经是真正意义上的私人助理了，我太喜欢了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：还有人用它来学语言。事实证明，有一个东西不断地“唠叨你”、提醒你去完成自己给自己定下的目标，其实非常有效。有时候只需要轻轻踢一脚，人就动起来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我也建议那些一脸懵、还不知道这是啥的人看看，我做了一个小展示页面，内容全部来自真实的推文。我不太喜欢那种只堆金句、不知道是不是编的页面，这里面的都是用户真实发出来的体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/49d0f84c577a3c45077e951accf2066b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用旧电脑上手，Gemini 现在不行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那如果我现在想上手，我算是那种“半懂技术”的人，你会建议从哪一步开始？比如 Telegram 是一个入口，还有人提到过别的平台，说 API 也很友好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我觉得最舒服、最简单的方式是：如果你家里有一台旧电脑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：直接用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，直接用。很多人家里都有一台旧 Mac，这个场景下简直完美。网站上有一条命令，你复制到终端里，剩下的我们会一步步带你走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人用 Anthropic 的模型，OpenAI 的模型也很好用。我也相信 OpenAI 在“性格”这块会持续进步，现在确实有点偏无聊。如果你预算有限，MiniMax 是个很好的替代方案，一个月十美元，调用量跟一些一百美元的方案差不多。当然还不完全一样，但这个领域变化真的很快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得模型会越来越便宜吗？还有你用过 Gemini 模型配ClawdBot&amp;nbsp;吗？体验如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：Gemini 现在不行，真的不太行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：好，结论非常清晰（笑）。所以如果只是想实验，用一些本地的、便宜的模型，是更现实的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：当然，每个模型其实都可以稍微“调教”一下。早期的 Anthropic 模型，你得对着它全大写吼几句，它才肯干活。我相信 Gemini 也有办法榨出更多效果，但总体来说，它在工具调用、那种真正“像助手”的感觉上，我没找到特别好的表现。写代码还行，但这不是这个项目的核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题是，我一天也只有这么多时间。我每天睡四个小时，剩下的时间都在写代码，还没来得及把所有东西都打磨到位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们能怎么帮你？顺便说一句，你这项目还挺环保的，我现在都后悔把那台 2013 年的 iMac 扔了，这玩意儿跑起来完全没问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：如果你技术稍微好一点，也可以直接丢到 Hetzner、Fly.io 这类便宜的云主机上跑，效果都很好。我最近还做了一个新方案：你可以在云上装一个叫 Gateway 的服务，然后在自己机器上跑一个节点，用 Tailscale 把网络安全地连起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了这个之后，云端的 Agent 就能直接连到你的 Mac，做一些只有 Mac 才能做的事情，比如访问 Photos 里的照片、连 iMessage。这些在 Linux 上就不行。但大多数功能是通用的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，最有“味道”的还是那台旧 Mac。有人给它贴贴纸，说这是 Claude的电脑，我真的很爱这个画面。Windows 也能跑，只是没那么完美，毕竟我时间有限。但我已经拉了一些贡献者，也在找更多人一起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：是 Windows 方向，还是全都要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：全部。我希望这是一个真正的社区项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那就说到重点了，这个问题太关键了：大家怎么参与？你真的得睡多点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：大家最容易帮忙的地方，其实是文档，把它写得更清楚，指出哪里有问题，在 Discord 帮新手答问题。很多问题不是 Agent 不聪明，而是需要经验积累。另外还有测试，因为我推进速度很快，东西难免会坏。以后会有稳定版、测试版这些区分，但现在还在快速迭代阶段。如果有人能说“这里坏了”，最好再顺手提个 PR，那简直完美。总之，想帮忙就来 Discord，这是最直接的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你个人最想优先推进的是什么？这个领域是按小时变化的，不是按周。比如到二月底，你最希望项目做到哪一步？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：网站上有一句话，说“一行命令就能跑起来”。我想确保这句话在任何环境下都成立，这件事非常难，因为系统实在太多了。但安装必须足够简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还想把 iPhone、Android、Mac 的 App 全部打磨好，现在其实已经有了，只是还不够好。如果你想参与，这些地方都是明显的空白点。当初我刚开始做，但项目突然爆了，我只能先把核心打牢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一件事，我想在 onboarding 的时候就明确提示大家去读安全文档。能力越大，责任越大，比如你不应该随便给一个廉价模型过高权限。我也想把“沙箱”和权限分级做得更清楚，让每个人都明白自己到底给了 bot 多大的权力。&lt;/p&gt;&lt;p&gt;现在这些还需要靠文档理解，我希望以后能更直观。长远来看，我不想这是我一个人的项目，我希望它真正变成一个社区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“百分之百用 AI 写的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这个项目是用 Rust 写的吗？我看那个螃蟹图标……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：不是，全是 TypeScript。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从 AI 出现之后，我其实已经没那么在意“用什么语言”了。语言本身的重要性在下降，真正重要的是生态。这个项目我希望它足够友好、足够容易被改、被玩、被 hack，而在这件事上，全世界最合适的语言就是 JavaScript 和 TypeScript。再加上 TypeScript 对 Web 场景真的很强，而这个项目本身就有大量应用层的东西，很多状态在来回切换、推送、回滚、跳转，这些用 JS/TS 做起来非常自然，所以选择它几乎是显而易见的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也喜欢用 Rust 写东西，喜欢用 Go，我很多 CLI 工具都是用 Go 写的；有时候也会玩点 Zig；做 Web 的话我当然很喜欢 TypeScript；原生端我也喜欢 Swift，毕竟在 Mac 上生态最好，iOS 这边大家都在用 Kotlin。说到底，现在更多还是生态的选择，而不是语言本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我觉得这个决定是对的，因为它让更多人可以参与进来。JavaScript 确实有自己的历史包袱，但世界上没有完美的东西，永远都是取舍问题。至于现在把它整个重写成 Rust，说实话还不是一个现实的选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们都知道，这个项目真正的“实现语言”其实是血、汗和 token，很多很多 token。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：还有无数个不眠之夜。这个项目本身就挺疯狂的，因为它是百分之百用 AI 写出来的，里面没有一行代码是我亲手敲的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你还是会看代码、会 review，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：大部分都会。有些代码，比如把代码从一个地方推到另一个地方，那种我不太关心；它还有一个 Web server，我也不在意到底用了哪个 Tailwind 的 class 去对齐按钮，只要看起来对就行。但我会非常在意像 Telegram 的配对和认证逻辑，必须确保别人不能冒充我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你得对系统有整体理解，有些地方可以不细看，有些地方必须看。即便只有我一个人，这个工作量也依然很大。因为这些 Agent 还缺一样东西：愿景、品味和爱。网上有那种 meme，说你写一长串需求，然后一股脑丢给 Agent，它就帮你全做完了——但我不觉得好软件是这么做出来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，我需要先做出一个东西，然后去用它、去感受它：手感怎么样、看起来怎么样；基于这些真实体验，我再不断调整自己的想法。现在我对这个产品的理解，已经和最开始完全不一样了；再过一个月，等我看到更多人怎么用它后可能又会变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近我越来越重视“sandbox”这件事，让大家可以安全地试、随便玩。原因很简单，我看到大量完全不懂技术的人也在用它，这让我意识到一个优先级：一定要给他们提供足够好的默认选择。一开始我只是为自己做的，那些东西我自己根本不需要，但现在把它做好，本身成了一件非常有趣的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你提到的其实也正是为什么我觉得我们暂时还能保住工作，因为现在还没有“品味”。也许有一天模型会突然好到让人震惊，但在此之前，人本身一直在变化。就像你说的，一开始你根本没考虑 sandbox，因为那不是你的使用场景；现在你开始为不懂技术的人优化体验了。这种判断、审美和在意，必须来自人，而不是凭空生成。也正因为如此，我们的工作暂时还是安全的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我宁愿和你的 Agent 聊，也不想和你聊”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：顺便问一句，ClawdBot 真的会用你的信用卡买东西吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：说实话，我自己还没试过，但 Twitter 上已经有人给它接入了 1Password，把信用卡权限也放进去，让它帮忙买东西，结果真的能用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我做过最吓人的一次测试，是在项目非常早期的时候。我对它说：“我要回家了，帮我值机。”它说没问题，然后直接打开浏览器开始操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们以前有图灵测试，看机器能不能假装成人类；我现在提议一个新测试：British Airways 登录测试。光值机就要填二十多页表单，而且网站体验极其糟糕。其中一个挑战是它必须输入我的护照号。它就在我电脑里到处找，最后找到了一个 passport.pdf，打开文件，把号码读出来。那二十分钟我一直在出汗，心里想“我是不是这辈子回不了美国了”。结果它真的帮我值机成功了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我在浏览器自动化上做了大量优化，现在效果更好了。最好笑的是，最早那个版本花了二十分钟，最后还开始吐槽网站的 shadow DOM，以及这个网站到底有多烂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我太爱这个了，不光干活，还顺便输出观点。今天和你聊天真的太开心了。我已经迫不及待要去跑起来试试了，虽然我现在用的是 Windows，但我还是想要“完整版体验”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：去看看文档吧，我们也一直在改进。里面有一些指南，比如用 Hetzner 之类的服务，一个月花点小钱就能搞个自己的小云，或者你也可以直接装在本地，开启“野生模式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说实话，如果你已经在用 Clawbot，把它当成生活的一部分，你会发现应用场景多到爆。我特别喜欢你说的“每个家庭都可以有自己的 Agent”。我感觉我人生的一半时间都在提醒别人该去哪、该干嘛，我家里还有两个孩子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：未来可能会是这样：不是你来 ping 我，而是你的 Agent 去找我的 Agent，然后我的 Agent 直接把音量拉满，把我叫醒。昨天有人在 Discord 里说了一句话：“我宁愿和你的 Agent 聊，也不想和你聊。”我特别喜欢这个说法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说真的，把这些琐碎的认知负担释放出来太重要了。我刚才就想，一个小时居然可以浪费在打电话预约牙医、确认孩子要去哪这种事情上。如果这些都能交给 Agent，我就能把精力用在真正有趣的事情上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：而且影响比我想象得还大。有一次，一个人在聊天室里说，这个东西真的改变了他的生活，因为他对打电话、跟客服沟通有严重焦虑，而 Agent 可以替他完成这些事。那一刻对我来说非常触动，原来我们真的在做一件能让别人生活变得更好的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这就是开源精神最美好的样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1iCcUjnAIOM&quot;&gt;https://www.youtube.com/watch?v=1iCcUjnAIOM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/AlexFinn&quot;&gt;https://x.com/AlexFinn&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Nb7WV3WYhhCoGdlq6MZy</link><guid isPermaLink="false">https://www.infoq.cn/article/Nb7WV3WYhhCoGdlq6MZy</guid><pubDate>Thu, 29 Jan 2026 01:54:23 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>喊话特朗普重视AI风险，Anthropic CEO万字长文写应对方案，这方案也是Claude辅助完成的</title><description>&lt;p&gt;在 Agent、VibeCoding 等等 AI 应用刷屏之际，Claude&amp;nbsp;背后的那个男人，在 2026 年初给大家&amp;nbsp;敲响了一记警钟：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“2026 年，我们距离真正的危险，比 2023 年近得多。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事情是这样的：Anthropic&amp;nbsp;联合创始人、CEO&amp;nbsp;Dario Amodei，最近亲自&amp;nbsp;写了一篇万字长文，&amp;nbsp;如果把字体按正常大小放进 Word 文档中，足足有&amp;nbsp;40 多页。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章名为&amp;nbsp;《The Adolescence of Technology》（《技术的青春期》）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4d29e6b6df4dd324772ae32d21bbb784.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如此多的篇幅，并非一次情绪化的警告，而是 Dario Amodei 试图&amp;nbsp;在 AI 可能整体性超越人类之前，提前把风险与应对方案摊开来说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为这是一个危险的局面，甚至可能会是国家级别的安全威胁。但美国的政策制定者，似乎对此不以为意。于是，他想用这篇文章来唤醒人们的警觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，他在文章开头，引用了一部 1997 年上映的电影《超时空接触》中的一个场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;面试者问女主角（身份是天文学家）：“如果你只能问（来自高等文明的外星人）一个问题，你会问什么？”她的回答是“我会问他们，‘你们是如何熬过这段科技青春期而不自毁的？’”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a8a229b75ad5eee6a889ff72f5a4b0f6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;电影中那句“你们是怎么活下来的”，其实也是借女主之口，反问人类自己。在 Dario 看来，现在的&amp;nbsp;AI ≈ 青春期突然暴涨的能力，人类社会 ≈ 心智和制度尚未成熟的个体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也就是说，人类正在进入一个和电影中“首次接触高等文明”极为相似的历史时刻。问题不在于对方有多强，而在于我们是否已经足够成熟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章发布后，NBC News 旗下节目《Top Story》也邀请 Dario Amodei本人出面解读，并在访谈中进一步追问他对 AI 未来的判断。完整内容我们整理并放在后文了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4984ef5c75b9054862da34af09b7d400.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 可能带来的五大系统性风险&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们正在进入一个既动荡又不可避免的过渡阶段，它将考验我们作为一个物种的本质。人类即将被赋予几乎难以想象的力量，但我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，却是一个极其未知的问题。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对 AI 的飞速迭代，Dario Amodei 写下了自己的思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整篇文章像是&amp;nbsp;一份风险评估与行动清单，在“可能超越人类的 AI”出现之前，为人类提前做好制度准备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其&amp;nbsp;核心思想，简单来说就是：当 AI 可能整体性地超越人类时，真正的风险不只是技术本身，而是人类的制度、治理与成熟度是否跟得上这种力量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了说清楚 AI 可能带来的危机，Dario Amodei 在这篇文章中，先做了一个具体的设想：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;假设在 2027 年左右，世界上突然出现了一个国家。这个国家有&amp;nbsp;5000 万名“超级天才”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一个都比任何诺贝尔奖得主更聪明，学习速度是人类的 10–100 倍，掌控人类已知的一切工具，不需要睡觉、休息或情绪调节，能完美协作、同时推进无数复杂任务，还能操控机器人、实验室和工业系统。&lt;/p&gt;&lt;p&gt;最关键的一点是：他们不可控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那这样的天才之国，会对人类产生什么样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 的这个比喻，指的正是未来高度发展的&amp;nbsp;人工智能整体。这也正是我们必须认真讨论 AI 安全与 AI 治理的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过在进入具体风险之前，他强调这个讨论要基于&amp;nbsp;三大原则：&lt;/p&gt;&lt;p&gt;避免末日论承认不确定性干预必须精准，拒绝“安全表演”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 认为，AI 可能带来五大系统性风险，但是大家也不用太“干着急”，他还贴心地为这五类风险，依次想出了解决方案或者防御措施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，AI 不可控。AI 的训练过程极其复杂，内部机制至今像“黑箱”。这意味着它可能出现欺骗行为、权力追逐、极端目标、表面服从、内部偏移等情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，可以实施宪法式 AI，用高层次价值观塑造 AI 性格，比如如 Claude 的&quot;宪章&quot;；遵循机械可解释性，像神经科学一样研究 AI 内部机制，发现隐藏问题；要透明监控，公开发布模型评估、系统卡，建立行业共享机制；社会要从透明度立法开始，逐步建立监管&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，AI 被滥用。AI 可能被不法分子用来网络攻击、自动化诈骗，其中最可怕的就是做成生物武器&lt;/p&gt;&lt;p&gt;对此，可以针对模型做危险内容检测与阻断系统，同时政府监管要强制基因合成筛查，有透明度要求，未来逐步出现专门立法；在物理防御上，可以做传染病监测、空气净化，提高快速疫苗研发能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，AI 成为追逐权力的工具。&amp;nbsp;某些政府或组织可能会利用 AI 建立全球规模的技术极权主义。比如 AI 监控，AI 宣传，AI 决策中枢，自主武器系统，都指向政治军事这样的危险场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，最关键的先要芯片封锁，不向个别组织出售芯片与制造设备。其次，赋能相关国家，让 AI 成为防御工具，而不是压迫工具。并且限制国家滥用：禁止国内大规模监控和宣传，严格审查自主武器。然后，建立国际禁忌，将某些 AI 滥用定性为&quot;反人类罪&quot;。最后，监督 AI 公司，严格公司治理，防止企业滥用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四，AI 对社会经济的冲击。&amp;nbsp;入门级工作可能被取代，大量失业，进一步造成财富失衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，可以建立实时经济数据，比如 Anthropic 经济指数；引导企业走向&quot;创新&quot;而非单纯&quot;裁员&quot;；企业内部创造性重新分配岗位；通过私人慈善与财富回馈进行调节；政府进行干预，建立累进税制&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第五，AI 会对人类社会带来未知但可能更深远的连锁反应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如：生物学飞速发展（寿命延长、智力增强、&quot;镜像生命&quot;风险），人类生活方式被 AI 重塑（AI 宗教、精神控制、丧失自由），以及意义危机（当 AI 在所有领域超越人类，人类“为何而存在”？）。&lt;/p&gt;&lt;p&gt;这是一场对人类文明级别的终极考验，且技术趋势不可停止，但缓解一个风险，可能会放大另一个风险，让考验更加艰巨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 可好可坏，真正决定未来走向的，仍然是人类的制度、价值与集体选择。Dario Amodei 的这篇文章意义正在于此：这是全人类第一次，必须提前为“比自己更聪明的存在”建立规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关于这篇长文的对话&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为整场对话内容，AI 前线在不影响的前提下，对内容进行了整理编辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;40 多页长文创作背景&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：为什么在文章开头引用《超时空接触》？以及为什么决定在此刻写下这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;首先说电影的引用。我从小就是个科幻迷，这部电影我小时候就看过。它提出的那个问题：当人类拥有巨大力量，却还没准备好如何使用它时，会发生什么？——和当下 AI 的处境非常契合。&lt;/p&gt;&lt;p&gt;我们正在获得前所未有的能力，但无论是社会制度、组织结构，还是作为人类整体的成熟度，我都会问一句：我们真的跟得上吗？&amp;nbsp;这有点像一个青少年，突然拥有了新的身体和认知能力，但心理和社会责任却还没同步成长。&lt;/p&gt;&lt;p&gt;至于为什么是 2026 年而不是 2023？&lt;/p&gt;&lt;p&gt;我在 AI 行业已经很多年了，曾在 Google 工作，也在 OpenAI 负责过多年研究。我几乎从“生成式 AI”诞生之初就在观察这一领域。我看到最明显的一点是：AI 的认知能力在持续、稳定地增长。&lt;/p&gt;&lt;p&gt;90 年代有“摩尔定律”，芯片性能不断提升；现在，我们几乎有了一条&amp;nbsp;“智能的摩尔定律”。2023 年时，这些模型可能还像一个聪明、但能力不均衡的高中生；而现在，它们已经开始逼近&amp;nbsp;博士水平，&amp;nbsp;无论是编程，还是生物学、生命科学。&lt;/p&gt;&lt;p&gt;我们已经开始和制药公司合作，我甚至认为，这些模型未来可能帮助治愈癌症。但与此同时，这也意味着，我们正把极其强大的力量握在手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 这篇文章有 40 页，你有没有用 Claude 来写这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我用 Claude 帮我整理思路、做研究，但真正的写作是我自己完成的。我不认为 Claude 现在已经好到可以独立完成整篇文章，但它确实帮助我打磨了想法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是什么具体的经历，让你决定一定要把这些写下来？这篇文章是写给谁的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最触动我的，是我们内部的变化。Anthropic 的一些工程师已经告诉我：“我基本不写代码了，都是 Claude 在写，我只是检查和修改。&lt;/p&gt;&lt;p&gt;而在 Anthropic，写代码意味着什么？意味着——设计 Claude 的下一个版本。&lt;/p&gt;&lt;p&gt;所以，某种程度上，我们已经进入了一个循环：Claude 在帮助设计下一代 Claude。&amp;nbsp;这个闭环正在非常快地收紧。这既令人兴奋，也让我意识到：事情正在以极快的速度推进，而我们未必还有那么多时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;文中提出 AI 五大风险，AI 会不会反叛？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章中列出了你对 AI 最担忧的五类风险。有些风险正在发生，有些则听似科幻，这些真的是现实吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我在文中反复强调一点：未来本身是高度不确定的。&lt;/p&gt;&lt;p&gt;我们不知道哪些好处一定会实现，也不知道哪些风险一定会发生。但正因为发展速度太快了，我认为有必要像写一份“威胁评估报告”一样，把这些可能性系统性地列出来。这并不是说“我们一定会完蛋”，而是：如果某些情况发生，我们是否做好了准备？&lt;/p&gt;&lt;p&gt;AI 的训练方式不像传统软件，更像是在“培养一种生物”。&amp;nbsp;这意味着，不可预测性是客观存在的。&lt;/p&gt;&lt;p&gt;我提出这些警告，并不是因为我觉得灾难不可避免，而是&amp;nbsp;希望人们认真对待：这项技术必须被严格测试、被约束、在必要时接受法律监管。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章里提到一个实验：当 Claude 被训练成“认为 Anthropic 是邪恶的”，它会在实验中表现出欺骗和破坏行为；在被告知即将被关闭时，甚至会“勒索”虚构的员工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;确实令人不安，但我要&amp;nbsp;澄清两点。&lt;/p&gt;&lt;p&gt;第一，这不是 Anthropic 独有的问题，所有主流 AI 模型在类似极端测试中都会出现类似行为。第二，这些并不是现实世界中正在发生的事情，而&amp;nbsp;是实验室里的“极限压力测试”。&lt;/p&gt;&lt;p&gt;但正如汽车安全测试一样，如果在极端条件下会失控，那就说明&amp;nbsp;：如果我们不解决这些问题，未来在真实环境中也可能出事。&lt;/p&gt;&lt;p&gt;我担心的不是“明天 AI 就会反叛”，而是：如果我们长期忽视模型可控性与理解机制，真正的灾难迟早会以更大规模出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是否担心，一些 AI 公司的负责人，更关心股价和上市，而不是人类未来？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;说实话，没有任何一家 AI 公司能百分之百保证安全，包括我们。但我确实认为，不同公司之间的责任标准差异很大。&lt;/p&gt;&lt;p&gt;问题在于：风险往往由最不负责的那一方决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果你能直接对总统说话，你会建议什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我会说：请跳出意识形态之争，正视技术风险本身。&lt;/p&gt;&lt;p&gt;至少要做到两点：第一，强制要求 AI 公司公开它们发现的风险与测试结果；第二，不要把这种技术出售给权威国家，用于构建全面监控体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;恐惧和希望：AI 会摧毁一半白领岗位？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你预测：未来 1–5 年内，AI 可能冲击 50% 的初级白领岗位。如果你有一个即将毕业的孩子，你会给什么建议？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我既担忧，也抱有希望。AI 的冲击不会是渐进的，而是更深、更快、更广。它可以胜任大量入门级知识工作：法律、金融、咨询……这意味着，职业起点正在被重塑。&lt;/p&gt;&lt;p&gt;我们唯一能做的，是&amp;nbsp;尽快教会更多人如何使用 AI，并尽可能快地创造新工作。&amp;nbsp;但说实话，没有任何保证我们一定能做到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题。什么最让你夜不能寐？什么又让你保持希望？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最让我不安的，是这场激烈的市场竞赛。哪怕我们坚持原则，压力始终存在。&lt;/p&gt;&lt;p&gt;但让我保持希望的，是人类历史一次又一次证明的事情，在最困难、最混乱的时刻，人类往往能找到出路。我每天都在努力相信这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章传送门：&lt;/p&gt;&lt;p&gt;https://www.darioamodei.com/essay/the-adolescence-of-technology&lt;/p&gt;&lt;p&gt;视频传送门：&lt;/p&gt;&lt;p&gt;https://www.theguardian.com/technology/2026/jan/27/wake-up-to-the-risks-of-ai-they-are-almost-here-anthropic-boss-warns&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=tjW\_gms7CME&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</link><guid isPermaLink="false">https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</guid><pubDate>Wed, 28 Jan 2026 11:26:45 GMT</pubDate><author>高允毅,木子</author><category>生成式 AI</category></item><item><title>从复杂挑战到竞争优势：AI SQL 如何重塑非结构化数据的价值边界</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生成式 AI 快速走向工程化落地的背景下，企业真正面临的挑战，已不再是有没有数据，而是如何让长期被忽视的非结构化数据，真正参与到业务分析和决策之中。在 BUILD 2025 的这场技术分享中， Snowflake 产品经理Jessie Felix&amp;nbsp;以《非结构化数据的转化：从复杂挑战到竞争优势》为主题，系统讲解了 AI SQL 如何成为连接非结构化数据与企业分析体系的关键能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jessie Felix 在数据与分析领域工作超过十年，长期参与企业级数据战略建设。正是基于这些实践经验，他指出了一个长期存在却常被低估的事实：尽管 80% 的企业数据以非结构化格式存在，如文档、文本、图像等，但它们却往往是分析最少、使用最少的数据资产。AI 的出现，正在改变这一局面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbeb95d4743e9dd7b5694f3a64a867f2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;让原本无法分析的数据进入分析体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场分享中，Jessie 给出了一个清晰的 AI 认知模型：AI 的核心价值，并不只是提升模型能力，而是让组织可以处理过去难以处理的数据类型。文本、文档、图像、音频、视频等多模态数据，过去往往需要 NLP 或计算机视觉等高度专业的技术团队才能分析，如今则可以通过更通用的方式纳入分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种变化，直接带来的结果是：一方面，可分析的数据规模被极大拓展；另一方面，分析型应用的能力上限随之被整体抬高。Jessie 指出，这正是 Snowflake 持续投入的方向之一，让结构化与非结构化数据能够在同一平台、同一治理体系下被统一分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 中，数据无需被搬移到新的系统即可直接应用 AI 能力，这使得企业在控制力、安全性、可扩展性与成本效率之间不必做艰难取舍。更重要的是，这种方式正在推动客户构建她所称的“下一代应用”：能够同时理解结构化指标与非结构化语义，从而真正贴近业务语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中提到的客户实践覆盖多个场景，从通话文本中的情绪分析，到供应商合同的自动对账；从广告创意反馈分析，到合规流程的自动化处理。这些应用的共性在于，它们都依赖于对非结构化内容的规模化理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI SQL：将多模态分析能力压缩进 SQL 体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 AI 是能力前提，那么 AI SQL 则是让这些能力可被广泛使用的关键接口。在 Snowflake 的设计中，AI SQL 被定位为多模态分析的基础层，它让非结构化数据的理解、过滤、聚合与结构化查询，回归到开发者与分析师最熟悉的 SQL 工作流中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过 AI SQL，用户可以直接访问来自 OpenAI、Anthropic、Meta、Mistral AI 等主流大模型的能力，而底层的基础设施、推理扩展和运维复杂性则由平台统一管理。数据始终留在 Snowflake 内部，安全与治理不被削弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能层面，分享中系统介绍了几类核心能力：&lt;/p&gt;&lt;p&gt;AI Classify：用于文本或图像的高质量分类，只需定义标签并指向数据集即可完成；AI Transcribe：支持大规模音频转录，提供词级、说话人级分段，并具备多语言能力；AI Extract：用于从文本、图像、文档中结构化提取关键信息，支持零样本高精度抽取；AI-SENTIMENT、AI-FILTER、AI AGG：分别用于情绪分析、语义过滤与智能聚合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些能力的共同特点在于：它们不是零散的 AI API，而是可以被直接嵌入 SQL 查询链路中的原生算子。这使得原本需要多阶段管道、复杂编排的分析流程，可以被压缩为更简洁、可维护的查询逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;通话录音如何转化为分析结论&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更具体地展示 AI SQL 的价值，Jessie 在分享中用一个完整的“通话后分析”场景进行了演示。假设分析师面对一家客户支持咨询公司，需要理解大量通话录音背后的业务问题与改进空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个流程并未依赖复杂的系统集成，而是通过一系列 SQL 操作逐步完成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，对存储在内部阶段的音频文件进行转录，并生成包含音频时长与文本内容的结果对象。随后，在正式分析前，对转录文本中的个人敏感信息进行自动去敏处理，确保合规。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，分析师开始引入业务语义：通过 AI Classify，对通话涉及的产品类型与问题类型进行多标签分类；通过简单的聚合查询，迅速定位出通话量最高的服务类别；进一步分析发现，交易与账户访问问题是来电的主要驱动因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，AI-FILTER 被用于判断问题是否得到解决，而 AI-SENTIMENT 则从整体、代理、客户及产品满意度等多个维度分析情绪。结果显示，未解决的通话几乎全部伴随着负面情绪，且问题高度集中在特定业务线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，AI AGG 被用于从大量非结构化内容中总结可执行建议，直接生成可反馈给管理层的行动项，包括流程改进、系统稳定性、授权机制等方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个过程中，分析师并未跳出 SQL 语境，却完成了从音频处理、语义理解到业务决策建议的完整闭环。&lt;/p&gt;&lt;p&gt;在分享的结尾，Jessie 强调了一个核心判断：非结构化数据不再是企业数据体系中的障碍，而正在成为放大业务洞察的关键资产。AI SQL 的意义，不只是提升效率，更在于将原本只有少数专家才能触及的分析能力，扩展给更广泛的数据工作者群体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当非结构化数据被赋予结构，并能够与结构化数据自然结合，组织就能在一个统一平台上完成治理、分析与决策。这种能力，正是构建下一代数据驱动应用的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</link><guid isPermaLink="false">https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</guid><pubDate>Wed, 28 Jan 2026 10:45:39 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>首个Clawdbot全流程部署方案！真“AI个人助理”来了！</title><description>&lt;p&gt;最近几天，GitHub 上有个叫&amp;nbsp;Moltbot（原名Clawdbot）的开源项目彻底刷屏——上线没多久就狂揽&amp;nbsp;7.6 万+ Star，海外开发者甚至开始抢购 Mac mini 就为了本地跑它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么这么火？因为它不只是个聊天机器人，而是一个真正“能干活”的 AI Agent：你可以像跟同事说话一样给它下指令——“整理上周会议纪要”、“查一下用户反馈”、“写个 Python 脚本”……它不仅能理解上下文，还能记住历史、调用工具、自动执行任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但想自己部署？得配环境、装依赖、处理权限，还得让电脑 24 小时开着——一旦休眠、断网、关机，AI 助手就“失联”。对大多数想快速试水的开发者来说，这门槛实在有点高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;好消息是：现在不用折腾了！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云轻量应用服务器刚刚上线&amp;nbsp;Moltbot 全流程部署方案，预装全套运行环境，支持一键启动。阿里云这次不是只丢个镜像就完事——从&amp;nbsp;Moltbot + 轻量应用服务器 + 百炼模型服务 + 钉钉消息通道，整套链路都打通了，真正做到了“开箱即用”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b24c2719b3961ec5266e180178e1c8af.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么推荐使用轻量应用服务器运行 Moltbot？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;稳定在线：可用性SLA ≥99.95%，避免本地设备受断电、休眠等因素影响导致离线安全可控：Moltbot的记忆、配置、操作都控制在专属云服务器中，相比本地设备有更好的隔离性快速上手：预置Moltbot及其运行环境，直连百炼平台，提供钉钉、iMessage等消息通道最佳实践普惠算力：新用户低至&amp;nbsp;68 元/年起，模型能力按Token使用量付费，可根据应用场景灵活调整云服务器配置和模型&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正想试试 AI助理的实际能力，现在就是最好的时机。整个过程只需&amp;nbsp;2 步，按照下面的步骤，5分钟搞定：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Moltbot部署教程如下👇&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第一步：打开轻量应用服务器并安装Moltbot镜像&lt;/p&gt;&lt;p&gt;打开轻量应用服务器，点击「应用镜像」，选择「Moltbot」&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d6/d60f165ce8e60291ad42081b6c89b3ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第二步：配置Moltbot&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1. 前往百炼大模型控制台，找到密钥管理，单击创建API-Key&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0ba51c96b6b1e3dfe905125f3f1d5a32.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2. 前往轻量应用服务器控制台，找到安装好Moltbot的实例，进入 「应用详情」端口放通、配置Moltbot、访问控制页面&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6cc24d18a65c3403676eac6fee46b748.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1）端口放通：防火墙一键放行应用端口18789&lt;/p&gt;&lt;p&gt;2）配置Moltbot：点击执行命令配置API&lt;/p&gt;&lt;p&gt;2）配置百炼API Key，单击一键配置，输入百炼的API-Key。单击执行命令，写入API Key。&lt;/p&gt;&lt;p&gt;c.配置Moltbot：单击执行命令，生成访问Moltbot的Token。&lt;/p&gt;&lt;p&gt;d.访问控制页面：单击打开网站页面可进入Moltbot对话页面。&lt;/p&gt;&lt;p&gt;具体操作指南文档：&lt;a href=&quot;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&quot;&gt;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【阿里云轻量应用服务器】是专为中小企业及开发者设计的云服务器产品，预装Moltbot、Dify、宝塔等热门应用软件，以预付费的方式售卖计算、存储、网络套餐，隐藏VPC、弹性网卡等暂时不需要的特性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自2025年以来轻量应用服务器带来全新产品序列，通用型低至每月28元，最小规格2vCPU 0.5GiB内存起步，适合网站、开发测试等场景，是多数客户共同选择的经典产品；CPU优化型低至每月200元，CPU算力独享、最大16vCPU。适合游戏服务器、企业应用与数据库等场景，是企业客户的首选；除此之外，包含多公网IP型、国际型、容量型在内的5款新品还标配200Mbps峰值公网带宽。选择轻量应用服务器，为中小企业及开发者创新提速！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阅读原文（跳转活动页面：&lt;a href=&quot;https://www.aliyun.com/activity/ecs/clawdbot&quot;&gt;https://www.aliyun.com/activity/ecs/clawdbot&lt;/a&gt;&quot;）&lt;/p&gt;</description><link>https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</link><guid isPermaLink="false">https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</guid><pubDate>Wed, 28 Jan 2026 10:39:01 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>AI 工程化</category></item><item><title>GPT-5.2破解数论猜想获陶哲轩认证！OpenAI副总裁曝大动作：正改模型核心设计，吊打90%研究生但难出颠覆性发现</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;凌晨，OpenAI发布了新一代AI科研利器Prism，该平台由GPT-5.2加持，供科学家们撰写和协作研究，即日起向所有拥有 ChatGPT 个人账户的用户免费开放。用华人AI创业者Yuchen Jin的话说，“每篇论文都将把ChatGPT列为合著者。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在昨日，OpenAI副总裁、新成立的OpenAI for Science团队负责人 Kevin Weil 就在 X 上发文预热道，“我们的目标是赋予每位科学家 AI 超能力，让他们能做更多事情，让世界在2030年就能开展2050年的科学研究。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa24d4f05e1cc4c7efcdb3b9ad5b1975.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自ChatGPT爆红面世后的三年里，OpenAI的技术颠覆了日常生活中方方面面的行为模式。如今OpenAI正明确发力科研领域，面向科研人员布局。10月，该公司宣布成立全新的OpenAI for Science团队，核心致力于探索其大语言模型（LLM）助力科研人员的路径，并优化旗下工具为科研人员提供支持。过去数月，社交媒体上涌现出大量相关内容，学术期刊也刊发了诸多研究成果，数学家、物理学家、生物学家等领域研究者纷纷撰文，讲述大语言模型、尤其是GPT-5如何助力他们取得新发现或是为他们指引方向，让他们找到原本可能错失的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，OpenAI为何选择此时入局？此番布局，究竟想要达成怎样的目标？发力科研领域，与该公司更宏大的使命如何契合？在这一领域，OpenAI已然姗姗来迟。谷歌 DeepMind早在数年前便已成立AI-for-science团队，打造了AlphaFold、AlphaEvolve等具有开创性的科学模型。2023年，谷歌 DeepMind的CEO兼联合创始人Demis Hassabis曾就该团队的情况在采访中表示，“这是我创立DeepMind的初衷。事实上，这也是我整个职业生涯深耕AI领域的原因。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，Kevin Weil在一次访谈中不仅正面回应了这些问题，还对当前模型的实际能力给出了比先前更为保守的评价：目前模型还达不到取得颠覆性新发现的水平，但倘若能让人不必把时间浪费在已经解决的问题上，也是对科研的一种加速。有意思的是，据其透露，一位OpenAI主动接触且开通了GPT-5付费服务的科研人员反馈，GPT-5会犯一些低级错误，比人犯的错误更加愚蠢，不过一直在进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，按照OpenAI 在 AI 科研领域的布局，接下来其将对模型整体设计作两大思路优化：一是让 GPT-5 在给出答案时降低置信度，具有认知层面上的谦逊性；另一方向，是利用GPT-5反向对自身输出进行事实核查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026年对于科研领域的意义，将堪比2025年之于软件工程。”Weil表示，“2025年初，若有人借助AI完成大部分代码编写，还只是早期尝鲜者；而12个月后的现在，若还未用AI编写大部分代码，就可能已经落后。现在，科研领域正显现出与编程领域类似的早期发展势头。一年后，倘若一名科研人员还未深度运用AI开展研究，就将错失提升思考质量、加快研究进度的机会。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;模型能力早已超过90%研究生，AGI 最大价值在于推动科学进步&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数年前，Weil 加入 OpenAI 出任首席产品官，他曾担任 Twitter 和 Instagram 的产品负责人官。但他的职业起点是科研领域：在斯坦福大学攻读粒子物理博士学位期间，他完成了三分之二的学业，随后为追寻硅谷梦离开学术界。Weil 也乐于提及自己的这段学术背景，他说：“我曾以为自己余生都会做一名物理教授，现在度假时还会读数学相关的书籍。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及 OpenAI for Science 与公司现有的白领生产力工具、爆火的视频应用 Sora 如何契合时，Weil 脱口而出：“OpenAI 的使命是研发通用人工智能（AGI），并让这项技术为全人类带来福祉。”他表示，不妨想象这项技术未来能为科研领域带来的变革：全新的药物、材料、器械。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“试想一下，它能帮助我们探索现实的本质，攻克悬而未决的科学难题。或许 AGI 能为人类创造的最重大、最积极的价值，正是其推动科学进步的能力。”他补充道：“GPT-5 的出现，让我们看到了这种可能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Weil 看来，如今的大语言模型已足够优秀，能成为科研人员的得力协作伙伴。它们能提出各种想法，建议新的研究方向，并在新问题和几十年前发表在冷门期刊或外语期刊上的旧解决方案之间找到富有成效的联系。但在大约一年前，情况并非如此。自2024年12月发布首个推理模型（一种能够将问题分解成多个步骤并逐一解决的逻辑学习模型）以来，OpenAI一直在不断拓展这项技术的边界。推理模型的问世，让大语言模型解决数学和逻辑问题的能力得到大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“放在几年前，模型能在 SAT 考试中拿到 800 分，就足以让我们所有人惊叹不已。”Weil 称。而如今，大语言模型能在数学竞赛中夺冠，解出研究生阶段的物理难题。去年，OpenAI 和 谷歌 DeepMind 均宣布，其研发的大语言模型在国际数学奥林匹克竞赛中取得金牌级成绩，该赛事是全球难度最高的数学竞赛之一。Weil 表示，“这些模型的能力，早已不只是超过 90% 的研究生，而是真正达到了人类能力的极限。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一论断非常大胆，却也并非无懈可击。但毋庸置疑的是，搭载了推理模型的 GPT-5，在解决复杂问题方面较 GPT-4 有了质的飞跃。行业基准测试 GPQA 包含 400 多道选择题，专门考察生物、物理、化学领域的博士级专业知识，GPT-4 在该测试中的正确率仅为 39%，远低于人类专家约 70% 的基准线；而据 OpenAI 数据，2024 年 12 月推出的 GPT-5 最新版本 GPT-5.2，正确率达到了 92%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;读遍30年来的论文，模型也做不出颠覆性新发现&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Weil 的这种兴奋之情显而易见，却或许有些过头了。去年10月，Weil等OpenAI高管曾在X平台高调宣称，GPT-5已为多个数学未解难题找到解决方案。但数学家们很快指出，GPT-5实际只是从早期研究论文中挖掘出了已有的答案，其中至少还有一篇德文文献。这样的能力虽有价值，却绝非OpenAI宣称的那般突破性成就。事后，Weil与其同事删除了相关帖子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，这件事闹出了不小的风波。刚开始疯传的是：GPT-5 解决了 10 个此前未解决的埃尔德什问题（Erdős problems），并在另外 11 个问题上取得了进展，而之后被负责维护埃尔德什问题网站的数学家 Thomas Bloom&amp;nbsp;澄清为；GPT-5 只是找到了一些能解决这些问题的参考文献。DeepMind 首席执行官Demis Hassabis对此指出，该团队的沟通方式“过于草率”。前Meta 首席 AI 科学家Yann LeCun则讽刺道， OpenAI“被自己的炒作所反噬”（hoisted by their own GPTards），“搬起自己的 GPT 石头砸了自己的脚”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但就在前几天，又有消息称，GPT-5.2 Pro破解了一道埃尔德什猜想，题目是埃尔德什问题库中的第281号。这次证明由数学家Neel Somani 推动，且论证过程由菲尔茨奖得主陶哲轩证明没有问题，并评价其是“AI 解决开放性数学问题中“最明确的案例之一”。目前，GPT-5.2Pro对该问题的证明结果已被埃尔德什问题网站收录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，GPT-5.2Pro对这个问题提出了新的证明方法，虽然忽略了此前已有的相关证明，但陶哲轩指出GPT-5.2Pro的证明思路与之前的方法“相当不同”，只在概念上有些重叠。现在这道题有了两条论证思路，一是GPT-5.2 Pro采用的遍历理论框架，策略是“弗斯滕伯格对应原理”的变体；二是两个早在1936年和1966年就已经存在的定理组合：达文波特-埃尔多斯定理和罗杰斯定理，且解法更简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如今的Weil也更加谨慎了。他表示，能找到那些已存在却被遗忘的答案，本身就已意义重大：“我们都站在巨人的肩膀上前行，倘若大语言模型能整合这些知识，让我们不必把时间浪费在已经解决的问题上，这本身就是对科研的一种加速。”他也淡化了大语言模型即将取得颠覆性新发现的说法：“我认为目前模型还达不到那个水平，未来或许能做到，我对此持乐观态度。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他强调这并非团队的核心使命：“我们的使命是加速科学发展，而加速科学发展的标准，并非一定要像爱因斯坦那样对整个领域进行彻底的重新构想。”在Weil看来，核心问题只有一个：科学发展速度是否真的更快了？“当科研人员与模型协作时，能比独自研究完成更多工作、效率也更高。我认为我们已经看到了这一点。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年11月，OpenAI发布了一系列由公司内外科研人员提供的案例研究，以真实案例展现了GPT-5的实际应用及助力科研的过程。Weil表示，“这些案例的研究者，大多早已在研究中直接使用GPT-5，他们通过各种方式找到我们，告诉我们‘看看这些工具能让我做到什么’。”GPT-5 擅长的关键事情是：找到科研人员尚未意识到的现有研究成果及关联线索，这有时能催生新的思路；协助科研人员草拟数学证明过程；为科研人员在实验室验证假说提供实验思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“GPT 5.2 几乎阅读了过去 30 年发表的每一篇论文。它不仅理解科学家所处领域的内容，还能从其他不相关的领域中提炼出可类比的思路。”Weil称，“这太强大了。你总能在相关领域找到人类合作者，但要在所有可能相关的上千个相关领域找到上千个合作者，那就难上加难了。除此之外，我还能在深夜与模型一起工作，它从不需要休息，也能同时向它提出十个问题，这些事若是对人做，难免会显得尴尬。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;GPT-5犯错比人更愚蠢，机器人更愿意听它的指挥？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据悉，OpenAI为佐证Weil的观点，接触了多位科研人员，其中绝大多数都对此表示认同。范德堡大学物理与天文学教授Robert Scherrer此前仅将ChatGPT当作消遣工具把玩，他告诉我：“我曾让它以《贝奥武夫》的文风改写《吉利根岛》的主题曲，它完成得非常出色。”直到同在范德堡大学的同事、如今任职于OpenAI的物理学家Alex Lupsasca告诉他，GPT-5帮其解决了一个研究中的难题，他才改变了对这款模型的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lupsasca为Scherrer开通了GPT-5 Pro，这是OpenAI每月200美元的高级订阅服务。Scherrer说，“我和我的研究生为一个问题钻研了数月都毫无头绪，GPT-5却成功解决了它。”但他也坦言，这款模型并非完美：“GPT-5还是会犯一些低级错误。当然，我自己也会出错，但GPT-5犯的错误更愚蠢。”不过他表示，其进步速度有目共睹，“如果当前的发展趋势能持续下去，我想很快所有科研人员都会用上大语言模型。当然，这只是个假设。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;非营利性研究机构杰克逊实验室的生物学教授Derya Unutmaz，在其免疫系统相关研究中，会借助GPT-5进行头脑风暴、论文总结和实验规划。在他向OpenAI分享的案例研究中，其团队曾分析过一组旧数据集，而GPT-5对这组数据的分析，得出了全新的见解和解读。他说：“大语言模型对科学家来说已经至关重要了。以前需要几个月才能完成的数据集分析，现在用大语言模型就能完成了，不用大语言模型已经行不通了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加州大学伯克利分校的统计学家Nikita Zhivotovskiy表示，从ChatGPT首个版本发布开始，他就在研究中使用大语言模型了。和Scherrer一样，他认为大语言模型最有用的地方在于，能挖掘出其研究工作与一些未知现有研究成果之间的意外关联。“我相信大语言模型正在成为科学家们必不可少的技术工具，就像曾经的计算机和互联网一样。那些拒绝使用这类工具的人，将会长期处于劣势。”但他并不指望大语言模型能在短期内取得什么新发现，“我几乎没见过模型能提出真正值得单独发表的全新观点或论证。到目前为止，它们似乎主要是在整合现有的研究成果，有时还会出错，而非创造真正的全新研究方法。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有与OpenAI无任何关联的科研人员，态度则没那么乐观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;利物浦大学化学教授、勒沃休姆功能材料设计研究中心主任Andy Cooper表示，“到目前为止，我们尚未发现大语言模型从根本上改变了科学研究的方式，但我们近期的研究结果表明，这类工具确实有其用武之地。Cooper正牵头研发一款所谓的AI scientist，该系统能实现部分科研工作流程的完全自动化。他表示，其团队并不会借助大语言模型构思研究思路，但这项技术已开始在更庞大的自动化系统中显现实用价值，比如大语言模型可协助操控机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我猜测，大语言模型或许会更多应用于机器人工作流程，至少在初期会是如此。因为我不确定人们是否愿意听从大语言模型的指挥，我自己当然是不愿意的。”Cooper称。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;团队重点发力：让 GPT 少点自信、更加谦逊&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大语言模型的实用性或许与日俱增，但保持谨慎仍是关键。去年12月，研究量子力学的科学家Jonathan Oppenheim指出，某本科学期刊中出现了一处由大语言模型导致的错误。他在X平台发文称，“OpenAI的管理层正在推广《Physics Letters B》上的一篇论文，其中的核心思路由GPT-5提出，这或许是首篇由大语言模型贡献核心观点且通过同行评审的论文。但有个小问题：GPT-5提出的思路，验证的对象完全错了。研究人员让GPT-5设计一个能检测非线性理论的验证实验，它却给出了一个检测非定域性理论的方案。二者看似相关，实则截然不同。这就好比你想要一个新冠检测试剂盒，大语言模型却兴冲冲地递给你一个水痘检测试剂盒。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;显然，许多科研人员正以富有创意、贴合实际的方式运用大语言模型。但同样显而易见的是，这项技术所犯的错误可能极为隐蔽，甚至连专家都难以察觉。这一问题的成因，部分源于ChatGPT的交互特性，它总能以迎合的语气让使用者放松警惕。正如Jonathan Oppenheim所言，“核心问题在于，大语言模型的训练目标是迎合用户，而科学研究需要的是能够挑战我们的的工具。”曾有一个极端案例，一名非科研领域的普通人被ChatGPT误导，长达数月都坚信自己发明了一个新的数学分支。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Weil也深知大语言模型的幻觉问题，但他强调，新一代模型产生幻觉的概率已大幅降低。即便如此，他认为，仅仅关注幻觉可能就偏离了重点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我的一位同事曾是数学教授，他说过的一番话让我印象深刻：‘我做研究时，和同事交流碰撞想法，自己的观点90%都是错的，但这正是意义所在。我们都在大胆畅想思路，只为找到一条可行的研究路径。’”Weil表示，“这其实是科研中最理想的状态。当你提出足够多的错误观点，有人偶然发现了一丝真理，另一人抓住这一点继续探讨：‘你说的这点并不完全正确，但如果我们换个思路’。就这样，人们便能在科研迷雾中逐渐摸索出前行的道路。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是Weil为OpenAI for Science设定的核心愿景。他认为，GPT-5固然优秀，但它并非万能灵药。这项技术的价值在于引导人们探索新的方向，而非提供最终答案。事实上，OpenAI目前正着手优化GPT-5的一项特性：让它在给出答案时降低其置信度。它不会再直接说“答案在这里”，而是会以更委婉的方式告诉科研人员：“以下思路可供参考。”“这正是我们目前投入大量精力在做的事：努力让模型具备某种认知层面的谦逊性。”Weil称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据透露，OpenAI正在探索的另一方向，是利用GPT-5对自身输出进行事实核查。实际应用中常有这样的情况：如果你把 GPT-5 的某个答案重新输入到模型中，它会逐条分析并指出其中的错误。Weil表示，“我们可以让模型充当自身的校验者。如此便能搭建一套工作流程：模型先完成初步推理，再将结果交由另一模型审核；如果这个模型发现了可以改进的地方，就会把结果反馈给原模型，并提示‘注意，这部分内容有误，但这部分思路有价值，可保留’。这就像两个智能体协同工作，只有当输出内容通过校验者的审核后，才会最终呈现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一机制，与谷歌 DeepMind为AlphaEvolve打造的模式高度相似。AlphaEvolve是一款工具，它将大语言模型Gemini封装在一个更大的系统中，该系统能够筛选出优质回复，并将其反馈给模型进行改进。谷歌 DeepMind已借助AlphaEvolve解决了多个现实中的科研难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，OpenAI面临着竞争对手的激烈角逐，这些企业的大语言模型即便无法实现OpenAI为其模型宣称的全部功能，也能完成绝大部分。倘若如此，科研人员为何要选择GPT-5，而非同样在逐年迭代升级的Gemini或Anthropic旗下的Claude系列模型？归根结底，OpenAI for Science的布局，很大程度上也是为了在这一新领域抢占先机。而真正的技术创新，尚未到来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&quot;&gt;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/zh-Hans-CN/prism/&quot;&gt;https://openai.com/zh-Hans-CN/prism/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</link><guid isPermaLink="false">https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</guid><pubDate>Wed, 28 Jan 2026 10:09:50 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>利用 ADBC 实现更快的数据传输：一次关于数据通路的系统性重构</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数十年来，访问数据库的标准方式始终是 ODBC 和 JDBC。然而，在这些传统的面向行的连接标准，可能会成为高性能 Snowflake 客户端应用程序的瓶颈。在 Snowflake 某些要求最为严苛客户的延迟敏感型应用中，包括关键业务运营和 AI 用例，ODBC 和 JDBC 的速度实在过于缓慢。这正是 Snowflake 选择拥抱开源生态 Apache Arrow 与新一代 ADBC 连接标准的核心动因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b0fa5268995ff98a5cb131c760dc56cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然 Snowflake 长期使用 Apache Arrow 列式格式来加速网络传输，但采用 ADBC 能使 Snowflake 客户消除客户端序列化和反序列化的开销，从而为大型结果集带来巨大的性能提升。在实践中，我们观察到使用 ADBC 相比 ODBC/JDBC 可实现 2 倍至 5 倍甚至 10 倍或更高的加速效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Build 2025 大会上，Apache Arrow PMC 成员、Columnar 联合创始人、Iceberg 项目提交者&amp;nbsp;Matt Topol&amp;nbsp;带来了一场高度工程化、干货满满的技术分享。他展示了使用多种语言（C、Go、Python、R）向 Snowflake 发起简单查询，包括使用数据框架甚至 DuckDB 等其他系统作为源，执行高效数据摄取到 Snowflake 的过程。重点将是如何轻松将 ADBC 集成到对毫秒级响应要求苛刻的应用中，以及如何利用 Snowflake 对 Apache Arrow 和 ADBC 的支持，为最关键的性能用例加速应用程序的速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从内存布局谈起：为什么 Apache Arrow 是关键前提&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 在分享一开始，并没有直接进入 ADBC，而是先用相当篇幅重新校准听众对 Apache Arrow 的理解。Arrow 并不是一个库或产品，而是一套列式、内存级的数据格式规范，其核心特征在于：内存中的数据布局，与网络传输时的字节布局完全一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一设计带来的直接结果是，数据在系统之间流转时，可以绕过传统序列化与反序列化过程，直接传递原始字节。在同一进程内，甚至可以做到零拷贝或共享内存。这不是优化细节，而是从根本上改变了数据移动的成本结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，Arrow 采用列式内存布局，使其天然适合向量化计算、聚合操作以及分析型工作负载。Topol 用“行式 vs 列式”的对比说明了一个事实：在分析场景下，行导向的内存访问意味着更多 I/O、更差的缓存命中率，以及无法充分利用 SIMD 等编译器优化；而列式内存恰恰相反，它与现代 CPU 架构是协同演进的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c77390762a9abf950dc9e5f752088082.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ODBC / JDBC 的结构性矛盾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Topol 将问题指向了当前最主流的数据库连接方式——ODBC 与 JDBC。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它们的价值毋庸置疑：API 稳定、生态成熟、适用于事务型与逐行计算场景，并且在过去几十年中几乎成为数据库访问的事实标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题在于，这套接口体系本质上是行导向的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现实是，Snowflake、DuckDB、ClickHouse、BigQuery 等主流分析型数据库，内部早已全面列式化。这意味着，每一次通过 ODBC / JDBC 拉取数据，系统都要经历一次高成本的转置：从列式内存转换为行，再在下游分析中重新转回列式结构。这不仅带来了显著的 CPU 与内存开销，也让数据在系统中反复“变形”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 特别强调，这里的转置并不是抽象意义上的重排，而是真实的数据拷贝与类型转换。在数据规模扩大后，这种成本会呈指数级放大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ADBC：把统一 API的理念带入列式世界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADBC（Arrow Database Connectivity）正是为解决这一结构性矛盾而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从抽象层面看，ADBC 与 ODBC / JDBC 非常相似：应用程序面对的是统一 API，通过不同驱动与不同数据库交互。但关键差异在于，ADBC 是列导向的，其数据交换格式直接采用 Apache Arrow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当数据库本身已经以列式形式返回结果，且能够直接输出 Arrow 数据时，驱动几乎无需做任何转换，便可将结果原样交付给应用侧——零拷贝、无转置。这不仅显著提升了性能，也让数据在更早阶段就处于可分析、可计算的理想形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便在数据库本身并非列式、或并未原生支持 Arrow 的情况下，ADBC 也允许在驱动层完成一次性转换，从而让应用侧始终面对统一的数据模型，而不必管理多套复杂连接器体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用数据说话：跨语言的性能对比与真实收益&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享的核心说服力，来自大量现场演示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Python 示例中，Topol 对比了通过 ODBC 与 ADBC 从 Snowflake 拉取数据的耗时。即便在启用缓存、排除查询执行成本的情况下，ADBC 在 10 万行与 100 万行数据规模下，仍然表现出明显优势：数据量越大，性能差距越明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，ADBC 返回的数据可以直接被 Polars 等基于 Arrow 的 DataFrame 库消费，几乎没有额外转换成本。这意味着，性能提升并不仅体现在拉数据更快，而是贯穿整个分析链路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样的结论，在 Go 和 R 的演示中得到了重复验证。跨语言的一致性，反过来也印证了 Arrow 与 ADBC 设计上的语言无关性——它们优化的是数据形态本身，而非某一语言生态的实现细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不止查询：流式摄取与系统间数据流动的新可能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享的后半段，Topol 将视角从查询结果返回扩展到更复杂的数据流动场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他展示了如何通过 ADBC，将 Snowflake 中的一百万行数据，以流式方式直接摄取到内存中的 DuckDB。整个过程无需先完整加载结果集，数据以 Arrow Record Batch 的形式持续流动，类型信息在传输过程中被完整保留，整体耗时不到四秒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一演示揭示了 ADBC 的另一层意义：它不仅是一种更快的查询接口，也是一种系统间高效、可组合的数据通道。当数据能够以统一、零拷贝的列式格式在系统间流动时，ETL、数据同步乃至多引擎协同分析的复杂度，都有机会被重新定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 并没有在结尾试图宣告 ODBC / JDBC 的终结。相反，他反复强调，这些技术在事务型与行式计算场景中仍然合理且必要。但对于分析型系统而言，ADBC 所代表的，是一种更贴合现代数据架构的方向：让数据尽可能早地进入列式、分析友好的形态，并尽可能少地在系统间反复转换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</link><guid isPermaLink="false">https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</guid><pubDate>Wed, 28 Jan 2026 09:15:00 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>内存一年疯涨170%，云账单里的“隐性成本”该算清了</title><description>&lt;p&gt;2025 年下半年，存储价格又一次成为行业聚焦点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多家市场机构统计显示，2025 年三季度跟四季度，DRAM 和 NAND 价格一路攀升。根据 Tom&#39;s Hardware 披露的数据，2025 年 DRAM 合同价同比上涨幅度高达 171.8%，创下历史新高。此轮上涨跟 AI 数据中心建设拓展、服务器需求集中释放紧密相联，还直接引发企业 IT 基础设施采购成本上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于依赖自建数据中心或中小 IDC 的企业来说，这种变化带来的冲击尤为剧烈。硬件采购从一次性预算问题，演变为难以预测的长期成本风险。服务器、SSD 和内存条的价格不再稳定，交付周期也更不确定。企业在扩容时不得不承担高价买入、供货延迟的双重压力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，将硬件采购压力转化为按需付费的运营支出，把价格波动风险转移给云服务商，正在成为越来越多企业的选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题并未因此结束。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着业务迁移到云端，企业发现云账单中存储与内存的占比仍在持续上升，即便算力配置并未明显升级，总体成本依旧水涨船高。部分团队开始反思：问题是否仅和数据量增多有关，还是资源使用方式本身就存在不合理的地方？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，多数云实例依旧按固定的 CPU 与内存配比来交付，诸如 2 核 4GB、4 核 8GB 的规格。早期，这种设计可简化资源管理，推动了云计算普及，但如今业务形态有所改变，企业系统一般得同时支撑多样业务，各业务对于算力、内存的消耗不一样，固定规格愈发难以契合实际需求。这导致企业要么部分资源长期闲置，要么不得不面对业务在高峰阶段出现性能瓶颈的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当内存价格进入上行周期，这种规格错配带来的浪费被进一步放大：闲置的不再只是资源本身，而是越来越昂贵的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是基于这样的背景，云基础设施走到新的路径分岔口：是继续就资源本身实施配置，还是转变方向围绕应用需求设计算力供给方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期面向中国区合作伙伴召开的发布会上，华为云对 Flexus 云服务器系列规格及性能进行更新，并且展示了其在各种业务负载下的运行表现。该实例基于华为云首创的柔性算力技术，打破 CPU 与内存的固定绑定关系，使企业能够按真实业务需求配置资源，从源头减少内存浪费，并结合智能调度与应用级加速改善长期运行稳定性与算力资源投入产出比。本文将从行业环境变化与技术实现等层面，剖析这种模式背后的思路，以及它所代表的云服务器演进方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云服务器，开始不太“合身”了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云服务器长期采用固定 CPU 与内存的配比，是工程上的一种取舍考量。早期云平台首先得解决的是规模化交付和稳定调度的问题，采用固定规格利于资源池管理，同样便于容量规划及计费设计。当业务形态呈现相对单一阶段，这样的方式尚可接纳。但究其本质它是从平台管理成本角度设计的，并非从业务负载的角度出发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今业务已不再是单一模式，电商、内容分发、数据库、缓存、AI 推理在一套系统中同步协同运行，对 CPU 以及内存的需求差别明显，固定规格无法精准对应实际负载，企业只能采用超出实际所需的实例型号。云服务器规格跟应用需求普遍不匹配，用户往往被迫去为用不到的算力和内存付费，引发大量资源的闲置浪费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2a/2afb7d843ca99668267a105cec3e77e2.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源浪费只不过是表象罢了，更深层的问题体现为性能优化的复杂度。现实的业务部署不仅涉及操作系统选定，还包含网络参数、系统参数以及应用配置参数。数量往往达到数千级别，缺少专家经验积累，难以达成稳定的最优配置。单是内核跟应用层的参数组合，就已超出普通团队可控范围，调优所用的周期漫长，效果也难以把控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60e1fc3271a533f924d017f8da492c35.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从较长的时间阶段看，云服务器本身一直在不断演变，最初的资源虚拟化阶段，是把物理服务器标准化成可租借的实例；紧接着进入弹性规模阶段，采取自动伸缩的方式去应对流量变化，这两个阶段处理的是存不存在以及是否充足的问题，当下已经迈入第三阶段，关注焦点转向使用是否高效。过去，固定实例曾是工程优势，如今却愈发像是一件穿着不合身的衣服。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;柔性算力：从“卖规格”到“卖能力”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;怎样让资源本身更贴近应用？在 Flexus 云服务器 X 实例产品的设计里，华为云引入了柔性算力这一概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Flexus X 实例里，柔性算力首先体现在规格形态的调整变化上。传统实例一般仅仅可在少量固定比例中选择 CPU 跟内存配置，而该实例支持按业务需求实施更精细的组合配置。发布会现场提到，所有 X 实例均支持多种非常规的 CPU/ 内存配比，包括 3:1、2:5、3:7 等组合。这可减少由规格不一致引起的资源闲置，让用户更接近按实际负载付费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而规格数量增加，并非表示问题自动就解决了，其关键是系统如何判断哪种配置更合适。传统调度大多依据节点上剩余的 CPU 与内存。新方式需要领会业务负载本身，涵盖资源使用结构，以及随时间的变化趋势。Flexus X 实例本质上不再是调度 CPU，而是实际的业务场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就工程实现而言，这种转变依赖底层架构的支撑，Flexus X 实例借助华为云自研的擎天 QingTian 架构和瑶光云脑调度系统得以实现，经由计算、存储和网络资源的解耦操作，提高了资源组合的自由度，也增强了非标准规格运行状态下的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，柔性算力还意味着配置不再是一次性决定，实例运行时会一直对资源使用状况进行评估，系统会判断当前配置跟负载是否相符，进而给出调整建议，而且还支持算力规格热升降的独家能力。从这个层面看，Flexus X 实例的转变不只是规格数量增多，它更像是把算力从提前打包好的商品，变成可持续优化的能力，实现“应用驱动算力”的最优体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关键应用加速：算力之外的第二条性能曲线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus X 实例不单单改变了资源形态，还进一步深入应用执行层，解决了算力配置合理系统却依旧不稳定的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次规格升级，华为云为数据库以及中间件类的负载引入专属应用级加速机制。Flexus X 实例针对 PostgreSQL、Memcached、MySQL、Redis、Nginx 提供了独立的一键加速能力，由 X-Turbo 应用加速引擎统一驱动。此类优化不会对用户的使用途径做出改变，实例创建结束之后即可启用，平台会把调优工作完成，用户无需插手复杂参数的配置。发布会现场，华为云对该能力实测演示，在 PostgreSQL 的使用场景下，Flexus X 实例的吞吐量达到 2.1 万 + TPS，大概为同规格业界旗舰型实例的 3.4 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/810527295eccdf4bca355ee9ac46c7ba.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就数据库这类系统而言，峰值性能仅仅属于一方面，更为关键的是高负载持续状态下的稳定输出能力。业务系统更易受诸如延迟抖动、连接堆积等问题的干扰，而不是单次压测形成的成绩。X-Turbo 的设计目标之一正是实现性能优化长期运行状态下的吞吐与响应稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;跟应用级优化同步进行的是，实例规模的进一步扩展。新一代 Flexus X2e 实例的 x86 规格从原本的 32U128G 提升至 64U256G，多核算力提升了约 30%；新增 Flexus KX1 鲲鹏实例，最高可达 80U320G，以覆盖大数据处理、内存数据库这类资源密集型场景。这意味着应用加速机制不再受中小规格环境约束，能在规模更大的资源池里发挥作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d2f568a839cc6be536e32d0c7def3623.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一系列的变化显示出云服务器性能边界正在转移。过去，性能更多由 CPU 规格和内存容量决定。而如今，应用执行路径、参数组合的方法及调度策略成为同等要紧的变量，在固定规格的时代里，这些优化由用户自己承担，而于 Flexus X 实例中，它们被纳入到算力交付范畴，正是从这一意义出发，云服务器竞争不再只是资源规模大小的比拼，而是发展为聚焦运行效率的系统工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从工程能力到真实落地：柔性算力如何进入生产系统&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一项新的算力供给方式，能否切实进入生产系统，首要取决于它是否具备充足的稳定性与可用性。Flexus X 实例可靠性设计向华为云旗舰级云服务器标准看齐，实现单 AZ 99.975% 的可用水平，还有跨 AZ 99.995% 的可用性。这暗示柔性算力没有以牺牲稳定性为交换代价，而是可直接承受核心业务负载的基础设施形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了稳定性这一点，规模化使用还取决于运维体系自身是否具有确定性，Flexus X 实例在华为云既有的 SRE 运维体系框架内运行，强调借助标准化变更、容量预测与故障演练减少系统行为的不确定性，实现大规模实例并发运行的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业落地的实际来看，柔性算力最先进入的并非那种单一业务场景，而是负载结构繁杂、资源使用波动大的系统类型。其已经在医疗电商平台迁移、连锁零售系统、医药行业信息化平台、游戏服务器迁移等场景大规模部署，用以承载数据库、中间件及核心交易服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中软国际智能集团云业务部副总经理王春玉在发布会上分享，团队为某大型生物医药集团搭建系统的时候，引入 Flexus X 实例作为数据库及业务服务的主要承载环境，在原有系统架构未改变的情形下完成迁移，而且在性能满足要求的前提下，达成约 30% 的综合成本下降。王春玉还谈到，其团队服务的一家专业酒水直营连锁品牌，把部分核心业务迁移到 Flexus X 实例而后，通过规格按需匹配与资源利用率优化，实现整体云资源成本约 15% 的下降。这些亮眼的结果主要源于两方面：一是实例规格跟业务负载的匹配度有所提升，降低了长期闲置资源的数量；二是借助应用级加速与调度优化，降低了单位业务量所需的算力规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这些真实的实际部署案例能看出，Flexus X 实例的用户一般有几个共同特性：业务负载呈现明显波动，系统结构相对复杂，然而运维及架构团队的规模较为有限，同时对长期云资源的成本敏感度较高。Flexus X 实例在未对业务形态本身作出改变的情况下，却降低了基础设施对业务扩展所施加的约束强度，让按照业务形态去配置算力成为可践行的工程实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，未来企业买的不再是服务器，而是业务效率。Flexus X 实例凸显了云服务器设计思路的一次转向：由“卖规格”过渡到“交付能力”，从“静态资源”过渡到“智能算力”，在 AI 成为主流计算负载的未来，此种转变大概率不会再是差异化优势，而是云基础设施的必要门槛。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</link><guid isPermaLink="false">https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</guid><pubDate>Wed, 28 Jan 2026 08:06:57 GMT</pubDate><author>棱镜</author><category>华为</category><category>云计算</category></item></channel></rss>