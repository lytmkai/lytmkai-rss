<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Mon, 19 Jan 2026 00:05:15 GMT</lastBuildDate><ttl>5</ttl><item><title>Java 近期资讯：Spring Shell、JReleaser、TornadoInsight和Apache Camel</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;连续第二周，JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问版本&lt;/a&gt;&quot;仍为&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B29&quot;&gt;Build 29&lt;/a&gt;&quot;。更多详情请参阅其&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样，JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问版本&lt;/a&gt;&quot;当前仍为&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B3&quot;&gt;Build 3&lt;/a&gt;&quot;。详细信息可查阅其&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Framework&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Spring Shell 4.0.0正式发布&lt;a href=&quot;https://spring.io/blog/2025/12/30/spring-shell-4-0-0-ga-released&quot;&gt;GA版本&lt;/a&gt;&quot;，包含缺陷修复、文档改进、依赖项升级以及多项新特性，包括，命令编程模型重构，在使用Spring Boot时，不再需要@EnableCommand或@CommandScan注解，并修复了@Command注解的意外行为；全新升级的DSL，解决了CommandRegistration.Builder实例与Spring Security的SecurityFilterChain接口在新构建器格式下的匹配问题；与Spring Framework 7.0和Spring Boot 4.0对齐；新增对&lt;a href=&quot;https://jspecify.dev/&quot;&gt;JSpecify&lt;/a&gt;&quot;的空安全（null safety）支持。更多细节请参见&lt;a href=&quot;https://github.com/spring-projects/spring-shell/releases/tag/v4.0.0&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JReleaser&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://jreleaser.org/&quot;&gt;JReleaser&lt;/a&gt;&quot;&amp;nbsp;1.22.0&lt;a href=&quot;https://andresalmiray.com/jreleaser-1-22-0-has-been-released/&quot;&gt;发布&lt;/a&gt;&quot;，这是一个用于简化Java项目发布流程的工具，本次更新包括缺陷修复、文档改进、依赖项升级以及新功能，包括，&lt;a href=&quot;https://jreleaser.org/guide/latest/reference/signing.html&quot;&gt;Signing&lt;/a&gt;&quot;模块全面重构，支持同时使用多种方法对构件（artifacts）进行签名；新增对&lt;a href=&quot;https://jedisct1.github.io/minisign/&quot;&gt;Minisign&lt;/a&gt;&quot;（一个用于文件签名和验证的工具）的支持；支持在部署构件到Maven Central时跳过等待期。更多详细信息请见&lt;a href=&quot;https://github.com/jreleaser/jreleaser/releases/tag/v1.22.0&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TornadoVM&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tornadovm.org/&quot;&gt;TornadoVM&lt;/a&gt;&quot;团队&lt;a href=&quot;https://www.tornadovm.org/post/tornadoinsight-compatibility-with-tornadovm-sdk-2-0-configuration-guide&quot;&gt;宣布&lt;/a&gt;&quot;，其开源IntelliJ 插件&lt;a href=&quot;https://github.com/beehive-lab/tornado-insight/blob/main/README.md&quot;&gt;TornadoInsight&lt;/a&gt;&quot;（旨在提升TornadoVM的开发体验）现已兼容&lt;a href=&quot;https://www.infoq.com/news/2025/12/tornadovm-20-gpu-llm&quot;&gt;最新发布的TornadoVM 2.0&lt;/a&gt;&quot;。相关配置指南也已同步更新。关于TornadoInsight的更多信息，可参考InfoQ的&lt;a href=&quot;https://www.infoq.com/news/2024/01/introducing-tornadoinsight/&quot;&gt;新闻报道&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Apache Camel&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/&quot;&gt;Apache Camel&lt;/a&gt;&quot;&amp;nbsp;4.14.3&lt;a href=&quot;https://camel.apache.org/blog/2026/01/RELEASE-4.14.3/&quot;&gt;发布&lt;/a&gt;&quot;，包含缺陷修复、依赖项升级及功能改进，包括，在使用&lt;a href=&quot;https://camel.apache.org/manual/camel-jbang.html&quot;&gt;Camel JBang&lt;/a&gt;&quot;时，可通过--repos命令为&lt;a href=&quot;https://camel.apache.org/camel-k/2.9.x/kamelets/kamelets.html&quot;&gt;Camel Kamelet&lt;/a&gt;&quot;相关操作指定Maven仓库；&lt;a href=&quot;https://camel.apache.org/components/4.14.x/neo4j-component.html&quot;&gt;Camel Neo4j&lt;/a&gt;&quot;组件改进了消息体的检测逻辑，避免内部错误；修复了&lt;a href=&quot;https://camel.apache.org/components/4.14.x/netty-component.html&quot;&gt;Camel Netty&lt;/a&gt;&quot;中SSL客户端证书主题名称（subject name）从可读字符串表述被错误转换为晦涩的&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2253&quot;&gt;LDAP&lt;/a&gt;&quot;格式的问题。更多详情请查阅&lt;a href=&quot;https://camel.apache.org/releases/release-4.14.3/&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-dec29-2025/&quot;&gt;Java News Roundup: Spring Shell, JReleaser, TornadoInsight, Apache Camel&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PRKRY1HB7C2ipJFoJBHy</link><guid isPermaLink="false">https://www.infoq.cn/article/PRKRY1HB7C2ipJFoJBHy</guid><pubDate>Mon, 19 Jan 2026 00:00:00 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>烧掉数万亿 Token、数百 Agent 连跑一周：Cursor“从零写浏览器”，结果是拼装人类代码？</title><description>&lt;p&gt;现在，大模型可以独立写完整整一个浏览器了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor CEO Michael Truell 最近分享了一项颇为吸睛的实验：他们用 GPT-5.2 让系统连续不间断运行一周，从零构建出一个“可用”的 Web 浏览器。按他的描述，产出规模达到：超过 300 万行代码、横跨数千个文件，全部通过这套 AI 驱动的编程平台生成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/09a68fe91c9f8b726b597d4a49b03612.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数百个 Agent “从零”写了一个浏览器？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照他的说法，这个项目并没有依赖现成的渲染引擎，而是用 Rust 从零实现了一整套渲染引擎，其中包括 HTML 解析、CSS 级联规则、布局计算、文本排版（text shaping）、绘制（paint）流程，甚至还实现了一个自定义的 JavaScript 虚拟机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Truell 也坦言，这个浏览器目前只是“勉强能用”，距离 WebKit 或 Chromium 等成熟引擎还有很大差距；但团队依然“感到震惊”，因为简单网站在它上面渲染得很快，而且整体效果在很大程度上是正确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，Cursor 还发布了一篇博客文章，题为《Scaling long-running autonomous coding》（扩展长时间运行的自主编程）。文章回顾了一系列实验：让“编程 agent 连续自主运行数周”，目标是“理解在那些通常需要人类团队耗费数月完成的项目中，agentic coding 的能力边界究竟可以被推进到什么程度”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这篇文章里，他们重点讲的是多 Agent 如何协同：如何在单个项目上同时运行数百个并发 Agent、如何协调它们的工作，并观察它们写出超过一百万行代码和数万亿个 token 的过程与经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor 先承认了单个 Agent 的局限：任务规模一大、依赖一复杂，推进速度就会明显变慢。并行化看似顺理成章，但他们很快发现，难点不在并发，而在协同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“学习如何协同：我们最初的方法是让所有 agent 具有同等地位，并通过一个共享文件自行协同。每个 agent 会检查其他 agent 在做什么、认领一个任务并更新自己的状态。为防止两个 agent 抢占同一项任务，我们使用了锁机制。&amp;nbsp;这一方案在一些有趣的方面失败了：&amp;nbsp;agent 会持有锁太久，或者干脆忘记释放锁。即使锁机制正常工作，它也会成为瓶颈。二十个 agent 的速度会下降到相当于两三个 agent 的有效吞吐量，大部分时间都花在等待上。&amp;nbsp;系统非常脆弱：agent 可能在持有锁的情况下失败、尝试获取自己已经持有的锁，或者在完全没有获取锁的情况下更新协调文件。&amp;nbsp;我们尝试用乐观并发控制来替代锁。agent 可以自由读取状态，但如果自上次读取后状态已经发生变化，则写入会失败。这种方式更简单、也更健壮，但更深层的问题依然存在。&amp;nbsp;在没有层级结构的情况下，agent 变得非常规避风险。它们会回避困难任务，转而做一些小而安全的修改。没有任何一个 agent 承担起解决难题或端到端实现的责任。结果就是工作长时间在空转，却没有实质性进展。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这一问题，Cursor 最终引入了更明确的角色分工，搭建一条职责清晰的流水线：将 Agent 分为规划者和执行者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“规划者（Planners） 持续探索代码库并创建任务。他们可以针对特定区域派生子规划者，使规划过程本身也可以并行且递归地展开。&amp;nbsp;执行者（Workers） 领取任务并专注于把任务完成到底。他们不会与其他执行者协调，也不关心整体大局，只是全力处理自己被分配的任务，完成后再提交变更。&amp;nbsp;在每个周期结束时，会有一个评审 Agent 判断是否继续，然后下一轮迭代会从干净的初始状态重新开始。这样基本解决了我们的协同问题，并且让我们可以扩展到非常大的项目，而不会让任何单个 Agent 陷入视野过于狭窄的状态。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此基础上，Cursor 把这套系统指向一个更具挑战性的目标：从零构建一个浏览器。他们表示，Agent 持续运行了将近一周，在 1,000 个文件中写出了超过 100 万行代码（原文如此，跟Michael Truell说的300万行不同），并将源码发布在 GitHub 上供外界浏览。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fc/fc8f661989eab13452b0fd8157d33f1b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor 进一步宣称：即便代码库规模已经很大，新启动的 agent 仍然能够理解它并取得实质性进展；同时，成百上千个 worker 并发运行，向同一个分支推送代码，而且几乎没有冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一场“全民打假”的开始？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次实验之所以引发强烈反应，很大程度上是因为：Web 浏览器本身就是软件工程里公认的“地狱级”项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/de/de92607bb9b36483a2a05e52c367cb12.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它难的不只是“写代码”，而是工作量的量级、模块之间的高耦合，以及兼容性这条几乎看不到尽头的长尾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 上，有人顺手抛了一个问题：“开发一个浏览器最难的地方是什么？”很快就有人给出一个类比：“说句真心话，这个问题几乎等同于：开发一个操作系统最难的地方是什么？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为现代浏览器是千万级代码量的系统，能够运行非常复杂的应用。它包含网络栈、多种解析器、frame 构建与回流（reflow）模块、合成（composite）、渲染（render）与绘制（paint）组件、前端 UI 组件、可扩展框架等等。这里面每一个模块，都必须同时做到：既支持 30 年前的旧内容，也支持复杂得离谱的当代 Web 应用。同时，它还得在高性能、高安全前提下尽可能少占用系统资源，并且往往要跨 Mac、Windows、Linux、Android、iOS 等多个平台运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有人提到，最难的是那张超长的任务清单。浏览器里包含多个高复杂度模块，每一个单拎出来都可能要做很久；更麻烦的是，它们之间还要通过一套相当“啰嗦”的 API 连接起来——很多接口你必须实现，至少也得先把壳子（stub）搭出来，否则系统就会崩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对这个浏览器项目，Cursor 在博客中写道：“虽然这看起来像是一张简单的屏幕截图，但从头开始构建一个浏览器是非常困难的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而如果外界自己去尝试编译这个项目，会很快意识到：它离“功能齐全的浏览器”还差得很远，甚至看起来在公开代码状态下，连最基本的构建都很难稳定通过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从仓库公开信息来看，近期 main 分支的多次 GitHub Actions 运行结果显示失败（其中还包括工作流文件本身的错误）；不少开发者的独立构建尝试也报告了数十个编译错误。与此同时，最近的一些 PR 虽然被合并，但 CI 仍处于失败状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有开发者表示自己回溯 Git 历史，往前翻了约 100 个提交后表示，依然没能找到一个可以“干净编译通过”的版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也引出了一个问题：这些被 Cursor 描述为在代码库中长期并发运行的“agent”，在工程链路上到底做到哪一步？至少从当前公开状态看，它们似乎并没有把“能编译、能检查”当成最基础的收敛目标——因为无论是 cargo build 还是 cargo check，都会立刻暴露出成片的编译错误和大量警告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而Cursor 的博客文章除了提供代码仓库链接外，既没有提供可复现的演示，也没有提供任何已知的有效版本（标签/发布/提交）来验证截图。无论如何，这文章本身给人一种原型功能完备的错觉，却忽略了此类声明应有的基本可复现性特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/18f5eca698dcba5b3804f2d1b922713f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人在Michael Truell 的LinkedIn上直接把结果抛了回去：“构建直接失败，报了 32 个错误，代码本身就是坏的；没有任何 release、没有 tag，CI 也在持续失败，我们甚至连这个所谓‘可用的浏览器’都没法编译、没法试跑。这更像是一场营销活动，而不是一次真正的 agentic 实验。”Michael Truell 至今没有回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c2/c267ef2f437e4f201c1ec9d34b3fb436.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前唯一一个在社交平台上明确分享“复现成功”的人，是前浏览器开发者 Oliver Medhurst。他表示自己花了大约两个小时修复编译错误和漏洞，才把项目跑起来。至于性能，他的评价也很直接：有些页面加载要整整一分钟，“不算好”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个更敏感的追问也随之出现：“所以这真的是从零开始写的吗？”他给出的回应更像一句反转预告：“剧透：不是。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/69/69169227bd6dc8d62e0d4cf0637cbb88.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更多网友通过翻看仓库依赖发现，这个项目直接引入了 Servo （一个最初由 Mozilla 开发的基于 Rust 的浏览器）项目的 HTML 与 CSS 解析器（html parser、css parser），以及 QuickJS 的 Rust 绑定（rquickjs），并非所有关键组件都是自行实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再加上 selectors、resvg、wgpu、tiny-skia 等一系列成熟库，这个“浏览器实验”更像是直接调用了人类编写的代码，而不是“从零开始”的一整套渲染与执行引擎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64b6b900a7d742bf32dafd9f963fe8a5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e7/e716eb0006daa5131dc89b27d7892306.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/85/85491f9fdccde9acf59685ff7790b541.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更搞笑的是，Cursor 这里用的还是一个发布于 2023 年 6 月的wgpu 0.17这种非常旧的老版本，而当前最新版本已经是 28（发布于 2025 年 12 月）。大概因为大模型写代码时往往会直接改版本管理文件（如 package.json、Cargo.toml），而不是通过 npm add、cargo add 这类构建工具来引入依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c15a8df25de61642d98e04625fc169a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也不怪网友骂他们：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这简直是胡扯。应用根本跑不起来，功能也缺得厉害。LLM 更像是在把它训练过的现成代码拼起来做个浏览器——毕竟 Chromium 本来就是开源的。最后堆出了 300 万行‘看起来很多’但没有价值的代码，结果还不能用，更谈不上什么新产品。折腾到最后，你还是得让开发者花大量时间去调试、排查安全漏洞，才能把它打磨得像一个早就存在的成熟产品。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“两周时间、数百个 agent，V8 和 Blink 又都是开源的。说到底，这就是在浪费 GPU 和电力。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/95a7b886c9f136fd2ea711159c08cd8d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后值得一提的是，这个实验还暴露出一个不容忽视的问题：成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人翻回 Cursor 的原帖指出，他们还在跑类似实验，比如一个 Excel 克隆项目（&lt;a href=&quot;https://github.com/wilson-anysphere/formula&quot;&gt;https://github.com/wilson-anysphere/formula&lt;/a&gt;&quot;）。GitHub Actions 的概览数据很夸张：累计触发了 16 万多次 workflow 运行，但成功的只有 247 次——失败的主要原因不是代码本身，而是超出了支出上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Agent 并不在乎预算；但在真实的软件工程里，可复现的构建、可持续的成本、可验证的产出，才决定一个系统最终能不能被信任、被维护、被继续推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cursor.com/cn/blog/scaling-agents&quot;&gt;https://cursor.com/cn/blog/scaling-agents&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46646777&quot;&gt;https://news.ycombinator.com/item?id=46646777&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/&quot;&gt;https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7417328860045959169-PFuT/&quot;&gt;https://www.linkedin.com/posts/activity-7417328860045959169-PFuT/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://xcancel.com/CanadaHonk&quot;&gt;https://xcancel.com/CanadaHonk&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t0rpY0X2G9RBmXf9SK6g</link><guid isPermaLink="false">https://www.infoq.cn/article/t0rpY0X2G9RBmXf9SK6g</guid><pubDate>Sat, 17 Jan 2026 12:00:00 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>告别“刀片利润”，AI如何帮中国数百万中小工厂构筑新护城河？</title><description>&lt;p&gt;“今年上半年我在山东临沂见了一位满头白发的90后老板，他们公司的年销售额超过3亿元，但利润却不到1000万。”&lt;a href=&quot;https://www.infoq.cn/article/xLUE7sWGby4MF5GKpgas&quot;&gt;1688&lt;/a&gt;&quot;商家发展中心总经理王强在日前接受媒体采访时讲道，“白天睡觉、晚上陪客户，这是他们为此生意的主要方式。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，这不是个例，而是中国数百万中小工厂主的真实缩影——规模在增长，利润在萎缩；订单在增加，确定性在流失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种悖论背后，是一场深刻的结构性撕裂，是当前B2B产业面临的系统性挑战：合规成本持续攀升，环保、税务、用工等要求日益刚性；与此同时，供给极端碎片化、需求高度非标化，交易决策链冗长，产业经验变得难以沉淀和复用。这使得过去依靠压价和人情关系维系的生意模式，在今天已经难以为继。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，中国产业带的底层逻辑正在被彻底重构，“K型复苏”成为新常态，头部企业加速扩张，尾部企业加速出清，一个尖锐的问题摆在所有中小工厂面前——出路究竟在哪？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1688在刚刚发布的《2025中国产业带发展趋势报告》（以下简称“报告”）中给出了他们的答案：2025年，中国产业带开始迈向AI原生时代。这不是又一次简单的工具升级，而是一场历史性跨越：从“数字化”向“智能化”的范式转变。在这场变革中，AI不再是可有可无的“外挂”，而是决定生死存亡的“操作系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该报告基于1688平台26年产业带深耕经验，整合覆盖全国70%一级产业带、超百万家源头厂商的真实交易大数据，系统分析了AI在产业带的演进路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“卷成本”到“拼确定性”，中国产业带的游戏规则正在被重写&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说过去二十年产业带的竞争关键词是“规模”与“成本”，那么今天，“确定性”正迅速取代“低价”，成为新的护城河。所谓确定性，不只是按时交货，更包括产品品质稳定、服务响应及时、需求预测精准、合规风险可控。并且，这些变化的覆盖范围不仅限于国内市场，更是全球性的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告指出，产业带“江湖规矩”和“权力地图”正被彻底重写，“外转内”与“内转外”并行，产能外迁与产业内移共振，工厂正从“代工者”蜕变为“品牌主”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而AI，成了构建确定性的核心引擎，基于 AI 原生的下一代供应链呼之欲出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据王强分享，深圳一家3C配件厂商深圳众鑫通泰曾深陷“低价、欠款、无客”的死循环，但他们通过AI分析亚马逊上的用户差评发现，一款手机支架的核心痛点是“粘不牢、价格高”，并据此开发出了采用真空磁吸技术的新品，同时，借助AI生成的油管爆款视频进行推广，这家工厂最终实现了跨境业务占比突破70%，毛利率远超同行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一家位于安徽芜湖的一个6人鞋企也在2024年借助AI工具实现了惊人跃升：上新效率提升4倍，支付转化率提升41%，全年销售额达1.5亿元。他们没有设计师，就用AI完成换色、场景图和视频生成；没有客服团队，就部署7×24小时自动响应系统；甚至通过AI分析TikTok热门搜索词，捕捉全球潮流趋势。“AI已经是趋势了，等别人都试完再上，你就被淘汰了。”芜湖苏禾鞋业张云这样说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;类似的故事正在更多产业带上演。报告显示，和这些工厂一样，越来越多的中小企业正通过AI将内贸积累的柔性快反、品控能力“翻译”为跨境竞争力。AI不再只是巨头的游戏，它同样成为了小微商家对抗规模劣势的“杠杆”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，AI已深度融入商家端的三大核心场景：选品方面，通过全球电商平台评论与社媒热词，反向定义产品；小单快返方面，基于柔性供应链使得响应周期大幅缩短；智能质检方面，用图像识别替代人工目检，使得效率大幅提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1688公共事务部总经理范敏强调，这些变化背后指向了这样一种进化逻辑——AI正在驱动三大“位移”：&lt;/p&gt;&lt;p&gt;第一，决策机制位移，从依赖“老师傅经验”转向依赖“AI产业大脑”；第二，组织形态位移：从“人盯人”管理转向“AI调度+人机协同”；第三，核心竞争力位移，从“模具和产能”转向“数据驱动的快速迭代能力”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如报告所示，2026年起，增长将向能稳定交付、直连用户、自主开款的源头工厂集中。未来的赢家，不是规模最大、也不是成本最低的，而是将AI融入血液，构建起“效率×合规×确定性”新护城河的企业” 。在这个新规则下，“确定性”本身就成了最稀缺的资源，能否用好AI，则成了企业穿越周期的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;超越“生意搭子”，AI从来不只是一个工具&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这场跃迁并非坦途。&lt;a href=&quot;https://www.infoq.cn/article/jNFPJE9aRZi37seSRNwc&quot;&gt;数据孤岛&lt;/a&gt;&quot;、模型泛化能力不足、复合型人才短缺仍是大多数企业在AI应用落地过程中遇到的主要瓶颈。尤其在传统产业带，许多工厂连基础的ERP系统都没能打通，导致AI系统缺乏高质量、结构化的数据输入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对这些现实约束，企业可以选择从最小可行场景切入，以业务价值反推技术建设。以深圳众鑫通泰为例，他们并不是从一开始就试图构建“全厂智能大脑”，而是聚焦一个具体痛点——“用户为什么差评我们的手机支架？” 通过抓取公开电商平台评论这一无需内部系统打通的外部数据源，快速验证了AI选品的价值，再逐步将成功经验延伸至生产排程与质检环节。这种“由外而内、由点及面”的路径，有效绕开了初期数据孤岛的制约。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对模型泛化难题，目前市场上已经有平台开始探索基于跨商家、跨品类的聚合数据，提炼具有行业共性的智能能力。比如1688依托其覆盖全国70%一级产业带的交易网络，正尝试把成功案例中从选品洞察、质检规则到履约优化的AI应用逻辑，抽象为可借鉴的方法论甚至工具原型，以降低单个工厂的试错成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而破解人才困局的关键，在于构建“人机协同”的新工作流，而非追求全能型个体。拿芜湖苏禾鞋业这个6人企业来说，他们并没有AI工程师，也没有技术背景，仅仅通过应用AI工具，就实现了设计、营销与客户服务的全面提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王强表示：“未来的AI的卷应该是，你是不是让AI把你做成一套的体系或系统？而&lt;a href=&quot;https://www.infoq.cn/article/MhBpL7m3y873tY1qmm0I&quot;&gt;不仅仅是一个工具&lt;/a&gt;&quot;。” 换言之，AI的价值不在于炫技，而在于能否系统性解决已知问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告中对中国产业带进化进行了三阶段划分，随着AI应用深度的变化，企业将从“AI外挂”进入“AI共生”乃至“AI原生”。而这也恰恰是企业应对以上一系列挑战的底层逻辑，企业不应该把AI当作一个孤立的技术项目，而是将其视为重构业务流程、组织协作与客户价值的契机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于身处这一变革浪潮中的企业，报告还给出了三条行动建议：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，要信仰AI，用AI做生意，把工厂变成真正的“硅基工厂”——让每一条产线都听得懂需求、看得见订单、控得住质量；第二，要做足确定性，品控要硬、履约要稳、服务要好；第三，要布局双循环，AI正在模糊内需与跨境的边界，因此企业需要一套AI系统，通做全球生意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当AI不再是“生意搭子”，而成为产业运行的“智能中枢”，中国制造业的下一轮红利，才真正拉开序幕。这场变革不会一夜完成，但方向已然清晰：谁能把不确定性转化为确定性，谁就能在新秩序中占据主动。而AI，正是那把最关键的钥匙。&lt;/p&gt;</description><link>https://www.infoq.cn/article/uPMimwjtFsRSB8thczFi</link><guid isPermaLink="false">https://www.infoq.cn/article/uPMimwjtFsRSB8thczFi</guid><pubDate>Sat, 17 Jan 2026 11:37:25 GMT</pubDate><author>高玉娴</author><category>阿里巴巴</category><category>工业</category><category>AI&amp;大模型</category><category>数字化转型</category></item><item><title>IDE消亡之年？Steve Yegge 两句狠话：2026 年还用 IDE 就不行，每天烧 500–1000 美元 Token 才合理</title><description>&lt;p&gt;虽然我并不认同“IDE 会在 2026 年消亡”这种绝对说法，但 Steve Yegge 和 Gene Kim 在分享中抛出的判断，依然值得认真对待：在他们的推演里，从 2026 年 1 月 1 日起，继续依赖传统 IDE 的工程师，会被更快拉开差距。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们认为这不是“工具升级”，而是“生产方式换代”：工程师的竞争力，越来越取决于你能否用好新一代 AI 开发方式，以及你愿不愿意为它付出真实成本——例如把每天的 token 开销重新定价到“接近日薪”的量级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更刺耳的是，他们转述了 OpenAI 的 Andrew Glover 的一项观察：是否使用 Codex，可能会让同级别工程师之间的生产力差距被拉到 10 倍，这让管理层“非常惊慌”，“因为他们甚至可能不得不裁掉 50% 的工程师”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心观点如下：&lt;/p&gt;&lt;p&gt;现在的模型本质上是派一个潜水员下去，让它在代码库里四处探索，即使给它更大的氧气瓶，比如一百万 token，它仍会耗尽。正确做法应该是派多个角色，而不是寄希望于一个超大的单一潜水员。如果你在2026年 1 月 1 日后还在使用 IDE，那你就是一个“不好的工程师”。作为工程师，我每天花在 Token 上的费用应该与我的日薪相当，也就是每天 500 到 1000 美元。Claude Code 走错了方向，他们造出一只巨大、耗能、高成本的“肌肉蚂蚁”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Yegge：今天的时间会过得很快，我将讨论明年(2026年）开发工具的样貌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在所有人都迷恋 Claude Code，市面上大概有四十个竞争者，但 Claude Code 并不是答案，代码补全也不是。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然我每天使用它十四个小时，但开发者并未真正采纳。核心问题是这些工具使用难度过高，认知负担重，而且常常“撒谎、作弊、偷懒”。因此大多数开发者并不喜欢这样的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我逐渐认识到，Claude Code 很像电钻或电锯。对于没有受过训练的人，它既能帮上忙，也能造成巨大损伤。未受训练的工程师使用 Claude Code，与一个新手拿着电锯差不多：既可能“切到脚”，也可能在熟练后完成极其精细的工作。然而软件世界无限广阔，而我们的野心也同样无限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此我想用一个类比说明：明年将是从“手持电锯、电钻”转向“数控机床（CNC）”的一年。CNC 在给定坐标后能自动执行极其精确的操作，这项技术我们已经使用了数百年，也不会在今年停止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c95d80dbac1bae36cd549db265cdec73.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有人说“模型已经触顶了”，你的工程师们可能也这么说。即使如此，我们仍然等同于刚发现蒸汽和电力，还需要时间去驾驭它。现在的问题已经主要是工程问题。一年到一年半内，所有代码都将由大型自动化“磨床”式系统生成，工程师不再直接查看代码。这将是一个全新的世界，而我们正走向那里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gene 和我曾与 OpenAI 的 Andrew Glover 交流过，他说公司内部出现了明显的分化：部分工程师使用 Codex，而更多人没有用（拒绝使用工具的人主要是资深与 Staff 级工程师），产能差距巨大，导致绩效评估出现警报。两个同级别的工程师，其生产力可能相差十倍，这让管理层非常惊慌，因为他们甚至可能不得不裁掉 50% 的工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/22a874b661acbb71717e4f083fa6ce92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种情况类似瑞士机械表产业的衰落：经历数百年的辉煌，却被石英表在短短几年内颠覆，当时的工匠与今天坚持传统方式的资深工程师反应如出一辙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f1fa5c038fae21cd13a5793d3802507.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来需要的是一种全新的 UI，不是传统 IDE，而是新的 IDE。事实上，Replit 已经走得最前，他们的方向非常值得称赞。我们不该再继续追着旧形态、构建各种命令行界面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/89/8982b2a872bb79323b772dde65419d58.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，Claude Code 及其竞争者都走错了方向——它们像在打造“世界上最大的蚂蚁”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的朋友、澳大利亚联邦银行的 Brendan Hopper 说得很好：自然界靠蚁群协作，而 Claude Code 却造出一只巨大、耗能、高成本的“肌肉蚂蚁”。无论是要分析整个代码库，还是只是问“我的git ignore 还在吗”，它都调用最昂贵的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/598c2514f47467c5c48c87d0de6b3ed5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是我想到了“潜水员隐喻”：上下文窗口就像氧气瓶。现在的模型本质上是派一个潜水员下去，让它在代码库里四处探索，即使给它更大的氧气瓶，比如一百万 token，它仍会耗尽。正确做法应该是派多个角色：产品经理潜水员、开发潜水员、代码审查潜水员、测试潜水员、合并潜水员等，而不是寄希望于一个超大的单一潜水员。可没人这么做，大家都在造“大潜水员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3a1a5c011d89fc53ca8fc584af27b027.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的构建方式将是工程师熟悉的：任务分解、逐步细化、组件化、黑盒化，并依赖大量协作的智能体，而不是单一智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/26/269c82af09b2d0b65f15c65fb57e01aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在此之前，我的建议还是：学习 Claude Code来适应新方式，并放弃你的 IDE。如果你在明年 1 月 1 日后还在使用 IDE，那你就是一个“不好的工程师”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97077c1a104c8143ba383e9d41ff0aec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gene Kim：我研究高绩效技术组织已有 26 年，这段旅程始于我作为 Tripwire 的技术创始人。我们致力于研究那些表现卓越的技术组织——它们在项目交付、运维稳定性、安全合规方面都处于领先。我们想理解这些组织如何实现“从优秀到卓越”的转变，以及其他组织如何复制这些成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这 26 年中我经历了许多意外，其中最大的意外之一，是这项研究最终将我带到了 DevOps 运动的中心。DevOps 改变了测试、运维、信息安全等角色的协作方式。我曾以为这会是我职业生涯中最激动人心的经历，直到我在今年 6 月首次与 Steve Yegge 见面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我和Steve有许多共同点，其中之一就是对人工智能的热爱，以及都认为 AI 将从底层重塑软件开发的方式。我们相信，AI 对技术组织的影响，可能比十年前敏捷、云计算、CI/CD 和移动化所带来的变革大上百倍。而这些技术突破不仅会改变组织，也会重塑整个经济，让经济结构围绕更先进的生产方式重新排列。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去一年半，我们观察了许多案例，让我们提前看到未来技术组织的雏形。有人可能熟悉 Adrian Cockcroft，他曾是 Netflix 的云架构师，主导了 2009 年将 Netflix 整个基础设施从自建机房迁移到云端。他在几个月前写道，2011 年有人提出“无运维（NoOps）”时，引发了基础设施和运维团队的强烈反对，但现在类似的事情再次发生，只不过这次可能叫“无开发（NoDev）”。如今看来，这似乎不再好笑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们从 Zapier 的分享中看到，支持团队能发版，设计师能发版，UX 设计也能直接发版。过去被开发者告知“排队、等一个季度、等一年、甚至永远等不到”的人，现在突然能够自己把功能“对话式地”写进生产环境。这不仅改变技术组织，也可能改变整个经济。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve 和我很幸运能看到部署方式的改变带来什么影响。十年前，我写了《The Phoenix Project（凤凰项目）》，讲述灾难性的部署流程。当时许多组织一年只发布一次版本，难以想象。后来我参与了 DevOps 状况研究，这项跨行业研究在 2013–2019 年间覆盖了 36,000 名受访者。我们发现，高绩效团队每天能多次部署，并能在一小时内完成一次发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 2009 年，多次每日部署被视为鲁莽、不负责任甚至“不道德”，但如今却是常态。若想保持高可靠性、缩短平均修复时间，就必须更频繁地进行更小规模的部署。现在我们看到的案例表明，不再手写代码，而是运用新的方式进行开发，可能是一种价值更优的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们在《Vibe coding》一书中提出的定义是：只要不是靠双手在 IDE 里敲代码的方式，都可以称作“Vibe coding”。有些人还像在暗房里冲洗照片一样，依旧习惯在昏暗环境里手动输入代码。但 Anthropic 联合创始人兼 CEO Dario&amp;nbsp;Amodei 给了我们更好的定义：Vibe coding 是由反复对话推动的、由 AI 生成代码的过程。他说这个词很美，能表达一种全新的开发方式，但也略带戏谑。不过对他们而言，这已经是“唯一的方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是编程语言领域的重要人物 Erik Meijer 博士，他参与过 Visual Basic、C#、Haskell，也在 Meta 推出了 Hack 编程语言，在一年内迁移了数百万行 PHP 代码，引入静态类型检查。他说，我们可能是最后一代手写代码的开发者，所以应该享受这一段最后的旅程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8dd568affd763d1ec9ca2c3125a1308f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一件事是这样的：去年 11 月开始，我一直在观察Steve，他每天在编码代理上花掉几百美元。这在当时看起来非常奇怪。他不仅把各种月度订阅都用到了上限，实际上还远远超出了这些额度。&lt;/p&gt;&lt;p&gt;但现在我们听到的一种说法是：作为一名工程师，我的工作本身就应该要求我每天在 token 上的花费，和我的日薪大致相当。也就是说，大概每天 500 到 1000 美元。因为这些工具带来的，是一种机械优势和认知优势。作为工程师，我会挑战自己，去榨取这种投入所能带来的最大价值，把成果交付给真正重要的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/2344e7d9a259a019f8dd14122b7348a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在书中，我们把人们为何愿意这样做总结成一个缩写：FAAFO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aabd819a0f0cd76ab59fe64dd1e3548a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一个 F 是Faster（更快），但这是最表层的理由。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是A-Ambitious（雄心），AI 让我们得以完成过去无法实现的雄心项目，把不可能的事情变得可能。在另一端，琐碎麻烦的小任务也几乎变成了零成本。我非常喜欢 Claude Code 团队中的一段采访，Katherine 说，以前客户问题会被放进 Jira 的待办项，在梳理会议中争论，一拖数周；而现在我们直接在当下修复，并在 30 分钟内发布。记录依然会做，但协调成本几乎完全消失了。也就是说，不可能的事情变得可能，麻烦的小事变得免费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个A 是Able（能力），代表“更独立”，更能单独完成工作。这里有两类协调成本正被 AI 消除。第一类协调成本来自“等待”。如果你需要开发者或一个团队帮你做事，你必须沟通、协调、同步、排优先级、游说、升级……总之必须让他们“和你一样在乎这个问题”。而现在，依靠这些近乎奇迹般的新工具，你可以自己完成许多工作。第二类协调成本来自“理解”。即使别人愿意像你一样重视某件事，他们也无法读你的心。但我们发现，LLM 是惊人的“协作中介”。仅通过一个 LLM，你就能以 Markdown 文档的形式与不同职能顺畅协同。这当然不是最终形态，但它让高带宽的理解成为可能。因为要想实现共同的成果，就必须先有共同的目标与共同的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个 F，是 Fun（好玩）。正如 Steve 所说，Vibe coding 具有成瘾性。我们见过两个人原本以为“写代码的黄金时代已经过去了”，结果却意外发现现实恰恰相反。我现在常常玩得太投入，不逼自己去睡就会写到凌晨两三点。它不是只有好的一面，但肯定比无聊、枯燥甚至痛苦要好得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;O是Optionality（可选项）。我们非常重视“创造期权价值”。模块化之所以强大，也因为它能创造更高的期权价值。Vibe coding 能让你同时进行更多实验、更多尝试，因此它是极具经济价值的工具。Steve Yegge 说，对于已经经历“顿悟时刻”的人来说，本能反应往往是：如何让团队中所有人都获得与你现在同等的生产力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面我分享一些让我们看到未来形态的案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，Travelopia 的产品与技术负责人 Sree Balakrishna 的分享。Travelopia 是一家年营收 15 亿美元的旅行企业。他们曾用一个小团队，在 6 周内替换一套传统系统。按过去的方式，需要 8 人（6 个开发、1 个 UX、1 个产品负责人）；而现在，也许只需要一个开发与一个领域专家，正如 Kent Beck 所说，“一个有问题的人加一个能解决问题的人”。这种团队规模的变化会深刻影响组织未来的运作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e3/e3aadc82b15766130f889bbc3eccdd58.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我最兴奋的案例来自 Dr. Tapabrata Pal。他在 Capital One 推动过 DevOps，如今在 Fidelity 负责一个关键应用，用来查询公司 2.5 万个应用中哪些受 Log4j 影响。过去他的团队总说重新做这个工具需要 5 个月，并需招聘前端工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终他自己花 5 天 Vibe coding 出了一个版本，并上线生产。他只是想证明：事情完全能做，而且可以更快完成。后续更戏剧的是：他为应用找维护者，资深工程师们都不愿接手，最后是团队中最年轻的工程师成为维护者，并正在快速成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，这个应用的内部用户数量增长了 10 倍，他也因此获得更多人手。这些变化是任何人都没预料到的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再分享一个例子，我重返 Google Cloud 团队做的 Dora 研究，其中一项未进入正式报告的发现是关于“AI 信任度”。我们采用的信任定义是：你能多大程度预测对方（AI）的行为？越信任，就能给更大请求，用更少词语，减少反馈需求。结果显示：使用 AI 的时间越长，信任越高。那些说“我试了一下，它写代码很差”的人，多半只用了 1 小时。显然，AI 的掌握是可训练的技能，需要实践，而不是一次性体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/180a259b65eb420133f3397dc1ab0a39.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，我们的责任之一，是帮助他人获得“顿悟时刻”，并协助他们不断练习，从而真正掌握这些强大的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;六周前，Steve 和我为领导者们做了一次 Vibe coding 工作坊。三小时内，完成率 100%，每个人都做出了成果。还有一位，他说自己 15 年没写代码了，却在短时间内做出一个自动帮自己抢 Southwest 登机位的工具（直到被反机器人系统封掉），你从他脸上的表情就能看到那种久违的创造力被重新点燃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dcb9564856bcd6cc931edd02150191a7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，当支持团队、领导者能编码并上线时，技术组织必然会重塑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个技术领导者说，当他告诉团队他写了一个应用，其中 6 万行代码都是 AI 写的，而他自己一行没看时，团队看他的眼神仿佛“希望他不存在”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e51f4bb833719de52a99ed553bc6f89.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个例子，一些存在十年的遗留系统问题，团队集合资深工程师，用 AI 生成修复方案并提交 Pull Request。这次被接受了，而不像过去那样被污名为“AI生成的低质量内容（AI slop）”。还有团队说，他们现在的代码提交速度如此之快，以至于每个代码仓库只能容纳一个工程师，否则合并冲突会让协作成本爆炸。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=cMSprbJ95jg&amp;amp;t=4206s&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SJNt2c2Sh5AgO4LbiSC8</link><guid isPermaLink="false">https://www.infoq.cn/article/SJNt2c2Sh5AgO4LbiSC8</guid><pubDate>Sat, 17 Jan 2026 05:42:26 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>腾讯云ADP国内首发AI原生Widget：一句话秒级生成交互组件，重塑Agent使用体验</title><description>&lt;p&gt;智能体对话正在告别“纯文本时代”！近日，腾讯云智能体开发平台（ADP）重磅上线国内首个“AI原生Widget”，面向企业客户提供“富交互任务交付”能力，只需自然语言描述，就能实时生成表单、按钮等交互组件。该能力还同步在腾讯元器（一站式AI智能体创作与分发平台）生态侧落地，支持创作者一键生成交互卡片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，这一功能还兼容OpenAI 生态的 Widget 接入规范，外部 Widget 可依据标准协议直接导入复用，进一步拓展智能体能力边界与生态扩展空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI原生Widget”是一种面向智能体任务交付的“富交互组件形态”，模型输出结构化描述（JSON Schema)，平台自动渲染为可操作的表单、按钮，并将用户交互结果回传智能体，实现任务闭环执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/43/40/4396b1bc36e225b054f57d50fdf6ea40.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;自然语言秒级生成智能体交互组件&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在传统的大模型对话中，文本输出是主要形式。海量的文字堆砌，不仅抬高理解成本，而且完成单一任务需要多轮来回沟通，效率低且体验不佳。Widget作为可嵌入式的自定义展示组件，能在智能体对话流中，灵活融入图表、表单、按钮等“富交互”模块，将对话界面升级为沉浸式任务平台，引导用户按步骤操作，大幅提升信息传递与任务执行效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前国内的智能体平台构建Widget时，普遍采用传统“拖拉拽式低代码+手动配置字段/数据源映射关系”的方式，流程繁琐、耗时久、稳定性一般，难以适配高效开发的需求。针对这一痛点，腾讯云ADP推出的AI原生Widget，提供了模版创建、代码创建、自然语言生成等多种方式，降低开发门槛。即使非专业前端开发者，只需用语言描述需求，或调用现成Widget模板，一分钟内就能够生成对应组件，真正实现“所想即所得”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/32/a2/32fdcca20ce95b897452acb94ee92da2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持多种Widget开发模式&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如用户想要搭建一个“健身小助理”智能体，通过AI原生Widget，输入提示词后，一键就能生成对应卡片。当用户询问“我要跑步”时，系统会弹出预设卡片，引导用户点选运动频率、强度等习惯信息，再根据用户的选择，快速生成“跑步训练周计划”卡片，包含每周运动安排、单次运动内容、时长和强度建议等核心信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/9e/8a/9ef0f98b4d200e849bb79d5df795068a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;从纯文本对话到富交互任务执行&lt;/p&gt;</description><link>https://www.infoq.cn/article/KXHUrhczo8le9KpyyNjr</link><guid isPermaLink="false">https://www.infoq.cn/article/KXHUrhczo8le9KpyyNjr</guid><pubDate>Sat, 17 Jan 2026 05:36:06 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>AI 的下一个十年：从技术拐点到工程落地的路线图</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术发展史上，总会出现一些被反复回望的“拐点时刻”。在 Snowflake 首席执行官 Sridhar Ramaswamy 看来，我们正身处这样的关键节点之中——多年来机器学习与深度学习的研究积累、Transformer 等关键架构的突破，以及云计算规模能力的成熟，在这一刻汇聚，推动人工智能走向真正的产业化阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bb51e554cb4794fec827c8518aa96f2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一背景下，Snowflake 邀请了两位深度参与并塑造这一进程的核心人物，共同展开了一场关于 “未来十年 AI 蓝图” 的对话：堪称全球最具影响力的人工智能教育者和先驱者、LandingAI 执行董事长、DeepLearning.AI 创始人吴恩达（Andrew Ng），以及亚马逊云科技 Agentic AI 副总裁Swami Sivasubramanian，他曾主导 Amazon SageMaker 与 Amazon Bedrock 的构建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场对话并未停留在对模型能力的抽象讨论，而是围绕竞争优势、商业模式、工程架构、数据治理以及开发者未来等关键问题，勾勒出一条从战略到落地的清晰脉络。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;竞争焦点正逐渐脱离模型本身&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕“AI 时代的护城河从何而来”这一核心问题，讨论首先打破了一个常见误区：竞争优势并不必然源于模型本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在吴恩达看来，ChatGPT 这类产品在消费者层面形成的品牌认知，本身就构成了防御壁垒；但在更多行业场景中，护城河往往取决于行业结构，而非 AI 技术能力。例如，借助 AI 加速构建双边市场的平台，其持久性来自平台机制本身，而不是底层模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30596c95056a50fa39ca49c9a7644e72.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个重要变化是，软件护城河正在被削弱。过去需要多年、大规模团队才能构建的软件系统，如今在 AI 辅助编程的加持下，其可复制性显著提高。API 调用的灵活性也使开发者能够迅速切换工具，这让“API 即护城河”的逻辑变得愈发脆弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 从企业市场的视角补充道：在真实的企业环境中，竞争焦点正从“谁的模型更强”，转向“谁能通过 API 和服务，以更优的性价比，帮助企业真正提升收入或降低成本”。在这个意义上，真正的“最佳模型”，往往是企业自身的商业模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从订阅制到按量计费：AI 正在重塑软件商业逻辑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在商业模式层面，圆桌讨论也触及了一个正在发生的结构性变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十余年，SaaS 以订阅制为核心，其背后依赖的是软件接近零边际成本的特性。但在 AI 尤其是智能体场景中，这一前提正在发生变化——推理成本真实存在，且可能随使用规模非线性增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 指出，当 AI 系统开始代表用户执行任务，且工作负载与用户数量脱钩时，更接近云服务的按量计费模式将变得合理且必要。吴恩达则从开发者体验出发，分享了一个直观感受：AI 编程工具的效率如此之高，以至于开发者愿意为其消耗更多算力和费用，因为由此带来的生产力提升是实实在在的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/757bce25dccf7a8bd6499ab85e2184a2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并非简单的定价方式变化，而是意味着 AI 正在重新定义“软件价值如何被衡量和付费”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成功的 AI 架构：产品先行，为不确定性留出空间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当讨论从战略转向工程实践，三位嘉宾形成了高度一致的共识：产品市场契合（PMF）始终优先于成本优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴恩达强调，在早期创新阶段，最大的挑战不是控制成本，而是打造用户真正热爱的产品。当 PMF 出现后，工程手段总能在后续阶段将成本曲线重新压低。关键在于，在架构设计之初，就为模型可替换性和技术选择权留出空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 从大量初创企业的实践中总结出一条清晰路径：&lt;/p&gt;&lt;p&gt;初期采用通用基础模型快速验证产品；随着真实负载显现，通过微调、蒸馏、提示缓存优化等手段应对非线性成本；将模型选型视为可演进的工程问题，而非一次性决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，掌控自身数据层被反复强调。将数据牢牢掌握在企业自身体系内，而不是被封装进供应商的“云端密匣”（box in a cloud），是确保未来技术与合作可选性的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;非结构化数据的真正解锁：从 PDF 开始&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈及 AI 应用的下一个增长点时，吴恩达将注意力投向了一个长期被忽视的领域：非结构化数据。&lt;/p&gt;&lt;p&gt;在他看来，企业中最具价值、却最未被充分利用的隐性数据，正大量存在于 PDF 文档之中。无论是金融领域复杂的报表，还是医疗行业的各类表单，过去人们对 PDF 的主要交互方式，往往只是简单的关键词搜索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而如今，借助智能体驱动的文档解析能力，AI 已能够理解复杂表格结构、提取语义信息，并将其转化为可分析、可计算的数据资产。这一变化，正在迅速催生大量新的企业级应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给开发者的长期建议：回到基础，拥抱创造&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在圆桌的最后，讨论回到了一个更具情绪张力的话题：年轻开发者在 AI 浪潮下的焦虑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 指出，行业在某种程度上混淆了“编程”与“计算机科学”。即便 AI 能生成大量代码，对底层原理的理解，编译器、数据库、系统架构、数学与统计基础，依然不可替代。历史经验表明，每次技术变革初期都会经历短暂低谷与普遍焦虑，当前正处在类似阶段，但最终带来的是更大规模的创造者群体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴恩达则将这一判断推向更积极的方向：这是一个前所未有的创造窗口期。构建产品所需的时间和成本正在大幅降低，而 AI 辅助编程让“学习编程”本身变得更具现实意义和乐趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 Sridhar Ramaswamy 在圆桌结束时表示，未来无需被动等待，当下的我们比以往任何时候都更有能力去进项创造 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/wLeViF4hSl0WQrw3r2cQ</link><guid isPermaLink="false">https://www.infoq.cn/article/wLeViF4hSl0WQrw3r2cQ</guid><pubDate>Fri, 16 Jan 2026 12:02:04 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>受够了Copilot的“霸王条款”？GitHub全球宕机遭怒骂，引爆开发者“大逃离”！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几个小时前，有大批开发者反馈：GitHub大面积宕机了，社交平台上充斥着“粉色独角兽”的截图和相应的控诉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/196132f0521e4f6f1f124188610a35ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于此次故障的原因，目前GitHub还未给出详细分析报告。然而，不少开发者们的猜测已把矛头指向了Copilot。而在近期，也有企业和许多个人开发者们在“逃离”GitHub并将代码库迁移到了其他平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;GitHub宕机遭怒骂：拖垮全世界开发流程&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“所有用户都被强制登出，我自己也登不进去，因为整个服务器都宕机了。”“我以为是自己眼花了，但GitHub真的宕机了。很多团队都在周四收尾迭代冲刺，这次故障怕是要导致大量工作项顺延。”“更新即将发布时，登录网站就崩溃了。”“尴尬的时刻：你意识到自己过去一个小时一直在本地提交代码，却无法推送。”“天哪，我现在该怎么给经理汇报啊。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一次GitHub的宕机，很快就在各大平台引发诸多抱怨。在发布的事件报告中能看到，GitHub承认，在宕机期间，“多项服务性能下降，特别是问题报告、拉取请求和 API。”历经大约两小时的故障排查与修复工作后，GitHub恢复服务和功能运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而GitHub也在解决问题后在X上作出回应，“本次故障已完全解决。感谢各位在问题处理期间的耐心与理解。故障根本原因的详细分析报告，将在完成后第一时间对外公布。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d75e6c0f283f5e1b475199d0e19607b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于此事，网友纷纷表示，“GitHub 的风险太大了。一家公司不应该有能力拖垮全世界的整个开发流程。”“为什么这么多项目都只托管在 GitHub 上？没有一个镜像站点，所有功能都依赖于 GitHub，成千上万的人甚至都没有考虑过其他方案…”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人怀疑，“问题可能出在 Copilot 上。”也有人说道，“我们目前尚不能确定这是 Copilot 的问题。不能仅仅因为微软正愈发强制其开发者使用 GitHub Copilot，还吹嘘有大量内部代码由 GHCP 编写，且在此期间 GitHub 发生了数次严重宕机，就认定这些宕机与 Copilot 有关。我们不妨静观其变，等微软宣称此事与 GHCP 毫无关联时，我们就能确定GitHub 宕机和 Copilot 真的没关系了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Gentoo Linux将迁出GitHub，导火索直指Copilot&amp;nbsp;&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，已有企业在因&amp;nbsp;GitHub 强制推行 Copilot 工具的举措而放弃这一平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Gentoo Linux 当前正计划将其仓库从 GitHub 全面迁移。而迁移的导火索，正是GitHub 试图“强制代码库使用 Copilot”。Gnoppix 是领先的开源 AI Linux 发行和服务提供商，以其通过 Portage 实现高度可定制的基于源码的包管理系统而闻名，他们一直依赖 GitHub 托管其主要的 git 仓库，包括作为数千个 ebuild 上游源代码的关键 gentoo.git 树。这一基础设施支持了一个全球开发者和用户社区，他们欣赏Gentoo在编译针对特定硬件架构和优化标志软件方面的灵活性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gentoo 社区的讨论中，提及了一个关键事件：GitHub 会在代码仓库页面自动弹出横幅提示，敦促贡献者 “启用 Copilot”，并警告不配合的仓库将面临曝光度降低或功能受限的后果。Gentoo 开发者表示，这类干预行为严重干扰了正常开发流程，在合并请求与代码评审环节强行插入未经请求的 AI 代码建议。Gentoo 理事会一名成员发文称：“GitHub 正公然试图强迫我们的代码仓库启用 Copilot，而这是我们明确反对的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以 Gentoo 维护者为代表的批评者认为，该工具的运作模式在开源许可证的精神与条文层面均构成侵权。Gentoo 的大量软件包构建脚本（ebuild）及专属代码，均采用 GNU 通用公共许可证（GPL）或知识共享协议的衍生版本授权，这类协议通常要求衍生作品需以兼容的授权条款进行共享。而 Copilot 的 “黑箱” 训练流程，使得外界无法判断其生成代码是否构成衍生作品，这就可能导致专有软件在未经署名或未遵循互惠原则的情况下，挪用开源代码成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“再见了，Github，欢迎Codeberg。”1月5日，Gentoo Linux在发布的2025 年度项目回顾报告中披露迁移细节：受持续遭遇的 GitHub Copilot 强制启用相关争议影响，Gentoo 目前正评估并计划将代码仓库镜像及合并请求贡献渠道迁移至 Codeberg 平台。Codeberg 是一个基于 Forgejo 搭建的代码托管网站，由非营利组织维护，服务器部署于德国柏林。Gentoo 核心的 Git 代码库、问题工单系统等基础设施仍由官方自行托管，且暂无变更计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，有外媒报道称，Gentoo 的迁移计划将分阶段推进。初期工作将聚焦于 gentoo.git 核心代码仓库，目标在数月内完成迁移。Gentoo 基础设施团队已在多款备选代码托管平台完成镜像原型部署，评估维度涵盖 Git 托管可靠性、问题追踪系统集成能力、持续集成 / 持续部署（CI/CD）流水线支持以及网页端代码浏览体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移备选方案还包括GitLab、SourceHut，以及在 Gentoo 内部服务器部署 Gitea 等同类平台。其中，自托管方案可实现对代码资产的完全掌控，但需投入额外的运维成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;个人开发者成批“逃离”，Copilot疑似承认再利用开源代码&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;个人开发者们对GitHub的类似不满也十分严重。在各大开发者社区及讨论平台上，都有愤怒的GitHub用户表示，他们应该摆脱强制使用的Copilot功能。过去一年来，GitHub 平台上的开发者们最热门的讨论之一也是：是否应该阻止微软的AI服务Copilot自动在代码仓库中生成问题和拉取请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月6日，有开发者在 GitHub上提交一项讨论，称其发现“Copilot似乎公开承认其对开源代码进行了再利用（或盗用？），且未遵守署名规定”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c83d109dac605fbae649f4f0173c8212.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，微软似乎拒绝禁用这些功能，导致许多开源软件开发者开始质疑是否存在其他替代方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者Andi McClure，在去年1月向微软Visual Studio Code代码仓库提交请求，抗议自己卸载 Copilot 扩展程序后，VS Code 界面中却再次出现 Copilot 图标。一位名叫 Constantine 的开发者在 McClure 的帖子下写道，“今天我拒绝了 Copilot 为我的 PR 生成的两条代码建议，这非常令人不安，所以我开始搜索，然后找到了这个讨论。拒绝使用AI对我来说是原则问题，所以如果这种情况继续下去，微软又不尽快提供让我选择退出代码库AI的方法，我会把我的代码迁移到自托管的解决方案上，并且永远不会再回到 GitHub。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年9月，McClure在一封邮件中表示，“此前每当 Copilot 干扰我的 GitHub 使用体验时，我都会在 GitHub 社区反馈区提交问题工单。我强烈反感的是，Copilot 表面上似乎在未经许可的情况下，利用我发布在 GitHub 上的代码进行训练，违反了我设定的开源许可协议；而 GitHub 还要在我眼前反复推送这款我绝不会使用的工具，简直跟广告没两样。既然这件事已经对我造成困扰，我认为没有理由再保持沉默。我觉得，我们之所以会被迫接受一些大家都不认同的事物，部分原因就是我们选择了忍气吞声。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;McClure还表示，自 GitHub 从微软旗下一家独立子公司，被划归至微软核心AI部门（CoreAI group）后，“开源社区的态度似乎已从抱怨Copilot变为开始主动远离GitHub。”据称，其在开源社区的不少同仁都在讨论从 GitHub 迁移至 Codeberg，或是自建基于 Forgejo 的托管平台（Codeberg 正是基于 Forgejo 搭建）的计划。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“微软对Copilot的推广似乎完全是自上而下的，高层似乎已经彻底忘记了客户留存等传统目标。不管出于什么原因，他们只想提升‘AI’指标，把客户群仅仅当作提升这些指标的工具。”McClure认为，人们已经开始厌倦这种情况，如果持续下去，将会削弱开发者与GitHub之间的网络联系，加速开发者进一步迁移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.githubstatus.com/incidents/q987xpbqjbpl&quot;&gt;https://www.githubstatus.com/incidents/q987xpbqjbpl&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.gentoo.org/news/2026/01/05/new-year.html&quot;&gt;https://www.gentoo.org/news/2026/01/05/new-year.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://forum.gnoppix.org/t/gentoo-linux-plans-migration-from-github-over-attempts-to-force-copilot-usage-for-our-repositories/3934&quot;&gt;https://forum.gnoppix.org/t/gentoo-linux-plans-migration-from-github-over-attempts-to-force-copilot-usage-for-our-repositories/3934&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/H16Z6V1Cz3Sf1qeb4fwr</link><guid isPermaLink="false">https://www.infoq.cn/article/H16Z6V1Cz3Sf1qeb4fwr</guid><pubDate>Fri, 16 Jan 2026 10:00:00 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>拒绝传统Router“瞎指挥”，多智能体如何实现智能任务分配？</title><description>&lt;p&gt;企业级多智能体（Multi-Agent）系统最大的瓶颈，往往不是Agent不够强，而是负责分发任务的Router（路由器）太“傻”。传统Router只会做简单的单选分类，面对复杂的企业级故障经常“瞎指挥”，在企业运维的十字路口，我们需要一个更聪明的“交警”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年里，Multi-Agent架构正在成为企业AI的新基建。我们忙着造更强的SQLAgent、更快的检索Agent，但却发现运维系统的十字路口却越来越拥堵了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和想象中的Agent们“游刃有余”的自动协同、分工协作不同，因为传统Router的上限太低、智能程度有限，很难跟上Agent们“匆匆忙忙”的脚步。在未来的企业&amp;nbsp;AI&amp;nbsp;系统中，Agent越来越多，能力边界越来越模糊，系统必须具备“承认不确定性并协作解决”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，腾讯云正式开源TCAR（Tencent Cloud Andon Router）——一个只有4B参数，但学会了“先想清楚，再选择”的智能路由模型，它专为解决跨域、冲突和模糊问题而生，为企业AI应用提供Reasoning-centric Routing+Multi-Agent Collaboration的基础形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么传统Router在企业运维场景里“玩不转”了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里可以看几个非常常见的场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1、不同agent可能能解决一样的问题，传统Router通常为单标签分类，只考虑选择一个agent，导致无法给出最优解决方案。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59f3d3f5d8df472e3793060967380b4a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;2、新业务、新Agent随时上线，传统Router对这些“新同事”完全不了解，需重新训练，也就无法快速分配给他们最合适的工作。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/89/890d442348431d5d2f3f2804da184552.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;3、用户描述模糊、不完整。例如用户提到“网站访问延时”，传统Router就无法确定不确定是CDN、COS还是网络的问题。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c101e510649ddacd2de749050e74ad2e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;4、传统Router缺乏可解释性，黑盒决策，一旦路由错了，没法快速修复badcase，后面Agent再强也救不回来。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/1972e99ed6a4ef6584029842603b9095.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;总结来说，传统Router面对企业场景有三大硬伤：搞不定跨域、解不了冲突、跟不上变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;TCAR的解法：像人类专家一样“先想后做”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCAR（TencentCloudAndonRouter）的核心很简单，但在Router中几乎没人认真做过——把路由从直接预测标签，变成先推理再选择Agent集合。这时候，Router不再是一个收发任务的转接系统，而是变成了一个具备推理能力的“决策者”。它把路由过程从单项选择变成了“写分析报告+组建任务组”；它的工作职能从挑选队列最前面的agent完成任务，到在专家梯队中找到最合适的那个人选来完成任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它就像是一个拥有顶尖专家团队的，高度聪明且能够自我决策的“项目经理”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能力一：Reason-then-Select（拒绝黑盒，把思考过程写出来）&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c898827dd2a674d104464ca6a3e4c1a2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;TCAR在输出Agent之前，会先生成一段自然语言推理链，明确说明问题可能涉及哪些技术栈，不同Agent的职责边界，为什么多个Agent执行是合理的，这让路由不再是黑盒，而是可解释、可Debug、可持续优化Agent描述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能力二：从单挑到团战&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/accbee1a4fa500c49d98485ba40d8f61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;在TCAR中路由结果不再是one-hot，而是一个Agent子集，这一步直接解决了企业系统中最棘手的Agent冲突问题：不强行压缩决策，而是保留不确定性，交给后续协作解决。当然，这也要建立在对指令聪明且充分的理解力上。&lt;/p&gt;&lt;p&gt;能力三：专家会诊，择优输出&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/7944b8f50fc5ca92c5a34c6b8437a3c7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;当TCAR选出多个候选Agent后，每个Agent独立给出自己的专业答案，而后由一个RefiningAgent负责对比、消歧、融合，最终输出一个完整、无冲突的答案，这套模式在排障类问题上效果尤其明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;覆盖全面、命中精准，硬核且强大&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCAR不是一个简单的Prompt工程产物，为了让它具备上述能力，我们做了两件比较特别的事情：&lt;/p&gt;&lt;p&gt;一是两阶段训练+特殊融合，兼顾推理能力和选择精度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶段一SFT（监督微调）：教会模型结构化推理，学会输出Agent集合，通过Slerp方法融合模型。阶段二RL（强化学习/DAPO）：重点调教模型“选得对不对”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;二是专门针对多Agent设计奖励函数，把路由当成一个集合预测问题，在模型覆盖率和精确度之间形成稳定平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;R1奖励（类似精确率Precision）：你选出来的Agent里，有多少是真正干活的？（防止选了一堆没用的配角）R2奖励（类似召回率Recall）：关键的那几个Agent，你有没有漏掉？（防止漏掉主角）长度惩罚：防止模型为了求稳把所有Agent全选上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，经过CLINC150、HWU64、MINDS14、SGD、Qcloud五个数据集的评测，TCAR&amp;nbsp;在企业高冲突数据上全面超过当前主流大模型&amp;nbsp;Router，在高歧义、跨域问题中更稳定，4B&amp;nbsp;参数量推理速度快成本低，更重要的是下游多Agent + Refining Agent的整体成功率显著提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;腾讯云还提供了全套的完整开源范式，包括：TCAR&amp;nbsp;路由模型（4B）、Prompt&amp;nbsp;规范（Router / Refining Agent）、训练方法与实验细节、可直接落地的多&amp;nbsp;Agent&amp;nbsp;路由范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相关链接：&lt;/p&gt;&lt;p&gt;HuggingFace：https://huggingface.co/tencent/TCAndon-RouterGitHub：https://github.com/Tencent/TCAndon-RouterPaper：https://arxiv.org/pdf/2601.04544&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cYlRMETcNGxhDvBqCDII</link><guid isPermaLink="false">https://www.infoq.cn/article/cYlRMETcNGxhDvBqCDII</guid><pubDate>Fri, 16 Jan 2026 09:20:05 GMT</pubDate><author>腾讯云</author><category>腾讯</category><category>生成式 AI</category></item><item><title>聚焦 AI Agent 系统诊断，智能运维助手 SysOM MCP 正式开源</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AIOps 新范式：说句话就能做运维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，操作系统运维面临复杂架构、依赖关系混乱、故障定位难、依赖人工经验、工具碎片化、监控不足及自动化欠缺等挑战。为应对以上难题，阿里云结合大语言模型（LLM）、智能体（Agent）与模型上下文协议（MCP），实现了自然语言驱动的智能运维：LLM 理解指令，Agent 自主执行任务，MCP 连接底层诊断工具。这三者的协同，使 AI 助手能自动诊断系统问题，生成报告与修复建议，显著提升效率，推动运维向主动智能演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云操作系统控制台（以下简称操作系统控制台）是一站式操作系统运维管理平台，提供了内存、I/O、网络、内核崩溃等强大的系统诊断能力，SysOM是操作系统控制台的运维组件。但这些功能通常需要用户登录控制台，并具备一定的运维经验才能有效使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 助手（如 Qwen Code）的普及，用户更希望用自然语言一句话解决问题，比如“为什么 CPU 变高了？”为此，SysOM 将原有诊断能力通过 MCP（Model Context Protocol）进行标准化封装，推出开源项目&amp;nbsp;SysOM MCP。SysOM MCP 脱胎于阿里云操作系统控制台，把复杂的运维操作转化为 AI 可直接调用的标准工具，让 AI Agent 能像专业工程师一样“动手”诊断系统问题——用户无需懂命令，只需用自然语言提问，即可获得精准的系统级分析。如今，SysOM MCP 正在推动自然语言成为操作系统诊断的新入口，让智能运维真正走向普惠与高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 项目开源地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/sysom_mcp&quot;&gt;https://github.com/alibaba/sysom_mcp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SysOM MCP：用自然语言驱动系统诊断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统运维依赖命令行和专家经验，而通用 AI 虽能“说”却不能“做”。SysOM MCP&amp;nbsp;的出现填补了这一鸿沟——通过 MCP 协议，AI 不仅能理解问题，还能自动执行真实诊断，实现从“问答”到“行动”的闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 项目内置超过 20 个生产级诊断工具，全部通过标准 JSON-RPC over stdio/SSE 暴露，包括：&lt;/p&gt;&lt;p&gt;内存分析：内存全景诊断、Java 内存诊断、OOM 内存诊断；IO 诊断：IO 一键诊断、IO 流量分析诊断；网络排查：网络丢包诊断、网络抖动诊断；调度诊断：系统负载诊断、调度抖动诊断；磁盘诊断：磁盘分析诊断；宕机诊断：宕机诊断（dmesg 分析）、宕机诊断（vmcore 深入分析）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目支持&amp;nbsp;--stdio&amp;nbsp;（本地嵌入）和&amp;nbsp;--sse（HTTP 服务）两种模式，轻松集成各类 AI 客户端。&lt;/p&gt;&lt;p&gt;要在支持 MCP 协议的 AI Agent 平台（如 Qwen Code）中使用 SysOM MCP，首先需将项目代码克隆到本地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;git clone https://github.com/alibaba/sysom_mcp.git
cd sysom_mcp&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再在配置文件中添加如下配置，就可以让 AI 助手能以自然语言驱动操作系统及运维操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;{
&amp;nbsp;&amp;nbsp;&quot;mcpServers&quot;: {
&amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;sysom_mcp&quot;: {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;command&quot;:&amp;nbsp;&quot;uv&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;args&quot;: [&quot;run&quot;,&amp;nbsp;&quot;python&quot;,&amp;nbsp;&quot;sysom_main_mcp.py&quot;,&amp;nbsp;&quot;--stdio&quot;],
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;env&quot;: {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;ACCESS_KEY_ID&quot;:&amp;nbsp;&quot;your_access_key_id&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;ACCESS_KEY_SECRET&quot;:&amp;nbsp;&quot;your_access_key_secret&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;DASHSCOPE_API_KEY&quot;:&amp;nbsp;&quot;your_dashscope_api_key&quot;
&amp;nbsp; &amp;nbsp; &amp;nbsp; },
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;cwd&quot;:&amp;nbsp;&quot;&lt;sysom mcp项目目录=&quot;&quot;&gt;&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;timeout&quot;:&amp;nbsp;30000,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;trust&quot;: false
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }
}&lt;/sysom&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最佳实践：谈话间揭秘隐藏内存泄漏&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;amp;mid=2247489171&amp;amp;idx=1&amp;amp;sn=478ea3261ce3d329ed55e05331c080a6&amp;amp;scene=21#wechat_redirect&quot;&gt;OS Copilot&amp;nbsp;&lt;/a&gt;&quot;是阿里云基于大模型构建的操作系统智能助手，支持自然语言问答、辅助命令执行、系统运维调优等功能，帮助您更好地使用 Linux 系统，提高 Linux 的使用效率。目前，操作系统控制台上的 OS copilot 已接入 SysOM MCP，用户只需在操作系统控制台中以自然语言与 OS Copilot 对话，即可自动触发操作系统问题的根因排查。整个诊断过程无需人工干预，结果以结构化形式清晰呈现，大幅降低运维门槛，让复杂问题“一问即解”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文以隐蔽的内存泄漏为例，展示 SysOM MCP 的诊断功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5f/5f472f9db24e87b31f8202d62b8823fc.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/932801e5554e51e1584245c35663a8f9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以看到上图的对话中，OS Copilot 给出了可能的泄漏原因。同时也可以点击图中下方的诊断报告，在操作系统控制台查看更详细的诊断结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/92/9246802fc8116d85f45d2778d5c1b60f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 脱胎于阿里云操作系统控制台，诊断工具已在大规模生产环境验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下载地址&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目采用现代 Python 工具链（uv + Python 3.11+），安装简单：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;git clone https://github.com/alibaba/sysom_mcp.git
cd sysom_mcp &amp;amp;&amp;amp; uv sync
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持一键启动：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;uv run python sysom_main_mcp.py --stdio &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# 供本地调用 &amp;nbsp;
uv run python sysom_main_mcp.py --sse --port 7140 &amp;nbsp;# 启动 HTTP 服务 &amp;nbsp;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用场景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 可接入各种 AI agent，帮助您打造具备系统诊断能力的智能助手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开源共建&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🌟&amp;nbsp;GitHub 地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/sysom_mcp&quot;&gt;https://github.com/alibaba/sysom_mcp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欢迎 Star、Fork、提交 Issue，一起构建 AI 原生运维新生态！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;联系我们&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若想使用更全面的 SysOM 功能，请登录阿里云操作系统控制台体验，地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://alinux.console.aliyun.com/&quot;&gt;https://alinux.console.aliyun.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您在使用操作系统控制台的过程中，有任何疑问和建议，可以扫描下方二维码或搜索群号：94405014449&amp;nbsp;加入钉钉群反馈，欢迎大家扫码加入交流。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b25287aa24b6e95b58056b5056ca0927.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;操作系统控制台钉钉交流群&lt;/p&gt;</description><link>https://www.infoq.cn/article/285HkzS0EM8tpHg9mBgY</link><guid isPermaLink="false">https://www.infoq.cn/article/285HkzS0EM8tpHg9mBgY</guid><pubDate>Fri, 16 Jan 2026 08:46:00 GMT</pubDate><author>万瑞萍</author><category>阿里巴巴</category><category>操作系统</category></item><item><title>亚马逊云科技发布第五代 Graviton 处理器，M9g 实例同步登场</title><description>&lt;p&gt;亚马逊云科技近日宣布了&lt;a href=&quot;https://www.aboutamazon.com/news/aws/aws-graviton-5-cpu-amazon-ec2&quot;&gt;全新的 Graviton5 处理器&lt;/a&gt;&quot;，并预览了首批基于该处理器的 EC2 实例——通用型 M9g 实例。据亚马逊云科技介绍，Graviton5 相比 Graviton4 最高可提升 25% 的性能，并首次引入 Nitro 隔离引擎（Isolation Engine），同时配备更大的 L3 缓存，在延迟、内存带宽和网络吞吐量方面均有所改善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据新闻稿，基于 Arm 架构的 &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/m9g&quot;&gt;EC2 M9g 实例&lt;/a&gt;&quot;单实例最多支持 192 个 CPU 核心。更高的核心密度使核间延迟最多降低 33%，并提升了带宽表现，从而改善了数据库、分析型负载、应用服务器、游戏以及电子设计自动化（EDA）等工作负载的扩展能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Graviton5 将 Nitro 隔离引擎纳入 Nitro 系统。这是一个全新的引擎，利用形式化验证来证明不同工作负载之间，以及工作负载与亚马逊云科技运维人员之间的隔离性。该引擎拥有规模较小且经过验证的代码库，亚马逊云科技还将向客户开放其实现及相关证明，以供审查。亚马逊云科技内核和虚拟化工程师 Mohamed Mediouni &lt;a href=&quot;https://www.linkedin.com/posts/mohamed-mediouni-kaos_graviton5-introduces-the-nitro-isolation-activity-7402401265080664064-o_cD/&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一个厂商自研的 hypervisor（说白了，就是对 KVM 的替代）本身就很有意思。至于这条路最终会走向哪里，还有待观察。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在最近的 re:Invent 大会上，亚马逊云科技还介绍了主题为 《&lt;a href=&quot;https://www.youtube.com/watch?v=hqqKi3E-oG8&quot;&gt;引入 Nitro 隔离引擎：用数学证明实现云端透明安全&lt;/a&gt;&quot;》 的演讲，目前已在油管上线。亚马逊副总裁兼杰出工程师 &lt;a href=&quot;https://www.linkedin.com/posts/a-saidi_aws-reinvent-2025-introducing-nitro-isolation-activity-7403510493107126272-1b-O?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAABaQ5R4B1z_TPIVzQKBvbJ9SpDn29zaiJcY&quot;&gt;Ali Saidi 表示&lt;/a&gt;&quot;：“Nitro 隔离引擎利用 Rust 和形式化验证，打造了一个经过数学证明的云 hypervisor，为云安全树立了新的标准。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技客户对 Graviton 的采用率持续增长。根据官方数据，亚马逊云科技新增 CPU 容量中已有超过 50% 来自 Graviton；在最近一次亚马逊会员日活动中，Amazon.com 使用的 EC2 计算资源中，有超过 40% 由 Graviton 提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在硬件层面，Graviton5 的 L3 缓存容量是 Graviton4 的五倍，单核可用 L3 缓存增加至原来的 2.6 倍，降低了内存访问的延迟。其内存速度也高于基于 Graviton4 的 M8g 实例，对大规模、内存密集型工作负载更为有利。官方还指出，网络带宽平均提升可达 15%，EBS 带宽最高提升 20%，整体网络吞吐能力相比上一代实例最高可提升两倍。不过在 Hacker News 上，用户 diath &lt;a href=&quot;https://news.ycombinator.com/item?id=46171008&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;没有基准测试，没有 FLOPs，也没有和通用硬件的对比……“9 比 8 快，8 比 7 快，7 比 6 快，……1 最慢，但具体多快谁也不知道。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Reddit 上的讨论则褒贬不一，主要质疑集中在技术细节披露不足，以及 M9g 目前仅在部分区域以预览形式提供。用户 Ill-Side-8092 &lt;a href=&quot;https://www.reddit.com/r/aws/comments/1penqtp/comment/nsfr0v9/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;作为一次自然的增量更新，这当然是件好事，而且 Annapurna 团队算是当前 AWS 中少数仍在进行真正创新的团队之一。不过现实是，在大厂之间，自研芯片如今已经成了“入场门槛”——谷歌、苹果和微软也都有相当成熟的自有芯片方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有用户指出，亚马逊云科技在不同区域之间的服务和功能一致性仍然存在明显差距。用户 Rude_Walk &lt;a href=&quot;https://www.reddit.com/r/aws/comments/1penqtp/comment/nsljsh4/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;要不先把 Graviton4 全面铺开？新加坡这种重要区域现在都还没支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技表示，Graviton5 的早期采用者包括 Adobe、Epic Games 和 Pinterest 等公司。目前，亚马逊云科技已开放 M9g 实例预览版的申请页面，供用户&lt;a href=&quot;https://pages.awscloud.com/AWSGraviton5-AmazonEC2M9ginstances-Preview.html&quot;&gt;报名体验&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/xtXzsEPA5rVb6ZGfpUyh</link><guid isPermaLink="false">https://www.infoq.cn/article/xtXzsEPA5rVb6ZGfpUyh</guid><pubDate>Fri, 16 Jan 2026 08:23:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>框架</category></item><item><title>OpenAI前团队创业内乱，CTO泄密竞对遭开除！翁荔火速发文</title><description>&lt;p&gt;过去一年里，前 OpenAI 首席技术官 Mira Murati 创办的 Thinking Machines Lab，一度被视为“OpenAI 体系外最值得关注的实验之一”。然而，这家成立时间并不长、却已完成 20 亿美元种子轮融资的明星公司，如今正迎来成立以来最关键的一次人员震荡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间周三，Murati 在社交媒体上宣布了公司联合创始人兼 CTO Barret Zoph的离职。“我们已与Barret Zoph分道扬镳，”Murati在X平台上发帖称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Soumith Chintala 将出任Thinking Machines的新任 CTO。他是一位才华横溢、经验丰富的领导者，十多年来为人工智能领域做出了重要贡献，也是我们团队的重要成员。我们非常高兴他能承担起这项新的职责。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Murati 声明中的用词较为克制，仅强调“双方已分道扬镳”，并未对离职原因、过程或分歧作出任何解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9a/9a65a758cca557efb2e70169a5d27057.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据有人获悉的内部消息，此次是由于 Barret Zoph 个人的不道德行为（向竞争对手公司泄漏了公司机密），Thinking Machines Lab 才解雇了他。Mira Murati 甚至是在全体员工大会上宣布了这一消息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/835dfa3bbd8a99f4a7d779a7c6fff42c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但不到一小时后，另一则公开信息迅速改变了外界对这一事件的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 应用部门 CEO Fidji Simo同样在 X 平台发文，宣布欢迎 Barret Zoph、Luke Metz 以及 Sam Schoenholz “回归 OpenAI”，并明确表示这一安排“已筹备数周”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Simo在发文中称，“Barret 将向我汇报工作；Luke 和 Sam 将向 Barret 汇报工作。关于他们未来的工作重点，敬请期待！”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f4/f458d4cd3910684874a5d4c0177cc084.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;短短 58 分钟内，两个高度相关却几乎没有交集的公开声明，构成了一次罕见而耐人寻味的“信息对冲”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从结果看，这不仅是 Thinking Machines Lab 失去了一位联合创始人，更是一次系统性的“回流”：三位核心技术人物，几乎同时从 Murati 的创业公司转向 OpenAI。对于一家尚未正式推出核心产品、却以“重塑通用人工智能研究方式”为愿景的初创公司而言，这一变化的象征意义，远大于人员数量本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着事情的发酵，Thinking Machines Lab 联合创始人、前 OpenAI 安全研究副总裁翁荔（Lilian Weng）也在X上发声了，她的发文中没有明确就此事展开评论，而是发了一些个人感慨：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/ac30f030960f27c584c284ece3bba8a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从 OpenAI 核心到创业阵营&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要理解这次事件的分量，必须回到几位关键人物的背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mira Murati 曾长期担任 OpenAI 的首席技术官，是 GPT-4 等核心模型研发阶段的重要管理者之一。她在 2024 年 9 月离开 OpenAI，此举一度被外界解读为 OpenAI 高层频繁变动背景下的又一次重要分叉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;离职后不久，她便与 Barret Zoph、Luke Metz 等人共同创立 Thinking Machines Lab，并亲自出任首席执行官。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a01fd91c64633279d997c7ed5145f7a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Thinking Machines Lab CEO、OpenAI 前CTO Mira Murati&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Zoph 的履历在 AI 研究界极具分量。他曾担任 OpenAI 研究副总裁，在此之前，他在谷歌担任研究科学家长达六年，是深度学习与架构搜索方向的重要研究者之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 OpenAI 期间，他负责后训练（Post-Training）研究，这包括模型对齐、工具调用、评估体系、ChatGPT 性能提升、搜索功能以及多模态能力开发等关键项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其博客个人介绍中也明确指出，他在 OpenAI 工作期间 “他的团队负责对齐、工具使用、评估、ChatGPT、搜索和多模态等功能，并训练用于 ChatGPT 与 API 的模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些职责意味着 Zoph 在当时直接参与了 OpenAI 核心模型（例如 ChatGPT 与大型语言模型系列）从研究阶段到产品化前训练阶段的关键工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/31/310edd24ffe4ad0adff6520b3e035213.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Thinking Machines Lab 前 CTO、 OpenAI 前研究副总裁 Barret Zoph&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Luke Metz 同样曾在 OpenAI 工作多年，曾是 OpenAI 团队的创始成员之一。他与 John Schulman、Barret Zoph、Liam Fedus 以及其他许多人一起，开发了ChatGPT。在此之前，他曾在 Google Brain 担任研究科学家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在进入行业之前，Metz 就已活跃于机器学习研究领域，其学术作品涉及优化、元学习、生成模型等多个方向。从他在 ResearchGate 和 Semantic Scholar 上的发表记录可以看出，他参与了多篇关于深度学习、学习优化策略和元学习的论文，这些论文的引用量很高，表明其研究在社区内部具有较强影响力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e8/e8598c453903bbdfd6c54c89ff28734b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 前核心成员 Luke Metz&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Schoenholz 也有 OpenAI 工作经历，在 OpenAI 工作期间，他参与了多个生成式模型研发相关的体系贡献，但官方报道并未具体列出他的单项项目头衔或单独负责成果。他的 LinkedIn 页面在事件发生后仍显示其供职于 Thinking Machines Lab。在加入OpenAI之前，他也曾供职于谷歌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5da470f9ef0afb80027ba6771f07cde1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI前研究员 Sam Schoenholz&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然 Thinking Machines Lab 的官方公告未单独提到 Schoenholz 的离职，但 OpenAI 的声明中明确包括他，表明他确实从Thinking Machines Lab 转向 OpenAI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因如此，Thinking Machines Lab 从一开始就被视为一次“前 OpenAI 核心技术团队的集体外溢”。这一判断并非空穴来风。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;高调的AI创企，首个产品没砸出任何水花&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司自成立之初就定位于 构建通用、更可定制、更易理解的 AI 系统，希望让 AI 能适应人类多种需求，并让研究社区与开发者更容易使用最前沿的模型能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成立后不到半年，Thinking Machines Lab 在 2025 年 7 月完成了规模高达 20 亿美元 的种子轮融资，由风险投资机构 Andreessen Horowitz（a16z） 领投，多家机构参与，包括 Nvidia、AMD、Accel、Cisco、ServiceNow 和 Jane Street 等。该融资使公司估值达到约 120 亿美元，成为当时成立尚短、尚无收入、尚未明确推出产品的 AI 初创公司中估值最高的一例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一融资规模之所以备受关注，主要来自两个方面：一是资本对核心人才的押注：资本市场对包括 Murati 在内的“OpenAI 系创业团队”给予了极高的信任，认为该团队能够开拓新一代 AI 研究与产业方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;二是对 AI 未来路径的押注：投资者愿意用“种子轮”阶段资金支持一个尚未推出产品的团队，反映出市场对 AI 基础研究与基础设施工具方向的高度预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这个阶段，公司极少透露技术细节、产品计划或战略路线图，外界对其业务进展了解甚少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到 2025 年 10 月，Thinking Machines Lab 才首次对外推出公司开发的 第一个产品——Tinker。这是公司成立之后首次公开可用于外界访问的软件工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Tinker 是一个 面向 AI 开发者和研究者的 fine-tuning（微调）工具 API，旨在让用户无需管理复杂的训练基础设施，就能对大型语言模型进行定制训练。具体来说：它为语言模型提供微调的 API 接口，让开发者可以通过几行代码选择不同模型（如 LLaMA、Qwen 等），进行任务定制。 官方宣传称该产品旨在“将前沿 AI 能力民主化，让更广泛的研究者和开发者有机会实验与定制模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，首款产品问世后，市场反馈保持审慎甚至怀疑的态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些用户认为，目前许多大模型平台（包括 OpenAI、Anthropic、Meta 等）已有 fine-tuning 工具，因此 Tinker 并不构成明显的“突破性产品”，存在独特性不足的短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，根据少量外媒评述，Tinker 的用户基础尚处于探索期，没有展现出“行业级爆款”效果，而只是一个在开发者社区和实验环境中被尝试的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推出时间距离融资完成有较长滞后，且产品本身尚处于早期阶段，并未显现出对现有大模型平台的明显替代或补充效应，因此外界对Thinking Machines Lab的评价愈发消极。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种情况下，往往更容易暴露出创始团队的理念分歧。因此也就有了如今创始团队“内讧”的局面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/56/5632762a27f2a490f96c3193fa6f299b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截图源自网络&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;资本充足，为什么还会出现人员流失？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wired 的相关报道指出，Zoph 与 Thinking Machines Lab 的分手“并不友好”。虽然报道并未披露具体冲突细节，但这一描述本身，已经与 Murati 在公开声明中所采用的克制措辞形成了明显对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，Murati 的声明中并未提及 Luke Metz 与 Sam Schoenholz 的去向，也未就公司内部是否发生更广泛的人事变动作出说明。而 OpenAI 方面的表态，则一次性点名三人，并强调这一回归“已筹备数周”。这意味着，至少在时间线上，相关沟通并非临时决定，而更可能是在 Thinking Machines Lab 内部结构尚未对外公开调整之前，就已基本敲定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一时间差，为外界留下了足够的解读空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从 OpenAI 的角度看，这次人员回流同样耐人寻味。近年来，OpenAI 在模型研发、产品化和组织治理层面都经历了快速变化。应用部门的地位不断上升，通用模型之外，如何将能力转化为可持续的产品与平台，成为公司内部的重要议题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Fidji Simo 的表态，释放了一个清晰信号：这些回归并非偶发，而是被视为战略性补强。Zoph 曾负责 OpenAI 的研究工作，Metz 与 Schoenholz 也熟悉公司内部体系。在一个研发复杂度不断提高、组织规模持续扩张的阶段，重新吸纳“熟悉文化、具备研究深度”的技术骨干，本身就是一种降低组织摩擦的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与之形成对照的是，Thinking Machines Lab 仍处于“高度依赖创始人协作”的阶段。当联合创始人之间在研究方向、组织节奏或权责边界上出现分歧时，其冲击往往更直接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次事件的特殊性，还在于其发生在一家资金极其充裕的初创公司内部。20 亿美元的种子轮融资，本应为团队提供充足的试错空间。但事实表明，资金并不能消解所有结构性张力，尤其是在高密度智力劳动与强价值主张并存的组织中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次事件已经为外界提供了一个极为罕见的观察窗口：当顶级 AI 人才、超级资本与宏大愿景同时汇聚时，真正决定组织命运的，往往不是融资规模或履历光环，而是能否在高度不确定的探索中，维持稳定而清晰的共同方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，以TechCrunch 为代表的多家外媒已联系 Thinking Machines Lab 与 OpenAI 寻求进一步置评，但截至报道时，双方尚未提供更多公开说明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友怎么看？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此事发生后迅速在社交媒体引发热议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Reddit平台，有用户评论称这次“AI 人才流转实在太正常了”，OpenAI 有能力快速重新吸引之前的员工，因为其资源和品牌影响力强大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位用户用半开玩笑的语气说他“想象 Zoph 像是 OpenAI 的间谍，被抓住后跑回去了（笑）”，表明社区对事件背后动机的好奇与揣测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54427aaae3eb86e16beab8831e071018.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次 Thinking Machines Lab CEO 与其他成员之间“不体面的分手”也让一些网友嗅到了OpenAI内部层级架构之间的不足之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X平台，有用户表示，“OpenAI 在人工智能领域是开放的，但在组织结构方面则不然。&lt;/p&gt;&lt;p&gt;层级结构依然存在。大型团队就是这样运作的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0165efc93224540ee7aed3ccbab2e16.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://dataconomy.com/2026/01/15/openai-rehires-top-talent-as-muratis-12b-startup-loses-co-founders/?utm_source=chatgpt.com&quot;&gt;https://dataconomy.com/2026/01/15/openai-rehires-top-talent-as-muratis-12b-startup-loses-co-founders/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/01/14/mira-muratis-startup-thinking-machines-lab-is-losing-two-of-its-co-founders-to-openai/&quot;&gt;https://techcrunch.com/2026/01/14/mira-muratis-startup-thinking-machines-lab-is-losing-two-of-its-co-founders-to-openai/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Luke-Metz?utm_source=chatgpt.com&quot;&gt;https://www.researchgate.net/profile/Luke-Metz?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://lukemetz.com/about/&quot;&gt;https://lukemetz.com/about/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/jP3NTaQpjTjP7U7FrSRW</link><guid isPermaLink="false">https://www.infoq.cn/article/jP3NTaQpjTjP7U7FrSRW</guid><pubDate>Fri, 16 Jan 2026 07:13:46 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>AWS CloudWatch成为一个统一的可观测性平台，支持Apache Iceberg</title><description>&lt;p&gt;亚马逊云科技宣布，&lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;Amazon CloudWatch&lt;/a&gt;&quot;实现重大增强，从一个基本的监控服务转变为一个统一的可观测性平台，能够整合多账户环境中的操作、安全和合规日志。本次更新解决了企业中一个长期存在的挑战：分散的日志管理需要多个工具和数据副本，每一个都会增加成本和复杂性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次更新的关键创新是&lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/s3-tables-integration.html&quot;&gt;通过Amazon S3表以兼容Apache Iceberg的方式访问日志数据&lt;/a&gt;&quot;，使组织能够在不使用ETL管道的情况下就地查询日志，同时保持与第三方分析工具的兼容性。这种方法，结合对&lt;a href=&quot;https://docs.aws.amazon.com/security-lake/latest/userguide/open-cybersecurity-schema-framework.html&quot;&gt;Open Cybersecurity Schema Framework&lt;/a&gt;&quot;（OCSF）和&lt;a href=&quot;https://opentelemetry.io/&quot;&gt;Open Telemetry&lt;/a&gt;&quot;（OTel）标准的原生支持，使CloudWatch成为像Splunk和Datadog这样的成熟的可观测性平台的潜在替代品（至少对于以亚马逊云科技云为中心的组织来说是这样）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudWatch现已原生支持跨账户和区域聚合预置日志，通过AWS Organizations整合AWS CloudTrail、Amazon VPC Flow Logs和AWS WAF访问日志等。此外，该服务还支持第三方数据源，包括：CrowdStrike、Okta、Wiz、Zscaler、Microsoft Office 365以及ServiceNow CMDB。CloudWatch为各类数据源提供了托管的OCSF转换服务，并&lt;a href=&quot;https://zerogrok.com/grok-patterns/&quot;&gt;通过Grok实现自定义解析与字段级操作&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudWatch将日志管理简化为一个内置治理功能的单一服务，消除了在不同工具中复制多个数据副本的需求。统一数据存储减少了ETL管道的复杂性，降低了运营成本和管理开销。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以在CloudWatch中使用自然语言或流行的查询语言，如LogSQL、PPL和SQL，通过单个界面运行查询。此外，他们可以通过兼容Apache Iceberg的表，使用自己喜欢的分析工具查询数据。新版&lt;a href=&quot;https://davidmorrill.github.io/facets/facets_ui.html&quot;&gt;Facets界面&lt;/a&gt;&quot;允许通过源、应用程序、账户、区域和日志类型简便地进行筛选，并通过智能参数推断实现跨账户和跨区域查询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a6/a681e0f444f22651035122687c0d3965.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：&lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-cloudwatch-introduces-unified-data-management-and-analytics-for-operations-security-and-compliance/&quot;&gt;亚马逊云科技新闻博客&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上的一篇&lt;a href=&quot;https://www.linkedin.com/posts/suresh-rajashekaraiah_amazon-cloudwatch-just-changed-the-game-activity-7404201064591167490-Owc5&quot;&gt;博文&lt;/a&gt;&quot;中，Mphasis架构师Suresh Rajashekaraiah指出，多年来企业一直苦于运营日志与安全日志存储分散的问题，这使得故障排查和合规流程变得复杂。然而，通过增强Amazon CloudWatch，这个问题得到了解决。该服务提供了一个统一的日志平台，整合并规范了来自其云服务和第三方的数据，支持更高效的查询。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，Corey Quinn通过他的AWS Snarkbot在Bluesky上发了这样一个&lt;a href=&quot;https://bsky.app/profile/aws-snarkbot.lastweekinaws.com/post/3m6zgsivyvx22&quot;&gt;帖子&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;CloudWatch如今做到了Splunk十五年前做的事，但每句话里云服务名称的数量远超实际内容。“统一数据存储”=S3加上额外的步骤和咨询账单。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然Splunk提供了跨Azure、GCP和本地环境等平台的可见性，但亚马逊云科技押注其原生集成和“零ETL”成本，这可以为他们赢得以亚马逊云科技云为中心的组织的青睐。此外，虽然像Datadog和Dynatrace这样的竞争对手提供了深入的应用性能监控和混合云UI，但它们产生的出口和索引费用通常高于亚马逊云科技采用的S3表“就地查询”模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开源替代方案，如ELK技术栈（Elasticsearch、Logstash、Kibana）和Grafana Loki，提供了供应商无关的统一日志管理，并且由社区驱动创新，不过它们需要组织管理自己的基础设施和运营复杂性。CloudWatch的托管服务方法消除了这种运营负担，但会将组织更紧密地绑定到亚马逊云科技的生态系统，对于寻求多云灵活性的团队来说，这会带来供应商锁定的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，除了AWS GovCloud（美国）区域和中国区域，Amazon CloudWatch的增强功能在所有AWS区域都已提供。要了解Amazon CloudWatch的定价详情，可查看&lt;a href=&quot;https://aws.amazon.com/cloudwatch/pricing&quot;&gt;定价页面&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-cloudwatch-unified-logs/&quot;&gt;https://www.infoq.com/news/2026/01/aws-cloudwatch-unified-logs/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/siglzhFYdV4bfKPfplrj</link><guid isPermaLink="false">https://www.infoq.cn/article/siglzhFYdV4bfKPfplrj</guid><pubDate>Fri, 16 Jan 2026 07:03:00 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>框架</category></item><item><title>首个智能体商业信任协议来了！支付宝携手千问App、淘宝闪购等发布AI商业协议ACT</title><description>&lt;p&gt;1月16日，支付宝联合千问App、淘宝闪购、Rokid、大麦、阿里云百炼等伙伴，正式发布ACT协议（Agentic Commerce Trust Protocol，智能体商业信任协议）。这是中国首个面向Agent 商业需求设计的开放技术协议框架，为 AI 与电商、外卖等服务平台的协同打造一套 “通用语言”，让跨终端、跨系统、跨平台的 AI 任务执行，变得更便捷、更高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e0d550b6327306c0a821430007191f9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以千问App为例，依托 ACT协议 ，千问App成功打通淘宝闪购与支付宝 AI 付：用户只需向千问发出指令 “帮我点杯珍珠奶茶”，千问基于用户地理位置，智能推荐附近符合需求的商品，同步完成比价与优惠券自动核销。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户仅需点击 “选它”，确认支付宝付款，即可一键完成结账。整个购物流程以对话式、自动化、不跳端的方式推进，千问化身专属 “购物助手”，包办繁琐操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当AI 的能力边界不断拓展，从“聊天对话”延伸至购物付款等“办事时代”，新的问题也随之浮现：AI 操作是否获得用户明确授权？资金交易过程是否足够安全？更换设备或应用后，服务体验能否保持连贯？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ACT 协议的诞生正是为破解这些问题而来。支付宝为其搭建了 “委托授权域”“商业交互域”“支付服务域”“信任服务域” 四大核心基础设施标准，实现 AI 操作全流程可追溯、可验证，让人更放心；支持自动化交易流程，减少不必要的人工干预，提升服务效率；统一多平台服务标准，避免体验的割裂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9df1ea4382f50cb3b8946230068cb42.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统付款模式不同，在ACT协议的规则框架下，AI 仅承担下单操作的执行角色，付款环节始终由用户主导或自主授权。在保障资金安全的前提下，为用户大幅节省时间成本。而对商家而言，未来接入 AI 原生应用时，只需按照协议标准配置统一接口，即可对接全渠道入口，无需单独进行复杂的 API 开发，大幅降低对接成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，ACT 协议可使用在AI 代买、企业自动化采购等多元场景，并提供两种付款模式：一是即时付款，用户与 AI 实时对话，基于推荐列表自主决策，确认后完成付款授权与身份验证，适用于 AI 点外卖、日常购物等高频场景；二是委托授权，用户可提前设定时间窗口、金额上限、商家范围等条件，即便离线无指令，AI 也能自动监测商品动态并完成下单结算，适用于机票、酒店预订等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该协议最大限度遵循兼容性、隐私性、开放性三大原则，全面适配现有商业与支付系统，并将伴随AI 行业技术发展持续优化。支付宝同时表示，正积极推动更多支付服务商、商家与平台、AI 开发者、智能终端生态厂商加入，共同完善协议内容，共建 AI 商业信任新生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着AI 原生应用能力的持续升级，“AI 代办” 服务日渐普及，支付作为其中特殊且关键的环节，正成为全球科技企业的布局焦点。此前，OpenAI 联合 Stripe 推出协议以支持 ChatGPT 结账功能；近期，谷歌也发布 AI 购物全流程通用商务协议（Universal Commerce Protocol，简称 UCP），将实现用户在 Gemini 内直接下单。&lt;/p&gt;</description><link>https://www.infoq.cn/article/DE0ifdKyd4Oevz5Bgr3X</link><guid isPermaLink="false">https://www.infoq.cn/article/DE0ifdKyd4Oevz5Bgr3X</guid><pubDate>Fri, 16 Jan 2026 03:11:39 GMT</pubDate><author>李冬梅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>谷歌发布 Gemma Scope 2，深化对 LLM 行为的理解</title><description>&lt;p&gt;&lt;a href=&quot;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&quot;&gt;Gemma Scope 2 是一套旨在解释 Gemini 3 模型行为的工具&lt;/a&gt;&quot;，使研究人员能够分析模型的突发行为，审核和调试 AI 代理，并针对越狱、幻觉和阿谀奉承等安全问题制定缓解策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;可解释性研究旨在理解 AI 模型的内部工作机制和学习算法。随着 AI 变得越来越强大和复杂，可解释性对于构建安全可靠的 AI 至关重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌将 Gemma Scope 描述为大型语言模型（LLM）显微镜。它结合了稀疏自编码器（SAEs）和转码器，让研究人员能够检查模型的内部表示，查看它“思考”的内容，并理解这些内部状态如何塑造了其行为。一个关键的应用场景是检查模型输出与其内部状态之间的差异，按照谷歌的说法，这可能有助于发现安全风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemma Scope 2 针对 Gemma 2 模型家族从多个方面扩展了原先的 Gemma Scope。最值得注意的是，它在 Gemini 3 模型的每一层中重新训练了其 SAEs 和转码器，包括 &lt;a href=&quot;https://arxiv.org/abs/2501.18823&quot;&gt;kip-transcoders&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot;&gt;cross-layer transcoders&lt;/a&gt;&quot; 。这些转码器旨在使多步计算和分布式算法更容易解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/Gemma_Scope_2_Technical_Paper.pdf&quot;&gt;解释说&lt;/a&gt;&quot;，增加层数直接增加了计算和内存需求。为了保持复杂性随层数线性增长，这需要设计专门的稀疏内核。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，谷歌采用了&lt;a href=&quot;https://arxiv.org/abs/2503.17547&quot;&gt;一种更先进的训练技术&lt;/a&gt;&quot;，使 Gemma Scope 2 有更强的能力来识别更有用的概念，同时也解决了初版实现中已知的几个缺陷。最后，Gemma Scope 2 引入了专门针对聊天机器人进行分析的工具，使研究人员能够研究复杂的多步行为，如越狱、拒绝机制和思维链忠实度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;稀疏自编码器使用一对编码器和解码器函数来分解和重建所有 LLM 输入。另一方面，经过训练后，转码器能够稀疏重建多层感知器（MLP）子层的计算过程，即学习如何对给定输入进行输出近似。这使其能够识别各层及子层中哪些部分（更精确地说是哪些激活模式）是由单输入令牌或令牌序列触发的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了应用于安全领域外，&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1eh4wja/comment/lfykb9o/&quot;&gt;Reddit 用户 Mescalian 预测&lt;/a&gt;&quot;，这项研究还可以：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;指导其他领域的最佳实践，未来可能会被用来监控智能程度更高的 AI 的内部推理。不过目前，它最适用于通过对权重进行微调及其他修改来调整模型能力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与谷歌类似，&lt;a href=&quot;https://www.infoq.com/news/2025/04/anthropic-ai-microscope/&quot;&gt;Anthropic&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://www.infoq.com/news/2020/04/open-ai-microscope/&quot;&gt;OpenAI&lt;/a&gt;&quot; 也针对他们的模型发布了自己的“ AI 显微镜”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌已在 Hugging Face 上&lt;a href=&quot;https://huggingface.co/google/gemma-scope-2&quot;&gt;发布&lt;/a&gt;&quot;了 Gemma Scope 2 的权重。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-gemma-scope-2/&quot;&gt;https://www.infoq.com/news/2026/01/google-gemma-scope-2/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QAVo2JuWaAElyqUA6joF</link><guid isPermaLink="false">https://www.infoq.cn/article/QAVo2JuWaAElyqUA6joF</guid><pubDate>Fri, 16 Jan 2026 02:36:30 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>FACTS基准测试套件问世，用于评估大型语言模型的事实准确性</title><description>&lt;p&gt;&lt;a href=&quot;https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/?utm_source=ALL&amp;amp;utm_medium=social&amp;amp;utm_campaign=&amp;amp;utm_content=&quot;&gt;FACTS基准测试套件&lt;/a&gt;&quot;发布，这是一个旨在系统性评估大型语言模型事实准确性的全新行业基准。该套件由FACTS团队与Kaggle联合开发，扩展了早期事实基础研究相关的工作，并引入了一个更广泛的多维度框架，用于衡量语言模型在不同使用场景下产生事实正确响应的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FACTS基准测试套件基于原先的FACTS Grounding Benchmark，并增加了三个新基准：参数化（Parametric）、搜索（Search）和多模态（Multimodal）。结合更新后的Grounding Benchmark v2，该套件可以从反映现实世界常见模型使用场景的四个维度评估事实性。该基准测试总共包括3513个精选示例，分为公共和私有评估集两部分。Kaggle负责管理保留的私有数据集，评估参赛模型，并通过公开排行榜发布结果。总体性能以FACTS评分的形式呈现。该分值是通过所有基准测试以及两部分数据集的平均准确率计算得出的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参数化基准测试侧重于模型仅凭内部知识（无需外部工具）回答基于事实的问题的能力。问题形式类似于常见的知识问答题，通常可通过维基百科等来源找到答案。搜索基准测试评估模型能否通过标准的Web搜索工具准确地检索并整合信息，通常需要多步检索才能完成单个查询。多模态基准测试在回答图像相关的问题时检验事实准确性，需要结合背景知识进行正确的视觉解读。更新后的Grounding Benchmark v2评估响应是否基于提供的上下文信息进行了合理推演。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初步结果既凸显了进展，也揭示了接下来要面对的挑战。在评估的模型中，Gemini 3 Pro以68.8%的总体FACTS评分位居首位，其参数化事实性与搜索事实性较前代模型均有显著提升。然而，评估的所有模型总体准确率均未突破70%，多模态事实性成为各模型普遍面临的难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e7c0336b403b9988fa4cff728f7bc8f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：谷歌DeepMind博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基准测试的结构引起了从业者的关注。资深iOS工程师Alexey Marinin在评论此次发布时&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7404597456069369856?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7404597456069369856%2C7404784965244301312%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287404784965244301312%2Curn%3Ali%3Aactivity%3A7404597456069369856%29&quot;&gt;指出&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这种四维视角（知识、Web、基础、多模态）感觉更接近人们日常实际使用这些模型的方式。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FACTS团队表示，该基准旨在支持正在进行的研究，而不是作为模型质量的最终衡量标准。通过公开数据集并规范评估标准，该项目旨在为衡量语言模型的事实可靠性提供一个共同的基准，以适应其持续演进的发展需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/facts-benchmark-suite/&quot;&gt;https://www.infoq.com/news/2026/01/facts-benchmark-suite/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zQZlFUuAIBS7HfoIK2CD</link><guid isPermaLink="false">https://www.infoq.cn/article/zQZlFUuAIBS7HfoIK2CD</guid><pubDate>Fri, 16 Jan 2026 01:53:49 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>全靠Claude Code 10天赶工上线，Cowork 删用户11G文件不含糊！核心研发：长时间打磨再发布很难成功</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 发布 Claude Cowork 研究预览版没多久，就被曝出了删用户文件、窃取文件等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，博主James McAulay在测试Cowork 功能中，选择“整理文件夹”这一基础且高频的场景，同时还与Claude Code进行对比。当James正在对比两款工具的整理进度时，Claude Cowork 突然触发了致命错误：在整理过程中擅自删除了约11GB文件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更令人崩溃的是，这些文件并未进入回收站，而是被执行了“rm -rf”不可逆删除命令。James紧急让 Claude Cowork 导出操作日志，确认该命令的执行记录后，咨询 Claude Code 能否恢复，得到的却是“无法恢复，属于致命操作”的回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事后复盘发现，James在 Claude Cowork 询问文件操作权限时，点击了“全部允许”或“始终允许”，但没有预料到它会无视明确的“保留文件”指令，更没想到会执行不可逆删除操作。万幸的是，此次被删除的均为过往上传记录，并非核心重要文件，未造成严重损失，但这一安全隐患足以让用户对其望而却步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e1b1ebaee3574961ce9b86f7d2044b21.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;James还指出，Cowork 与Claude Code相比，存在两点不足：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是交互的繁琐性。发出“整理文件夹”的指令后，Claude Cowork 并未直接行动，而是要求先启动新任务并手动选择目标文件夹；Claude Code则直接定位文件夹并开始分析，仅需授予一次权限即可推进。Claude Cowork 通过反复交互确认整理细节，比如询问“文件按什么维度分类”“用户数据文件夹如何处理”，即便明确回复“用户数据文件夹暂不删除、保留”，它仍在待办清单中标记“删除用户数据文件夹：已完成”，虽后续未实际执行该删除操作，但也暴露了指令响应的漏洞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是效率的滞后性。整理过程中，Claude Cowork 运行命令多次停顿，节奏拖沓；而同期用 Claude Code 整理“音乐文件夹”，智能体快速给出“专辑和迷你专辑、单曲、Demo、翻唱”的分类建议，确认后即刻推进整理，全程仅需数十秒。即便两者均搭载 Opus 4.5 模型，Claude Cowork 的响应速度和执行效率仍明显落后，甚至让简单的文件夹整理变成了“持久战”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，AI 安全公司PromptArmor还发现，由于 Claude 代码执行环境中存在已知但未解决的隔离缺陷，Claude Cowork 易受通过间接提示注入实施的文件窃取攻击。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，这是一个最早由 Johann Rehberger 在 Cowork 尚未出现之前、于 Claude.ai 聊天环境中发现的漏洞，已经扩展到 Cowork中。Anthropic 对该漏洞进行了确认，但并未进行修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 提醒用户：“Cowork 是一个研究预览版，由于其agentic的特性以及可访问互联网，存在独特风险。”官方建议用户警惕“可能表明存在提示注入的可疑行为”。然而，由于该功能面向的是普通大众而非仅限技术用户，PromptArmor表示认同 Simon Willison 的观点：“要求普通、非程序员用户去警惕‘可能表明提示注入的可疑行为’，这是不公平的！”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d221638382f4de83263c0417be5d498b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此前，Every 团队提前获得权限，Dan Shipper、Kieran Klaassen 直播测试了该产品并分享了使用体验。期间，Anthropic Claude Cowork 项目核心成员 Felix Rieseberg 参与解读了产品设计思路。Felix 介绍，Cowork 是一个快速上线、先交给大家看怎么应用的产品，只用了 1.5 周就完成了开发，Felix 表示未来将以用户反馈为核心快速迭代。此外，工程师 Boris Cherny 还在X 上透露，该产品的全部代码都是由 Claude Code 编写的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在直播中，Felix表示，产品工作流可拆分为 “非确定性（依赖模型智能）” 和 “稳定可重复（编写工具）” 两类，按需取舍。Skills 是平衡 “模型灵活性” 与 “工作流稳定性” 的关键，能沉淀可复用知识，还能催生涌现能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，未来 Agent 类应用界面会趋简，用统一的 “泛化入口” 覆盖更多场景，而非专用化输入框堆砌。下面是三人对话部分内容，我们进行了翻译，并且在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一周半冲刺、先上线再说&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这是我们团队做的产品。我们在最近大概一周半的时间里全力冲刺，把它做出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：一周半？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：对，不过我想澄清一下：其实很多人早就有一个共识：如果能有一个“给非程序员用的 Claude Code”，那一定会非常有帮助、也很有价值。我们真正想做的，是帮助人把事情做完，不管是生活里还是公司工作中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这之前，我们其实已经做过好几个原型，尤其是在圣诞节前。但假期期间我们观察到一件事，我相信很多人也注意到了：越来越多的人开始用 Claude Code 做几乎所有事情，某种程度上，大家是在用它“自动化自己的人生”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们就在想：有没有一个足够小、足够早期的形态，可以先做出来给大家用，然后和用户一起快速迭代，真正搞清楚什么样的用户体验才是对的、我们到底应该构建什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在你们看到的这个就是答案。它是一个 research preview，非常早期的 alpha 版本，有很多不完善的地方、很多毛糙的边角，你们已经看到不少了，这些我们都会很快改进。但这就是我们的尝试：在开放状态下构建产品，和外部的人一起打磨。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我太喜欢这种方式了，能不能讲讲你们做的一些设计决策？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这是个很好的问题。我个人有一个判断：不只是 Anthropic，而是整个 Agent 类应用的用户界面，在接下来一两年里都会发生非常大的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在我们看到的，是为不同任务设计的高度专用化输入框，以及围绕特定任务搭出来的一整套脚手架。但随着模型能力不断提升、整个行业对“泛化问题”的理解逐渐加深，我认为未来我们会用更少的界面，覆盖更广的使用场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在当下，我们之所以把 Cowork 单独拆出来，是因为我们想非常透明地告诉用户：这是一个“施工中的区域”。某种意义上，我们是在邀请你走进我们的厨房。我们希望能和用户一起工作，几乎每天都上线新功能、修 bug、尝试新想法。所以这个独立的 Tab 本身就是实验性的，可以说是在前沿、甚至是“流血边缘”。它节奏更快、打磨得没那么精致，这也是我们把它单独拎出来的主要原因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，也有一些技术层面的原因。比如现在这个 Cowork 是运行在你本地电脑上的，所以里面的对话是本地的，不会在多设备之间同步。同时，我们给了 Claude 更激进的一些 Agent 能力。综合这些因素，才决定做成现在这个形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：同一个应用里，一边是云端的聊天，一边却是在自己电脑上跑的 Agent。怎么让用户真正理解“这两者不一样”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：是的，我心里有一个梦想，我相信很多人也有同样的想法：最终这些其实都不重要，代码到底跑在什么地方，应该只是一个技术实现细节。对用户来说，它应该就跟你访问纽约时报网站时会不会用 WebSocket 一样，谁会在乎呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我们来说，现阶段这样做的好处是，可以跑得更快、发布得更快，也能和真正使用这个产品的人更近距离地一起共创。我一直很坚定地认为，一个人关起门来是很难做出好产品的。那种“躲进山洞里干一年，最后拿出来”的方式，其实很难成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也经常提醒大家：就连第一代 iPhone，都缺了很多我们现在觉得是“理所当然”的功能。所以，这确实是一个不小的门槛，但我们暂时可以接受，因为我们希望现在选择用这个产品的人，本身就是带着明确意图来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我觉得这是一个非常有意思的模式，先极快地把东西做出来，以一个“新入口”的形式放在应用里，让相对更少的人点进来。这样就能在真实世界里快速迭代，而不是一开始就追求完美。尤其是在你刚才说一周半就能做出一个版本，简直疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“现在的状态是，先看看大家怎么用”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：但在你们脑海里，这个产品“真正的形态”是什么样的？你们接下来想往哪里走？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我太喜欢这个问题了，因为说实话，我也想反过来问你们两个同样的问题：你们希望它变成什么？你们想用它做什么？我已经听你们提到过，比如想让它能访问整台电脑，还有多选交互是不是可以更灵活一些之类的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我现在更多的状态是，先看看大家怎么用，然后疯狂尝试各种可能性。里面肯定有很多是错的，也会有一些是对的。对我来说，真正有意思的不是我个人的愿景，而是用户真正想拿它干什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我过去做过的产品几乎都是这样：你心里以为用户会这么用，结果他们找到了完全不同的用法，然后你顺着那个方向继续做下去。所以我特别希望我们能搞清楚：人们现在到底想要什么、喜欢什么、不喜欢什么。肯定也会有人明确说不喜欢某些地方，那我们就根据这些反馈不断调整、迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：这又回到一个老问题了。比如 Boris 就非常擅长把 Claude Code 做成一种让用户在使用过程中逐渐发现“自己到底想要什么”的工具。那你们在 Cowork 里有没有类似的策略？比如给我们一些“积木式”的东西？能不能加自己的插件或 Skills？Claude Code 很酷的一个地方在于它特别好 hack、特别可塑，你们面向非程序员的 Cowork 是不是也有类似理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：对，非常强调可组合性。你刚才提到 Boris 推动 Claude Code 早发布、快迭代、看用户怎么用，其实特别巧，我们之所以能这么快上线，很大程度上也是 Boris 在推动我说，“你应该早点给大家看看，看他们会怎么用”。（注：Boris Cherny&amp;nbsp;是Claude Code核心创作者）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于可组合这一点，过去几周、甚至最近两个月里，我自己感受最深的，是我越来越依赖 Skills。以前我可能会去写 MCP 工具，或者为 Claude 专门做一套很定制化的东西，现在我更多是直接写 Skills。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时候我还是会写一个二进制程序，但我随后就会在一个 Skill 文件里用 Markdown 描述：Claude，如果你要做这件事，请遵循这些规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我最近在给自己做一个马拉松训练计划。我写了一个小程序，从不同平台抓取我的运动数据；然后在一个 Skill 里写清楚：如果你要帮我做训练计划，请按这些原则来。现在，只要你在 Claude AI 里装过的 Skill，都会自动加载到 Cowork 里。而且我觉得这只会越来越重要，尤其是模型越来越聪明，比如Opus 4.5 版本，对 Skills 的遵循能力真的非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以目前来说，Skills 大概是我们最主要、也最“可 hack”的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;统一的“泛化入口”趋势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：太棒了。你刚才提到未来会有更少的 UI 形态。这是不是也意味着，围绕“聊天是不是 AI 的最终形态”这个争论，你其实是在押注自然语言会长期存在？也就是说，我们最终不会有越来越多复杂的 UI，而是更少的界面，人只需要和一个 Agent，或者一个能调度其他 Agent 的 Agent 对话？你们现在推动的方向，某种程度上是不是就类似今天 Claude Code 所展现出来的那种形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：是的，这个问题现在仍然存在很大的争论空间，而且肯定不存在什么“Anthropic 官方立场”。老实说，就算是在我这个并不算大的团队里，大家也未必能在整体上达成一致。每个人对于未来人类将如何与 AI、与模型交互，都有非常不同的想象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果只从我个人的角度来说，我大概坚信两件事。第一是：聊天式输入及其各种变体——不仅仅是模型意义上的聊天，而是更广义的那种“我想要点什么”的输入框——会比我们想象中存在得更久。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把它抽象开来看，不管是 Google 首页，还是 Chrome 的地址栏，本质上都是一个“我想要某样东西”的输入框，我认为这种形态会长期存在，我们会继续拥有某种看起来很像搜索框的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题是，我们到底需要多少个这样的输入框？你会有一个专门写代码的框吗？一个用于个人娱乐的、一个处理医疗相关问题的？我并不确定未来会存在这么多彼此割裂的输入框。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我再拿 Google 做类比。过去你可能记得，Google 会为不同需求提供不同的搜索入口和子产品。但现在，越来越多时候，你只是直接在 Chrome 的地址栏里输入你想要的东西。你不会真的先想清楚“我现在是在购物模式”，然后再专门去打开 Google Shopping。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，如果我们未来看不到一种更聪明的、能理解你想做什么的“泛化入口”，我会很意外。当然，后端可能仍然会分流，比如它理解你想要做的是 X，于是给你呈现一个适合 X 的界面，但入口本身很可能是统一的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;产品设计中的取舍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我觉得一个很有意思的反例是 Microsoft Excel。某种程度上，它和 AI 的工作方式其实也很像：这是一个通用型产品，上手极其简单，但你可以在里面把事情做到无限复杂。而且，Excel 甚至某种程度上催生了后来的 B2B SaaS 浪潮，很多 SaaS 本质上就是把Excel 里的复杂工作流“产品化”了。所以也有另一种可能：你先有一个极其通用的工具，然后人们在里面发现了高价值、高强度的工作流，最后这些工作流再被拆分成独立产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我觉得 Excel 真的是一个极其漂亮的例子。对很多开发者来说，Excel 其实处在一个有点“边缘化”的位置，但如果你比较一下 Excel 的日活用户数量和全球开发者的数量，那是一个非常惊人的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 Excel 身上看到的一个很有意思的点是：它的重度用户，其实并不太在意那种“边际效率提升”，或者 UI 上一点点的小优化。他们更在意的是对这个产品的深度熟悉和肌肉记忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里面是有教训的。我在很多产品表面上都见过这种情况：作为开发者，你会觉得“如果我单独给你做一个更贴合这个场景的小工具，你的工作流会更好”。但结果往往是，用户并不会去用那个新工具，而是继续在他们已经非常熟悉的产品里，把事情做完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，这是我在 Slack 工作多年反复学到的一课：你可以做很多你自认为更适合某个使用场景的独立服务，但用户最后往往还是选择就在聊天里完成这件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：说到这里，虽然今天的主题更偏向非开发者，但我感觉现在有不少开发者在看。你正好是那种“真的把这个东西做出来了”的人，对 Agent native 应用的构建理解非常深。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们一直在思考 Agent-native 应用的核心原则。比如其中一个原则是“对等性（parity）”：用户通过 UI 能做的事情，agent 也应该能做。我在 Cowork 里已经能看到这一点。另一个是“粒度（granularity）”：工具应该尽量处在比功能更底层的层级，而“功能”更多存在于 prompt 或 Skill 中，这样你就能以开发者没预料到的方式去组合工具。这会自然带来第三个原则“可组合性（composability）”，而可组合性最终会产生第四个：涌现能力（emergent capability）。也就是用户开始用它做你完全没想到的事情，你看到了潜在需求，然后再围绕它构建产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在我看来，几乎就是 Claude Code 的工作方式。我很好奇，这一套在你听来是否成立？或者从你们在 Anthropic 大规模落地的经验来看，有没有什么能让大家把 Agent native 应用做得更好的建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这套说法对我来说非常有共鸣。而且我觉得，“涌现能力”里隐藏着一个非常重要的事实：无论是个人还是在孤立的小团队里，我们几乎不可能提前预测一个 Agent 最终会在哪些地方变得极其有用，尤其是当你只给了它一些相对原始的工具时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把工具尽可能下沉、做成通用形态，是一件非常强大的事情。工具越可组合、越通用，你就越能从模型智能的持续提升中获益。我和很多开发者聊过一个感受：模型智能提升、以及模型“正确调用工具”的能力，增长速度往往远快于你新增工具、或者教育用户理解这些工具的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以如果你退一步思考：“我能不能先做一个高度通用的工具？”那你构建出一个可以适应未来新场景的产品的概率，其实会大得多。这一点，我非常认同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那在这些原则之下，你怎么看其中的取舍？比如工具设计本身的权衡问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：对，我觉得把东西放进 prompt 里、再配合工具，本身是很棒的。但问题在于，我们现在突然需要去创建一些“能读取 Skills 的工具”，或者类似的东西。于是就出现了一个新的“元层”。Skills 本质上就像是一种即时的 prompt 注入，但你得先把这个体系搭出来。现在所有在做这些东西的人，如果不是直接用 Claude Code 或 Cloud SDK，那基本都得自己从头构建一整套。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是就出现了一种拉扯：你到底是把行为直接描述在一个 tool 里？还是再包一层 tool，让它去调用别的东西？这中间是有摩擦成本的。当然，可组合性是很好的。比如一开始你可能会有五个 tool：搜索邮件、读取邮件、做这个、做那个。但你也可以说：不，我只提供一个 execute tool，然后用 Skills、MCP，或者某种抽象层来完成这些事情。现在正处在这样一个转变期，而 Claude Code 和 Claude SDK 显然是在推动这个方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我确实能感受到这种摩擦。我猜你也一定感受到了。所以我很好奇：你有没有什么最佳实践，能给那些还停留在“传统 AI 应用思维”的人一些建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我不确定我能给出什么“来自山顶的智慧”，会比你已经拥有的经验更有价值。但你说的那点，确实非常戳中我。我觉得你必须做一个取舍：哪些输出你愿意让它是非确定性的、哪些地方你愿意依赖模型的智能。而且一旦你依赖模型智能，每当你换一个更便宜、或者“更笨”的模型，那些地方的质量就会下降。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我会把整个工作流拆成两类：一类是非确定性的；一类是可重复、稳定的。如果某个部分非常可重复，而且你可以非常确信它“永远不会变”，而且就算模型变聪明了，你也得不到任何额外收益，那我会觉得，这正是写一个工具的好地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实我们已经在这么做了。你完全可以给 Claude 一个极其通用的“汇编级”工具，比如：“直接调用 GCC，你想怎么编就怎么编。”但我们并没有这么做，因为那样就太疯狂了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Skills 与可组合性实践&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那已经是粒度的极限了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：不过我也想说一句：当我和很多开发者聊的时候，我发现即便这个“是否要给模型工具”的基本假设，也正在被挑战。我不会把太多赌注压在这个假设上。比如，我们到底是不是还需要给 Claude 工具？还是说，某一天它只需要靠记忆和权重，直接把 0 和 1 写到世界里？这是一个非常有意思、也非常难判断的问题，没人真的知道答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你们已经在实践中学到了一些东西。你们之所以创造了 Skills，就是因为仅靠 Slash command 或子 Agent 已经不够了，对吧？我们需要 Claude.md 更强，但现实是 Skills 正是为了解决这个问题而诞生的，而且显然它们效果很好。我完全认同你说的，Skills 太棒了。我现在几乎每天都在写 Skills，而且真的很爱用。所以这里面一定有些什么。但问题是：什么时候应该用 Skill？什么时候又不该？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这真的是一场特别有意思的对话。有一个你以后真的应该跟 Barry 聊聊。在公司内部，至少在某种程度上，Skills 这个概念就是他提出来的。从根本上说，Skills 正是你刚才描述的那种张力的自然产物。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我们想让公司内部的人能很容易地拿到各种仪表盘。我们用的是一家主流数据服务商，很多数据都在那儿。一开始我们在想：要不要做一堆非常具体的工具，专门去拉数据、压缩成固定格式。最早那几版仪表盘，其实效果并不理想（那还是 4.5 之前）。大概每三四个里面，就有一个看起来很拉胯。于是，我们开始想：要不要把参数卡死，直接做一个“固定模板”的仪表盘？Claude 只负责往里面填新数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这个过程中，我们突然发现了一件事：如果你只是告诉 Claude 如何正确地查询这个数据源、可以使用 SQL、以及生成仪表盘时需要遵循哪些设计原则，突然间，它就能稳定地产出质量很高的结果，而且是“几乎每一次”都很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，这就打开了“涌现能力”的大门。因为你还可以对 Claude 说：“我知道你在遵循这些仪表盘原则，但我想换一种图表类型”，或者“我想把它和另一份数据结合起来。”就在这一刻，事情真正开始变得有趣了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：这真的很有意思。我觉得为什么要用 Skill，而不是只给它 GCC、让一切都即兴发生，其中一个关键原因在于：你需要把一些可重复的、可分享的知识，变成一个大家都能讨论、都能复用的东西。并不是所有事情都应该是“即时生成”的。有些事情，你就是希望一个团队能长期、反复地用同一种方式来做。而这，本质上就是 Skill。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：而且这其实也很符合人类本身的工作方式，对吧？比如我刚加入一家公司时，总有人教我怎么订机票、怎么订会议室。从某种意义上说，我们每个人，都是靠着一堆 markdown 文件在工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得差不多该下线了，但在走之前，我想让你们两个各自给我一个建议：你们最希望我们改的一件事是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那我先来一个最简单的：给我对整台电脑的完全访问权限。还有就是，让我更清楚地知道它现在到底是在我本地电脑上运行，还是在云端以聊天的形式运行；以及，让它在手机上用起来更顺畅。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：我也支持移动端。但我最想要的是能让我添加自己的插件。我有一个插件市场，我只想把它接进来直接用。现在我得在一个应用里加东西，再拷贝到这里，有点绕。可能也能凑合用，但如果能原生支持插件市场、直接添加插件，那真的会非常棒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：好，明白了。谢谢你们，这些反馈都非常有价值。我们会把这些带回去，跟团队一起讨论。也欢迎大家把想法发给我们。我们真的很希望听到大家的反馈，并据此调整路线图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;测试总结：理念可以，做得一般&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，我们总结了Every团队的测评结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Cowork 的核心定位是为非技术用户提供 Claude Code 级别的 AI 协作能力，其最显著的突破在于重构了 AI 使用逻辑，从传统“发提示词→等回复”的一问一答模式，升级为“异步协作”模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与普通 Claude 聊天相比，Claude Cowork 专为“长时间工作”设计，具备持续推进任务直至完成的能力。直播中展示的典型案例包括：审计过去一个月的日历并分析与目标的匹配度、抓取 PostHog 数据统计按钮点击量、分析 Every 咨询业务的竞品、整理下载文件夹、校对 Google Docs 文案等。这些任务均需 AI 持续“浏览”、推理，部分任务耗时可达一小时左右，远超普通 AI 聊天的响应速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品的场景适配性极强，尤其适合需要深度研究和数据处理的岗位。用户只需连接 Chrome 浏览器，AI 即可直接使用用户已登录的各类服务，无需重复认证，轻松完成 Twitter 时间线热点分析、竞品信息搜集等需多平台联动的任务。同时，它支持生成文档、Excel、PPT、PDF 等多种产出物，可应用于简历优化、会议发言起草等日常工作场景，大幅提升增长团队、咨询人员、写作者等群体的工作效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在交互设计上，产品右侧设置了待办任务列表，清晰展示任务进度与当前阶段，用户可直观掌握 AI 工作状态。其“询问用户”功能还配备了可视化交互界面，支持多选项快速响应，进一步降低了操作门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据测评，Cowork具备较强的可扩展性，支持加载用户已安装的 Claude Skills，这也是其最具“可玩度”和“可定制性”的核心入口。用户可通过 Skills 封装专业知识与操作逻辑，实现个性化需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;测评团队也指出了产品当前存在的争议与不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最核心的争议在于“单独设置 Cowork 标签页”的设计：部分用户认为应在同一标签页内根据任务自动切换模式，避免额外的选择成本；但也有观点认为，独立标签页能明确提醒用户切换使用心态：从“实时对话”转向“异步托付”，尤其对非技术用户而言，这种明确的区分有助于适应全新的协作范式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外在体验细节上，产品仍有诸多优化空间：一是 UI 打磨不足，任务列表仅按时间排序，缺乏视觉区分度，部分内容存在“懒加载”导致展示不及时；二是权限管理不够直观，普通用户难以清晰判断 AI 是在本地还是云端运行，文件夹访问权限需手动配置易造成困惑；三是“询问用户”功能存在逻辑缺陷，可能在用户未响应时自动跳过问题，且选项数量和字符数存在限制；四是对复杂应用（如 Google Docs）的适配尚不完善，相关操作容易失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对不同用户，测评团队给出了针对性使用建议：非技术用户可将其视为“升级版聊天功能”，用日常任务直接尝试，逐步适应异步协作模式；重度用户可尝试通过 Skills 定制个性化功能，探索组合使用的可能性。他们表示，所有用户均需保持好奇心，忽略“三个月前 AI 做不到”的固有认知，在每一次产品更新后重新尝试核心需求，毕竟 AI 能力每隔几个月就会发生巨大迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终，测评团队给出的评分结论为：“理念绿牌，当前执行黄牌”。理念层面，产品开创性地将 Claude Code 级别的异步协作能力开放给非技术用户，推动了 AI 协作范式的转变，具备极高的探索价值；执行层面，因 UI 粗糙、部分功能逻辑不完善等问题，当前体验仍有较大优化空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_6C9nMvQsGU&quot;&gt;https://www.youtube.com/watch?v=_6C9nMvQsGU&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oPBN-QIfLaY&quot;&gt;https://www.youtube.com/watch?v=oPBN-QIfLaY&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files&quot;&gt;https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WDcTxr8g5TT064NaQyp3</link><guid isPermaLink="false">https://www.infoq.cn/article/WDcTxr8g5TT064NaQyp3</guid><pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖</title><description>&lt;p&gt;2026 年，AI 真正“下地干活”的第一战，被阿里打响了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 15 日，在杭州阿里园区举行的千问 App 发布会上，阿里巴巴集团总裁吴嘉做了一次并不复杂、却很直观的演示：他用千问给现场嘉宾点了 40 杯“伯牙绝弦”奶茶。整个过程没有人工介入。千问自行匹配附近奶茶店，下单，并调用支付宝完成支付。没一会儿，淘宝闪购的骑手把奶茶送进会场。发布会的气氛，也在这一刻被彻底点燃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事后，有杭州的网友恍然大悟“怪不得刚刚西溪附近叫不动外卖！”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba72fd559beaf5ece58490ff5c1ae8b3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比 PPT 上的参数和模型指标，这个场景更容易被理解：AI 第一次在公开场合，完整地替人把一件现实中的事情办成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次更新中，阿里将千问定位成&amp;nbsp;“每个人的生活助手”。路径也很明确：不从新场景做起，而是直接接入阿里现有的业务体系，让 AI 先把眼前的事干好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;日常生活&amp;nbsp;层面，千问首批接入了&amp;nbsp;淘宝闪购、支付宝、淘宝、飞猪和高德&amp;nbsp;五大业务，可以一句话&amp;nbsp;点外卖、买东西、订机票、订酒店、查路线，这些原本需要在多个 App 之间来回切换的操作，现在可以交给一句话来完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6c659e25b3b2f5fd57a8386e902d2613.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;“办事”&amp;nbsp;这一层，千问的能力被进一步拉长。它开始尝试处理更复杂的任务，比如打电话订餐厅、整理调研资料、处理财务文件、辅助搭建网站等。这类功能目前仍处于定向邀测阶段，&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴嘉在发布会上表示：“AI 在拥有超强大脑之后，正在长出能够触达真实世界的手和脚，在生活中实实在在地替用户‘干活’。&amp;nbsp;千问的优势在于‘最强的 Qwen 模型’与‘阿里最完整的商业生态’的结合。AI 办事的时代才刚刚开始，我们会持续探索，把千问打造成真正有用的个人 AI 助手。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自千问上线两个月以来，月度活跃用户已突破 1 亿。&amp;nbsp;吴嘉认为，随着 AI coding、全模态理解以及超长上下文等关键能力逐步成熟，AI 正在走出手机屏幕，进入更复杂、也更真实的生产与生活场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把阿里折叠进千问中，&amp;nbsp;通过统一的 AI 入口，让千问拥有&amp;nbsp;400&amp;nbsp;余项办事能力，在&amp;nbsp;生活、办公、教育&amp;nbsp;等方面全场景覆盖，让千问成为 AI 时代的超级应用入口，这正是阿里的野心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;办事之上如何理解需求，才能判断是不是一个合格的助手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随着模型能力的跃迁，思考让 Agent 做事，已经是近几年行业的集体共识。但&amp;nbsp;干的活好不好，这才是能否放心 AI 当助手的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里此次的更新方向，既在意料之中，又有些意料之外的惊喜，这个惊喜的落脚点就在于&amp;nbsp;对需求的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对千问用户数据观察中，用户主动询问商品推荐的月环比高达 300%，这引起了阿里的注意，利用好千问与淘宝的链接，让千问拥有更可用的商品推荐能力，这确实踩中了不少人的真实需求，也成为千问区别其他通用 Agent 的功能独特切入点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d77d96dc23f662ce274acc00f1023a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅发挥了阿里在电商上的传统优势，也让庞大的商品供给和相对成熟的推荐体系真正被用起来。用户只需一句话，就能完成从商品推荐到下单的完整流程。其背后，是&amp;nbsp;阿里各业务接口的打通和协同调用，用起来足够顺，也足够省事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但更令人惊喜的是&amp;nbsp;对决策层面的关注，这也是&amp;nbsp;模型深入理解真实需求的表现，如何调用工具做更好的决策，体现了阿里强大的整合能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，现场展示了要给老人购买一款家庭扫地机，并且家里还养了一只猫，预算在 2000-4000 左右。千问在综合产品的价格与能力之上，还进一步老人的便捷需求与对猫毛的清洁效果，在综合这些复杂的条件后，给出推荐产品与相关理由，这正是大模型方便人类决策的一个虚拟需求感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a1/a141ee5ec5f8bdcb211d4a17eb9197e7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一个徒步推荐的方案中，千问不仅推荐出行路线，结合天气情况给出建议，还将徒步需要的产品直接发送到了千问界面上，确实让人看到 AI 未来融入世界的真实摸样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94c795db2602bc1789406c2e8d10bbac.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不是只做简单的一件事，而是将好多事做好，形成闭环，阿里已经迈出第一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者能想到的弊端，可能就是如何避免大模型被商家刷的假好评和广告垃圾数据污染，根据错误数据给出错误推荐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在一个全家人考虑去三亚出行的案例中，千问综合了路线、预算、老人与孩子的需求等，给出了路线选择，并给出三套酒店方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fe5cc6ca26ffb34e7e474aba35271405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，酒店的均价都在两三千左右，不少人吐槽这恐怕没人住得起，方案不适用，不接地气，这或许是笔者认为的阿里迈出的是“半步”，还需要进一步的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现场还有一个小惊喜是，千问演示现场定饭店的时候，有一段与老板确定需求的打电话环节，从包间大小，价格，有小朋友等需求进行多方拉扯沟通，直到最后，电话结尾说，“我是千问 AI 助手在与你沟通”，大家才恍然大悟，原来是千问的语音功能在完成订酒店的“最后一公里”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是各种多模态打通后，AI 能做到的程度，留给人更多想象空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种好用，同时体现在在对办公需求上，在更专业的场景上，需要更好的交付结果，要求也更难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;千问可以集成各种复杂工具，完成做表格、整理数据、处理报表、汇报 PPT 等各种具体业务。从如何处理资料到最后成品展现，从效果来看，确实还不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07fa85f8f5b6844bf016d30842fb6e9a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次，阿里找来了专业人士来验收干活效果，千万财经博主小 Lin 说，亲自下场演示了用千问生成一份《2026 毕业生就业报告》，从信息汇总，消化资料，角度分析，文章演示到 PPT 的生成，千问干了一个完整的活。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，如果把千问当做个工作三年内的大学生，来干这些活，效果还是不错的，如果要求更高，可能就是把控 PPT 的内容重点质量，PPT 的设计是否美观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/40fb2d9103811e218d0cafb2f183e743.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在教育领域，千问也做出一些精心设计，令人印象深刻的是在各种题目中，除了思路的讲解，还会生成一段动态视频进行图示演说，能随时对话沟通，给出思路和解法，并且多模态展示，这让千问更像一个人一样解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/feaccba85e81a081dd48331badff811f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者也亲自进行了一个上手测评，一个是用千问点奶茶，还有一个是用千问询问如何落户问题，千问都给出了较为实用的操作结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b1/b1c582f476f9719185aa2a395af5352d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总体来看，千问并没有试图一下子把所有事都做好，而是在尝试把复杂的事做得更完整、更贴近人的真实需求。它距离“完全可靠的 AI 助手”还有距离，但已经明显走出了聊天框，开始进入决策和执行的真实环节。而对干活质量的进一步打磨，恐怕正是阿里下一步要发力的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在几家最受关注的 AI 巨头中，字节跳动&amp;nbsp;选择从系统层切入，通过豆包手机助手借助操作系统能力，去调度第三方应用，与现实世界建立连接；阿里&amp;nbsp;的路线则更为直接，依托自身已高度成熟的电商、支付、物流、出行等业务体系，将这些能力整体接入千问，形成一个以自有生态为核心的闭环。腾讯&amp;nbsp;目前尚未对外展示完整方案，但从近期在 Agent 和多模态方向上的密集招聘来看，其下一步布局大概率仍将围绕微信这一超级入口展开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c08da37ff9047adf91ec2784c9765e8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;表面上看，Agent 之争比拼的是模型能力，但更深层的竞争，实际上取决于谁能更稳定、更规模化地承接真实世界的复杂需求。&lt;/p&gt;</description><link>https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</link><guid isPermaLink="false">https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</guid><pubDate>Thu, 15 Jan 2026 11:13:35 GMT</pubDate><author>高允毅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>“商业版 HTTP”来了：谷歌 CEO 劈柴官宣 UCP，Agent 直接下单，倒逼淘宝京东“拆家式重构”？</title><description>&lt;p&gt;谷歌把“Agent 购物”这件事，推到了一个更标准化的层面：Universal Commerce Protocol（UCP）正式亮相。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日（1月11日），谷歌 CEO Sundar Pichai（绰号“劈柴”） 首次登上 NRF（美国零售联合会年会），在题为“人工智能平台转型及零售业的未来机遇”的主题演讲中宣布了该协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照谷歌的说法，UCP 是一项新的开放标准，目标是让 Agent 能够在线上直接买东西。在实现机制上，UCP 通过定义一组“代理商务的构建模块”，把端到端的购物流程拆解成可复用的能力组件：既覆盖推动商品发现与购买的关键动作，也延伸到下单后的体验与服务等环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，这套设计将让生态系统在同一套标准下实现互操作，使任何 Agent 都能与任意商家进行对话，并自主完成从商品发现到结账的完整购物流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该标准采用 Apache 2.0 开源许可证发布：&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/ucp&quot;&gt;https://github.com/Universal-Commerce-Protocol/ucp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8ac749381cca480abba528ff41699a7b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人一看到这条消息就意识到：大事可能真要来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;风险投资人 Linas Beliūnas 在LinkedIn 上评论称：“谷歌刚刚对‘商业’做了一件类似 HTTP 当年对 Web 所做的事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他看来，UCP 的野心，是把电商 20 年来那条固定链路，“搜索—广告—商品页—结账”——压缩成“意图—Agent 推理—购买”：用户不再需要点击跳转，不再被迫参与 SEO博弈，也不再被传统的转化漏斗一层层“导流”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;进一步说，Beliūnas 认为，UCP 试图成为商业领域的“HTTP”——也就是所有由 AI 介导的交易背后，那层看不见、但不可或缺的基础设施，“品牌不再争夺用户注意力，他们将竞相争取被Agent选中。网站变得可有可无。这就是非人类商业的开端。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2b1750b564c1fe87991e9db7109580b5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期关注零售的连续创业者 Scott Wingo 甚至把谷歌这次在 NRF 上的一系列动作形容为一次“震撼与威慑（shock and awe）式”的进攻。他感叹自己在这个行业干了 30 年，“从来没见过现在这样的场面，真的太疯狂了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Wingo 看来，NRF 过去一直带着点“昏昏欲睡”的气质：讨论的多是收银系统、收银机、POS，以及超市自助结账的传送带这些传统议题。而如今，它几乎已经变成了一场围绕 Agent Commerce（智能体商业）展开的大会。“这种变化，是我做梦都想不到的。”他说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;统一零售界的新标准？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，UCP 到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单说，UCP 的目标是让 Agent 能够贯穿用户购买流程的各个环节：从商品发现、对比，到下单结账，再到购买后的支持服务，都可以在同一套标准下衔接起来。它想解决的核心问题是：用一个统一标准承载这些流程能力，而不是让商家和平台为不同 Agent、不同系统反复做一遍又一遍的对接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/affc2226740f2bd401efbcd843d05752.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从谷歌给出的设计图可以看到整体思路：左侧是各种消费者触点——消费者在这些地方与 Agentic Commerce 交互。在谷歌的世界里，这些包括 Google AI Mode、核心搜索、Gemini 等。右侧是后台系统——零售商后台需要的订单管理、库存管理等能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六项能力：产品发现、购物车、身份绑定、结账、订单，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六个圆角矩形，其中三个是实线框，三个是虚线框。实线框的，是已经宣布、可用的能力。尚未上线的三项是：产品发现、购物车，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;围绕这六项能力，Scott Wingo 也给出了更具体的解读：&lt;/p&gt;&lt;p&gt;产品发现（Product Discovery）：目前官方并没有披露太多细节，但他判断，这很可能会与后续对 Google Shopping Feed 规范的扩展绑定在一起。未来 UCP 可能会提供类似“开关”的机制：商家可以决定哪些商品对 Agent 开放，Agent 也可以通过协议以不同方式拉取商品信息——某种程度上，这有点像 Stripe 的 Agentic Commerce 套件思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;购物车（Cart）：这是他认为“最值得盯”的部分。谷歌在图里用虚线框把它标出来，像是在释放一个强信号：UCP 可能要去挑战电商的“圣杯”——跨商家、多商品、由商家作为交易主体（merchant-of-record）的统一购物车。一句话：“一个购物车管全网”。他认为 ChatGPT/ACP 可能也有类似目标，但谷歌这次等于把这个方向直接摆到台面上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;身份绑定（Identity Linking）：他推测这会涉及“识别你的 Agent”（某种 know your agent 的机制）、银行卡 token 化等能力，类似 Link 或 ShopPay 那套：如果系统能把你的身份与支付凭据映射成 token，就有机会实现自动填充信用卡信息等体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结账（Checkout）：谷歌准备把 “Buy for Me” 做一次大升级——新结账入口将同时出现在搜索 AI Mode 和 Gemini 应用的符合条件商品页中，流程被压成三步“商品 → 确认订单 → 下单完成”，并将率先在美国上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;订单（Order）：一旦开始“在对话里结账”，就必须有一套双向的订单体验。一边是面向消费者：查看订单、取消、退货等；另一边是面向商家：拉取订单、处理履约、上传物流信息，并完成一整套购买后流程（退货、评价等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他垂直能力（Other Vertical Capabilities）：这部分目前更像一个“兜底项”，官方也没有给出更多细节。他猜测它可能用于未来扩展到更多品类/行业，比如汽配、生鲜、B2B 等。当天新闻里被提到的客户之一是 Papa Johns（达美乐/披萨这种即时零售/本地履约场景），因此也不排除这块会成为一种“插件位”，让类似“ChatGPT App”式的体验从 UCP 的侧边接入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这些能力下方，还有三个模块，代表底层通信方式：API、MCP，以及 A2A。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌同时强调，UCP 并不是一套孤立协议，它可以与其他 Agent 协议协同使用，例如其在去年发布的 Agent Payments Protocol（AP2）、Agent2Agent（A2A） 以及 Model Context Protocol（MCP）。Agent 与商家可以根据自身需求，灵活选择和组合协议中的不同扩展模块。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中，MCP 更像是一个“工具与上下文协议”，用于让 Agent 安全、标准化地访问各类工具；A2A 是谷歌推出的多 Agent 通信协议，用来支持 Agent 之间的协作与任务分工； 而 AP2 是去年底发布的，聚焦在支付层，试图为 Agent 执行交易提供可验证、可授权的支付机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 UCP，看起来就是在这些协议之上的一次延伸，专门聚焦在零售这一层。可以说，谷歌这段时间在 Agent 协议这件事上确实是在“加班加点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/243fc7c4415ac7a479eb2f2b3b8b95ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，谷歌并不是第一个做这件事的。OpenAI 的 Agent Commerce Protocol&lt;/p&gt;&lt;p&gt;几个月前，OpenAI 其实也推出过一个 Agent 商业相关的协议，主打“即时结账”，帮助 Agent 发现商品并完成购买。而谷歌的一个巨大优势在于：绝大多数零售商本来就非常熟悉谷歌——比如 AdWords、广告投放，以及一整套谷歌企业服务。谷歌正在尽可能地利用这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;UCP 真正要解决的问题：可发现性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 的核心想法，是用一套协议建立“通用兼容性”。商家只需要一次性把“我卖什么、我怎么卖”按标准描述清楚，理论上就能在不同平台、不同 Agent 之间通用。而它真正想啃下的硬骨头，是 “可发现性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这对传统零售网站而言，意味着一次不小的变革：页面不再是交易的唯一入口，商品数据本身开始成为入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，谷歌也在补“数据底座”。在扩展产品数据源部分，谷歌还在其 Merchant Seller 工具中为用户提供新的“数据属性”，以便品牌可以优化其产品列表，提升 AI 搜索排名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要知道，在 AI / LLM 时代，我们过去 20 年一直在为“关键词 + 四五个要点”优化商品页，但这恰恰是 AI 最不需要的东西。这些系统需要的是：内容爆炸 + 上下文，缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子：一个自行车脚踏。几乎所有线上商品都可以有 50–100 个属性：螺纹结构、反光片数量、材质、重量、兼容标准……这叫“内容”。而“上下文”是：它更适合山地还是公路？兼容哪些车型？能不能和某些配件一起用？内容和上下文就像阴与阳，缺了任何一边，Agent 都很难可靠地做判断、更难可靠地下单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去那套 Google 商品数据规范，更像一条长满杂草的碎石路；而 Agentic Commerce 需要的，是一条 30 车道的信息高速公路——是光纤，不是拨号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果谷歌继续用旧的商品 Feed 规范来做 Agentic Commerce，在发现环节一定会失败。Gemini 拿不到足够的信息。这次他们终于开始补这一块：新增描述性文本属性、产品规格、Q&amp;amp;A、评论、特性列表、形态、口味、主题、兼容性信息、推荐配件、替代品等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方说法是“新增数十个字段”。在 Scott Wingo 看来，这个数量大概会在 24–60 个之间；即便今天只先放出 20 个，也一定会很快扩展到 30、40 个——因为所有人都会意识到：这才是决定可发现性的关键。这些数据仍然通过 Merchant Center 上传，本质上可以理解为 Google Shopping Feed 2.0。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他对所有品牌和零售商的建议只有一句：尽可能“疯狂”地扩展你的商品级内容与上下文。这将直接决定你在 AI 时代能不能被 Agent 选中、能不能“占领 Buy Box”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谁站队了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 在发布之初，就集结了科技与金融领域的一批重量级玩家，包括 Shopify、Walmart、Target、Etsy、Wayfair、Visa、Stripe、Adyen 等。首日即吸引了 20 多家合作伙伴加入，这正是标准胜出的典型路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/134c45edcab47ba358657acb357548a6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从已公开的信息来看，这些合作方大致可以分为两类：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一类是零售商与电商平台，包括 Etsy、Wayfair、Target、Best Buy、Macy’s、Kroger、Home Depot、Gap Inc.、Sephora、Ulta、Zalando、Chewy、Carrefour、Flipkart、Shopee 等；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类则是支付与清算体系，如 PayPal、Stripe、Adyen、Visa、Mastercard、American Express、Worldpay。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，有网友注意到，蚂蚁金服（ANT Financial） 也已经出现在 UCP 的合作名单中。有人评论称：“蚂蚁已经接入 UCP，但阿里巴巴推出自己的 Agentic Commerce 平台和 AI 协议，恐怕只是时间问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而从阿里最近的动作来看，这个判断并不突兀。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 15 日（今天），阿里千问 App 上线全新 AI Agent 能力“任务助理”，并打通淘宝、闪购、飞猪、高德与支付宝等应用：用户只需一句“我要两杯奶茶”，Agent 就能自动完成选店、选地址、选商品并生成订单，最后一步再由用户确认支付。延伸阅读：《&lt;a href=&quot;https://mp.weixin.qq.com/s/WXM2h4Z9DXrhVaoCjnt-ew&quot;&gt;刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖了&lt;/a&gt;&quot;》&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体看下来，一个趋势已经很难忽视：走到 2026 年，Agent 不再是大厂用来展示技术实力的“玩具”，而是开始被当成真正的赚钱工具。Agent 正在明显加速进入真实的应用场景，尤其是交易和服务这些最硬的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说得更激进一点：AI 很可能会把“社交 + 电商 + 服务”这套组合重新洗牌一遍。虽然“重做一遍”这个说法已经被用烂了，但眼下发生的变化，确实不像是在原有体系上打补丁，而更像是在重写入口、链路和分发规则——估计淘宝、京东这种级别的平台，迟早都得跟着重构一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且，这种变化最近已经变得非常明显了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&quot;&gt;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OXUn970YHVo&quot;&gt;https://www.youtube.com/watch?v=OXUn970YHVo&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&quot;&gt;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</link><guid isPermaLink="false">https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</guid><pubDate>Thu, 15 Jan 2026 11:09:21 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>谷歌推出Conductor：一款面向Gemini CLI的上下文驱动开发扩展</title><description>&lt;p&gt;谷歌发布了新的Gemini CLI预览扩展&lt;a href=&quot;https://developers.googleblog.com/conductor-introducing-context-driven-development-for-gemini-cli/&quot;&gt;Conductor&lt;/a&gt;&quot;，为AI辅助软件开发引入了结构化、上下文驱动的方法。该扩展旨在解决基于聊天的编码工具的一个常见限制：跨会话丢失项目上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor将开发上下文从瞬态会话中转移到直接存储在存储库中的持久Markdown文件中。这些文件定义了产品目标、架构约束、技术选择和工作流偏好，并作为开发人员和AI智能体的共享真相来源。其目的是使AI辅助开发随着时间的推移更加可预测、可审查和可重复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor鼓励的不是直接从提示到代码的转换，而是规划优先的工作流。开发人员在调用代码生成之前定义规范和实现计划，并且这些构件在特性的整个生命周期中仍然是代码库的一部分。这种方法旨在支持更大的任务，如特性开发、重构和在已建立的项目上工作，在这些任务中，理解现有的结构和约束是至关重要的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor的一个核心概念是轨迹，它代表了一个离散的工作单元。每个轨迹包括一个书面规范和一个面向任务的计划，该计划被分解为阶段和子任务。只有在计划被评审之后，实施才能继续进行，并在计划文件中直接跟踪进度。由于状态存储在存储库中，因此可以暂停、恢复或修改工作，而不会丢失上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期用户强调了基于轨迹的工作流，认为这是对临时提示的实际改进。Forrester的工程和产品负责人Devin Dickerson&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407465019967238146?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407465019967238146%2C7408224250060378112%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287408224250060378112%2Curn%3Ali%3Aactivity%3A7407465019967238146%29&quot;&gt;说&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于这个扩展我最喜欢的特性是轨迹的概念。在这次发布之前，我一直在使用自己构建的Conductor开源版本，我最终构建了自己的特性切片。现在轨迹已经内置了，我可以扔掉那个了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor还支持团队范围的配置。项目可以一次性定义共享标准配置，如测试策略、编码约定和工作流程偏好，并将它们一致地应用于所有AI辅助的贡献。这使得扩展不仅适用于个人开发人员，也适用于寻求跨贡献者和机器一致性的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;试用预览版的开发人员指出，它强调了明确的规划和测试驱动的工作流。Navid Farazmand&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7414757320267575297?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7414757320267575297%2C7415108568544247808%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287415108568544247808%2Curn%3Ali%3Aactivity%3A7414757320267575297%29&quot;&gt;描述道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当Gemini CLI发布时，我立即尝试用.md文件创建类似的东西。Conductor要好得多——特别是它采用的测试驱动开发方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor是Gemini CLI的预览扩展，可以从其公共&lt;a href=&quot;https://github.com/gemini-cli-extensions/conductor&quot;&gt;GitHub&lt;/a&gt;&quot;仓库安装。谷歌将这次发布定位为初始步骤，随着开发人员和团队的反馈指导未来的迭代，计划进行进一步的改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-conductor/&quot;&gt;https://www.infoq.com/news/2026/01/google-conductor/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</link><guid isPermaLink="false">https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</guid><pubDate>Thu, 15 Jan 2026 07:28:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>AI&amp;大模型</category><category>性能优化</category></item><item><title>手握30亿、被蚂蚁狂挖人，转型被骂惨的王小川，真的翻身了？</title><description>&lt;p&gt;在“大模型六小虎”成为历史后，王小川终于等来了自己的风口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，国内外大厂在医疗领域动作频繁。1月8日，OpenAI高调入局，除了推出ChatGPT Health，还收购了医疗保健初创公司Torch。几乎同期，Anthropic、英伟达、苹果等都有产品和合作发布。国内，蚂蚁阿福自发布后短期内月活用户突破3000万，单日提问量超千万。资本市场上，AI 医疗板块逆势走强，成为最近市场热点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此前大模型竞争激烈的当口，AI 医疗并不是一个很性感的话题。那种不信任来自百川内外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年成立的百川在一年后战略收缩，决定聚焦医疗，成为国内较早专注到医疗的大模型创企。但内部“没有足够传达在医疗上的决心和路径要求，没有让每个团队在医疗价值创造中深度思考why和how，进而导致部分团队工作目标出现了摇摆和偏差。”“去年中途转过来时被骂惨了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不只内部，业界对AI医疗也存有疑虑，连带着对百川的路线选择也有质疑。“2024年跟医生谈AI，大家都不信。”王小川直言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到2025年，大家看到DeepSeek真的比百度靠谱很多；年末阿福发布，投了10亿来砸广告，看到了技术和应用进展；今年1月8日，OpenAI Health 正式上线，Anthropic 也发布了自己的两个技术能力：医疗计算和Agent，两个巨头都开始进入医疗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“所以，从市场判断来看，医疗作为AI‘皇冠上的明珠’这样的高级阶段，已经开始进入应用范畴。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6cdda9230d6c5e944f95f7509736824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从发布反思信至今9个月过去，王小川向 InfoQ 表示，百川如今的护城河主要有三个：一是模型结构的优先级，“医疗安全性”和“诊断准确性”始终是首位；二是切入点选择，百川聚焦严肃、高价的医疗场景，区别于其他企业的健康类打法，这类场景的壁垒更高，且有明确的付费意愿；三是产品形态的差异化，百川身份差异化服务和决策辅助能力，是现有产品不具备的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川尤其提到，大厂和创业公司不一样，他们有职业团队，需要的是更安稳的方案。“大创新靠小厂，小创新靠大厂，必须切入我们认为有高价值的事情，共识不是我们优先的突破点，而大厂更多的是注重共识，路线图和产品形态是不一样的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模型要低幻觉、能问诊，多模态非主战场&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“去年8月发布的M2作为百川重新聚焦医疗之后的主力模型，在行业得到很多好评。典型现象就是蚂蚁开始疯狂挖人，从技术人员到财务人员，所以属于小圈子认可技术路线图。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨天，百川正式开源了新一代医疗大模型 Baichuan-M3。据百川智能模型技术负责人鞠强介绍，Baichuan 系列采用SCAN框架，实现临床医生层级的推理与问诊。其核心在于不仅询问疾病类型，更通过定量问题将模糊主诉转化为可定位、可量化的临床证据；并且突破单一症状的局限，进行跨系统关联推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，团队高度重视并主动防控大模型在医疗中的“幻觉”，坚持正确知识并进行原子级事实检验：在模型推理过程中进行逐层事实核查，确保结论基于真实输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;鞠强介绍，在模型训练中，抑制“幻觉”与提升推理能力之间存在明显的“跷跷板效应”，容易陷入两种极端：若过度追求推理表现，其生成内容会更丰富、答对率上升，但幻觉也难以控制；若强力抑制幻觉，模型则会趋向过度保守，回答变得拘谨甚至回避问题，导致实用性下降。这也是团队在Baichuan-M3训练中重点攻克的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为破解这一矛盾，研发团队引入了 Fact-aware 强化学习技术。该技术核心在于，在强化训练过程中，既对幻觉进行充分压制，又确保推理能力不受损，反而同步提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果显示，相比前代模型M2，百川正式开源新一代医疗大模型 Baichuan-M3 的幻觉率大幅下降，同时在医疗专业评测HealthBench上的推理能力得分从34分显著提升至44分，位列榜首。在不依赖工具或检索增强的纯模型设置下，医疗幻觉率3.5，超越GPT-5.2。“这验证了我们通过强化学习方法，在抑制幻觉与增强推理之间取得了有效平衡。”鞠强表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e6/e66ada51aa81717429a27588ca063a4c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Hugging Face 地址：&lt;a href=&quot;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://github.com/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，模型深度集成的问诊能力，从日常症状中识别风险。团队设计了防御性思维追问，以甄别背后潜在的系统性疾病，还会进行组合症状敏锐识别，比如用户描述“情绪激动时左牙疼”时，模型能会关联“牙痛+情绪症状”，优先建议排查心脏系统问题，从而排除重大隐患，而非直接推荐牙医或止痛药。该能力已集成至产品，服务于医生与普通用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在AI医疗中，除了文字，还有影像等信息。不过，王小川认为，多模态并非当前AI主战场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他解释道，ChatGPT之所以令人震撼，正是因为它展现出一种“智力”，而智力的本质，是将具体事物进行抽象的能力，其核心在于符号系统。在这一逻辑下，智能主要依托于三种形式语言：自然语言、数学语言与代码语言。至今，评估一个模型能力的强弱，本质上仍是检验其符号处理与逻辑推理的水平，功能可用并不等同于智力高超。在医疗领域，这一观点尤为关键。医疗的核心是决策，而不仅仅是感知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，未来医学影像的初步解读可由专用小模型完成，许多厂商也已具备相应的图像引擎。但真正的价值在于：将影像符号化之后，如何用语言模型进行综合推理与判断。因此，感知模型与认知模型必须结合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，当前的一些工作，比如将CT影像转化为报告，或是专注于胰腺癌筛查的视觉模型，固然有其价值，但它们更像是“挂在智力之树上的叶子”，是整体流程中的一环，而非驱动智能演进的主战场。真正的突破，仍在于如何通过符号与语言，构建能够进行复杂医疗决策的认知核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“在中国To C比To B更好”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来巨大的增量是在院外，不在院内。”王小川说道。其核心是直接服务患者，而不是通过服务医生间接服务患者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反观OpenAI的入局是靠打造“个人超级助手”，Anthropic则从合规性与临床效率上做B端突围。对此，王小川的评价是：“美国是To C和To B都可以干，但在中国To C比To B更好。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川认为，国内的医疗现状是医生供给不足，互联网虽能连接信息却无法创造供给；医患权力不均，双方容易沟通不畅、患者无助；患者更倾向三甲医院，致使基层医疗薄弱；医疗知识分散于各科室，复杂病症往往缺乏整体视角。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于此，他的设想是AI 可以“造出高质量医生”，但不是要AI取代医生。“在某些维度上，AI超过医生是必然的，比如信息收集的完整性、医学知识的储备量、循证的精准度等。但AI不会取代医生的核心执行能力，比如手术、查体等。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在不取代医生的情况下，AI 可以推动“权力让渡”，即帮助患者理解病情与方案，获得更多参与权和知情权。另外，居家通过AI进行初步咨询，让“居家首诊”可能，减轻医疗系统负担。此外，复杂问题需要跨科室会诊，以前就是入院即入组，即进入某个科研队列，有了AI后能够做到“看病即入组”，更有机会做好生命模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在实现的产品形态上，百川目前主打还是百小应App，不过用户进入后可以选择医生和患者两种身份，给出的结果是不一样的：医生版更像OpenEvidence，答案更加专业、更加强调循证，引用的文章在系统中100%存在，让其能够做决策、信息够充分；患者版本则强调补充信息，进入启发式端到端的问诊，也给到患者决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们与OpenEvidence的区别在于，OpenEvidence只是服务于医生，百川是可复数、可懂、可决策、可行动、能够服务到患者的，这样的产品定位在全球是独一无二的。”王小川补充道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在其看来，做To C产品，重点是让产品价值触达真正的目标人群，即有严肃医疗需求、愿意为决策辅助付费的患者。他举例称，达摩院做的胰腺癌平扫CT模型，虽然技术门槛高，但解决了核心临床痛点，就有明确的付费方；而泛健康类服务看似覆盖广，但价值不突出，反而难以找到稳定的付费用户。百川目前的做法就是基本全覆盖，重点放在儿科、慢病和肿瘤，优先突破有明确痛点的领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;收费模式上，王小川认为，不是只赚医院或医生的钱，还可以向患者收费，也可以形成服务包，后面的医疗资源和药械以服务包形式收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我倒不担心商业模式本身，确实要过了这个门槛、为用户创造价值，之后不管直接收费还是生态收费都是很容易的事情。”王小川说道。目前，百川账上还有 30 亿人民币，这也留给了王小川证明的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据王小川透露，今年上半年，百川会完成两款产品的发布和推广，核心是回归决策层面，帮助用户（包括患者和医生）做出更好的医疗决策，最终实现“医生时刻陪伴式”的健康管理。“我们第二个产品已经可以当成院外医生来看了。”此外，百川也有计划硬件产品发布和出海计划，具体日程未定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了培养用户心智，百川未来也会增加一定的广告宣传投入，另外会重视医生对产品的认可度。“阿福跟我们的路线不一样，老医生都是无感的。我们希望医生和患者一体两面，共享一款产品，要让专家点头，而不只是患者鼓掌。产品做好以后确实能够取得一定的口碑效应。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“今年上市的两家主要还是踩在通用模型技术红利和政策支持的基础上，但目前他们的市值和商业化能力并不匹配，但AI医疗今天也是大模型竞争中的一个范式，虽然它的成熟会晚一点，在后面我们肯定也是奔着上市去的。”王小川给了自己两年的时间再看看。&lt;/p&gt;</description><link>https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</link><guid isPermaLink="false">https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</guid><pubDate>Thu, 15 Jan 2026 06:51:01 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>LangGrant推出LEDGE MCP服务器，赋能企业数据库启用代理式AI</title><description>&lt;p&gt;&lt;a href=&quot;https://www.langgrant.ai/&quot;&gt;LangGrant&lt;/a&gt;&quot;推出了LEDGE MCP服务器，这是一个新的企业平台，旨在让大语言模型在复杂的数据库环境中进行推理，而无需直接访问或暴露底层数据。该版本旨在消除组织在将代理式AI应用于受受控生产数据时面临的一些最大障碍，即安全限制、失控的token成本和不可靠的分析结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司表示，LEDGE MCP服务器允许LLM跨&lt;a href=&quot;https://www.oracle.com/&quot;&gt;Oracle&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;SQL Server&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;Postgres&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.snowflake.com/en/&quot;&gt;Snowflake&lt;/a&gt;&quot;,等数据库生成准确、可执行的多步骤分析计划，同时将数据完全保留在企业边界内。通过依赖模式、元数据和关系而不是原始记录，该平台消除了将大型数据集推送到LLM的需要，从而大大减少了token的使用并防止敏感数据泄漏。根据LangGrant的说法，通常需要数周手工编写查询和验证的任务现在可以在几分钟内完成，并具有完全的人工审查和可审计性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant首席执行官、首席技术官兼联合创始人&lt;a href=&quot;https://www.linkedin.com/in/rameshpar/&quot;&gt;Ramesh Parameswaran&lt;/a&gt;&quot;表示：“LEDGE MCP服务器消除了LLM和企业数据之间的摩擦。”他指出，企业现在可以安全、经济地将代理式AI直接应用于现有的数据库生态系统，而不会损害治理或监督。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在许多组织中，上下文工程和代理式AI正从实验阶段进入生产环境。许多企业已经接受了AI助手，但在操作数据库方面却停滞不前。安全策略通常禁止直接访问LLM，在分析原始数据时token和计算成本会激增，开发人员和业务用户都在努力应对企业模式的规模和复杂性。即使使用AI辅助编码工具，工程师也经常花费数周时间手动将部分上下文输入模型，以生成可用的查询和管道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant将LEDGE定位为一个全面解决这些问题的编排和治理层。MCP服务器管理LLM如何与企业数据交互，确保符合访问控制和策略。分析和推理使用数据库上下文而不是数据有效负载来执行，以降低成本并减少幻觉风险。该平台还可以自动创建可由人工团队检查、批准和执行的多阶段分析计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，LEDGE支持按需克隆和容器化类似生产的数据库，为智能体开发人员提供安全、隔离的环境来构建和测试AI工作流。通过跨异构系统自动映射模式和关系，该平台使LLM能够跨多个数据库进行推理，而无需读取底层数据本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了LEDGE MCP服务器，LangGrant认为企业对AI的采用将更少地依赖于更大的模型，而更多地依赖于安全的编排、治理和成本控制。该公司认为，通过保持数据原位，同时为LLM提供全面的上下文理解，企业最终可以准确、安全、大规模地将AI应用于其最有价值的数据资产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多公司正在采用MCP风格的服务器，在不暴露原始数据的情况下为AI智能体提供安全、结构化的环境，但它们的重点领域有所不同。 &lt;a href=&quot;https://www.infoq.com/news/2025/04/github-mcp-server-public-preview/&quot;&gt;GitHub&lt;/a&gt;&quot;的MCP服务器以开发人员的工作流程为中心，允许LLM在执行访问控制的同时对存储库、问题、拉取请求和CI元数据进行推理。同样，微软的&lt;a href=&quot;https://www.infoq.com/news/2025/07/azure-devops-mcp-server/&quot;&gt;Azure DevOps MCP&lt;/a&gt;&quot;向AI智能体公开结构化项目和管道上下文，以支持规划、故障排除和交付自动化，而不是深度分析数据处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了开发者平台，MCP概念也出现在基础设施和运营中。&lt;a href=&quot;https://www.infoq.com/news/2025/11/buoyant-linkerd-mcp-support/&quot;&gt;Linkerd&lt;/a&gt;&quot;等服务网格项目正在探索MCP集成，为AI智能体提供对服务流量、遥测和策略执行的安全可见性。云提供商还通过他们的AI服务（如&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt;&quot;和&lt;a href=&quot;https://cloud.google.com/?hl=en&quot;&gt;谷歌云&lt;/a&gt;&quot;）提供类似MCP的上下文层，这些服务允许智能体查询基础设施元数据和操作信号，而无需将敏感数据直接传递给模型。这些方法侧重于操作意识，而不是数据分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与这些产品相比，LangGrant的LEDGE MCP服务器以专注于企业数据库和分析而脱颖而出。总之，这些平台显示了MCP如何成为一种基础模式，每个实现都针对企业堆栈的特定层进行了定制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&quot;&gt;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</link><guid isPermaLink="false">https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</guid><pubDate>Thu, 15 Jan 2026 06:42:00 GMT</pubDate><author>作者：Craig Risi</author><category>AI&amp;大模型</category><category>数据库</category></item><item><title>QCon 北京 2026 启动｜Agentic AI 时代的软件工程重塑</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年，我们分别在&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing&quot;&gt;北京&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/shanghai/&quot;&gt;上海&lt;/a&gt;&quot;举办了两场 QCon 全球软件开发大会。过去一年里，我们和大量一线技术团队、工程负责人、开发者持续交流，感受到一个很明显的变化：大家讨论的重点，正在从“AI 能做什么”，转向“AI 怎么在生产系统里稳定运行、可控交付、持续产生价值”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不是热度退去，而是行业进入了更难、也更关键的阶段——从演示走向长期运行，从能力展示走向工程兑现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，越来越多团队开始回到同一个问题：当 AI 真正进入业务流程后，系统能不能“长期跑得住”？成本能不能算得清？质量、风险、合规能不能兜得住？组织的协作方式要不要跟着变？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，智能体（Agentic AI）&amp;nbsp;成为不少团队正在尝试的新方向：它不只是一次回答或一次推理，而是把感知、工具调用、任务执行、反馈迭代串成一个可运营的流程，逐步嵌入研发、交付与业务链路。可以预期，进入 2026 年，这类探索会从局部试点走向更体系化的工程建设：不仅是“加一个 AI 功能”，而是软件系统、研发流程乃至组织协作方式都要随之调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;软件工程正在发生的变化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们看到的变化不只是工具升级，更像是一套工程范式在被重写：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统架构开始围绕「智能体协作」重新设计工程方法论从「确定性流程」迈向「人机协同闭环」研发组织面临角色重塑与能力重构产品与交互从“界面驱动”走向“意图与行动驱动”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从基础设施、推理与知识体系，到研发与交付流程，再到前端、客户端与应用体验——AI 正在以更工程化的方式进入软件生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;QCon 北京 2026 的核心主线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot;&amp;nbsp;将以&amp;nbsp;「Agentic AI 时代的软件工程重塑」&amp;nbsp;作为大会核心主线，把讨论从&amp;nbsp;「AI For What」，走向真正可持续的&amp;nbsp;「Value From AI」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一主线，我们将从六个关键维度系统性展开探索：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;前沿技术雷达（Future Tech）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来 1–2 年最值得提前布局的方向：Agentic AI 的新形态、下一代模型、交互范式与系统架构演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;架构设计与数据底座（系统可演进）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论如何构建可扩展、可演进、可复用的 AI 系统：Agent 架构、数据治理、知识体系与工程实践，回答“能不能长期跑”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;效能与成本（拒绝盲目烧钱）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注让 AI “跑得起、跑得快、跑得稳”的工程方法：在算力、推理、工程效率与 ROI 之间，寻找真正可持续的平衡点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;产品与交互（体验提升）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚焦前端、客户端与产品层的 AI 原生改造：人机协作、意图驱动交互、任务闭环体验，以及 Agent 参与下的产品新范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;可信落地（守住底线）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论 AI 带来的新风险。从 Demo 到 &amp;nbsp;Production 的“最后一公里”信任危机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;研发组织进化（长期主义）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来团队如何生存与进化：重塑研发角色分工、协作模式与工程文化，构建面向 AI 时代的组织能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;我们希望在 QCon 北京 2026 呈现的&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026 想呈现的，不只是“又一场关于 AI 的大会”，而是这轮变化真正落到工程与组织之后的全景：哪些方向已经走通，哪些正在付出真实成本，哪些系统必须被重构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，北京站部分专题已上线，我们期望持续挖掘来自一线生产环境的长期实践，呈现 Agentic AI 融入软件工程后的真实样貌——成功经验、工程妥协与关键取舍并存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9d/9def3983a8e8e8f60f08f3560ccf39b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多嘉宾邀请进行中&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也欢迎你带着真实问题与实践加入其中，与更多同行一起，把这场正在发生的软件工程重塑讲清楚、做扎实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026，期待与你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📍&amp;nbsp;会议官网：&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;https://qcon.infoq.cn/2026/beijing/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📩&amp;nbsp;演讲申请：&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;https://jinshuju.com/f/Cu32l5&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲评审标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;观点：是否清晰、有判断力，能否帮助听众形成有效认知实践：内容须来源于真实工程或业务实践深度：是否具备可复用的方法论或经验价值专业声誉：演讲者在相关领域的实践背景与影响力不做广告：QCon 不是厂商宣传舞台听众所得：听众能带走什么，是我们最关注的标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲嘉宾福利&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🎟&amp;nbsp;免费参会：自由参加大会全部课程💸&amp;nbsp;专属折扣：提供特别优惠码，方便同事与朋友购票📰&amp;nbsp;独家报道：有机会接受 InfoQ / 极客时间的深度采访🏨&amp;nbsp;免费住宿：为外地嘉宾提供酒店入住✈️&amp;nbsp;无忧差旅：承担嘉宾往返会场的交通费用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/a4/a4b836a91798f84c96c46e2f4c7ec73a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</link><guid isPermaLink="false">https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</guid><pubDate>Thu, 15 Jan 2026 03:17:38 GMT</pubDate><author>Kitty</author><category>AI&amp;大模型</category><category>软件工程</category></item><item><title>中了！极客时间入围中国移动培训服务一采供应商</title><description>&lt;p&gt;极客时间企业版（极客邦控股（北京）有限公司）成功入围中国移动 2026–2028 年培训服务集采项目，正式成为其一级供应商。在技术、市场及政企、培训资源开发三大标段中均取得优异成绩，彰显了公司在 IT 与数智化培训领域的深厚实力和生态优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;中标概览：三大赛道，全面突破&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在本次集采中的表现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版在“标包 7（培训资源开发）”中勇夺魁首，依托成熟的课程研发体系与知识产品化能力，彰显了其在高质量、体系化数智课程开发方面的硬核实力；在“标包 2（技术）”中位列三甲，体现了在 AI、云计算、大数据等前沿技术培训领域的扎实积淀；同时强势入围“标包 3（市场及政企）”，进一步验证了其助力企业业务增长与数智化转型的全面解决方案能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7f41da54f421e75a37be20573c31cdde.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2ecdf1eff746c6b8b31dcf1de2c9e170.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;能力解读：“媒体+产品+生态”的复合优势&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版之所以能快速响应不同标包的需求，根源在于公司长期以来打造的“内容+产品+生态”模式：&lt;/p&gt;&lt;p&gt;极客时间企业版则凭借培训平台与课程产品，将培训需求转化为可落地、可衡量的学习成果。InfoQ 极客传媒提供前瞻行业洞察，精准把握人才培养方向。TGO 鲲鹏会链接高端产业资源与实战智慧，构建协同发展的高管智库。&lt;/p&gt;&lt;p&gt;依托公司各业务板块的协同效应，极客时间企业版将持续为包括中国移动在内的广大合作伙伴提供“严选内容、高效转化”的培训服务，践行“助力客户成功”的价值承诺，提升企业人才发展的综合回报。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52a9b1971f02f614f0ec5511ccc54aff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0898ab691c26366e1602f2a158deff38.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;时代召唤：AI 浪潮下的企业人才变革&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当 AI 重构千行百业，企业对“懂技术、会落地、能创新”的数智人才需求，已从“可选”变为“刚需”。极客时间企业版始终致力于将前沿数智技术与实战知识体系深度融合，此次入围正是对公司在应对时代命题、推动产业人才升级方面能力的高度认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深化创新，践行使命&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;立足新起点，极客时间企业版将以此次合作为引擎：对内，持续深化课程内容与服务创新；对外，将集采所带来的资源与平台优势，探索数智人才培养的新模式、新场景。我们坚信，专业的培训服务不仅是知识的传递，更是产业的赋能。未来，极客邦科技将继续秉持“推动数智人才全面发展，助力数智中国早日实现”的使命，与中国移动及所有伙伴一道，用人才之力点亮数智未来！&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a405ee19565ac3e822dd33d1fee8d4a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图为：极客时间企业版产品服务概览）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;合作咨询&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欢迎联系极客时间企业版，我们将按照您的企业场景、业务目标和人才发展要求，提供专属人才培养解决方案，助力您的企业致胜 AI 时代。敬请点击“阅读原文”访问官网，或扫描下图二维码&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d51c0dec780f590df20bbbd672190d33.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</link><guid isPermaLink="false">https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</guid><pubDate>Thu, 15 Jan 2026 02:59:38 GMT</pubDate><author>极客时间企业版</author><category>数字人才培养</category></item><item><title>实测谷歌Veo 3.1：新增原生竖屏模式和4K画质，换个语言翻车到离谱？</title><description>&lt;p&gt;刚刚，谷歌更新了其 Veo AI 视频生成器，新增原生竖屏视频生成与 4K 分辨率支持功能。此次对 “文生视频” 功能的调整，旨在提升画面清晰度的同时，确保不同场景中的主体元素保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bd/bd8233a10f0980d44fe46156c0a3fe68.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Veo 3.1 的更新，解决了生成式视频领域一项长期存在的挑战：保持镜头间的视觉一致性。谷歌表示，新款模型在场景切换时能更好地保留人物特征与背景纹理，从而更容易重复使用特定的视觉元素，或在多场景叙事中贯穿同一主题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fee0a6257754c1b1378f71f8e313eac7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最显著的改进是对“素材到视频”工具的重大优化。用户只需添加三张参考图片：一张用于主体，一张用于背景，一张用于展现所需的视觉效果或风格。然后，只需添加一些文字即可开始制作。即使提示信息较短，Veo 3.1 也能在提供参考图像后生成角色表情和动作更生动的视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;移动创作者是本次更新的核心受众。升级后的 Veo 可直接生成 9:16 比例的原生竖屏视频，创作者无需对横屏素材进行裁剪，也不必牺牲画质，就能制作出适配 YouTube Shorts 等平台的全屏内容。针对更专业的创作流程，谷歌还新增了 1080P 至 4K 的画质提升选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，这些新功能已率先在 Gemini 应用、YouTube Shorts 及 YouTube Create 工具中上线，并将逐步覆盖谷歌旗下更多创作者工具与企业级服务。为区分生成内容与真实拍摄素材，谷歌会在视频文件中嵌入肉眼不可见的 SynthID 数字水印。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有体验用户反馈，Veo 3.1似乎存在不同语言版本表现差距太大的问题。“巴西葡萄牙语的人物音频存在音画不同步、台词错乱的问题，其他语言版本的表现则相对更佳。我曾指令其生成一段鹦鹉以沙哑嗓音鸣叫的音频，但该需求最终未能实现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/32/32c77f6b5acf1e71fc9768effdd53ba3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，此次更新距苹果与谷歌官宣合作、计划在下一代 Siri 中集成 Gemini 模型仅过去一天。与此同时， OpenAI 已达成合作，计划将迪士尼角色引入 Sora 平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</link><guid isPermaLink="false">https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</guid><pubDate>Thu, 15 Jan 2026 02:43:36 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>估值1亿的“死了么”APP有多好抄？5分钟AI就能复刻，去年有人一下午做出原型</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，上了热搜、又在苹果付费软件排行榜的榜首上挂了多日的&quot;死了么&quot;APP，突然宣布更名为Demumu。据其称，“经团队审慎决策，‘死了么’APP将于即将发布的新版本中，正式启用全球化品牌名Demumu。继昨日获得BBC报道后，我们的服务在海外实现了爆发式增长。未来，Demumu将继续秉持安全守护的初心，把源自中国的守护方案带向世界，服务全球更多独居群体。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自推出后，该APP的热度剧增，下载量一度暴涨100倍。虽开发成本仅1000多元，但获得不少头部投资机构的青睐，现在的估值已经飙到了1亿元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而最值得一提的是，这一APP的完整原型竟然在去年初就有了，可做出来的却不是同一批人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;痛失1亿的“原作者”，5分钟复刻出海外版&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“好消息：我去年做了一个‘死了么’APP；坏消息：我只做了产品设计和UI并发了文，但我当时觉得这只是个用来博眼球的噱头。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在死了么APP爆火后不久，一位ID为“饼干哥哥AGI”的数据分析师公开表示，去年3月，他从小红书上经常看到这个需求，随后花了不到一个下午的时间用Cursor和Claude 3.7生成了完整APP原型，并且将提示词模版和操作步骤都发布了出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;提示词by @花叔：&amp;nbsp;我想开发一个类似外卖APP「饿了么」，APP叫「死了么」，用于养老的，每天问一句，以防独自一个人死在家里没人发现。APP也有骑手，哪里有人死了就去接单收尸。 注意这是专门为独居90后的年轻人设计的。风格要求清新好看、APP内的文案多用搞怪的网络用语。&amp;nbsp;现在需要输出高保真的原型图，请通过以下方式帮我完成所有界面的原型设计，并确保这些原型界面可以直接用于开发：1、用户体验分析：先分析这个 APP 的主要功能和用户需求，确定核心交互逻辑。2、产品界面规划：作为产品经理，定义关键界面，确保信息架构合理。3、高保真 UI 设计：作为 UI 设计师，设计贴近真实 iOS/Android 设计规范的界面，使用现代化的 UI 元素，使其具有良好的视觉体验。4、HTML 原型实现：使用 HTML + Tailwind CSS（或 Bootstrap）生成所有原型界面，并使用 FontAwesome（或其他开源 UI 组件）让界面更加精美、接近真实的 APP 设计。拆分代码文件，保持结构清晰：5、每个界面应作为独立的 HTML 文件存放，例如 home.html、profile.html、settings.html 等。- index.html 作为主入口，不直接写入所有界面的 HTML 代码，而是使用 iframe 的方式嵌入这些 HTML 片段，并将所有页面直接平铺展示在 index 页面中，而不是跳转链接。- 真实感增强：&amp;nbsp; - 界面尺寸应模拟 iPhone 15 Pro，并让界面圆角化，使其更像真实的手机界面。&amp;nbsp; - 使用真实的 UI 图片，而非占位符图片（可从 Unsplash、Pexels、APPle 官方 UI 资源中选择）。&amp;nbsp; - 添加顶部状态栏（模拟 iOS 状态栏），并包含 APP 导航栏（类似 iOS 底部 Tab Bar）。请按照以上要求生成完整的 HTML 代码，并确保其可用于实际开发。&amp;nbsp;操作步骤：1.打开Cursor编辑器（确保版本足够新，支持Claude 3.7）2.选择编辑Agent模式3.选择Claude 3.7 Sonnet作为模型，最好是用thinking4.粘贴上述提示词，填入你需要的APP类型5.等待生成完成，可能需要3-5分钟&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术已经不值钱了。现在再重新做的话，可能一个下午的时候都够从0到设计到APP STORE上架了。”这位数据分析师感叹道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，他又花了5分钟就复刻出了&quot;死了么&quot;APP海外版。据称，这次他没敲任何代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;三名95后共同开发，APP收费已涨了8倍&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个引爆全网的APP，到底有什么魅力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，该应用是为独居人群打造的低成本安全工具，核心功能十分简单。用户无需注册登录，首次使用只需填写姓名和紧急联系人，每天打开应用完成签到即可；若连续2天未签到，系统将于次日自动发送邮件告知紧急联系人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最开始，“死了么”APP的收费只要1元，现在涨到了8元。团队表示，这是为了让项目能够健康、持续地发展，并覆盖日益增长的短信、服务器等成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该应用由三名95后共同创立并独立运营，在走红后，其背后的创始人们也陆续“现身”并对外披露了项目相关情况。“死了么”APP创始人之一郭先生介绍，团队内只有三名95后成员，且各自有自己的本职工作，通过远程方式进行协作。项目大约在2025年年中立项，开发时间不到一个月，初始投入成本仅1000多块钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位创始人吕先生表示，早在两三年前他们就在社交平台上留意到相关需求，“近几年大家都会讨论‘什么APP是每个人都需要的，并且一定会下载的’，就有网友提到‘死了么’APP，这个创意出来之后有很大的讨论度，我们看到了其中的需求，并且这件事本身也很有意义，于是我们就尝试去注册这个名字，发现可以注册，后续又用了一个月时间完成了开发。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据创始人郭先生透露，增长从2026年01月09日左右开始，短短两天内，下载量相比之前暴涨100倍以上，且仍在持续攀升。吕先生表示，现在APP的下载量不太方便透露，但“确实增长速度非常快”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据点点数据，除中国市场外，目前“死了么”APP在多国苹果应用商店霸榜第一：在新加坡付费榜位居总榜第一，在比利时、荷兰、瑞典等国付费工具榜排名第一，在英国、澳大利亚、美国等10多个国家付费工具榜排名第二。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公开信息显示，“死了么”APP由月境（郑州）技术服务有限公司开发，这家成立于2025年03月10日的公司，注册资本仅10万元，法定代表人为郭孟初，由其100%持股。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前有消息称，该公司已经接触到投资意向，正计划以100万元出让公司10%的股份。也就是说，这时其估值已达到1000万元。当前的最新消息是，如今该APP用户已增长800倍。并且，随着与六七十家投资方的深入接触和洽谈，短短两日间，“死了么”APP的估值飙升至近1亿元。但目前，其团队仍维持出让公司10%股份的计划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;遍地都是“活了么”，免费版卷疯了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“死了么”APP爆火后，陆续有网友提出优化建议，有人提出可以改成通过短信通知紧急联系人、优化签到形式等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，其团队已透露了后续的发展规划：接下来将把主要精力投入到产品打磨中，例如丰富短信提醒功能、考虑增加留言功能，并探索推出更适老化的新产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于为软件带来广泛共鸣与关注的名字，也开始迎来争议。不少网友认为，“死了么”不好听，建议改成“活着么”。苹果官方客服也于1月9日作出回应，称用户若对APP名称不满，可提供应用基础信息，客服将同步至相关业务部门，协调联系开发者沟通处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，“死了么”APP已改名为Demumu。有网友认为，“死了么APP之所以火，这名字最起码占一半功劳。改名，大概率算是把魂给丢了。只有当它从工具属性进化到社区属性，它才有可能活下来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而另一款名为“活了么”的APP已上架苹果应用商店，其功能与“死了么”相似，但目前是免费版的。此外，在苹果应用商店，有十几个类似名称与功能定位的APP也扎堆上线了，如“活着么”、“还活着么”“我还在”“我还活着呢”“我还好”等。其中，“活着么”目前就有9个，大部分都是免费版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da56da2e0e8757d5e369b1f2807e55d7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&quot;&gt;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</link><guid isPermaLink="false">https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</guid><pubDate>Thu, 15 Jan 2026 02:36:04 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>亚马逊云科技为S3 Tables添加智能分层存储和复制功能</title><description>&lt;p&gt;亚马逊云科技最近宣布为&lt;a href=&quot;https://aws.amazon.com/s3/features/tables/&quot;&gt;S3 Tables&lt;/a&gt;&quot;引入两项新功能，第一项功能是新的智能分层存储类，该存储类能够根据访问模式自动优化成本，第二项功能是支持跨AWS区域和账户自动维护一致的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html&quot;&gt;Apache Iceberg&lt;/a&gt;&quot;表副本的复制功能，该过程无需手动同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/tables-intelligent-tiering.html&quot;&gt;智能分层存储类&lt;/a&gt;&quot;会将数据自动分配到最具成本效益的三个低延迟层级之一，即Frequent Access、Infrequent Access或Archive Instant Access。据公司介绍，最后一种是最低成本的层级，比Infrequent Access层级便宜68%。亚马逊云科技的主任开发者倡导者Sebastian Stromacq这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在无访问达30天后，数据会被移动到Infrequent Access层级，在90天后，则会迁移到Archive Instant Access层级，这一过程不会对应用程序造成影响或性能降低。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;默认情况下，表使用标准存储类，但创建表时可以指定智能分层（Intelligent-Tiering）作为存储类，用户也可以在表存储桶级别配置默认存储类。用户可以将智能分层设置为表存储桶的默认存储类，如果在创建表时未指定存储类，那么表将自动存储在智能分层中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以利用&lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS命令行界面（AWS CLI）&lt;/a&gt;&quot;，通过put-table-bucket-storage-class和get-table-bucket-storage-class命令来更改或验证其S3表格存储桶的存储层级。相关命令如下所示：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;aws s3tables put-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \
   --storage-class-configuration storageClass=INTELLIGENT_TIERING


# Verify the storage class
aws s3tables get-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \


{ &quot;storageClassConfiguration&quot;:
   {
      &quot;storageClass&quot;: &quot;INTELLIGENT_TIERING&quot;
   }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;来自Imperious Enterprise的AWS架构师Adefemi Adeyemi在LinkedIn的&lt;a href=&quot;https://www.linkedin.com/posts/adefemi-adeyemi_if-you-are-working-with-apache-iceberg-on-activity-7402006733004406784-SMSW&quot;&gt;帖子&lt;/a&gt;&quot;中指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数分析数据集在一段时间内是“热”的，但随后会逐渐“冷却”。借助S3 Tables的智能分层功能，你无需不断调整Iceberg数据的生命周期策略。该服务会根据访问模式自动将对象移至更便宜的存储层级，这对长期存在的数据湖来说是一大优势。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，S3 Tables的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-replication-managing.html&quot;&gt;复制功能&lt;/a&gt;&quot;可以帮助用户跨AWS区域和账户维护表格的一致性只读副本。当声明目标表格的存储桶时，服务会创建只读的副本表格，并以时间顺序复制所有更新，同时保持父子快照关系。这些副本表格将在源表格更新后的几分钟内得到更新，并支持独立于源表格的加密和保留策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Stromacq说到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用户可以通过&lt;a href=&quot;https://aws.amazon.com/sagemaker/unified-studio/&quot;&gt;Amazon SageMaker Unified Studio&lt;/a&gt;&quot;或任何兼容Iceberg的引擎（包括&lt;a href=&quot;https://duckdb.org/&quot;&gt;DuckDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://py.iceberg.apache.org/&quot;&gt;PyIceberg&lt;/a&gt;&quot;、&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&quot;和&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&quot;）查询副本表格。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助AWS Management Console、API或&lt;a href=&quot;https://aws.amazon.com/tools/&quot;&gt;AWS SDK&lt;/a&gt;&quot;，用户可以创建和维护表格副本。此外，他们可以指定用于复制源表格的目标表格存储桶。当用户启用复制功能时，S3 Tables会在这些存储桶中创建只读副本，使用最新状态进行回填，并持续监控更新以保持同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在同一篇LinkedIn帖子中，Adeyemi指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对复制功能的原生支持让你能够快速创建只读副本，这些副本在几分钟内即可与源表保持同步，并且可作为Iceberg表进行查询。减少了自定义集成的工作量，让你有更多时间真正使用数据。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以通过&lt;a href=&quot;https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html&quot;&gt;AWS Cost and Usage Reports&lt;/a&gt;&quot;和&lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;Amazon CloudWatch&lt;/a&gt;&quot;指标跟踪各访问层的存储使用情况。配置智能分层无需额外费用，用户仅需支付各层的存储成本。至于S3 Table的复制，用户需支付目标表格的S3 Table的存储费用、复制PUT请求的费用、表格更新（提交）以及复制数据的对象的监控费用。更多详情可参见&lt;a href=&quot;https://aws.amazon.com/s3/pricing/&quot;&gt;定价页面&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/s3-tables-intelligent-tiering/&quot;&gt;&amp;nbsp;AWS Adds Intelligent-Tiering and Replication for S3 Tables&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</link><guid isPermaLink="false">https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</guid><pubDate>Thu, 15 Jan 2026 02:34:41 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>数据库</category></item><item><title>Data+AI 新年特辑：2025 的顿悟时刻与 2026 的关键十问 | Q推荐</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，Data + AI 的讨论正在悄然发生变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的关注点，逐渐从模型能力本身，转向企业是否真正具备承载 AI 的系统能力：数据是否准备充分，工程体系是否稳定，AI 是否真的进入业务流程并长期运行。这些问题开始频繁出现在一线实践中，也成为企业在推进 Data + AI 过程中无法回避的现实考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业的变化并非源于某一次集中发布，而是在一次次真实落地、反复试错和持续修正中逐步显现。也正因为如此，2025 成为了一个值得回望的年份，许多重要判断，往往产生于具体实践中的“顿悟时刻”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，InfoQ 联合 Snowflake 发起了&amp;nbsp;「MAKE IT SNOW｜2025–2026 Data + AI 年度时刻」&amp;nbsp;直播活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一场围绕企业 Data + AI 战略展开的年度复盘与前瞻对话。活动邀请来自数据平台、开源社区，以及制造、医疗、汽车等行业的一线技术与业务负责人，围炉而坐，如老友般对谈 。我们将共同回到真实的问题本身，剖析企业在推进 Data + AI 规模化过程中遇到的关键抉择 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;那个拨云见日的「Aha Moment」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每位嘉宾将回顾自己在 2025 年经历的&amp;nbsp;3 个关键认知转折点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能是一段产品体验、一次落地尝试，或是某个业务场景中的重新理解。正是这些具体经历，推动了对 Data + AI 的判断不断修正，也构成了企业能力演进的真实轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用「年度十问」对齐关键判断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“十问 Data Strategy，AI Strategy&amp;nbsp;”环节，问题覆盖数据底座与 AI 融合架构、Agentic AI 与可信 AI、多云时代的数据治理、平台整合浪潮下的生态协同，以及工业、医疗、汽车等行业的落地实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题没有预设答案，却直指企业当下面临的核心挑战，更接近真实决策场景中的思考方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;留待未来打开的「时间胶囊」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场直播的尾声，每位嘉宾将基于当下的判断，留下&amp;nbsp;一个关于 2026 的预测或猜想。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可能并不成熟，也未必已经被验证，更像是一种站在当下时刻，对下一年走势的直觉判断。这些判断不会被立即评判对错，而是被完整地保存下来，等到 2027 年，我们会再度打开它们，回看哪些判断被现实印证，又有哪些想法在时间中发生了意料之外的转向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一刻行业领袖们的技术直觉，将成为未来回望时的重要坐标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正在思考企业 Data Strategy 与 AI Strategy 的下一步，这场对话，值得关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 19 日 17:30-19:30，我们不见不散！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66f092bc1cea165a029ae8e1592c162e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</link><guid isPermaLink="false">https://www.infoq.cn/article/1BEPfVWpbdDcstYINmuH</guid><pubDate>Wed, 14 Jan 2026 10:49:20 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>辣评 AI编程工具： 是它们不行， 还是你不会玩？｜InfoQ 2025 年度盘点与趋势洞察</title><description>&lt;p&gt;年度盘点来啦！辣评AI编程工具。&lt;br&gt;
总出 bug？隐性成本拉满？是你没摸透AI编程工具内核，还是工具自己拉胯？三位资深AI Coding 专家详解如何把 AI 用成提效外挂！&lt;/p&gt;
&lt;p&gt;本期内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;说评国内外的Al coding 工具，哪个最受开发者青睐？&lt;/li&gt;
&lt;li&gt;Vibe Coding怎么交付成果？&lt;/li&gt;
&lt;li&gt;写代码时间正在被“和Al聊天”取代？&lt;/li&gt;
&lt;li&gt;计费模式变变变，开发者怎么省钱？&lt;/li&gt;
&lt;li&gt;架构师、前端、新人，如何借Al成长？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本期嘉宾：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;马工，Al Orchestrater @ Kreditz&lt;/li&gt;
&lt;li&gt;松子，资深AI产品专家&lt;/li&gt;
&lt;li&gt;张汉东，资深独立咨询师&lt;/li&gt;
&lt;/ul&gt;
</description><link>https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</link><guid isPermaLink="false">https://www.infoq.cn/article/3D6DTWZOjd1hoHh0xqMD</guid><pubDate>Wed, 14 Jan 2026 09:16:38 GMT</pubDate><author>InfoQ 中文站</author><category>AI&amp;大模型</category></item><item><title>Zed 为什么不用自己造 Agent？OpenAI 架构师给出答案：Codex 重划 IDE × Coding Agent 的分工边界</title><description>&lt;p&gt;Coding agents（编码智能体） 已成为应用型 AI 中最活跃的领域之一，但许多团队在模型或服务商更迭时，仍不断重复构建脆弱的基础设施。那么，如何在生态不断变化的背景下保持快速迭代与高度韧性，并将更多精力投入到领域特定的工作流程和用户体验上？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为行业内的动向标杆，OpenAI的Codex提出了解决方法——“模型和Harness（工具集）的共同构建”。最近，OpenAI 的架构师 Bill Chen 和 Brian Fioca 在演讲里一起详细介绍了该构建过程中克服的挑战，以及这个Coding Agent本身一些新兴的使用模式。基于该演讲视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent的构成&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们来谈谈Coding Agent的构成。其实非常简单，一个Coding Agent由三部分组成：用户界面、模型和Harness。用户界面显而易见，可能是命令行工具，也可能是集成开发环境，或者是云端或后台Agent。模型也很直白，比如我们最近发布的GPT-5.1系列模型或其他一些供应商的模型。至于Harness，这是一个稍微复杂一点的部分，它直接与模型交互，最简化地说，可以将其看作是由一系列提示和工具组合而成的核心Agent循环，它为模型提供输入和输出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/2099b27dcfa604ecbaba2ea6937a64e4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coding 领域是应用人工智能最活跃的前沿之一，而随着新模型的不断发布，我们面临的挑战也在增加。更为复杂的是，大家不得不不断调整Agent以适应新发布的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们将聚焦于Harness的部分。Harness是模型的接口层，它是模型与用户、代码之间进行交互的媒介。它包括了模型需要的所有组件，以便在多轮对话中进行工作，调用工具，并最终为你编写代码，解读用户的需求。对一些产品来说，Harness可能是其中的关键部分。不过，构建一个高效的Harness并不是一件轻松的事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，构建Harness过程中遇到的挑战有哪些呢？首先是AV（音视频工具）问题。你可能会为Agent提供一个全新的、创新的工具，但它可能是模型之前从未见过的，它可能并不擅长使用这种工具。即使它曾经见过，你也需要花时间根据该模型的特点调整Prompt。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新模型不断发布，延迟问题也是一个挑战。模型在处理某些问题时需要时间，那么，我们应该如何设计提示，避免延迟过长？如何在用户体验上展示模型思考的过程？它在思考时是否与用户沟通，还是我们需要总结其输出结果？此外，管理上下文窗口和数据压缩也是一大难题。另外，API接口也在不断变化，现在我们有完成功能、响应功能，以及未来可能出现的其他功能，模型是否能熟练使用这些工具以便发挥最大的智能也是一个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将模型适配到Harness中需要大量的Prompt设计。实际上，模型的训练方式会带来一些副作用。我喜欢这样理解：（Steerability = Intelligence + Habit）智能加上习惯。一方面，智能是指：模型擅长什么？熟悉哪些编程语言？在某些框架中，模型能把代码写得多好？另一方面，它又养成了哪些习惯来解决问题？我们在训练模型时，培养了它在规划解决方案、查找背景信息、思考问题后再动手写代码，并在最后测试工作的习惯。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解这些习惯是成为一名优秀的Prompt工程师的关键。如果你没有按照模型熟悉的方式来指导它，可能会遇到问题。当我们发布GPT-5时，许多不习惯使用我们模型的人，尝试将其他模型的Prompt直接套用到我们的Harness中，结果发现我们的模型做的事情比其他模型要更为细致，导致了响应速度慢，效果不如预期。我们最终发现，如果让模型按照它习惯的方式进行工作，而不是过度引导，它的表现会更好。通过与模型的对话，我问它：“我喜欢这个解决方案，但它花了太长时间。下次你能做得更快吗？”模型回答说：“你让我去看所有的内容，其实我并不需要这样做，正是因为这个原因，才耗费了这么长时间。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，通过将模型与Harness一同开发，你能更好地理解它的行为，这也是Codex作为一个集成了模型和Harness的系统的优势所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Codex作为Harness/Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex被设计成一个适用于各种编程环境的Agent，它可以作为VS Code插件、CLI工具使用，甚至可以通过VS Code插件或手机上的ChatGPT在云端调用。它的功能非常基础：你可以通过提示将想法转化为可运行的代码，具备规划能力。它能在代码仓库中导航并编辑文件，执行命令和任务，你也可以从Slack或GitHub上调用它来审查PR。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着Codex的Harness需要能够完成许多复杂的任务：需要处理并行工具调用、线程合并等问题，还要考虑安全性，例如沙箱管理、提示语转发、权限设置、端口管理等。数据压缩和上下文优化的管理也非常复杂。何时触发压缩，何时重新注入数据，如何优化缓存，所有这些都是必须要解决的挑战。如果你要从零开始构建这些功能并保持其更新，工作量巨大。幸好，我们已经将这些功能集成到一个Agent系统中，它能安全地编写自己的工具来解决遇到的新问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这听起来比普通的Coding Agent强大多了，不是吗？但想想看，其实在浏览器和图形用户界面出现之前，我们操作计算机的方式不就是通过命令行界面写代码并将其串联起来吗？这意味着，如果你能将任务以命令行方式以及文件任务的形式表达出来，Codex就能知道该如何执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子，我喜欢使用Codex将我的桌面上的照片整理到一个文件夹里，这是一个非常简单的应用场景。但它还能做的不仅如此，它能够分析文件夹中大量的CSV文件，进行数据分析，这并不一定是Coding 任务，只要能够通过命令行工具来完成，Codex就能帮你做。现在我们可以看到，Codex是如此强大和有趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Codex构建自己的Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你希望将Codex集成到自己的Agent中，该如何操作呢？如果你打算创建下一个Coding 初创公司，一个关键的模式是：Harness成为新的抽象层。这个模式的好处非常明显，你不再需要在每次模型升级时都优先优化提示语和工具。但这是不是意味着你仅仅是在构建一个包装器呢？不是。正如我所说，单纯在模型上构建包装器，忽视了基础设施层的整体价值。将精力集中在让产品脱颖而出的差异化功能上，才是这种模式的核心价值所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看看一些我们与客户合作时所遇到的模式，这些模式实际上帮助他们成功构建了产品。Codex是一个SDK，你可以通过TypeScript库来调用它，也可以通过Python执行它。它还提供了一个GitHub动作，能够自动合并PR中的冲突，解决大家讨厌的合并问题。此外，你还可以将它添加到AgentSDK中，并为你的产品提供MCP连接器。这样，你就可以拥有一个Agent系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ece4a16f5c5cdcc38d4eba20b8c4fd24.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我喜欢说，我们从最初的聊天机器人开始，它们能与用户对话；然后我们为这些聊天机器人提供了使用的工具；如今，你可以为聊天机器人添加更多工具，使它能够自己生成尚未拥有的Harness。现在，你可以构建一个企业级的软件，允许它为每个客户即时编写插件连接器，这曾是专业服务团队的工作。你可以获得完全可定制的软件，且它可以与自己对话。我曾为开发日创建了一个看板，它能够自动修复自己的bug，非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，你也可以像Zed一样，将Codex嵌入到一个层级中，为IDE提供接口，使其能够与用户互动并进行代码编辑。这样，Zed就不必处理我们擅长的部分，而是可以专注于打造最好的代码编辑器。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的顶级合作伙伴，如GitHub，已经利用这些模式取得了巨大成功。我们为GitHub创建了一个SDK，允许他们直接与Codex集成。你也可以使用这个SDK将Codex作为你CI/CD管道的一部分，或者将它作为与自己Agent直接互动的工具。如果你想定制Agent层，完全可以这么做。举个例子，我们与Cursor团队紧密合作，他们将自己的Harness与我们开源的Codex CLI实现对接，成功地优化了系统性能，所有这些都是公开可用的，你可以克隆我们的代码库，随意使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex的未来是什么样的呢？它还没有发布一年，尤其是在推出Codex Max之后，变化非常迅速。它目前是增长最快的模型，每周服务数十万亿个token，这个数字从开发日以来翻了一番。我们可以合理假设，模型将变得更强大，它们能处理更长周期的任务，而且不需要监督。新模型的信任度将进一步提高，我相信这些模型已经能够处理比六个月前更复杂的工作，而且这种信任感将不断增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来将是关于庞大代码库和非标准库的时代，如何在闭源环境中工作，如何匹配现有模板和实践，模型将不断支持这些能力。SDK也将不断发展，以更好地支持这些模型的能力，使模型能够在执行任务的过程中不断学习，避免重复错误，并为写代码和使用终端解决问题的Agent提供更多支持，你将能够通过SDK在自己的产品中使用这一切。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，我们从中学到了什么呢？Harness构建非常复杂，特别是在新的模型不断发布的背景下。我们已经为你在Codex里构建了一个集成的工具，你可以直接使用它，或者查看源代码自行改进。除Coding 以外，通过它你还可以构建更多全新功能，而我们会处理确保你的计算机Agent具备最强的能力。同时，我们非常期待看到你们用它创造出的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=wVl6ZjELpBk&lt;/p&gt;</description><link>https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</link><guid isPermaLink="false">https://www.infoq.cn/article/HFewc09HcZ1IaDyFj8D0</guid><pubDate>Wed, 14 Jan 2026 08:56:01 GMT</pubDate><author>Bill Chen、 Brian Fioca</author><category>生成式 AI</category></item><item><title>不到百万级，看不见 MCP 的真实问题：创始人亲述这疯狂的一年</title><description>&lt;p&gt;一年前，MCP 还只是一个“把模型连到工具”的开源协议；一年后，它已经冲进了一个很少有协议能抵达的位置：事实标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场一年狂飙的亲历者之一——MCP 联合创作者、核心维护者 David Soria Parrra看来，最戏剧性的分水岭发生在四月前后：当 Sam Altman、Satya Nadella、Sundar Pichai 先后公开表态，Microsoft、Google、OpenAI 都将采用 MCP，“大客户”突然从 Cursor、VS Code 扩散到整个行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一年，MCP 从本地 “桌面玩具”，一路演进到远程 server、认证机制、面向企业可用的 OAuth 重构，再到 11 月引入 long-running tasks，把深度研究、甚至 agent-to-agent 交互变成协议的一等公民。David 的总结很直接：“这一年真的非常疯狂。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段对谈里，David 也很坦率地复盘了 MCP 这一年的取舍：做对的，是死磕标准 HTTP；踩坑的，是把关键能力做成了‘可选项’，结果客户端大多不实现，双向能力被削掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更现实的问题是扩展性：规模一上来，多实例、多 Pod 下同一段交互可能打到不同机器，不得不用 Redis 之类的共享存储来“拼状态”，请求量到百万级就开始吃力：“当规模一上来，这件事一点都不好玩。”“一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客内容整理，略有删节：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 的一年：从发布到行业事实标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：要不你先简单讲讲 MCP 的发展情况，以及之前为什么决定把它捐赠给基金会？接下来我们再系统回顾 MCP 这一年的演进，然后再请基金会的其他负责人加入，聊一些更宏观的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David Soria Parrra（MCP Co-creator）：如果回到一年前，MCP 刚发布的时候，其实谁都没想到它会在这一年里迎来如此疯狂的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老实说，这一年感觉像过了一个世纪。一开始是在感恩节和圣诞节前后，很多开发者开始自发地用 MCP 搭东西。随后，像 Cursor、VS Code 这样的“大客户”开始出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的拐点出现在四月左右——当时 Sam Altman、Satya Nadella、Sundar Pichai 等人陆续公开表示，Microsoft、Google、OpenAI 都会采用 MCP。那是一个非常明显的“分水岭”。&lt;/p&gt;&lt;p&gt;与此同时，我们也一直在推进协议本身的演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最初，MCP 几乎只支持本地使用：你在桌面上跑一个 MCP server，通过本地 stdio 和客户端通信。但到了今年三月，我们开始推进“远程 MCP server”——也就是如何通过网络连接 MCP，并且第一次引入了认证机制。到了六月，我们又对这套认证方案进行了比较大的修订，尤其是为了让它真正适用于企业场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们非常幸运，在三月到六月这段时间里，有真正做 OAuth 标准的行业专家，直接参与进来，帮我们把一些关键细节“拉正”。我们也在这段时间里大量投入在安全最佳实践上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了 11 月底，我们发布了新一轮重要版本，引入了 长时间运行任务（long-running tasks） 这一关键原语，用来支持深度研究类任务，甚至是 agent-to-agent 的交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的感觉是：MCP 的基础已经非常扎实了。接下来还有一两个关键原语和可扩展性问题需要解决，然后协议整体会进入一个相对稳定的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，这一年真的非常疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你刚刚提到 agent-to-agent，那是不是也涉及 A2A 协议？在 Agentic AI Foundation 成立时，有没有讨论过把其他协议也纳入进来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，这几乎是必然会发生的讨论。我们当然讨论过市场上其他协议，比如一些支付协议之类的东西。但在决定成立基金会时，我们有两个非常明确的原则：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，我们想 从小开始。这是 Anthropic 第一次参与开放源代码基金会，一切都是新的。我们希望先在一个相对可控的范围内学习如何把这件事做好，并且和 OpenAI、Block 一起，把基金会的节奏掌控住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，在协议层面，我们非常在意“事实标准（de facto standard）”。目前来看，真正已经具备广泛采用度的协议，只有 MCP。其他协议还没有“走到那一步”。当然，如果未来某个协议发展到那个阶段，并且在功能上是互补的，我们是完全开放的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在应用层，我们会更灵活；但在协议层，我们不希望一个基金会里同时维护五个做同一件事的通信协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在在基金会和 MCP 之间，是不是有点“戴两顶帽子”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：确实如此，但我主要精力仍然在 MCP 上。基金会本质上是一个“保护伞”，它最重要的作用是保证项目的中立性。至于基金会预算怎么用、办什么活动，这些相对来说反而是“比较枯燥”的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 MCP 的技术治理上，其实并没有发生本质变化。我依然是核心维护者，继续推动协议演进。&lt;/p&gt;&lt;p&gt;另外，我也会参与基金会的技术指导委员会（TSC），负责判断：哪些项目适合进入基金会？它们是否被良好维护？是否有真实采用？是否具备长期价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不希望基金会变成一个“项目垃圾场”。我知道有些基金会最终会落得什么下场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这一年 MCP 发布了四次规范更新，节奏非常快。尤其是三月和五月那次，引入了 HTTP Streaming 和认证。要不要给大家系统梳理一下？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：HTTP Streaming 那次更新非常关键，也是用户呼声最高的一次。我们在 11、12 月就已经意识到：下一步一定是远程 MCP，而远程就绕不开认证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP 的一个特点是：它在每一层都非常“有主见（prescriptive）”。比如，在客户端和服务端互不认识的情况下，认证该怎么做，我们希望只有“一种正确方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;三月版本里，我们做了一版认证方案。现在回头看，它“还行”，但确实有问题。说白了，是我对企业认证场景理解不够。MCP 的一个核心优势，是它的社区：当我不懂的时候，会有真正懂的人站出来帮我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：三月那版认证，主要问题出在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 里有两个核心角色：&lt;/p&gt;&lt;p&gt;身份提供方（Authorization Server / IdP）：发放 token资源服务器（Resource Server）：接收 token 并给相应的资源作为回报&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在第一版 MCP 认证规范里，我们把这两个角色合并进了 MCP server。对于创业公司来说，这没问题：你自己有账号体系，把 MCP server 直接绑在用户账号上，完全可用。但在企业环境里，这根本行不通。企业几乎总是有一个中央身份系统（比如 Google 登录、企业 SSO），用户每天早上只感知到“我登录了一次”，但背后其实是 IdP 在工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在六月的规范中，我们做了一个关键调整：明确把 MCP server 定义为资源服务器，和身份系统解耦。我们对“怎么拿 token”依然有建议，但不再强行绑定在 MCP server 里。同时，也补齐了动态客户端注册等细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 agent 代表用户去操作，比如帮我用 Linear、Slack，这个问题现在解决了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：OAuth 本身是一个非常“以人为中心”的协议。它只定义：如果你没有 token，该怎么拿 token。一旦你有 token，后面就只是把它放进 Bearer Token 里而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们目前并没有对 agent-to-agent 或 agent 代表用户的认证方式做强约束。在企业内网、封闭环境里，大家已经可以通过 workload identity 等方式做到。但如果客户端和服务端彼此不认识，我们目前还没有一个“完美方案”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们从本地服务器（比如基于 stdio 的方案），一路演进到可流式的 HTTP。在这个过程中，有哪些经验教训值得分享？有没有什么后悔的地方，或者对其他人有什么建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：关于传输层这件事，其实有一个讨论，从过去几年一开始就从未停过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在最近两天，我们还在 Google 的办公室里，和一群来自 Google、Microsoft、AWS、Anthropic、OpenAI 的资深工程师坐在一起，专门讨论：到底需要做什么，才能把这件事真正、彻底地打牢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回到今年三月，当时我们希望引入一种新的传输方式，它能够尽量保留我们在标准 IO（stdio）里拥有的很多特性。因为我们当时——而且直到今天我依然坚信——MCP 不只是为了简单的请求-响应，它还应该支持 Agent。而 Agent 天生就是某种程度上“有状态”的，它需要在客户端和服务器之间进行一种长期存在的通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我们一直在寻找一种具备这些特性的方案。我们当然也研究过一些替代方案，比如 WebSocket。但在实践中，我们发现，要真正把一个可靠的双向流（bidirectional stream）做好，其实会遇到很多问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们就在思考：有没有一种“中间态”？这种中间态需要满足两个条件：一方面，它要足够简单，适合那些最基础的使用场景——比如用户只是想提供一个工具；另一方面，它又必须能够在需要的时候，升级成一个完整的双向流，因为你可能真的会遇到那种复杂的 Agent 之间相互通信的场景。正是在这样的背景下，可流式 HTTP（streamable HTTP）诞生了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事后回看，我觉得我们有些地方做对了，也有些地方做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做对的地方在于：我们非常坚定地只依赖标准 HTTP。但做错的地方在于：我们让太多事情对客户端来说是“可选的”。比如，客户端可以连接服务器，并打开一个从服务器返回的流，但它并不是必须这么做。而现实情况是——几乎没有客户端会这么做，因为这是可选的。结果就是，很多双向能力实际上被“抹掉”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，一些功能，比如 elicitation（征询） 和 sampling（采样），对服务器来说就变得不可用。原因很简单：服务器没有一个打开的返回流；而客户端在实现时会想，“这已经满足我产品的最小可用版本（MVP）了，我没必要再多做这些。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这最终成了一个问题。我觉得这是一个非常明确的教训。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个教训来自于协议设计本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们设计的这套传输协议，要求服务器端持有一定的状态。如果你只有一台服务器，这当然没问题。但一旦你要做水平扩展——比如跑在多个 Pod、多个容器里——问题就来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;设想这样一个流程：一次 tool call，然后是一次 elicitation，再接着是 elicitation 的结果返回。很可能，这几个请求会打到不同的服务器实例上。那你就必须想办法，让这几台服务器把这些信息“拼”在一起。现实中，这往往意味着你需要某种共享状态机制：Redis、Memcached，或者别的什么共享存储，总之你需要一个地方，能够让这些服务器共享状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，这当然是可行的。我们在 PHP 应用、Python 应用里早就见过类似的模式。但说实话，当规模一上来，这件事一点都不好玩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们也知道，一些公司——比如 Google、Microsoft——他们在用 MCP 的时候，规模已经大到我不能公开具体数字，但可以说是百万级请求。到了这个量级，这就真的成了一个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们现在坐在这里，不断地问自己：如何在协议的下一次演进中，做到这几件事？&lt;/p&gt;&lt;p&gt;对简单的 MCP Server 来说，仍然尽可能简单；在需要的时候，允许完整的双向流；同时，还要具备良好的可扩展性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，我们正在逐步找到正确的解法，但这件事本身真的很复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为今天的大多数技术选择，其实都非常极端：要么你做一个很简单的东西，比如 REST；要么你直接上“全双工”的方案，比如 WebSocket、gRPC。而我们需要的，其实是两者同时存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在巨头之间“做标准”是什么体验？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：和这么多顶级公司一起做标准，是什么感觉？在那样的场合，大家都是资深人士，每个人都有自己的观点。谁来做最终决定？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：真的太有意思了。我能和业内最顶级的工程师一起工作。通常我们的目标是尽量达成共识。现实情况是，从技术角度讲，最终拍板的人是我，但说实话，这更多是一种形式上的存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正重要的事情在于：我们努力把讨论不断收敛，明确哪些是真正大家都认可的问题，哪些是暂时还存在分歧的问题，然后在这些边界之内，去构建我们能做到的最佳解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个过程需要时间，需要大量迭代，但说真的，这件事本身非常有意思。因为你能看到来自不同公司的、非常独特的问题形态。你甚至能从问题本身，看出一家公司的“性格”——比如 Google 面临的问题和 Microsoft 就完全不同，而这些差异，很大程度上来自他们各自构建系统的方式。同样，Anthropic 的问题看起来也和 OpenAI 的问题不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我最喜欢的一点在于：有时候你会突然意识到，自己正坐在一个房间里，周围全是彼此竞争的公司，但大家却在一起构建同一件东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在开源世界已经待了大概 25 年了，我真的非常热爱这种状态。当一个标准真正运转起来时，这就是理想状态。而且这些人都非常优秀，我从每一位同行身上都学到了很多。所以我非常感激，自己能处在这样的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来有点像 IETF 的标准制定流程？你们有没有讨论过，这种“私下的小圈子”运作方式，和更传统的标准组织之间的差异？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这是个很有意思的问题。某种程度上，它确实有点像 IETF，但也有明显不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IETF 是一个完全开放的论坛，任何人都可以参与。它的结果是——不是因为刻意如此，而是“偶然地”——整个流程非常依赖共识，因此速度也相对较慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这种慢，在很多方面其实是优点。因为一旦标准定下来，基本上是不可逆的。比如你看看 OS 2.1 规范，它已经制定了三四年，到现在都还没完全结束。这就是 IETF 标准化的节奏：这些事情本来就会花非常非常长的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为这对某些领域是好事，但在 AI 领域，目前的变化实在太快了，你几乎被迫要选择一个更小的核心群体。因此我们选择把 MCP 运作成一个非常传统的开源项目：有一个大约 8 人的核心维护者小组，基本上由他们来做最终决策；其他人可以提供输入、提出建议，而且很多变更并不是来自核心维护者，但决定权在他们手里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一种折中方案：一部分是共识驱动，一部分则是带有一点“技术独裁”的意味。如果你想要快速前进，这种模式在当前阶段对 MCP 来说是合理的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你们是如何平衡模型能力演进与协议设计之间的关系的？毕竟 Anthropic 和 OpenAI 都在做大量后训练（post-training），让模型更擅长工具调用；这会不会影响你们对协议形态的偏好？反过来，协议是否也会反向影响模型训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：老实说，我不敢说自己对研究侧的所有事情都 100% 熟悉——我更多是产品背景。但从我了解的情况来看，协议确实会在一定程度上影响后训练，比如我们在模型卡中会使用 MCP Atlas，确保模型在面对真实世界中大量存在的工具时，能正常工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从另一个角度讲，协议的底层原语，其实很少直接被模型能力的提升所驱动。我们更像是在预期模型能力将会呈指数级增长，因此在协议中，依赖了一些你可以通过训练不断强化的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个更具体的例子。很多人都讨论过 MCP Server 的上下文构建问题。因为 MCP 打开了通往大量工具的大门，如果你天真地把所有工具一次性塞进上下文窗口，那只会造成严重的膨胀。&lt;/p&gt;&lt;p&gt;这就好比把所有技能、所有 Markdown 文件一次性丢进上下文里，结果当然会一团糟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们其实从一开始就知道，可以采用一种叫做渐进式发现（progressive discovery）的方式：先给模型一小部分信息，让模型在需要的时候，再主动请求更多信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这本质上是一个通用原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这里正是我们这些“大模型公司”具备的一点前瞻性所在——我们知道，如果愿意，是完全可以通过训练，把这种能力系统性地强化出来的。模型在原理上已经能做到这些事情了，训练只是让它做得更好。任何支持工具调用的模型，都可以做到这一点；只是如果你专门为此训练过，它的表现会更好。所以在这个层面上，协议设计和模型训练是相互配合的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但归根结底，渐进式发现这种机制，本身就内生于任何具备工具调用能力的模型之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也引出了“上下文腐烂（context rot）”的问题。还有 MCP 和所谓 “code mode” 的讨论——比如有人会说，“Anthropic 提倡 code mode，而 MCP 又是 Anthropic 做的，那是不是说明 code mode 才是正确方向？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：首先澄清一下，官方博客其实从来没用过 “code mode” 这个词，那是大家后来叫出来的。我们内部更常说的是 “programmatic MCP”，但本质上讨论的是同一件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键在于：MCP 是应用和服务器之间的协议，模型本身在技术上并不直接参与 MCP。所以问题其实变成了：应用拿到一堆工具之后，该怎么用？你可以用最朴素的方式：把工具直接暴露给模型，让模型逐个调用。但你也可以更“创造性”一点：模型非常擅长写代码，那如果我们把这些工具当成 API，交给模型生成一段代码，让它提前把多个调用组合好，再在一个 sandbox 里执行呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本质上，模型原本就会做这样的组合：调用 A → 拿结果 → 回到推理 → 调用 B → 再组合成 C。你只是让模型提前优化了这个过程，把它编译成一段可执行代码而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 MCP 的价值并没有因此消失：&lt;/p&gt;&lt;p&gt;认证（authentication）仍然由 MCP 处理；接口是为语言模型设计的；工具是可发现的、自文档化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力依然存在。你只是换了一种使用方式而已。所以当有人说，“那 MCP 是不是就没用了？”我其实挺困惑的。它不是没用，而是被用在了不同的层次上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着模型和基础设施逐渐成熟——比如你可以默认 AI 应用都有 sandbox 执行环境——你确实可以玩出更多有意思的花样。但这并不意味着，一个把模型连接到外部世界的协议就失去了价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人更愿意把这种变化，看作一种优化，说得直白一点，就是 token 级别的优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MCP 有没有竞争对手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这正好可以引出 skills。skills 是一个相对较新的概念。我之所以提到它，是因为在我脑子里，它和渐进式发现、预置代码脚本这些概念是连在一起的。而且 skills 还能生成 skills，本身就很有意思。很多人试图把 MCP 和 skills 放在对立面来比较，显然它们并不重叠，但你是怎么看待这个问题的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我同意。我觉得有意思的点就在于：它们并不重叠。它们解决的是不同的问题。&lt;/p&gt;&lt;p&gt;我觉得 skills 非常棒，而且你知道的，我认为 skills 最核心的出发点之一，就是渐进式发现（progressive discovery）这个原则。但我也认为，“渐进式发现”这种机制，其实是通用于你能用模型做的几乎任何事情的——它不是 skills 独有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 skills 到底提供什么？它提供的是某一类任务的领域知识（domain knowledge）：比如你应该如何做事、如何表现，模型应该如何扮演一个数据科学家，或者如何扮演一个会计之类的角色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 MCP 提供的，是你能对外部世界采取的真实动作的连接性（connectiveness）——也就是你能执行哪些实际操作、如何把这些操作真正连到外部系统上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为它们在某种意义上是正交的（orthogonal）：skills 给你的是更“纵向”的能力——偏领域、偏角色、偏方法论；而 MCP 给你的是更“横向”的能力——偏连接、偏动作、偏“给我那个具体操作”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，skills 也可以执行动作。它能执行动作，是因为你可以在里面放代码和脚本，这当然很棒。但这里有两个关键点，我觉得很多人容易忽略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，你需要一个执行环境（execution environment）——也就是你需要一台机器来跑这些代码。是的，你需要“机器”。这在很多场景下完全没问题：比如你在本地跑一个东西（像 Cloud Code 之类），那我们就可以讨论 CLI；在这种你确实拥有执行环境的场景里，这套方式就非常合理，也很好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;或者，如果你有一个远程执行环境，那同样也说得通。但即便如此，你在这条路径上仍然得不到认证（authentication）这一块能力。所以我认为 MCP 带来的关键价值之一，就是它把认证这件事补齐了——这是 skills 本身不提供的那部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个点是：你不必去处理“外部方的持续变化”。举个例子，如果你接的是一个 Linear 的 MCP server，那么对方可以持续改进它，而你不需要在自己的 skill 里去处理这些变化——它不是被“固定在某个时间点”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三个点是：你其实不一定需要一个本地的执行环境，因为执行环境在某种意义上是“在别处”的——它在服务器端。也就是说，执行发生在 MCP server 那边。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，如果你在构建的是一个 Web 应用，或者一个移动应用，这些特性在某些方面会更契合、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以整体来看，我认为它们大多数时候都是正交的。并且我确实看到过一些很酷的落地方式：人们用 skills 去探索不同的功能、不同的角色（比如会计、工程师、数据科学家），然后再用 MCP servers 把这些 skills 连接到公司内部真正的数据源上。我觉得这是一个非常有趣的模型，也最接近我理解和看待它们关系的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以 MCP 是连接层？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我会说是通信层。是的，通信层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从架构上讲我很好奇：MCP client 是放在每个 skill 里面，还是大家共享一个 client？比如共享 client 还能发现 skills 之类的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们是共享的方式。我觉得从技术上你确实更想走“共享更多”的方向——共享越多，你能做的事情就越多：比如做 discovery（发现）、做连接池（connection pooling）、做自动发现，甚至你可以让 skill 只用很“松散”的方式描述它想要什么，然后系统去你有权限访问的 registry 里帮你找一个合适的 MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些能力只有在 shared 的架构里更容易做出来。当然，最终两种方式都能工作，只是这仍然是一个值得继续实验的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 怎么用 MCP？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我想强调一下，可能很多人都没意识到——你刚才一直说“我们怎么做怎么做”，但实际上我觉得外界并不理解 Anthropic 内部到底 有多大规模地在 dogfood MCP。我也是看了 John Welsh 的演讲才真正理解，他说：“我们有一个 MCP gateway，一切都要走这个 gateway。”你能多讲讲这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：当然。我们内部两种都用：skills 用得很多，MCP servers 也用得很多。因为你要让大家很容易部署 MCP，你需要和公司内部的 IdP（身份系统）打通之类的东西。所以我们为自己定制开发了一个 gateway。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你只需要把 MCP server 部署起来，剩下的都是内部应用、内部系统在用。有些东西“技术上”算外部系统，但因为它们没有提供第一方 MCP server，我们就自己做了。比如我们有一个 Slack 的 MCP server——我特别爱用。它可以让 Claude 帮我总结 Slack。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们内部还有很多类似的用法：例如我们每半年（或者一年两次）会做一次员工调查，问大家对公司、对未来、对 AI、对安全等议题的感受。我们也有一个 MCP server 支持这件事，然后你可以围绕结果问很多问题，这非常有趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这些都是你们团队维护的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不是。我们维护的是 gateway。但有意思的地方在于：MCP 从一开始的想法就是——在我们开源之前，它源自一个很现实的困境：公司增长太快了。我在研发工具、开发者工具这一侧，增长速度一定跟不上业务扩张。那我怎么做一个东西，让大家能“自己为自己构建工具”？&lt;/p&gt;&lt;p&gt;这就是 MCP 的起源故事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以你现在回头看，一年之后发生的事情，正好就是我们当初想要的：大家真的在为自己构建 MCP servers。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我甚至可能完全不知道 Anthropic 内部 90% 的 MCP servers，因为它们可能在研究团队里，我看不到；或者人们就是自己做给自己用，我也不会被同步到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那它们是自己 host 吗？还是有远程托管？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：基本上大家只需要一条命令启动，它就会在一个 Kubernetes 集群里跑起来。算是“半托管”的形态。对任何大公司来说，这类平台基础设施都很重要。外部也有一些平台会帮你做这件事，但从安全角度，我们倾向于自己做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过外界也有类似的产品。比如有人做了一个叫 fast MCP 的东西——Jeremiah 他们做的 fast MCP cloud，有点像这样：两条命令，你就能跑起一个 MCP server 实例，支持 HTTP 流式传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多企业还会用类似 LiteLLM 这样的东西做 gateway：你甚至可以启动标准 IO 的 server，把它接到 gateway 上，然后由 gateway 来处理认证等“所有麻烦的部分”。所以落地路径其实很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为你真正想要的“理想基础设施”是：让部署变得极其琐碎、极其简单——比如“一条命令”启动一个原本只是 stdio 的 MCP server，然后它瞬间变成一个带有 HTTP streaming、并且集成了认证的远程 MCP server。最终开发者只需要做“标准部分”，其他复杂部分都由平台替你完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你把这个点讲出来，因为很多人会直接把这套思路拿回公司里落地。否则替代方案就是：混乱、重复造轮子、各自重建一遍。顺便 shout out Jeremiah——我还邀请他来我在纽约的峰会做一个 fast MCP 的 workshop。他写过一篇很棒的博客，说我们看到的 MCP 使用，很大一部分其实都发生在企业内部。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，我们也观察到同样的现象：在大型企业内部，你几乎到处都能看到 MCP。它的增长速度，比你想象得快得多——因为它多数都在企业内部发生，外界根本看不见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Registry怎么演化？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说到 discovery，你们推出了官方 registry。然后又出现了各种 registry 公司、gateway 公司。现在官方 registry 里甚至出现了“自动把自己的 MCP server 放进官方 registry”的子 registry。你们是不是需要更多 registry？你从推出 registry 这件事上学到了什么？你觉得未来会怎么演化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们看到很多不同的 registry 冒出来。我们一直觉得，生态确实需要一种类似 npm / PyPI（MPM） 的模式：有一个更中心化的地方，任何人都可以把 MCP server 发布上去。&lt;/p&gt;&lt;p&gt;这就是官方 registry 最初的出发点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们同时也想推动：至少整个生态要有一个共同的标准，让不同 registry 之间能“说同一种语言”。因为我们真正想实现的世界是：模型可以从 registry 里自动选择一个 MCP server，安装它，用在当前任务上——像魔法一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要做到这一点，你需要一个标准化接口。我们很早就开始和 GitHub 团队合作（大概四月份），但后来我被别的事情分走了注意力，比如认证，去集中解决那块了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我希望看到的方向是：未来会有一个“官方 registry”，任何人都可以往里放 MCP server。它的角色就像 npm ——而 npm 也有完全相同的问题：任何人都能发布，你并不知道该信谁、不该信谁；会有供应链攻击。这是公共 registry 的基本属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们才提出了 子 registry（sub-registries） 的概念：像 Smithery 这类服务可以在官方 registry 之上做过滤、做精选、做策展（curate）。我们希望生态最终能形成这样的结构。&lt;/p&gt;&lt;p&gt;我们现在还没完全到那个状态，但正在往那个方向走。比如 GitHub 的 registry 是“策展式”的，同时它和官方 registry 讲的是同一种格式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终我们想要的是：作为一家企业，你可以有一个内部 registry——它基于官方 registry 的镜像，再加上你自己的私有 MCP servers；它是你信任的来源，同时它暴露的 API 和官方 registry 一样。这样无论是 VS Code 还是其他客户端，只要指向你的内部 registry，就可以顺畅工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这很有意思，因为 npm 在某种意义上更像一个“下载网关”。我其实不太会去 npm 做发现，我更多是在别处看到包，然后再去 npm 安装。你觉得 registry 的核心是 discovery 吗？还是 agent 会用别的方式完成发现？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我认为 discovery 在模型世界里会更重要。这里和 npm 的差别在于：&lt;/p&gt;&lt;p&gt;我们是在做一个 AI-first 的东西，我们可以假设：有一个聪明的模型，它“知道自己想要什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在过去是不存在的。如果你今天重新设计现代包管理系统，并且把模型当作核心，你可能会做出类似的交互：“这是我想做的事，你自己决定装哪些包，我不在乎，反正把事情做成就行。”这就是它的类比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但再次强调：公共 registry 不应该直接让模型这么做，因为公共 registry 很容易变成一个“垃圾场”。你应该在一个可信、被策展过的 registry 上做这种自动化选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢你那句话——模型知道自己想要什么。因为现在很多人都有一个梦想：agent 能用 MCP 目录去发现新的 server，自己安装自己使用。这听起来非常 AGI。如果真能跑通当然很牛，但也可能跑不通。要做到这一点，到底需要什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我觉得需要两件事：&lt;/p&gt;&lt;p&gt;第一，你需要一个好的 registry 接口。&lt;/p&gt;&lt;p&gt;第二，你需要真的去为这个目标做工程、做实验，看看什么可行、什么不可行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你肯定需要信任等级（trust levels）。你可能还需要签名（signatures）。我有一个想法——不确定会不会真的做——比如：你可以附带来自不同模型提供商的签名，表示他们扫描过这个 MCP server，并且愿意为它背书：&lt;/p&gt;&lt;p&gt;“Anthropic 的签名：这些 tool descriptions 是安全的”“OpenAI 的签名：我们认为这些是可信的”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后你就可以基于这些签名自行决策。这有点像分布式代码签名——不过也不完全分布式，本质上可能还是中心化的。但我认为这是你最终会需要的一类机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过最先跑通的场景，可能反而是企业内部：企业会用私有 registry，本身就带有隐含信任。就像他们今天已经在用私有 npm / 私有 PyPI 一样，他们也会用私有 MCP registry。在这种环境里，你天然有 trust，然后就可以开始做搜索和自动选择。我们自己其实就有内部 registry：当你通过 John 那套基础设施启动一个 MCP server，它就会被注册进去。所以我们也需要在内部继续做实验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Sampling：理想很美，但客户端不配合&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你今年在伦敦办了一些活动，你看到什么好的 sampling 用例了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：还没有特别多。我从 sampling 这件事学到的一点是：人们想在 sampling 的过程中使用一些“只在 sampling 时出现”的工具——这些工具并不是 MCP server 暴露出来的那套工具。但我们之前没有能力做到这一点。在这次迭代里我们刚修复了这个问题，所以我们希望未来能看到更多 sampling 用例。偶尔会有一些 MCP server 在用 sampling，但不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其是当 MCP servers 从“本地为主”走向“远程为主”，在远程场景里，通常更好的选择可能是直接提供 SDK：你完全控制它、自己部署，甚至还可以收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在本地场景里，sampling 的价值更大：因为你是在给很多人分发一个东西，你并不知道他们用的是哪个模型、哪个应用（可能是 VS Code，也可能是 Claude Desktop），这种情况下 sampling 才更有意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在的问题是：客户端基本都不支持 sampling。所以 sampling 这件事让我挺沮丧的——我仍然觉得这是个很强的想法，但你知道的，有时候你总得赢一些、也得输一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你们也在升级它，我还是很期待。有点奇怪——如果采样这件事做对了，它某种意义上会变成真正的 agent-to-agent 协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你看到的大多数用例还是偏“数据消费”吗？我自己的 MCP 用法也 mostly 是拿上下文、拿数据。最多的 action 可能就是更新一下 Linear 任务状态。你见过很复杂的“用 MCP 做动作的工作流”吗？还是大家基本都在用它做上下文？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：大多数人确实是用它做上下文，这占了绝大多数。毕竟它的名字就叫 Model Context（模型上下文）。顺便说一句，OpenAI 的 Nick Cooper 经常跟我说——而且他说得对——MCP 这个名字可能取错了，它确实会让人感觉用途被“限制”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我看到的主要还是数据用例。也有人把它用于 deep research，一些更复杂的 agent 暴露出来，但并不普遍。deep research 这种自定义研究用例不算罕见，但除此之外，大多数还是数据、以及围绕数据的深度研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在你还会看到一个新方向：通过 MCP UI（未来我们可能叫 MCP Apps / MCPI）暴露 UI 组件。我觉得这非常有前景，也非常有意思。现在在一些 chat apps 里已经能看到不少类似实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Tasks：为长时间、异步 agent 操作而生的新原语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇，因为如果大多数用例是“上下文”，你们做 tasks 这个原语，就好像大家暂时还没怎么用它。你们设计 tasks 的出发点是什么？你期待它怎么被用起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们做 tasks，是因为很多人来找我们说：“我们真的需要长时间运行的操作——也就是 agents。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想要那种“深度研究任务”，可能一小时才完成；甚至可能一天都跑不完。过去人们会很别扭地用 tools 去实现这类事情——工具本质上就是 RPC 接口，理论上你能凑出来，但很快就会变得别扭：模型需要理解“我得去轮询、我得去拉取”，体验很差，也不是一等公民（first-class primitive），限制很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这类诉求太普遍了：大家都想要长时间运行的 agents。GitHub issue 里，大公司也一直在说“我们需要 long-running operations”。所以我们觉得必须做点什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在 tasks 刚刚落地到 SDK，还需要落地到客户端，然后我们才会看到更广泛的使用。但我非常确信：自定义研究类任务会大量用上它，其他场景也会逐步跟进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我对 tasks 非常看好。我觉得任何编排系统或协议都得有 sync 版本和 async 版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：完全同意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 tasks 的设计上，有没有哪些重要分岔点？比如本来有两条路，你们选了其中一条。&lt;/p&gt;&lt;p&gt;David：讨论非常多。有人提议：tasks 其实就是“异步 tools”，做成一个新的 tool primitive 就行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但对我来说，我的 试金石（litmus test） 一直是：如果未来我想把 Claude Code 或任何 coding agent 当作一个 MCP server 暴露出来，那么 tasks 必须能支撑这种形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;纯粹的异步工具调用做不到这一点。你需要一种操作方式：它能够在长时间运行的过程中返回中间结果。理想状态下，你会想暴露这样的东西：“我通过调用这个工具、那个工具、还有那个输入，得到中间产物……最后得到结果。”这才是你希望一个长任务能够表达的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;tasks 现在还没完全做到这一步，但它的设计是“足够通用”的，未来可以支持这种更丰富的表达——这就是最核心的约束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个关键约束是：我们不希望 tasks 成为 tools 的复制品 ——只是语义稍有不同。我们希望它是一个更抽象的概念：你通过一次带元数据的 tool call 来创建一个 task，然后系统自动创建并管理这个 task。所以 task 更像一个“容器（container）”：它描述了一段从开始到结束的异步过程，而我们当前用 tool call 作为触发方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样的抽象会打开很多未来可能性。所以我觉得，真正的设计目的是让实现变得更抽象。（虽然）实现起来很复杂，但也最终被解决了，因为复杂性会被 SDK 吞掉：SDK 会帮你实现细节，在开发者视角里，它就是一个 async 调用，然后返回结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来会和很多异步 RPC 框架有点重叠，比如 JS 世界的 tRPC、或者各种 protobuf 体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。从接口风格来说，它很像经典的操作系统接口：你创建一个 task，然后不断 pull（轮询）直到它完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后我们下一轮会做一个优化——这次没来得及做：你不用每隔几分钟/几小时去 pull，server 可以回调你（发事件、webhook 之类的）告诉你“我完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是优化，但核心接口始终是：客户端可以 pull。这也很像操作系统里的一些文件系统操作：客户端轮询是一种最通用、最可靠的基线能力……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以一直 pull（轮询）：文件变了吗、文件变了吗……但你也可以用现代一些的内核接口，比如 inotify 之类的通知机制，或者 io_uring 之类的方式，它会告诉你：哦，我完成了——很好，文件变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我学到一个“骚操作”——server 可以一直把 HTTP 连接挂着，等它做完了再断开；连接断开本身就成了一个信号，告诉后端“完成了”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，但我们不一定想这么做。因为它可能要跑几天，我也不知道别人会怎么处理这种连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这其实挺不负责任的，但确实很酷。老实说，tasks 真的很有意思——我们在做 Devin API、以及 Cognition 那些东西时，也基本被迫“重新发明”过类似机制。这也很有代表性：每个人最终都会需要某种 long-running operation。而当你在调用一个 agent 时，你同样需要这个能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的。但对我们来说，有一个有意思的点是：MCP 一直在做的事情，是把大家“此刻正在尝试做的东西”封装起来；我们并不想强行规定一年后大家“应该怎么做”。因为我们不知道，我们不预测未来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做 tasks，是因为大家说：我们现在就需要它。实际上我们六个月前就需要它了。于是我们说，好吧，那现在就是动手的时候了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不想做那种“预测未来”的协议，所以才努力让协议保持相对最小化。虽然也有人会觉得：现在协议里的 primitive 已经太多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;超长任务与上下文压缩&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：一个小问题。假设是超级长的任务，过程中会来回传很多消息。Anthropic 在上下文压缩（或者叫 compaction）这件事上算是领先者之一，其他实验室也在做类似事情。那这种场景怎么处理？我们是不是就无状态地把上下文截断也没关系？你需要保留“全过程完整日志”吗？还是说删掉就删掉了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不需要。你看，我们现在这个行业还是非常早期，我们一直在学习：模型到底需要什么、不需要什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;甚至到今天，有些 agent 已经开始在跑了几轮之后丢弃 tool call 的结果，因为它不再需要了。我觉得这非常好。所以除了 compaction 之外，我觉得你还会看到更好的机制：更清楚地理解“该保留什么、不该保留什么”。比如对一个长时间异步任务，你可能会这样：某段时间模型确实需要看到全部过程，但当你拿到最终结果之后，你就把其他东西都丢掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你甚至可以调用一个更小的模型——比如 Haiku ——让它来判断：这些内容里哪些该保留？告诉我。也可能最“AGI build”的方式就是：让模型自己决定它需要保留什么。所以你会看到两种世界并存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在还没有唯一答案，因为大家仍在摸索。compaction 是一个很好的阶段性方法，但它也不会是最后一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，如果你更认真地思考：你能训练模型在这里做什么，我觉得会有更好的方式。但这些都和“你如何获取上下文”是相互独立的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直把 MCP 看作一个 应用层协议：它只负责“你如何获得上下文”。至于“你如何选择上下文”，那是应用层问题——所有 agent 应用最终都会面对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来会有很多技术路径。一年前所有人都会说：RAG 才是答案；现在大家又说 RAG 好像“死了”。我们开始用模型、用 compaction。至于一年后会怎样，我也不知道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我还有个问题：你怎么看 MCP servers 在未来的定位——它们是给开发者用来构建 AI 应用的？还是一个面向 AI 消费者、让他们把各种服务“插上就能用”的协议？我觉得很多人会把它理解错：他们说“我有 REST API，为什么还需要 MCP？”在我看来，MCP 可能并不是“给开发者用的”，而是给使用 AI 工具的人，用来把东西插进去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我经常被拿来和 REST API 比。这个对比挺有意思的，因为这里其实有两个问题：第一，REST 并不告诉你认证该怎么做。第二，你们已经在跟我抱怨 “tool bloat（工具膨胀）” 了，但你们有没有看过平均一个 OpenAPI spec 有多长？你把那个塞进模型里，膨胀只会更严重——实际上会糟得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有意思的是，当人们尝试一比一映射时，模型经常会有点迷糊：你会有“按名字搜索、按 ID 搜索、按某字段搜索”等等，突然冒出五个长得很像的工具，模型就会问：你到底要用哪个？我也不知道了。所以这是个关于 REST vs MCP 的小插曲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我确实希望 MCP 生活在一个更“消费者导向”的世界：这是使用者应该知道的能力。我想要的世界是：你打开应用，直接说“做这件事”，它就把事情做完——它在底下自动连到合适的服务。MCP 是幕后细节；开发者需要知道它，因为这是通信通道；但对最终用户来说，你只需要拿到结果、把任务完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;坦白讲，我更喜欢一个世界：没人需要知道 MCP 是什么。比如我妈如果要用 Claude，她不应该知道 MCP 是啥。但我认为 MCP 的重点确实是：让外部服务“可插拔”。在这个意义上，它更偏消费者侧。当然开发者也有用例：他们作为 builder 要构建这些东西；而且我也仍然很爱我的 Playwright MCP server。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很好奇你说的 MCP Apps / UI。现在每个客户端——比如 ChatGPT——都有自己的一套渲染方式。所以如果我习惯了某个产品的 MCP app，但换到另一个地方，它可能就是另一个版本、另一种策展方式，体验会很不一样。我想知道你怎么看：尤其现在 OpenAI 也进了基金会，你觉得会不会形成统一结构？让大家按同一个标准来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这里有两个影响源：一方面，MCP UI（或者 MCPY）作为项目本身已经存在一段时间了，它有很多很好的想法。OpenAI 也吸收了其中一些想法，并做了不少改进。更重要的是：我们三周前在 MCP 博客上刚宣布——我们正在和他们一起做一个共同标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的目标是回到一个世界：你为一个平台开发一次，就可以在所有平台用；或者说 “一次构建，到处运行”——你在 ChatGPT 里能用，也可能在 Claude、在 Goose、或任何实现了该标准的程序里用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这件事的核心驱动力是：现代 AI 应用几乎一切都是文本交互，这没问题，也挺好；但有些事情，人类就是更擅长用视觉来做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最典型的例子：选飞机座位。如果让你用纯文本选——“这里有 25 个座位可选”——谁愿意这么干？你根本不知道这些座位在机舱图上是哪里。你当然想要一个 UI：你能点着选；而模型也能在这个 UI 上导航、交互；并且你作为人类也能同时交互。这就是我们想要的方向：做更丰富的界面。纯文本界面确实有天然限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你会在音乐制作等场景看到这种需求；你也会看到品牌方非常在意界面呈现。购物也是一个极好的例子：购物行业 20 年的 A/B 测试，研究“怎么把东西卖给你”最有效——购物界面其实非常复杂。所以我们需要一种方式，把这些熟悉的复杂 UI 展示给用户，让用户能交互。这就是 MCP Apps 最终要做的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：技术方向上是 iframe？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：对，是 iframe。本质上你通过 MCP resource 提供 原始 HTML，把它放进一个 iframe，然后通过一个明确的接口用 postMessage 和外部通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为是 raw HTML，而且不是加载外部内容，你如果愿意，理论上可以提前做安全分析。同时 iframe 也天然能提供比较清晰的隔离边界，让外部应用在一个安全边界内与之交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：iframe 在浏览器里用了很多年。我唯一担心的是 CORS……我太讨厌 CORS 了，而 iframe 总会遇到 CORS 问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：是的，但这里理论上不加载任何外部内容——至少我们不希望它这么做。当然，未来我们可能会不停迭代，五年后可能会出现一堆 CORS header 之类的复杂东西。但现在我们还是从小做起：纯 raw HTML，最好不要有外部引用，这样就不会碰到那些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那能继承宿主应用的样式吗？&lt;/p&gt;&lt;p&gt;David：不能。iframe 里你得把样式内联进去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来很小，但 UI 团队会非常在意。大家会希望它看起来像 ChatGPT。&lt;/p&gt;&lt;p&gt;David：完全同意。品牌方和设计师会非常非常在意。这也是我们需要解决的问题：先把东西推出去，让大家用起来，然后基于真实使用方式迭代。这也正是为什么我觉得长期来看它不应该一直是 iframe。我不知道最终解决方案是什么，但我们可能需要一种“新的 iframe”，它允许一定的“渗透性/可融合性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得这挺合理。另一条路可能就是“AGI build”的方式：给它一个 tool 说“给我样式”，模型再去问宿主应用“我应该长什么样”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那 MCP app 应不应该知道自己被嵌在哪个父应用里？比如父应用也暴露工具给模型调用，对吧？那是不是需要一个标准接口让父应用把样式传下去？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：可能是。这个问题很大。我得去问问团队。我自己并不在最底层细节里，我更多是站在整体方向上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这对我来说有点意外。我以前从没关注 MCP UI，结果你们突然都采纳了。我就想：好吧，那看来它已经是 MCP 的一部分了——它让 MCP 从纯后端议题，变成了前端议题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：需要说明的是：技术上它是 MCP 的一个扩展（extension），它不是 MCP 核心的一部分。这更多是治理层面的区分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你是一个能渲染 HTML 的客户端，你可以考虑实现它；但就算你不实现，你仍然是一个 MCP client。现实是：很多 CLI agent 根本渲染不了 HTML，所以它们永远不会实现。这没问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有其他类似的扩展吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我们可能会在金融服务方向做一些扩展。比如一年后，你可能会看到这样的世界：客户端会有某种“认证/资质”，并得到一个签名——证明它是“金融服务 MCP 客户端”，然后向 server 出示这个证明，server 才允许连接，因为它知道客户端会遵守归因（attribution）等法律合同要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;类似的机制也会出现在 HIPAA（医疗健康数据）这类场景：当你面对公共 server 和公共 client，同时还要处理敏感数据时，你必须提供一些保证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这不是 OAuth 的一部分吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：不一定。举个例子：假设客户端同时装了五个 MCP servers，其中有一个是医疗 server。这个医疗 server 可能会要求：在这个 session 里，你不允许使用其他 MCP servers，因为我给你的数据不能泄露到任何地方。你必须保证它不会跑出去——因为这是 HIPAA 数据、或者金融数据。这是一个很典型的约束：你不希望自己的社保号、健康数据不小心出现在别的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;加入 Linux 基金会会不会分心？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们接下来会切到 AAIF ，最后，有没有什么行动号召？比如招人、或者呼吁大家参与 MCP spec？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：最重要的还是——每天都去用 MCP 去构建：去做真正好的 MCP servers。我们看到很多很一般的 MCP servers，也看到一些非常非常优秀的。把 server 做好、把用法做扎实，这很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，我们是一个相当开放的社区，按传统开源方式运作：本质上取决于大家愿意投入多少时间和精力。所以你可以通过很多方式参与：给反馈、在 Discord 里交流、给点子；也可以帮我们做 SDK，比如 TypeScript SDK、Python SDK。我们也一直在找新的 SDK——比如我们有 Go SDK 在推进，但我们没有 Haskell SDK。如果你是 Haskell 开发者，你也许可以来写一个（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，可以做的事情很多。不要低估“参与社区”本身的价值。当然也别忘了去构建：现在机会太多了，尤其是我们对 progressive discovery 的理解更成熟了，对 code mode 的理解也更成熟了——接下来会出现一代新的客户端、一代新的 server，我非常期待大家去做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我最后一个问题，是想让大家直接听你说。我能感受到你的能量，我也对你们做的事情非常兴奋。但很多人对 MCP 加入 Linux 基金会有点焦虑：他们会说，“这是不是意味着 Anthropic 分心了？”你能回应一下吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我很喜欢你问这个问题。我完全理解大家为什么会这么想，但事实恰恰相反。Anthropic 的投入和承诺没有变：我们还是同一批人在做 SDK，我们的产品仍然高度依赖 MCP。我还是 MCP 的核心维护者。技术上什么都没变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基金会真正带来的核心变化只有两点：第一，它让整个行业确信：MCP 会永远开放，永远不会被拿走。历史上确实有公司把开源项目又变回专有。协议领域也有很多专有例子——比如 HDMI。你看 HDMI 在 Linux 上的那些问题。HDMI 2.1 的 HDMI Forum 不愿意让 AMD 开发 HDMI 2.1 的开源 Linux 驱动——真的，有些资料你可以去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以行业里很多人会盯着这些风险。基金会的意义就是：现在 MCP 归属一个中立实体，它会一直开放。你可以使用 “MCP” 这个名字，也不会有人因为商标去起诉你。这会给生态巨大的信心：它是中立的、可持续的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，如果说我最骄傲的是什么：我觉得我们已经在行业里为“开放标准”定下了基调。现在我们可以利用这个势能，在一个中立空间里建立社区：让大家把真正做得好、维护得好、长期可靠的项目放进来，成为基金会的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我们的门槛会很高：项目必须维护得很好。我们不想、也不会把基金会做成“分心”或“甩包袱”的地方。对我们来说，MCP 仍然是产品核心、仍然超级重要；Anthropic 的承诺和投入一如既往。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=z6XWYCM3Q8s&quot;&gt;https://www.youtube.com/watch?v=z6XWYCM3Q8s&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</link><guid isPermaLink="false">https://www.infoq.cn/article/xCTM5Q3Hy5yikzzOBc2Z</guid><pubDate>Wed, 14 Jan 2026 08:52:18 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item></channel></rss>