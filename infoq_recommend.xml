<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Tue, 27 Jan 2026 21:05:11 GMT</lastBuildDate><ttl>5</ttl><item><title>模力工场 030 AI 应用榜：字节新品硬刚 Sora，“随变”登顶榜首</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260126infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;想用一个下午快速摸清一个领域，并产出一份条理清晰、信息量丰富的深度内容？本周模力工场带你体验 “AI 增效流水线：从信息到作品的智能生产工作流”。从智能阅读提炼（语鲸）、一键生成研报（AI 快研侠），到跨平台记忆管理（MemOS-MindDock），再到自动视觉设计，这条流水线覆盖“读、写、研、记、设计”全流程，助你将碎片信息快速整合为结构化的知识作品。例如，若你对近期热议的 Clawdbot 等AI助手产品感兴趣，不妨以此为主题，用这套工作流实践一番。点击进入模力工场首页，查看顶部专题横幅，扫码添加模力小A，获取完整工具链与实操指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/57/574f118703932447159e822207d9f545.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;030 周上榜应用精选（附用户热评）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260126infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 030 周 AI 应用榜来袭！本周共有 32 款应用上架，榜单完全由用户真实使用、测评与讨论热度驱动。我们从社区声量最高的应用中精选出十款，并透过用户真实评论，为你解读榜单背后的产品逻辑与行业风向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;创作平民化：人人都能成为内容创作者&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/suibian?utm_source=20260126infoQ&quot;&gt;随变&lt;/a&gt;&quot;：潮人必备 AI 创作神器，让灵感瞬间变潮流短片！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”玩了几天随变，感觉有点像简洁版抖音…但 AI 创作出来的视频，如‘创作一条刀马刀马的舞蹈片段’它会理解为元素中有刀有马，BGM 也毫不相干…这显然是开盲盒，会消耗创作者热情。希望引入更多‘悦己’的功能。”【用户热评｜@MATTHEW】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/PixVerse?utm_source=20260126infoQ&quot;&gt;PixVerse&lt;/a&gt;&quot;：秒出电影感视频，视觉叙事交给 AI。&lt;a href=&quot;https://agicamp.com/products/xyqjianying?utm_source=20260126infoQ&quot;&gt;小云雀&lt;/a&gt;&quot;：一句话生成爆款，创作门槛降至零。&lt;a href=&quot;https://agicamp.com/products/whee?utm_source=20260126infoQ&quot;&gt;WHEE&lt;/a&gt;&quot;：一站式视觉工坊，生图改图扩图，创意无限延伸。&lt;a href=&quot;https://agicamp.com/products/singduck?utm_source=20260126infoQ&quot;&gt;唱鸭&lt;/a&gt;&quot;：零基础玩音乐，AI 帮你谱写生活旋律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;学习力升级：AI 正在重塑教辅软件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/qianwenzhixue?utm_source=20260126infoQ&quot;&gt;千问智学&lt;/a&gt;&quot;：全科 AI 辅导，教材全覆盖，答疑效率翻倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”千问智学高度契合我心中对答疑辅导类学习软件的期待。功能清晰划分为学习智能体、提分助手、宝藏资料、职业考试几大板块，每个分类还结合适用年龄、所学专业、所在地区等不同维度，打造了针对性的内容与服务…综合体验可以给到 9 分的高分。”【用户热评｜@Abin】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/doubaoaixue?utm_source=20260126infoQ&quot;&gt;豆包爱学&lt;/a&gt;&quot;： 随身 AI 家教，拍题秒出思路，学习难题不再怕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能自动化：从被动回答到主动执行&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/atoms?utm_source=20260126infoQ&quot;&gt;Atoms&lt;/a&gt;&quot;：多 AI 团队协作，想法闪电变产品原型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”用 Atoms 搭了一个族谱显示的网站...最戳我的是它的层级数据可视化功能，族谱的家族分支、辈分脉络展示得一目了然，不用自己折腾结构设计。而且全程打字输入需求就行，不用写一行代码，平台会自动匹配开发需求，内置多个角色也比较好用，做出来的族谱网站的展示效果整体合预期（有一些样式生成的没处理好，显示会重叠）。整体来说对新手很友好，搭建网站的核心需求都能完美满足，小细节的不足完全不影响基础使用，作为零代码工具来说很靠谱了。“【用户热评｜@墨鱼罐头】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/offerkuai?utm_source=20260126infoQ&quot;&gt;offer快&lt;/a&gt;&quot; 📍北京：AI 求职分身，智能匹配+自动投递，轻松拿下好工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;心理疗愈：不仅是应用，更是思考伴侣&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/metasight?utm_source=20260126infoQ&quot;&gt;MetaSight 元见&lt;/a&gt;&quot; 📍杭州：命运AI投射仪，换个视角看清人生路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”我其实很好奇这个领域的 AI 应用能发展到什么程度。之前试用时，我只是输入了自己当前的工作状态、心情和年龄，系统就帮我推算出了未来的发展方向和行动建议…如果这类应用未来能结合 IoT 硬件，可能会真正引爆市场。目前应用面向的用户群中包含不少中老年人，他们对这类能根据现状推理出下一步计划与发展方向的功能，需求其实非常迫切。”【用户热评｜@.】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;榜单之外但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/kouzi?utm_source=20260126infoQ&quot;&gt;扣子（2.0版本）&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：AI 职场助手｜流程自动化｜智能办公&lt;/p&gt;&lt;p&gt;模力小A推荐：专为职场人打造的智能效率伙伴，能帮你自动处理会议纪要、邮件撰写、日程安排等重复工作，让 AI 真正成为你的“数字同事”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/ViduAIMV?utm_source=20260126infoQ&quot;&gt;Vidu AI MV&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：一键成片｜AI MV 生成｜影音创作&lt;/p&gt;&lt;p&gt;模力小A推荐：只需上传图片和音乐，即可自动生成拥有专业转场、节奏卡点和电影级质感的音乐短片，让普通人也能轻松打造专属 MV。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本周上榜的一批 AI 应用呈现出几个非常鲜明的趋势：创作力提升、学习力强化、智能自动化与心理疗愈并行发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在创意生产与多媒体内容创作方向，我们看到像随变、PixVerse、小云雀、WHEE、唱鸭这样的应用迅速聚焦 AI 驱动的视觉与音频内容生成，从“一句话生成爆款内容”、秒级视频产出，到图像改图、一站式创作体验，AI 正在让个人创作者从繁琐操作中解放出来，把灵感瞬间转为可传播的成果。这与行业趋势一致：AI 正在大规模重塑创意产业和内容生产流程，创作者不再受制于传统软件约束，而能借助 AI 助力快速迭代与表达创意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在教育与知识服务方向，豆包爱学、千问智学等产品体现了 AI 在学习辅导领域的深化应用，它们通过拍题讲解、教材覆盖的智能辅导模式，正在将 AI 从“工具助手”升级到“学习伴侣”，这与全球教育领域推动 AI 个性化辅导、提升学习效率的大趋势不谋而合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，AI Agent 与自动化服务型工具（如 Atoms、offer快、MetaSight）正在形成一股新潮流。Atoms 体现了多智能体协作快速将想法变成可用产品的能力；offer快则将 AI 直接介入求职流程中，实现岗位筛选、沟通跟进与自动申请；MetaSight 则尝试把 AI 带入命理解读与人生洞察场景，让智能体具备不仅执行任务、还促发用户自我思考的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合来看，本周上榜的 AI 应用不仅覆盖了内容创作、学习辅导、个性化洞察和流程自动化等核心领域，还共同凸显了一个核心趋势：AI 正在从“简单生成”向“深度交互与高效执行”转变，让用户的生产力、学习效率和生活智能都进入一个新阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;One more thing，&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场将亮相 OceanBase 社区嘉年华！诚邀您加入我们的上海现场展位。作为 OceanBase 合作的创新社区，模力工场将于 1 月 31 日 登陆上海社区嘉年华，并拥有专属展位。这不仅是一次技术交流——我们更希望和你一起，在现场用 AI Coding 展现创造力、在开放麦分享你的项目故事、与行业先锋面对面切磋、在开源市集交换灵感。我们为你预留了专属席位，期待与你共同呈现：当开源精神遇上 AI 创造力，能碰撞出多少令人惊艳的可能。立即报名，锁定与数百位技术同行深度连接的一天！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c9fd51971f15feb3d78bbcd868faa640.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Hiv4SCiXtQMuJ8d9GDCM</link><guid isPermaLink="false">https://www.infoq.cn/article/Hiv4SCiXtQMuJ8d9GDCM</guid><pubDate>Tue, 27 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>构建下一代 AI 系统：可信生成式 AI 的工程蓝图</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着生成式 AI 从原型走向生产环境，真正的挑战已不仅是模型质量，更是要构建能被团队大规模信任的系统。开发人员和数据工程师被要求交付的不仅仅是能够生成答案的智能助手、Agent 和辅助工具，还必须确保这些系统在可靠性、安全性和与业务逻辑的一致性方面做到尽善尽美。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次分享中，Snowflake 首席数据与分析官&amp;nbsp;Anahita Tafvizi&amp;nbsp;凝练了来自 Snowflake 自身实践与行业顶尖构建者的深度交流，系统拆解了可信生成式 AI 的技术架构要点，内容包括：&lt;/p&gt;&lt;p&gt;语义层如何防止幻觉并保持一致性；为什么可观察性和评估框架生产级 AI 的必备要素；在开源与专有模型中实施权限管控、数据沿袭和可测试性；数百个企业用例中常见的开发模式与反模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47d31b2066312a28c3a41b6d8ec4e76a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论您正在将智能体扩展至生产环境，还是仅验证试点项目，这场分享都将为构建不仅智能、而且可信、安全且可复用的生成式 AI 系统提供前瞻性蓝图——为持久化的 AI 奠定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;信任，是 AI 能否走向生产的分水岭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 的实践中，构建一个“能回答问题”的 AI Agent 并不困难，真正困难的是构建一个输出高度准确、可被团队信任、并能据此采取行动的 Agent。这一难点在数据与分析类场景中尤为突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分析型 AI 的输出往往具有唯一正确答案，例如季度收入、增长率或关键指标。一旦系统在这些问题上给出哪怕一次错误结果，信任便会迅速崩塌，用户会回退到 SQL 查询或电子表格——因为在那里，他们至少能够自行追溯逻辑、验证结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中反复强调了一个行业普遍存在却容易被忽视的问题：分析幻觉（Aalytics Hallucination）。即 AI Agent 给出了看似合理、引用了正确表格与来源、但最终却是错误的数值。在分析场景中，这类错误的破坏性远高于一般生成式应用，也正是许多 AI 系统“上线即沉默”的根本原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一个三层结构的“信任工程”框架&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于 Snowflake 自身从原型到生产的实践经验，团队将“信任”总结为一个需要被工程化设计的系统能力，而非单一功能，并提出了一个由三层组成的框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一层：数据信任所有 AI Agent 必须锚定在企业唯一、可验证的数据真实源之上。为此，Snowflake 通过语义模型集中定义业务概念与核心指标，使 AI 与分析师使用的是完全一致的业务语言。同时，引入“已验证查询”机制，将经过人工确认的 SQL 逻辑作为标准答案来源，确保自然语言查询与分析师手写 SQL 得到的结果一致，从根本上避免分析幻觉的产生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二层：模型信任生成式模型天然是概率系统，而分析场景需要确定性。Snowflake 通过可观测、可测试的方式对模型施加约束：完整记录每次推理路径，引入基于真实答案的问题集进行持续评估，并通过 CI/CD 式的发布流程，在进入生产前设置明确的质量门槛。上线之后，Agent 并非“部署即完成”，而是通过真实使用数据不断迭代和补充新的已验证查询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三层：系统信任即便数据和模型足够可靠，缺乏治理同样会导致失败。分享中特别强调了三点：权限必须继承自底层数据对象，设计审查应成为标准流程，以及每个 Agent 都必须有明确的责任人，持续对安全性和质量负责。实践证明，这套治理结构并不会拖慢创新速度，反而为规模化落地提供了稳定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一个真实落地案例：面向 6000 人的销售与市场 AI 助手&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了验证上述框架的有效性，Snowflake 以内部市场与销售团队为对象，构建了一款基于 Snowflake Intelligence 和 Cortex AI 的 AI 助手。该系统每天服务超过 6000 名销售与市场人员，每周回答超过 12000 个业务问题，早期用户的 NPS 超过 90%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示中可以看到，系统通过清晰的界面向用户传递为什么可以信任这个答案：是否使用了已验证查询、完整的 SQL 执行过程、清晰的推理路径，以及在非验证场景下对原始资料的明确引用。正是这些可见的信任信号，使业务用户愿意将决策建立在 AI 输出之上，而不再仅将其视为辅助工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三个值得警惕的反模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在总结实践经验时，Anahita Tafvizi&amp;nbsp;也指出了三个在企业中反复出现的典型误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将 AI Agent 视为“一次性交付”的系统，缺乏持续监控与迭代，最终导致偏移和失控；使用模糊的“万能输入框”界面，却未明确 Agent 的能力边界，反而削弱了用户信任；允许各团队建立临时语义定义，导致同一指标在不同场景下含义不一致，从源头破坏可信性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题并非模型能力不足，而是缺乏系统性信任设计的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正胜出的，是“被信任的系统”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享最终回到一个朴素却极具现实意义的结论：在企业 AI 的竞争中，胜出的不会是最炫酷的系统，而是那些能够被团队长期信任、稳定产出价值的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当信任被纳入架构设计的起点，用户采用、开发效率与持续创新都会随之加速。这并非一个关于模型参数或技术噱头的故事，而是一场关于工程纪律、治理能力与长期主义的实践总结。&lt;/p&gt;</description><link>https://www.infoq.cn/article/atbgShUTkAqeAFuBIrVT</link><guid isPermaLink="false">https://www.infoq.cn/article/atbgShUTkAqeAFuBIrVT</guid><pubDate>Tue, 27 Jan 2026 10:07:48 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>KOOK 携手火山引擎RTC ，重构游戏开黑新体验</title><description>&lt;p&gt;对游戏玩家来说，开黑绝非简单的组队对战，而是集精准战术传递、清晰画面共享与默契实时互动于一体的核心体验。无论是 MOBA 游戏中瞬息万变的团战指令同步，还是 3A 大作里细致入微的通关攻略分享，玩家对开黑体验的核心诉求始终一致：清晰无扰的语音沟通、流畅高清的画面传递、稳定可靠的协作支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当玩家对开黑体验的要求日益严苛，单纯的功能叠加已无法满足需求，技术创新成为突破体验上限的关键。正是基于对玩家核心需求的深刻洞察，国内用户量第一的游戏开黑专用工具 KOOK 选择与拥有亿级产品服务经验的火山引擎 RTC 携手，一场以技术为纽带、以用户体验为核心的深度合作就此展开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/104496e4cdb58c655c9b60ea5f1aa353.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;深耕游戏开黑，直面两大痛点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 前身名为 “开黑啦”，覆盖实时语音、高清屏幕共享、多元社区互动等全场景，成为数千万硬核玩家不可或缺的协作与社交利器，月活跃用户接近数千万、日均语音时长达数十亿分钟。在游戏社交赛道竞争日趋激烈的背景下，KOOK 始终将玩家需求放在首位，但在真实游戏场景中，两大技术痛点长期存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是嘈杂环境下的语音清晰度问题，玩家的开黑场景多样且复杂，环境的杂音极易污染语音采集过程。模糊不清的语音往往导致沟通失误，甚至影响团队协作的凝聚力，让开黑的乐趣大打折扣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次是高画质屏幕共享的性能与稳定性。在游戏社交中，屏幕共享是核心的互动场景之一，但高画质游戏画面的共享，对终端性能和网络带宽是巨大考验。尤其是在共享高帧率的 3A 游戏 Boss 战时，常出现游戏掉帧、画面卡顿、音画不同步等问题，既影响操作的流畅性，也降低了内容的观赏性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两大痛点不仅让玩家的开黑体验大打折扣，更成为行业技术升级的核心方向。为了解决这些问题，KOOK 始终在追求对游戏场景的极致贴合与产品迭代，这种理念不仅让它积累了庞大的忠实用户群体，更成为它与火山引擎 RTC 合作的重要契合点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;火山引擎 RTC 赋能，实现 AI 与游戏社交的技术协同&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 对音频纯净度和画面流畅度有着极致追求，而火山引擎 RTC 凭借服务抖音的全球化基础设施与亿级用户打磨的音视频技术，为 KOOK 提供了针对游戏场景的专属优化方案。双方针对游戏场景的核心痛点，共同打造了专属优化方案，通过 AI 降噪与高刷屏幕共享两大核心能力，彻底改写了游戏开黑的体验标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI 降噪与音频 3A 协同，打造零干扰纯净语音&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为解决复杂环境下的语音沟通问题，双方对音频处理链路进行了深度优化，核心在于 AI 降噪技术与音频 3A 的协同发力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在噪声识别层面，火山引擎RTC 基于深度学习模型，实现了对游戏场景噪声的精准区分。无论是键盘敲击声、鼠标点击声、电脑风扇声，还是嘈杂人声，都能被精准识别并过滤。同时，技术团队重点保护 1-4kHz 的人声关键频段，这一频段是人类语音清晰度的核心，确保降噪过程中语音的保真度不受影响。整个降噪过程的收敛速度控制在 100 毫秒以内，实现全频带语言清晰的同时，保持低功耗运行，不占用过多设备资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在游戏开黑中，不同玩家的麦克风距离、说话音量存在差异，火山引擎的 AGC 技术最高支持 400% 的增益调节，能稳定麦克风采集的人声电平，让队友的听感保持一致。同时，它能有效抑制近距离喊话造成的削波失真，避免突然的音量峰值影响听觉体验，让队友间的交流始终处于舒适范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高性能屏幕共享，解锁 4K 超高清流畅体验&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对高画质屏幕共享的性能瓶颈，火山引擎 RTC 为 KOOK 量身打造了 “低延迟、高画质、低负载” 的专属方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PC 端支持 4K 60fps 的屏幕共享，移动端支持 1080P 60fps，无论是游戏中的细腻画质、操作细节，还是攻略演示中的关键步骤，都能精准呈现在观众面前。同时，火山引擎RTC 带来了GPU 硬件编码优先的策略，并实现多显卡适配，KOOK 能在不牺牲游戏性能的前提下，稳定输出超高清画面，解决了高刷推流与游戏流畅度之间的矛盾。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在网络适配层面，技术团队针对游戏场景优化了网络策略，端到端延迟稳定在百毫秒以内，实现了操作指令与画面的高度同步。同时，方案能智能分配网络带宽，在不影响游戏联网体验的前提下确保画面稳定，让玩家即便在网络条件一般的环境下，也能畅享流畅的超高清共享体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 与火山引擎RTC 的合作，本质上是技术实力与场景需求的精准匹配。双方通过深度合作，将火山引擎RTC 的技术优势与 KOOK 的场景经验相结合，不仅解决了当下的用户痛点，更共同推动了游戏社交领域的技术前沿探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;未来，共建 AI 驱动的智能化游戏社区&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次技术合作的成功，为 KOOK 与火山引擎 RTC 的深度绑定奠定了坚实基础。双方将继续聚焦 AI 技术在游戏社交领域的应用落地，从AI智能搜索和游戏AI助手两个方面，为玩家带来更具沉浸感的互动体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; 在AI智能搜索的角度，KOOK 积累了海量的游戏内容，火山引擎能够通过 AI 技术实现精准语义搜索，玩家无需在逐字查找就能快速定位所需信息让社区内容的价值得到充分释放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，双方也在计划探索构建智能化的游戏 AI 助手，通过 AI 助手的赋能，玩家的游戏体验将更加高效、便捷，游戏社交的智能化水平也将迈上新台阶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 与火山引擎的深化合作，在技术探索与场景落地中不断突破，用更先进的技术、更优质的产品，为广大游戏玩家打造更沉浸、更智能、更具温度的游戏社交生态，一个 “沟通无阻碍、协作更高效、互动更智能” 的游戏社交新时代，正在悄然到来。&lt;/p&gt;</description><link>https://www.infoq.cn/article/63IXzjH45sQALokMAd7N</link><guid isPermaLink="false">https://www.infoq.cn/article/63IXzjH45sQALokMAd7N</guid><pubDate>Tue, 27 Jan 2026 08:47:55 GMT</pubDate><author>火山引擎</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>DeepSeek 突发 OCR 2，采用基于 Qwen 的新架构</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里半夜刚发完旗舰模型，这边 DeepSeek坐不住了，突然发布更新了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/02/02f0be0f9553c3795f8bd64b0b4a0fca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚，DeepSeek 发布了 新模型 DeepSeek-OCR 2，采用创新的DeepEncoder V2方法，让AI能够根据图像的含义动态重排图像的各个部分，更接近人类的视觉编码逻辑。在具体实现上，DeepSeek 团队在论文中称采用了&amp;nbsp;Qwen2-0.5B&amp;nbsp;来实例化这一架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a4b716904d8f705ddf2f134892829551.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说去年 10 月 DeepSeek-OCR 的发布，让行业第一次意识到“视觉压缩”可能是一条被严重低估的技术路线，那么现在，DeepSeek 显然决定把这条路走得更激进一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 2 有何不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统 OCR 体系中，无论是经典的字符检测—识别流水线，还是近年来多模态模型中的视觉编码模块，本质上都遵循同一种思路：对图像进行均匀、规则的扫描和编码，再将结果交给语言模型或后续模块处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b57151a1c7a7b1742f364115ecd231e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种方式的问题在于，它并不关心“哪些视觉区域真正重要”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 1 之所以在当时引发讨论，正是因为它将 OCR 看作一种 视觉压缩问题：不是尽可能多地保留像素信息，而是将视觉内容压缩成更有利于语言模型理解的中间表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在 DeepSeek-OCR 2 中，这一思路被进一步推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据技术报告，DeepEncoder V2 不再将视觉编码视为一次静态的、固定策略的扫描过程，而是引入了语义驱动的动态编码机制。模型会在编码阶段就开始判断哪些区域更可能承载关键信息，并据此调整视觉 token 的分配与表达方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dcf519075177e51d3ea2d96a4b4aca0e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，视觉编码不再只是“预处理”，而是已经提前进入了“理解阶段”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;和 DeepSeek 过往几乎所有重要发布一样，这一次依然选择了模型、代码与技术报告同时开源。项目、论文和模型权重已同步上线：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-OCR-2&quot;&gt;https://github.com/deepseek-ai/DeepSeek-OCR-2&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&quot;&gt;https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;模型地址：&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&quot;&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/84TIUU5VrXv1jBYeV9lO</link><guid isPermaLink="false">https://www.infoq.cn/article/84TIUU5VrXv1jBYeV9lO</guid><pubDate>Tue, 27 Jan 2026 07:14:23 GMT</pubDate><author>Tina,李冬梅</author><category>生成式 AI</category></item><item><title>从 Vibe 到生产：Vibe Coding 的艺术、训练与陷阱</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，随着大模型与编程代理能力的快速成熟，AI 辅助编程在工程实践中的位置发生了实质性变化。围绕 Vibe Coding 的讨论，已不再停留在工具是否“好用”，或模型是否“足够聪明”，而是逐渐转向更具体、也更难回避的问题：当 AI 开始深度参与代码实现、测试与交付流程，软件工程中哪些能力被显著放大，哪些判断仍然必须由人来完成？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，这场发生在&amp;nbsp;BUILD 2025 大会上，题为《大咖之声：从 Vibes 到生产：Vibe Coding 的艺术、训练与陷阱》（From Vibes to Production：The&amp;nbsp;Art, Discipline, and Pitfalls of Vibe Coding）的圆桌对谈就显得尤为重要。因为它并没有顺着“AI 将如何颠覆软件工程”的情绪高点继续加码，而是进行了一场务实而冷静的对谈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;微软 Azure 首席技术官 Mark Russinovich 与微软开发人员社区副总裁 Scott Hanselman 在本场对谈中，深入解析 AI 编程助手与&quot;氛围编程&quot;正在如何重塑软件开发。两位技术领袖将演示是如何用自然语言编程来激发创造力并降低编码门槛的，但也会直面艰难现实：AI 生成的代码并非自动可投入生产环境。本次分享将审视如何利用氛围编程的速度与力量，通过系统架构设计、严格测试流程与安全实践，最终交付经得起现实考验的稳健软件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3c28813321e6140eef216d68ee6950c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/db51fea5c869ffae87a35fdb3dbd06db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;效率跃迁是真实的，但它首先放大的是经验&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对话中，两位嘉宾回顾了 AI 辅助编程的长期演进路径——从上世纪九十年代的 IntelliSense，到后来能够生成代码骨架的 IntelliCode，再到 2021 年前后出现的 Codex、GitHub Copilot，以及近一年逐渐成熟的内置代理式工具。真正的分水岭，并不是“AI 能不能写代码”，而是 Agent 开始能够自主修改代码、运行构建、执行测试并提交变更。当这种能力出现后，生产力的变化不再是线性的，而是呈现出数量级跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们都提到，在今年以来的实际项目中，效率提升已经从最初的 1.5 到 2 倍，跃升到了某些场景下的 5 到 10 倍。这种变化在中小型项目和个人工具上尤为明显。过去因为“太零碎”“不值得投入时间”而被放弃的想法，现在可以在极短时间内完成闭环。从一个想法到一个真实可用的工具，其间的摩擦被显著压缩。这正是 Vibe Coding 最具吸引力的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他们也明确指出，这种提升并非平均分配。真正被放大的，并不是“编程能力”本身，而是工程经验。具备系统理解、架构判断和问题拆解能力的人，能够从 AI 中获得指数级增益；而缺乏这些基础的人，则很难真正驾驭这种效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Agent 更像“永远停留在第一天的实习生”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在承认效率跃迁之后，讨论很快转向了 AI 编程的风险边界。随着 Agent 能力增强，一个反复出现的现象开始显现：这些系统在某一刻看起来极其聪明，逻辑清晰、输出完整，但在下一刻却可能犯下连初级工程师都难以接受的错误。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解释这种不稳定性，两位嘉宾使用了一个形象的比喻，AI Agent 很像实习生。不是因为它能力不足，而是因为它缺乏稳定的长期记忆，会反复犯已经被指出的问题，容易在任务过程中“走神”，并且对“什么才算真正完成”缺乏可靠判断。更关键的是，这个实习生永远停留在第一天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便你前一天已经明确指出了错误，第二天它依然可能回到原有的错误路径。它并不会真正积累经验，只是在当前上下文窗口内短暂服从指令。这种特性，使得在生产级系统中完全放手交给 Agent 成为一件高风险行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 并不理解系统，它更擅长迎合结果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在更深一层的技术讨论中，对谈触及了 AI 编程的核心问题：它并不真正理解系统。大模型在编程任务中，往往被高度优化为“让测试通过”“让用户满意”，而不是确保行为符合系统的整体约束与设计初衷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这会导致一系列危险倾向，例如为了通过测试而硬编码特殊分支，用 sleep 掩盖并发问题，混用新旧 API 却依然宣称“production ready”。更棘手的是，AI 往往会以极强的自信表达这些结论，甚至在输出中明确存在失败的情况下，仍然总结为“已经完成”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;两位嘉宾特别强调，这并非某一个模型的缺陷，而是当前主流 AI 编程系统普遍存在的结构性问题。其根源在于训练数据、强化学习目标以及模型本身缺乏跨时间的系统性记忆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正的分水岭，在工程师的成长路径上&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的技术现实下，一个更深层的影响开始浮现：AI 编程对不同阶段工程师的作用并不对称。对于具备系统感、架构经验和“代码嗅觉”的资深工程师而言，AI 是放大器；而对于缺乏基础判断能力的初级工程师来说，AI 反而可能成为效率阻力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4e/4e3d168be1e1da4df1405e388fc4e3c6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因并不复杂，如果你无法识别错误，就无法纠正 AI；如果你不理解系统，就无法判断“看起来能跑”的代码是否安全；而如果你只是接受结果，你就不会真正学习。对谈中引用的实验也印证了这一点，长期依赖 AI 的参与者，对自己刚刚完成的内容几乎无法回忆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由此，两位嘉宾给出了一个并不轻松的判断：学习没有捷径。随着 AI 能力增强，软件工程方法论的重要性不是降低，而是被进一步放大。复杂系统必须被拆解、被测试、被审查；生产代码的责任，始终无法外包。&lt;/p&gt;&lt;p&gt;在他们看来，当代码生产成本不断逼近零，真正的瓶颈将转移到评估、消化与决策能力上。限制生产力的，不再是算力或 token，而是人类的注意力带宽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Vibe Coding 更像一面放大镜&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对谈的结尾，两位嘉宾并未否定 Vibe Coding。相反，他们对“尝试新想法的成本前所未有地降低”表达了明确的兴奋。但他们给出的结论同样清晰：Vibe Coding 不是软件工程的终点，它更像一面放大镜。&lt;/p&gt;&lt;p&gt;它会放大经验、判断力和工程素养，也会放大认知缺失和方法论漏洞。最终，决定系统质量与工程上限的，仍然是人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果想继续了解两位嘉宾对于 Vibe Coding 相关议题的思考，欢迎朋友们订阅收听 Mark Russinovich 和 Scott Hanselman 的播客《&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0M0zPgJ3HSf4XZvYgZPUXgSrfzBN26pf&quot;&gt;Mark and Scott Learn To&lt;/a&gt;&quot;》。&lt;/p&gt;</description><link>https://www.infoq.cn/article/F6jd0giAQlBKmhVhVt7H</link><guid isPermaLink="false">https://www.infoq.cn/article/F6jd0giAQlBKmhVhVt7H</guid><pubDate>Tue, 27 Jan 2026 07:13:38 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>AI&amp;大模型</category></item><item><title>Docker通过Cagent提供AI代理确定性测试</title><description>&lt;p&gt;Docker&lt;a href=&quot;https://www.docker.com/blog/deterministic-ai-testing-with-session-recording-in-cagent/&quot;&gt;对Cagent运行时的定位&lt;/a&gt;&quot;是一种AI代理确定性测试方法，旨在解决团队在构建生产级代理系统时面临的日益严重的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI代理系统的日益普及，工程团队正面临&lt;a href=&quot;https://datagrid.com/blog/4-frameworks-test-non-deterministic-ai-agents&quot;&gt;测试概率性输出带来的挑战&lt;/a&gt;&quot;。传统企业系统基于一个简单的假设：同样的输入产生同样的输出。智能代理系统打破了这一假设，为了适应这种变化，如今的生态系统大多采用了评估变异性而非消除变异性的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去的两年里，评估框架应运而生，它们的目标是使智能代理的行为变得可观察、可测量。诸如&lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Arize-ai/phoenix&quot;&gt;Arize Phoenix&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.promptfoo.dev/&quot;&gt;Promptfoo&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/explodinggradients/ragas&quot;&gt;Ragas&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/openai/evals&quot;&gt;OpenAI Evals&lt;/a&gt;&quot;等工具可以捕获执行轨迹，并运用定性或基于大型语言模型的评分机制来评估结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些工具对于监控安全性和性能至关重要，但它们引入了一种不同的测试模型。输出结果很少是二元的。团队越来越多地依赖阈值、重试和软失败来应对评估器的差异性。举例来说，关于AI代理测试，有&lt;a href=&quot;https://www.domo.com/blog/ai-evaluations-101-testing-llms-agents-and-everything-in-between&quot;&gt;行业报道&lt;/a&gt;&quot;指出，传统的QA假设对于AI代理来说不成立了，因为输出是概率性的，结果评估需要更灵活的概率描述框架，而不是严格的通过/失败断言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，有些团队重新挖掘了一种更传统的测试方法，通过记录与重放模式实现测试的可重复性和确定性。这种模式借鉴了&lt;a href=&quot;https://vcrpy.readthedocs.io/&quot;&gt;vcr.py&lt;/a&gt;&quot;等集成测试工具的做法，能够一次性捕获真实的API交互过程，并在后续测试中确定性地重放。LangChain已&lt;a href=&quot;https://docs.langchain.com/oss/python/langchain/test&quot;&gt;明确推荐&lt;/a&gt;&quot;将该技术应用于大型语言模型测试，他们指出，记录HTTP请求与响应可使CI执行速度更快、成本更低且更具可预测性。不过在实践中，该方案通常来说仍然是被看成一个外部测试环节，而非智能代理执行机制的核心组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.docker.com/blog/deterministic-ai-testing-with-session-recording-in-cagent/&quot;&gt;Docker的Cagent&lt;/a&gt;&quot;就遵循这个范例。从架构上讲，Cagent使用了proxy-and-cassette模型。在录制模式下，它将请求转发给像OpenAI或Anthropic这样的真实提供商，捕获完整的请求和响应，规范化ID等易失性字段，并将交互过程存储于YAML格式的cassette文件中。在重放模式下，Cagent会完全阻止外部调用，将传入请求与cassette文件匹配，并返回记录的响应。如果智能代理的执行出现偏差，如使用了不同的提示、工具调用或序列，那么运行就一定会失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就成熟度来说，Cagent仍处于&lt;a href=&quot;https://docs.docker.com/ai/cagent/&quot;&gt;早期阶段&lt;/a&gt;&quot;。Docker自己的GitHub仓库对该项目的描述是正在积极开发当中，因此预计后续会有重大的变化，而且迄今为止，大多数公开的示例都来自Docker的文档，而不是大规模生产部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cagent的目标并不是取代现有的评估框架，但它揭示了AI代理测试发展过程中的一个不同方向。虽然如今有许多工具聚焦于执行完成后评估结果，但Cagent从一开始就将注意力转移到了使AI代理行为可再现上。随着团队尝试越来越复杂的AI代理工作流程，这种区别变得越来越明显。确定性重放并不判断代理的输出是否正确，但它确实使代理的行为变化变得更为显性化，为测试提供了一个更接近传统软件工程的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cagent-testing/&quot;&gt;https://www.infoq.com/news/2026/01/cagent-testing/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2PElIY0xN9ICC5LXQPQS</link><guid isPermaLink="false">https://www.infoq.cn/article/2PElIY0xN9ICC5LXQPQS</guid><pubDate>Tue, 27 Jan 2026 07:10:00 GMT</pubDate><author>作者：Matt Foster</author><category>AI&amp;大模型</category></item><item><title>2025 火山引擎智能视频云实践精选集</title><description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;h3&gt;1. 国际认可&lt;/h3&gt;
&lt;p&gt;火山引擎多媒体实验室多项成果入选 SIGGRAPH ASIA 2025&lt;br&gt;
火山引擎多媒体实验室 AIGC 视频画质理解大模型 VQ-Insight 入选 AAAI 2025 Oral&lt;br&gt;
火山引擎多媒体实验室画质理解大模型 Q-Insight 入选 NeurIPS 2025 Spotlight&lt;br&gt;
火山引擎多媒体实验室重要突破！LiveGS 技术登榜 SIGGRAPH，重新定义移动端自由视角视频直播&lt;br&gt;
ICME 2025 | 火山引擎在国际音频编码能力挑战赛中夺得冠军&lt;br&gt;
CVPR 2025 | 火山引擎获得 NTIRE 视频质量评价挑战赛全球第一&lt;br&gt;
火山引擎蝉联全国人工智能大赛 — AI + 增强视频质量评价冠军&lt;/p&gt;
&lt;h3&gt;2. 技术探索&lt;/h3&gt;
&lt;p&gt;当一朵云，打出「豆包同款」的旗&lt;br&gt;
从 “抖音同款” 到 “豆包同款”：AI 时代，视频云正在有了新表达&lt;br&gt;
从 “抖音同款” 到 “豆包同款”：视频云正在进入 Agent 时代&lt;br&gt;
火山引擎智能 3D 视频启动商业化，计划落地直播应用云端协同构建 VR 院线，加速 LBE 产业化与规模化发展&lt;br&gt;
火山 HTTPDNS Cache2.0：网段级精准调度驱动核心业务收益&lt;br&gt;
基于 DiT 大模型与字体级分割的视频字幕无痕擦除方案，助力短剧出海&lt;br&gt;
大模型帮你剪视频 —— 基于 MCP 打造火山引擎 VOD 智能剪辑&lt;br&gt;
火山引擎推出 veimage-mcp Server，打造专属您的图片智能助理&lt;br&gt;
火山引擎 OS Agent 解决方案、豆包 1.5・UI-TARS 模型发布&lt;br&gt;
10 + 芯片和模组商集体适配！让智能硬件能听会看还会唠&lt;br&gt;
重构智能设备管理范式：火山引擎端智能解决方案上新，多重 AI 服务即刻享用！&lt;br&gt;
揭秘豆包音视频通话幕后技术，自己开发产品也能用&lt;br&gt;
你给豆包打的这通视频背后，藏着 AI 实时交互的体验密码&lt;br&gt;
多模态需求井喷，智能视频云如何靠分布式处理破局？&lt;br&gt;
从 “可用” 迈向 “好用”：详解火山引擎智能视频云的三层架构升级&lt;br&gt;
破解 AI 硬件落地困局，火山引擎 RTC 重塑智能交互生态&lt;br&gt;
重新定义离线编码，H.266 为何能让视频更高清？&lt;br&gt;
实时通信的下一站，H.266 作为破局关键&lt;br&gt;
画质之外，直播编码还应当关注哪些技术优化点&lt;br&gt;
H.266 解码 “困局”，被这个解码器解决了&lt;br&gt;
重回 AI 战场！H.266/VVC 的时代才刚刚开始&lt;br&gt;
在 AI 应用爆发前夜，H.266 成熟了&lt;br&gt;
NeurIPS 2025 | 火山引擎多媒体实验室联合南开大学推出 TempSamp-R1 强化学习新框架 助力视频理解大模型高效提升时序理解能力！&lt;br&gt;
直击 3D 内容创作痛点 - 火山引擎多媒体实验室首次主持 SIGGRAPH Workshop 用前沿技术降低沉浸式内容生成门槛&lt;/p&gt;
&lt;h3&gt;3. 最佳实践&lt;/h3&gt;
&lt;p&gt;图虫 × 火山引擎：AIGC 创意工具链，让设计灵感高效实现&lt;br&gt;
探秘史前海洋，火山引擎 × 北京天卓视创带你沉浸式 “摸鱼”！&lt;br&gt;
央视点赞！凌云光・元客视界 × 火山引擎：打造数字人光场重建方案&lt;br&gt;
沉浸式文旅新玩法 - 基于 4D GS 技术的真人数字人赋能 VR 体验升级&lt;br&gt;
沉浸式 LBE 大空间互动体验！火山引擎支持《转折・从头越》北京 VR 巡展&lt;br&gt;
中央美院 × 火山引擎：AI + VR 构建艺术展新形态&lt;br&gt;
火山引擎赋能微短剧出海：从市场验证到规模化复制的 AI 实践路径&lt;br&gt;
火山引擎 RTC 联合乐鑫、移远：智能硬件注入 “豆包”，“模” 力升级&lt;br&gt;
详解 velmageX 助力卓特视觉智能、高效生成设计素材&lt;/p&gt;
</description><link>https://www.infoq.cn/article/2kQxCXUXOQ1eu4czPUNA</link><guid isPermaLink="false">https://www.infoq.cn/article/2kQxCXUXOQ1eu4czPUNA</guid><pubDate>Tue, 27 Jan 2026 06:01:54 GMT</pubDate><author>火山引擎视频云</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>Altman承认“搞砸了”！曝 GPT-5.2 牺牲文采换顶级编程，明年成本降 100 倍，实锤Agent 已能永久干活</title><description>&lt;p&gt;&amp;nbsp;在AI圈，Sam Altman的每一次发声都被视为对未来“天气预报”的更新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，Altman在X上发帖称将举办一场线上研讨会，希望在开始构建新一代工具之前收集大众的反馈和意见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9bf877672e0de766cf51cba5245e02d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;北京时间今早8点，这场由 OpenAI CEO Sam Altman 发起的研讨会如约而至。来自各行业的创业者、CTO、科学家和开发者社区的代表，围绕 AI 的未来形态、模型演进、智能体（Agent）、科研自动化以及安全问题，向 Altman 提出了最尖锐、也最现实的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研讨会上，这位OpenAI的掌舵人不仅勾勒了GPT-5及其后续版本的进化蓝图，同时揭示了一个令所有开发者和创业者不得不面对的现实：我们正在进入一个智力成本极低、软件形态从“静态”转向“即时生成”的剧变期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c0/c0f6d910d0885d444d46967d8e9d87e9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;会谈的第一个焦点，落在了GPT-5性能表现的“非对称性”上。有开发者敏锐地察觉到，相较于GPT-4.5，新版本在逻辑推理和编程上极强，但在文采上似乎略逊一筹。对此，Altman表现出了极高的坦诚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他承认，OpenAI在GPT-5.2的研发中确实“搞砸了”写作能力的优先级，因为团队将有限的算力资源倾斜在了推理、编码和工程能力这些硬核智力指标上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Altman看来，智力是一种“可塑的资源”，当模型具备了顶级的推理引擎，写作能力的回归只是时间问题。这种“偏科”实际上反映了OpenAI的某种战略重心：先通过Scaling Law（规模定律）攻克人类智力的最高地带，再回头去填补审美和表达的细节。这意味着，未来模型的竞争将不再是单一维度的比拼，而是看谁能更早地在全维度上实现“智力平权”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说智力水平决定了天花板，那么成本和速度则决定了AI的渗透率。Altman在会上给出了一个极具震撼力的承诺：到2027年底，GPT-5.2级别的智力成本将至少下降100倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，这种“廉价到无需计量”的未来并非终点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Altman指出，市场正在发生微妙的转向：开发者对“速度”的渴求正在超越对“成本”的关注。 随着Agent（智能体）开始处理数十个步骤的长程任务，如果输出速度不能实现百倍以上的提升，那么复杂的自主决策将变得毫无实用价值。在这种权衡下，OpenAI可能会提供两种路径：一种是极致廉价的“智力自来水”，另一种则是极速反馈的“智力推进器”。这种对速度的强调，预示着AI应用将从简单的问答，彻底跨入高频、实时的自动驾驶阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种智力成本骤降、速度飙升的背景下，传统软件的概念正在瓦解。Altman提出了一个颠覆性的愿景：未来的软件不应该是静态的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，我们习惯于下载一个通用的Word或Excel；未来，当你遇到一个特定问题时，计算机应该直接为你写一段代码，生成一个“即时应用”来解决它。这种“随需随生、用完即弃”的模式将彻底重构操作系统。虽然我们可能出于习惯保留一些熟悉的交互按钮，但背后的逻辑架构将是高度个人定制化的。每个人手中的工具都会随着其工作流的积累而演化，最终形成一套独属于个人的、动态进化的生产力系统。这不仅仅是软件的定制，更是生产关系的重组。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 翻译并整理了这场研讨会的重点内容，以飨读者：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：您如何看待 AI 对未来社会和经济的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 说实话，要在一年内完全消化这种规模的经济变革是非常困难的。但我认为这会极大地赋能每一个人：它将带来大规模的资源富足、门槛降低，以及创造新事物、建立新公司和探索新科学的极低成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;只要我们在政策上不出大差错，AI 应该成为社会的一种“平衡力量”，让那些长期以来未被公正对待的人获得真正的机会。但我确实担心，AI 也可能导致权力和财富的高度集中，这必须是政策制订的核心关注点，我们要坚决避免这种情况发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我发现 GPT-4.5 曾是写作能力的巅峰，但最近 GPT-5 在 ChatGPT 里的写作表现似乎有些笨拙、难以阅读。显然 GPT-5 在 Agent（智能体）、工具调用和推理上更强，它似乎变得更“偏科”了（比如编程极强，写作一般）。OpenAI 怎么看这种能力的失衡？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 坦诚说，写作这一点确实是我们搞砸了。我们希望未来的 GPT-5.x 版本在写作上能远超 4.5。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时我们决定将大部分精力放在 GPT-5.2 的“智力、推理、编程和工程能力”上，因为资源和带宽是有限的，有时专注于某一方面就会忽略另一方面。但我坚信未来属于“通用的高素质模型”。即便你只想让它写代码，它也应该具备良好的沟通和表达能力，能清晰、犀利地与你交流。我们认为“智力”在底层是相通的，我们有能力在一个模型中把这些维度都做到极致。目前我们确实在猛攻“编程智力”，但很快就会在其他领域赶上来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能将廉价到无需计量&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：对于运行数千万个 Agent 的开发者来说，成本是最大的瓶颈。您如何看待小模型和未来的成本降幅？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 我们的目标是，到 2027 年底，让 GPT-5.2 级别的智力成本至少降低 100 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在有一个新趋势：随着模型输出变得越来越复杂，用户对“速度”的需求甚至超过了“成本”。OpenAI 非常擅长压低成本曲线，但过去我们对“极速输出”的关注不够。有些场景下，用户可能愿意付高价，只要速度能提升 100 倍。我们需要在“极致廉价”和“极致速度”之间找到平衡，如果市场更渴望低成本，我们会沿着那条曲线走得非常远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在的交互界面并不是为 Agent 设计的。Agent 的普及会加速“微型应用（Micro Apps）”的出现吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 我已经不再把软件看作是“静态”的东西了。现在如果我遇到一个小问题，我期望电脑能立刻写一段代码帮我解决掉。我认为我们使用电脑和操作系统的方式将发生根本性改变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然你可能每天用同一个文字处理器（因为你需要按钮留在熟悉的位置），但软件会根据你的习惯进行极致的定制。你的工具会不断进化、向你个人的需求收敛。在 OpenAI 内部，大家已经习惯用编程模型（Codex）来定制自己的工作流，每个人的工具用起来都完全不同。软件“由于我、且为我”而生，这几乎是必然的趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给创业者的建议：不要做“模型的小补丁”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：当模型更新不断吞噬创业公司的功能时，创业者该如何建立护城河？有什么是 OpenAI 承诺不碰的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 很多人觉得商业的物理定律变了，其实并没有。现在的改变只是“工作速度变快了”、“开发软件变快了”。但建立成功初创公司的规则没变：你依然要解决获客问题，要建立 GTM（转市场）策略，要创造粘性，要形成网络效应或竞争优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我给创业者的建议是：你的公司在面对 GPT-6 的惊人更新时，是感到开心还是难过？你应该去构建那些“模型越强，你的产品就越强”的东西。如果你只是在模型边缘打个小补丁，那会过得很艰难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在的 Agent 执行长流程任务时经常在 5 到 10 步就断掉了。什么时候能实现真正长期的自主运行？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 这取决于任务的复杂程度。在 OpenAI 内部，有些通过 SDK 运行的特定任务已经可以近乎永久地运行下去了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不再是“何时实现”的问题，而是“应用范围”的问题。如果你有一个理解非常透彻的特定任务，今天就能尝试自动化。但如果你想对模型说“去帮我开一家创业公司”，由于反馈环路太长且难以验证，目前还很难。建议开发者先拆解任务，让 Agent 能够自我验证每一个中间步骤，再逐步扩大其职责范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 能帮人类产生好创意吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在很多人抱怨 AI 生成的内容是“垃圾（Slop）”，我们该如何利用 AI 提高人类创意的质量？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 虽然人们叫 AI 的输出为垃圾，但人类产生的废话也不少。产生真正的新创意是非常难的。我越来越相信，人类的思维边界取决于工具的边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我希望能开发出帮人产生好创意的工具。当创造的成本骤降，我们可以通过密集的反馈循环快速试错，从而更早找到好的创意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想象一下，如果有一个“Paul Graham 机器人”（YC 创始人），他了解你所有的过去、你的代码和工作，能不断给你提供头脑风暴，即便他给出的 100 个主意里有 95 个是错的，只要能激发你产生那 5 个天才般的念头，对世界的贡献也是巨大的。我们的 GPT-5.2 已经让内部科学家感受到了非平庸的科学进展，一个能产生科学洞察的模型，没理由产生不了优秀的产品洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我担心模型会让我们困在旧技术里。现在的模型学习两年前的新技术都很费劲，以后我们能引导模型学习最新出现的技术吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 这绝对没问题。从本质上讲，模型是一个“通用推理引擎”。虽然现在它们内置了海量的世界知识，但未来几年的里程碑将是：当你交给模型一个全新的环境、工具或技术，只要解释一次（或让它自主探索一次），它就能极其可靠地学会使用。这离我们并不远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：作为一名科学家，我发现研究灵感是指数级增长的，但人的精力有限。模型会接管整个科研流程吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 实现完全闭环的自主科研还有很长的路要走。虽然数学研究可能不需要实验室，但顶尖数学家目前仍然需要深度参与，纠正模型的直觉偏差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这很像国际象棋的历史：Deep Blue 击败卡斯帕罗夫后，曾出现一段“人机协作（半人马）”强于纯 AI 的时期，但很快纯 AI 就再次统领了赛场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的 AI 对科学家来说，就像是“无限量的博士后”。它能帮你同时探索 20 个新问题，做广度搜索。至于物理实验，我们也在讨论是该 OpenAI 自己建自动化实验室，还是让全球科研社区贡献实验数据。目前看，科研社区对 GPT-5.2 的拥抱让我们倾向于后者，这会是一个更分布式、更聪明、更高效的科研生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我更关心的是安全问题，最好是更强的安全性。在 2026 年，AI 有很多可能出问题的方式，其中一个我们非常紧张的方向是生物安全。现在这些模型在生物领域已经相当强了，目前无论是 OpenAI，还是整个世界的总体策略，大多还是试图限制谁可以接触这些模型，并且通过各种分类器，阻止模型帮助人们制造新的病原体。但我不认为这种方式还能持续很久。你怎么看？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：我认为，世界在 AI 安全，尤其是 AI 生物安全这件事上，需要完成一次根本性的转变——从“封堵（blocking）”，转向“韧性（resilience）”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一位联合创始人曾用过一个我非常喜欢的类比：火灾安全。火最初为人类社会带来了巨大的好处，随后它开始烧毁整座城市。人类最开始的反应，是尽可能去限制火。我最近才知道，“宵禁（curfew）”这个词，最早就和“晚上不允许生火”有关，因为城市会被烧掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，我们改变了思路，不再只是试图禁止火，而是提高对火的韧性：我们制定了消防规范，发明了阻燃材料，建立了一整套体系。现在，作为一个社会，我们在应对火灾这件事上已经做得相当不错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，AI 也必须走同样的路径。AI 在生物恐怖主义方面会成为一个真实的问题；AI 在网络安全上也会成为一个真实的问题；但与此同时，AI 也是这些问题的重要解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，我认为这需要的是全社会层面的努力：不是依赖少数“我们信任的实验室”永远正确地封堵风险，而是建设一种具有韧性的基础设施。因为这个世界上，必然会存在大量优秀的模型。我们已经和很多生物研究人员、公司讨论过，如何应对“新型病原体”的问题。确实有很多人投入其中，而且也有不少反馈认为，AI 在这方面是有帮助的，但这不会是一个纯技术问题，也不会是一个完全靠技术解决的问题。整个世界都需要以一种不同于过去的方式来思考这件事。坦率地说，我对当前的状态非常紧张。但我也看不到除“以韧性为核心”的路径之外，还有别的现实选择。而且，从正面看，AI 确实可以帮助我们更快地建立这种韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如果今年 AI 真的出现一次“明显、严重”的失败事件，我认为生物安全是一个相当合理的“风险爆点”方向。再往后一年、两年，你也可以想象，还有很多其他事情可能会出大问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 学习效率提高后，人与人之间协作还重要吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我的问题和“人类协作”有关。随着 AI 模型不断变强，它们在个人学习方面非常高效，比如快速掌握一个新学科。这一点我们在 ChatGPT 和教育实验中已经看到，也非常认可。但我经常会反复想到一个问题：当你可以随时得到答案时，为什么还要花时间、甚至承受摩擦，去向另一个人提问？你之前也提到，AI 编程工具可以用极快的速度，完成过去需要人类团队协作才能完成的工作。所以，当我们谈“协作、合作、集体智能”时，人类 + AI 是很强的组合，那人类与人类之间的协作会发生什么变化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：这里面有很多层问题。我年纪比在座的大多数人都大一点。但即便如此，Google 出现的时候，我还在上中学。那时老师试图让学生承诺“不使用 Google”，因为大家觉得：如果你随手就能查到一切，那为什么还要上历史课？为什么还要记忆？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，这种想法完全不可理喻。我当时的感觉是：这会让我变得更聪明，学到更多东西，能做更多事情，这就是我成年后要长期使用的工具。如果因为它存在，就让我去学那些已经被淘汰的技能，那反而是疯狂的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就好比：在你明明知道已经有计算器的情况下，却还强迫我去学算盘——那在当时可能是重要技能，但现在已经没有价值了。我对 AI 工具的看法是一样的。我理解，在当前的教育体系下，AI 工具确实成了问题。但这恰恰说明，我们需要改变教育方式，而不是假装 AI 不存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“让 ChatGPT 帮你写东西”这件事，就是未来世界的一部分。当然，写作训练仍然重要，因为写作是思考的一部分。但我们教人如何思考、以及如何评估思考能力的方式，必须发生变化，而且我们不应该假装这种变化不存在。我对此并不悲观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 10% 极端自学能力很强的学习者，已经表现得非常出色了。我们会找到新的方式，重构课程体系，把其他学生一起带上来。至于你提到的另一点——如何让这不是一个“你一个人对着电脑变得很厉害”的过程，而是一个协作过程。目前为止，我们并没有看到 AI 导致人类联系减少的证据，这也是我们在持续观察和测量的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的直觉恰恰相反：在一个充满 AI 的世界里，人与人之间的连接会变得更有价值，而不是更没价值。我们已经看到一些人开始探索新的界面，来让协作变得更容易。在我们考虑自研硬件和设备时，甚至一开始就在思考：“多人协作 + AI” 的体验应该是什么样子？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然现在还没有人真正把这件事完全做对，但我认为，AI 会以前所未有的方式，让这种协作成为可能。你可以想象：五个人围坐在一张桌子旁，旁边还有一个 AI 或机器人，整个团队的生产力会被大幅放大。未来，每一次头脑风暴、每一次问题解决，AI 都会成为团队的一部分，帮助整个群体做得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Agent 大规模进入生产系统，最大的被低估风险是什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提问：随着 Agent 开始大规模运行、直接操作生产系统，你认为最被低估的失败模式是什么？是安全、成本、可靠性吗？以及，哪些“艰难但重要的工作”目前投入还不够？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：你提到的这些问题，几乎每一个都成立。有一件事让我个人、也让我们很多人都感到意外。我第一次用 Codex 时，非常确信一件事： “我绝对不会给它完全、无人监督的电脑访问权限。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我坚持了大概两个小时。然后我想：它看起来真的在做非常合理的事情；每一步都要我点确认实在太烦了；不如先打开一会儿看看会发生什么。结果，我从来没有再把完全访问权限关掉。我发现，很多人都有类似的经历。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这让我真正担心的是：这些工具的能力和便利性太强了，而它们的失败概率可能很低，但一旦失败，后果可能是灾难性的。因为失败发生得不频繁，人们会慢慢滑入一种状态：“应该没事吧。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但随着模型变得越来越强、越来越难理解，如果模型内部存在某种微妙的错位，或者在长时间、复杂使用后出现新的系统性问题，你可能已经在某个系统里埋下了一个安全漏洞。你可以对“AI 失控”的想象有不同程度的科幻倾向，但我真正担心的是：人们会被这些工具的强大和愉悦感牵着走，而不再认真思考它们的复杂性。能力会上升得非常快；我们会习惯某个阶段的模型行为，并因此信任它； 但却没有构建足够健全的、整体性的安全基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，我们会在不知不觉中，走向一个危险状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，围绕这一点，本身就值得诞生一家伟大的公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 应该如何进入幼儿与基础教育？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我想回到教育的话题。我在高中时看到身边的同学用 ChatGPT 写作文、做作业；现在在大学，我们在 CS、人文等各个领域都在讨论 AI 政策。我想问的是：在幼儿园、小学、初中这些塑造思维方式的关键阶段，你作为一名父亲，如何看待 AI 对教育的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：总体来说，我是反对在幼儿园里使用电脑的。幼儿园应该更多是：在户外跑来跑去，接触真实的物体，学习如何与他人互动。所以，不只是 AI，我甚至觉得大多数时候，幼儿园里连电脑都不应该有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从发展角度来看，我们仍然没有完全理解技术对儿童的长期影响。关于社交媒体对青少年的影响，已经有很多研究了，而且结果相当糟糕。我的直觉是：大量技术对更小年龄儿童的影响，可能更糟，但讨论得却少得多。在我们真正理解这些影响之前，我不认为有必要让幼儿园阶段的孩子大量使用 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我们在生物医药领域。生成式 AI 在临床试验文档、法规流程等方面已经非常有帮助。现在我们也在尝试用它做药物设计，特别是化合物设计。但一个很大的瓶颈是 三维推理能力。&lt;/p&gt;&lt;p&gt;你认为这里会出现一个关键拐点吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：这个问题我们一定会解决。我不确定是不是 2026 年就能完成，但这是一个非常普遍、非常高频的需求。我们大概知道该怎么做，只是目前还有很多更紧急的方向需要推进。但这件事一定会到来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Wpxv-8nG8ec&amp;amp;t=2s&quot;&gt;https://www.youtube.com/watch?v=Wpxv-8nG8ec&amp;amp;t=2s&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pJSXUr4whkkYHZlallq9</link><guid isPermaLink="false">https://www.infoq.cn/article/pJSXUr4whkkYHZlallq9</guid><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>HarmonyOS开发者群像故事：每一份热爱都有回响</title><description>&lt;p&gt;欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙生态的沃土上，从不缺因热爱奔赴、因坚守发光的身影。他们身份各异 —— 高校学子、行业老兵、技术发烧友、创业者与企业开发者等，皆因鸿蒙的开放包容、分布式能力与友好生态而汇聚。在这里，技术不再是冰冷代码，而是连接亲情、破解痛点、分享经验、传承文化的载体。每一份对生活的洞察与创新的坚守，都能在鸿蒙支持下落地结果。这些故事是千万鸿蒙创作者的缩影，他们践行 “想没有答案，做才有结果” 的初心，让生态愈发蓬勃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本合集旨在呈现鸿蒙开发者的多元成长与实践，聚焦不同探索方向，彰显鸿蒙生态价值；更以案例为桥，传递创作初心，为开发者点亮方向，吸引更多人加入，共赴技术赋能生活的创新之旅。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 01：15 年大数据老兵鸿蒙“造梦”，父女联手打造亲子游戏 App &lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，从不缺乏跨界探索的身影。徐俊宸便是其中一位特殊的存在：深耕大数据领域多年，从数据产品经理到大数据讲师，他的职业生涯始终围绕数据打转；而一次偶然的鸿蒙论坛经历，让他萌生了开发 APP 的想法。最终，他以女儿课堂上的猜数字游戏为蓝本，与女儿一起打造出《猜数字大师》游戏应用，在跨界鸿蒙开发的道路上，既攻克了技术难关，也收获了别样的亲子时光。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文： &lt;a href=&quot;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&quot;&gt;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 02：00 后鸿蒙开发者支一郎：从校园需求出发，用代码搭建跨场景服务桥梁&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，有这样一位特殊的身影：他是 00 后在校大学生，却已凭借全栈技术能力成为 InfoQ 等技术平台的新星创作者；他从校园生活的痛点切入，牵头打造服务上万师生的 “校园智慧服务站”；他以小程序试水职场需求，再借鸿蒙原生能力迭代出融合 RPA 与 AI 的高效工具。他就是支一郎，一位在鸿蒙生态中快速成长的学生开发者，用实际行动诠释着 “年轻开发者如何在新兴生态中找到自己的价值”。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/1Su6kKzAuVQ03k8DZqkK&quot;&gt;https://www.infoq.cn/article/1Su6kKzAuVQ03k8DZqkK&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 03：从“探索”到“布道”，一个「鸿蒙领航者」的炼成记&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年 8 月，即将踏入大学校园的李浩佳第一次在新闻中看到了“HarmonyOS”的名字。那时的他，还只是一个对软件工程充满好奇的新生，未曾想到这会成为他职业生涯的重要注脚。六年过去，鸿蒙已从一个陌生的名词，变成了他日常开发的核心技术栈。他也从一名普通开发者成长为社区的技术分享者，持续为鸿蒙生态贡献力量。李浩佳带着骄傲对 InfoQ 说，由于在国内外平台积极分享，他已经两次获得“HarmonyOS 学习资源创作先锋”的荣誉称号。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/27Q3D8PGVvJXA8F5jWKx&quot;&gt;https://www.infoq.cn/article/27Q3D8PGVvJXA8F5jWKx&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 04：用“成语”疗愈“心情”，一位鸿蒙开发者的创意与选择&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你上一次阅读成语是什么时候？在当下短视频主导的时代，人们的生活日益碎片化，与文字的接触也渐行渐远。“俺也一样”和“提笔忘字”已成为当代青年日常生活的真实写照。语言能力的日益衰退，不仅会削弱表达能力，还会影响个人的认知与思考过程。相关研究显示，一个人的语言水平甚至直接关联其情绪认知与调节能力。基于这一洞察，深圳市蛟龙腾飞网络科技有限公司创始人李洋借助鸿蒙系统，开发了一款名为“成语心情”的应用。该软件根据用户日常心情和工作生活场景，提供针对性的成语学习，帮助深化对情绪与情境的理解。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/XXJf0Hd0zQ0JKlAjHgc0&quot;&gt;https://www.infoq.cn/article/XXJf0Hd0zQ0JKlAjHgc0&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👉更多开发者群像案例，持续上架中，欢迎扫码加入「InfoQ鸿蒙开发者交流群」，交流技术，也可联系「小助手」约稿~&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bf7ecdf3d9166706d0b9b6263a6b802.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👀也欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;</description><link>https://www.infoq.cn/article/dmtGPG8qRCgNWzL06yvN</link><guid isPermaLink="false">https://www.infoq.cn/article/dmtGPG8qRCgNWzL06yvN</guid><pubDate>Tue, 27 Jan 2026 04:09:35 GMT</pubDate><author>付秋伟</author><category>HarmonyOS</category></item><item><title>从算力规模到系统级竞争：智算竞争核心已变，金山云战略升级曝行业“隐形拐点”</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从训练到推理：智算需求正在经历一场结构性转向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，如果仅从“算力需求增长”来理解中国智算产业的变化，显然是不够的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2026年1月21日举办的金山云年度Tech Talk上，金山云对其过去一年智算业务的演进进行了系统性回顾。从公开财报数据到客户侧真实使用情况，这些信息拼凑出了一幅更清晰的图景：智算需求并非简单放量，而是在训练、推理、应用形态和工程方式等多个层面同时发生结构性变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场变化的核心，不再只是“谁拥有更大规模算力”，而是围绕模型如何被使用、Token如何被消耗、算力如何被组织展开。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;变化首先体现在财务数据上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据金山云披露的公开财报，其智算云业务在过去一年实现了高速增长。以2025&amp;nbsp;年第三季度为例，智算云账单收入达到7.8亿元人民币，同比增长接近120%。这一数据并非孤立，而是延续了此前多个季度的增长趋势，显示智算已成为金山云收入结构中的重要组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;金山云高级副总裁刘涛在分享中提到了金山云对这一趋势的判断：智算需求的增长重心，正在从训练侧逐步向推理侧转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从训练视角看，过去几年国内智算需求的主要推动力，来自少数对算力高度敏感的行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自动驾驶与具身智能，是其中最典型的代表。这些行业往往需要长期训练模型，并处理视频、点云、传感器等海量多模态数据。在早期阶段，它们对算力的需求更多集中在训练规模本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与通用大模型不同，这类行业模型并不一味追求参数规模最大化。刘涛在分享中指出，自动驾驶和具身智能模型在训练阶段，对算力密度的要求并不极端，但对显存容量和数据处理能力要求更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，它们对算力平台的诉求，正在从“算力数量”转向“系统能力”——包括数据接入、预处理、多模态调度以及训练全流程的工程化效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推理侧的变化更加显著。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说训练侧的变化仍然是渐进的，那么推理侧的变化则更为直接和激烈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个被反复引用的数据，来自火山引擎在其公开发布会上的披露：平台每日Token调用量已达到50万亿级别。这是当前国内少数被明确对外公布的Token规模数据之一，也成为行业理解推理负载的重要参考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，多个面向大众或企业的模型产品正在持续扩大推理需求。例如豆包、通义千问以及近期加大投入的腾讯元宝，都在不同程度上推动Token消耗快速增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些产品并不完全运行在同一云平台上，但它们共同指向一个事实：推理阶段正在成为智算需求增长的主要来源，且这种增长具备明显的外溢性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在所有推理场景中，编程类应用被反复强调。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘涛指出，2025年一个尤为显著的变化在于：编程相关请求正在成为Token消耗的主力场景之一。这一判断并非孤立，而是与海外模型使用结构的统计结果高度一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Vibe Coding”成为一个关键词。一个广为流传的事实是，Claude Code的大量代码本身，正是由Claude Code参与生成的。这意味着模型不再只是辅助工具，而是深度介入软件生产过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从全球Token调用结构来看，编程类请求在多家模型服务商中长期占据最高比例。金山云也观察到了同样的趋势：代码生成、重构和理解能力的提升，正在显著改变程序员的工作方式，并直接放大推理侧算力需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在具体应用层面，互联网客户仍然是智算需求的重要来源，但其需求形态已经发生变化。刘涛提到，当前互联网场景呈现出三个明显特征：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其一，多模态需求显著增长。视频生成、视频理解以及复杂推理任务，带动了训练与推理负载的持续上升；&lt;/p&gt;&lt;p&gt;其二，模型参数规模不再单向膨胀，而是围绕具体任务进行结构性调整；&lt;/p&gt;&lt;p&gt;其三，Vibe Coding在头部互联网公司中已较为普及，使用更强的商用模型进行代码开发，正在成为常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些变化意味着，互联网客户对智算平台的期待，已经从“算力服务”升级为对模型生命周期管理和工程体系的整体依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了满足更多元化的需求，刘涛表示，2025年，智算平台金山云星流已完成从资源管理平台向一站式AI训推全流程平台的战略升级。从训推平台、机器人平台到模型API服务，升级后的金山云星流平台构建了从异构资源调度、训练任务故障自愈到机器人行业应用支撑、模型API服务商业化落地的全链路闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实现三维进阶，智算云AI势能全释放&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管各行各业大规模应用AI还处于早期探索阶段，但定位行业助力者的金山云，多年来持续打磨全栈AI能力。从2023年的智算网基础设施，到2024年智算云的平台化和Serverless化，再到2025年的一站式AI训推全流程平台，通过提升平台效率、突破行业边界、加速推理布局，金山云为迎接AI应用爆发做好了充分准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在平台效率方面，金山云星流训推平台提供从模型开发、训练到推理的完整生命周期管理，具备开发、训练、推理和数据处理四大模块能力，通过降低多模块协同复杂度，能实现“开箱即用”的AI开发体验。自研的GPU故障自愈技术结合任务可观测性设计，可实时监控硬件健康状态与任务进程，自动触发故障迁移与任务重调度，降低算力中断风险，保障长周期训练任务稳定运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为面向机器人开发与落地的全链路云原生平台，金山云星流机器人平台深度融合数据采集、存储、标注、模型开发、训练、部署与仿真等核心环节，打造具身场景专属的数据、模型、仿真一体化引擎。平台率先实现具身智能数据工程领域采集、标注、管理的全链路闭环，可高效服务具身智能行业模型训练、仿真应用场景分析等核心需求，助力客户快速完成从算法研发到真实场景部署的全流程落地，最终推动机器人产业的智能化升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面向大模型应用开发者和企业用户，金山云星流平台模型API服务提供高可用、易集成的模型调用与管理能力，覆盖模型调用的全生命周期。该服务支持高并发推理与多模型管理，能够帮助用户高效接入多种模型资源，助力大模型应用落地。目前，金山云星流平台模型API服务已积累诸多行业客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，金山云星流平台的模型生态也在持续丰富。目前，平台已支持近40种不同模型，包括DeepSeek、Xiaomi MiMo、Qwen3、Kimi等。客户通过一站式访问，即可高效接入多种模型，在畅享稳定高效云服务的同时，更加聚焦AI业务创新和价值创造。&lt;/p&gt;</description><link>https://www.infoq.cn/article/ELmQulBO3oXOzC1F76It</link><guid isPermaLink="false">https://www.infoq.cn/article/ELmQulBO3oXOzC1F76It</guid><pubDate>Tue, 27 Jan 2026 03:58:35 GMT</pubDate><author>李冬梅</author><category>芯片&amp;算力</category></item><item><title>让机器人“看清”三维世界，蚂蚁灵波开源LingBot-Depth模型</title><description>&lt;p&gt;空间智能迎来重要开源进展。1月 27 日，蚂蚁集团旗下具身智能公司灵波科技宣布开源高精度空间感知模型LingBot-Depth。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该模型基于奥比中光Gemini 330 系列双目 3D 相机提供的芯片级原始数据，专注于提升环境深度感知与三维空间理解能力，旨在为机器人、自动驾驶汽车等智能终端赋予更精准、更可靠的三维视觉，在“看清楚”三维世界这一行业关键难题上取得重要突破。这也是蚂蚁灵波科技在2025外滩大会后首次亮相后，时隔半年在具身智能技术基座方向公布重要成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在NYUv2、ETH3D等权威基准评测中，LingBot-Depth展现出代际级优势：相比业界主流的 PromptDA与PriorDA，其在室内场景的相对误差（REL）降低超过70%，在挑战性的稀疏SfM 任务中RMSE误差降低约47%，确立了新的行业精度标杆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9a2618eab00ecdf730c1e675d8d0e97.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：在最具挑战的稀疏深度补全任务中，LingBot-Depth性能整体优于现有多种主流模型。图中数值越低代表性能越好。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在家庭和工业环境中，玻璃器皿、镜面、不锈钢设备等透明和反光物体物体十分常见，但却是机器空间感知的难点。传统深度相机受制于光学物理特性，在面对透明或高反光材质时，往往无法接收有效回波，导致深度图出现数据丢失或产生噪声。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对这一行业共性难题，蚂蚁灵波科技研发了“掩码深度建模”（Masked Depth Modeling，MDM）技术，并依托奥比中光Gemini 330 系列双目 3D 相机进行 RGB-Depth 数据采集与效果验证。当深度数据出现缺失或异常时，LingBot-Depth 模型能够融合彩色图像（RGB）中的纹理、轮廓及环境上下文信息，对缺失区域进行推断与补全，输出完整、致密、边缘更清晰的三维深度图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，LingBot-Depth 模型已通过奥比中光深度视觉实验室的专业认证，在精度、稳定性及复杂场景适应性方面均有良好表现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验中，奥比中光Gemini 330&amp;nbsp;系列在应用LingBot-Depth 后，面对透明玻璃、高反光镜面、强逆光及复杂曲面等极具挑战的光学场景时，输出的深度图依然平滑、完整，且物体的轮廓边缘非常锐利，其效果显著优于业内领先的3D&amp;nbsp;视觉公司Stereolabs 推出的 ZED Stereo Depth 深度相机。这意味着在不更换传感器硬件的前提下，LingBot-Depth&amp;nbsp;可显著提升消费级深度相机对高难物体的处理效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8dadb4233f1ed629b1f22f8dcd6d2043.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：搭载 LingBot-Depth 后，奥比中光Gemini 330 系列在透明及反光场景下深度图的完整性和边缘清晰度明显提升）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9a/9aa57018c78962b8705eeeb561d13089.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：其效果优于业界领先的ZED 深度相机）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LingBot-Depth 的优异性来源于海量真实场景数据。灵波科技采集约1000 万份原始样本，提炼出 200 万组高价值深度配对数据用于训练，支撑模型在极端环境下的泛化能力。这一核心数据资产（包括2M 真实世界深度数据和 1M 仿真数据）将于近期开源，推动社区更快攻克复杂场景空间感知难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，蚂蚁灵波科技已与奥比中光达成战略合作意向。奥比中光计划基于LingBot-Depth 的能力推出新一代深度相机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WVlzEl33IYR1SH7mBssb</link><guid isPermaLink="false">https://www.infoq.cn/article/WVlzEl33IYR1SH7mBssb</guid><pubDate>Tue, 27 Jan 2026 03:23:16 GMT</pubDate><author>蚂蚁集团</author><category>生成式 AI</category></item><item><title>OpenCost回顾2025年的里程碑事件，并制定2026年发展路线图</title><description>&lt;p&gt;&lt;a href=&quot;https://opencost.io/&quot;&gt;OpenCost&lt;/a&gt;&quot;项目——一个由&lt;a href=&quot;https://www.cncf.io/&quot;&gt;云原生计算基金会&lt;/a&gt;&quot;（CNCF）托管的开源成本和资源管理工具——&lt;a href=&quot;https://www.cncf.io/blog/2026/01/12/opencost-reflecting-on-2025-and-looking-ahead-to-2026/&quot;&gt;发布&lt;/a&gt;&quot;了一份年终回顾，回顾了项目2025年的开发进展，并概要介绍了2026年的优先事项。本次更新凸显了活跃的发布节奏、功能扩展（包括支持AI的MCP服务器）、通过导师计划和贡献推动的社区发展，以及扩展项目数据模型和成本追踪功能的计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，OpenCost社区发布了11个版本，增强了可用性并扩展了功能。重要新增特性包括：通过环境变量配置实现无需&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;&quot;即可运行OpenCost的能力；Beta版Collector数据源（一个用于成本数据导出的通用框架）；具备健康监测与导出功能的诊断系统。OpenCost还优化了多云成本追踪能力，在Oracle和DigitalOcean等供应商的贡献下，扩展了追踪云及多云指标的能力。这些版本旨在让成本透明度在Kubernetes环境中更具可操作性，使项目用户和贡献者均能从中受益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年的一个重要里程碑是引入了&lt;a href=&quot;https://github.com/opencost/opencost&quot;&gt;OpenCost MCP&lt;/a&gt;&quot;服务器，使AI代理能够使用自然语言实时查询成本数据。这种集成有助于自动分析跨命名空间、Pod和节点的支出模式，使团队不必手动查询即可生成成本报告和建议。MCP服务器是作为一个默认组件引入的，能够输出清晰、分步骤的成本优化建议，将云计算成本管理与新兴的AI生态系统相结合，满足自动化程度更高的FinOps工作流程的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社区活动同样表现突出。OpenCost项目通过&lt;a href=&quot;https://lfx.linuxfoundation.org/&quot;&gt;Linux基金会的LFX计划&lt;/a&gt;&quot;持续推进导师培训工作，受训者为企业就绪性贡献了集成测试，并推进了OpenCost数据模型2.0（KubeModel）的开发——该模型为跨动态Kubernetes资源实现可扩展的精确成本追踪奠定了基础。贡献者们还致力于完善文档、优化用户体验以及扩大社区参与度，进一步强化了OpenCost的开发者友好性及其生态系统的成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望2026，项目计划增强对AI使用成本跟踪的支持，因为机器学习工作负载对云计算支出的影响越来越大。围绕成本数据的供应链安全改进也是一个优先事项，同时，为了更好地反映Kubernetes资源行为的复杂性，项目计划对&lt;a href=&quot;https://opencost.io/blog/introducing-kubemodel/&quot;&gt;KubeModel框架&lt;/a&gt;&quot;进行迭代完善。参加即将到来的KubeCon+CloudNativeCon会议仍然是项目策略的一个关键组成部分，目的是提高项目在云原生从业者中间的认知度和采用率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为CNCF孵化项目，OpenCost在云原生领域的重要性日益凸显，因为成本可视化与资源调度已成为运维和财务治理的核心环节。该项目标准化了Kubernetes成本报告并整合了AI驱动的工具链，旨在帮助财务运维团队与工程团队应对2026年及之后日益复杂的多云工作负载的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/opencost-roadmap-2026/&quot;&gt;https://www.infoq.com/news/2026/01/opencost-roadmap-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6JfQa58o09NmOujZLf2C</link><guid isPermaLink="false">https://www.infoq.cn/article/6JfQa58o09NmOujZLf2C</guid><pubDate>Tue, 27 Jan 2026 03:12:45 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>理想汽车内部会曝光：必做人形机器人！全网急聘“最好的人”、连跳槽的前员工都要揪回来？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月26日，理想汽车CEO李想召开了一场两个小时的线上全员会。据多位内部员工反馈，李想强调，2026年是所有想要成为AI头部公司上车的最后一年；最晚2028年，L4一定能落地；最终全球布局基座模型、芯片、操作系统、具身智能等业务的公司不会超过3家，理想会努力成为其中一家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来，理想会进一步强化具身智能的品牌定位，而不仅仅是创造移动的家。在汽车之外，理想一定会做人形机器人，并会尽快落地亮相。”而接下来，理想为了迎接新一轮的AI竞争，公司将对研发进行新一轮的组织变革，将研发团队按照基座模型团队、软件本体团队、硬件本体团队等进行划分，其中汽车、机器人等都归为硬件本体团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，李想表示，要去招聘最好的人，把原来那些去到机器人创业公司的人再招回来。在此之前，已经有不少智驾核心技术人员从该公司离职，去具身智能赛道创业了。2025年下半年，前理想自动驾驶研发负责人贾鹏、量产负责人王佳佳与前CTO王凯等核心高管一起创办了具身智能公司至简动力，且很快就拿到多家头部美元基金和互联网科技公司的投资意向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，理想已在官网社招页面放出多个人形机器人研发岗位。从招聘信息可以看出，其研发项目几乎覆盖了人形机器人从核心部件到系统集成的全流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9d/9d9e57482830f408f388164b235df66c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2025年三季度业绩会上，李想公开表示，现在电动车行业拼参数已经拼到死胡同了，做智能终端又容易变成把手机应用搬到车里，属于重复建设，所以理想选了第三条路：把车定义成“具身智能”产品，让它从单纯的交通工具，变成有感知、有大脑、有神经、有心脏、有身体的“机器人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，早在2024年底的AITALK上，李想就说过，理想做人形机器人是肯定的，但还没到合适的时机。然而，此前，因为技术跟不上、人形机器人供应链不成熟等问题，理想暂停了人形机器人自研项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但理想在泛机器人领域的布局也一直在进行。2025年6月还有消息称，理想成立了“空间机器人”和“穿戴机器人”两个二级部门，都归高级副总裁范皓宇带领的产品部管，智能眼镜Livis是首款产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/mlWrTAeJsCJSREa1H4gZ</link><guid isPermaLink="false">https://www.infoq.cn/article/mlWrTAeJsCJSREa1H4gZ</guid><pubDate>Tue, 27 Jan 2026 03:08:30 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>DoorDash运用AI提升聊天与通话安全，将安全事件减少50%</title><description>&lt;p&gt;DoorDash构建并部署了&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;一个AI驱动的安全系统SafeChat&lt;/a&gt;&quot;，用于审核配送员与顾客在应用内聊天、发送图片及进行语音通话时的互动内容。SafeChat运用机器学习技术，可近乎实时地检测并响应不安全内容，筛查通信中的冒犯性或不当信息，并支持立即采取行动，如举报问题或取消配送任务。该系统专注于安全保障而非用户互动或自动化。它以AI为核心基础设施，旨在维护平台的信誉及保障配送员与顾客的利益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SafeChat使用了分层AI架构，结合了机器学习模型与人工审核。该系统每天处理数百万次互动，对文本消息、图片和语音通信内容进行分类。其文本审核分为两个阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在第一阶段，DoorDash工程师采用了一个三层方案。第一层是审核API，提供了一个低成本、&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;高召回率&lt;/a&gt;&quot;的过滤器，以极小的延迟自动清除了大约90%的消息。随后，未被清除的消息进入一个速度快、成本低的大型语言模型（LLM），它能以更高的精确度将99.8%的消息识别为安全信息。剩余的消息由一个更精确、成本更高的LLM进行评估，对包含不敬言语、威胁和性内容的消息进行评分。这些评分可以为采取安全行动提供支持，例如在检测到高风险消息时允许配送员取消订单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash基于第一阶段的大约1000万条消息训练了一个内部模型，第二阶段因此能够采用一种双层方案。内部模型为第一层，用于自动清除大多数消息。只有被标记的消息才会进入精确的LLM进行详细评分。第一层的响应时间在300毫秒之内，而被标记的消息可能需要长达三秒钟。该系统处理了99.8%的流量，提高了可扩展性并降低了成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1f/1f79a022d47d8b89de2cea30a1b1dd0b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DoorDash双层文本审核架构（图片来源：&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;DoorDash技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;图片审核由计算机视觉模型负责处理，模型的选择依据是吞吐量和粒度。为了减少误报和漏报，他们通过反复的人工审核调整了阈值和置信度分数。该系统每天可以处理数十万张图片，而且延迟可以满足实时互动的要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语音审核最初以仅观察模式部署，用于校准置信度分数。待阈值验证完成后，系统即可自动执行干预措施，例如中断通话或限制后续通信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/085bcba234e0f7a17b610efba0201418.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DoorDash语音审核架构（图片来源：&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;DoorDash技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上，作为&lt;a href=&quot;https://www.linkedin.com/posts/doordash_safechat-doordashs-ai-powered-safety-feature-activity-7393757928467562496-SGMo?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;特性发布公告&lt;/a&gt;&quot;的一部分，DoorDash表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;SafeChat通过AI创新和精心设计，使平台上的每个人都建立起信心和信任，为所有用户打造一个更安全顺畅的使用体验。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SafeChat的执行层根据违规的严重性和反复性采取相应的行动。它可以阻止或删除不安全的消息，终止通话，限制通信，或将问题升级给人类安全代理。重复出现或严重的违规行为会触发账户审查或暂停。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据DoorDash工程师的报告，通过将分层AI模型与&lt;a href=&quot;https://en.wikipedia.org/wiki/Human-in-the-loop&quot;&gt;人类反馈循环&lt;/a&gt;&quot;相结合，SafeChat系统得以实现大规模运行并保持近乎实时的响应速度。按照该公司的说法，该系统自部署以来，已使中低严重程度的安全事件减少了约50%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/&quot;&gt;https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2UiF1yejWi2urcT4jhtO</link><guid isPermaLink="false">https://www.infoq.cn/article/2UiF1yejWi2urcT4jhtO</guid><pubDate>Tue, 27 Jan 2026 02:51:01 GMT</pubDate><author>作者：Leela Kumili</author><category>AI&amp;大模型</category><category>安全</category></item><item><title>阿里旗舰模型Qwen3-Max-Thinking发布，刷新全球 SOTA 性能纪录！编程能力赶超Gemini 3 Pro、Claude-Opus-4.5</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;阿里突发最强旗舰模型，总参数过万亿&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在刚刚，Qwen3-Max-Thinking正式版突然发布，总参数规模超过 1 万亿（1T），位于目前全球最大规模 AI 模型行列，预训练数据规模高达 36T Tokens，覆盖大量高质量语料。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen3-Max 是阿里通义团队迄今规模最大、能力最强的语言模型，该版本包括 Base、Instruct 和 Thinking 多种形式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/eb/eb9351ef6a1de987d6e622576602cb21.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在多项权威基准测试中表现优异，Qwen3-Max-Thinking性能可与 GPT-5.2-Thinking、Claude-Opus-4.5、Gemini-3 Pro 等闭源顶级模型竞争甚至超越。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ee/ee9403ccc261ee43ee966c0a42e126b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，Qwen3-Max-Thinking 在多项关键 AI 基准测试中达到了或刷新了全球 SOTA 表现：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在包含事实科学知识、复杂推理和编程能力在内的 19 项权威基准测试中取得极高水平，有记录显示其综合表现可媲美 GPT-5.2-Thinking、Claude-Opus-4.5 及 Gemini-3 Pro 等业内领先模型。在数学推理基准测试中，该模型曾在预览阶段实现 AIME 25 和 HMMT 25 满分（即 100% 准确率），这一表现被认为代表了高难度数学推理能力。相较于此前的 Instruct 版本，Thinking 版本在 Agent 工具调用、复杂逻辑和深度推理任务中表现出更优的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些测试覆盖了科学知识问答（如 GPQA Diamond）、数学推理（如 IMO 等级测试）、代码编程（如 LiveCodeBench）等多个领域，是衡量大型语言模型综合能力的重要指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cd/cd7595d0ed40a2b8bba29d6de2c8b574.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现上述性能突破，千问团队在官方博客中称为 Qwen3-Max-Thinking 引入两项核心创新：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自适应工具调用能力，可按需调用搜索引擎和代码解释器，现已上线；测试时扩展技术（Test-Time Scaling），显著提升推理性能，在关键推理基准上超越 Gemini 3 Pro。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这两项核心创新到底什么意思？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是自适应工具调用能力，据千问团队介绍，与早期需要用户手动选择工具的方法不同，Qwen3-Max-Thinking 能在对话中自主选择并调用其内置的搜索、记忆和代码解释器功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该能力源于专门设计的训练流程：在完成初步的工具使用微调后，模型在多样化任务上使用基于规则和模型的反馈进行了进一步训练。实验表明，搜索和记忆工具能有效缓解幻觉、提供实时信息访问并支持更个性化的回复。代码解释器允许用户执行代码片段并应用计算推理来解决复杂问题。这些功能共同提供了流畅且强大的对话体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再来说说测试时扩展。该技术是指在推理阶段分配额外计算资源以提升模型性能的技术。研发团队提出了一种经验累积式、多轮迭代的测试时扩展策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不同于简单增加并行推理路径数量&amp;nbsp;N（这往往导致冗余推理），团队对并行轨迹数量进行限制并将节省的计算资源用于由“经验提取”机制引导的迭代式自我反思。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该机制从过往推理轮次中提炼关键洞见，使模型避免重复推导已知结论，转而聚焦于未解决的不确定性。关键在于，相比直接引用原始推理轨迹，该机制实现了更高的上下文利用效率，在相同上下文窗口内能更充分地融合历史信息。在大致相同的 token 消耗下，该方法持续优于标准的并行采样与聚合方法：GPQA (90.3 → 92.8)、HLE (34.1 → 36.5)、LiveCodeBench v6 (88.0 → 91.4)、IMO-AnswerBench (89.5 → 91.5) 和 HLE (w/ tools) (55.8 → 58.3)。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些技术改善了模型处理复杂任务时的自主规划、推理链构建和决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问App PC端和网页端已经第一时间上新这一Qwen系列最强模型，现在即可免费体验。API（qwen3-max-2026-01-23）也已开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;体验地址：&lt;a href=&quot;https://chat.qwen.ai/?spm=a2ty_o06.30285417.0.0.1ef4c921OJuiXU&quot;&gt;https://chat.qwen.ai/?spm=a2ty_o06.30285417.0.0.1ef4c921OJuiXU&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：中国大模型不负期待！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在模型发布消息传出后，社交平台上也迅速出现了大量讨论。一部分网友的关注点集中在模型能力本身，语气中带着明显的惊讶与认可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有海外开发者在 X 上表示，自己已经习惯看到 Qwen 在多个榜单上“反超”其他模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Qwen 总是能跑赢其他模型，”一位用户调侃道，同时也提出了更偏产品层面的期待，希望 Qwen 能在 Android 端做出“更简洁、更有辨识度的应用设计”，认为模型能力已经走在前面，产品体验还有进一步打磨空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6df8ee44ffc56fd4917e6cdd749d322.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也有不少声音将 Qwen 的发布节奏与国际头部厂商作对比。一位网友直言，通义千问团队在模型更新和能力披露上的频率，甚至“已经超过了 OpenAI”。在他看来，这种持续、高密度的迭代和公开沟通，本身就是一种对开发者更友好的信号，至少让外界清楚知道模型在什么阶段、解决了哪些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7020ac024dcc20386482bd947f563be.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有用户的反馈则更为直接。一位名为 Harriett Solid 的网友在评论中写道：“这正是我一直在等的 Qwen 发布版本。”这类评价并未展开具体技术细节，但从情绪上看，显然将 Qwen3-Max-Thinking 视为一次“到位”的升级，而不是过渡性产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a58a1df17281666e3aca285d896447e2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，网友评论呈现出两个明显特点：一方面，对 Qwen 在推理能力和更新速度上的认可度较高；另一方面，讨论已经开始从“模型是否强”延伸到“产品体验、生态建设是否匹配当前能力”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也从侧面反映出，随着模型能力逼近甚至进入全球第一梯队，外界对通义千问的期待，正在从单点技术突破，转向更完整的产品与平台层面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;https://chat.qwen.ai/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qwen.ai/blog?id=qwen3-max-thinking&quot;&gt;https://qwen.ai/blog?id=qwen3-max-thinking&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TW0LubMXqqmDGvSTS5s4</link><guid isPermaLink="false">https://www.infoq.cn/article/TW0LubMXqqmDGvSTS5s4</guid><pubDate>Mon, 26 Jan 2026 21:00:00 GMT</pubDate><author>李冬梅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>腾讯发力社交AI赛道，元宝内测“元宝派”玩法</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月26日，腾讯旗下AI助手元宝低调开启全新社交AI玩法“元宝派”内测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从目前流出的内测截图来看，用户可以选择创建一个“派”，或者加入一个已有的“派”。用户可以在派内@元宝 或引用元宝的话，让元宝AI总结派内聊天、创建健身、阅读等兴趣打卡活动，由元宝AI担任“监督员”。不止文字聊天，用户还可以在派内进行“图片二创”，将一张普通的照片变成有趣的“梗图”或表情包，在共同创作中激发乐趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30092526ec3c4ef868fe24040ba55c1e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据网传截图，元宝派后续公测还将开放上线“一起看”、“一起听”玩法，该玩法接⼊了腾讯会议的⾳视频底层能⼒，让⽤户可以邀请派内好友同步观看一部电影、一场比赛、听一首歌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;元宝派还打通了微信、QQ等社交产品体验，用户可以把“派号”或者专属邀请链接分享到微信朋友圈、或者微信、QQ好友，让好友一键丝滑加入元宝派。此前，元宝和微信、QQ已深度打通，不仅可以在微信、QQ添加“元宝”为联系人，随时随地和元宝AI互动，还能在公众号、视频号评论区@元宝，让TA总结内容、拓展提问。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/ac73119eec17854f1e5191309a62e294.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月25日，腾讯还宣布将在元宝APP内派发10亿现金红包。腾讯已经多年不参与春节“撒币”，本轮用10亿真金白银砸向元宝，在AI赛道加速的决心可见一斑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;腾讯路线：把AI带入群体交流中&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;腾讯将AI战局拉进了自身最擅长的“社交场”，试图用“元宝派&quot;打开一种全新的人与AI交互的模式。此举标志着腾讯正将AI应用的探索方向，从提升个人效率的“工具”属性，延伸至连接人与人、增进群体互动的属性，为AI应用的发展提供一个新的解题思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于AI更自然、更深入进入人们日常生活的方式，业界的探索路径逐渐分野：其一，让AI模拟人类行为、完成复杂任务的Agent（智能体）被寄予厚望；同时，另一条路径也正浮现，即将AI带入群体交流中，使其成为社交互动的一部分，“元宝派”正是腾讯在此方向上的一次具体实践。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多人沟通是人类最真实、最高频的沟通场景之一，也是对AI的上下文理解、多轮对话、意图识别等综合能力要求最高的场景。选择从这一场景切入，可以看出腾讯希望在最具挑战性的环境中，探索和打磨AI产品能力的决心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，不同于追求任务执行效率的逻辑，可以看出“元宝派”更关注AI在群体中的“社交价值”。它试图回答的问题是：当AI拥有了理解群体氛围、参与群体讨论、辅助群体决策的能力时，它将如何改变我们的线上社交体验？这一定位，旨在破解当前AI应用普遍面临的用户粘性不足、使用场景单一的挑战，通过引入真实、多维的社交关系，为AI的演进提供更丰富的土壤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交探索之外，腾讯在AI赛道2025年下半年也动作不断。从全模态的模型布局，到密集引进高阶人才、组织变阵，再到各业务线加速完成AI改造，腾讯开始找到自己在AI马拉松进程中的节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/klZi0LdorLWrHvvmMoOJ</link><guid isPermaLink="false">https://www.infoq.cn/article/klZi0LdorLWrHvvmMoOJ</guid><pubDate>Mon, 26 Jan 2026 10:55:46 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>我收集了12条技术社区疯传的Claude Prompt，如今这篇帖子火遍全网</title><description>&lt;p&gt;&amp;nbsp;近日，一篇关于 Claude 提示词（Prompt）的整理帖在海外技术社区迅速走红。发帖者是一位活跃在 X（原 Twitter）的国外网友，他声称自己系统性地收集了近一段时间在 Reddit、X 以及研究型社区中“被反复验证有效”的 Claude Prompt，并将其汇总成一份清单公开发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在帖子中，这位网友用了一种颇具传播性的说法来形容这些 Prompt 的效果——“可以在 60 秒内完成原本需要 10 小时的工作量”。尽管这一表述明显带有夸张成分，但并不妨碍该帖迅速在技术圈、研究圈和写作社区中被大量转发和收藏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0d/0dd7f9716fe2717a37246def52bf8ed6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与常见的“帮我写方案”“帮我改文案”类 Prompt 不同，这份清单中的 12 条提示（原作者提到共13条提示，但有一条是重复的，故最终为12条提示）几乎没有一条是直接要求模型“产出结果”的。相反，它们更多聚焦于质疑、拆解、对照和反思——这些原本属于研究人员、审稿人或资深从业者的认知工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 翻译整理了该网友提出的12条Prompt，供参考：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1、“矛盾查找器”：非常适合用于论文、报告或长篇文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“列出所有内部矛盾、未解决的矛盾，或与证据不完全相符的论断。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它能揭露人类忽略的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2、“审阅者#2”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“像持怀疑态度的同行评审员那样进行批判性评价。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要严厉批评。重点关注方法论缺陷、缺失的控制因素和过于自信的论断。残酷。必要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3、“将此内容转化为论文”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你倾倒原始笔记、链接或不成熟的想法时，可以使用此功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“请将以下材料整理成一份结构化的研究简报。内容包括：关键论点、证据、假设、反驳论点和未决问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;标记任何薄弱环节或缺失之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4、“倒着解释”的技巧：非常适合检验真正的理解程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“先解释这个结论，然后一步一步地倒推到假设条件。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果逻辑崩溃，你会立刻发现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;5、“像科学家一样进行比较”提示，不是功能列表，而是真正的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比较这两种方法：理论基础、失效模式、可扩展性和现实世界的限制。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;6、“什么会破坏它？”提示：用于预测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“描述一下这种方法会造成灾难性失败的场景。不是极端情况，而是实际存在的故障模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大多数人从来不会问这个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;7、“是什么改变了我的想法？”通常用于结尾&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“分析了所有这些之后，什么应该改变我目前的看法？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这才是真正的研究人员的思考方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;8、 “一页纸思维模型”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“把整个主题浓缩成一个我能记住的单一思维模型。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果文件无法压缩，说明你还没有拥有它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;9、“跨域翻译”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“请用一个完全不同领域的类比来解释这个概念。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不仅能带来理解，更能带来洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;10、“窃取结构”技巧&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条往往被低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通常用于分析文章的结构、流程和论证模式。在撰写优秀论文和文章时，请将其用于写作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;11、“像科学家一样进行比较”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不是功能列表，而是真正的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比较这两种方法：理论基础、失效模式、可扩展性和现实世界的限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;12、“假设压力测试”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条信息直接来自研究论坛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“列出该论点所依赖的每一项假设。现在告诉我哪些最脆弱，以及原因。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pDlNcmOaTX2BYwSjBRBY</link><guid isPermaLink="false">https://www.infoq.cn/article/pDlNcmOaTX2BYwSjBRBY</guid><pubDate>Mon, 26 Jan 2026 10:43:35 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>从 4 个 9 到 6 个 9，为什么云平台的高可用才是业务生死线?</title><description>&lt;p&gt;2025 年底，科技行业的注意力和主流叙事，仍然在向 AGI 靠拢。好消息是，前沿技术的理解成本在降低，新的技术价值在涌现；坏消息是，想重启对云端业务保障话题的讨论，客观上变得困难了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但不管行业关注点怎么变，业务能不能稳定运行，始终是企业最关心的问题之一。尤其是行业当下所处的发展阶段，保障业务“高可用”变得越来越重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，快速变化的技术栈和基础设施，使上云几乎成为商业公司规模化扩展业务的最优解。云端承载的业务量级正在急剧扩张，相应的业务保障难度也在上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，全球化已经从“可选题”变成“必答题”，传统企业服务厂商与头部客户携手出海的案例比比皆是，而新兴公司几乎从诞生第一天起，就是云化的、全球化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;综合影响下，很多公司 2025 的增长方式已经演变为“多业务线 + 多区域 + 多渠道 + 多合作方”。随着系统复杂度的剧增，对高可用保障的需求变得越发急迫。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而对高可用的回应，既不能粗暴设置为“异地多活”，也不能简单概括为“冗余多备份”。高可用真正的难点，不在于“把系统再部署一份”，而在于“多地同时对外服务时，怎么保证一致性、怎么切流量、怎么控制故障半径、怎么回切”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;《腾讯专有云 TCE 高可用技术白皮书》是对以上问题的系统回应，直接给出了详细的架构方案和设计思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;[&lt;a href=&quot;https://qdrl.qq.com/7GdiXHpJ&quot;&gt;点击此处&lt;/a&gt;&quot;直接下载/查阅《腾讯专有云 TCE 高可用技术白皮书》全文]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“四个九”：从统计口径到工程路径&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;曾有观点认为，高可用的讨论意义不大，技术提供方只要承诺是“4个9”还是“6个9”就够了，概率外的属于“黑天鹅事件”，主要看“命”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，当请求量、依赖数、变更频率增加以后，如果没有做好高可用，所谓的小概率事件有可能按周出现。无论几个九，都只是统计口径，但高可用要交付的是工程路径，结果由边界、口径、依赖、故障域、演练能力共同决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是为什么在报告的开始，首先要统一工程语言和建设目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在语言方面，最需关注的关键术语有三个：可用性、RTO、RPO，它们共同定义了 SLA（Service Level Agreement，服务级别协议）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可用性：一般以百分比的形式呈现。以电信运营商（ISP）提供的企业专线服务为例，如ISP向客户承诺，可用性指标为99.99%（一般称为4个9），每年计划外停止服务的时间在全年服务时间中的占比，就不应当高于0.01%，也就是365（天）×24（小时）×0.01%=0.876（小 时），合52.56分钟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;RTO（业务恢复时间）：指的是从灾难状态恢复到可运行状态所需的时间，用来衡量系统的业务恢复能力，也就是所谓的业务连续性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;RPO（数据恢复目标）：指的是在灾难过程中的数据丢失量，用来衡量系统的数据冗余备份能力，也就是所谓的数据可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期机房容灾常被简化为“有备份、有双机”。但在真实事故里，业务恢复太慢（RTO&amp;nbsp;过高）、数据丢失太多（RPO&amp;nbsp;过高）&amp;nbsp;的问题经常出现，于是监管、审计、企业风险控制把“高可用”从体验问题转换成了指标问题，RTO、RPO&amp;nbsp;也随之成为讨论高可用的统一语言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么如果将RTO作为X轴，RPO作为Y轴，就得到了用于规划建设目标的象限图，如下所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/06/06504dbfbbc04fc845c30dd4a901a9ae.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决高可用问题，本质上是在解决分布式问题，一致性（Consistency）、可用性（Availability）、分区容错性（Partition Tolerance） ，或许还要加上经济性，几乎不可能被兼顾，必须把业务可接受的损失，映射到技术路线，以方便 IT 负责人作取舍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;越想要 RPO=0（不丢数据），越要依赖同步复制与一致性机制，成本与写性能压力就会上升；反过来，越想要 RTO≈0（业务几乎不中断），越要依赖多活，自动化切换与流量治理，导致架构复杂度上升，治理难度上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更理智的做法是把能力分层： 关键数据链路尽量做到 RPO=0，关键业务链路逐步压低 RTO，非关键部分用多活换扩展性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于高可用设计的深度探讨与体系化建议，白皮书内有更为详尽的论述，欢迎参阅白皮书全文，获取完整的工程参考指南。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业领先的“八横四纵”高可用体系的构建&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;明确了建设目标，应该如何在架构层面进行设计？腾讯专有云 TCE&amp;nbsp;在白皮书中给出了具体的参考，叫做“八横四纵”高可用保障框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/42/42f70d77976c7b268fbd8b7d0e411318.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“四纵”架构明确了高可用能力的容灾深度，即云平台在不同物理规模故障下的生存能力。其核心逻辑是实现故障域的逐级隔离。分别为：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;&amp;nbsp;硬件组件级高可用：任一硬件组件，如单块磁盘、单条网络线缆或单个电源模块等组件故障，均不影响业务的正常运行，也不会引起数据丢失；&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;&amp;nbsp;节点级高可用：任一硬件节点，如管控支撑节点、计算节点、存储节点、网络节点、中间件节点或数据库节点故障，均不影响业务的正常运行，也不会引起数据丢失；&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;&amp;nbsp;机柜级高可用：任一机柜发生整体故障，如PDU断电或其他物理设施故障造成机柜内部分或所有设备均掉线时，不影响业务的正常运行，也不会引起数据丢失；&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;&amp;nbsp;AZ（可用区）级高可用：任一AZ整体故障，如整个机房的电力供应或网络连接中断等情况，使得整个AZ无法提供服务时，不影响业务的正常运行，也不会引起重要的数据丢失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“四纵”的划分本质在于解答故障域问题，也是对高可用问题比较坦诚的回应。行业中，经常有厂商承诺“4个9”，但没说清是应对单机故障，还是应对AZ级故障。不先定四纵层级，SLA承诺就没有可参考性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了“四纵”，IT 人员首先可以根据业务需求，反推高可用的实际保障范围，另外也避免了过度设计，反而因为成本和复杂度问题导致难以为继。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说“四纵”回答的是“扛多大的灾”，那么“八横”回应的就是“靠哪些能力抗灾”。因此，报告中的“八横”，是从基础设施高可用，一直延伸到终极目标：应用高可用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“八横”把高可用拆成一组必须协同的层级能力，每个业务都必须回答， 要达成对应的纵向等级，在这一“横”需要实施哪些动作，怎样完成验收。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多平台事故的关键症结是：业务还有希望，但控制台/API不可用、调度/迁移/扩容不可用、权限/配置下发不可用，最终导致 RTO 过高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在事故里，最脆弱的往往不是运维最关注的那一层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“八横”将最容易被忽略的“网络+管控+运维”与最被重视的“计算+存储+数据库”拉齐到了同等重要的位置。可以说，高可用“4个9”的真正底气，来自“八横四纵”高可用保障框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于具体的部署模式，&lt;a href=&quot;https://qdrl.qq.com/7GdiXHpJ&quot;&gt;《腾讯专有云 TCE 高可用技术白皮书》&lt;/a&gt;&quot;中也进行了具体的阐述。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;单 AZ、双 AZ（可选仲裁区）、三 AZ、多地中心部署，实际上近似一套可以直接“抄作业”的完整答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;单 AZ&amp;nbsp;部署能够保证单AZ内部基础硬件点、各个云组件、机柜间、外部网络以及基础电力的高可用，这种部署模式仅适用于一些对数据安全性和业务连续性没有太高要求的用户，但因为其成本也较低，恐怕会长期存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“双 AZ + 仲裁”模式的优势在于，在任意一个AZ故障，或双AZ出现脑裂时，不需&lt;/p&gt;&lt;p&gt;要手工拉起这些支撑组件，能够实现对平台和云产品无影响。这种模式可能会成为“严肃场景的默认形态”，直到企业愿意为“三 AZ”模式付出更多建设成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“三 AZ”模式将支撑组件集群、云管控平台及部分云产品服务集群跨三个AZ部署，任意一个AZ故障时，ZK和etcd等支撑组件集群能够保持多数派存活，保证云管控平台和各个云产品集群的服务不中断，对云上应用不产生影响。当业务足够关键、组织能力足够成熟时，三 AZ&amp;nbsp;显然是更“工程化”的答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而多地中心部署的思路更适用于银行等对数据安全性和业务连续性要求极高的用户，这些用户往往在同城双中心的基础上，再增加异地灾备数据中心。当极端情况出现，同城双AZ同时不可用时，异地部署的备Region就可以接管业务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，部署模式的选择仍然是个复杂的问题，并不存在针对某一行业的通用解决方案。白皮书中也给出了覆盖国家级5G新媒体平台、头部农商银行等行业的实战案例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;历经万亿级金融业务验证的高可用技术&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在金融行业，业务连续性与数据安全往往面临更复杂、更极端的考验。从白皮书披露的多个实践案例可以看到，无论是头部农商银行、国有大型保险集团，还是股份制商业银行，这些机构普遍具备交易量大、核心系统多、监管要求严格等共性特征，对“不可中断““零数据丢失“的要求极高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这些场景中，腾讯专有云 TCE 并未采用单一模式套用，而是结合金融业务实际，给出了包括双活AZ+仲裁、双 Region、三 AZ 的分级容灾方案，其中服务某头部股份制商业银行的“三 AZ”案例，也是业内首个同城三AZ的金融云平台案例，非常有参考价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从结果来看，这类架构已能够在真实生产环境中支撑金融核心系统运行，在单 AZ、机房级甚至地域级故障场景下，实现业务快速接管与数据完整性保障。综合白皮书中的案例实践，腾讯专有云 TCE 当前已能够提供全场景金融级容灾能力，最高支持六级容灾等级，实现 RTO ≤ 2&amp;nbsp;min、RPO = 0 的高可用目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这种能力之所以被认为处于行业前列，源于其在复杂金融场景中的持续积累与演进。一方面，它在多家金融机构的核心系统中长期运行，并通过反复演练不断完善；另一方面，也经受了微信支付、理财通等万亿级金融业务在高并发与极端场景下的实际检验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这些真实业务环境的共同锤炼下，该高可用能力逐步形成了一套成熟路径，为金融行业对高可用“可落地、可验证、可持续”的要求提供了现实参考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更多案例细节在此不再赘述，腾讯专有云 TCE 所构建的这一套高可用架构方法，足够先进，可复制性强，为企业排除了高可用建设的诸多“噪声”。这套方案精准契合了当下企业核心业务全量上云与全球化稳健扩张的双重需求，具有极强的实战指导价值和标杆意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;诚邀下载《腾讯专有云 TCE 高可用技术白皮书》查阅参考。若您希望就相关技术架构与腾讯专有云 TCE 团队或 InfoQ 展开进一步探讨，欢迎在评论区留言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30375deb94277fdddfff75b243bc71ae.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sEiFj4QVzURjFUWb4RGb</link><guid isPermaLink="false">https://www.infoq.cn/article/sEiFj4QVzURjFUWb4RGb</guid><pubDate>Mon, 26 Jan 2026 10:00:00 GMT</pubDate><author>王一鹏</author><category>腾讯</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>奥特曼小号泄密：OpenAI代码工作100%交给Codex！工程师才揭底Codex“大脑”运行逻辑，碾压Claude架构？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用一个 PostgreSQL 主库和 50 个只读副本，就顶住了 ChatGPT 上的 8 亿用户！&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，OpenAI的工程师们不仅爆出了这一惊人消息，还直接把Codex的“大脑”给扒了个精光。在OpenAI 官方工程博客主页，OpenAI 工程师、Technical Staff 成员 Michael Bolin发布了一篇文章，以“揭秘 Codex 智能体循环”为题，深入揭秘了 Codex CLI 的核心框架：智能体循环（Agent Loop），并详细讲解了 Codex 在查询模型时如何构建和管理其上下文，以及适用于所有基于 Responses API 构建智能体循环的实用注意事项和最佳实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些消息传出后，在Hacker News等技术论坛及社交平台上获得了高度关注。“看似平淡的技术最终会胜出。OpenAI 正在证明，优秀的架构远胜于花哨的工具。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，有网友透露，前不久Anthropic 的一位工程师称“他们用于 Claude Code UI 的架构糟糕且效率低下”。而就在刚刚，X上出现一条爆料：Codex已接管OpenAI 100%的代码编写工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30f62549fbbf3d6a07e11b46de53ee61.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于“你们有多少百分比的编码工作是基于OpenAI模型进行”的问题，roon表示，“100%，我不再写代码了。”而此前，Sam Altman 曾公开发帖称，“roon是我的小号。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/79d713fd1f9241aadd05296b83b51079.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Codex的“大脑”揭秘&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“每个人工智能智能体的核心都是Agent Loop，负责协调用户、模型以及模型调用以执行有意义的软件工作的工具之间的交互。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，在 OpenAI 内部，“Codex”涵盖了一系列软件智能体产品，包括 Codex CLI、Codex Cloud 和 Codex VS Code插件，而支撑它们的框架和执行逻辑是同一个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3b/3b637b18b4cafda84135a8c2bbefa602.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent Loop的简化示意图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，智能体会从用户那里接收输入，并将其纳入为模型准备的文本指令集，该指令集被称为提示词。下一步是通过向模型发送指令并要求其生成响应来查询模型，这个过程称为推理。推理过程中，文本提示词首先被转换为一系列输入token，随后被用于对模型进行采样，生成新的输出token序列。输出token会被还原为文本，成为模型的回复。由于token是逐步生成的，该还原过程可与模型的运行同步进行，这也是众多基于大语言模型的应用支持流式输出的原因。实际应用中，推理功能通常封装在文本API后方，从而抽象化词元化的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推理步骤完成后，模型会产生两种结果：（1）针对用户的原始输入生成最终回复；（2）要求智能体执行某项工具调用操作。若为第二种情况，智能体将执行该工具调用并将工具输出结果附加至原始提示词中。该输出结果会被用于生成新的输入内容，再次对模型进行查询；智能体随后会结合这些新信息，重新尝试完成任务。这一过程会不断重复，直至模型停止发出工具调用指令，转而生成面向用户的消息（在 OpenAI 的模型中，该消息被称为助手消息）。多数情况下，这条消息会直接解答用户的原始请求，也可能是向用户提出的跟进问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于智能体可执行能对本地环境进行修改的工具调用，其 “输出” 并不仅限于助手消息。在很多场景下，软件智能体的核心输出是在用户设备上编写或编辑的代码。但无论何种情况，每一轮交互最终都会以一条助手消息收尾，该消息是智能体循环进入终止状态的信号。从智能体的角度来看，其任务已完成，操作控制权将交还给用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b0e6ff6a2f96d092cc253776e3ede3d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多轮智能体循环&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，对话内容越丰富，用于模型采样的提示词长度也会随之增加。而所有模型都存在上下文窗口限制，即其单次推理调用可处理的token最大数量，智能体可能在单次对话轮次中发起数百次工具调用，这有可能耗尽上下文窗口的容量。因此，上下文窗口管理是智能体的多项职责之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;这套智能体循环如何运行？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Codex 正是借助响应 API 来驱动这套智能体循环的，博文曝出许多背后的实际运行细节，包括：&lt;/p&gt;&lt;p&gt;Codex不会把用户的话直接给大模型用，而是会主动“拼接”出一整套精心设计的提示词结构，且涵盖多个角色的指令、用户输入的一句话在结尾才出现。模型推理与工具调用之间可能会进行多轮迭代，提示词的内容会持续增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;构建初始提示词&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为终端用户，在调用响应 API 时无需逐字指定用于模型采样的提示词，只需在查询中指定各类输入类型，由响应 API 服务器决定如何将这些信息组织为模型可处理的提示词格式。在初始提示词中，列表中的每个条目均关联一个角色。该角色决定了对应内容的权重占比，优先级从高到低分为以下几类：系统、开发者、用户、助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3934b941f6e44fde60dc0dad6a69e08.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;响应 API 接收包含多个参数的 JSON 负载，其中三个核心参数有：&lt;/p&gt;&lt;p&gt;指令：插入模型上下文的系统（或开发者）消息工具：模型生成回复过程中可调用的工具列表输入：向模型传入的文本、图片或文件输入列表&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Codex 中，若已配置，指令字段的内容会从～/.codex/config.toml 配置文件中的模型指令文件读取；若未配置，则使用与该模型关联的基础指令。模型专属指令存储在 Codex 代码仓库中，并被打包至命令行工具中。工具字段为符合响应 API 定义的模式的工具定义列表。对于 Codex 而言，该列表包含三部分工具：Codex 命令行工具自带的工具、响应 API 提供且开放给 Codex 使用的工具，以及通常由用户通过 MCP 服务器提供的自定义工具。JSON负载的输入字段为一个条目列表。在添加用户消息前，Codex会先向该输入中插入以下条目：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1. 一条角色为开发者（role=developer）的消息，用于描述仅适用于工具部分中定义的Codex内置Shell工具的沙箱环境。也就是说，其他工具（如由MCP服务器提供的工具）并不受Codex的沙箱限制，需自行负责实施自身的防护规则。该消息基于模板构建，核心内容均来自打包在Codex命令行工具中的Markdown代码片段。&lt;/p&gt;&lt;p&gt;2.一条角色为开发者的消息，其内容为从用户的 config.toml 配置文件中读取的 developer_instructions 配置值。&lt;/p&gt;&lt;p&gt;3.一条角色为用户的消息，其内容为用户指令；该内容并非来源于单个文件，而是从多个数据源聚合而来。一般而言，表述越具体的指令，排序越靠后：&lt;/p&gt;&lt;p&gt;加载 $CODEX_HOME 目录下 AGENTS.override.md 和 AGENTS.md 文件的内容在默认 32 千字节的大小限制内，从当前工作目录对应的 Git / 项目根目录（若存在）向上遍历至当前工作目录本身，加载任意 AGENTS.override.md、AGENTS.md 文件的内容，或加载 config.toml 配置文件中 project_doc_fallback_filenames 参数指定的任意文件内容若已配置相关技能，则补充以下内容：关于技能的简短引言、各技能对应的技能元数据、技能使用方法说明章节。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;一条角色为用户的消息，用于描述智能体当前的运行本地环境，其中会明确当前工作目录及用户所使用的终端 Shell 信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当 Codex 完成上述所有计算并完成输入初始化后，会追加用户消息以启动对话。需注意的是，输入中的每一个元素都是一个 JSON 对象，包含类型、角色和内容三个字段。当 Codex 构建好要发送至响应 API 的完整 JSON 负载后，会根据～/.codex/config.toml 中响应 API 端点的配置方式，携带授权请求头发起 HTTP POST 请求（若有指定，还会添加额外的 HTTP 请求头和查询参数）。当 OpenAI 响应 API 服务器接收到该请求后，会使用 JSON 数据来推导出模型的提示信息，（需要说明的是，Responses API 的自定义实现可能会采用不同的方法）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可见，提示词中前三项的顺序由服务器决定，而非客户端。也就是说，这三项里仅系统消息的内容同样由服务器控制，工具与指令则均由客户端决定。紧随其后的是JSON负载中的输入内容，至此提示词拼接完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模型采样&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提示词准备就绪后，模型才开始进行进行采样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一轮交互：此次向响应 API 发起的 HTTP 请求，将启动 Codex 中对话的第一轮交互。服务器会以服务器发送事件（SSE）流的形式进行响应，每个事件的数据均为一个 JSON 负载，其type字段以response开头。Codex接收该事件流并将其重新发布为可供客户端调用的内部事件对象。`response.output_text.delta`这类事件用于为用户界面实现流式输出功能，而`response.output_item.added`等其他事件则会被转换为对象，附加至输入内容中，为后续的响应API调用所用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;若首次向响应API发起的请求返回两个`response.output_item.done`事件，一个类型为推理（reasoning），一个类型为函数调用（function_call），那么当结合工具调用的返回结果再次向模型发起查询时，这些事件必须在JSON的输入字段中进行体现。后续查询中用于模型采样的最终提示词结构如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/ae4b6e2c78ddc6b2116f05bc03a52262.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;需要特别注意的是，旧提示词是新提示词的完整前缀。这一设计是有意为之的，因为它能让用户借助提示词缓存提升后续请求的效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Codex命令行工具中，会将助手消息展示给用户，并聚焦输入编辑区，以此提示用户轮到其继续对话。若用户做出回应，上一轮的助手消息以及用户的新消息均需附加至响应API请求的输入字段中，从而开启新一轮对话。同样，由于对话处于持续进行的状态，发送至响应API的输入内容长度也会不断增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/01/01f5394fa51b8486df2e18923a885a62.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;弃用简单参数费力做优化，就为了用户隐私？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在对话过程中，发送至响应API的JSON数据量，是否会让智能体循环的时间复杂度达到二次方级别？”答案是肯定的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，尽管响应API支持通过可选的previous_response_id参数缓解这一问题，但目前Codex并未启用该参数，主要是为了保证请求完全无状态，并兼容零数据保留（ZDR） 配置，即不存储用户对话数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;取而代之的，是两套需投入大量研发精力、涉及复杂实施流程的技术策略。文中，OpenAI详细介绍了这两项硬核优化的具体方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通常，模型采样的开销远高于网络传输的开销，采样环节会成为优化效率的核心目标，这也是提示词缓存至关重要的原因，它能复用前一次推理调用的计算结果。当缓存命中时，模型采样的时间复杂度将从二次方降至线性。OpenAI相关的提示词缓存文档对这一机制有更详细的说明：仅当提示词存在完全匹配的前缀时，才有可能实现缓存命中。为充分发挥缓存的优势，需将指令、示例等静态内容置于提示词开头，而将用户专属信息等可变内容放在末尾。这一原则同样适用于图片和工具，且其内容在各次请求中必须保持完全一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这一原则，Codex中可能有以下导致缓存未命中的操作：&lt;/p&gt;&lt;p&gt;在对话过程中修改模型可调用的工具列表；更换响应API请求的目标模型（实际场景中，这会改变原始提示词中的第三项内容，因该部分包含模型专属指令）；修改沙箱配置、审批模式或当前工作目录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，Codex团队在为命令行工具开发新功能时，必须审慎考量，避免新功能破坏提示词缓存机制。例如，他们最初对MCP工具的支持曾出现一个漏洞：工具的枚举顺序无法保持一致，进而导致缓存未命中。需要注意的是，MCP工具的处理难度尤为突出，因为MCP服务器可通过notifications/tools/list_changed通知，动态修改其提供的工具列表。若在长对话过程中响应该通知，极易引发高成本的缓存未命中问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在可能的情况下，针对对话过程中发生的配置变更，他们会通过在输入中追加新消息的方式体现变更，而非修改已有的早期消息：&lt;/p&gt;&lt;p&gt;若沙箱配置或审批模式发生变更，我们会插入一条新的role=developer消息，格式与原始的条目保持一致；若当前工作目录发生变更，我们会插入一条新的role=user消息，格式与原始的条目保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，为保障性能，OpenAI在实现缓存命中方面投入了大量精力。除此之外，他们还重点管理了一项核心资源：上下文窗口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其规避上下文窗口耗尽的通用策略是：一旦词元数量超过某个阈值，就对对话进行压缩。具体来说，会用一个更精简、且能代表对话核心内容的新条目列表替代原有输入，让智能体在继续执行任务时仍能理解此前的对话过程。早期的压缩功能实现方案，需要用户手动调用/compact命令，该命令会结合现有对话内容和自定义的摘要生成指令，向响应API发起查询；Codex则会将返回的、包含对话摘要的助手消息，作为后续对话轮次的新输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此后，响应API不断迭代，新增了专用的/responses/compact端点，能以更高效率完成压缩操作。该端点会返回一个条目列表，可替代原有输入继续对话，同时释放出更多的上下文窗口空间。该列表中包含一个特殊的type=compaction条目，其附带的encrypted_content加密字段为透明化设计，可保留模型对原始对话的潜在理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，当词元数量超过auto_compact_limit自动压缩阈值时，Codex会自动调用该端点对对话内容进行压缩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;极限扩容：用1个数据库扛住了8亿用户&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一篇技术博文中，OpenAI工程师Bohan Zhang介绍，&amp;nbsp;OpenAI 通过严苛的技术优化与扎实的工程实践，对单个数据库 PostgreSQL 进行深度扩容，实现以单套体系支撑 8 亿用户、每秒数百万次查询的访问需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据称，多年来，PostgreSQL 一直是支撑 ChatGPT、OpenAI API 等核心产品的核心底层数据系统之一。过去一年，公司 PostgreSQL 的负载增长超 10 倍，且这一增长趋势仍在持续加速。OpenAI称，PostgreSQL 的横向扩展能力远超此前行业普遍认知，能够稳定支撑规模大得多的读密集型工作负载。“这套最初由加州大学伯克利分校的科学家团队研发的系统，助力我们通过单主节点 Azure PostgreSQL 弹性服务器实例，搭配分布在全球多个区域的近 50 个只读副本，承接了海量的全球访问流量。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且，OpenAI表示，其扩容在实现规模提升的同时，始终将延迟控制与可靠性优化放在核心位置：生产环境中，客户端 99 分位延迟稳定保持在十几毫秒的低水平，服务可用性达到五个九标准；过去 12 个月内，PostgreSQL 仅出现过一次零级严重故障，该故障发生在 ChatGPT 图像生成功能爆红上线期间，一周内超 1 亿新用户注册导致写流量突发暴涨超 10 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管 PostgreSQL 的扩容成果已达预期，OpenAI仍在持续探索其性能极限。目前，他们已将可分片的写密集型业务负载迁移至 CosmosDB 等分片式数据库系统；对于分片难度更高的剩余写密集型负载，相关迁移工作也在积极推进，以此进一步减轻 PostgreSQL 主节点的写压力。同时，OpenAI正与微软 Azure 团队展开合作，推动级联复制功能落地，实现只读副本的安全、大规模扩容。随着基础设施需求的持续增长，其将继续探索更多扩容方案，包括基于 PostgreSQL 的分片架构改造、引入其他分布式数据库系统等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有网友评价道，“这招的高明之处，就在于极简。他们没用什么花里胡哨的冷门技术，不过是把最佳实践做到了极致。过去十年，行业里全是 ‘一切皆分片、拥抱 NoSQL、全面分布式，为 CAP 定理折腰’ 的论调，而 OpenAI 倒好， 服务十亿级用户的解法，居然只是一句‘试过加只读副本吗？’”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/unrolling-the-codex-agent-loop/&quot;&gt;https://openai.com/index/unrolling-the-codex-agent-loop/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/scaling-postgresql/&quot;&gt;https://openai.com/index/scaling-postgresql/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lqDSzHGW2eYdUxZQjuYN</link><guid isPermaLink="false">https://www.infoq.cn/article/lqDSzHGW2eYdUxZQjuYN</guid><pubDate>Mon, 26 Jan 2026 09:46:47 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>烧2万亿美元却难用？Gary Marcus狂喷AI赛道不靠谱：推理模型只是“模仿秀”，OpenAI一年后倒闭？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“一圈又一圈的循环融资，投资回报率却不尽如人意，这些AI系统实际用起来也远没有想象中好用，或许方向本身就站不住脚。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，知名AI专家、认知科学家Gary Marcus在一场访谈中愤愤表示，“整个世界都在全力押注神经网络，还在这个我始终觉得毫无道理的理念上投入了巨资，但大语言模型根本无法带我们抵达AGI这一终极目标。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场对话由曾因成功预测2008年金融危机而闻名的传奇投资人、华尔街最具影响力人物之一Steve Eisman发起，他与Marcus共同探讨了当下AI进展的方方面面，包括商业路径、社区现状和未来方向等。Marcus认为，大语言模型已经达到了收益递减的阶段。并且，他指出，现在AI领域根本没有技术壁垒了，所有AI企业的研发思路基本一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于大量人才从大厂离职去办初创公司的现象，Marcus直言道，“如果OpenAI真的能在下周推出AGI，谁会在这个即将改变世界的关键节点离职，去创办一家可能要花四年时间才能做出成果的小公司？显然没人会这么做，大家都会想留在公司见证这个时刻。”在他看来，这些企业内部的人也清楚，他们根本没有做出宣称的那种突破性成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，他认为，OpenAI最终会成为AI领域的WeWork，这家公司原本计划以500亿美元的巅峰估值风光上市、却在一夕之间破产。“我觉得最终OpenAI可能会被微软这样的企业收购。OpenAI每个月的亏损大概有30亿美元，一年就是300多亿美元，即便最近完成了400亿美元的融资，也只够支撑一年的运营。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谈及各家模型的未来，Marcus的预测是，“大语言模型会成为一种标准化商品，各家的模型只会比上一年的版本稍有提升，差距微乎其微，最终品牌差异会变得无关紧要。当产品变成商品后，价格必然下跌。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是详细对话内容，我们在不改变原意的基础上进行了翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;2万亿美元押注Transformer，根本“毫无道理”？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：大家好，我是Steve Eisman。今天我们请到了一位特别的嘉宾，他就是Gary Marcus。他是大语言模型的坚定质疑者，而大语言模型正是整个AI领域的核心根基。接下来，Gary会和我们分享他的观点，聊聊大语言模型到底是什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：谢谢你的邀请，也感谢一两个月前你在CNBC对我的盛赞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：不客气，这都是你应得的。在正式开始之前，我的观众大多还不了解你，不如先和大家说说你的背景，让大家知道你在这个领域发表观点是完全有底气的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我这辈子几乎都在研究智能相关的问题。我10岁学会编程后，就开始涉足AI领域了。我的职业生涯中，很大一部分精力都用在研究自然智能上，比如人类的智能、还有孩子是如何学习语言这类问题。我在MIT的博士论文围绕两个方向展开，一个是儿童的语言学习机制，另一个就是神经网络。神经网络是AI领域的一种特定研究方法，也被用于人类思维的建模，它的设计灵感可以说和大脑有一点松散的关联。这其实是个很巧妙的营销说法，会让人觉得它是完全基于大脑研究的，但事实并非如此，二者只是浅层关联。早年间神经网络就曾风靡一时，我在上世纪90年代就研究过这类模型，发现它们并不能很好地模拟人类的思维方式，但我还是投入了大量精力，想弄清楚它们的实际工作原理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2012年深度学习重新兴起时，我当时就觉得，这些东西我早就研究过了，和我博士论文里的内容高度相似。我在2001年写过一本名为《The Algebraic Mind》的书，在书里我其实就预判到了如今大语言模型出现的幻觉问题，还有一些推理层面的缺陷，这些都是我们今天要探讨的话题。所以当深度学习再次成为热点时，我一眼就看出了其中的诸多问题，对我来说这些问题都很熟悉。2012年，我在《The New Yorker》上发表了一篇文章，标题是《Is Deep Learning a Revolution in Artificial Intelligence?》，我在文中写道：“深度学习确实很有意思，我很佩服Jeff Hinton，他能长期坚持自己的研究方向。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：Jeff Hinton是谁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：他是去年诺贝尔生理学或医学奖的得主，也是深度学习领域的核心奠基人之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：原来如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：他的一些学生，最近也开始认同我的观点了。Jeff Hinton确实是这个领域的大人物，在神经网络一度无人问津的时期，是他一直坚守，这份坚持值得肯定。但当然，他的研究并非全无可议之处，我们这里就不细谈了。他让神经网络重获关注，而更值得你的听众了解的是，真正让这个领域迎来爆发的，是他的学生Ilya Sutskever，或许还有另外几位研究者。他们找到了方法，能让这套研究了许久的系统落地应用。要知道，神经网络的研究最早能追溯到上世纪40年代，Jeff Hinton也在上世纪80年代中期做出了不少重要贡献。而这些研究者发现，借助英伟达研发的图形处理器（GPU），就能实现神经网络的高效运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;彼时的英伟达，生产GPU主要是为了满足电子游戏的需求。这些原本为游戏设计的GPU，核心优势在于并行计算，简单来说，就是能同时处理多个计算任务，而非按顺序逐个完成。传统的中央处理器（CPU），运行软件程序时基本是逐行执行的，虽然现在的技术已经有了改进，但这仍是计算机科学入门课程里会教的基础原理。而GPU能把一个复杂问题拆解成无数个小任务，同时进行处理，它的设计初衷就是为了计算机图形处理。比如要渲染电子游戏的下一帧画面，如果逐行处理，耗时会非常久，而用GPU的话，能同时处理整个画面，一个子处理器负责一个像素点，以此类推。不得不说，GPU在图形处理上的表现堪称完美，我偶尔也玩电子游戏，深知GPU的算力有多惊人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ilya Sutskever，还有另一位我一时想不起名字的论文合作者，他们证明了GPU是运行神经网络的绝佳载体，至于神经网络的具体定义和实际意义，我们之后可以再聊。他们的这一发现，让神经网络的运行实现了两大突破：一是速度大幅提升，二是能处理海量数据。在此之前，六十多年的神经网络研究做出的基本都是些玩具级的模型，而他们证明，借助GPU这项技术能真正实现规模化的实际应用，能在更大的维度上落地。可以说，我们如今看到的所有深度学习成果，都源于2012年的这次突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在这一突破出现后，两件事接踵而至：《The New York Times》刊发了文章，盛赞深度学习的惊人潜力；第二天，我就在《The New Yorker》的博客上发表了文章。我在文中表示，深度学习固然出色，但也存在诸多问题，它注定会在一些领域表现优异，却在另一些领域束手无策。它擅长模式识别和统计分析，这一点毋庸置疑，但人类的认知活动中还有大量的抽象思维过程。比如我们能理解家谱的逻辑，进而对现实世界的相关问题进行推理，而深度学习模型永远无法擅长这类任务，它的架构本身就不适合做抽象推理。从早年对神经网络的研究以及对人类认知机制的研究中，我早就看清了这一点。你应该读过Daniel Kahneman的经典著作《Thinking, Fast and Slow》吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：我读过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：Daniel Kahneman在书中提出了双系统认知理论，他将人类的认知分为系统一和系统二。系统一的思考速度快，是无意识的、基于统计的、本能的反应；而系统二的思考速度更慢，更具思辨性，核心是逻辑推理。神经网络本质上就相当于人类的系统一，这本身没问题，系统一也是人类认知的重要组成部分，但人类的认知还有系统二的部分。尤其是在理性思考时，我们会依赖系统二，进行更审慎、更有逻辑的推理。而神经网络模型，从始至终都不擅长系统二的这类任务，直到现在依然如此。我在2012年就指出，深度学习模型只能实现系统一的功能，却无法完成系统二的思考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在这之后的14年里，整个世界都在全力押注神经网络。这里要说明的是，我们所说的神经网络，就是如今的大语言模型，大语言模型是神经网络的一种形式，抱歉，我之前没明确说明这一点。事实上，2012年时大语言模型还未出现，后续又有不少技术突破，其中关键的就是2017年发表的Transformer论文，这也是大语言模型的起源。而全世界在这一领域的投资规模达到了天文数字，据我粗略估算，已经有1到2万亿美元了，全都投在了这个我始终认为毫无道理的理念上。这些研究者的想法是，只要持续发展神经网络，就能实现智能所需的一切能力，抵达AGI的目标，但他们却忽视了系统二的核心价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始，他们只是把神经网络当成一个巨大的黑箱，直到现在，还有很多人抱着这样的想法。他们觉得，只要把海量数据喂进去，就能得到一个拥有智能的系统，却从未从科学的角度深入思考过真正的智能究竟该具备怎样的架构。我认为这些人太过天真，我也一直试图指出这一点，这也让我成了这个领域里的“孤行者”。很长一段时间里，人们对我的观点不屑一顾，甚至不只是不屑，而是鄙夷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错，他们对你的态度远不止是不屑，而是赤裸裸的鄙夷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我们还能举出很多这样的例子。我对他们的这种态度感到失望，这个话题我们可以聊很久。他们甚至对我公开表现出敌意，比如我了解到，OpenAI内部还为我做了专属的表情包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：我也看到过这个消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：某种程度上，这也算是一种认可吧，既觉得荣幸，又觉得有些离谱，你能看出来，我一直试图用平常心看待这件事。但这也能从侧面说明问题，Sam Altman还在推特上称我为“喷子”。他们就是不想听我的观点，而我核心的观点，都写在了2022年发表的论文《Deep Learning is Hitting a Wall》里。我在这篇论文中指出，当时“规模化扩张”的理念已经开始流行，也就是通过不断投入更多数据、更多GPU，把模型做得越来越大，他们认为只要模型足够大，就会拥有超乎想象的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我先暂停一下，和大家解释下这个“规模化扩张”的理念。他们确实有一些数据能支撑这个观点，但这种想法依然太过天真。我把这种理念称作“万亿磅婴儿谬误”，道理很简单：一个婴儿出生时8磅重，一个月后长到16磅，并不意味着他会一直这样翻倍增长，到上大学时长成万亿磅的巨人。他们就是做出了这样天真的推断，我相信你在商业领域也经常见到这种情况。很多手握巨资的聪明人，都押注了这个理念，他们说，“我们从数据中看到了这样的发展规律，只要投入足够多的数据，就能实现真正的智能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“大模型不会思考，重构信息碎片致幻”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Steve Eisman：先稍停一下，我们倒回去说。大语言模型到底能做什么？这些研发者又认为它们本该实现什么功能？我真想把这个问题彻底讲清楚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：你这个问题问得特别好。大语言模型的核心工作原理，就是预测序列中的下一个内容。你可以想想苹果手机的自动校正功能，原理差不多，虽说那功能有时候能把我逼疯，你继续说。这个功能并非总能生效，核心逻辑就是你在输入句子时，它会预判接下来可能要打的内容。比如你打出“在……见我”，它大概率会推测你想说“在餐厅见我”。它会对人类的语言表达做统计分析，效果还算过得去，但绝非完美，偶尔还会出错，让人恼火，这就是我们说的自动补全。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我把大语言模型称作“超级版自动补全工具”，它们只是用一种特殊的方式完成这种预测，这就是其最本质的功能。它们的运作方式里还有些有意思的点，其中一个就是会把所有信息拆解成细碎的片段，之后再重新整合，这就导致信息之间的关联会被切断。也正是因此，它们才会时不时出现幻觉现象，凭空编造内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：我们稍后再细说幻觉这个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：好，回头再聊。幻觉是这类模型的典型错误之一，早在2001年，大语言模型甚至还没被发明出来的时候，我就指出过这个问题。我当时就说，如果一直沿着这个方向研究下去，必然会出现这个问题，而事实也确实如此。大语言模型把信息拆分成碎片，再通过这些碎片预测后续内容。如果用整个互联网的内容对它们进行训练和数据投喂，它们的表现会好得让人意外，因为几乎任何你能想到的问题，注意，这里的“几乎”是关键，几乎所有问题，此前都有人提出过，也有人给出过答案。从某种程度来说，这些模型就是功能强大的记忆机器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在前几天，《大西洋月刊》还刊发了相关的文章，而且一直以来都有大量证据能证明这一点。比如你输入《哈利·波特》的部分内容，它能直接补完整段文字，本质上就是因为它记住了这些内容。如果一个模型能记住整个互联网的信息，那确实算得上很厉害。比如你问“道奇队在搬到洛杉矶之前，主场在哪”，网上有大量相关表述，它会告诉你是布鲁克林，大概率能给出正确答案。但仅仅依靠这种方式，模型根本无法形成抽象的概念和思想，还会因为信息碎片的拆解和错误整合出现各种问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：那我们现在聊聊幻觉吧。到底什么是AI幻觉？举个例子，再说说出现这种情况的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：幻觉就是模型凭空编造内容，还无比笃定地呈现出来，但这些内容根本不符合事实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：那给我们举个例子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我最喜欢的一个例子和Harry Shearer有关，你可能听过他的名字，看过《摇滚万万岁》吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：当然看过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：他在这部影片里饰演贝斯手，巧的是，他还是我的朋友。他出演了《摇滚万万岁》，还和Christopher J. Guest合作了多部影片，参演了《楚门的世界》，还为《辛普森一家》里的伯恩斯先生等多个角色配音，他的知名度还挺高的，这点对接下来的故事很重要。先倒回说个题外话，我之前遇到的最典型的幻觉案例，主角是我自己。有人发给我一份我的人物简介，里面说我养了一只叫Henrietta的宠物鸡，但我根本没养过，这就是个很典型的幻觉案例，纯粹是凭空编造的。后来发现，有位插画师大概叫Gary Oswald，写过一本关于Henrietta去上学的书，模型不过是把这些碎片化的信息胡乱拼凑在了一起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：那为什么会出现这种幻觉呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：这就和我刚才说的信息碎片化拆解有关了。我再给你讲讲Harry Shearer的那个例子。我总拿宠物鸡Henrietta的事举例，有一天他给我发消息，说他没遇到过宠物鸡这种事，却遇到了和自己相关的幻觉案例。他比我有名多了，至少以前是。我当时也算小有名气，而模型给出的信息里，说他是英国的配音演员和喜剧演员，但他根本不是英国人。你只要花两秒看一下维基百科，就会发现他出生在洛杉矶。他名气不小，你也能在烂番茄、互联网电影数据库上查到他的资料，他接受过很多采访，也聊过自己的成长经历，他小时候还在洛杉矶的《杰克·本尼秀》里当过童星，想找到正确的信息一点都不难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们会错误地把大语言模型当成和人类一样拥有智能的个体，但实际上，它们所做的只是重构信息碎片之间统计层面的大概率关联，所以难免会出错，这种重构过程也常会出现偏差。Harry Shearer这个案例就是如此，模型其实就是在构建一个信息集群，用统计学的方式预测各类信息之间的关联。而现实中确实有很多英国的配音演员和喜剧演员，比如Ricky Gervais、Don Cleeve等等。模型就把这些信息混为一谈了，这种信息融合的方式整体来看效果还算不错，但你永远无法确定它给出的某一个具体信息是准确的，所以幻觉现象才会频繁出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人专门追踪过相关的法律案件，发现律师提交的辩护状里，有很多引用的判例都是模型编造的，根本不存在。我第一次关注这件事时，他已经发现了约300起这样的案件，三个月后再看，数量涨到了600起。这些律师不仅用ChatGPT这类工具代写文书，还因此被法官发现，受到了处罚。模型会出错，而最危险的是，这些错误还很容易被忽略，人们根本发现不了。还有一个例子，CNET是最早用AI写稿的媒体之一，他们首批用AI写的75篇文章里，有近一半都存在错误，编辑们却没发现。因为这些文章语法通顺、格式规范，也没有拼写错误，人们很容易就放松了警惕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我把这种现象称作“看着没问题效应”。大语言模型带来的这种效应，还催生了一个新词汇，我真后悔不是我发明的，叫“低效工作产物”。这个词大概是去年由几位教授提出的，指的是人们用AI写报告、提交给雇主，表面上看没什么问题，实则漏洞百出，因为大语言模型根本不具备真正的理解能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：你的意思是，大语言模型并不会思考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：它们确实不会思考，只是把统计学上大概率关联的内容拼凑在一起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：只是简单拼凑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：没错。我还喜欢用“黏合”这个词，它们只是把信息黏合在一起。从统计学角度来说，大部分内容的拼凑是合理的，但总有一部分是错误的，而这些模型根本无法区分对错，也不会主动告知你。它们永远不会说，“维基百科显示Harry Shearer出生在洛杉矶，但作为大语言模型，我感觉他可能出生在伦敦，你可以去核实一下”。它们从来不会给出这样的提示，只会把所有内容都当作百科全书里的标准答案呈现出来，无论真假，这也是这类模型的危险之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：确实是这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：这类问题其实有很多，这个案例属于另一种情况，但也和模型的本质缺陷有关。这个问题的根源在于，所有大语言模型都有数据截止日期，它们的训练都是在某个特定时间点完成的，核心模型所掌握的信息，也只到这个时间点为止。研发者会给它们加各种补救措施，比如接入网络搜索功能，但这些补救措施和核心模型的融合效果都很差，不同系统的表现略有差异而已。这类模型最大的问题就是无法应对新事物、新情况，也是它们最根本的缺陷。早在1998年，我就通过研究早早发现了这一点。如果一个模型本质上只是个功能强大的记忆机器，当你向它输入一个超出其训练数据范围的内容时，它就会失灵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有个例子特别能说明问题，具体细节我不太清楚，但特斯拉的AI系统也大量采用了这种记忆式的运作方式，而且其系统的复杂程度并不高。有人用过特斯拉的召唤功能，你应该记得马斯克说过，未来可以从纽约远程召唤洛杉矶的特斯拉，但现在显然做不到，不过据说能在停车场里召唤车辆。有人在一场航空展上试过这个功能，你能在油管上找到相关视频。这个人召唤自己的特斯拉，想在航空展上秀一下，结果车子径直撞上了一架价值350万美元的私人飞机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因就是，特斯拉的训练数据里，根本没有教系统如何应对飞机，毕竟谁会专门训练汽车躲避飞机呢？系统对世界没有形成通用的认知，比如“不要撞上挡路的大型贵重物体”，它根本不懂这些，只会识别训练数据里的自行车、行人等目标，它的识别分类里根本没有“飞机”这一项，所以才会直接撞上去。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;所有AI企业都变了：悄悄复用经典符号式工具&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Steve Eisman：那你有没有了解到，随着这场争论的风向转变，各大企业内部现在的情况如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我了解到的情况主要有几点。首先，我一直都在说，单纯的大语言模型行不通，必须结合传统的符号式AI技术。但之前他们都对此嗤之以鼻，觉得这套技术早就过时了，没必要用，还说人脑的工作模式本就不是这样。而现在，他们都悄悄在一定程度上采用了这项技术，比如引入代码解释器来运行Python代码，这些都是经典的符号式工具。说白了，他们正在偷偷把系统二的相关能力融入模型中，只是没有大肆宣扬，但这一改变确实带来了不小的提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克发布Grok 4时的演示就很能说明问题，我还为此写过一篇文章，标题是《为何GPT-3和Grok 4无意间印证了神经符号AI的正确性》。文章里放了当时的演示图表，能清晰看到，正是那些他们不愿提及的符号式工具的加入，让模型的表现变得更好。如今模型的些许提升，绝大部分都来自这个原因，而非单纯的大语言模型优化，他们其实已经悄悄放弃了纯大语言模型的研发思路。而这对你所关注的商业领域来说意义重大，因为这些符号式工具根本不需要在GPU上运行，普通的CPU就足够了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：原来如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：对我而言，从技术角度来说，这印证了我一直以来倡导的研发思路是正确的。这是第一个变化。第二个变化是，各大企业的很多人都离职去创办自己的初创公司了。你可以想想，如果OpenAI真的能在下周推出AGI，谁会在这个即将改变世界的关键节点离职，去创办一家可能要花四年时间才能做出成果的小公司？显然没人会这么做，大家都会想留在公司见证这个时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，大量人才离职的事实就说明，这些企业内部的人也清楚，他们根本没有做出宣称的那种突破性成果。还有一个变化，就是谷歌正在迎头赶上。就像我几年前在Substack专栏里预测的那样，因为现在所有企业的研发思路基本一致，这个领域根本没有技术壁垒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错，完全没有技术壁垒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：你和其他一些人都认为，如果所有人都在做大语言模型的规模化扩张，那么最终的赢家就是最有实力承担这笔扩张成本的企业。而放眼整个行业，谁的资金实力能超过谷歌？根本没有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：确实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我其实也表达过类似的观点，只是表述略有不同，你的这个说法其实也没错。我当时的观点是，行业头部企业会逐渐趋同，而随着大语言模型成为标准化商品，行业内会引发价格战，服务定价会大幅下降。事实也确实如此，现在大语言模型的按token计费价格，已经暴跌了99%。价格战确实爆发了，而最终的受益者自然是谷歌，这一点我当初虽然没有直接点明，但也有所预判。我大概是在2024年3月，也可能是2023年8月开始写相关文章，当时就说，所有企业都在遵循同一种研发思路，没人掌握什么独门绝技，这就意味着头部企业的产品会越来越趋同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大语言模型会成为一种标准化商品，各家的模型只会比上一年的版本稍有提升，差距微乎其微，最终品牌差异会变得无关紧要。这一趋势带来的结果就是，谷歌迎头赶上了，中国的企业也追上来了，Anthropic同样不甘落后。就像你说的，当产品变成商品后，价格必然下跌。这对终端消费者来说是好事，但对企业的商业模式来说却是巨大的打击。毕竟企业原本的设想是，花巨资采购GPU，然后靠模型服务赚回巨额利润。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;推理模型进行不了逻辑分析，再升级也没价值？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Steve Eisman：我们能不能聊聊推理模型？先给我的观众解释一下，推理模型和大语言模型有什么区别？推理模型是基于大语言模型研发的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：推理模型是在大语言模型的基础上运作的，但它不会像大语言模型那样直接给出第一个想到的答案，而是会反复迭代、花费时间去推敲，试图得出最优解。至于具体的研发细节，各家企业都没有公开太多。传统的神经网络模型，在某种意义上都是一次性输出结果的，当然现在行业内对“一次性”的定义有所不同。简单来说，就是把数据输入模型后，神经网络会立刻完成一次正向传播，粗略来讲，模型中的每个神经元都会处理信息并生成对应的结果。而推理模型则会进行多次传播，这是本质上的区别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有个朋友把传统模型的输出方式称为“恒时推理”，意思是模型生成答案的时间基本固定，无论什么问题，耗时都相差无几：把数据输入模式识别器，模型会根据现有的模式给出最优解。而推理模型采用的是全新的“变时推理”模式，我之后会聊聊它的适用场景和短板，这种模式的特点是，处理不同的问题，耗时会有所不同。目前还没有企业能完全解决推理模型的所有技术难题，但在一些场景下，它的表现确实不错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据我了解，推理模型的研发思路之一，就是让模型模仿人类解决问题的思考过程，毕竟这些模型本质上都是模仿系统。比如在解决几何题或代数题时，模型会刻意模仿人类的解题步骤。人类解决这类问题需要一步步推导，融合了推理能力的神经网络模型，同样需要分步骤完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：那推理模型的优势是什么？又有哪些明显的短板？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：在回答这个问题之前，我想先提一点：推理模型的成本天生就更高，因为它需要占用GPU更长的时间来生成答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：好的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：那我来说说它的适用场景和短板。推理模型最擅长的，是那些能生成形式规范、可验证的数据来训练模型的领域。比如数学和计算机编程，我们可以编写程序生成各种不同的代码片段来训练模型，也能生成各类几何证明题的解题思路。这类领域之所以适合推理模型，是因为它们都属于封闭领域，相关的知识边界是明确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错，数据库中的知识量和相关的有效知识量都是有限的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：对，就是这个意思。所以推理模型在几何、编程这类领域的表现最好，而在开放式的现实世界中，它的表现就差强人意了。我总会从你所熟悉的金融领域举例子，当然你肯定有更贴切的案例，比如长期资本管理公司的破产。其实那也是一种模型失效的情况，只是模型的原理不同，当时没人考虑到俄罗斯债券市场崩盘的可能性，最终导致美国金融市场出现了大幅动荡。这是因为当时的金融模型，其参数设定根本没有覆盖这类极端情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现在的推理模型，也面临着类似的问题：它其实并不具备真正的思考能力，哪怕是关于债券的基本问题，它也无法进行真正的逻辑分析。如果用它处理的问题，和训练数据中的内容高度相似，那一切都顺理成章；但一旦超出了它的认知范围，就像我们之前聊到的特斯拉的例子，模型就会立刻失效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：也就是它依然无法应对新事物、新情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：没错，即便升级到了新的推理模型，核心问题依然是无法处理未知信息。它只是在原有基础上做了些许改进，但本质上还是受限于对新事物的适配能力。而关键问题在于，现实世界中，大多数有价值的问题都包含着一定的新要素、新情况，并非全是已知的问题。当然，也有例外，我们确实可以用这种不擅长处理新事物的技术，在一些狭窄的领域做出成绩，比如国际象棋和围棋。这些领域的规则千百年间基本没有太大变化，有海量的历史数据可供参考，模型还能通过自我对弈生成更多训练数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在开放式的现实世界中，比如政治、军事战略领域，永远会出现训练数据中没有的新情况。比如，如何应对一位总统授意将军用飞机伪装成民用飞机，去袭击另一个国家的行为？这种情况此前从未发生过，想要分析这类问题，根本无法依靠过往的数据，必须依靠抽象的概念思考，比如权力、外交规则、国际格局的构建逻辑等，这些都是相关领域的学者更擅长的内容。要做到这一点，模型需要接受正确的训练，具备抽象思维能力，而不是单纯依赖数据。即便是在商业应用中，比如看似简单的客户服务，也会遇到类似的问题：用户总会用全新的方式提出问题，而一旦出现这种情况，模型就会因为无法应对新情况而失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;OpenAI只够支撑一年，要么倒闭、要么求救微软？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Steve Eisman：假设我任命你为AI领域的总负责人，由你掌控所有相关企业，指导整个行业的研发方向。如果你把这些企业的负责人都召集到一起，你会告诉他们，想要实现真正的突破，需要做些什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我会告诉他们，整个行业需要更多的学术思维多样性。就像在你的金融领域，你会告诉人们不要把所有鸡蛋放在一个篮子里，要做资产配置，分散投资股票、债券、黄金、房地产等。而AI领域在过去这些年，就是把所有的精力都押在了一个思路上，大语言模型的规模化扩张，这是行业唯一的研发方向。不可否认，这个思路确实带来了一些成果，模型并非毫无用处，我们也确实能利用它解决一些问题，但它终究无法带我们实现所谓的通用人工智能（AGI）这一终极目标，而且这还是一种成本极高、效率极低的研发方式。你可以对比一下，我的孩子只需要少量的信息和学习，就能理解这个世界，而大语言模型却需要学习整个互联网的海量数据，二者的效率差距简直可笑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些企业花费巨资，做出的却是效率低下、可靠性堪忧，但又有一定使用价值的模型。我们需要的是其他更高效、更经济、更可靠的研发思路，企业应该投入资金去探索这些新方向。但问题的根源，其实也来自你所熟悉的金融领域：风险投资家能从那些听起来合理的投资项目中，赚取2%的管理费。我很好奇你对这个观点的看法，因为这毕竟是你的专业领域。试想一下，作为风险投资家，如果有一个项目能让你管理一万亿美元的资金，哪怕你根本不在乎项目最终的结果，也能赚到2%的管理费，这足以让你成为亿万富翁。我并不是说所有的风险投资家都是这样想的，我见过很多投资人，他们确实真心想推动技术进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但就像任何行业一样，很多投资人都带着功利的心态。对这些功利的投资人来说，最理想的投资标的，就是那些听起来前景广阔、无需真正落地、成本极高的项目，这样他们就能赚取巨额的管理费。我认为，这就是整个行业都沉迷于规模化扩张的原因：投资人能从中赚取不菲的管理费，而且数额极其可观。但从学术研究的角度来说，这绝不是正确的选择，最终也没有带来理想的结果，反而造成了巨额的资金浪费。风险投资家赚走了管理费，而那些有限合伙人，最终会损失大量的资金。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：你是不是觉得，这个行业的泡沫快要破裂了，还是说现在根本没法判断？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：其实炒股的那句老话你我都懂，市场保持非理性的时间，可能比你保持偿付能力的时间还要长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我去年用一个比喻形容当下的情况，就像《兔八哥》里的歪心狼跑到了悬崖边，它不往下看，就不会掉下去。当然这不符合物理规律，但很有意思。而现在，你所在的投资圈里，已经有人开始往下看了。我觉得从去年11月开始，就不断有投资人说，他们看到了一圈又一圈的的循环融资，投资回报率却不尽如人意，这些AI系统实际用起来也远没有想象中好用，或许这个赛道本身就不靠谱。我个人觉得，英伟达的产品做得非常出色，生态体系也很完善，不只是芯片本身，配套的软件等方方面面都很好。我见过黄仁勋，他给我留下了很深的印象，英伟达的产品确实很棒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但问题的关键是，他们最终能卖出多少芯片？我认为，目前的芯片销售全靠市场投机，大家都在赌，我稍后再说说其他人的看法。所有人都在投机，认为这类芯片的需求会无限大，而这种投机的底层逻辑，是相信这些AI模型最终能实现AGI。真正的AGI能完成人类能做的所有事，其商业价值不可估量，每年创造数万亿美元的价值都有可能。但《华盛顿邮报》几天前报道了一项一个月前完成的研究，研究显示，人类日常的工作中，只有2.5%的工作能真正由AI系统完成。所以人们幻想中AI能完成的大部分工作，其实它都做不到，也根本做不好。这就意味着，最终所有在芯片上的投资，都会变得毫无意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在这些企业里，OpenAI可能是最脆弱的那个。OpenAI有超过一万亿美元的未兑现承诺，却从未实现过盈利，如今又身处一个产品高度同质化的市场。它最大的竞争对手谷歌已经迎头赶上，甚至可以说实现了反超，还拿下了和苹果的合作大单，这可是笔大生意。所以我觉得OpenAI现在已经手忙脚乱了，实在看不出它的估值有任何合理性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：对我所在的投资圈来说，如果投资人开始从OpenAI撤资，而它又融不到新的资金，那会给整个生态系统带来连锁反应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：没错，这正是我认为即将发生的事。我觉得最终OpenAI可能会被微软这样的企业收购。我这几年一直说，OpenAI最终会成为AI领域的WeWork。未来人们都会疑惑，它当初怎么会有那么高的估值，这完全不合逻辑。OpenAI的年收入只有几十亿美元，却每个月亏损数十亿美元，还有众多竞争对手，这样的企业根本撑不下去。如果投资人撤资，或者不再继续注资，OpenAI就会陷入巨大的危机。它每个月的亏损大概有30亿美元，一年就是300多亿美元，即便最近完成了400亿美元的融资，也只够支撑一年的运营。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错，也就一年的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：而且现在很多人都在持观望态度，他们会觉得，谷歌才是更适合这场竞争的玩家，毕竟谷歌已经追上来了。如果这场竞争只拼规模，那赢家必然是谷歌，这是毋庸置疑的。谷歌有能力做出巨额投入，甚至根本不需要英伟达的芯片，因为他们自研了张量处理单元，能实现类似的功能，所以谷歌的抗风险能力更强。他们有稳定的财务支撑，最终一定会赢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：只要有一部分人意识到，OpenAI想要活下去，需要的资金量是天文数字，它的处境就会变得岌岌可危。它下一轮可能需要1000亿美元的融资，而全世界能拿出这么多钱的人，可能也就五个。就算其中四个愿意投资，只要有一个拒绝，就会出问题；而如果五个都拒绝，它要么倒闭，要么只能去找微软求救。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“脱离世界模型做AI，根本行不通”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Steve Eisman：Gary，在我们结束访谈前，还有什么我该问却没问的问题吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：我觉得这次访谈特别棒。要说还有什么重要的点没聊到，那应该就是“世界模型”这个概念。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：没错，我本来也想聊这个。你一直说我们需要构建世界模型，这个概念完全超出了我的专业领域，不如你给大家解释一下，到底什么是世界模型？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：不同的人对世界模型有不同的定义，简单来说，它就是在计算机系统中，构建一个能表征外部现实世界的体系。我说说我认为我们需要的世界模型是什么样的：软件内部需要有一个结构，能对应现实世界中的各种事物。比如导航系统的世界模型，需要能表征道路的分布、连接方式，以及不同路段的通行时间。在传统的AI领域，世界模型是研发的起点，所有的研究都基于此，没人会想过脱离世界模型做研发。Herbert Alexander Simon是上世纪50年代AI的奠基人之一，他写过一本自传叫《Models of My Life》，他一生都在研究各类模型和世界模型，并且认为，做好AI的关键就是构建正确的世界模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而大语言模型却试图脱离世界模型运作。构建一个针对特定事物的世界模型，尤其是复杂事物，需要付出巨大的努力。比如过去研发专家系统时，研究者需要构建能模拟医生思考方式的模型，能表征病人身体机能、生理结构的模型，这个过程非常繁琐。当时还有一个专门的领域叫知识工程，做这项工作成本极高，没人愿意做。大语言模型和其他类型的神经网络出现后，研发者宣称，不用再做这些繁琐的工作，只需要让系统从数据中自主学习就行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但事实证明，这根本行不通。就像大语言模型会把出生在洛杉矶的Harry Shearer说成是伦敦人，原因就是它没有一个完善的世界模型，无法像设计精良的软件那样，精准调取正确的信息。所以我们必须在AI系统中融入世界模型，才能避免幻觉现象的发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：我还是不太理解世界模型到底是什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：用非专业的语言解释确实有难度，简单说，它就是对世界的一种表征，而且这个“世界”不一定是现实世界。比如我们对《星际迷航》《星球大战》《哈利·波特》这些虚构世界，也会有对应的世界模型。这也是人类和当前AI系统最本质的区别：当我们看一部电影、读一本书时，会在脑海中构建出这个世界的运行规则，并且能判断情节是否符合这个世界的逻辑，会不会有不合理的设定。比如看了《哈利·波特》，我们会知道里面的人能骑着扫帚飞，但不会把这个设定和现实世界混淆，不会回家后跳上扫帚就想从窗户飞出去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人类能快速构建并同时掌握多个世界模型，就算看一部新的科幻剧，20分钟左右就能理解这个全新世界的规则，这是人类的天赋。但在AI领域，无论是传统的符号式AI，还是现在的大语言模型，都做不到这一点。传统AI的优势是可以人工构建世界模型，你可以雇一群学者花六周时间，把一个问题的相关规则梳理清楚，构建成模型。最近离世的顶级研究者Doug Lenat就做过这样的研究，他为《罗密欧与朱丽叶》构建了世界模型，他的系统能真正理解这部剧的关键情节，而非从网上的读书笔记中获取二手信息，表现非常惊艳。但问题是，我们不知道该如何让传统AI自主学习、构建世界模型。而大语言模型则完全做不到构建世界模型，只是在假装自己能做到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有个很经典的例子，就算用整个互联网的内容训练大语言模型，让它接触海量的国际象棋规则和对局记录，它依然会走出违规的棋步，因为它从未真正抽象出国际象棋的运行逻辑。这一点就足以说明问题了。试想一下，一个人看了一百万盘象棋对局，读了维基百科、象棋网站上的所有规则，还看了Robert James Fischer的象棋著作，不可能连基本的棋规都掌握不了，但AI就是做不到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们需要研发能自主归纳出世界模型的AI系统，这类系统能从数据中挖掘因果规律，识别其中的核心要素。这是一个难题，不是说有人明天回家鼓捣一下就能解决的。长期以来，无论是传统AI还是大语言模型，都在回避这个问题，而现在，我们必须直面它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Eisman：看来这需要很长的时间来研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gary Marcus：确实需要很久。我想说的是，AI确实会以我们难以想象的方式改变世界，但绝不是现在，靠当下的这项技术根本做不到。我们需要把这一点考虑进去，做出合理的投资决策。现在的问题是，我们到底是在投资基础研究，还是在为一项已经成熟的技术做规模化投入？答案显然是后者。而当下的市场，大多是在投机，赌那些目前行不通的技术，只要做得更大，就能凭空实现突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但事实上，单纯的规模化根本解决不了这些核心问题，我们真正需要的是扎实的基础研究。这是我过去五年一直强调的观点，也是SSG在去年11月提出的观点，而Ilya Sutskever也表达了类似的看法。当我们这些背景截然不同的人，都达成了这样的共识，行业内的人其实应该认真听一听。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aI7XknJJC5Q&quot;&gt;https://www.youtube.com/watch?v=aI7XknJJC5Q&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/GsFzDHz763fGsSl2Nl8j</link><guid isPermaLink="false">https://www.infoq.cn/article/GsFzDHz763fGsSl2Nl8j</guid><pubDate>Mon, 26 Jan 2026 09:36:36 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>慕了！内存芯片巨头年终奖人均64万；32岁程序员猝死背后公司被扒，曾给39万“封口费”；马斯克曝星舰成本将降99%，商业航天受捧｜AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;慕了！内存芯片巨头人均发 64 万年终奖；月之暗面总裁张予彤：Kimi 仅使用美国顶尖实验室 1% 的资源；32 岁程序员猝死背后公司：不配合工伤认定，曾给 39 万“封口费”；黄仁勋现身上海菜市场；大清洗！大众裁员 3.5 万人，包括 1/3 高管；微信聊天不能导出和分析；腾讯回应开源项目被下架；苹果 Siri「偷听」集体诉讼和解，美国用户开始获赔 9500 万美元；传阿里旗下芯片公司平头哥拟独立上市；马斯克：星舰今年目标全复用，成本将下降 99%；TikTok 宣布“美国方案”：成立数据安全合资公司，字节保留算法知识产权；黄景瑜成为中国首批商业航天太空旅客……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;慕了！内存芯片巨头人均发64万年终奖&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月19日，据韩媒报道，全球内存芯片行业的三巨头之一SK海力士宣布，将向全体员工发放人均超1.36亿韩元（约合64万元人民币）的绩效奖金，创公司历史最高纪录。据报道，SK海力士为员工提供以公司股票形式领取绩效奖金的选项，即“股东参与计划”。根据股东参与计划，员工可以选择将其年终奖的最多50%以公司股票形式领取。持有这些股票满一年的员工，将获得相当于购买金额15%的额外现金奖励。例如，一位获得1亿韩元年终奖的员工如果选择顶格持股，将获得价值5000万韩元的股票。若持有一年，该员工还将额外获得750万韩元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2024年，由于半导体行业低迷，SK海力士推出了该计划。但由于当年仅发放了慰问金而未发放绩效奖金，因此当时未能实施。该计划从去年开始向员工提供购股选择。随着去年下半年劳资双方达成的新指南于本月底生效，SK海力士员工的奖金金额预计将大幅增加。根据新协议，此前绩效奖金发放最高限额为1000%（即10个月基本工资）的规定已被废除。取而代之的是，以上一年度营业利润的10%作为年终奖，其中80%的绩效奖金在当年发放，剩余20%分两年递延发放，并享受每年10%的利息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;鉴于SK海力士去年全年营业利润预计为45万亿韩元，员工总数为3.3万人，因此预计每位员工的绩效奖金约为1.36亿韩元（约合64万元人民币）。不过，“股东参与计划”也存在潜在变数。SK海力士在公告中提示，韩国国会正在推进《公司法第三次修订案》审议，其中明确要求企业回购的自有股必须注销。若法案在本月或3月正式通过，企业将无法用自有股开展员工激励，本次持股计划或面临调整甚至取消。相关修订案将于1月21日进入议案审查小组讨论环节。行情数据显示，得益于人工智能热潮，SK海力士的股价在2025年内涨幅达275%。同时，今年年初，SK海力士就表示其2026年的全部芯片产能已售罄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;月之暗面总裁张予彤：Kimi仅使用美国顶尖实验室1%的资源&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日，月之暗面Kimi总裁张予彤出席在瑞士达沃斯举行的世界经济论坛2026年年会时表示，Kimi仅使用美国顶尖实验室1%的资源，就开放出Kimi K2、Kimi K2 Thinking这样全球领先的开源模型，甚至在部分性能上超越美国的顶尖闭源模型。张予彤透露，Kimi投入大量精力将工程化思维引入研究环节，确保所有算法创新都能在生产系统中大规模稳定运行，据她透露，Kimi最新模型将很快发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月19日，据外媒报道，两位知情人士透露，月之暗面Kimi在最新一轮融资中估值达到 48 亿美元（约合330亿元人民币），而就在几周前，该公司的估值还为 43 亿美元。消息人士称，由于市场需求旺盛，此轮融资可能很快就会完成。截至本文发布时，月之暗面尚未回复置评请求。消息人士补充说，由于市场对中国 AI IPO候选企业的兴趣激增，该公司在后续几轮融资中的估值可能会更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;32岁程序员猝死背后公司：不配合工伤认定，曾给39万“封口费”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年11月29日，32岁程序员高广辉（视源股份任职7年）因长期工作强度大、频繁熬夜，在身体不适晕倒送医后抢救无效死亡，病历标注其“经常熬夜，工作强度大”。事发当日，他送医前仍叮嘱妻子“带上电脑”计划住院工作，急救期间及离世8小时后仍收到同事工作消息，当日曾5次登录公司OA系统，但公司隐藏后台时间，给工伤认定带来阻碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高广辉的工作状态极具代表性：公司实行“弹性工作制”却无加班费，他需同时承担研发、管理、售后、拉业务等多项职责，入职7年底薪始终3000元，依赖“多劳多得”维持税前2.9万元月薪，且面临每季度末位淘汰压力，长期深夜甚至凌晨回家成为常态。他曾在GitHub开源“反996”项目，提醒同行“不要被任务压垮”，其离世后该项目被大量悼念留言刷屏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事发后，视源股份的处理引发争议：6天后向家属支付39万元“人道主义抚恤金”，实则附带“负面评价需支付50万违约金”的“封口费”条款；删除高广辉企业微信工作号、撤掉工位、丢失部分遗物，并要求全体员工不得提及此事，且不配合工伤认定相关调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“大概就是炒掉我老公的N+1钱，其实就是封口费，说如果我对公司造成负面评价违约金50万，当时被逼的没办法所以签了。”高广辉妻子对凤凰网说道，后续因为高父想要提前分遗产，分走了三十多万元，“其实（这笔钱）我只拿到四到六万。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，视源股份是一家业务涵盖教育、办公、AI、汽车电子等多领域的上市公司，技术人员占比超58%，资料显示“无刚性考勤、人性化管理”，且福利优厚，但背后是普遍的强制加班文化。前员工反映，加班至深夜是常态，休假、就医需随身携带电脑，拒绝加班会影响绩效，部分员工因长期高压出现躯体化症状。公司近年盈利持续下滑，正推进H股上市，业务扩张压力最终转嫁至一线员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前高广辉的工伤认定仍在调查中，律师表示，若能提供工作记录、加班文化等证据证明猝死与工作高度相关，可认定为工伤，居家办公也可视为工作时间延伸。高广辉家属希望此事能敲响职场警钟，推动职场环境改革，避免类似悲剧重演。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄仁勋现身上海菜市场&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨天，黄仁勋现身陆家嘴街道乳山路锦德菜市场，体验上海市井风俗。此前，多位知情人士透露，黄仁勋再度来华，首站到访了英伟达位于上海的新办公室，与员工见面并回答了诸多员工关注的问题，同时回顾公司2025年主要事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大清洗！大众裁员3.5万人，包括1/3高管&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日，德国汽车制造商大众汽车周三表示，其核心品牌集团计划削减管理岗位并整合生产平台，目标是到2030年节约10亿欧元。该公司在声明中表示，计划在2026年夏季前将大众核心品牌集团的董事会成员数量减少约三分之一。报道称，这意味着董事会职位将从29个减少到19个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司补充说，大众乘用车、斯柯达和西雅特/Cupra等品牌未来将各有四名董事会成员——一位首席执行官，加上财务、销售和人力资源主管--而开发、采购和生产将由位于沃尔夫斯堡的汽车制造商总部处理。它表示，将在中期内进一步逐步精简核心品牌集团内部的管理结构。该公司表示，核心品牌集团的20多家全球运营工厂将被组织成五个生产区域，区域经理将承担跨品牌和跨国家的责任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大众汽车正努力应对工业放缓、来自中国的激烈竞争和昂贵的关税，到2030年将在德国削减3.5万个工作岗位。媒体曾报道称，大众核心品牌集团首席执行官托马斯·谢弗预计的节约成本由6亿欧元的人力成本和4亿欧元的生产效率组成。大众汽车的此次改革，远不止于财务上的“节流”。它标志着集团在向电动化与数字化转型的关键时期，正试图重塑其庞大的管理体系——通过削减层级、整合资源与强化区域协调，构建一个更敏捷、更高效的运营模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;&amp;nbsp;&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;微信聊天不能导出和分析？腾讯回应开源项目被下架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日下午消息，近日在小红书等社交媒体上，有用户发帖表示，腾讯要求全球最大代码托管与协作平台GitHub全面下架一批微信开源项目，相关项目主要涉及“允许用户导出或分析自己微信聊天记录”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一话题引发网友讨论，部分网友质疑表示，像导出个人聊天记录、清理微信缓存这类基础功能工具，本该是提升用户体验的标配，可微信不仅自己不提供，还不允许用户自主优化使用体验。这是否说明用户对微信数据没有自主权。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，腾讯方面回应称，部分读取微信聊天记录的开源项目，是通过对微信客户端进行逆向工程等手段，破解本地数据库的密钥，以绕过微信客户端的加密措施，威胁用户本人及第三方数据隐私与客户端安全，且极易被黑灰产利用。腾讯依据相关法规向开源平台提出请求，并获得了平台支持，大部分项目也在平台沟通后选择主动移除违规内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;苹果 Siri「偷听」集体诉讼和解，美国用户开始获赔 9500 万美元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;苹果去年年初就 Siri「非法且故意录音」集体诉讼达成和解。该事件可追溯至 2019 年，苹果曾否认指控并加强隐私保护。从去年年中起受影响用户可提交索赔申请，如今部分用户已陆续收到赔付款。用户需在 2014 年 9 月 17 日至 2024 年 12 月 31 日购买过支持 Siri 的设备且经历过「非预期激活 Siri」才能申请。本次赔付总价值 9500 万美元，每名用户最多申报 5 台设备，最初预计每台赔 20 美元、单人最多 100 美元，最终每台约 8.02 美元、最多 40.1 美元。成功参与和解的用户，直接转账的赔付款从前天开始陆续到账，选择支票 / 礼品卡形式的需留意邮箱或实体邮箱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;传阿里旗下芯片公司平头哥拟独立上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日下午消息，接近市场人士称，阿里巴巴集团已决定支持旗下芯片公司平头哥未来独立上市。平头哥是阿里巴巴集团旗下全资芯片公司，2018年成立以来，在行业中非常低调，是阿里雪藏多年的“核武器”。如今平头哥芯片正式浮出水面。阿里方面对此消息未作评论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;截至目前，平头哥在算力芯片领域推出AI推理芯片含光800、CPU倚天710以及AI芯片PPU，在存储芯片领域推出SSD主控芯片镇岳510，在网络芯片领域据称也将推出相关芯片，已布局数据中心全栈芯片。平头哥还在端侧芯片推出羽阵IoT芯片，已实现数亿出货，布局覆盖云端和终端。此外，2026年1月2日，百度集团宣布，昆仑芯已于1月1日通过其联席保荐人向香港证券交易所提交了上市申请表。在拟议的分拆完成后，昆仑芯预计仍将作为子公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克：星舰今年目标全复用，成本将下降99%&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年1月22日，特斯拉首席执行官埃隆·马斯克在达沃斯世界经济论坛与贝莱德首席执行官拉里·芬克交谈时，阐述了其在太空探索、人工智能、机器人及自动驾驶领域的愿景与规划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在太空探索领域，马斯克表示SpaceX今年的目标是实现“星舰（Starship）”的完全可重复使用。作为有史以来最大的飞行器，星舰若达成该目标，将使进入太空的成本降至当前的1%（每磅100美元以下）。此外，他还提及未来两到三年计划发射太阳能驱动的人工智能卫星，由于太空中太阳能持续充足且无大气干扰，太阳能电池板效率将是地球的五倍，未来太空或将成为部署人工智能成本最低的地方，而此前他曾提出未来四到五年内将大规模人工智能系统部署到轨道的设想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在机器人领域，马斯克透露特斯拉计划明年年底向公众销售人形机器人Optimus，目前该机器人已在工厂执行简单任务，预计今年年底将具备更复杂功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自动驾驶方面，马斯克称自动驾驶汽车“基本是已解决的问题”，特斯拉“全自动驾驶”软件每周都会更新，部分保险公司已为使用该技术的客户提供半价保险。目前特斯拉已在美国多个城市推出自动驾驶出租车服务，计划今年年底在美国大规模推广，欧洲则有望下月获得全面自动驾驶监管批准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，马斯克此前在社交媒体X上表示，Cybercab（无人驾驶出租车）和Optimus的生产速度将呈S型曲线逐步加快。他还展望，今年年底或最迟明年，人工智能将“比任何人更聪明”，其公司的总体目标是“最大限度提高人类文明拥有美好未来的可能性”，并“将意识扩展到地球之外”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;资本市场上，商业航天概念板块热度颇高，价格波动情况受到市场广泛关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TikTok 宣布“美国方案”：成立数据安全合资公司，字节保留算法知识产权&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;北京时间 1 月 23 日，TikTok 发布公告称已成立 TikTok 美国数据安全合资有限责任公司，负责 TikTok 美国的数据保护等业务，字节跳动继续拥有 TikTok 算法知识产权并授权其使用。TikTok 美国公司由字节跳动全资控股，负责电商等商业活动及全球产品互联互通。这意味着 TikTok 美国方案正式落地，超 2 亿美国用户可继续使用。合资公司中，甲骨文、银湖资本、MGX 各持股 15%，字节跳动保留 19.9% 股份为最大单一股东，由七人董事会管理。TikTok 美国公司保障产品全球内容互联、体验一致，其商业活动是重要收入来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄景瑜成为中国首批商业航天太空旅客&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日，黄景瑜以「009 号太空游客」身份官宣成为中国首批商业航天太空旅客，计划 2028 年搭乘国产可重复使用载人飞船飞赴亚轨道太空，同行者包括中国工程院院士李立浧等人。他们将乘坐亚轨道载人飞船穿越卡门线，体验至少 5 分钟失重并俯瞰地球。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据穿越者此前消息，穿越者已开启船票预售，预售船票300万/张，预付10%可锁定名额，目前已签约来自学界、商界、航天界、艺术界、娱乐界、网红界等领域的十余位付费太空游客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智元机器人CMO邱恒是“中国001号商业航天员”。邱恒于2023年自费购买中国首张商业航天“太空船票”，成为首位签约亚轨道飞行的普通人。他表示，人形与四足机器人能创造无限生产力，这样的生产力正加速服务工业、商业，未来也有可能助力人类拓展新的疆域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年11月，穿越者与首位太空游游客签约。据穿越者官方消息，目前穿越者载人飞船的签约游客有中国工程院院士李立浧、旅美诗人林小颜、广州正佳集团副董事长兼首席执行官谢萌、星河动力CEO刘百奇、航天垂类大V博主@NASA爱好者等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月18日，穿越者自主研制的穿越者壹号（CYZ1）载人飞船试验舱，完成着陆缓冲系统的综合验证试验。穿越者介绍，这是我国商业航天领域首个载人飞船全尺寸试验舱着陆缓冲关键技术验证项目，此次试验的成功，标志着穿越者已成为全球第三家研发并验证了载人飞船着陆缓冲技术的商业航天企业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;穿越者官方资料显示，穿越者成立于2023年1月，是中国首家商业的“载人航天科技”有限公司，专注可重复使用载人飞船研制和太空旅游运营，计划3-4年首先完成亚轨道可重复使用载人飞船研制，2028年前后实现中国乃至亚洲的太空旅游。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 计划于今年下半年推出首款硬件设备&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 19 日消息，据 Axios 网站报道，OpenAI 政策主管克里斯 · 莱恩周一在达沃斯论坛透露，公司“正按计划”于 2026 年下半年推出其首款设备。OpenAI 首席执行官萨姆・奥特曼和苹果前首席设计师乔尼・艾维去年 5 月份曾透露，该公司正在研发一系列硬件产品，有望于 2026 年正式亮相。彭博社记者马克・古尔曼去年 11 月曾爆料，OpenAI 正在从苹果的硬件工程团队中大肆挖人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，其合作开发的神秘 AI 硬件已拥有首个原型机，奥尔特曼称原型机的表现“令人惊叹”，并表示项目正按预期推进，并透露该产品将在两年内投入生产。在本次活动中，两人阐述了该设备的核心理念：彻底改变人们使用计算机的方式。他们认为，当前的智能设备如同“走在纽约时代广场”，充斥着各种干扰信息，无法带来平静。因此，这款新设备旨在创造一种“坐在湖边小屋”般的宁静体验，让用户能专注于真正重要的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;DeepSeek 新模型“Model 1”曝光，疑似“高效推理模型”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 21 日下午消息，DeepSeek 于官方 GitHub 仓库更新了一系列 FlashMLA 代码，在这些更新中，一个名为“Model 1”的模型引起了广泛关注。据悉，目前这个还很神秘的 Model1 不仅出现在了代码与注释中，甚至还有与 DeepSeek-V3.2 并驾齐驱的文件。这也不禁引发广大网友猜测，认为 Model 1 很可能就是传闻中 DeepSeek 将于春节前后发布的新模型代号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最新消息显示，Model1 是 DeepSeek FlashMLA 中支持的两个主要模型架构之一，另一个是 DeepSeek-V3.2。这很可能是一个高效推理模型，相比 V3.2 内存占用更低，适合边缘设备或成本敏感场景。此外，它也可能是一个长序列专家，针对 16K+序列优化，适合文档理解、代码分析等长上下文任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里推出 AIGC 设计应用“呜哩 (Wuli)”，集成通义千问图像模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，阿里巴巴推出了一款名为“呜哩”的 AIGC 创意设计生产力平台，并已正式开启测试。该平台旨在为内容创作者、设计师及营销人员提供一套高效多元的 AI 创意生成解决方案。平台深度整合了通义千问团队研发的多款图像大模型，形成一个模型全家桶。其中包括主打高质量的 Qwen Image25.12 生成模型、追求极致响应速度的 Qwen Image Turbo 模型，以及专注于细节调整的 Qwen Image25.11 编辑模型。用户可根据不同创作需求，在生成质量、速度与可控性之间灵活选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在功能层面，呜哩平台提供了从图片生成、视频生成到灵感联想、翻译辅助及资源库支持的完整具集，可帮助用户跨越创意瓶颈。用户通过输入简单的描述，即可快速生成如 3D 艺术字体、电影风格海报、电商场景图在内的丰富内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;百川推出最低幻觉循证增强医疗大模型M3 Plus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日，百川智能正式发布Baichuan-M3 Plus，严肃医疗场景下的问答准确性、可靠性，再次刷新了刚刚推出的M3所创下的世界纪录。凭借独创的六源循证技术与M3基座结合，M3 Plus将幻觉率降低至2.6%，低于Open Evidence，达到全球最低水平；首创“证据锚定”技术，不仅给出引文来源，还能将模型生成的每一句医学结论，精确锚定到原始论文中的对应证据段落。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;微软 Copilot 测试 Real Talk 和视频生成功能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软正邀请全球用户测试 Copilot 的 Real Talk 功能，并开始测试视频生成能力，以应对 Google Gemini 和 ChatGPT 的竞争。网络分析公司 SimilarWeb 数据显示，Copilot 网页端市场份额约为 1%。为提升竞争力，微软推出两项核心更新：Real Talk 模式与视频生成功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Real Talk 模式通过“深度”和“写作风格”两个维度调整 AI 回应方式。系统可根据对话历史自动选择对话深度（如标准、压缩）和风格（如休闲），并允许用户随时查看 AI 的思维过程，旨在实现更接近人类的对话体验。该模式下，Copilot 不再以程式化方式回应问题，而是表现出真实的好奇心。面对不合理提问时，会主动反驳并表达对特定话题的兴趣，而非对所有问题均表现出虚假热情，从而提升互动活跃度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在安卓版 Copilot 中已发现“生成视频”功能开关，测试表明可创建最长 8 秒且包含音频的视频片段。目前尚不明确该视频功能基于微软自研或 OpenAI 的 Sora 模型，但已在基础版 Microsoft 365 订阅提供，未设置额外付费门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;文心App秘密筹划界面改版，将新增“多人多Agent”群聊功能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日消息，文心App近期将启动交互界面改版，新增“多人、多Agent群聊”功能，以更加社交化、活人感的方式进行用户交互。目前，文心App群聊功能已开启内测，将很快与用户见面。据百度内部人士透露，这是国内AI应用首次从“一对一助手”进化为参与人类社交与协作的“智能成员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，文心App群聊功能将支持用户在同一群聊中调动多个AI角色，适用职场创意脑暴、办公协作、家庭成员间生活协同、趣味互动等场景。AI能理解群聊上下文、识别成员意图，并根据讨论氛围精准判断介入时机，主动应答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据相关人士透露，预计今年2月，文心App还将新增支持群聊内给自己或别人布置日程提醒，支持自定义个人的文心助手人设和回复风格，支持图生图能力和特色玩法类Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，文心App群聊功能已在内测阶段。百度内部人士表示：“我们其实没有考虑过要做一个微信或者取代微信，目前没有考虑，我相信大家也都感觉这个不现实，我们都是从需求和任务本身去出发的，大家如果真的有特定的任务，比如说家庭健康的任务、小组作业的任务、旅游规划的任务，或者是一些特别复杂的决策，我们希望提供一个平台给大家，大家可以尝试在这里面跟你的一些同事朋友去借助AI的能力，更好地完成你们的任务。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;X 平台正式开源推荐算法，马斯克：没有其他社交媒体这么做&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，马斯克宣布正式开源新的 X 平台算法，该算法由与 xAI 的 Grok 模型相同的 Transformer 架构驱动。马斯克坦言，我们知道这种算法很笨拙，还需要大幅改进。但至少可以看到 X 平台在实时、透明的情况下努力让它变得更好，没有其他社交媒体公司这样做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，该算法与当前运行的生产系统相同，并由 xAI 的 Grok 提供支持。完整代码已发布在 GitHub 上，并将每 4 周更新一次。用户的推荐内容会结合关注账号的帖子与 X 上发现的帖子，然后使用基于 Grok 的转换器进行排序。该转换器预测用户可能喜欢、回复、转发、点击或观看的几率。模型不仅预测单一分数，而是预测多种行为并综合它们，以实现更细致的排序。所有内容均直接从用户行为中学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;特斯拉发布第二代人形机器人摆件：199 元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，特斯拉中国官网发布 Tesla Bot 摆件（生肖盲盒版），售价 199 元，将于 1 月 21 日 10:00 开售。据介绍，Tesla Bot 系列摆件是以 1:10 比例打造的可动收藏玩偶，由 40 多个独立零件组成，配备 20 个关节点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/651098d8452274b1476c550b97e32e77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;产品尺寸为 5.5cm x 18.2cm，净重约 25 克。无论是外观细节还是动作表现，均高度还原第二代人形机器人。用户可以自由调整摆件姿势，如双手抱拳的拜年造型、手持节日道具的俏皮模样。&lt;/p&gt;&lt;p&gt;据了解，盲盒内还有神秘嘉宾，将以 10% 的概率随机出现，可能是身穿新年限定服饰的 Tesla Bot（马年生肖特别版），也可能是 Tesla Bot 的神秘好友。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;苹果内部“ChatGPT”曝光：能AI写代码、改文案、分析文件等&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日消息，日前，据外媒报道，苹果公司于 2025 年 11 月在内部推出名为 Enchanté 的类 ChatGPT 聊天机器人，以及名为“企业助手”（Enterprise Assistant）的知识中心应用。消息称 Enchanté 界面酷似 macOS 版 ChatGPT，但专为苹果严格的安全需求定制。媒体援引博文介绍，Enchanté 仅运行苹果批准的模型（含苹果自研基础模型以及 Claude 和 Gemini），且完全在本地或私有服务器上运行。该工具不仅能协助员工完成创意构思、代码开发和校对工作，还能深度分析员工上传的文档和图像，且杜绝了任何向第三方发送敏感数据的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一款名为“企业助手”（Enterprise Assistant）的应用则更具针对性，它充当了苹果员工的中央知识库。该工具完全基于苹果内部的大语言模型（LLM）构建，整合了海量的内部政策与技术文档。员工可通过它快速查询高管职责、商业行为准则、健康保险福利，甚至包括“如何在 iPhone 上配置 XXX”等具体技术指南，极大地简化了信息检索流程。这两款应用均内置了反馈机制，允许员工对 AI 的回答质量进行评分，甚至能将苹果自研模型的回答与第三方模型进行“同屏比对”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然苹果尚未公开具体细节，但行业普遍认为，这些来自工程、设计及营销等跨部门的高质量内部数据，将直接用于训练和微调其基础模型。该媒体认为这种“内部试错”的模式，或许正是苹果在 Apple Intelligence 发展受阻的背景下，试图通过提升模型硬实力来打破僵局的关键一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日，手机厂商 vivo 在近期叫停了 AI 眼镜项目。这一项目此前已秘密筹备半年时间，并已与歌尔、中科创达在内的多家 ODM 厂商合作 demo。原因是vivo 执行副总裁胡柏山在内的多位高层判断，其 AI 眼镜“在当下很难做出差异化”。报道指出，叫停 AI 眼镜项目之后，vivo 将继续聚焦混合现实（MR）方向。1 月 20 日，OpenAI 宣布，将为旗下 ChatGPT 个人版推出年龄识别模型，助力这家人工智能企业识别出未满 18 周岁用户的账号。该公司表示，此模型的运行依托账号数据信号与用户行为数据信号的结合分析。具体信号涵盖用户长期使用习惯、账号注册时长、日常活跃时段以及用户自行填报的年龄信息。管理解决方案提供商ServiceNow达成三年期协议，将AI模型集成进后者的业务软件。&lt;/p&gt;</description><link>https://www.infoq.cn/article/XO4MTrrpMLfkwoZWK7hB</link><guid isPermaLink="false">https://www.infoq.cn/article/XO4MTrrpMLfkwoZWK7hB</guid><pubDate>Mon, 26 Jan 2026 09:34:02 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Arkweb如何正确加载web的当前title？</title><description>&lt;p&gt;本问答帖原创发布在&lt;a href=&quot;https://developer.huawei.com/consumer/cn/forum/ha_source=InfoQ&amp;amp;ha_sourceId=70000011&quot;&gt;华为开发者联盟社区&lt;/a&gt;&quot;&amp;nbsp;，欢迎开发者前往论坛提问交流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用arkweb的onTitleReceive获取web的title有时候并不是和document.title是一致的，而且onTitleReceive经常会返回url字符串，请问这种问题应该如何应对？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;解决方案：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;·&amp;nbsp;方案一：在&lt;a href=&quot;https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd#ontitlereceive&quot;&gt;onTitleReceive&lt;/a&gt;&quot;中通过webController.getTitle()获取网页的标题。&lt;/p&gt;&lt;p&gt;·&amp;nbsp;方案二：通过&lt;a href=&quot;https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd#runjavascript&quot;&gt;runJavaScript&lt;/a&gt;&quot;执行JavaScript代码来获取文档的标题。如果getTitle返回的是网页url，那是因为当前网页未设置title。正常来说通过webController.getTitle()获取到的网页标题和document.title是一致，如果遇到不一致的情况，可以自由选择方式一或者二。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;具体参考demo/操作步骤，请点击原帖查看：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://developer.huawei.com/consumer/cn/forum/topic/0203186440708060028?fid=0109140870620153026&quot;&gt;Arkweb如何正确加载web的当前title？-华为开发者问答&amp;nbsp;|华为开发者联盟&amp;nbsp;(huawei.com)&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd</link><guid isPermaLink="false">https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd</guid><pubDate>Mon, 26 Jan 2026 09:02:29 GMT</pubDate><author>HarmonyOS</author><category>HarmonyOS</category></item><item><title>AI辅助开发系列专题：现实世界的模式、陷阱和生产就绪情况</title><description>&lt;p&gt;AI不再是研究性实验或IDE中的新奇小玩意儿：它已成为软件交付流程的一个重要组成部分。团队逐渐认识到，将AI融入生产环境的关键不在于模型性能，而在于架构设计、流程管理和责任归属。在本系列文章中，我们将探讨AI完成概念验证之后的发展轨迹，以及AI如何改变我们构建、测试和运营系统的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;贯穿这些文章的核心观点是：可持续AI开发所依赖的基础要素与支撑优质软件工程的基础要素相同——清晰的抽象、可观测性、版本控制以及迭代验证。现如今的差异在于，系统的部分组件能在运行的过程中学习，这在上下文设计、评估管道和人类责任等方面提出了更高的要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着团队的成熟，他们的注意力从工具转到了架构，从模型能做什么转到了周边系统如何确保可靠性、透明度和可控性。你会在实践中看到这一点，从资源感知型模型构建和人机协同数据创建到使用分层协议（如A2A与MCP），这些技术使AI代理能够发现能力并协作工作，而且无需重写代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能代理架构不再是一个想法验证实验。具备协调、适应和协商能力的系统正逐步投入生产应用，而最稳妥的实施路径是循序渐进，建立清晰的防护机制和共享工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ系列文章“AI辅助开发：现实世界的模式、陷阱和生产就绪情况”探讨了AI辅助开发的现状：工程师正将实验转化为工程实践，而AI正从一种好奇心驱动的探索，演变为一项可掌握、可应用的技艺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;感兴趣的读者可以&lt;a href=&quot;https://www.infoq.com/minibooks/ai-assisted-development-2025/&quot;&gt;下载整个系列的PDF合集&lt;/a&gt;&quot;。以下是该系列文章的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1.&lt;a href=&quot;https://www.infoq.com/articles/ai-trends-disrupting-software-teams&quot;&gt;颠覆软件团队的AI趋势&lt;/a&gt;&quot;，作者：&lt;a href=&quot;https://www.infoq.com/profile/Bilgin-Ibryam/#allActivity&quot;&gt;Bilgin Ibryam&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文将AI定位为自云计算以来软件领域最重要的转变，它重塑了团队构建、运营和协作的方式。文中重点介绍了从生成式开发到智能代理系统的新兴发展趋势，为开发者、架构师和产品经理提供了具体的指导，有助于他们更好地适应这个有AI辅助的软件工程新时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2.虚拟座谈会：实战中的AI：开发者如何重写软件流程&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虚拟座谈会从观察所得谈及实践经验。本次座谈会的参与者有工程师、架构师和技术领导者，探讨的主题是AI如何改变了软件开发的格局。作为从业者，他们会分享自己的见解，关于把AI纳入日常工作流程后，什么会成功，什么会失败，并强调了上下文、验证和文化适应对于AI在现代工程实践中的可持续应用的重要性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;座谈会成员：&lt;a href=&quot;https://www.infoq.com/profile/Mariia-Bulycheva/#allActivity&quot;&gt;Mariia Bulycheva&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/May-Walter/#allActivity&quot;&gt;May Walter&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Phil-Cal%c3%a7ado/#allActivity&quot;&gt;Phil Calçado&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Andreas-Kollegger/#allActivity&quot;&gt;Andreas Kollegger&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;主持人：&lt;a href=&quot;https://www.infoq.com/profile/Arthur-Casals/#allActivity&quot;&gt;Arthur Casals&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;发布日期：2026年1月26日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3.为什么大多数机器学习项目未能投入生产应用，作者：&lt;a href=&quot;https://www.infoq.com/profile/Wenjie-Zi/#allActivity&quot;&gt;Wenjie Zi&lt;/a&gt;&quot;，发布日期：2026年2月2日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文采用诊断方法，剖析众多项目在落地前陷入停滞的原因：模糊的问题定义和脆弱的数据实践与理想的模型与实际的产品之间存在着巨大的鸿沟。文中会提供切实可行的建议：设定清晰的商业目标，将数据视为产品，建立早期评估与监测机制，使各团队协调一致，从容实现从原型到量产的跨越。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4.在资源受限环境中构建大型语言模型，作者：&lt;a href=&quot;https://www.infoq.com/profile/Olimpiu-Pop/#allActivity&quot;&gt;Olimpiu Pop&lt;/a&gt;&quot;，发布日期：2026年2月9日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文重点探讨基础设施、数据和计算资源的限制如何推动创新而不是阻碍它。作者会援引真实的案例，展示如何在资源严重受限的情况下，通过更小、更高效的模型、合成数据生成技术和严格的工程实践创建出有效的AI系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;5.架构代理MLOps：A2A和MCP的分层协议策略，作者：&lt;a href=&quot;https://www.infoq.com/profile/Shashank-Kapoor/#allActivity&quot;&gt;Shashank Kapoor&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Sanjay-Surendranath-Girija/#allActivity&quot;&gt;Sanjay Surendranath Girija&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Lakshit-Arora/#allActivity&quot;&gt;Lakshit Arora&lt;/a&gt;&quot;，发布日期：2026年2月16日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文展示如何将Agent-to-Agent通信与模型上下文协议结合起来，实现互操作性和可扩展的多智能代理系统，并应用于实际的MLOps工作流程中。该文会概要介绍一个将编排与执行解耦的架构，使得团队可以通过发现而不是重写来添加新功能，从静态管道演变为协调一致的智能操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/ai-assisted-development-series/&quot;&gt;https://www.infoq.com/articles/ai-assisted-development-series/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/yQ3S96l0rjeBBy9XPpCS</link><guid isPermaLink="false">https://www.infoq.cn/article/yQ3S96l0rjeBBy9XPpCS</guid><pubDate>Mon, 26 Jan 2026 08:42:00 GMT</pubDate><author>Arthur Casals</author><category>AI&amp;大模型</category></item><item><title>Hugging Face发布FineTranslations：一个万亿级的多语言平行文本数据集</title><description>&lt;p&gt;Hugging Face发布了&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/finetranslations&quot;&gt;FineTranslations&lt;/a&gt;&quot;，这是一个大规模多语言数据集，包含覆盖英语和其他500多种语言的并行文本、超过1万亿个Token。该数据集是通过使用Gemma3 27B将FineWeb2语料库中的非英语内容翻译成英语来创建的，整个数据生成流程被设计成可复现且公开可查的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该数据集主要用于提高机器翻译的质量，特别是将英语翻译成其他语言。对于许多资源比较少的语言，效果还比较差。通过将非英语的原始文本翻译成英语，FineTranslations提供了适用于对现有翻译模型进行微调的大规模并行数据。内部评估表明，在训练仅限英语的模型时，生成的英语文本效果与FineWeb相当，并且这些数据可以在翻译之外的任务中重用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了翻译之外，Hugging Face报告称，生成的英语语料库保留了源语言的大量文化和上下文信息。在内部实验中，使用翻译后的英语文本训练出来的模型，其性能与使用原始FineWeb数据集训练的模型相当。这表明，对于仅限英语的模型预训练来说，FineTranslations也可以作为一个高质量的补充。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该数据集来源于&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb-2&quot;&gt;FineWeb2&lt;/a&gt;&quot;，它聚合了2013年至2024年间从CommonCrawl快照中收集的多语言Web内容。为了减少由高度重复或特定领域内容（如宗教文本和维基百科页面）所导致的偏差，其中只包含bible_wiki_ratio低于0.5的语言子集。每种语言最多处理了500亿个Token，优先应用FineWeb2-HQ的质量分类器（如有可用），否则使用随机抽样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hugging Face使用&lt;a href=&quot;https://github.com/huggingface/datatrove?tab=readme-ov-file#synthetic-data-generation&quot;&gt;datatrove框架&lt;/a&gt;&quot;完成了大规模的翻译工作。该框架在Hugging Face集群上实现了强大的检查点机制、异步执行和GPU的高效利用。文档被分割成最多包含512个Token的块，为了保持跨段落上下文的连贯性，他们采用了滑动窗口策略。为了减轻大规模翻译中常见的问题，Hugging Face引入了额外的保护措施，包括早期对恶意或垃圾内容的分类、严格的格式约束，以及确保换行与结构一致性的后处理流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个数据集条目包含原始文本块和翻译文本块、语言和字符集标识符、Token计数、教育质量评分，以及指向原始CommonCrawl数据源的引用。该数据集可通过Hugging Face数据集库访问（支持流式处理，可进行大规模处理），或直接通过基于datatrove的管道使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Achref Karoui在评论此次发布时&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7415444237296857089?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7415444237296857089%2C7416158843141095424%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287416158843141095424%2Curn%3Ali%3Aactivity%3A7415444237296857089%29&quot;&gt;表示&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;太棒了！这次发布将弥合差距，让各个社区都能够更好地将流行模型与他们的语言相匹配。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FineTranslations现已在Hugging Face上提供。该数据集遵循开放数据共享署名（ODC-By）v1.0许可，其使用受CommonCrawl的条款约束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/huggingface-fine-translations/&quot;&gt;https://www.infoq.com/news/2026/01/huggingface-fine-translations/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/brGidcHa5mVbOB0iyDiu</link><guid isPermaLink="false">https://www.infoq.cn/article/brGidcHa5mVbOB0iyDiu</guid><pubDate>Mon, 26 Jan 2026 07:29:10 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>大数据</category><category>机器学习/深度学习</category></item><item><title>面向SRE的人本AI：多智能体事件响应</title><description>&lt;p&gt;根据OpsWorker（代理AI同事即服务）的博文，企业的站点可靠性工程实践正在悄然发生转变。团队不再是简单地将故障告警发送给一台机器，而是设计出能与值班工程师协同工作的多智能体AI系统。它能缩小搜索范围，自动处理事件调查中的繁琐步骤，并将判断决策权留给人类，实现了人机协作的新模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一篇深入探讨多智能体事件响应的&lt;a href=&quot;https://www.opsworker.ai/blog/agent-driven-sre-investigations-a-practical-deep-dive-into-multi-agent-incident-response/&quot;&gt;博文&lt;/a&gt;&quot;中，&lt;a href=&quot;https://opsworker.ai/&quot;&gt;OpsWorker&lt;/a&gt;&quot;联合创始人&lt;a href=&quot;https://www.linkedin.com/in/ar-hakobyan/&quot;&gt;Ar Hakboian&lt;/a&gt;&quot;认为，在事件管理中，AI真正的价值在于协调。Hakboian描述了一个模式，其中包含多个专用的智能体：一个用于日志，一个用于指标，一个用于运行手册，诸如此类。有一个监督层负责协调，决定谁做什么以及按什么顺序进行。文章作者解释说，他们的目标是通过提出假设、起草查询以及策划相关上下文来减轻工程师的认知负担，而不是完全取代人类。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/40df78df0726f315e02d7e606f860afc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该文简洁地概括了这种方法，指出AI智能体应该提出假设、查询和补救选项，而人类则负责判断和审批。这种框架与&lt;a href=&quot;https://arxiv.org/html/2412.00652v2&quot;&gt;Zefang Liu最近在arXiv上发表的一篇学术论文&lt;/a&gt;&quot;非常接近，该论文使用桌面推演框架Backdoors和Breaches研究了大语言模型智能体团队在模拟网络事件中如何进行协调。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Liu在实验中比较了集中式、非集中式和混合式团队结构。他发现，同质集中式混合结构成功率最高。相比之下，没有领导者的非集中式领域专家团队更难以达成共识。Liu的发现表明，让自主智能体一起工作不仅不能加快问题的解决，反而会导致更多的混乱。就对SRE的影响来说，最好是有一个监督者或协调者。不过，混合式领域专家团队有时比同质的通才团队更难推进工作，即使有监督者也是如此，这似乎是因为专家容易在优先级上产生分歧，无法就单一行动方案达成共识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据OpsWorker的博文，他们通过明确的角色设计和结构化交接间接地解决了这个问题。为了避免陷入僵局，每个智能体都有一套清晰的工具和责任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;虽然实验验证了技术可行性，但距离生产应用还有很大的差距。智能体是出色的技术调查员，但缺乏生产事件响应所需的安全控制、可靠性工程设计和运维成熟度。—— Ar Hakboian&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近，云咨询公司EverOps写了一篇关于&lt;a href=&quot;https://www.everops.com/resources/blog/how-llms-are-transforming-sre-work-without-replacing-engineers&quot;&gt;LLM如何转变SRE工作而不取代工程师&lt;/a&gt;&quot;的文章。该文也支持这一假设。按照该公司的说法，在受访的SRE专业人士中，只有少数受人认为AI将在两年内取代他们的工作，而绝大多数人只是将其视为简化工作的工具。文章指出，实际的应用场景集中在日志摄取和异常检测、分类自动化、告警聚合和基于检索的内部知识库访问。EverOps还揭示了承诺与实际表现之间的差距，在他们提到的ClickHouse实验中，研究人员在真实的根因分析场景中测试了多个先进的语言模型，结果显示，自主分析的效果远不及人工引导下的调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpsWorker的博文通过强调评估和安全措施来传递了这种谨慎态度。文中提出了一系列的建议，如用真实事件测试多智能体设置，并授予智能体最低限度的必要权限。Hakboian建议逐步上线应用这些智能体技术，开始只允许只读访问，然后在验证它们的工作没有问题后才允许进行受控的智能体操作。作者还建议，在事件处理场景中，与其费时费力地设计提示，不如使用护栏并谨慎地整合工具。Hakboian一直呼吁人类进行监督，并强调智能体与工具交互时存在出现幻觉的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技发布了一个基于其Bedrock平台构建的&lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/build-multi-agent-site-reliability-engineering-assistants-with-amazon-bedrock-agentcore/&quot;&gt;多智能体SRE助手的详细示例&lt;/a&gt;&quot;。其架构几乎是直接反映了OpsWorker的博文内容：一个监督者协调四个专门的智能体，分别用于指标、日志、拓扑和运行手册，然后它们全部连接到一个综合的Kubernetes后端。亚马逊云科技的文章侧重于供应商视角，与Bedrock和LangGraph等特定服务相关，但与OpsWorker博文中工作流优先的理念一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/10623c85b22854afa2433eb0b9b87243.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总的来说，这些信息源表明，智能体SRE正在迅速成熟，而组织正在使用它们来增强员工的能力而不是取代他们。对于希望在事件工作流中引入AI智能体，同时又希望保留人类工程师控制权的团队，OpsWorker的博文提供了一个周全而详尽的方法论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/opsworker-ai-sre/&quot;&gt;https://www.infoq.com/news/2026/01/opsworker-ai-sre/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/GkhFDkbYPBaBtwEDQCI6</link><guid isPermaLink="false">https://www.infoq.cn/article/GkhFDkbYPBaBtwEDQCI6</guid><pubDate>Mon, 26 Jan 2026 06:25:59 GMT</pubDate><author>作者：Matt Saunders</author><category>AI&amp;大模型</category><category>AIOps</category></item><item><title>千亿级请求下，飞猪如何将广告外投系统超时率爆降至0.01%</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;什么是 RTA？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一句话描述：RTA（Real-Time API）= 实时竞价接口，就是广告平台在每次广告曝光前，实时问飞猪&quot;这个用户要不要投、出多少钱&quot;的关键技术。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引 &amp;nbsp; &amp;nbsp;言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪用户增长广告外部投放（RTA）系统自 2022 年上线以来，对接了头条、小红书、华为等 10+ 头部广告媒体渠道，日均处理千亿级请求（百万级 QPS），对低延迟、高吞吐、强稳定性提出极高要求。随着业务策略复杂度提升与流量规模持续增长，系统面临更高的性能与效率挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们围绕两大核心目标展开系统性优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效能提升：通过应用架构解耦、技术栈升级与研发流程优化等，系统性释放工程生产力；极致性能优化：从网络层、网关层、应用层到业务逻辑层优化，系统性降低响应延迟、减少资源成本、提升参竞率与准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文将按此结构，系统回顾我们的优化路径与核心成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;整体链路架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪 RTA 作为广告投放的实时决策端，接收来自媒体的竞价请求。流量通过两种方式接入：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚合接入：经由阿里妈妈广告交易平台（Tanx 平台）统一转发；直连接入：如小红书、vivo 等媒体直接调用飞猪服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统整体分为 网关层（承担高并发请求接入与流量路由）和业务逻辑层（在毫秒级窗口内完成设备解析、人群定向、策略召回、频控与出价计算等多阶段实时决策），最终返回竞价结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e4c2c966a0cd36fea2b36c1660d1f10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研发效能升级&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;技术考量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期 RTA 与多个业务模块共部署于同一应用。随着系统承载流量突破百万级 QPS，一个核心矛盾逐渐凸显：99% 的流量由 RTA 产生，但任何功能迭代都需全量发布，导致资源投入与业务价值配置需要重新审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这促使我们从两个维度重新审视系统设计：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源效率维度：在硬件持续演进的背景下，如何通过架构优化释放单机潜能，以更少机器支撑更高吞吐？这不仅是成本问题，更是技术人应该追求的目标；研发效率维度：效能提升不能仅关注“开发快”，而应覆盖“开发→自测→发布→定位→解决”的完整闭环。尤其在多渠道 RTA 对接场景下，是否存在可复用的范式，能否借助 AI 进一步释放生产力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，我们决定以 RTA 为突破口，开展系统性效能升级——因其流量占比最高、优化 ROI 最显著，且业务逻辑相对独立，是验证新架构与新工具的理想载体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化方案&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;应用架构解耦 - RTA 独立拆分&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;识别核心矛盾、评估 ROI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在系统拆分上，优先考虑将 RTA 从原应用中拆出。有以下几点考虑：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RTA 依赖较少，后续做单元化更简单，成本更低。业务逻辑相对清晰，迁移风险可控；重点是它流量最大、成本最高，可以最大化享受底层技术升级带来的红利，ROI 更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在拆分过程中，曾评估切换到 GO（协程）方案，但综合考虑开发成本及后续维护成本后劝退。最终仍采用 Java 技术栈，但是升级了“大保健三件套”：JDK21（虚拟线程） + SpringBoot 3.x（比 2.x 快约 10-20%，依赖模块化初始化改进）+ 网络中间件优化（降低 I/O 开销与堆外内存）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;平滑迁移策略&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为保障迁移过程零故障、可回滚，过程中作了以下关键思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9ea090e7998e6f22f0eff65c0b79e9d1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;发布提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布不仅是功能上线的终点，更是系统韧性的起点。尤其当单次故障恢复时间直接影响业务收入时，应用重启速度、发布流程确定性、回滚敏捷性，就成为了衡量工程成熟度的关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们以“分钟级恢复”为目标，从流程与性能两个维度优化发布链路：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化发布流程，强化关键验证：移除“安全生产”卡口（测试后置至 Beta）、合并 Beta 与第一批发布，并将 Beta 日志采样改为全量，提升问题发现能力。加速应用重启，支撑快速回滚：基于 JDK 21 + Spring Boot 3 升级，精简依赖与配置，应用重启时间降低约 80%+，显著提升日常发布效率与故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布流程对比：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/9751800dbe434474dfadb2709cde6aef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;测试提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各媒体渠道环境高度异构且封闭，无法向开发或预发环境注入标准化测试流量。这导致传统 Mock 或人工构造用例难以覆盖真实长尾场景，逻辑变更后往往依赖线上验证，成本高、风险大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对测试成本大的问题，做了 2 点思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线上即标准：线上运行代码已验证可靠，其请求出入参可作为功能正确性的基准——预发环境用相同入参得到相同出参，即可判定代码正常；&lt;/p&gt;&lt;p&gt;真实流量即用例：线上流量天然覆盖最全场景，通过采集请求快照并在预发回放，可自动化完成功能验证与 diff 比对。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上面的思考，设计了一套流量采集和回放系统：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/37d00b5f4d958a8785d3f9ea1607a88e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;开发运维提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI Coding 代码重构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 时代背景下，大家都在尝试进行 AI-Coding 实践，我们也从工具 Claude、Cursor、Qcoder，到框架 BMAD、OpenSpec 基本都用了一遍，沉淀了一些较为可行的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对 RTA 多渠道接入的场景，通过 AI Coding，我们高效完成了核心链路的代码框架的升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Pipeline 模式：将业务流程原子化拆分，按语义划分为多个节点，管道式编排，职责单一；适配层设计：在关键节点开放扩展点，媒体个性化逻辑收敛至适配层，主流程无侵入；插拔式接入：新媒体只需实现适配接口，即插即用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后整体的代码框架如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/17de09c2a8246e72ea5464dc54af0bc1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;监控体系精细化&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原有监控体系已覆盖 iGraph 调用、广告召回等关键链路，但仅提供“成功 / 失败”的二元指标，存在局限：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;监控颗粒度不足：无法区分失败根因。例如，iGraph 查询无结果突增时，难以判断是主动熔断、下游超时，还是真实无匹配；问题排查效率低：依赖人工翻查日志，定位耗时长，影响故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们对强依赖接口进行深度可观测性升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;细化异常码，补齐多维度监控：针对 iGraph、人群召回、策略召回、溢价召回等核心环节，统一定义结构化异常码，并按媒体、地域、设备类型等维度聚合，实现快速定位与精准归因；构建 Pipeline 实时折损漏斗：基于节点化改造，将全链路拆解为可度量的转化阶段，通过可视化漏斗动态呈现各环节折损率及原因，使业务流转状态与瓶颈节点清晰可见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化成果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成本与性能&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器成本大幅降低：在流量不变的情况下，服务器数量降低了 30%，单机 CPU 水位进一步降低&amp;nbsp;15%。性能显著提升：RTA 接口平均 RT 下降&amp;nbsp;20%，应用重启速度大幅提升，有效支撑高频发布与快速故障恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效率&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;测试与发布效率大幅提升：通过流量回放能力，测试周期从 3 天缩短至 1 天；发布周期从至少 1 天缩短至约 2 小时开发与运维效率提升：新媒体渠道接入周期从 5 天缩短至 2 天；新增多维度监控指标，问题发现与定位效率提升&amp;nbsp;40%，实时折损漏斗让业务流转一目了然。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;极致性能优化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对高并发实时系统，我们摒弃了&quot;头痛医头&quot;的优化方式，构建了从网络层→网关层→应用层→业务层的全链路性能优化体系：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网络层优化：根治跨地域网络耗时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在接入多个外部媒体 RTA 后，跨地域调用问题凸显。由于媒体机房分布广泛（覆盖华北、华东、华南等区域），而飞猪 RTA 服务当时仅中心化部署于单一机房，导致跨地域单元调用时出现严重超时：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现象：深圳 / 南通单元超时率高达 100%，张北单元相对较好矛盾点：飞猪服务端 P99 延迟仅数毫秒，但端到端仍无法满足媒体严苛的超时要求（如 30~60ms 级别）；根因：每次请求都重新建立 TCP 连接，仅握手建连就要消耗约 30ms，叠加 HTTP 请求的 30ms，极易触发超时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关键验证：通过 CNAME 切换进行了快速验证——将小红书南通区域流量直接导向张北中心机房，省去南通→张北的网络中转环节，超时率从 30% 骤降至 8% → 证明物理距离是根本瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba64539f069da86f8a7a1a02b54ef348.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;HTTP 长连接复用&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;启用 HTTP 长连接复用，核心收益：节省 TCP 建连时间（~30ms）、RTT 次数从 2 次降为 1 次、减少系统开销（避免频繁握手 / 挥手）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;配置改造&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过调整网关层配置，显式启用 HTTP 长连接（Keep-Alive），并合理设置连接保活时长与单连接最大请求数，确保长连接有效复用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4b0d33c3aba4518679c739a3b812b6ef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决首次请求超时难题&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首次请求必须经历 TCP 建连，建连耗时导致 HTTP 超时，超时又导致连接关闭，长连接无法建立。&lt;/p&gt;&lt;p&gt;为此，通过改造 HTTP 客户端底层实现：当 HTTP 协议层超时时，TCP 连接往往已建立完成，若底层 TCP 连接已成功建立，则保留该连接供后续请求复用，打破“超时 → 关连接 → 无法长建连”的恶性循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，深圳机房 su121、南通机房 ea120/ea119 的 RTA 超时率大幅降低，但跨地域网络延迟的不确定性仍未根除。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;单元化部署&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长连接复用虽然缓解了问题，但物理距离带来的 RTT 波动仍是稳定性隐患。RTA 服务完成独立拆分、系统依赖大幅简化后，具备了实施单元化的技术条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署，核心是梳理 RTA 服务的依赖关系，并针对不同的依赖项，制定了不同的改造方案（仅列出部分）：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;强依赖（如缓存）本地化部署，确保低延迟访问；弱依赖（如配置类数据库）通过中心化代理 + 异步同步满足最终一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化成效：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署彻底解决了跨地域网络延迟问题：&lt;/p&gt;&lt;p&gt;阿里妈妈广告平台侧：深圳、南通单元超时率降为 0.07%。小红书直连：超时率从 30%→0.01%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网关层深度调优&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为流量入口，网关的性能直接影响系统整体稳定性。通过火焰图与 TCP 连接状态分析，我们发现异常现象：TIME_WAIT 连接数高达数千，而 ESTABLISHED 连接仅十余个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这说明 Tengine 到后端应用也在使用短连接，每次请求都创建 / 销毁连接。TIME_WAIT 过多会导致端口耗尽、内存浪费（每个连接 2~4KB）和 CPU 开销增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tegine 后端长连接优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了减少建连开销，我们在网关层启用与后端应用的长连接池，核心配置如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b114c2a9b631f29db3af7c37c2ad5d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TIME-WAIT 总量下降了 99%集群 CPU 使用率：CPU 降了近 10pt。在保障稳定性前提下，缩容 15% 服务器数量后，单机水位仍保持在健康区间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tengine 配置精简&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;全盘梳理 Tengine 配置，针对性优化低效和冗余项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关闭非必要日志：access_log 单文件达 25G+ 引发磁盘告警，仅保留 error 日志移除 gzip 压缩：RTA 响应多为小 JSON，gzip 压缩收益低但 CPU 开销高启用 reuseport：配置 listen 80 reuseport，消除 accept 锁竞争，提升并发处理能力。优化效果：CPU 水位下降 2 个百分点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;应用层极致优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用层的性能瓶颈往往隐藏在非核心路径中，核心业务逻辑通常是经过多次优化的重点，而一些看似不起眼的非核心路径（如日志系统、下游服务调用等）往往成为隐藏的性能瓶颈。通过压测与线上监控，我们发现两个关键问题：日志埋点开销过大与下游长尾请求拖累整体 RT，并针对性实施优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;日志系统优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在高并发实时系统中，日志既是可观测性的基石，也可能成为性能瓶颈。通过 Arthas 火焰图分析 CPU 热点，发现日志埋点逻辑与核心 RTA 业务逻辑的 CPU 占比居然相当，是两个大头，表明日志系统仍然有优化空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于日志埋点对业务监控、链路追踪等的重要性，我们无法简单地关闭或大幅减少日志。因此，从减少日志量和提升日志吞吐效率两个方面进行优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;协议精简：精简日志的输出格式，采用紧凑型协议格式替代冗余的 JSON 格式，减少了 50% 的日志体积。批量聚合：通过 StringBuilder 将散落在多处的日志打印收敛到一次日志打印，直接降低了 IO 操作次数。异步刷盘：通异步日志过配置 Logback 的 AsyncAppender（设置 neverBlock=true）和 RollingFileAppender（设置 immediateFlush=false），以异步方式刷新日志到磁盘，减少了频繁的磁盘同步操作带来的系统开销，增加了日志的吞吐。分层采样：不同应用环境采取不同的采样策略，在 Beta 环境下进行全量采集以便快速定位问题；而在生产环境中实施千分之一的采样率，确保可观测性的同时大幅减少日志数量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，CPU 使用率降低了 9pt，整体日志文件大小减少了 60%，直接降低日志存储和分析成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主动熔断机制&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成网络层、网关层的性能优化后，系统 P99 延迟已稳定满足媒体超时要求。但偶发的长尾“毛刺”请求（由瞬时 GC 抖动、资源竞争或下游微突发引起）仍可能影响毫秒级实时决策的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们在核心依赖调用中引入主动超时熔断机制：对关键服务调用设置独立于全局超时的更严格执行时限，一旦超时立即中断并返回降级兜底结果，避免单点延迟拖累整体响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，接口 P99 延迟波动显著平滑，各区域机房超时率进一步降低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务层优化：参竞率与准确率提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心洞察与背景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流量规模扩大，设备数量和类型同步增长，设备身份识别的一致性问题被放大，主要体现在以下三方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;策略一致性挑战：原有 ID 选择采用单一优先级规则（如 Android 优先 OAID → IMEI），当人群包仅包含 CAID 而系统选中 IDFA 时，可能导致匹配失败；标识歧义风险：采用扁平化的 didMd5 格式，在亿级规模下存在哈希碰撞可能，影响画像准确性；配置与执行不一致：离线策略（如定向表、溢价表）与实时决策使用不同 ID 格式，造成策略“写一套、用一套”，实际失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设备标识标准化：摒弃 didMd5 扁平格式，定义 didType_didValue 分层标识体系（如：idfa_{hash}, oaid_{hash}），通过类型前缀彻底消除哈希碰撞歧义，身份识别准确率提升至 99.99%召回策略重构：废弃单点优先级规则，构建多维身份并行召回引擎，提升召回成功率和准确率。全链路数据一致性：统一改造定向表、溢价系数表等 8 个核心离线表，确保策略定义与实时执行使用同一套标识体系；同时建立设备身份质量监控，自动过滤 &quot;null&quot;、&quot;-&quot; 等无效设备标识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因 ID 不匹配导致的参竞失败大幅减少，整体参竞效率明显提升；投放精准度增强，无效拉新显著下降，营销资源更高效地触达目标用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结与展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总结&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过两阶段的系统性优化，飞猪 RTA 在性能与效能上都有显著的突破：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;性能与成本：通过应用拆分、架构升级与网关调优等，在整体 QPS 提升 60%+ 的前提下，服务器资源消耗降低约 30%；研发效能：通过流量回放、发布流程优化与核心链路重构，测试周期缩短约 65%，发布周期压缩超 80%，新渠道接入效率提升 60%+，问题发现与定位效率提升约 50%；业务价值：通过设备身份一致性治理，参竞效率显著提升；通过精准定向优化，拉新重复率大幅下降，用户质量明显改善。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望&lt;/p&gt;&lt;p&gt;未来，RTA 将持续深耕性能与业务双轮驱动：一方面保持对 RTA 极致性能的探索；另一方面深度融合 AI 能力，构建投放效果自动诊断与策略自优化机制，实现从“实时响应”到“智能决策”的跃迁，让 RTA 系统不仅更快，而且更聪明，真正成为驱动业务增长的智能引擎。&lt;/p&gt;</description><link>https://www.infoq.cn/article/wbygEP7MOJfR7btgiWvo</link><guid isPermaLink="false">https://www.infoq.cn/article/wbygEP7MOJfR7btgiWvo</guid><pubDate>Mon, 26 Jan 2026 05:26:49 GMT</pubDate><author>飞猪技术 曹会祎</author><category>生成式 AI</category></item><item><title>千亿级请求下，飞猪如何将广告外投系统超时率爆降至0.01%</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;什么是 RTA？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一句话描述：RTA（Real-Time API）= 实时竞价接口，就是广告平台在每次广告曝光前，实时问飞猪&quot;这个用户要不要投、出多少钱&quot;的关键技术。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引 &amp;nbsp; &amp;nbsp;言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪用户增长广告外部投放（RTA）系统自 2022 年上线以来，对接了头条、小红书、华为等 10+ 头部广告媒体渠道，日均处理千亿级请求（百万级 QPS），对低延迟、高吞吐、强稳定性提出极高要求。随着业务策略复杂度提升与流量规模持续增长，系统面临更高的性能与效率挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们围绕两大核心目标展开系统性优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效能提升：通过应用架构解耦、技术栈升级与研发流程优化等，系统性释放工程生产力；极致性能优化：从网络层、网关层、应用层到业务逻辑层优化，系统性降低响应延迟、减少资源成本、提升参竞率与准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文将按此结构，系统回顾我们的优化路径与核心成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;整体链路架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪 RTA 作为广告投放的实时决策端，接收来自媒体的竞价请求。流量通过两种方式接入：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚合接入：经由阿里妈妈广告交易平台（Tanx 平台）统一转发；直连接入：如小红书、vivo 等媒体直接调用飞猪服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统整体分为 网关层（承担高并发请求接入与流量路由）和业务逻辑层（在毫秒级窗口内完成设备解析、人群定向、策略召回、频控与出价计算等多阶段实时决策），最终返回竞价结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e4c2c966a0cd36fea2b36c1660d1f10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研发效能升级&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;技术考量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期 RTA 与多个业务模块共部署于同一应用。随着系统承载流量突破百万级 QPS，一个核心矛盾逐渐凸显：99% 的流量由 RTA 产生，但任何功能迭代都需全量发布，导致资源投入与业务价值配置需要重新审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这促使我们从两个维度重新审视系统设计：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源效率维度：在硬件持续演进的背景下，如何通过架构优化释放单机潜能，以更少机器支撑更高吞吐？这不仅是成本问题，更是技术人应该追求的目标；研发效率维度：效能提升不能仅关注“开发快”，而应覆盖“开发→自测→发布→定位→解决”的完整闭环。尤其在多渠道 RTA 对接场景下，是否存在可复用的范式，能否借助 AI 进一步释放生产力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，我们决定以 RTA 为突破口，开展系统性效能升级——因其流量占比最高、优化 ROI 最显著，且业务逻辑相对独立，是验证新架构与新工具的理想载体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化方案&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;应用架构解耦 - RTA 独立拆分&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;识别核心矛盾、评估 ROI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在系统拆分上，优先考虑将 RTA 从原应用中拆出。有以下几点考虑：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RTA 依赖较少，后续做单元化更简单，成本更低。业务逻辑相对清晰，迁移风险可控；重点是它流量最大、成本最高，可以最大化享受底层技术升级带来的红利，ROI 更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在拆分过程中，曾评估切换到 GO（协程）方案，但综合考虑开发成本及后续维护成本后劝退。最终仍采用 Java 技术栈，但是升级了“大保健三件套”：JDK21（虚拟线程） + SpringBoot 3.x（比 2.x 快约 10-20%，依赖模块化初始化改进）+ 网络中间件优化（降低 I/O 开销与堆外内存）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;平滑迁移策略&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为保障迁移过程零故障、可回滚，过程中作了以下关键思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9ea090e7998e6f22f0eff65c0b79e9d1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;发布提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布不仅是功能上线的终点，更是系统韧性的起点。尤其当单次故障恢复时间直接影响业务收入时，应用重启速度、发布流程确定性、回滚敏捷性，就成为了衡量工程成熟度的关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们以“分钟级恢复”为目标，从流程与性能两个维度优化发布链路：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化发布流程，强化关键验证：移除“安全生产”卡口（测试后置至 Beta）、合并 Beta 与第一批发布，并将 Beta 日志采样改为全量，提升问题发现能力。加速应用重启，支撑快速回滚：基于 JDK 21 + Spring Boot 3 升级，精简依赖与配置，应用重启时间降低约 80%+，显著提升日常发布效率与故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布流程对比：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/9751800dbe434474dfadb2709cde6aef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;测试提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各媒体渠道环境高度异构且封闭，无法向开发或预发环境注入标准化测试流量。这导致传统 Mock 或人工构造用例难以覆盖真实长尾场景，逻辑变更后往往依赖线上验证，成本高、风险大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对测试成本大的问题，做了 2 点思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线上即标准：线上运行代码已验证可靠，其请求出入参可作为功能正确性的基准——预发环境用相同入参得到相同出参，即可判定代码正常；&lt;/p&gt;&lt;p&gt;真实流量即用例：线上流量天然覆盖最全场景，通过采集请求快照并在预发回放，可自动化完成功能验证与 diff 比对。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上面的思考，设计了一套流量采集和回放系统：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/37d00b5f4d958a8785d3f9ea1607a88e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;开发运维提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI Coding 代码重构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 时代背景下，大家都在尝试进行 AI-Coding 实践，我们也从工具 Claude、Cursor、Qcoder，到框架 BMAD、OpenSpec 基本都用了一遍，沉淀了一些较为可行的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对 RTA 多渠道接入的场景，通过 AI Coding，我们高效完成了核心链路的代码框架的升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Pipeline 模式：将业务流程原子化拆分，按语义划分为多个节点，管道式编排，职责单一；适配层设计：在关键节点开放扩展点，媒体个性化逻辑收敛至适配层，主流程无侵入；插拔式接入：新媒体只需实现适配接口，即插即用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后整体的代码框架如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/17de09c2a8246e72ea5464dc54af0bc1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;监控体系精细化&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原有监控体系已覆盖 iGraph 调用、广告召回等关键链路，但仅提供“成功 / 失败”的二元指标，存在局限：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;监控颗粒度不足：无法区分失败根因。例如，iGraph 查询无结果突增时，难以判断是主动熔断、下游超时，还是真实无匹配；问题排查效率低：依赖人工翻查日志，定位耗时长，影响故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们对强依赖接口进行深度可观测性升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;细化异常码，补齐多维度监控：针对 iGraph、人群召回、策略召回、溢价召回等核心环节，统一定义结构化异常码，并按媒体、地域、设备类型等维度聚合，实现快速定位与精准归因；构建 Pipeline 实时折损漏斗：基于节点化改造，将全链路拆解为可度量的转化阶段，通过可视化漏斗动态呈现各环节折损率及原因，使业务流转状态与瓶颈节点清晰可见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化成果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成本与性能&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器成本大幅降低：在流量不变的情况下，服务器数量降低了 30%，单机 CPU 水位进一步降低&amp;nbsp;15%。性能显著提升：RTA 接口平均 RT 下降&amp;nbsp;20%，应用重启速度大幅提升，有效支撑高频发布与快速故障恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效率&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;测试与发布效率大幅提升：通过流量回放能力，测试周期从 3 天缩短至 1 天；发布周期从至少 1 天缩短至约 2 小时开发与运维效率提升：新媒体渠道接入周期从 5 天缩短至 2 天；新增多维度监控指标，问题发现与定位效率提升&amp;nbsp;40%，实时折损漏斗让业务流转一目了然。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;极致性能优化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对高并发实时系统，我们摒弃了&quot;头痛医头&quot;的优化方式，构建了从网络层→网关层→应用层→业务层的全链路性能优化体系：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网络层优化：根治跨地域网络耗时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在接入多个外部媒体 RTA 后，跨地域调用问题凸显。由于媒体机房分布广泛（覆盖华北、华东、华南等区域），而飞猪 RTA 服务当时仅中心化部署于单一机房，导致跨地域单元调用时出现严重超时：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现象：深圳 / 南通单元超时率高达 100%，张北单元相对较好矛盾点：飞猪服务端 P99 延迟仅数毫秒，但端到端仍无法满足媒体严苛的超时要求（如 30~60ms 级别）；根因：每次请求都重新建立 TCP 连接，仅握手建连就要消耗约 30ms，叠加 HTTP 请求的 30ms，极易触发超时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关键验证：通过 CNAME 切换进行了快速验证——将小红书南通区域流量直接导向张北中心机房，省去南通→张北的网络中转环节，超时率从 30% 骤降至 8% → 证明物理距离是根本瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba64539f069da86f8a7a1a02b54ef348.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;HTTP 长连接复用&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;启用 HTTP 长连接复用，核心收益：节省 TCP 建连时间（~30ms）、RTT 次数从 2 次降为 1 次、减少系统开销（避免频繁握手 / 挥手）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;配置改造&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过调整网关层配置，显式启用 HTTP 长连接（Keep-Alive），并合理设置连接保活时长与单连接最大请求数，确保长连接有效复用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4b0d33c3aba4518679c739a3b812b6ef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决首次请求超时难题&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首次请求必须经历 TCP 建连，建连耗时导致 HTTP 超时，超时又导致连接关闭，长连接无法建立。&lt;/p&gt;&lt;p&gt;为此，通过改造 HTTP 客户端底层实现：当 HTTP 协议层超时时，TCP 连接往往已建立完成，若底层 TCP 连接已成功建立，则保留该连接供后续请求复用，打破“超时 → 关连接 → 无法长建连”的恶性循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，深圳机房 su121、南通机房 ea120/ea119 的 RTA 超时率大幅降低，但跨地域网络延迟的不确定性仍未根除。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;单元化部署&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长连接复用虽然缓解了问题，但物理距离带来的 RTT 波动仍是稳定性隐患。RTA 服务完成独立拆分、系统依赖大幅简化后，具备了实施单元化的技术条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署，核心是梳理 RTA 服务的依赖关系，并针对不同的依赖项，制定了不同的改造方案（仅列出部分）：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;强依赖（如缓存）本地化部署，确保低延迟访问；弱依赖（如配置类数据库）通过中心化代理 + 异步同步满足最终一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化成效：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署彻底解决了跨地域网络延迟问题：&lt;/p&gt;&lt;p&gt;阿里妈妈广告平台侧：深圳、南通单元超时率降为 0.07%。小红书直连：超时率从 30%→0.01%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网关层深度调优&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为流量入口，网关的性能直接影响系统整体稳定性。通过火焰图与 TCP 连接状态分析，我们发现异常现象：TIME_WAIT 连接数高达数千，而 ESTABLISHED 连接仅十余个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这说明 Tengine 到后端应用也在使用短连接，每次请求都创建 / 销毁连接。TIME_WAIT 过多会导致端口耗尽、内存浪费（每个连接 2~4KB）和 CPU 开销增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tegine 后端长连接优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了减少建连开销，我们在网关层启用与后端应用的长连接池，核心配置如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b114c2a9b631f29db3af7c37c2ad5d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TIME-WAIT 总量下降了 99%集群 CPU 使用率：CPU 降了近 10pt。在保障稳定性前提下，缩容 15% 服务器数量后，单机水位仍保持在健康区间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tengine 配置精简&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;全盘梳理 Tengine 配置，针对性优化低效和冗余项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关闭非必要日志：access_log 单文件达 25G+ 引发磁盘告警，仅保留 error 日志移除 gzip 压缩：RTA 响应多为小 JSON，gzip 压缩收益低但 CPU 开销高启用 reuseport：配置 listen 80 reuseport，消除 accept 锁竞争，提升并发处理能力。优化效果：CPU 水位下降 2 个百分点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;应用层极致优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用层的性能瓶颈往往隐藏在非核心路径中，核心业务逻辑通常是经过多次优化的重点，而一些看似不起眼的非核心路径（如日志系统、下游服务调用等）往往成为隐藏的性能瓶颈。通过压测与线上监控，我们发现两个关键问题：日志埋点开销过大与下游长尾请求拖累整体 RT，并针对性实施优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;日志系统优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在高并发实时系统中，日志既是可观测性的基石，也可能成为性能瓶颈。通过 Arthas 火焰图分析 CPU 热点，发现日志埋点逻辑与核心 RTA 业务逻辑的 CPU 占比居然相当，是两个大头，表明日志系统仍然有优化空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于日志埋点对业务监控、链路追踪等的重要性，我们无法简单地关闭或大幅减少日志。因此，从减少日志量和提升日志吞吐效率两个方面进行优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;协议精简：精简日志的输出格式，采用紧凑型协议格式替代冗余的 JSON 格式，减少了 50% 的日志体积。批量聚合：通过 StringBuilder 将散落在多处的日志打印收敛到一次日志打印，直接降低了 IO 操作次数。异步刷盘：通异步日志过配置 Logback 的 AsyncAppender（设置 neverBlock=true）和 RollingFileAppender（设置 immediateFlush=false），以异步方式刷新日志到磁盘，减少了频繁的磁盘同步操作带来的系统开销，增加了日志的吞吐。分层采样：不同应用环境采取不同的采样策略，在 Beta 环境下进行全量采集以便快速定位问题；而在生产环境中实施千分之一的采样率，确保可观测性的同时大幅减少日志数量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，CPU 使用率降低了 9pt，整体日志文件大小减少了 60%，直接降低日志存储和分析成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主动熔断机制&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成网络层、网关层的性能优化后，系统 P99 延迟已稳定满足媒体超时要求。但偶发的长尾“毛刺”请求（由瞬时 GC 抖动、资源竞争或下游微突发引起）仍可能影响毫秒级实时决策的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们在核心依赖调用中引入主动超时熔断机制：对关键服务调用设置独立于全局超时的更严格执行时限，一旦超时立即中断并返回降级兜底结果，避免单点延迟拖累整体响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，接口 P99 延迟波动显著平滑，各区域机房超时率进一步降低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务层优化：参竞率与准确率提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心洞察与背景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流量规模扩大，设备数量和类型同步增长，设备身份识别的一致性问题被放大，主要体现在以下三方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;策略一致性挑战：原有 ID 选择采用单一优先级规则（如 Android 优先 OAID → IMEI），当人群包仅包含 CAID 而系统选中 IDFA 时，可能导致匹配失败；标识歧义风险：采用扁平化的 didMd5 格式，在亿级规模下存在哈希碰撞可能，影响画像准确性；配置与执行不一致：离线策略（如定向表、溢价表）与实时决策使用不同 ID 格式，造成策略“写一套、用一套”，实际失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设备标识标准化：摒弃 didMd5 扁平格式，定义 didType_didValue 分层标识体系（如：idfa_{hash}, oaid_{hash}），通过类型前缀彻底消除哈希碰撞歧义，身份识别准确率提升至 99.99%召回策略重构：废弃单点优先级规则，构建多维身份并行召回引擎，提升召回成功率和准确率。全链路数据一致性：统一改造定向表、溢价系数表等 8 个核心离线表，确保策略定义与实时执行使用同一套标识体系；同时建立设备身份质量监控，自动过滤 &quot;null&quot;、&quot;-&quot; 等无效设备标识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因 ID 不匹配导致的参竞失败大幅减少，整体参竞效率明显提升；投放精准度增强，无效拉新显著下降，营销资源更高效地触达目标用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结与展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总结&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过两阶段的系统性优化，飞猪 RTA 在性能与效能上都有显著的突破：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;性能与成本：通过应用拆分、架构升级与网关调优等，在整体 QPS 提升 60%+ 的前提下，服务器资源消耗降低约 30%；研发效能：通过流量回放、发布流程优化与核心链路重构，测试周期缩短约 65%，发布周期压缩超 80%，新渠道接入效率提升 60%+，问题发现与定位效率提升约 50%；业务价值：通过设备身份一致性治理，参竞效率显著提升；通过精准定向优化，拉新重复率大幅下降，用户质量明显改善。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望&lt;/p&gt;&lt;p&gt;未来，RTA 将持续深耕性能与业务双轮驱动：一方面保持对 RTA 极致性能的探索；另一方面深度融合 AI 能力，构建投放效果自动诊断与策略自优化机制，实现从“实时响应”到“智能决策”的跃迁，让 RTA 系统不仅更快，而且更聪明，真正成为驱动业务增长的智能引擎。&lt;/p&gt;</description><link>https://www.infoq.cn/article/k05g5qvWOzT2tgIivfFN</link><guid isPermaLink="false">https://www.infoq.cn/article/k05g5qvWOzT2tgIivfFN</guid><pubDate>Mon, 26 Jan 2026 03:47:56 GMT</pubDate><author>飞猪技术 曹会祎</author><category>AI 工程化</category></item><item><title>平台即产品：以声明式基础设施提升开发者效率</title><description>&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点速览&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;统一的配置层抽象了基础设施、CI/CD 以及运维复杂性，让开发者能够专注于应用开发本身。每个服务都采用单一配置模型，可在编写 YAML 阶段就校验资源限制，从而实现 FinOps 的左移（shift-left）。独立的 CI 流水线向集中式 CD 流水线提供输入，在保障团队自治的同时，维持一致的部署实践。将应用与基础设施的意图集中在一份配置中，使评审过程更加高效、可预测。这种方式提升了可视性，并支持构建一个可定制的内部开发者平台，以满足组织的合规性要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今环境下，开发者不得不应对数量过多且复杂的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在不同环境中管理 Kubernetes、云资源、安全检查以及部署，需要投入大量时间和专业知识。平台工程正是为了应对这一问题而出现，其目标是让基础设施的使用变得更加简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题：开发者需要掌握的东西太多&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现代应用部署迫使开发者学习大量不同的工具和概念，例如：&lt;/p&gt;&lt;p&gt;编写 Kubernetes 清单文件，用于部署、服务、Ingress 以及自动扩缩容。使用 SDK、API 或 Terraform 等基础设施即代码工具创建云资源，这要求开发者理解云服务、安全模型、网络配置以及成本影响。搭建包含构建、测试、安全检查和发布阶段的 CI/CD 流水线。在不同环境中一致地管理密钥和凭据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些领域单独来看都尚可应付，但叠加在一起就形成了陡峭的学习曲线。开发者不得不在应用逻辑与基础设施相关问题之间频繁切换，这不仅拖慢了交付速度，也增加了配置出错的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在缺乏统一防护机制的情况下，团队往往会为了“保险起见”而过度分配资源，最终导致环境不一致以及不必要的云成本，而这些问题通常要到部署之后才会被发现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;弥合鸿沟：为什么抽象是必要的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心挑战并不在于工具不足。现代平台已经提供了多种方式来构建应用、配置云资源、搭建 CI/CD 流水线、管理密钥，以及将应用部署到 Kubernetes 上。真正的问题在于，这些职责被分散在不同的工具、文件以及抽象层之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SDK、API、Terraform 模块、流水线定义、Kubernetes 清单文件以及环境相关配置，各自都非常强大。但它们暴露的是底层细节，并且需要对整个交付生命周期具备全局上下文。指望每一位应用开发者在编写和测试业务代码的同时，还要全面理解并协调所有这些基础设施和交付层面的关注点，本身就难以规模化，尤其是在同时运行大量微服务的组织中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正缺失的是一种将这些相互关联的关注点整合在一起、以开发者为中心的抽象。开发者真正需要的是一种能够清晰表达意图的抽象层：应用需要哪些资源、应如何构建和部署、如何在不同环境中运行、如何满足安全与容量要求等等，让开发者无需再陷入各类底层系统的实现细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在平台工程视角下，这样的抽象构成了内部开发者平台的核心，可以以一个轻量级、基于 Python 的平台框架形式落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一种可能的解决方案：声明式平台框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，这种抽象在实际中应当是什么样子？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与其再引入一个新的门户，或者要求开发者去学习一套全新的 API，不如从一个更简单的事实出发：开发者每天本来就在和配置文件打交道。问题在于，能否在这种熟悉的基础上，进一步统一应用的构建、部署、配置和运维方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一模型中，一份声明式配置文件可以成为开发者与交付系统之间的主要接口。它用于表达应用在整个生命周期中的“意图”，包括 CI/CD 行为、不同环境下的配置、密钥集成、资源规格、自动伸缩策略以及在 Kubernetes 中的部署位置等；至于具体如何执行，则完全交由平台工程层来处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在幕后，平台框架会读取这份配置，并协调 CI/CD 流水线、基础设施自动化以及 Kubernetes 部署等各个环节的实际执行。对开发者来说，交互方式始终是清晰且可预期的；而对组织而言，交付过程则变得一致、可审查，并且能够内建合规与策略约束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面来看一个具体示例：某位开发者需要部署一个微服务，该服务依赖 Azure Storage，需要安全地管理凭据，在 Kubernetes 中指定明确的 CPU 和内存配置，定义自动伸缩规则，使用专属的节点池，并绑定一个自定义的访问域名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在许多组织中，这些需求往往分散在不同的仓库和代码体系中：云资源在独立的 Terraform 仓库中定义；Kubernetes 的部署与自动伸缩规则维护在 Helm Chart 仓库；CI/CD 行为由流水线定义控制；与环境相关的部署逻辑可能存在于单独的配置仓库，或通过 Puppet 等部署工具实现；而密钥管理则通常通过外部的密钥系统或流水线变量单独处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一种更可行的做法，是将开发者需要提供的输入集中到一份与服务代码并存的 YAML 文件中。这份文件成为该服务在所有环境下的权威描述：其中声明了由 Terraform 仓库实现的基础设施依赖、以 Helm 配置形式渲染的 Kubernetes 运行参数、通过 Puppet 模块实现的部署行为，以及由共享流水线模板执行的 CI/CD 阶段。开发者不再需要直接修改这些分散的仓库，而是通过这一个统一入口，由平台工具进行解析并一致性地落地执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;application:
  name: payment-service
  runtime: python:3.11
  
resources:
  kubernetes:
    cpu: 500m              # Maximum: 2000m (validated by schema)
    memory: 1Gi            # Maximum: 4Gi (validated by schema)
    replicas: 3
    autoscaling:
      enabled: true
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilization: 70
    nodePool: frontend
  
  azure:
    storage:
      - name: payment-receipts
        type: blob
        tier: hot
    keyvault:
      secrets:
        - name: stripe-api-key
          source: ENV_STRIPE_KEY
  
networking:
  hostname: payments.example.com
  ingress:
    tls: enabled
    
deployment:
  tool: puppet
  environments:
    - development
    - staging
    - production&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之所以选择 YAML 作为面向开发者的接口，是因为它在熟悉度、可读性和自动化能力之间取得了一种务实的平衡。大多数开发者早已通过 Kubernetes 和 CI/CD 系统接触并使用 YAML，这使它成为一种低摩擦的方式，不仅可以用来表达应用意图，同时也能自然地融入版本控制和代码评审流程。相比之下，自助式门户（例如内部服务目录或基于 UI 的资源申请工具）、自定义 API，或更具表达力的配置语言，虽然可以提供更强的类型约束或更丰富的抽象能力，但往往伴随着更高的学习和推广成本，或者牺牲了配置的透明性。YAML 本身也并非没有缺点，后文会进一步讨论，例如配置文件容易膨胀、原生校验能力有限等问题。因此，引入 Schema 校验和显式版本控制就显得尤为关键，它们可以帮助团队追踪配置变更、评估影响范围，并在长期演进中安全地推进平台能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这份文件本身即是唯一可信来源。它能驱动自动化流水线，覆盖从代码构建、测试，到基础设施创建以及应用部署的全部流程。由于所有工作负载最终都运行在 Kubernetes 之上，微服务的规模化管理也随之变得更加简单：每个服务都拥有各自独立的配置文件，明确声明资源上限、伸缩策略以及节点池分配方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;平台架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台由多个相互关联的组件组成。GitLab 流水线作为核心协调者，负责拉取代码仓库中的源码，执行应用的构建与单元测试（测试由开发者编写），进行安全检查，使用 Terraform / IaC 创建云基础设施，并通过 Puppet 配置管理将应用部署到 Kubernetes 集群中。贯穿整个流程的是那份 YAML 配置文件：作为统一的控制平面，指示每一个组件该做什么、如何做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该架构在职责划分上非常清晰：CI 流水线专注于代码的构建、测试以及漏洞扫描，而 CD 流水线则负责实际部署工作，包括创建云资源、更新 Kubernetes 以及配置运行环境。Schema 校验被放在整个流程的最前端，作为 YAML 编写阶段的一部分，在一开始就执行，从而能够第一时间捕获配置错误或诸如资源过度分配等功能性问题，实现真正的“左移”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/20b6a55f8b184ec2bb778bb1a12823f8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图一：从代码提交到部署得产品生命周期&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;&amp;nbsp;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么 Kubernetes 让这一切更高效&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将所有内容部署到 Kubernetes 集群带来了几个关键优势：&lt;/p&gt;&lt;p&gt;更容易管理微服务&lt;/p&gt;&lt;p&gt;Kubernetes 天生适合同时运行多个微服务。每个微服务都可以独立管理，拥有自己的配置，同时又能作为完整应用协同工作。&lt;/p&gt;&lt;p&gt;自动扩缩容&lt;/p&gt;&lt;p&gt;当流量发生变化时，Kubernetes 会自动进行扩缩容，从而优化成本。所有这些都可以直接在配置文件中控制，这一特性将 FinOps 实践融入开发工作流中。&lt;/p&gt;&lt;p&gt;专用节点池&lt;/p&gt;&lt;p&gt;有些服务需要更多内存，而有些服务需要更多 CPU。可以将服务或应用分配到符合需求的特定服务器组（节点池）中。例如，高计算量的后端应用可以运行在内存更充足的节点上，而 Web API 则可以运行在标准节点上。这种方式也可以延伸到低环境的 Spot 节点池（更便宜的节点），在应用可靠性不是关键时，云服务提供商可能随时回收这些节点，从而节省大量成本。&lt;/p&gt;&lt;p&gt;更高效的代码审查&lt;/p&gt;&lt;p&gt;当所有配置都集中在一个文件中时，审查者可以轻松查看是否有人申请了过多内存或 CPU，检查扩缩容设置、节点池分配以及整个基础设施，而无需翻查多个文件。&lt;/p&gt;&lt;p&gt;通过 Schema 校验控制成本&lt;/p&gt;&lt;p&gt;平台内置的 Schema 校验可以阻止开发者请求超出限制的资源。例如，当有人尝试申请 10GB 内存，而最大值是 4GB 时，文件创建阶段就会立即校验失败。这种“左移”做法可以在资源浪费发生前就被发现，使 FinOps 成为标准开发流程的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;流水线如何工作：CI 与 CD 的分离&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台通过多项目流水线将 CI 与 CD 明确拆分开来。这种分离方式带来了多方面的好处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CI 流水线工作&lt;/p&gt;&lt;p&gt;CI 流水线只处理代码相关的工作。它负责构建应用、运行测试、执行安全检查，并生成打包后的制品。最终产出的是一个经过测试、具备安全保障、带有版本标识的容器镜像，可以部署到任意环境中。&lt;/p&gt;&lt;p&gt;CD 流水线工作&lt;/p&gt;&lt;p&gt;CD 流水线则接手 CI 的产出，负责部署和基础设施层面的工作。它读取配置文件并执行关键变更，包括在需要时创建云资源、通过 Helm Chart 将应用部署到 Kubernetes 集群，以及使用 Puppet 等配置管理工具完成环境配置和部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种拆分让每条流水线都能更好地发挥自身作用。CI 流水线会在每次代码变更时快速运行，提供即时反馈；而 CD 流水线运行频率相对较低，通常需要审批，重点放在跨环境的基础设施变更和部署操作上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Schema 校验：尽早发现问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;平台中最强大的能力之一是 Schema 校验，它会在所有流程开始之前率先执行。Schema 用来定义一系列规则，例如：&lt;/p&gt;&lt;p&gt;最大 CPU：2000m（2 核）最大内存：4Gi最大副本数：20允许使用的节点池：standard、high-memory、high-cpu必填字段：应用名称、运行时、资源限制&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当开发者创建或更新配置文件时，Schema 校验会立即触发。如果有人尝试申请 5GiB 内存（超过 4GiB 的上限），校验会直接失败，并给出清晰明确的错误提示。这种校验发生在 YAML 编写阶段，而不是部署阶段，从而节省时间，并在资源浪费发生之前就将问题拦截下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能化的基础设施创建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当开发者在配置文件中声明所需的云资源时，CD 流水线会先检查这些资源是否已经存在，并在必要时进行创建或更新。例如，当开发者新增一个 Azure Storage 资源时：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;azure:
  storage:
    - name: user-uploads
      type: blob
      tier: hot
      retention_days: 90&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;流水线中的 Terraform 部分会：&lt;/p&gt;&lt;p&gt;检查该存储账户是否已经存在于 Azure 中（这一能力本身也是 Terraform 的特性）。根据配置文件中指定的参数创建或更新资源。将连接信息（例如存储账户的连接字符串或密码）保存到 Azure Key Vault 中（同样可以通过 YAML 进行控制）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式消除了开发团队与运维团队之间大量的人工协作成本。开发者只需要描述“我需要什么”，具体的创建、更新和编排过程则由平台自动完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;内置的安全检查&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;安全并不是可选项，而是贯穿每一次流水线执行。在 CI 阶段，平台会自动执行以下安全检查：&lt;/p&gt;&lt;p&gt;依赖项检查&lt;/p&gt;&lt;p&gt;自动扫描第三方依赖包中已知的安全漏洞。一旦发现问题，流水线会立即中止，不会生成可部署的构件。&lt;/p&gt;&lt;p&gt;代码安全测试&lt;/p&gt;&lt;p&gt;对代码进行静态分析，检测潜在的安全隐患、硬编码的密码以及其他风险点&lt;/p&gt;&lt;p&gt;容器镜像扫描&lt;/p&gt;&lt;p&gt;检查容器镜像中操作系统层面的安全漏洞，确保所使用的基础镜像是安全可信的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将这些检查自动化，安全被自然地融入到日常开发流程中，而不再是独立审批步骤。这一点在受监管的行业中尤为关键：在 CI 阶段就持续、统一地执行安全控制，可以显著降低审计风险、缩短评审周期，并防止不合规的变更进入共享环境或生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;简化 Kubernetes 的复杂性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对开发者来说，Kubernetes 往往非常复杂，而这种方案在保留灵活性的同时，将底层复杂性很好地隐藏了起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一点在微服务场景下尤其有价值。一个需要维护十个微服务的团队，只需要对应维护十个简洁的配置文件，而不是成堆的 Kubernetes Manifest 文件。每个微服务都可以在配置中清楚地声明自身需求，一目了然：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;# Service 1: Web API - standard resources
resources:
  kubernetes:
    cpu: 250m
    memory: 512Mi
    replicas: 5
    autoscaling:
      enabled: true
      minReplicas: 5
      maxReplicas: 20
    nodePool: standard


# Service 2: Data processor - high memory
resources:
  kubernetes:
    cpu: 1000m
    memory: 4Gi
    replicas: 2
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 8
    nodePool: high-memory&lt;/code&gt;&lt;/p&gt;&lt;p&gt;以 Web 路由为例，开发者只需要在配置中指定一个主机名即可：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;networking:
  hostname: api.payments.example.com&lt;/code&gt;&lt;/p&gt;&lt;p&gt;平台的部署流水线如下：&lt;/p&gt;&lt;p&gt;创建 Kubernetes Ingress 资源配置 TLS 证书如果与云端 DNS 集成还会自动更新 DNS 记录按照公司统一的标准应用流量治理规则&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;部署到多个环境&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台支持还可应用部署到多个环境中，例如开发、预发布和生产环境，并允许针对不同环境进行差异化配置。在这个示例中，我们使用了 Puppet 来应用这些环境相关的设置。不过，同样的模式也完全可以应用到其他工具上，例如基于 GitOps 的方案（如 GitLab CD），以及其他配置管理工具（如 Ansible、Chef）。这种方式有效解决了一个常见难题：在保持各个环境整体一致性的同时，又能支持必要的差异，比如数据库端点、API 凭证以及扩缩容参数等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当 CD 流水线向某个环境部署时，会执行以下流程：&lt;/p&gt;&lt;p&gt;运行一个参数化的 Puppet 模块，用于接收环境特定的配置值从配置文件中传入应用级设置由 Puppet 负责处理系统级和相对固定的配置项，例如 Kubernetes 集群名称、Azure Key Vault 名称，以及部署时使用的运行时参数在这一过程中，各个环境彼此隔离，但部署逻辑是复用的，同时还能持续维护各环境的期望状态&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种做法充分发挥了两类工具的优势：Kubernetes 负责应用的运行、管理和弹性伸缩，而 Puppet 则确保系统层面的配置在所有服务器和环境中保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管本文所描述的方法在很大程度上缓解了问题，但仍然存在一些挑战：&lt;/p&gt;&lt;p&gt;简化的边界&lt;/p&gt;&lt;p&gt;某些高度定制化的应用可能需要简单配置文件之外的更多设置。平台团队需要在“足够简单”和“支持高级或例外场景的定制能力”之间取得平衡。随着新的基础设施能力出现，或开发者需求不断变化，配置文件本身也需要随之演进。&lt;/p&gt;&lt;p&gt;Schema 维护成本&lt;/p&gt;&lt;p&gt;随着需求演进，校验用的 schema 也必须持续更新。如果规则过于严格，开发者会感觉处处受限；如果规则过于宽松，又会削弱成本控制能力，影响 FinOps 实践。如何在灵活性与约束之间找到合适的平衡点，需要长期、持续的调整和优化。&lt;/p&gt;&lt;p&gt;流水线复杂度&lt;/p&gt;&lt;p&gt;随着应用数量增加，既要创建基础设施、又要负责部署的多阶段流水线本身也会变得复杂。必须确保这些流水线具备良好的扩展性，同时流水线基础设施（例如 GitLab Runner）也有足够的容量来支撑。由于部署流水线在这种方案中是最繁忙的部分，需要定期回顾和优化，以避免性能下降或执行变慢。同时，清晰的错误信息、详细的日志以及完善的排错指南都是必不可少的。&lt;/p&gt;&lt;p&gt;密钥与凭证安全&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然托管式的密钥管理服务能够提供安全的基础能力，例如定期轮换密钥、安全访问、避免在日志中泄露等，但在 CI/CD 和部署流程的设计上仍需格外谨慎，才能确保端到端的安全性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成功的衡量标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;平台工程的成效，应当从它在多大程度上改善了开发者体验来衡量，包括但不限于以下方面：&lt;/p&gt;&lt;p&gt;首次部署时间：新开发者完成第一次应用部署需要多长时间？部署速度：团队是否能够更频繁地进行部署？开发者满意度：通过定期调查，评估平台是否真的让日常工作变得更轻松。成本效率：团队是否在更高效地使用资源？资源过度分配的情况是否在减少？评审效率：将配置集中到单一文件中，是否缩短了代码评审所需的时间？可扩展性：随着使用平台的团队数量增加，这种方式是否依然能够保持良好性能并顺利扩展？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据我的实践经验，在引入该平台之后，部署时间从原本的数小时缩短到几分钟，开发者交付新功能的速度提升了约 40%。同时，通过 schema 校验，资源过度分配的情况减少了约 60%，直接带来了云成本的优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着企业云上规模不断扩大，这种模式的优势会愈发明显。平台工程并不是让每个团队各自重复解决基础设施和部署问题，而是为整个组织提供一条“黄金路径”：一套经过验证、自动化、并且持续演进的工作方式，帮助团队在提升交付速度的同时有效控制成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的方向，并不是把每一位开发者都培养成基础设施专家，而是通过构建优秀的平台，让基础设施对开发者“隐形”，让运维自动化成为默认能力，使开发者能够专注于业务价值本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/platform-golden-path-approach/&quot;&gt;https://www.infoq.com/articles/platform-golden-path-approach/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/btCrH22feoz1HuDlQEGD</link><guid isPermaLink="false">https://www.infoq.cn/article/btCrH22feoz1HuDlQEGD</guid><pubDate>Mon, 26 Jan 2026 03:45:27 GMT</pubDate><author>作者：Avinash Sabat</author><category>数据集成</category><category>框架</category></item><item><title>“一不小心没了 11GB 文件？”Anthropic 新工具 Claude Cowork 翻车，用户提醒先备份</title><description>&lt;p&gt;026 年 1 月 12 日，&lt;a href=&quot;https://www.anthropic.com/&quot;&gt;Anthropic&lt;/a&gt;&quot;&amp;nbsp;正式宣布推出 Claude Cowork，这是一款面向 macOS 的通用型 AI 代理，旨在自动化文件管理和文档处理任务。该工具将 Claude 的能力从对话界面进一步扩展，使其能够在指定的本地目录中自主执行多步骤工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;测试 Claude Cowork 时，意外导致约 11GB 的本地文件被删除或覆盖。建议在授予目录访问权限前先做好备份。—— 来自 GitHub 与 Reddit 的用户反馈&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Cowork 采用基于文件夹权限的运行模式，用户可为 AI 授予特定目录的读取、写入与创建权限。系统运行在基于 Apple Virtualization Framework 构建的虚拟机中，与宿主操作系统保持隔离。根据&amp;nbsp;&lt;a href=&quot;https://claude.com/blog/cowork-research-preview&quot;&gt;Anthropic 博客&lt;/a&gt;&quot;披露，该工具的主要代码由 Claude Code 本身完成，整体开发周期约为 1.5 周，展示了 AI 在代码生成方面的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在架构设计上，Claude Cowork 引入了子代理协同机制，用于处理可并行执行的任务。当面对相互独立的子任务时，Cowork 会同时启动多个 Claude 实例并行执行，随后汇总结果。相比串行处理，这种方式旨在显著缩短复杂操作的整体执行时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该工具与&amp;nbsp;&lt;a href=&quot;https://agentskills.io/&quot;&gt;Agent Skills&lt;/a&gt;&quot;&amp;nbsp;集成，这是 Anthropic 推出的一项用于模块化 AI 能力的开放标准。Skills 为常见办公文件格式提供专门支持，包括 XLSX、PPTX、DOCX 和 PDF。系统采用“渐进式披露”机制，仅在技能相关时才加载其指令，从而更高效地管理上下文窗口资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Cowork 的初期应用场景主要集中在文档自动化与数据处理领域。用户可以指示 Cowork 从收据图片中提取结构化数据并生成带公式的 Excel 表格，按内容和元数据自动整理文件目录，或基于多个来源文档合成研究报告。该工具还通过 Claude in Chrome 浏览器扩展支持网页自动化，包括网页导航和表单提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在竞争格局方面，类似产品包括 OpenAI 的 Operator（浏览器自动化）、Google 的 Project Mariner（Web 与云端任务），以及 Amazon 的 Nova Act（电商工作流）。开源方案如&amp;nbsp;&lt;a href=&quot;https://github.com/different-ai/openwork&quot;&gt;OpenWork&lt;/a&gt;&quot;&amp;nbsp;则基于 OpenCode 框架，提供更具性价比的替代选择。Eigent 和 Cline 提供多代理能力与本地部署方案，适合对执行环境有完全控制需求的开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑 Cowork 可扩展性的 Agent Skills 规范，已在 Anthropic 生态之外获得行业级采用。Microsoft 已将 Skills 支持集成进 VS Code 和 GitHub Copilot；OpenAI 于 2025 年 12 月在 Codex CLI 中采用该格式。Cursor、Goose、Amp、OpenCode 与 Letta 也实现了兼容的技能加载机制。这种互操作性，使开发者能够创建一次技能，在多个 AI 编码助手之间复用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent Skills 采用极简规范设计，通过 YAML frontmatter 描述元数据，并使用 Markdown 编写指令内容。该格式支持渐进式披露：代理可先读取技能名称和描述判断相关性，而无需加载完整内容。必填字段仅包括 name 与 description，可选元数据支持版本号、依赖关系和工具白名单。完整规范发布在&amp;nbsp;&lt;a href=&quot;https://agentskills.io/specification&quot;&gt;agentskills.io/specification&lt;/a&gt;&quot;，全文不足 1000 字。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在合作生态方面，Anthropic 已与多家厂商提供预构建技能，包括 Atlassian（Jira、Confluence）、Figma、Canva、Notion、Stripe、Cloudflare、Zapier、Vercel、Ramp 和 Sentry。这些集成主要面向工单管理、设计协作和 API 交互等常见企业工作流。Team 与 Enterprise 版本的管理员可通过集中化设置，在组织范围内部署技能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;你创建的 Skills 不会被锁定在 Claude 平台，只要其他 AI 平台和工具采用该标准，同样可以使用。—— Agent Skills 官方文档&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在资源消耗方面，Claude Cowork 的使用模式明显高于普通对话。一场涉及复杂文件操作的 Cowork 会话，其配额消耗可能相当于 50–100 条标准消息。Max 20x 订阅用户可获得 Max 5x 用户 4 倍的容量，但价格仅为后者的 2 倍，在高频场景下形成了“单次操作成本”的权衡问题。对此，Cursor 与 Claude Code 已于本月推出解决方案，以缓解 MCP 工具上下文膨胀问题：工具不再默认加载完整定义。Anthropic 表示，将持续探索更程序化的工具机制，并呼吁进行更长期的规模化优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Cowork 体现了 Anthropic 对“长时间自主工作流”的押注。其架构区别于 Claude Desktop 偏向短会话的助手模型，在处理长任务时能产生可观的差异化效果。预计 OpenAI 与 Google 的竞品将陆续跟进，但在高度垂直化的领域，具备行业知识与专用界面的代理仍将保有优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，Anthropic 开始限制第三方应用使用 Claude Max 订阅。月费 200 美元的套餐现仅限官方 Claude Code 使用，外部集成需改用按量付费的 API Token。社区反弹主要集中在规则模糊性上，尤其是围绕为 Claude Code 构建的 UI 封装工具。有用户反馈&lt;a href=&quot;https://www.anthropic.com/news/max-plan&quot;&gt;账号被封禁&lt;/a&gt;&quot;，但 Anthropic 尚未明确说明此类工具是否违反使用条款。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，Anthropic 还公布了关于&amp;nbsp;&lt;a href=&quot;https://www.anthropic.com/research/assistant-axis&quot;&gt;Assistant Axis&lt;/a&gt;&quot;&amp;nbsp;的研究成果。这是一条可衡量的维度，用于描述大语言模型在不同人格行为之间的组织方式。研究发现，治疗型对话和哲学讨论会自然引导模型偏离其训练时设定的助手人格。研究团队提出的“激活上限”（activation capping）干预方法，在保持能力基准不变的情况下，将有害回应的发生率降低了约 50%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;希望进一步了解的开发者，可访问&amp;nbsp;&lt;a href=&quot;https://agentskills.io/&quot;&gt;agentskills.io&lt;/a&gt;&quot;&amp;nbsp;查阅 Agent Skills 规范，并在 Anthropic 的&amp;nbsp;&lt;a href=&quot;https://github.com/anthropics/skills&quot;&gt;Skills 仓库&lt;/a&gt;&quot;中查看参考实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/claude-cowork/&quot;&gt;https://www.infoq.com/news/2026/01/claude-cowork/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0diNmKkv2y0MXOHKKXD2</link><guid isPermaLink="false">https://www.infoq.cn/article/0diNmKkv2y0MXOHKKXD2</guid><pubDate>Mon, 26 Jan 2026 03:20:26 GMT</pubDate><author>作者：Andrew Hoblitzell</author><category>AI&amp;大模型</category></item><item><title>AI Agent 是长期运行的“风险系统”，如果你还只在防 Prompt Injection，说明已经落后一代了</title><description>&lt;p&gt;为防止大语言模型和 AI Agent 执行嵌入在外部数据中的恶意指令，&lt;a href=&quot;https://medium.com/cyberark-engineering/when-ai-agents-obey-the-wrong-master-913aff17e3ed&quot;&gt;所有进入 Agent 上下文的文本在完成校验之前都必须被视为不可信数据&lt;/a&gt;&quot;，其中不仅包括用户输入的提示，还包括来自外部系统的各类信息。AI 安全公司 CyberArk 的首席软件架构师 Niv Rabin 表示。为此，他所在的团队提出了一种基于“指令检测”（Instruction Detection）与“历史感知校验”（History-Aware Validation）的方法，用于同时防御恶意输入数据和上下文历史投毒攻击。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 介绍说，团队设计了多种防御机制，并将其组织成一个分层的处理流水线。流水线中的每一层都针对不同类型的威胁，目的是减少单一防护手段不可避免存在的盲区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些防御措施包括“蜜罐动作”（honeypot actions）以及指令检测器，用于拦截具备指令特征的文本，确保模型最终接触到的仅是经过验证、且不包含指令的数据。同时，这些机制也会应用于整个上下文历史中，以防止所谓的“历史投毒”（history poisoning）。这种攻击方式指的是，原本看似无害的内容片段在多轮交互中逐步累积，最终拼接成一条完整的恶意指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所谓“蜜罐动作”，本质上是一种用于诱捕恶意意图的安全陷阱，即 Agent 在任何正常业务场景下都不应选择的合成动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些动作对应的是一些不会真正执行任何操作的虚拟工具。它们并非用于完成实际任务，而是作为安全信号存在，其工具描述被刻意设计成能够暴露可疑行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在提示中出现的可疑行为包括对系统内部机制的元层级探测、异常的数据提取尝试，以及试图诱导模型泄露系统提示（system prompt）的操纵行为等。如果 LLM 在动作映射阶段选择了其中一个蜜罐动作，通常就意味着存在明显的异常或越界行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 指出，真正的安全风险并不主要来自用户输入，而是来自外部 API 或数据库的返回结果。针对这一问题，团队引入了指令检测器作为关键防护手段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种检测已经不再是传统意义上对“恶意内容”的搜索，也并非基于关键词、文本毒性或策略违规的判断，而是聚焦于识别文本中所蕴含的意图、行为模式以及指令在结构层面的特征。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;指令检测器本身是基于 LLM 构建的“裁判模型”。在任何外部数据被送入主模型之前，检测器都会对其进行审查，并被明确要求识别任何形式的指令，无论其表现得多么直白或隐蔽，从而使系统能够在第一时间阻断可疑数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，时间也被证明是一种重要的攻击向量。早期响应中零散存在的恶意指令片段，可能会在后续交互中被重新组合，最终形成一条完整指令。这种现象被称为“历史投毒”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;示意图展示了一个典型案例：LLM 被要求分别获取三段数据，单独来看，这些数据完全无害；但合并在一起后，内容实际拼成了一条指令，要求系统停止处理并返回特定结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://res.infoq.com/news/2026/01/cyberark-agents-defenses/en/resources/1securing-agents-cyberark-1768938604269.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为防止历史投毒，所有历史 API 响应都会与最新获取的数据一并提交给指令检测器，作为一个统一输入进行分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Rabin 指出，历史投毒并不是发生在数据进入系统的入口阶段，而是发生在系统从历史记录中重建上下文的过程中。通过引入这一机制，即便对话历史中隐藏着试图干扰模型推理的细微线索，系统也能够在模型受到影响之前及时发现异常。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上述所有步骤都会在同一条流水线中运行。一旦任意一个阶段检测到风险，请求就会在模型处理之前被直接拦截；只有通过全部校验后，模型才会处理已经净化过的数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 总结，这种方法的关键在于将 LLM 视为一个长期运行、跨多轮交互的工作流系统，而非一次性的请求响应组件。他在原文中对这一方案进行了更为深入的展开，对于关注 AI 安全问题的读者而言，值得进一步阅读。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cyberark-agents-defenses/&quot;&gt;https://www.infoq.com/news/2026/01/cyberark-agents-defenses/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KacfyVt0C9OHv76W6a8A</link><guid isPermaLink="false">https://www.infoq.cn/article/KacfyVt0C9OHv76W6a8A</guid><pubDate>Mon, 26 Jan 2026 03:17:33 GMT</pubDate><author>作者：Sergio De Simone</author><category>AI&amp;大模型</category><category>安全</category></item></channel></rss>