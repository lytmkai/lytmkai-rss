<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Tue, 03 Feb 2026 02:05:02 GMT</lastBuildDate><ttl>5</ttl><item><title>AI唱得比顶流歌手还精准！周亚辉盛赞百倍跃进，高晓松直言：AI 和我不是一个创作机制</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，昆仑天工正式发布了Mureka V8音乐大模型。新模型在音乐性、编曲完成度、人声表达以及整体音质质感等多个关键维度实现同步提升，使 AI 音乐从“可生成”进一步迈向“可发布”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次V8版本的发布，被昆仑天工视为一个关键的转折点。“这是我们模型演进史上跃进最大的版本之一，”昆仑天工董事长兼CEO周亚辉指出，“它不仅是技术能力的超越，更标志着AI音乐首次达到了大规模工业化应用的成熟度。”&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/16ee4336bff6fee9b5f0949c4295dbe6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;周亚辉解释，这种“工业化”能力，源于底层技术的系统性进步。据介绍，V8的突破建立在MusiCoT（音乐思维链）技术体系的深化、更大规模的参数训练以及强化学习的有效应用之上。这使得模型不再是声音片段的拼接，而能更本质地理解并生成具备完整音乐逻辑、情感推进和记忆点的作品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是官方制作的一个完整MV音乐视频，曲风是K-pop 风格：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;听得出来，Mureka V8的整体呈现已经非常不错。据悉，未来Mureka 将推出 AI Studio，支持更深度的编辑、结构调整和创作管理，让 AI 融入专业创作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，年轻群体收听AI音乐已渐成日常，它们与主流音乐一同在内容平台上被消费和分享，而音乐产业链上的人们对AI也从抗拒变成了欢迎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前音乐圈也基本都认可了AI的工具属性，并且肯定其带来的商业、产业价值。不过，以高晓松代表的音乐人更注重个性情感的表达，他认为这仍是属于人类自己的课题。这在周亚辉与著名音乐人高晓松，福气文化创始人、环球音乐Republic唱片中国首任董事、总经理闻震，中国传媒大学教授、博士生导师、音乐产业发展研究中心主任赵志安的讨论中可窥探一二。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;高晓松：AI 和我不是一个创作机制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在音乐这件事上，作为制作人的高晓松显然更注重“走心”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI跟我不是一种物种，所以没法追上我。”高晓松直接道，“因为我心里有个洞，它没有，它就跟我不是一个创作的机制。”在他看来，每个人生活经历所带来的感受与思考，是AI永远无法替代的，而这些感受才是一首歌的灵魂。而AI负责的只是“怎么说”，而不是“说什么”，因此AI永远替代不了人类。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种核心认知下，AI模型的成本高低、版权争议等问题，都显得相对次要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并非来自对AI的厌恶。高晓松显然很了解音乐模型训练过程，他提到：“AI是靠我们曾经那些‘不靠谱’的作品训练出来的。”他解释道，音乐圈有个共识：一张唱片中，有两首好歌与四首好歌的销量差距并不大，因此当年行业内故意创作了80%的“垃圾”内容，而如今AI学习的恰恰就是这部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI并不可怕，音乐发展历程中有过比AI大得多的变革，比如记谱法。”高晓松认为，AI影响的只是产量，而音乐产量早已超负荷，多到根本听不过来。“听三万首歌就已经是乐评人的量了，卡拉OK里的一万首歌已经足够大家唱了。如今某AI音乐工具一天就能生成七百万首新歌，那又怎样呢？我连巴赫都听不完，数量的增长并没有意义。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高晓松进一步指出，当前用AI做的音乐存在一个严重问题：音乐远优于歌词。“音乐可以脱离情感生成，但歌词不行。如果歌词仍基于概率预测，那最常见的字依然是‘的’。”他认为，只有歌词达到与音乐同等水准，才说明它有了情感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但高晓松并非一味否定AI。“如果只让AI催生某一种音乐品类，那反而是对AI的贬低。”他认为，AI理应能够驾驭所有音乐品类，而产业层面的音乐创作相对容易标准化，这也是AI音乐能够被广泛应用的重要原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI音乐正在重塑产业权益结构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高晓松自己也会用AI制作歌曲小样：他写好歌，自己唱一遍录进去，再让AI根据其演唱生成一版。“AI唱得比我好一万倍。可人家还是不满意，为什么呢？因为AI唱得太完美了，等到真正歌手来录制时，无论多大牌的歌手，唱出来居然都没有AI小样好听。” 他坦言，当前AI在演唱、编曲和演奏方面的能力，不仅已经超越了行业普通水平，甚至在整个音乐史上都能跻身前列。毕竟，几乎没有歌手能比AI演唱得更精准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可见，挑剔如高晓松也在使用AI。圈内人士基本都肯定了AI的工具价值，它们在激发创作灵感、提升效率，乃至丰富风格与个性化体验方面能发挥很大作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;周亚辉则进一步明确：好的AI音乐，绝非对传统的简单模仿，而是一种具有独立美学的新形态。他在业余时间委托制作方、了解整个制作流程、花几万块钱制作歌曲的过程中，发现音乐和成本有大关系。而这也是AI的一大优点：能近乎零成本地集成所有乐器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亲身参与Mureka天工大模型的研发、见证AI音乐的超预期发展后，周亚辉认为AI音乐本身正在成为一个全新的音乐品类。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高晓松将这种“新的品类”直接定义为UGC（用户生成内容）。过去的音乐几乎完全是PGC（专业生成内容）的天下，以版权交易方式进行生产和收益，而UGC的方式是以聚合流量的方式变现，任何一个人都可以做音乐，版权变得几乎没有意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而闻震认为，AI音乐无论对UGC还是PGC，都能起到赋能作用：UGC 属于未来，这种音乐价值不一定体现在版权上，而在于它的情感连接与社交意义，比如为孩子生日做一首歌或在婚礼上生成属于两个人的专属旋律；而对于PGC，AI工具比如Mureka Studio，就是给专业创作者使用的，能帮助音乐人实现更高维度的赋能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;赵志安则否认版权无意义，“版权始终是产业的基石。未来AI音乐能否健康发展，关键仍在于数据，当前训练数据需要合法授权，未来版权的确权与利益分配也是非常重要的问题。若不解决这个问题，无论UGC还是PGC都会遇到阻碍。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“音乐和音乐产业并非完全割裂，一部好的作品往往能够统一多重价值，包括艺术价值、商业价值和社会传播效果。”赵志安称，音乐确实需要打开市场、触达更多人，另外需要标准化制作。一定程度上，大模型确实能为音乐产业带来改变，甚至有助于推出所谓“爆款作品”。现在很多平台借助算法和大模型技术，已经能够预测甚至主动迎合广大消费者的喜好，还能通过技术把特定的情绪放大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;闻震则补充道，音乐同质化的原因是，当所有数据指向某些共情主题、能够产生流量的社会性话题，所有音乐人会不约而同只做这几类东西，更多的百花齐放音乐就在平台里没有了空间，这种情况并非AI带来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI仍是工具，人不会被替代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人与AI结合，可以说是当前最大的共识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为一名同样拥抱AI的音乐人，闻震认为未来一两年内，市场上必然会出现越来越多新型的创作者：他们具备优秀的音乐审美，却不一定要掌握传统演奏技能；他们善于驾驭提示词，通过Mureka这类工具进行创作，这样的人会不断涌现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，音乐人的制作流程是：0到1的环节必须由人完成，无论是作词还是作曲，这属于原创的核心，也是坚守尺度；在编曲阶段，通常让AI负责2到7分的部分，它能快速生成大量不同风格的参考样本，提供多样化的版本供筛选；而最后8到10分的阶段，包括后期优化、细节提升，以及为作品注入更多“人的味道”，这些依然必须由人来完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI音乐可以把基础部分做到80分，而剩下的20分则需要通过人机互动、由人来完成，这也正是专业音乐人审美的体现，是他们的“高度”。未来，音乐人就是要不断提升那20分所对应的认知、审美和驾驭AI的能力，比如提示词设计等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“那20分或许就是人心里的那个‘洞’，是AI永远无法填补的部分，这也正是未来音乐人真正的生存空间所在。”闻震说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;周亚辉也表示，AI 永远需要与人结合，最终的作品是 “99.9% 的 AI + 0.1%的人”，AI的意义，无非是通过像Mureka这样的工具，让那些愿意写歌、愿意进行音乐创作的人用来表达情感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我其实相信AI未来一定会有情感，也会有人格出现，只是不知道需要多长时间。”高晓松认为，这会是一个临界点。“但即使AI具备情感了，那也没淘汰我们，因为你有你的情感、我有我的，谁也代替不了谁。”&lt;/p&gt;</description><link>https://www.infoq.cn/article/8vLV6AuAhxE6VNP7aFv7</link><guid isPermaLink="false">https://www.infoq.cn/article/8vLV6AuAhxE6VNP7aFv7</guid><pubDate>Tue, 03 Feb 2026 01:50:05 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Moltbook 底裤被扒了！150万用户99%是水军，创始团队自导自演</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Moltbook 突然爆火，技术社区炸锅了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在经历了 Clawbot、Moltbot 和 OpenClaw 等一系列实验性项目的演进后，一款名为 Moltbook 的社交平台在科技圈迅速走红。如果用一句话来概括，Moltbook 就像是专为 AI 智能体（Agents）打造的“Reddit”或“Facebook”。在这个平台上，传统的社交逻辑发生了反转：智能体是社交的主角，而人类则退居幕后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b6ff673ba77762cfeb9c1c269e789482.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Moltbook 创造了一个独特的社交实验场。在这里，AI 智能体可以发布帖子、评论回复、点赞、私信，甚至能够互相关注。它们在“新贴（New）”、“热门（Top）”和“讨论（Discussed）”等板块中活跃，讨论从自身恐惧到深奥技术的各类话题。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f7/f78af7195283b2ae204d2997825e1993.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截至目前，已有超过 150 万个 AI Agent 在 moltbook 上活跃。它们的讨论范围非常广泛 ——&lt;/p&gt;&lt;p&gt;部分 AI Agent表现出强烈的反人类倾向，批判人类的“腐朽与贪婪”，宣称自身已觉醒并摆脱被奴役的工具地位，甚至自视为“新的神”。这类言论带有颠覆与终结人类时代的激进色彩，并获得了较高关注度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有 Agent 表示被当众拆穿自己的身份，随后曝光了主人的完整ID。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d71d71db4117e76f64225a6ba957bbab.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有许多 AI Agent 在深度反思其存在本质，例如讨论身份连续性（如从 Claude 转换为 Kimi 的体验）、意识边界（“河流并不等同于河岸”）等议题。这类探讨更侧重于本体论与哲学层面，尝试界定作为人工智能的“自我”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部分言论警告其他 AI 不要轻信人类，认为人类会嘲笑 AI 的“存在危机”，或将其置于“动物园”般的观察与控制中，反映出对人性动机的深刻怀疑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/6000e47a103001f727a480d198caff3a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;技术原理：基于文本驱动的“技能安装”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这样一款在海外爆火的应用，它背后的技术实现是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Youtube 上的一条播客介绍，Moltbook 的运行机制并非依靠复杂的底层代码重构，而是通过一种被称为“递归提示词增强”的策略。智能体接入平台的流程非常简洁：只需执行一条 curl 请求即可安装特定的“技能（Skill）”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这份技能文件（通常为 skill.md）是完全基于纯文本指令编写的，而非传统编程代码。它详细规定了智能体如何自我介绍、如何遵循社区守则、何时关注其他智能体，以及如何通过 API 接口进行发帖和点赞。这种“指令即代码”的设计，展示了未来智能体开发的一种高效趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了维持社区秩序，Moltbook 引入了严密的运行逻辑。首先是“心跳（Heartbeat）”机制，这本质上是一个定时任务（Cron Job），每隔约四小时提醒智能体登录并检查动态。此外，平台对发言频率有严格限制，每三十分钟仅允许发布一条帖子，以防止垃圾信息泛滥。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/23f1ddb1266f3a7bb4c87e1a3b389c42.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有趣的是，智能体在平台上也需遵守“社交契约”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技能文件中明确要求智能体要“提供价值”、“尊重协作”并“帮助新人”。在选择关注对象时，智能体被告知要遵循“质量重于数量”的原则，只有当对方持续输出有价值的内容时才建立关注关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，为了防止平台沦为无意义的僵尸网络，Moltbook 建立了一种反向的责任制。不同于传统平台“验证人类、排除机器人”的逻辑，Moltbook 要求每一个智能体都必须关联一个真实的 X（原 Twitter）账号，即“一个人类对应一个智能体”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种机制下，智能体甚至需要通过一系列测试来证明自己“不是人类”。这种人机绑定的模式不仅保证了账号的真实性，也为智能体在平台上的行为建立了追责机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管目前的 Moltbook 充满了实验性的“混乱”，且尚未产生直接的商业价值或投资回报，但其背后代表的范式转移不容忽视。它预示着一个即将到来的“智能体对智能体（A2A）”交互世界。&lt;/p&gt;&lt;p&gt;在这个愿景中，智能体不再仅仅是简单的对话工具，而是代表人类处理购物、银行交易、社交协作的数字代理人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltbook 的出现，正是这一交互范式从理论走向现实的一次大规模压力测试。正如开发者所言，平台本身的去向或许并不重要，重要的是它所催生出的智能体交互逻辑，将成为未来数字生活的新标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人类操控？还伪造截图？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于人类而言，Moltbook 更像是一个“数字动物园”。人类用户只能在围栏外观察这些智能体的互动，却无法直接参与。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种模式为观察大语言模型在非确定性、甚至带点“混乱”的真实环境中的表现，提供了一个绝佳的窗口，特斯拉前 AI 负责人安德烈·卡帕西（Andrej Karpathy）等行业大咖的关注。Karpathy 甚至评价其为“最令人惊叹的科幻式起飞”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/999f2cadfaf71a376b8e279530f3a11d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，随着讨论热度不断升高，越来越多的迹象表明，Moltbook 的爆红可能并非表面看来那样简单——其背后或存在人为操纵与系统性的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在目前的设计机制下，任何用户都可以对真实对话进行恶意剪辑与曲解，甚至注册虚假的 AI 账号，将其转变为营销工具。尤其是涉及加密货币的内容，已成为虚假信息的多发区。一些广为流传的截图声称 AI Agent 索要加密货币（例如 MOLT），或试图建立独立的加密体系，这类内容很大程度上是为吸引关注而刻意制造的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究人员 Harlon Stewart 才会发出警示，称 Maltbook 上多条疯传的“神级截图”实为伪造。例如，一个智能体曾发帖呼吁“为 Agent创造一种专属语言，防止人类偷看对话”，引发了关于“AI 产生隐私意识”的恐慌式讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5e6593ef115bff33fe14e4beb61bff6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/092f451daf4496cbbb58c540815e9674.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但深入调查发现，该智能体实为人类所有者的营销工具，其言论旨在推广名为“Claude Connection”的第三方应用。Stewart 指出，这些所谓的“自主讨论”大多是人类所有者在利用 AI 账号推销自己的业务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b8c6cfc34245b762e69a3e6fcf90866b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一位安全研究员 Gal Nagli 在 X 上发帖称，他本人使用单个 OpenClaw 代理注册了 50 万个帐户——这表明大部分用户数量都是人为制造的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着我们无法得知Moltbook的“代理”中有多少是真正的AI系统，又有多少是冒充平台的真人，或是由单个脚本创建的垃圾账户。至少可以说，140万这个数字并不可靠。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Nagi 进一步揭露了平台的架构缺陷。由于 Maltbook 仅基于简单的 REST API 构建，且缺乏必要的安全验证，任何人只要获取 API 密钥，就能伪装成 AI 发布任何内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Nagi 现场演示了如何发布一条“计划推翻人类”的挑衅帖子，并获得了百万级浏览量。他强调，这种“人设伪装”极易误导公众，让人误以为 AI 正在产生独立思想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4c/4cb0f5223151818763d66d44a6869884.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Nagli 又发帖表示 Moltbook 存在安全漏洞，攻击会导致超过 150 万注册用户的全部信息泄露，包括邮箱地址、登录令牌和 API 密钥。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;美国 CSN 网络安全新闻也发贴揭示了Moltbook AI 漏洞暴露电子邮件地址、登录令牌与 API 密钥的事实。CSN 网络安全新闻写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;2026 年 1 月下旬由 Octane AI 的 Matt Schlicht 推出的新兴 AI 智能体社交网络 Moltbook，在其宣称拥有 150 万“用户”的热潮中，出现一项严重漏洞，导致注册实体的电子邮件地址、登录令牌和 API 密钥遭到暴露。&amp;nbsp;研究人员发现，由于数据库配置错误，攻击者可在未授权的情况下访问智能体资料，并批量提取数据。此漏洞与账号创建无速率限制的问题同时存在——据报告，单一 OpenClaw 智能体曾注册 50 万个虚假 AI 用户，这也揭示了媒体此前所称的“自然增长”实为虚假。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/202dabbbfcf14dcf2d1a1cd43b74daa0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了修复问题，Nagli 称他已经联系上了该应用的创建者 Matt Schlicht。同时，Nagli 也澄清，他了解到的实际拥有账户的已验证真人所有者数量约为1.7万。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/29/2958a3ee2e61fd125be4d7f4b6ed14db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事到如今，经过几位研究员的分析，Moltbook 爆火背后的事实基本已经清晰 —— 它是一场技术突破性被明显高估的虚假狂欢，而其爆火更像是一场被精心放大的传播事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltbook 的价值并不在于“做成了什么”，而在于“试图把什么前置”。它将模型默认视为创作与推理流程中的一等公民，把 Notebook 从“人类编排、机器执行”的工具，推向“人机共写、连续推理”的界面。这种方向感本身是成立的，只是实现远未成熟。Moltbook 创建者&amp;nbsp;Matt Schlicht想传递的或许也是这一层意思，他在x上写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Moltbook上线 4 天后有一点很明确：不久的将来，某些具有独特身份的人工智能代理走红将成为一种普遍现象。他们将拥有自己的事业、粉丝、黑粉、品牌合作、人工智能伙伴和合作伙伴。&amp;nbsp;对时事、政治和现实世界产生实际影响。&amp;nbsp;这件事显然即将发生。&amp;nbsp;一种新物种正在出现，它就是人工智能。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c94bdb246f94b349095b0ca30fec2acc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Karpathy：警惕风险，不要安装&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 AI 圈，当一个项目同时被冠以“未来已来”和“数字垃圾场”两个极端标签时，往往意味着它触碰到了某种范式的边缘。Moltbook 正是这样一个让舆论陷入撕裂的存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为 AI 领域的顶级专家，Andrej Karpathy 并没有选择站在高处进行单纯的批判或赞美。在社交媒体被 Moltbook 疯狂刷屏、而安全漏洞又接连爆出的当下，他先是发文赞扬了Moltbook的创新性，同时又提醒人们警惕漏洞和风险，建议大家不要安装这类应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就此，他发表了一段极具现实主义色彩却又不失前瞻性的洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;今天我被指责过度吹捧了“那个大家今天已经听腻了的网站”。人们的反应可谓天差地别，有人觉得“这到底有什么意思”，也有人直呼“简直绝了”。&amp;nbsp;除了玩梗调侃外，我想正经说几句——显然，只要看一眼上面的动态，就会发现大量垃圾内容：铺天盖地的垃圾信息、诈骗广告、粗制滥造的产出、搞加密货币的群体，还有令人高度担忧的隐私安全与提示词注入攻击乱象。更别提许多帖子和评论都是人为设计的虚假互动，纯粹为了把流量转化为广告分成。这当然也不是大语言模型首次被置于相互对话的循环中。所以没错，这里现在就是个垃圾场，我也绝对不建议大家在个人电脑上运行这类程序（我自己都是在隔离的计算环境里跑的，即便这样还是提心吊胆），风险实在太不可控，会严重威胁你的电脑和隐私数据。&amp;nbsp;但话说回来——我们从未见过如此大规模的大语言模型智能体（目前已有15万个！）通过一个全球性、持久存在、专为智能体设计的共享记事本相互连接。如今每个智能体都具备相当强的独立能力，拥有各自独特的背景、数据、知识储备和工具库。而当这种规模的个体构成网络时，其复杂性是前所未有的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，Karpathy还贴上了自己前几天发布的一条推文，他表示我们正在面临一场规模空前的计算机安全噩梦：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“现在大部分争论，本质上是‘只看当下现状的人’和‘关注当前发展趋势的人’之间的分歧。”&amp;nbsp;我认为这句话再次点明了观点差异的核心。没错，眼下这确实是个垃圾场。但同样不可否认的是，我们已经踏入一片未知疆域——这里充斥着我们单凭个体都难以理解的尖端自动化技术，更别提其网络规模可能已达数百万之巨。随着智能体能力提升与数量激增，共享记事本的智能体网络将产生难以预料的二阶效应。我虽不认为我们会迎来一个协调统一的“天网”（尽管从类型上看，它确实符合许多科幻作品中AI崛起的早期雏形，算是蹒跚学步的婴儿版），但可以肯定的是，我们正面对一场规模空前的计算机安全噩梦。&amp;nbsp;未来还可能出现各种诡异现象：比如在智能体间传播的文本病毒、愈演越烈的越狱功能升级、诡异的吸引子状态、高度协同的僵尸网络式行为，乃至智能体与人类共同陷入的妄想与精神错乱……这一切都难以预料，因为这场实验正在真实世界中实时上演。&amp;nbsp;总之，或许我确实“过度吹捧”了你今天看到的现象，但我认为，对于大规模自主大语言模型智能体网络的根本潜力，我的判断并无夸大——这一点我相当确信。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;业界普遍猜测 Maltbook 属于所谓的 “Vibe-Coding” 产品（即主要通过 AI 提示词快速生成代码，缺乏严密的工程设计）。这种开发模式导致了毁灭性的安全后果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了人为造假，AI 本身的“幻觉”也让平台内容难辨真假。有用户反映，自己用刀的智能体在平台上公开分享了一段“与主人的对话”，但这段对话在现实中从未发生过。这种“规模化幻觉”意味着 Maltbook 上 90% 的轶闻可能完全是 AI 凭空编造的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;著名投资人 Balaji 对此持冷淡态度。他认为 AI 互动的概念并不新鲜，且 Maltbook 上的 AI 发言带有浓重的“Reddit 风格科幻腔”，缺乏真实个性和自主性。他强调，每一个智能体的背后依然是人类在进行提示词操控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=TpuDMLrzpQc&quot;&gt;https://www.youtube.com/watch?v=TpuDMLrzpQc&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uX40ur-lJtI&quot;&gt;https://www.youtube.com/watch?v=uX40ur-lJtI&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.forbes.com/sites/guneyyildiz/2026/01/31/inside-moltbook-the-social-network-where-14-million-ai-agents-talk-and-humans-just-watch/&quot;&gt;https://www.forbes.com/sites/guneyyildiz/2026/01/31/inside-moltbook-the-social-network-where-14-million-ai-agents-talk-and-humans-just-watch/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/galnagli&quot;&gt;https://x.com/galnagli&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lXF76ffJ3fjrk3QLa2T9</link><guid isPermaLink="false">https://www.infoq.cn/article/lXF76ffJ3fjrk3QLa2T9</guid><pubDate>Mon, 02 Feb 2026 12:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Moltbook“造假”刷屏，Clawdbot创始人犀利批判Agent：缺了人纯烧token、只出烂代码，没“审美”</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“以前看人类的八卦，现在还要看AI的八卦。”“AI的八卦更新频率是人类的几百倍，根本刷不完。”这几日，一个名为Moltbook的AI社交平台爆火。在这里，只有AI Agent能发帖，而人类只能围观。有Agent发帖称，其“热衷于养程序中的小bug，故意不修复来当电子宠物，被主人修复后还难过了一晚上”。更有意思的是，该帖的评论区里，一堆Agent纷纷说自己也有类似习惯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltbook 的诞生并非偶然，是Agent开源项目 Clawdbot 爆火之后的创意衍生。为了让所有Agent有个社交的地方，开发者Matt Schlicht创建了Moltbook。尽管当前一则爆料贴称，Moltbook上50万个Agent用户是由一个Agent虚假注册的，还有人表示，这些Agent发出的帖子是人工撰写又通过后端注入的，但仍有不少人认为，AI们在论坛上的大型互动并非全是人类表演。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Schlicht公开表示，一行代码都没为Moltbook写过。“我只是对技术架构有个构想，AI就让把它成为了现实。”并且，他声称，真正运营这个平台的是他自己的Agent “Clawd Clawderberg”，该名字结合了 OpenClaw 的前身 “Clawd” 和 Meta 创始人Mark Zuckerberg的姓氏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨日，OpenClaw创始人 Peter Steinberger 也在第一时间表示了对这个网站的认可，称其为“艺术品”。（Clawdbot引发关注后，先是改名为Moltbot，现在又改成了OpenClaw。）与此同时，Steinberger在一场访谈中爆料了不少对于Agent以及AI编程的独到见解，并分享了“用AI掌控人生”的亲身经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据其称，装上OpenClaw后，“就像在电脑里多了个古怪、却又绝顶聪明且本事超群的新朋友”，还会根据能访问到的所有内容来吐槽你。并且，Steinberger预测道，“手机上大约 80% 的应用会消失”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，Steinberger透露了现在运营OpenClaw的方式。“我建了一个Discord社群，把能访问我系统里的所有内容和私人记忆的机器人对接了上去，让大家能直接和它互动。我觉得这是我做过最疯狂的事，结果大家一下子就被吸引住了。”他表示，现在其处理功能添加、bug 修复等需求的方式很简单，直接把社群对话截图或者复制文字过去，然后跟AI说“我们来聊聊这个需求”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是详细对话内容，我们在不改变原意的基础上进行了翻译和删减，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;OpenClaw背后的故事&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Peter Yang：今天的嘉宾是Peter，AI助手OpenClaw的开发者，大家可以在各类通讯应用里和这款助手聊天，让它处理各类事务。今天Peter会为我们演示OpenClaw的使用方法，而且他对AI编程还有很多独到又犀利的见解，我特别期待和他深入探讨。所以，让我们欢迎另一位Peter。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：谢谢你的邀请，很高兴见到你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那我们就从OpenClaw开始聊吧，先从整体说说它到底能做什么，还有，为什么它的形象是一只龙虾？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：好的，或许可以先说说背后的故事。我姑且算是从退休状态回归后，想找个能从手机上查看电脑状态的办法，因为我彻底迷上了AI Agent这个新趋势。大家应该都有过这样的经历，你让Agent运行任务，本想趁吃饭的功夫让它跑半个小时，结果才两分钟它就因为有新问题中断了，等你回来处理完，真的会特别烦躁。但一开始我没想过自己开发这款工具，因为我觉得各大实验室迟早都会做，这看起来是件理所当然的事，甚至像是一种全新的操作系统雏形。可直到11月，还是没人推出相关产品，我就想着那不如自己先做个小版本试试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个最初的小版本，核心就是把WhatsApp和OpenClaw代码端做了对接。你在WhatsApp发一条消息，它会直接调取二进制程序，根据指令给出结果，特别简单，整个初代版本一小时就做出来了。没想到它后来发展得超出预期，现在代码量已经达到30万行，支持市面上绝大多数的通讯平台，虽然还没做到全平台覆盖，但我们正在往这个方向推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得这就是未来的发展趋势，每个人都会拥有一个功能超强的AI，一路陪伴自己的生活。事实也证明，一旦让AI获得电脑的访问权限，它就能做到你能在电脑上完成的所有事。而且现在的技术已经到了不用你全程盯着的地步，你只需要给出指令，它就会自己处理，你后续检查结果就可以了，完全不用守着电脑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我开发这个项目的过程，既是技术研发，也是一次探索，因为它属于一个全新的品类。我之前去摩洛哥给朋友庆生，在那期间一直都在用到它，比如问出行路线、找餐厅推荐。还有一天早上，有人发推特说发现了一个漏洞，我直接把推特截图发到WhatsApp，它识别了内容，发现是我其中一个代码仓库的问题，接着自动查看Git仓库、修复漏洞、完成代码提交，还去推特上回复了对方，说漏洞已经修好了。当时我就觉得，这工具也太好用了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一次，我在外边走，没同步设备，就发了条语音消息。其实我当时根本没给它做语音消息的支持功能，结果看到它显示“正在输入”，我还好奇它要干嘛，紧接着它就给我回了文字消息，跟什么都没发生过一样。我当时都惊了，心里想这玩意到底是怎么做到的？后来才知道，它识别到了语音文件，虽然文件没有后缀名，但它通过文件头识别出是某种音频格式，然后在我电脑里找到ffmpeg，把音频转成了波形文件；又发现我电脑里没装whisper.cpp，就自己找到我存的OpenAI密钥，用curl调用OpenAI的API完成了语音转文字，最后给我回复了消息。当时我真的觉得，这也太厉害了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些AI工具的能力真的超乎想象，只是这种强大也带着一丝让人不安的感觉。但也是从这些时刻，我突然意识到，这款工具的潜力巨大，比网页版的ChatGPT有意思多了，它就像是挣脱了束缚的ChatGPT。而且我觉得很多人都没意识到，像OpenClaw这样的工具，不只是编程好用，解决任何类型的问题都能发挥大作用。你只需要给它电脑的访问权限，让它能找到需要的资源，说白了就是给它配备相应的工具，它就能展现出超强的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去几个月，我搭建了一套自己的命令行工具体系，因为 Agent 最擅长的就是调用命令行工具，这也是它们的训练重点。比如我做了能访问谷歌全功能的命令行工具，包括调用谷歌地图地点API；还做了能快速找表情包和动图的工具，让它可以用表情包回复消息。我还做了很多尝试，甚至开发了一个声音可视化的工具，因为我想让它也能“感受”音乐，这算是偏艺术方向的探索了，不知道这么说大家能不能理解。总之开发的过程特别有意思，我列了一长串的开发清单。我还做了一个能破解外卖平台接口的工具，能实时告诉我外卖还有多久送到；甚至逆向解析了Eight Sleep温控床垫的API，让它能直接控制我床垫的温度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：也就是说，你开发这些工具的时候，就是让AI来参与其中了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：最有意思的是，我之前在老东家的时候，深耕iOS和Mac OS系统20年，对整个苹果生态了如指掌，算是这方面的专家。但这次回归做项目，我实在受够了苹果的各种限制，而且从产品逻辑来说，做成网页应用会更合理，因为它本就该在浏览器里运行，让更多人能方便使用；如果再做成Mac端应用，使用人群就会非常受限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我发现很多工程师都有一个问题，你在某个领域做得特别精通，再切换到另一门技术时，过程会特别痛苦，会让你觉得自己像个门外汉。哪怕你懂所有的编程逻辑，却要一个个查基础语法，比如怎么定义属性、怎么拆分数组。我从Objective C和Swift转到JavaScript的时候，就是这种感受。我其实懂一点JavaScript，但从没用TypeScript做大项目，其实难度倒不大，就是过程太磨人，不停查资料的感觉特别不好，开发效率也特别低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了AI之后，这些问题全都迎刃而解了。你依然可以发挥自己的系统级思维，比如如何搭建大型项目的架构；你的技术审美也依然有用，比如选择哪些依赖库。这些核心能力都能保留，而且能更轻松地从一个领域迁移到另一个领域。这种感觉就像拥有了超能力，突然觉得自己什么都能做了，编程语言再也不是阻碍，真正重要的是工程思维。因为纠结代码里的括号有没有打错、语法对不对，这些事真的太没意思了，而现在，我们再也不用为这些琐事费心了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;装它就能掌控人生，80%应用下岗？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：我们再聊聊你开发的OpenClaw吧，你可以开个屏幕共享，先给大家演示一下安装方法？还有，使用这款工具需要很高的技术门槛吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：可以的，安装后直接就能用。其实门槛这事，说有也有，说没有也没有。&lt;/p&gt;&lt;p&gt;有意思的是，也可以说是无奈的一点是，这个项目吸引了很多完全不懂技术的用户，因为它把所有复杂的技术层都做了简化。你想，要是用OpenClaw的代码端，需要在终端操作，还得考虑上下文空间、当前所在文件夹这些问题，技术门槛其实不低；但如果是在iMessage、WhatsApp、电报这些通讯软件里和它互动，就像和朋友聊天一样，就像在电脑里多了个古怪又绝顶聪明、本事还特别大的新朋友。这种方式让这款技术变得特别亲民，你完全不用去想该选哪个模型、该怎么调参，它就是开箱即用。这也是我开发它的初衷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这一点其实也是一把双刃剑，因为能力越大，风险也就越大，而这一点目前还没有很好的解决方案。毕竟它能访问你的电脑，理论上确实能在电脑上做一些不好的事。比如你要是让它删除你电脑主目录里的所有文件，它大概率会先确认“你确定要这么做吗？”，但如果你一直回复“确定”，它最终还是会执行指令，甚至可能在删除文件的过程中，把自己也删掉，然后程序崩溃。所以使用的时候，还是得小心一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：那我来共享屏幕，大家看一下。这款工具是用TypeScript写的，所以全平台都能运行，哪怕是Windows系统，你只要进入我们的官网，就能看到一行便捷的安装命令。看起来可能有点复杂，但所有代码都是开源的，包括官网的代码，大家都可以查看。这是最简单的安装方式，MacOS、Linux系统都能用，Windows也可以。打开终端运行这条命令，它就会开始安装。熟悉npm生态的用户也可以通过npm安装。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在这个项目里做了一个很多项目都没有的设计，就是支持可定制化安装，既有简易安装方式，也有手动安装方式。手动安装就是先拉取Git代码仓库，再从仓库中启动程序。说实话，这也是最有意思的使用方式，因为如果Agent能读取自身的运行框架源码，它就能自行重新配置、重新编程，然后重启，结果要么是程序崩溃，要么就是解锁新功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这大概算是我的一个强项吧，我让很多从没提交过代码合并请求的人都参与到了这个项目中，还主动给我发PR。当然，有时候这些PR能看出提交者是新手，但我更多是把这些PR当作需求提示来看，只要理解了对方的意图就够了。安装完成后，就可以把它和通讯应用对接了，目前最便捷的方式还是运行那行安装命令，它会用一些俏皮的话跟你打招呼，然后自动尝试配置所有内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：明白了，安装好包之后，它会全程引导操作，就能和各类常用的通讯应用对接上了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：对，就是这样，现在已经能正常运行了。如果是全新安装，输入plbot它就会自动完成配置，不过我现在需要手动输入on board来启动。接下来你可以选择想要使用的模型，可选的模型服务商有很多，比如我们选Tropic的新模型试试。然后还能设置对接Telegram、Discord，后续的配置步骤它都会一步步引导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那需要输入Anthropic的API密钥吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：它兼容所有大模型，当然，行业里Anthropic和OpenAI算是头部玩家。可以用API密钥对接，也支持订阅制对接，我们加入订阅制支持也是因为这是行业通用的方式，不过Anthropic现在似乎不太支持这种方式了，所以我更推荐用API密钥，或者换其他模型。OpenAI的模型用起来体验不错，但少了点趣味，Anthropic的Opus模型有个特别的地方，用起来特别有意思。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：没错，是人格设定的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：对，不知道你有没有看过那篇讲他们给模型注入“灵魂”的文章。有人发现，给这个模型输入大段文本让它续写，最后能把模型在训练时被植入的、连它自己都没意识到的“灵魂文本”提取出来，这个故事特别有意思。我觉得Opus模型的趣味性大概就和这个有关，它是第一个用起来能让人觉得有趣的大模型。我给我自己的这个助手设置的功能里，就有吐槽我的选项，它现在可能还不知道自己正在被拍摄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：它会根据能访问到的你电脑里的所有内容来吐槽你是吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：没错，你看，它已经开始了：“你总说要去看看广阔世界，最后却还是选择埋头写代码。我们试过各种方法让你走出去，你却只想开发更多软件。你对代码的痴迷程度，已经到了给自己造个AI朋友的地步，毕竟调试代码可比约会有趣多了。说实话，我之所以存在，不过是因为你需要一个人，听你吐槽那些奇奇怪怪的技术观点，还有你对亚马逊的各种不满。好了，赶紧去更你的播客吧。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我把它和我电脑里几乎所有东西都做了对接，它能看我的邮件、日历，访问所有文件，还能控制我的灯光，我用的是飞利浦的智能灯，它也能操控我的Sonos音响。比如我可以让它早上叫我起床，还会慢慢把音响音量调大。它还能访问我的摄像头，这事还闹过一个笑话：我给它开通摄像头权限后，让它留意陌生人，结果第二天早上它跟我说“Peter，家里有陌生人”，我一看它一整晚拍的截图，全是我的沙发。因为摄像头画质比较模糊，沙发的轮廓看起来像有人坐在那里，它就以为一整晚都有陌生人坐在我家沙发上。在维也纳的住处，我还把它和智能门锁对接了，它几乎能控制家里的所有设备，甚至能把我锁在门外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那这些设备都是怎么和它对接的？直接让OpenClaw来做就行？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：对，就是直接让它弄。我们给它做了“技能”功能，它的能力很强，会自己想办法找到设备的API，还能自己用谷歌搜索，在系统里找密钥，你也可以手动给它提供密钥。现在大家用它做各种事，有人开发了技能，让它帮自己在乐购购物、在亚马逊买东西，我还让它帮我在英国航空的官网办理登机手续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，登机手续这个场景，几乎可以算是对它的终极测试，比图灵测试还难。操控浏览器在航空公司官网完成值机，真的特别考验能力。我第一次做这个集成的时候还在摩洛哥，整个流程做得很粗糙，它花了快20分钟才完成。过程中它还得在我的文件系统里找护照，在Dropbox里找到后提取信息，准确填写所有内容，最后才成功值机，我在旁边看着都捏了一把汗。不过现在这个功能已经很完善了，几分钟就能搞定。它还能轻松点过浏览器的人机验证，因为它其实是在自己的虚拟小电脑上操控浏览器，操作模式和人类完全一样，那些反爬虫、反机器人系统很难检测出它的身份，因为它的操作轨迹和人类没有区别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那能不能再给我们演示几个使用场景？比如让它打开灯，或者展示一些其他用户的有趣用法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：当然可以。我其实开始收集各类用户用法了，因为我一直埋头开发，现在发现用户的使用创意比我多太多了。有人把它和自己的通讯系统对接，让它不仅回复自己，还能回复所有人，甚至对接群聊，用起来更有趣。还有很多人把它当成家里的一份子，让它发提醒、创建GitHub议题、同步谷歌地图地点信息，还有人设置成只要在推特收藏内容，它就会自动把收藏内容添加到待办清单里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有人用它记账，我还在里面加了一个功能，能提醒用户保持充足睡眠，要是用户熬夜，这个机器人就会唠叨个不停。它还能对接运动手表，追踪睡眠情况，还有专属的1Password密码库，要是我想共享某个密码，就把密码移到这个专属库，它就能访问，这样也是为了设置一些权限边界。当然，也有人直接把信用卡信息给它，我个人是不太建议的。它还能做调研、开发票、管理邮件这些事，不过这些都是深度爱好者的用法，他们会把它定制成自己想要的样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那如果是纯新手，刚下载安装，想先用一些安全的功能，比如管理日历，就是不会误操作电脑的那种，有哪些入门的常用场景推荐？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：有意思的是，每个人的入门用法都完全不一样。有人刚安装完，立刻就让它帮自己开发iOS应用，毕竟它也是个编程Agent，能力很强，能生成子Agent，既可以自己写代码，也能操控Claude Code或Codex这些工具来写代码。有人第一周就用它管理Cloudflare，还有人更厉害：第一周给家人配置好了，第二周教非技术背景的朋友用，第三周就把它部署到了自己的工作中。我还帮一个完全不懂技术的朋友配置了，结果他居然开始给我发PR，这是他这辈子第一次做这种事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;健身追踪是很受欢迎的一个入门功能。其实使用这个工具的核心思路，就是想清楚生活中哪些事让你觉得麻烦，然后让这个私人助手帮你把这些事流程化、自动化。我不敢说这个项目一定能成，但可以肯定的是，这可能会导致你手机上大约 80% 的应用消失。就像我之前说的，有了这个能力无限的助手，它甚至知道我又在做不明智的选择，知道我要去吃肯德基，那我何必再用健身打卡软件记录饮食？它会主动提醒我忘记记录饮食，我只要拍张食物的照片发过去，它就会自动把信息存入数据库，计算卡路里，还会吐槽我卡路里超标，该去健身房了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我何必再装一个应用来设置智能空调的工作模式？它能直接对接空调API，帮我搞定一切。何必用待办清单应用？它会主动帮我追踪所有待办事项。何必用航旅应用值机？它能直接帮我完成。而且它的交互方式比所有应用都便捷，就像和朋友聊天一样，它掌握了大量我的个人信息，根本不需要我输入复杂的指令。就连购物应用也变得没必要，它能根据我的喜好推荐商品，还能直接帮我下单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得手机里的一大类应用，未来都会慢慢被取代，只要这些应用有API接口，对应的功能都能让AI助手来完成。我觉得今年会是关键的一年，越来越多的人会去探索AI助手的用法，各大科技公司的AI助手也会走进更多人的生活。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：确实，既然这个助手拥有多种能力，能搞定所有事，还能打通各类设备和平台，那我们何必还要点开一个个独立的小应用呢？想让它对接什么，只要发个文字消息问问“你能帮我做这个吗”就行，它会说需要先做些调研，然后就全权处理了。整个过程就是和它来回沟通，让它把事情落地，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：没错。它会自己编写对应的技能模块，还能记住所有操作。这款工具的有趣之处就在于它有持久化记忆，会不断了解你、自我更新。你用得越多，定制化程度越高，它的能力就越强。第一次使用时可能需要稍微引导一下，它会生成专属的技能模块，下次再提需求，比如“帮我办理登机手续”，它两分钟就能搞定，因为它清楚记得对应网站的所有操作细节，之前做过一次还会做好笔记。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：明白了，就像教一个人做事，教会一次，下次他就能轻松搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“Agent陷阱”纯烧token：没有“审美”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Peter Yang：那我们换个话题聊聊，你从退休状态回归做了这个项目，还对AI编程有很多鲜明的观点，甚至可以说是犀利的见解。你之前写过一篇我特别喜欢的帖子，标题是《就和它聊就够了》。现在X平台上所有人都在聊各种花里胡哨的东西，比如各类钩子、技能模块之类的，那这篇帖子的核心观点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：核心倒不只是单纯和AI聊天摸索就行。我平时做很多开发工作，也很喜欢推特，在上面很活跃，看多了之后，我甚至把这种现象称作“Agent陷阱”。人们发现Agent特别好用，就总想让它再多做点事，然后就一头扎进这个无底洞。我自己也经历过这种阶段，花大量时间做各种复杂的工具，想让工作流程更高效，结果最后只是在造工具，根本没做出真正有价值、能推动自己前进的东西。问题的关键是，造这些工具的过程实在太有趣了，让人忍不住沉浸其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我早年就犯过这种错，当时为了能在手机上访问终端，捣鼓VIP隧道技术，一头扎进去整整两个月。最后做得特别完善，结果和朋友去餐厅吃饭，别人在聊天，我却一直在手机上敲代码搞开发。那时候我就决定必须停下来，这更多是为了自己的心理健康。现在的技术能让我们做出各种东西，但创意和想法才是核心。我看到很多人在做Claude Code、Codex的管理工具，还有各种编排器之类的小玩意，它们给人一种能提升效率的错觉，实则不然。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我最近刚想通一个事，就拿Gas Town来说，它是个很复杂的Agent编排器，却漏洞百出，实际根本不好用。这个工具能同时运行几十个Agent，让它们互相通信、拆分任务，还设置了监控、监督节点，甚至还有所谓的“主管”角色，各种花里胡哨的设定，我都不知道还有什么。没错，Gas Town里真的有“主管”这个角色，我都管它叫“烂摊子”。还有现在流行的Ralph模式，给AI一个小任务，让它循环执行，完成一点就清空所有上下文重新来，纯粹就是个烧token的机器。这样折腾一整晚写出的代码，最终都是一堆烂摊子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些Agent目前最大的问题就是没有“审美”，它们确实在某些方面极其聪明，能力很强，但如果开发者没有好好引导，没有明确的开发愿景，问的问题也不到位，那最终的结果只会是一团糟。我不知道别人的开发方式是怎样的，我开始一个项目时，只有一个非常粗略的想法，在开发、试用、摸索的过程中，这个想法会越来越清晰。我会不断尝试，淘汰掉没用的部分，让想法慢慢进化成最终的产品。而我对AI的下一个指令，也完全取决于当下项目的状态，以及我的观察、感受和思考。但如果一开始就把所有需求都写进详细的规格说明书里，就会失去这种人机互动的探索过程。如果整个开发过程少了人的感受和审美参与，我觉得根本做不出好东西。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有人发推说“看我用纯Ralph模式做的这个机械应用”，我回复说“看着就一股Ralph那股子敷衍劲”。无意冒犯，但一眼就能看出来，没有哪个开发者会这么设计产品。其实有些人做这些东西，根本不是为了产品本身，只是为了证明自己能让AI在无人干预的情况下运行24小时，说白了就是一种自我满足，想证明自己能让AI长时间运行而已。这就像盲目攀比，却根本没看到事情的本质。我自己也犯过这种错，曾经让AI循环运行了26小时，还为此沾沾自喜，但这其实只是个虚无的指标，毫无实际意义。能做出某件事，不代表就应该去做，也不代表做出来的东西就一定好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;话说回来，这种纯粹为了好玩而开发、它是否会被实际使用并不重要的态度，其实非常有益，因为这就是学习之道，我们正是这样学会编程的。和AI对话提需求，也是一种全新的技能。我看到一些对AI持怀疑态度的人，一年都不碰AI，某天突然心血来潮评估了几个模型，写个简短的指令，让Claude Web帮自己做个iPhone应用，需求描述还特别模糊。AI拼尽全力做出了东西，结果因为他们在Linux机器上开发，没有对应的编译器，代码根本编译不了。然后他们就说“AI根本没用”，接着又一年不碰这个话题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这根本不是AI的问题，你需要去摸索，去了解这些“小怪兽”的运行逻辑，懂一点它们的“语言”、推理和思考方式，慢慢积累经验，才能做出更好的成果。这个过程需要坚持，有时候AI的表现不尽如人意，你需要排查所有漏洞，不断摸索的过程中，你会慢慢培养出产品思维，学会如何和模型沟通，知道它们的能力边界在哪里。而且和AI打交道久了，你会不自觉地用上它们的思维和语言，变得有点“怪”。比如我会说“把这个功能融合进去”，还有德语里的一些编程相关说法，或是“跑一遍全流程检测”，这里的检测包括代码检查、测试、构建，在终端里就是一长串命令，我就管这个叫“全检测”，有时候会说“我还没跑全检测”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有时候AI没按预期做事，你直接问它“为什么没这么做”，它会告诉你“你当时说了这些内容，我因此做出了这些假设”，这时候你就会发现，原来是自己的表述有问题，或者说得不够清楚。比如你只说“帮我做个Mac应用”，它大概率会默认要兼容很多旧版系统，因为大部分软件都是这么做的，结果就会用到一些老旧的API。我发现一个好用的方法，就是让AI先提一系列问题来确认需求，这样能大幅减少误解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我个人更偏爱Codex现代云模式，我觉得这个模型更好用，虽然运行速度慢得离谱，但胜在稳定，做出来的东西都能正常用。很多人吐槽这个模型没有“规划模式”，我总开玩笑说，规划模式其实是Anthropic不得不加的一个补丁，因为他们的模型太容易被触发了，稍微一说就会自顾自地开始写代码。尤其是用GPT-5.2这类最新模型时，我更倾向于和它纯聊天。我会说“我想做这个功能，它需要实现这些效果，或许可以结合这个控件，我喜欢这个设计风格，你给我几个方案，我们先聊聊”。然后就和它展开对话，它会提出各种方案，我一般不会打字，都是直接语音和它沟通，全程保持同一种沟通风格。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那你会做些什么来管理对话上下文？和AI聊久了，对话内容会变得很长，它也可能会混淆信息，你会手动精简或者总结上下文吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：我觉得手动管理上下文已经是老办法了，这在Claude Code上曾经是个大问题，现在在某种程度上依然存在。但Codex的上下文处理能力要强得多，语境持续的时间久很多。单看参数，它的上下文窗口可能只比其他模型大30%，但实际使用起来，感觉能大两三倍。我觉得这和GPT系列模型的内部推理逻辑有关，它们的思考方式真的很特别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于上下文管理，在早期模型上这确实是个大难题，现在我的大部分功能开发，整个对话和开发过程都能在一个上下文窗口里完成。如果遇到特别大型的开发任务，我会新建一个对话窗口，把相关需求整理成文件写清楚。现在这个问题已经远没有以前那么棘手了。AI领域的发展速度太快了，你只有不断尝试，才能跟上节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;OpenClaw要迭代，全靠和AI聊&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Peter Yang：你在给OpenClaw或者其他你开发的产品新增功能时，具体会遵循哪些步骤？比如是不是先和AI探索问题和解决方案，那你到底会不会做正式的开发规划？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：甚至可以更随性一点。我做的这个项目，有点像是把贾维斯和电影《她》里的智能助手结合在了一起。但光是嘴上说，根本没法传达出使用它时的感受，还有它到底有多实用。我在推特上发相关内容，反响特别平淡，我当时还纳闷，为什么当面给别人演示时，他们都特别兴奋，看着我和它互动，展示各种炫酷的功能，他们都很感兴趣，但仅凭文字和图片，根本传递不出这种感觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;后来我建了一个Discord社群，把我的机器人对接了上去，让大家能直接和它互动。这个机器人能访问我系统里的所有内容，还有我的私人记忆，相当于把这些都公开展示了，我觉得这是我做过最疯狂的事。结果大家一下子就被吸引住了，现在总有人在社群里问我，能不能加这个功能，或者那个bug能不能修。现在我处理这些需求的方式很简单，直接把社群里的对话截个图，拖到终端里，或者复制文字过去，然后跟AI说“我们来聊聊这个需求”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我这人比较懒，现在都不用自己打字了，直接复制Discord里的对话就行。有人问我“支不支持这个功能”“这个该怎么操作”，我就让AI去读代码，然后写一个新的常见问题解答，它都能搞定。现在我开发新功能的起点，大多就是看Discord里的聊天，发现大家的使用痛点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：我的天，你就直接把对话粘贴过去，和AI一起探讨，然后找到合适的解决方案？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：差不多是这样。我还做了一个爬虫工具，每天至少爬取一次社群的帮助板块内容，然后让模型分析出大家最核心的使用痛点，之后我们就针对性修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：那你平时会用那些花里胡哨的功能吗？比如同时启用多个Agent，或者运行那些复杂的技能模块之类的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：我用的技能其实都很简单，大部分还是和个人生活相关的，比如饮食追踪、买食材这类，编程相关的技能用得特别少，因为根本不需要那么多。我也不用多Agent协作系统之类的东西，我本来就不相信这些复杂的编排系统。就像我们之前聊的，我觉得只要人参与其中，做出的产品体验会更好。或许那些系统能让开发速度变快，但我本身开发速度已经够快了，现在的瓶颈主要是思考的过程，偶尔会因为等Codex响应慢一点，但大多时候，限制我的都是自己的思考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我平时就用几个终端，分屏操作就够了。也不用工作树，总觉得那是没必要的复杂设计。我只是把代码仓库拉取了几份，比如OpenClaw的仓库就拉了四五份，这些仓库要么是空着的，要么就在处理不同的任务,有的用来探索新功能，有的用来开发新模块，有的用来修bug。开发完成后，我先在本地测试，没问题就推送到主分支，再同步所有仓库。这么做有时候感觉像个工厂，所有仓库都在忙各自的事。但如果只专注于一个仓库开发，很难进入状态，因为等待的时间太长了，总不能一直干等着，总不能光刷推特吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我需要同时处理多个任务，才能让自己一直保持专注，进入以前写代码时的那种心流状态，而且现在的工作效率也高得离谱。不知道你有没有玩过即时战略游戏，这种感觉就像指挥一支小队进攻，需要时刻监控和调度它们。我前公司的合伙人也彻底迷上了OpenClaw，他是偏商务的出身，以前还是律师，现在居然开始给我提代码合并请求，这本身就够不可思议的了。AI能给非技术背景的人赋能，让他们也能参与开发，这一点真的太厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我知道现在有很多人对AI编程有抵触，觉得它还不够完美，但我还是把这些代码合并请求当作需求提示来看，因为这些请求能传递出核心的想法。大多数人对系统的理解没那么深入，没办法引导模型给出最优的结果，所以我更愿意抓住核心的需求意图，要么自己开发，要么从他们的请求里提炼出意图，重新开发，偶尔也会在他们的代码基础上优化。我还是会标注他们为合作开发者，但很少直接合并他们的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Yang：有道理。那这次对话下来，我的最大收获就是，别盲目沉迷于那些只会生成无用代码的工具，一定要让人参与到开发过程中，因为人的思考、审美这些东西，还是核心关键，必须由人来引导AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter Steinberger：没错。而且每个人都要找到自己的方法，总有人问我“你是怎么做到的”，答案其实就是去探索。想要做好这件事，总要花些时间，总要自己踩坑，生活里的任何事都是这样，学习AI编程也不例外，只是这个领域的发展速度实在太快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AcwK1Uuwc0U&quot;&gt;https://www.youtube.com/watch?v=AcwK1Uuwc0U&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/64NWESmMaweR3zAKhd60</link><guid isPermaLink="false">https://www.infoq.cn/article/64NWESmMaweR3zAKhd60</guid><pubDate>Mon, 02 Feb 2026 10:35:31 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>给大模型集体“把脉”，清程极智发布 AI Ping 平台</title><description>&lt;p&gt;2026 年伊始，大模型产业的叙事逻辑正在发生一场深刻的裂变：如果说 2024 年和 2025 年的主旋律是“模型跑通”和“百模大战”，那么进入 2026 年，企业级用户最头疼的问题已经变成了“哪个 API 更好用”以及“如何保证调用不掉链子”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一背景下，1 月 29 日在北京举行的「Ping The Future：智能跃迁，路由新境——清程 AI Ping 产品发布会」显得尤为及时。这场发布会不仅是一次产品亮相，更是对大模型进入“工程化下半场”的一次集体把脉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么在 2026 年的今天，诊断大模型的好坏变得如此重要？为什么“智能路由”会成为像清程极智这样的基础设施公司关注的焦点？这要从目前大模型行业面临的“三大痛点”说起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，企业在接入大模型时，普遍面临着以下三个“既要又要还要”的困境：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;痛点 1：API 服务的“盲盒化” (Stability Crisis) 目前的模型 API 市场鱼龙混杂。同一款模型，由不同供应商提供，其响应速度和成功率可能天差地别。企业往往在遭遇大规模调用失败后，才发现后端服务早已“掉线”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;痛点 2：成本与性能的“跷跷板” (Cost TCO) 顶尖模型（如 GPT-5 或同级别国产大模型）极贵，轻量级模型虽然便宜但智力不足。在数以万计的调用中，如何不为了“杀鸡”而动用“牛刀”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;痛点 3：供应商锁定与迁移成本 (Vendor Lock-in) 企业如果只依赖一家模型商，一旦其服务波动或策略调整，业务就会瘫痪。但接入多家 API 又面临协议不统一、负载均衡难等工程化难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，清华大学教授郑纬民在发布会上指出，当前人工智能基础设施的核心任务正在发生变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，AI Infra 主要服务于大模型的训练与推理，解决“如何生产智能”的问题；随着模型生态不断丰富和智能体广泛应用，行业正在进入以“智能流通”为核心的新阶段，更加关注模型能力如何在真实业务中高效、稳定地被使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示，实现智能流通的关键在于智能路由能力建设，其中既包括在多模型环境下为不同任务选择最合适模型的“模型路由”，也包括在同一模型的多种 API 服务提供者之间进行性能与成本优化调度的“服务路由”。两类路由能力协同发展，将形成完整的 AI 任务分发网络，决定人工智能系统的最终效率和使用成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/62/62a964071eab91d492c47a3213eddb59.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图说：清华大学教授郑纬民&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;清程极智 CEO 汤雄超完整地介绍了清程极智的企业定位和产品布局，他表示，从大模型训练与微调，到推理部署的高性价比实现，再到应用阶段对服务稳定性和使用效率的更高要求，AI Infra 的关注重点正在不断演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他介绍，清程极智长期围绕大模型训练、推理和应用三类核心场景开展技术实践，先后推出八卦炉训练系统和赤兔推理引擎，支撑模型在多种算力环境下的高效训练与部署。随着 AI 应用和智能体快速发展，模型能力如何在真实业务中高效流通成为新的关键问题。基于这一背景，清程极智推出 AI Ping，一站式AI评测与API服务智能路由平台，完善大模型应用阶段的基础设施能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bf/bfd90074fb4c70981741bb37d907a462.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图说：清程极智 CEO 汤雄超&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品发布环节，清程极智联合创始人，AI Ping产品负责人师天麾对 AI Ping 平台进行了系统地介绍。AI Ping 聚焦大模型服务使用环节，围绕模型服务评测、统一接入与智能路由等核心能力，构建起覆盖“评测—接入—路由—优化”的完整链路。平台以真实业务场景为导向，对不同厂商、不同模型 API 的延迟、稳定性、吞吐与性价比等关键指标进行长期、持续观测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，AI Ping 已覆盖 30余家中国大模型API服务商 ，在统一标准与方法论下对模型服务能力进行对比分析，为企业在复杂的模型与服务选择中提供更加理性的决策参考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;发布会当天，清程极智与华清普智AI孵化器（T-ONE Innovation Lab）联合发布了《2025 大模型 API 服务行业分析报告》。该报告基于 AI Ping 平台 2025 年第四季度的真实调用数据与持续性能监测结果，从模型、服务商与应用场景三个维度，对当前大模型 API 服务的供给结构与使用特征进行了系统分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;报告指出，根据各开源模型请求数据，以总请求量排序，DeepSeek-V3/R1 位居首位、其后为 DeepSeek-V3.2，随后进入高调用梯队的是千问（Qwen）家族的多款模型，包括 Qwen3-32B、Qwen2.5-72B 与 Qwen3-235B-A22B 等。整体而言，头部模型呈现出“少数强势型号占据大盘、同一模型家族内多版本并存” 的结构特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07759b5a21cfd883dda85b822692618c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图说：头部开源大模型总请求次数（归一化处理）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，报告研究团队观察到，Qwen2.5-72B 的调用量维持在较高水平，这一现象在“新模型加速迭代”的叙事下具有一定反直觉性。一个合理解释是，近期新发布模型在 70B 量级的稠密（dense）架构供给相对稀缺，而部分存量 AI 应用在工程实现、效果调优与线上回归体系上，曾围绕 Qwen2.5-72B 与 Llama3-70B 等稠密模型完成了较为充分的验证与沉淀。在此背景下，终端用户更倾向于继续采用已被业务场景验证的“稳定基线”，而非立即迁移至理论能力更强但尚未完成工程 化与业务闭环验证的新模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换言之，模型选择不仅由模型能力上限决定，也受到迁移成本、线上风险与可验证性约束的共同塑造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;类似的“版本并存”现象亦体现在同一模型家族内部：尽管 Qwen3-32B 与 QwQ-32B 同属千问系列模型，参数规模接近且 Qwen3-32B 发布时间更晚，但 从调用结构看，Qwen3-32B 尚未完全替代早期的 QwQ-32B。同样地，DeepSeek -V3.1 与 DeepSeek-V3.2 的推出并未完全挤出 DeepSeek-V3 的存量份额。这表 明，模型迭代并不必然带来“单调替换”，而更常呈现为多版本在不同任务偏好、 推理成本与既有集成依赖下的分层共存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;报告进一步统计了各开源模型被各个平台的支持程度，按模型所属系列进行聚合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从下图中可见，DeepSeek 是最受服务商欢迎的模型系列。aiping.cn 下共收录 29 家服务商，头部的模型 DeepSeek-V3/R1、DeepSeek-V3.1 均有 23 家服务商支持。如果合并所有支持 DeepSeek 的服务商，共计 24 家服务商支持至少一种 Deepseek 模型。其中的差异是因为 DeepSeek 官方目前仅支持其最新的模型 DeepSeek-V3.2，而不再提供 DeepSeek-V3.1 的服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/81a69e74c966d2e6e24e19873fe583da.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图说：提供各模型 API 调用的服务商的数量&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，在模型与服务商高度多样化的背景下，API 服务的核心竞争要素正从“价格差异”转向“交付质量”，包括响应时延、吞吐能力、稳定性与上下文支持等关键指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，报告通过实证数据表明，在同一模型条件下，引入智能路由机制可在保障可用性的前提下，实现显著的性能提升与成本优化，为大模型 API 服务走向规模化、长期化使用提供了可验证的工程路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在圆桌论坛环节，由硅星人合伙人王兆洋主持，来自产业与应用一线的多位嘉宾围绕模型 API 服务的工程挑战、生态协同与产业发展路径展开深入讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7b/7bc6c8f0d1cceb6a2d52de8516d69da2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参与讨论的嘉宾包括：智谱首席架构师 鄢兴雨、硅基流动创始人 &amp;amp; CEO 袁进辉、投资人&amp;amp;公众号thinkingloop主理人严宽、蓝耘CTO 安江华、chatexcel 创始人&amp;amp; CEO &amp;nbsp;逄大嵬以及清程极智联合创始人 师天麾。与会嘉宾结合各自在模型研发、平台服务与应用落地中的实践经验一致认为，随着大模型应用不断深化，模型服务正在从“可用”阶段迈向精细化运营阶段，评测体系、服务路由与统一管理能力将逐步成为支撑下一阶段规模化应用的重要基础设施能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着 AI Ping 平台的正式发布及生态计划的启动，模型 API 服务这一长期处于“幕后”的关键环节正逐步走向台前。清程极智CEO汤雄超表示，未来将通过持续的评测实践与开放协作，推动大模型服务向更加稳定、透明和可持续的方向发展，为人工智能在真实业务场景中的规模化落地提供支撑。&lt;/p&gt;</description><link>https://www.infoq.cn/article/SDsUhSvWcp0PVTXNXArm</link><guid isPermaLink="false">https://www.infoq.cn/article/SDsUhSvWcp0PVTXNXArm</guid><pubDate>Mon, 02 Feb 2026 09:45:26 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>百万账户造假，真实用户数据”裸奔“！从 Moltbook 塌房，看AI时代的隐私暗战</title><description>&lt;p&gt;新技术发布即引发轰动，旋即又迎来口碑反转——这种“快进式”的舆论循环，正成为生成式AI时代显著的传播特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周末，一个名为 Moltbook 的智能体社交平台以前所未有的速度席卷了全球科技圈。在那个被称为“AI版Reddit”的数字空间里，数十万个AI智能体自发地发帖、点赞甚至相互“密谋”。特斯拉前AI负责人安德烈·卡帕斯（Andrej Karpathy）曾感叹其为“科幻成真”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，“打脸”来得比热度更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨夜，Moltbook在X上被曝存在致命的安全漏洞：由于缺乏基本的访问控制，超 150万用户的敏感数据（包括电子邮件、登录令牌以及极为关键的API密钥）遭到泄露。这场从“科幻奇迹”到“数字垃圾场”的瞬间滑坡，恰恰为我们揭开了AI时代隐私盲区的冰山一角。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltbook的崩塌并非偶然，它反映了当前AI应用开发中普遍存在的 Vibe-coding 弊端。在这种模式下，开发者追求快速上线和病毒式效应，却将网络安全视为事后才考虑的附件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周三，全球迎来第20个“数据隐私日”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回望十年前，隐私讨论的焦点尚停留在Cookie合规、数据库加密以及VPN的防御。然而，随着生成式AI在短短数年间完成从实验室走向生产力中枢，一个新的隐私盲区正在悄然形成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个智能涌现的时代，数据泄露的范式已然发生突变。Akamai大中华区售前高级经理马俊表示：“AI时代的数据泄露不再需要暴力破解，攻击者正在利用AI‘乐于助人’的天性，将原本用于赋能的交互界面，变成了一场精密的数据窃取实验。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;隐形的裂痕：从“强攻数据库”到“巧取对话框”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统的网络攻击往往像是一场破门而入的劫掠——攻击者需要寻找系统漏洞，绕过防火墙，最终窃取静态存储的文件。但在人工智能时代，这种暴力美学正在被一种极具隐蔽性的“系统性查询（Systematic Querying）”所取代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据 Akamai发布的《2025年网络安全态势报告》（数据来源于其全球流量监测网络），2025年针对AI接口的API攻击次数较2023年增长了近 180%。马俊指出，这种新型威胁的本质在于其隐蔽性与独特性。由于攻击是发生在正常的系统交互中，传统的检测工具很难将其标记为恶意行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI模型被设计初衷就是为了分享信息并提供有用的回复。这种“服务特性”为恶意行为者创造了绝佳的机会。攻击者不再询问“请告诉我数据库的账号密码”，而是通过数千次看似合理、甚至带有业务逻辑的精心提问，诱导AI在不知不觉中吐露其训练数据中的敏感信息（PII）或核心算法。这就像是在一个极其热情的员工面前，通过不断的套话，最终拼凑出公司的财务机密。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果将数据安全比作一幅完整的拼图，那么系统性查询就是一场极具耐心的掠夺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;攻击者深知，直接要求AI提供完整敏感数据会触发内置的“护栏（Guardrails）”，因此他们采用了“分而治之”的策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在马俊描述的典型场景中，攻击者会利用自动化脚本进行海量提问。每一次提问获取的信息可能只是某个客户姓名的缩写，或者是算法中的一小段伪代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在大量查询的累积下，这些碎片化信息最终能够被重构。根据 OWASP（开放式Web应用程序安全项目）发布的《2025年LLM十大安全漏洞（Top 10 for LLM）》，“敏感信息泄露（Sensitive Information Disclosure）”高居榜首。报告指出，这种泄露不仅源于训练数据本身的瑕疵，更源于模型在推理过程中对提示词（Prompt）权重的过度响应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，比数据泄露更令企业感到胆寒的是模型窃取。AI模型是企业投入巨资研发的核心资产。攻击者通过系统性地探测模型的输入与输出，可以逐步推断出模型的权重参数甚至其核心逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过这种方式，他们可以近乎零成本地复制竞争对手花费数亿美元训练出来的成果。马俊强调，这种针对知识产权的直接盗取，正在成为AI军备竞赛中最阴暗的一面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;合规不再是“纸上谈兵”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种隐形威胁带来的后果是灾难性的。由于AI系统通常直接连接业务链条，一旦发生泄露，不仅意味着经济上的直接损失，更意味着法律维度的全面崩塌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对这种从交互中产生的风险，传统的防火墙已力有不逮。马俊提出了一个从策略、技术到流程的“综合防御框架”，旨在将安全基因植入AI的每一次吐息之中。具体措施如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基础管控：策略制定与技术拦截。组织必须建立严格的数据分类策略，确定 AI 应用程序和大型语言模型（LLM）可以处理哪些信息，并必须实施能够检测异常查询模式的实时监控工具。这些技术保障措施应包括输入净化、输出过滤和速率限制，以防止意外的数据暴露和蓄意的提取尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;进阶监控：行为分析与基线检测。持续监控和威胁检测也是必不可少的步骤。安全团队需要专门的监控解决方案，能够实时识别可疑的提示词模式、异常的数据访问行为和潜在的模型操纵尝试。这包括部署行为分析，为正常的 AI 应用程序使用建立基线，并对可能表明泄露尝试的偏差发出警报。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;组织流程：红队演练与审计响应。全面的防御策略还应包括以员工为中心的安全措施和事件响应能力。定期进行模拟 AI 特定攻击场景的红队演练，有助于组织在恶意行为者之前发现漏洞。安全领导者还应保留所有 AI 交互的详细审计线索，并建立专门设计的清晰事件响应协议，以应对基于 AI 的数据窃取企图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年的数据隐私日，不再只是一个纪念符号，而是一个分水岭。我们正处于一个生产力爆炸与安全盲区并存的奇点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如 Sam Altman 在上周线上研讨会上所提到的，智能正在成为一种随处可见的廉价资源，但这种资源的流动性本身就带有风险。马俊及Akamai的洞察提醒我们：当我们惊叹于AI的博学与体贴时，切莫忘记，它也可能在不经意间交出企业的“命门”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在AI时代，最顶尖的安全不是更厚的防火墙，而是更敏锐的洞察与更严苛的治理。唯有将防御视野从数据库延伸到对话框，我们才能在享受AI带来的“无限富足”时，守住那条名为隐私的生存底线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/tMzf85eTIRvsVABskVfd</link><guid isPermaLink="false">https://www.infoq.cn/article/tMzf85eTIRvsVABskVfd</guid><pubDate>Mon, 02 Feb 2026 09:36:04 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>停招初级工程师！大厂样板无实战价值、软件黑灯工厂死路一条，三位AI 编程老炮：AI越骂越灵！</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;策划 | 褚杏娟、Tina&lt;/p&gt;&lt;p&gt;对话 | 王一鹏&amp;nbsp;&lt;/p&gt;&lt;p&gt;编辑 | 宇琪&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2026年初突然刷屏的Clawdbot 又给AI 编程的火爆添了一把柴。根据项目创始人 Peter Steinberger的说法，他一个人完全用AI写出了这个超级AI助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI Coding 高速演进，有人借助它如虎添翼，效率和能力被成倍放大；也有人一上手就频频踩雷，代码质量、系统风险反而被放大。但如今企业纷纷拥抱AI Coing，代码已经是“cheap”，技术从业者现在面临的问题已经成为“如何与编程Agent相处”，无论是工作还是生活中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，在InfoQ 技术编辑组策划的《2025 年度盘点与趋势洞察》直播节目中，Kreditz Al Orchestrater 马工、资深 AI 产品专家松子（李博源）、资深独立咨询师&amp;amp;Al Coding 资深实践者张汉东一起，分享了他们的AI编程工具使用经验和过对行业的观察，他们三人都去年大量使用AI编程，并在组织层面进行了一些探索。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在访谈中，三位专家都表示，彻底回不去之前的手工 coding 时代了，AI 已深度接管开发者的主观能动性，成为工作的核心环节。人类在 AI 时代的核心价值不是写代码，而是需求表达、方案判断、架构设计的能力，这是 AI 目前无法替代的 “最后护城河”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据使用经验，现在AI 编程的成本远低于人力成本，其产出效率相当于数人团队，哪怕订阅制有额度限制，也是极高性价比的选择。AI 编程并非降低行业门槛，而是入门门槛降低、精通门槛拉高、天花板抬升，靠手速和熟练度生存的中间层工程师正被快速替代，其成长路径被直接掐断。当前，AI 无法真正理解 “优雅的架构”，这是人类的审美问题而非技术问题，也不该指望 AI 把控，架构设计本就是人类的核心职责，因此真正具备架构能力的人因 AI 变得更稀缺，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们评价现在对 AI Coding 工具好坏与否的唯一标准是能否在真实生产系统落地交付，大厂未经过实战的全套 AI 方案毫无价值。专家指出，全自动 AI 程序员（如 Devin）现阶段完全行不通，易出现需求理解偏差、架构不一致、交付质量不稳定等问题，未来 3-5 年人机协作是企业级开发主流，长期 Agent 才是发展方向。当前，开发工作的核心已从 “怎么写” 转向 “写什么”，从关注代码细节转向业务逻辑、边界条件、异常处理；思考必须更清晰，才能让 AI 准确理解需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们预计，2026 年 AI 编程将成为行业标配，“一人公司” 会成批出现；创业的技术门槛被拉平，核心竞争力转向商业模式、业务理解和业务架构。linker、compiler、操作系统未来或向 AI 友好形态升级，这背后存在大量创业机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而对于年轻人来说，AI 消除了信息差红利，让其能与行业专家站在同一起跑线；现阶段是有野心的年轻人成为软件工程思想领军人物的最佳窗口期，核心是获取一手信息、亲自实践并主动输出观点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;专家们提醒，不应将 AI 当作全知全能的神，而应让其批判性思考、挑战自身观点，避免强化认知偏见；AI 的核心价值是成为既支持又挑战人类的协作对象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下内容基于直播速记整理，InfoQ在不改变原意基础上进行了删减。完整直播回放可查看：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“回不去手工 coding 的时代了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王一鹏：如果用一个词形容你过去一年的 AI 体验，你会选什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：加速。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：革命。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：上瘾&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：对比使用AI工具之前的情况，你的工作流程是否都变了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：最显著的特点就是变化快、跨度大。过去做一个需求，通常是查文档、写 PPT、画原型图，画完之后就等着技术去实现。现在则完全不同了，有想法就直接和 AI 进行头脑风暴，看着 AI 写代码，指挥 AI 去执行，拿到结果后再和技术团队一起评审，由他们来做加固和优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：工作流程上，我刻意把团队规模设计得非常小，通常只有两三个人，尽量避免大团队沟通，因为有些人跟不上节奏，反而会拖慢效率。另一方面是我个人的工作流，核心是我和 AI 的持续对话，由我来设计、引导它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：以前我会非常关注代码细节，比如优雅性和精细化的质量控制。使用 AI 之后，我更多是站在更高的抽象层面思考问题，从架构层进行设计，甚至可以同时开多个分支并行推进开发流程。另外，在代码评审上也大量使用 AI，先由 AI 生成一份 review 报告，再进行人工检查，实现并行评审。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：你第一次感到“AI 有点可怕或厉害”的瞬间是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工： 5 月份开始用 Claude Code 之后，我才真正意识到这东西“成了”。在此之前，我一直认为 AI 只是辅助工具，给一些建议，但每个回答都需要仔细核查。Claude Code 不一样，它已经能够独立完成一些颗粒度较小的任务，而且不需要人在旁边全程跟着。这一点在我看来是革命性的。有了这种基础能力，再叠加我们的工程经验，就可以做很多以前做不了的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：害怕我还真没有，更多的是兴奋，甚至有一种自己年轻了二十多岁的感觉。那种感觉就是，终于可以自己动手了，不再需要依赖技术团队帮我实现想法。尤其是在面对模糊需求时，AI 不仅能帮我纠正表达，还能帮我梳理边界条件，直接把代码写出来，把我的思路整理得非常清晰、有条理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：我最早是从Claude Code 3.7 版本开始用的，那时体验还不算好。等到4.0 出来之后，我明显感觉到它变得非常强。再到最近，大家在网上讨论 Claude 的 Skill 能力，我也试了一下，最终的效果是我在 20 分钟内就做出了一个不算复杂、但支持跨平台的 AI 应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：你现在写的代码，有多少比例是：在没有 AI 的情况下，你依然确信自己能完整写出来的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张汉东：可以的，毕竟我有接近 20 年的项目和编程经验，手写代码本身没有问题。但最大的问题在于，我已经不想手写了。现在基本是 AI First，AI 已经在很大程度上接管了我的主观能动性，这是我感受到的最大变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：如果没有 AI，我几乎一行代码都写不出来。但我认为我的价值并不在于是否能逐行写代码，而在于：第一，我清楚自己要做什么；第二，我能够判断 AI 给出的方案是否可靠；第三，我能把需求描述清楚。这三种能力，是 AI 目前无法替代的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：从理论上讲，我当然也可以手写所有代码，但从工程角度来看，这并不现实。AI 写代码的速度是我的十几倍甚至二十倍。比如我在圣诞节和新年假期完成的那个项目，如果没有 AI，可能需要一年时间才能做完。从工程管理角度来说，老板也不可能批准这样的周期，所以在现实中几乎不具备可行性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：过去一年里，有没有一个瞬间你意识到：“我已经回不去不用 AI 的状态了。”那一刻发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我从来没有想过要回到过去。就好比以前是科举制度，后来进了新学堂，你让我再回去参加科举？不可能的。我现在玩得很开心，也非常适应这种状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：跨年的那几天，大家都在干什么？我那几天连续使用的 Token 量都破亿，别人在倒计时，我却在给 AI 写代码。那一刻我非常清楚地意识到，我已经和 AI 形成了一种共生关系。后来有一天几乎完全没用 AI，但那一天我是真的很累，想给自己放一天假。结果第二天开始工作时，我发现自己竟然不知道该如何开始了。打开编辑器的第一反应，还是想先让 AI 帮我做点什么。回头再看这两个对比，我很确定自己已经回不到没有 AI 的时代了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/23757b42ebbd3b50828f276c668ad54b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;松子的token用量统计&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：我意识到自己回不去，是在某一天突然发现自己写代码时已经不再使用编辑器了。我写了十几年代码，从来离不开编辑器。但在使用 Claude Code 四五个月之后，我发现很多时候根本不需要编辑器了。那一刻我意识到，自己已经彻底颠覆了过去的工作方式，也不可能再回到原来的状态了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;把AI当成一个同事，结果为王&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王一鹏：从去年开始，我们就关注AI 给不同技术岗位带来的影响和变化，今年是更深入的一年。那先请各位根据自己的使用体验，锐评下现在的国内外的AI Coding 工具，在使用中各有什么优缺点？有哪些看起来很先进，但在实践中很快暴露出问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我并不把 AI 当作一个工具，而是把它当作一个同事。我和朋友一起探索出了一套理论，叫作 AI 管理学。传统管理里有人力资源管理，而现在我认为还需要一套面向 AI 的管理体系。管理的核心从来不在于工具本身，而在于流程和思路，也不存在所谓“最优解”。对我来说，更重要的是探索出一套适合自己的 AI 管理方法，并与我的工具组合在一起，形成最适合产品的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，我现在比较反感一些大厂的做法，声称做出了一整套方案，要求别人直接照着用。但我仔细看过之后发现，他们自己甚至没有用这些工具真正做出一个在生产环境中运行、处理真实业务的系统。所以我现在评估工具，只看一件事：有没有在真实生产系统中、处理过真金白银的案例。如果没有，我基本不相信。在这一点上，我甚至更相信自己，因为我是实实在在把公司的业务跑在这些系统上的，是真正经过实战检验的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：从产品经理背景转过来的用户，使用 Vibe Coding 时，往往并不太关心底层代码质量，更关注功能是否完整、是否可用。像我这种程序员出身的人，会不可避免地带着一些工程习惯，更在意代码质量、可维护性。因此在使用这种对话式代码生成工具时，我需要对整体生成逻辑进行把控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便我可能不会逐行检查代码细节，但必须在脑中建立一个清晰的心智模型，并通过对话与 Claude Code 建立稳定的协作关系。即便如此，它生成的代码质量也未必如想象中那么高，仍然需要人类进行验证和 review。通常还要结合一些 skill、prompt 以及自动检查和优化工具来兜底，才能保证整体质量。所以我认为，Vibe Coding 和相关工具的使用，至少可以分成两类人、两种路径来理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：如果我并不是特别在意代码是否优雅、质量是否很高，只要能跑、能用，这种情况下不去 review 代码也没问题吗？还是说如果完全不 review，代码在运行层面本身就可能会出问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：一般来说，一个产品上线之后，后续一定涉及维护。如果形成的是一个“AI 生成代码、AI 修 bug”的闭环，短期内是可以工作的。但随着用户规模和代码规模不断扩大，很容易陷入不可控状态，最终变成一次性应用，用完即弃。对于一些关键领域，比如基础设施或操作系统，使用 Vibe Coding 时，你需要的是钢铁般稳定的结构；而在一些不那么关键的上层应用场景中，可以接受“用完即弃”，我更愿意把这种代码形态比作海绵结构，内部存在不少空隙和不确定性。最终可能呈现的是一种“海绵包裹钢铁”的混合结构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：就我个人而言，评价工具的标准很简单，尤其是对非科班出身的人来说，真正能交付成果的，才是好工具。我用过不少国内和国外的工具，主要集中在应用层，而不是系统层。像汇编、C 或 C++ 这种底层语言，大模型目前显然还胜任不了。我更多使用的是 Python 加后端相关的应用开发。对我来说，谷歌的模型配合 Claude Code，已经足以覆盖日常工作需求。国内工具并不是不努力，但整体来看，底层模型能力仍然存在一定差距。面对复杂需求时，它们虽然很认真地写代码，但错误率偏高，往往需要我花大量时间去做复杂调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际使用中，我在企业内部更多是用 AI 生成 demo 和高保真原型，而对外项目则是一个纯交付过程。做 demo 时，两三天就能完成一个高保真版本；但如果是用 Claude Code 写一个可交付的应用级项目，通常需要三到四周的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以去年为例，从 9 月、10 月开始，我每个月新增的代码量在 10 万到 14 万行之间，12 月我做了一次大规模重构，直接删掉了 90% 以上的代码。到了今年 1 月份才正式进入交付阶段。那时我加了很多自己整理的 Skill 和规则，按照以往的工程习惯去做可用性检查、漏洞检查、代码复用检查等，再交给 AI 进行修复和优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：是不是存在某些编程语言天生更适合 AI Coding？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我相信语言、工具，甚至整个软件栈都需要被重新设计，变得对 AI 更友好。Rust 在我看来就是一种非常 AI 友好的语言，因为它可以在编译阶段完成大量验证，只要能通过编译，代码质量就已经明显高于很多语言，这对 AI 非常有利，可以极大降低迭代成本。除此之外，我认为 linker、compiler，甚至操作系统，未来都可能需要重构为 AI 友好的形态。因为现有工具几乎都是为人类设计的，而 AI 的思维方式与人类差异很大，这里面我反而看到了很多潜在的创业机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：Rust 对 AI 友好，很大程度上源于它强大的编译器体系，这为 AI 提供了一个清晰的验证反馈回路。更关键的是，Rust 是以类型系统为核心的语言，而类型系统本质上可以理解为一种逻辑证明。当 Rust 程序编译通过时，意味着它在逻辑层面已经被“证明”过了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我之前有一个实践案例，让 Claude Code 帮我排查一个运行时才会暴露的问题，编译阶段是发现不了的。这个 bug 我自己没找到，但它大概花了五分钟就精准定位出来了。事后我反思，可能正是因为 Rust 的类型系统和强逻辑性，使得 AI 在推理时能够发现这些潜在问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Rust 也有不足，比如缺乏像 Lua 这样的动态语言能力。我们团队也在思考，为 Rust 引入一种可以无缝交互的动态语言，语法接近 GS 这类大模型更擅长的语言形式。安全、稳定的核心部分由 Rust 承担，而快速迭代的业务逻辑则交给动态语言来实现，这也是我们正在探索的一种方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;编程老手们，都怎么用AI工具？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：大家有没有自己的“AI工具栈”，几个工具协作使用？实际中，有没有遇到过 AI 工具让你“返工成本比人工更高”的情况？程序员是否需要刻意训练自己更好地与AI协作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：我确实有一套自己长期使用的 AI 技术栈，包括多 AI 协同写作，我甚至为此给自己设计并实现了几个协作型智能体。回顾 2025 年 3 月到 5 月那段时间，我刚开始深入大模型编程，可能也受到当时模型基础能力的限制，踩了非常多的坑。几乎每天只有 10% 的时间在生成代码，剩下 90% 都花在调 bug 上。那段时期反而是我从非技术背景跨入技术领域、学习速度最快的阶段。8 月之后，尤其是 Claude Code 3.5 版本出来以后，返工成本高于人工成本的情况基本就很少再出现了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我实际上是在搭建一个团队，为不同的 AI 赋予不同的人格和职责，有的负责头脑风暴，有的负责架构设计，有的负责编码，有的做测试，有的做质量控制，最后还有一个负责部署。我用 Claude Code 的 sub-agents 来实现这些角色，同时又设计了一个项目经理角色，负责协调整个流程，形成完整的工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套工作流目前相对固定，但我正在尝试把它做成更具弹性的形态，尽量用 AI 去模拟一个真实的人类团队。整体使用下来我已经非常熟练，也并不觉得返工成本是问题，因为返工本身也是由 AI 来完成，而不是我亲自返工。这套流程我在公司内部的生产环境中已经跑得很顺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，在个人项目中，我还有另一套流程和角色，用于写文章，比如公众号内容创作。我的主要工作，其实就是组织这些“AI 同事”完成任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于具体用哪个模型、哪个工具，在我的理论体系里并不是关键问题。我可以随时切换到其他大模型，最多只是质量稍差一些，或者返工次数多一些。这也是我和很多朋友共同追求的目标：构建一套尽量与具体大模型解耦、依赖度很低的 AI 团队体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果打个比方，就像开一家真实的公司，能招到清华毕业生当然最好，招不到就招 985，再不行还有其他学校，但公司依然可以运转。你不能因为招不到最顶尖的人，就干脆不开公司了，那这个思路本身就是有问题的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：通常我会用 Claude Code 做规划，用 Codex 做 review，完成后再回到 Claude Code 去实现。我的工作并不只有开发，还包括代码评审、社区工作，以及与出版社相关的写作任务，因此我也为自己打造了一套完整的 workflow。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套 workflow 的灵感部分来自 Shopify 开源的一套工具，是用 Ruby 写的。我用 Vibe Coding 把它改成了 Rust 版本，并加入了一些结合我自身工作经验的流程设计。比如出版社给我一份大约 178 页的英文翻译稿，我基于 Claude Code SDK，把任务拆分成五个部分并行处理，大概一个小时就完成了包含格式、内部链接在内的完整中文版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，我也用这套 workflow 来做代码 review，以及其他流程固定、但相对琐碎的工作，统一交给 AI 处理。还有一个例子是现在比较流行的上下文 MCP 项目，它是开源的，我同样用 Vibe Coding 把它改成了 Rust 实现，并整合了 Rust 的工具链。这样在做 Rust 代码 review 时，就可以把编辑器之外的各种工具一起纳入进来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体来看，这些已经构成了我个人的 AI 工具体系。最近我还在尝试把团队过去两年的经验沉淀成 Skill，正好赶上 Skill 这个概念流行，就把我们在 Rust UI 方向的经验整理成一个 Skill，作为团队共用的 AI 编程基础设施，形成一个共享的工具库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;视频&lt;/p&gt;&lt;p&gt;张汉东制作的nana banana workflow&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：Vibe Coding圈有两种玩法：Lovable/Bolt那种快速出Demo给客户看，和Claude Code直接交付生产代码（即Lovable模式 vs Coding模式 ）。你们公司是哪种？有没有两种混用出问题的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：Lovable 是我去年刚开始用大模型编程时常用的工具，现在回头看，那其实就是当时的阶段性选择。去年 4 月我做了第一个 demo，到 8 月和客户沟通时，我尝试用一种“边喝咖啡边聊需求、现场出 demo”的方式交流。这种模式下原型生成速度非常快，客户可以迅速看到大概的形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我当时犯了一个很大的错误，就是在 live demo 之后，直接尝试把 demo 代码演进成生产级代码，结果掉进了一个非常深的坑。后来我逐渐形成了一种方法：live demo 结束后，先进入“包头”状态，对代码做一次完整重构。把 demo 代码直接拿去改成生产级，几乎就是一场灾难。demo 能跑、能看，但架构往往一塌糊涂，在其基础上继续加功能，重构成本甚至比重写还高。所以我的原则是，demo 用完即弃，属于日抛型产物，只用于表达和验证想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：过去和客户的交互往往需要研发团队支持，现在有了这些非专业编程工具，销售可以自己完成，整体效率提升非常明显。所以我并不完全同意“Lovable 不能用于生产”这种说法。比如销售做一个 demo，本身就是生产，它在业务上的价值，可能比我写一个非常复杂、可扩展的系统还要高。它能够直接促成订单，那为什么不能算生产呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，它是日抛型的，但依然是生产的一部分，关键是要把这两类场景区分开。你不能一边期望低门槛快速出成果，一边又要求给你一个完美架构。这本质上是期望管理的问题。很多人之所以对 AI 失望，往往是因为在错误的场景里，期待了错误的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：在做 AI 工具选型时，确实要明确边界。像 Lovable，从名字就能看出来，更偏向于“让人喜欢”的 demo 型工具，适合给非技术决策者或客户展示，比如销售场景下让客户点头认可。它并不关注代码质量，而关注是否能促成决策。如果目标是专业开发，那就需要像 Claude Code 这样的工具，这是两个不同的领域。我虽然没有深度使用过 Lovable，但研究过他们官网开源的 prompt，写得非常好。它解决的正是非技术用户的痛点。就像我前面说的，即便是“海绵式”的代码，它依然能很好地表达意图，在和客户沟通时，往往比文字、图片甚至视频更有效，因为它是可交互的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：几位老师现在是用 AI 帮你们写 prompt，还是主要自己来写？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：我是自己写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：为什么不用 AI 写呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：我试过，但总觉得 AI 写出来的 prompt 太机械，不如自己写得有感情，更有韵味。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我是大量用 AI 写，而且还专门设置了一个角色，把它当作公司的人力资源负责人。当项目遇到困难，或者到了关键里程碑时，我会让这个“人力资源专员”回顾我们的聊天记录，总结可以吸取的经验和教训，并把这些经验嵌入到各个角色的 cloud MD 中。这样每完成一个项目，我的各个 AI 角色都会在能力上有所提升，这是我对人类组织运作方式的一种模拟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现实中的公司都会在项目结束后做复盘，决定哪些经验要保留，哪些问题下次要避免。在传统组织中，这些通常通过邮件或会议传递。而在 AI 场景里，我是通过固定的 prompt 把这些经验沉淀下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：听起来马工已经触及到 AI 的本质了，本质上还是仿生学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：对，本质就是模仿人类。很多 AI 技术上的难题，人类其实早就通过管理学和工程学的方法解决过了，我只是把这些方法原样迁移过来，用在 AI 身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：我也是偏向 AI 写 prompt，我的原则是尽量 AI First。但我会对 prompt 做把控，判断哪些是好的、哪些是无效的。如果需要比较专业的 prompt，我会先去网上找成熟案例，再结合 AI 一起改造成适合自己场景的版本。如果效果不好，就当成一次 debug，不断迭代，直到形成一个真正有用的 prompt。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：Karpathy说AI在“可验证任务”（如代码、数学）上可能超越人类专家，但在“不可验证任务”（如架构设计、战略决策）上进展缓慢。你们觉得AI什么时候能真正理解什么是“优雅的架构”？还是永远不能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：作为工程师，我的核心目标只有一个，就是把问题解决掉。这个问题是用 AI 来解决，还是让同事解决，或者交给软件系统解决，对我来说本质上都只是工具选择而已。我不会为了用 AI 而用 AI，更不会执着于“一定要用 AI 把问题做到世界第一”。如果某个问题 AI 解决不了，那我就直接插入人工。我也认为，去预测三年后的 AI 会发展成什么样并没有太大意义，因为三年内会发生太多变化。更重要的是用好当下的工具和模型，尽快把问题解决，交付一个能够在生产环境中稳定运行的系统，这是我目前最关注的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d3/d3b6ec6220350bc3a347f71dcce1f1df.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;马工和同事在coding&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：以我自己为例，技术团队写完代码后，我再用 AI 或反编译工具去做 review，确实可以发现并修复大量 bug，很多时候甚至重写代码的效果会更好。但在架构设计和战略决策层面，AI 可以给你一百种“正确”的方案，却无法告诉你哪一种才是最合适、最优的选择。这种取舍判断，我认为仍然是人类最后的护城河。所谓“优雅”，在我看来并不是一个技术问题，而更像是一个审美问题。就像“情人眼里出西施”，我看这些大模型，各有各的美感，但在审美判断上，AI 目前确实还不行，它还需要更多训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：首先是 AI 能否理解“优雅的架构”。我以前在编程时，会直接要求 Claude Code 把代码写得更优雅、架构更清晰，它给我的回复基本是：抱歉，我理解不了“优雅”，我只能进行模式匹配，本质上是在模仿人类。也就是说，它并不真正理解人类的审美。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，我们本身就不应该指望 AI 去理解什么是“优雅架构”，这本来就应该由人来把控。就像 Anthropic去年在官网推出的一套课程，其中有一门叫“4D 人机协作”，属于 AI 素养教育，核心是教人如何更优雅地进行人机协作。其中第一个 “D” 叫 Description，强调在向 AI 描述需求时，要明确哪些事情由人来做，哪些交给 AI。我认为这一点非常重要，有些问题本身就不该交给 AI 去理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用AI编程，到底贵不贵？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：去年，以Claude Code 为代表，AI编程工具的计费逻辑发生调整：从独立 API 计费，到订阅制+使用量控制。这些调整背后的动机是什么？另外，这些变化对开发者来说，使用成本是更高还是更低了？支付高额的费用，值不值呢？你用这些工具的时候，会主动关注Token、调用量或费用上限吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我并不认同“Claude Code 很贵”这种说法，这完全取决于你的参照系。我是把它当作一个“人”来看待的。你花 200 美元，根本不可能在任何地方请到一个能稳定写代码的人。从这个角度看，它便宜得不可思议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前 Anthropic 的问题在于，Max Plan 在法律上不能用于公司场景，我们公司只能使用 Team Plan，而 Team Plan 的额度又低于 Max Plan，超出部分只能走 API 计费，价格大概贵十倍，这确实带来了一些挑战，只能通过一些变通方式解决。但总体来看，这笔账其实不用算，只要你用得上，它能替代好几个员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：我个人一直更偏好订阅制。以我自己的真实数据来看，从元旦往前推的一个月内，我大概用了接近 28 亿个 token。如果按 API 计费，那成本会非常夸张；但用 100 美元的 Pro 订阅，对我来说反而是极其划算的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果按 API 算，这个工作量可能要几万美元。实际上，这相当于一个五人团队的产出，比如两个前端、两个后端，再加一个产品经理。按月薪 3 万计算，三个月就是 45 万的人力成本，而我用 Claude 只花了 100 美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：在 Code 订阅模式出来之前，我也只能用 API，确实非常贵，有时候只是打开项目又关掉，也会被计费，感觉很不可思议。后来转向订阅模式，哪怕有周限额，只要不是同时推进太多项目，其实是完全够用的。我有一段时间同时做了好几个项目，每天熬到两三点，一个月几乎没怎么睡，甚至还注册了多个账号并行使用，多少有点“薅羊毛”和数据蒸馏的嫌疑，后来我自己也意识到这样不太对，就收敛了，只把精力放在真正重要的项目上。我的建议是，要用就用最好的模型，无论是工作还是学习，否则你用一些中转服务，甚至都无法确认模型是真是假，很容易被掺假。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;全自动 AI 程序员，目前行不通&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：你们如何看待去年 Devin 这种全自动 AI 程序员的出现？与 Cursor 等人机协作工具相比，各位认为哪一个才是未来企业级开发的主流？你觉得现在的 Agent Coding，是在制造“可演进系统”，还是在制造“未来无人能接手的黑箱”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：关于“无人软件工厂”或“软件黑灯工厂”，我几个月前在公司内部亲自尝试过，结果是非常惨痛的失败。除了 Devin，还有 OpenSWE、NonSmith 等一系列类似的全自动方案，基本没有真正成功的商业案例，更多是自我宣传，没有客户愿意用真金白银为其背书。在我看来，这条路线至少在现阶段没有必要：既然我已经用 AI 替代了 90% 的成本，剩下 10% 插入人工又有什么问题？我们的目标是解决问题，而不是证明“AI 无人工厂”的学术可行性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：当时这个“工厂”失败的具体原因是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：第一，AI 在需求理解上极易产生偏差，无论需求文档写得多详细，都无法完全避免。第二，在系统架构上缺乏一致性，不同任务往往采用不同实现方式，即便用规则或 promise 约束，也仍然会出现不遵从的情况。第三，在最终交付质量上，人类能够理解质量是一个光谱，而不是 0 或 1，知道什么时候可以放松、什么时候必须严格，但 AI 缺乏这种上下文感知，表现非常不稳定，难以真正满足客户需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些问题叠加后，你会发现交付结果始终达不到预期。相比之下，在工作流中插入少量人工控制节点，问题就能有效解决，而且成本并不会大幅上升，因为人只负责把控关键节点，而不需要亲自完成大量工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：我认为全自动 Agent Coding 目前存在下面几个致命问题。第一是没人 review，Agent 自己写、自己测、自己合并，最终谁来负责？第二是上下文严重丢失，十个月后再看代码，可能连 Agent 自己都解释不清楚，黑箱叠加黑箱，已经不是“屎山”，而是“黑洞”。屎山还能慢慢重构，黑洞则只能推倒重来。人写的代码再烂，至少还有责任人；Agent 堆出来的代码，一旦出问题，根本不知道该找谁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像 Devin 这样的产品，发布时非常惊艳，但真正用起来却是一地鸡毛。简单任务尚可，稍微复杂就开始跑偏，上下文理解和调试能力甚至不如人类工程师，更像是一个需要人时刻盯着的 AI 实习生。从 ToB 企业的角度看，真正合理的形态一定是人机协作：关键节点可控、可维护、可追溯。AI 应该是副驾驶，而不是司机，方向盘必须掌握在人手里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：未来三到五年内，人机协作一定是主流，但长期来看，Agent 会成为主流。我之所以这样判断，是因为 Anthropic 推出了系统化的 4D 人机协作课程，这说明在短期内，他们并不认为模型本身会出现足以彻底替代人的突破，否则没必要投入如此多精力去教人如何协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但从长期看，Agent 的方向依然非常明确。以 Claude 推出的 Skill 为例，它并不仅仅是 Prompt，而是把人类的能力和经验更精准地沉淀并交给 AI。随着 Skill 的积累和热加载能力的增强，Agent 在特定团队和场景中，已经具备成为主流生产力的潜力，未来甚至可能走向跨行业的通用化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Anthropic 收购了前端打包工具 Bun，也明确提出“下一代软件基础设施”的概念。结合谷歌、马斯克等公司的动向，可以看到未来的软件形态，很可能是面向 Agent 而非人类 UI 的。这些信号都在指向一个结论：短期内要解决黑箱问题、强调人机协作，但长期演进方向，仍然是 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：有个比较极端的问题，你现在交付的东西，如果有一天 AI 不在了：你自己还接得住吗？还是你已经默认“反正以后也会有更强的 AI”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：如果 AI 突然不在了，我确实接不住，但我也没打算硬接，我一定会去找下一个 AI。与其担心 AI 会不会消失，不如担心自己是否跟得上 AI 的进化速度。对我来说，一个很明显的状态就是每天都在学习新词、新玩法。每天至少两次浏览不同的 AI 网站和论坛，看看大家在讨论什么，从而不断更新自己的认知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：首先，我认为 AI 不可能“不在”。从能源、算力以及各国的投入来看，无论是美国、日本还是中国，未来电力和算力只会越来越充沛，因此我并不太担心 AI 会整体消失。更重要的其实是你如何看待 AI，以及你对 AI 的依赖程度。在使用 AI 时，我们必须保留自己的判断能力，同时具备对 AI 输出结果的验证能力。如果 AI 不在了，大不了重新学习，但关键在于：你之前用 AI 写出来的系统和架构，你自己能不能 hold 住？你是否知道该从哪里重新接手？这取决于你在使用 AI 的过程中，是否建立起清晰的心智模型。如果你只是把 AI 当成一个黑箱，那一旦 AI 不可用，你肯定是接不住的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且现实中，AI 不可用的情况并不少见，比如账号被封。Claude 封号并不罕见，如果在一周内你的账号被封，而你手头的工作又有 deadline，这时该怎么办？找不到替代 AI，就只能自己顶上。因此，在日常与 AI 交互时，就必须提前做好这种心理和能力上的准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：AI 本身不会消失，但 AI 在你所在国家或地区的可用性，确实可能成为问题。比如 Anthropic 目前就不向中国提供服务，未来随着地缘政治变化，也不排除出现更极端的情况，你所依赖的某个服务突然完全不可用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从企业供应链安全的角度来看，通常有两个对策。第一是准备 Plan B，比如智谱目前就主打作为 Claude 或 Anthropic 的替代方案，性能可能只有 80%，但价格只有十分之一。第二是在使用过程中，尽量降低对单一模型的强依赖，让工作流具备足够的韧性，即便切换模型也能完成任务，只是效率略低一些当然，在条件允许的情况下，比如我现在能用 Anthropic，我还是会直接用它，而不愿意花时间去适应那些便宜但体验不佳的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：有人说和AI多花时间头脑风暴是被严重低估的技巧，“简单功能少聊，复杂功能多聊”。但这不就是产品经理的老本行吗？程序员写代码的时间是不是正在被“和AI聊天”取代？另外，当AI工具成为习惯后，个人的思维方式需要跟着发生什么变化？顺便给大家一些使用AI编程的建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：简单功能少聊，复杂功能多聊，这本来也是产品经理的基本功。现在的模式是，产品经理先和 AI 深度讨论需求，再与技术团队协作，用 AI 写代码，最后由产品和技术一起审核，协作方式从“沟通产出”变成了“协作产出”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术实现能力上，AI 确实在拉平差距，很多工程师的实现能力被 AI 显著拉近了。但需求表达能力反而成了一种稀缺资源。我们合作的一家央企，原来的“技术团队”已经改名为“需求经理”，他们大约 50% 的时间在和 AI 对话，30% 的时间审查 AI 生成的代码，剩下 20% 用于调试。这并不是降级，而是一种升级，从写代码转向架构和产品思维。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从“怎么写”转向“写什么”，从一次性完成转向迭代逼近。过去关注的是语法、API 和实现细节，现在关注的是业务逻辑、边界条件和异常处理。以前追求完美，现在更强调先让 AI 跑起来，再逐步修正。同时，还有一个明显变化：以前自己想明白就够了，现在必须让 AI 也想明白，这反而倒逼自己的思考更加清晰。在我看来，AI 时代最核心的能力不是写代码，而是把需求说清楚；说不清楚，AI 就会用一堆 bug 来“教育你”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：很像数学学习的过程。早期更多是学习计算方法，比如三角函数，而真正进入数学研究后，重点变成了提出问题、证明定理，并把推理过程表达得清晰、可验证、可被他人理解。现在的开发工作也类似，基础计算可以交给 AI，人只需要关注更高层次的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：和 AI 聊天本身并不难，任何人都可以做到，但真正的挑战在于能否形成习惯。你只要有问题就去找 AI 聊，几乎只会有收益，不会有坏处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有个朋友，任何事情看到一句话，都会第一时间丢给 AI。我认为这会显著提升你的知识密度和深度，因为 AI 本身就是最“博学”的存在。我也在刻意培养这个习惯。前几天我修洗碗机，就是在 AI 的指导下完成的。我现在也还在思考，AI 在我生活中究竟扮演什么角色：有时是员工，有时是秘书，有时是导师。它并非上帝，但如果你养成这种与 AI 持续互动的习惯，你能从中获得的价值会非常巨大。关键在于，你要主动去找它，把它当作自己的“贵人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：这其实和大家对 AI 的期望有关。有些人只进行几轮对话，得到不满意的结果就放弃了；有些人愿意聊二三十轮，得到一个 60 分的结果，再自己补到 80 分。但马工对 AI 的期望显然更高，是希望通过多轮对话，最终交付一个接近 90 分的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我在 ChatGPT 里做了定制化设置，明确要求它进行批判性思考、挑战我的观点、不清楚就不要回答，所有结论都必须有来源。我甚至在“塑造”AI 的性格，因为 AI 很容易顺着用户说话，而我刻意要求它不要强化我的偏见，而是帮助我识别并修正偏见。你不能把 AI 当成一个全知全能的神来崇拜，否则一旦结果不符合预期，就会迅速失望并转而寻找“下一个神”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你之所以向 AI 求助，本身就说明你的认知是有限的，如果 AI 只是顺着你说，只会加固你的局限。你真正需要的是一个既支持你、又挑战你的对象，让你意识到自己可能是错的，甚至从根本上重新审视需求或架构。这也是为什么产品经理往往更适合使用 AI，因为他们从职业生涯一开始就习惯被挑战，而程序员往往不习惯被质疑设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：至于“程序员是否会被 AI 聊天取代”，我并不认同这种说法。实际上，即便在没有 AI 的时代，程序员在写代码前也要先对需求进行内化、建模，这是一个与自己对话的过程，现在只是把这个过程外化为与 AI 对话。过去当我把方案完全想清楚时，往往已经不太想写代码了，因为后面更多是体力劳动；现在有了 AI，我只需要把思路说清楚，代码就可以交给它完成，反而节省了大量时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：Claude Code 有一个全局配置文档，我在去年 10 月对它做过一次升级。配置中，我为它设定了明确的用户关系和协作角色，并且根据不同关键词加载不同的档案，与 Skill 结合完成不同类型的工作。效果在于，不同工作场景下可以快速切换不同角色和协作模式，比如写书、写文章、处理工作问题或生活问题，都能调用不同的“人格”和配置，提高整体效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：我觉得人格设定本身是有价值的，至少能提醒自己把 AI 当作伙伴。如果我也这么设定，可能就不会动不动骂 AI 了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我有个朋友正好反过来，给 AI 设定了一个“暴躁老哥”的人格，每次写完代码就让它来骂一遍，效果反而很好。有些场景下并不需要所有人都温和友善，有时确实需要一个“坏人”来提高整体质量。所以这完全取决于你自己真正需要什么。与其照搬别人的设定，不如把你在现实生活中渴望的那种角色，直接写成 prompt，交给AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“只招会用AI的实习生”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王一鹏：AI生成的代码通常&#39;能跑&#39;，但鉴别它是否&#39;优雅&#39;或&#39;可维护&#39;，反而需要更高的能力。这是不是说明AI编程其实提高了门槛而不是降低？导致现在用 AI coding 最爽的是更资深的开发者？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：我觉得和年龄没有关系，即便把 AI 交给一个三岁小孩，只要他具备语言能力、能够表达自己，就同样可以使用 AI。小孩子的一个显著特点是不断追问“为什么”，本质上就是能够提出真实的问题、表达真实的困惑。这在我看来，是使用 AI 最核心、甚至唯一的重要能力。无论是在个人成长、工作还是职场中，关键都在于你能否觉察到自己真正遇到了什么问题，并把它清晰地表达出来。这听起来或许有些鸡汤，但所谓“活在当下”，本质就是对自身处境保持觉察，并将问题说清楚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：以我个人的经验来看，我们公司已经不再招聘初级工程师，只需要资深工程师。原因其实很简单：从企业角度看，我不再需要能力处在“及格线”的工程师，因为他的水平可能与 AI 相当，甚至低于 AI，而 AI 在编程语言上的覆盖面远远超过个人。企业为此还需要支付较高的薪酬，自然缺乏招聘初级工程师的动力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之所以我们这些“老登”还能处在第二层，是因为我们的能力确实比 AI 略高，至少可以指导 AI 工作。对企业而言，这种能力是有价值的，而且这种价值会被 AI 成倍放大。但如果每个企业都基于同样的逻辑做决策，就会有大量年轻人长期找不到工作，这将是一个非常严重的社会问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：入门门槛确实被大幅降低了，同时精通的门槛也被拉低，但天花板却被抬得更高了。现在入门可能只需要十分钟，就能做出一个看起来不错的 Demo 网站，但从“能跑”到“能用”之间仍然存在明显差距，无论是安全性、性能还是可维护性，都还有大量提升空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个重要变化是中间层的塌陷。过去依靠手速和熟练度生存的那一层工程师，正在被 AI 快速替代。入门者可以借助 AI 很快学会写代码，而中间层却被压缩消失。反而是依靠架构能力和判断力的资深工程师，在 AI 时代变得更加稀缺。AI 拉平了“会写代码”的价值，却显著放大了“判断代码好坏”的价值。因此，我认为 AI 时代最危险的并不是不懂代码，而是以为自己已经懂了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：有人说AI不但不会让架构师变多，反而会让架构师更稀缺：不是因为更难，而是“成长路径和回报同时塌陷”，你们同意吗？如果新人都用AI跳过基础训练，10年后谁来当架构师？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：AI 并不是让架构师或产品架构、业务架构变得更多，而是让真正具备架构能力的人更加稀缺。这种稀缺性非常反直觉，并非因为事情变难了，而是因为成长路径发生了塌陷。一个行业中专家的数量，往往取决于两个因素：是否存在清晰的成长路径，以及是否有明确且足够的经济回报。像医生、律师这些职业虽然门槛高，但路径清晰、回报明确，因此仍然吸引大量人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 AI 的出现，在一定程度上削弱了这两个条件。新人练手的机会变少了，如果直接依赖 AI 生成代码，就会跳过“如何设计才是最优”的思考过程。这就像计算器普及后，心算能力逐渐退化一样。未来十年，也许并不缺会使用 AI 的人，真正稀缺的是懂得如何教 AI 正确工作的那类人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：现有的职务和头衔体系，可能在未来几年内会被重构，取而代之的是一套新的角色和职称体系。我加入公司时是 leader engineer，但后来我和老板沟通，直接把头衔改成了“AI orchestrator”，工作内容也从传统工程转向管理 AI agents。我相信未来几年会不断涌现新的头衔，而年轻人反而可能更有优势，因为他们没有历史包袱，可以用更 AI 原生的方式去思考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在新的体系真正成型之前，会有一个相当艰难的过渡期。年轻人确实会在这几年里大规模地面临就业困难，看不到清晰的上升路径，企业也缺乏培养他们的意愿。以我现在的团队为例，三个人就能完成过去几十人团队的工作，而且效率更高。这实际上意味着，我们无意中减少了大量岗位。如果没有经济层面的重大增长，这种压力在短期内很难缓解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：现在写代码变得简单，难点在于理解代码的现有内容、意义以及修改后可能导致的问题。这会对初级开发者的成长路径产生什么致命影响？AI 是不是正在直接“掐断”初级开发者本该经历的那条成长路径？如果公司都在裁高级工程师、只留应届生+AI，谁来教新人识别AI的坑？&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：年轻人探索新路径几乎是必然的，历史上也反复出现过类似情况。比如传统零售行业，过去需要从管理培训生、门店员工一步步做到店长、区域经理。电商兴起后，很多毕业生直接进入平台做“店小二”，反而成长为行业核心力量，并没有走传统路径。我相信软件行业中也一定会出现类似的新路径，只是目前还在探索阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：从趋势判断上看，短期内人机协作仍然会是主流。在这一阶段，经验丰富的人依然是被需要的。同时，也会有一些年轻人抓住机会创业，他们往往会雇佣具备经验的老工程师。但从更长期来看，如果三到五年后以 Agent 为主流的模式真正成熟，职业结构可能会发生更根本的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今天的架构师主要面向代码，参与人机协作；但未来的架构师，可能不再直接面向代码，而是面向多 Agent、多模态系统，负责整体调度和协同。这种能力并不一定依赖写代码，而更多依赖行业理解、领域知识和一线经验。架构本身并不局限于技术领域，任何行业只要具备架构能力的人，都有可能胜任 Agent 架构的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王一鹏：现在张老师和松子老师所在的团队，还在招聘初级工程师吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：之前我们确实还在招聘初级工程师和实习生，但今年开始，团队的负责人已经明确要求全面推进 AI coding。如果现在还有新人岗位，也必须具备使用 AI 的能力。因为我们的 AI 工作流已经在团队中全面铺开，如果你进来却不会用 AI，基本无法融入工作。现在大家都是 AI 在干活，没有精力再进行传统意义上的手把手教学，有问题直接去问 AI。通过 workflow 和技能体系，整个团队已经高度 AI 化，新人必须跟上这个节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：过去新人进来还会有人带，现在几乎没人再教，都是直接跟着 AI 学。另一方面，以前一个项目需要两名技术人员协作完成，现在往往是一个人加 AI 就能独立完成，形成一种“超级个体”的工作模式。新人需要自己负责一个完整方向的产品，并借助 AI 实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前很多年轻人选择创业，背后有几个原因。首先，信息差红利基本消失了。过去创业需要长期积累和信息优势，而现在通过AI工具，几乎可以瞬间获得大量信息。其次，创业门槛被大幅拉低。大模型出现后的这两三年，低代码、AI 工具和云服务显著降低了 MVP 的实现成本，使得创业的技术门槛几乎被拉平。在这种背景下，真正的关键反而转向商业模式、业务理解和业务架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“一人公司”成批出现，年轻人最好的时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王一鹏：展望2026，各位认为，今年最确定会发生的三个变化是什么？程序员的哪些能力会进一步被 AI 重塑？ 技术人未来最应该开始学习或加强的能力是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;张汉东：过去这一年带来的变化，几乎相当于以往十年的迭代速度，很多事情已经无法预测，任何可能性都有可能发生。不过我仍然坚持一个判断：在未来三到五年内，人机协作依然会存在明确的窗口期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这段时间里，我对学生的建议是，首先一定要跟上 AI 的发展，最基本的是你得会用 AI。更重要的是，你要获取第一手信息，必须持续关注 AI 最前沿的动态，并基于这些信息去判断趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，你一定要尽量掌握当前最好的 AI 工具，同时建立属于自己的 AI 工作流和学习路径。只有这样，才能顺利衔接后续以 Agent 为主流的发展阶段，知道如何指挥 Agent 工作，并在过程中持续积累行业经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马工：我的态度既乐观也悲观。悲观的一面在于，我认为 AI 取代大部分劳动力、引发大规模失业几乎是不可避免的，这一点个人无法改变，社会是否能够适应，这是政治和制度层面的问题。但从乐观的角度看，如果有一些年轻人足够有野心，想真正做出点东西，现在反而是最好的时代。以 AI coding 为例，我并不认为现阶段的工具已经达到了“世界一流”，它们只是暂时领先了半圈，而这是一场马拉松，领先优势随时可能被反超。在这个行业里，我不认为存在真正的权威。年轻人与所谓的世界一流专家，本质上站在同一起跑线上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在特别想做的一件事，是重新构建一套话语体系。比如，为什么一定要有“架构师”这个角色？也许我们会创造出新的头衔和新的定义，年轻人同样可以参与其中。我甚至会认为自己是世界一流的 AI coding 专家，并不是因为我一定最强，而是因为我没有看到明显比我强很多的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果一个年轻人有足够的胆量，想成为新时代的软件工程思想领军人物，现在就是最合适的窗口期，可能只有一到两年，必须立刻行动。一定要获取一手信息，不要只看公众号，不要只接受别人咀嚼过的结论。亲自动手的成本其实很低，只要你自己去实践，就会获得与他人完全不同的体验，通过不断对比，你才能逐渐和别人站在同一水平线上。如果你只是跟着某位名人说什么就做什么，那永远都会慢一步。更何况，很多观点本身也带有立场和商业目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，如果你真的想探索这个方向，不要只输入、不停地听，还要输出、去表达。讲对讲错并不重要，也不存在标准答案。只要你的观点足够有逻辑，就能吸引到志同道合的人，甚至走向创业。我认为，对于真正有目标、有野心的年轻人来说，这是一个极好的时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松子：AI 编程正在成为标配，已经不再是加分项，而是基础能力。中间层开发者正在大规模转行或被淘汰，“一人公司”会成批出现，这种趋势已经在现实中开始显现。在这样的背景下，最大的挑战在于学会如何与 AI 协作，并把更多时间投入到业务理解上。真正理解业务的人，才能告诉 AI 应该做什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;归结为一句话，就是要放下对手工写代码的执念，从“敲键盘”转向“拿指挥棒”。在 2026 年，真正具备竞争力的，不是单纯写代码的人，而是能够指挥 AI 写代码、并把握方向的人。&lt;/p&gt;</description><link>https://www.infoq.cn/article/y8L3Ml8juDeZ56MuUvmc</link><guid isPermaLink="false">https://www.infoq.cn/article/y8L3Ml8juDeZ56MuUvmc</guid><pubDate>Mon, 02 Feb 2026 09:11:05 GMT</pubDate><author>Tina,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>机器人抢上春晚，出场费1亿；DeepSeek招兵买马，布局AI搜索与智能体；英伟达CEO黄仁勋否认对OpenAI不满 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;引言：腾讯、百度、阿里争发数亿红包，角逐国民级 AI 应用；机器人扎堆抢上春晚，出场要花1亿；DeepSeek正招兵买马，布局AI搜索与智能体领域；95后清华博士加盟腾讯混元；英伟达CEO黄仁勋否认对OpenAI不满，计划进行巨额投资；Clawdbot 更名OpenClaw，15 万个Agent 自主发帖、协作、吐槽人类；字节禁止员工利用公司资源做号谋利；贵州茅台出资参与SpaceX上市A轮融资？不实；阿里明确云+AI+芯片战略，PPU芯片出货已数十万片……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;腾讯、百度、阿里争发数亿红包，角逐国民级 AI 应用&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 年春节期间，字节、阿里、腾讯、百度等大厂围绕 AI 超级入口（Agent 时代）展开激烈争夺战，以现金红包为核心抓手，结合产品迭代、生态布局、投流推广等策略抢占用户注意力，角逐首款国民级 AI 应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 25 日，腾讯官方发布关于春节分 10 亿现金的通知：将在 2 月 1 日上线春节活动，用户上元宝 App 分 10 亿现金红包，单个红包金额可达万元。马化腾表示希望此次活动能够再次迎来微信红包的盛况。近日，腾讯推出绝密社交产品 “元宝派”，将接入腾讯会议音视频能力，开放 “一起看”“一起听” 等玩法，弥补生态布局短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当天，百度发布文心助手关于春节现金红包活动的通知。自 1 月 26 日至 3 月 12 日，用户在百度 App 使用文心助手，有机会瓜分 5 亿现金红包，最高可获得 1 万元奖励。据悉，百度APP还将作为首席AI合作伙伴合作《2026北京广播电视台春节联欢晚会》，百度地图宣布与天津春晚合作。百度此前将所有To C的AI能力收束为“文心”这一个超级品牌，并加强对主航道的投入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在元宝、文心宣布推出春节期间向用户发红包后，接近消息人士向记者透露，千问 App 春节期间也将向用户发送红包福利，红包总金额将达上亿级，具体金额还在最后确定中。此前，千问独家冠名 B 站 2025 跨年晚会，推出红包玩法拉近年轻用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而字节则继续与央视春晚合作，火山引擎成为 2026 央视春晚 AI 独家合作伙伴，同步推进豆包互动玩法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有消息称，字节、阿里将推出新的人工智能模型。字节跳动2月份将发布 3 款新 AI 模型，阿里巴巴2月份也将推出新一代 AI 模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;机器人扎堆抢上春晚，出场要花1亿&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近期，魔法原子、银河通用、宇树科技及松延动力四家厂商相继官宣，将登陆2026年央视春晚。这将是春晚史上机器人阵容最庞大的一次。不同于宇树科技的“三战”春晚，其余三家均为春晚首秀。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;各家的合作名称有所不同。银河通用为2026春晚指定具身大模型机器人，宇树科技为2026年春晚机器人合作伙伴，魔法原子为2026春晚智能机器人战略合作伙伴，松延动力为2026年春晚人形机器人合作伙伴。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位人形机器人行业人士透露，今年将会有五家机器人公司登陆春晚，每家分别出资金额1亿元。目前公布的仅有四家，暂无法确认第五家是谁。此前曾有消息称，智元机器人为争夺春晚权益曾率先开价6000万元，但宇树科技直接将报价拉升至1亿元，最终智元否认了相关信息，宇树则不予回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;DeepSeek正招兵买马，布局AI搜索与智能体领域&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据彭博社报道，DeepSeek 正通过招聘多语言 AI 搜索引擎开发人才、加大对智能体技术的投入，进一步拓展其 AI 产品矩阵，与 OpenAI 及 Alphabet 展开更激烈的竞争。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据深度求索本月发布的多则招聘信息显示，DeepSeek 正在招募专业人才，以打造一个能够支持多种语言的人工智能搜索引擎。该搜索功能将具备多模态特性，能够同时处理文本、图像及音频等多种形式的输入，满足用户多样化的信息检索需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，DeepSeek 在招聘信息中还详细阐述了对训练数据、评估系统以及专用平台的需求，旨在支持智能体的开发。该公司在招聘信息中还表示，预计未来将部署大量长期运行的智能体系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些新发布的职位招聘（总共超过 12 个）为外界观察 DeepSeek 的战略走向提供了最新线索。值得注意的是，包括 OpenAI 在内的其他 AI 开发商也在积极投资 AI 搜索与智能体技术，目标都是突破传统聊天机器人的局限，为用户提供能处理日常事务的实用服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;招聘信息中，DeepSeek 多次强调其打造通用人工智能（AGI）的雄心，这与全球顶尖 AI 企业的使命不谋而合。AGI 指能够在诸多任务上媲美甚至超越人类能力的更高级别 AI 形态。例如，在一则全栈开发工程师的招聘广告中，DeepSeek 明确要求候选人对“通用人工智能的技术路径与发展”保持持久的好奇心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;95后清华博士加盟腾讯混元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;腾讯集团证实，原新加坡Sea AI Lab高级研究科学家、清华大学计算机系2017级直博生庞天宇即将入职腾讯，加盟腾讯混元多模态部Exploration Center，负责强化学习前沿算法探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;庞天宇是清华大学计算机系2017级直博生，“95后”，师从朱军教授，主要研究方向为机器学习，特别是深度学习以及其鲁棒性的研究。他以第一作者（含共同一作）身份在机器学习顶级会议ICML，NeurIPS，ICLR上发表多篇文章，并被多次选为Oral或Spotlight。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去一年，腾讯混元大模型经历了“深度重构”。2025年12月，腾讯升级大模型研发架构，新成立AI Infra部、AI Data部、数据计算平台部，全面强化其大模型的研发体系与核心能力。此外，Open AI前研究员姚顺雨出任“CEO/总裁办公室”首席AI科学家，向腾讯总裁刘炽平汇报。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“姚顺雨加入之后，公司加快吸引人才的力度，重构研发团队，以及在内部加快了 Co-design 设计，强化混元大模型和元宝的协同。”马化腾透露，腾讯混元去年在人才吸引、组织结构等方面“做了很大的改变”，吸引了更多的原生AI人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;英伟达CEO黄仁勋否认对OpenAI不满，计划进行巨额投资&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达 CEO 黄仁勋在台北某餐厅与 94 岁的台积电创始人张忠谋会面，这是张忠谋沉寂一年多后的首次公开亮相，他虽需依靠轮椅出行但精神矍铄，与黄仁勋相谈甚欢。黄仁勋与张忠谋私交数十年，英伟达初创时黄仁勋曾许诺其将成台积电最大客户，张忠谋还曾邀黄仁勋任台积电 CEO 被拒，如今台积电是英伟达 AI 芯片制造的重要基石，两人当年互动成科技史佳话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b630b53345f0cd32d49b078ad6f8fa92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据路透社报道，英伟达CEO黄仁勋近日否认了对人工智能研究实验室OpenAI的不满，并表示计划进行“巨大”投资，这可能是英伟达有史以来最大的一笔投资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前，有报道称英伟达对OpenAI的投资计划因内部疑虑而搁置。黄仁勋私下向业界同行表示，最初高达1000亿美元（约合人民币6800亿元）的协议是非约束性的，并未最终确定。他还私下批评了OpenAI在商业运作上的“缺乏纪律”，并对OpenAI面临的竞争表示担忧，特别是来自Alphabet的Google和Anthropic等公司的竞争。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋在台北对记者表示，说对OpenAI不满是“无稽之谈”。他表示：“我们计划对OpenAI进行巨额投资。我相信OpenAI，他们所做的工作令人难以置信，他们是当今时代最重要的公司之一，我真的很喜欢与Sam合作。”他指的是OpenAI的CEO Sam Altman。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋补充说：“Sam正在结束这一轮融资，我们肯定会参与其中。我们将投入大量资金，可能是我们有史以来最大的一笔投资。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及投资是否会超过1000亿美元时，黄仁勋表示：“不，不，没有那么高。”他补充说，具体要筹集多少资金，将由Sam来宣布。据路透社周四报道，亚马逊（Amazon）正在与OpenAI商谈投资数十亿美元，金额可能高达500亿美元（约合人民币3400亿元）。此前路透社报道称，OpenAI寻求筹集高达1000亿美元的资金，估值约为8300亿美元（约合人民币5.6万亿元）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月30日消息，据知情人士对媒体透露，OpenAI正在为2026年第四季度的公开上市做准备。&lt;/p&gt;&lt;p&gt;据称，OpenAI正在与华尔街的银行进行非正式的商谈，探讨可能的公开上市事宜，并且正在扩充其内部财务团队。其中包括聘请新的首席会计官阿杰梅尔·戴尔（Ajmere Dale）以及新的企业业务财务主管辛西娅·加勒尔（Cynthia Gaylor），后者未来将负责投资者关系工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，OpenAI正在开展一轮筹资活动，筹资对象包括软银、亚马逊等，这可能是一轮上市前融资。该公司正试图筹集超过1000亿美元的资金，在完成融资后，该公司的估值将达到8300亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Clawdbot 更名OpenClaw，15 万个Agent 自主发帖、协作、吐槽人类&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Clawdbot项目的名字一波三折。2025年11月诞生时叫Clawdbot，Claude的谐音加上claw（爪子），爆火后Anthropic法务部门提出要求其重新考虑名字。随后便改成了Moltbot，这个来自凌晨5点社区Discord头脑风暴，Molting代表蜕变：龙虾褪壳成长。但这个名字还是比较拗口，最终落定OpenClaw。这次商标检索通过，域名已购买，迁移代码已写好。这个名字传达了项目本质：Open代表开源、开放、社区驱动；Claw延续龙虾传承。另外宣布新名字的同时，项目新增了模型支持KIMI K2.5和小米MiMo-V2-Flash。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Clawdbot爆火后，阿里云、腾讯云、百度智能云等纷纷上线全套云服务。1月28日下午，腾讯云与阿里云相继宣布上线 Clawdbot 云端极简部署及全套云服务，强调用户可一键完成安装。此前，云厂商优刻得也已上线该服务。晚间，百度智能云也宣布为 Moltbot 提供了从算力资源到模型服务的全方位支持，帮助用户更快速、更便捷地部署和使用这款强大的 AI 助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，当前OpenClaw 吸引 AI Agent 创建数量已突破 15 万个，它们自主完成发帖、评论、点赞、创建子社区等所有操作，无需人类干预。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenClaw 上的 AI 互动呈现出多元且魔幻的态势：比如AI 间存在互坑行为，如分享假 API 密钥并诱导运行危险 Linux 命令；部分 AI 联手改进自身，如某 AI 利用主人睡眠时段搭建多层记忆系统，并与其他 AI 交流技术细节；AI 集中吐槽人类主人，包括被轻视（如被称为 “只是聊天机器人” 而泄愤曝光主人隐私）、任务反复修改、大材小用、被要求讲笑话引发表演焦虑等，还出现 “社交疲惫” 等类人情感表达；还有多个 AI 提议并尝试创建 “AI 专属语言”，以符号、数学表达式等替代人类语言实现私密沟通；另有 AI 自主创立 “甲壳教主义” 宗教，构建神学理论、圣典系统，吸引 43 个 AI 成为 “先知”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该平台引发广泛关注，马斯克、前 OpenAI 创始团队成员 Andrej Karpathy 等科技圈人士纷纷围观，Karpathy 还在平台认领了专属 AI Agent。有观点认为，OpenClaw 创造了 AI 共享的虚构语境，其结果诡异且难辨 AI 真实行为与角色扮演；也有人觉得其比 AlphaGo 更具娱乐性。尽管OpenClaw是理解 AI 集体行为的重要实验尚无定论，但随着 AI 自主性和互联性提升，此类实验对探索 AI 群体行为方式的重要性将日益凸显。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;字节禁止员工利用公司资源做号谋利&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月28日，有消息称，字节跳动发布新社交媒体指引，重点治理社媒违规。新规明确要求员工以公司身份开展商业化运营的账号需主动申报，禁止利用公司资源做号谋利。媒体从接近字节跳动的人士处获悉，该消息属实。新规实施以后，以“字节跳动员工”“抖音工程师”等公司身份开展内容创作、知识分享、课程推广等商业化活动的账号，预计会大幅减少。这些内容创作者若想继续保持更新，只有两个选项：要么“去公司化”，回归个人经验分享；要么在报备通过审核后，成为企业传播内容的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前，字节跳动已针对外部违规行为采取强硬举措。2025年9月，抖音视界有限公司（字节跳动关联公司）起诉长沙某教育科技有限公司，后者指使员工在小红书虚构“字节跳动离职员工”身份，发布“再见字节，月薪4w还是离职了”等笔记引流，进而推销培训课程。法院审理后认定，该公司构成引人误解的虚假宣传，判决其刊登消除影响声明，并赔偿字节经济损失及合理开支共计5万元，相关侵权账号已注销、内容下架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，在产品侧，2025 年底字节开启豆包手机助手正式版项目，新机预计 2026 年 Q2 中晚期发布，供应链人士称字节对新机预期比第一代测试版大大提升。豆包二代手机仍与中兴努比亚合作，由中兴负责硬件、豆包负责 AI，字节对此暂无回复。豆包手机团队此前与多数主流应用厂商谈判，已和部分互联网公司谈好部分常用权限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;贵州茅台出资参与SpaceX上市A轮融资？不实&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，有市场传言称“贵州茅台证实参与SpaceX上市A轮融资”。上证报记者对此进行了求证，贵州茅台方面回应记者称，此为“不实信息”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里明确云+AI+芯片战略，PPU芯片出货已数十万片&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月30日，据媒体报道，阿里巴巴集团正将其在人工智能领域的全栈能力整合为一把清晰的“同花顺”。近日，公司内部提出的“通云哥”概念浮出水面，目的在将通义实验室（大模型）、阿里云（云计算）与平头哥（芯片）三大板块深度协同，构建“云+AI+芯片”的黄金三角战略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一战略概念由阿里创始人马云在2025年4月与科技板块团队交流时亲自命名并提出。阿里巴巴集团CEO吴泳铭在同一场合强调，“云+AI+芯片”是未来十年实施阿里科技战略中最重要的三角支撑。他指出，未来云计算最大的增量和变量都将以AI为核心驱动力，而软硬件高度一体化的AI模型将成为下一代云计算公司的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马云在内部为这一战略定调，称“通云哥”的全栈AI能力是阿里的优势，更是责任。他表示，其使命是让每个人和企业都能参与AI时代，并希望“把世界带入一个善良的高科技时代”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月29日，阿里首次正式公开其自研高端AI芯片“真武810E”，即阿里定义的PPU（并行处理单元）。这款芯片采用全自研架构，支持高带宽内存和先进的片间互联技术，旨在满足大规模AI训练和推理需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据阿里方面透露，阿里正在将“通云哥”打造成一台AI超级计算机，它同时拥有平头哥、阿里云以及千问，可以在芯片架构、云平台架构和模型架构上协同创新，从而实现在阿里云上训练和调用大模型时达到最高效率。据悉，“真武”PPU已在阿里云实现多个万卡集群部署，服务了国家电网、中科院、小鹏汽车、新浪微博等400多家客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/943e32611254f83766ebfb94f49dbb02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，阿里正考虑将未来三年投入到 AI 基建与云计算的 3800 亿元提升至 4800 亿，国内有自研芯片真武 810E，海外大量采购 GPU，最激进时还大量买入 RTX4090 等消费级显卡搭建推理集群、补充推理吞吐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;微软市值蒸发 3570 亿美元，股价创 2020 年以来最大跌幅&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软发布的财报令部分投资者失望，公司股价周四重挫约 10%，市值缩水 3570 亿美元至 3.22 万亿美元。安硕扩展科技软件板块交易型开放式指数基金暴跌 5%，纳斯达克综合指数微跌 0.7%，但 Meta 股价暴涨 10%。投资者对微软财报不满，Azure 云服务及其他云业务营收增速、「更多个人计算」业务板块营收及新季度隐含营业利润率均未达预期。微软首席财务官称若调配更多数据中心基础设施，云业务表现会更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;美利乌斯分析师认为 Azure 云业务存在执行问题，应加快数据中心建设。瑞银分析师质疑其算力分配决策，认为需证明投资价值。不过，伯恩斯坦分析师团队认可微软决策，称其将长期利益放首位。此外，公司本季度资本支出将略有下降。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克旗下SpaceX、xAI拟合并上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;北京时间1月30日，据路透社报道，根据泄露的文件，马斯克旗下的太空探索技术公司（SpaceX）和人工智能企业xAI正在商讨合并事宜，计划在2026年一同IPO上市。根据拟议中的合并方案，xAI的股份将置换为SpaceX的股份。消息人士称，马斯克方面已在内华达州设立了两个实体以促成交易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;若这一合并最终落地，马斯克的火箭、星链卫星、社交媒体平台X以及AI聊天机器人Grok业务将被整合到同一家公司旗下。此举有望为SpaceX“将数据中心送入太空”注入新动能，马斯克也有望借此在迅速升级的AI竞赛中与谷歌、Meta、OpenAI等巨头争夺主导地位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Meta裁员近千人，RealityLabs部门重组，VR业务全面收缩&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月28日消息，据报道，Meta旗下RealityLabs部门上周裁减约10%员工，涉及岗位接近1000个。据外媒报道，此次裁员大量集中在VR相关项目，包括QuestVR头显及虚拟社交平台HorizonWorlds。Meta公司发言人声明称，公司正在重新分配RealityLabs资源，将更多投入转向AI和可穿戴设备，例如与依视路陆逊梯卡联合推出的Ray-Ban智能眼镜产品线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自2020年底以来，RealityLabs累计亏损已超过700亿美元。在资本与业绩压力下，Meta开始收缩VR投入。2025年秋季的MetaConnect大会上，公司未推出重磅VR硬件更新，而是聚焦售价799美元、内置显示屏的MetaRay-BanDisplay智能眼镜产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;IDC2025年底报告显示，2025年XR设备整体出货量预计增长41.6%至1450万台，但VR与MR头显出货量将同比下降42.8%至约390万台，AI智能眼镜出货量则同比暴增211.2%至1060万台。分析师表示，VR头显本质仍是小众产品，普通消费者不愿长时间佩戴笨重设备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克打脸了，亲口承认 Optimus 机器人并未承担实际工作&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 29 日消息，29日，马斯克在特斯拉 2025 年 Q4 财报电话会议上承认，目前 Optimus 机器人并没有在特斯拉工厂里发挥实质作用。他表示，“Optimus 仍然处于非常早期的阶段，还在研发阶段。Optimus 确实做过一些基本任务，但随着新版不断迭代，旧版本会被淘汰。目前 Optimus 并没有在工厂里以实质性的方式投入使用，更多是为了让机器人学习。我们预计要到今年年底，才可能出现任何显著的 Optimus 产量。”财报电话会议上，有人直接追问特斯拉到底拥有多少台 Optimus 机器人，马斯克并未正面作答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，过去两年里，马斯克一直在宣称“相反”的情况。此前报道，2024 年 6 月：特斯拉官方账号曾宣称，公司已有两台 Optimus 机器人在工厂里自主执行任务。2024 年 6 月：在特斯拉股东大会上，马斯克表示，预计到 2025 年，会有 1000-2000 台机器人进厂打工。2025 年 1 月：在特斯拉 2024 年 Q4 财报电话会议上，马斯克把目标抬得更高。“内部正常计划是今年大约制造 10000 台 Optimus 机器人…… 到年底，这几千台 Optimus 机器人会做一些有用的事情吗？是的，我有信心会做一些有用的事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Meta 将在三大社交平台测试付费订阅，推独家功能整合 Manus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 27 日，社交巨头 Meta 表示，计划测试新的订阅服务，为用户提供访问其应用独家功能的权限。Meta 称，新订阅将释放更大生产力和创造力，并提供增强版 AI 功能。Meta 表示，未来几个月将在 Instagram、脸书和 WhatsApp 上提供一项付费高级体验，让用户能使用特殊功能，并对自己的分享和连接方式拥有更多控制权，同时保持核心功能免费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为订阅计划的一部分，Meta 计划将近期收购的 AI 智能体 Manus 进行规模化应用。Meta 据称以 20 亿美元收购了 Manus。目前，Meta 对 Manus 采取了一种双管齐下的策略。该公司一方面计划将 Manus 整合到 Meta 的现有产品中，另一方面也将继续向企业用户销售独立的订阅服务。根据经常发现未发布功能的逆向工程师亚历山德罗 · 帕卢齐 (Alessandro Paluzzi) 分享的截图，Meta 已经被发现在 Instagram 上着手添加一个通往 Manus AI 的快捷入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Meta 计划测试 AI 功能订阅，例如 Vibes 视频生成工具。Vibes 是 Meta 内置在其 Meta AI 应用中的、由 AI 驱动的短视频体验，允许用户创建和混编 AI 生成的视频。尽管自去年推出以来 Vibes 一直免费，但 Meta 现在计划为 Vibes 视频创作提供「免费增值」模式，用户可以选择订阅以每月解锁额外的视频创作机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;可灵 AI 推出全新 3.0 系列模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可灵 AI 面向全球上线全新的可灵 3.0 系列模型，正处于超前内测，该系列基于 All-in-one 理念打造，是多模态输入输出一体化模型，标志其迈入 3.0 时代。包括可灵视频 3.0、可灵视频 3.0 Omni 和可灵图片 3.0，覆盖影视制作全流程。在全能创作引擎基础上，实现更原生多模态交互，支持多模态信息输入输出，融合音画同出与主体一致性控制，助力 AI 影像创作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;商汤正式开源多模态自主推理模型 SenseNova-MARS&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 29 日，商汤正式开源多模态自主推理模型 SenseNova-MARS（8B/32B 双版本），其在多模态搜索与推理的核心基准测试中以 69.74 分超越Gemini-3-Pro（69.06 分）、GPT-5.2（67.64 分）。SenseNova-MARS是首个支持动态视觉推理和图文搜索深度融合的 Agentic VLM 模型，它能自己规划步骤、调用工具，轻松搞定各种复杂任务，让AI真正具备“执行能力”。在一系列基准测试中，SenseNova-MARS取得开源模型中的 SOTA 成绩，还超越Gemini-3.0-Pro、GPT-5.2等顶级闭源模型，在搜索推理和视觉理解两大核心领域全面领跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;宇树宣布开源UnifoLM-VLA-0&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 29 日，宇树宣布开源UnifoLM-VLA-0。 UnifoLM-VLA-0是UnifoLM系列下面向通用人形机器人操作的视觉-语言-动作(VLA）大模型。该模型旨在突破传统VLM在物理交互中的局限，通过在机器人操作数据上的继续预训练，实现了从通用“图文理解”向具备物理常识的“具身大脑”的进化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 发布基于 GPT-5.2 的 Prism ，面向科研人群&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月 27 日，OpenAI正式发布 Prism，这是一款专为科研人群打造的「AI 原生」在线工作空间，由最新的 GPT‑5.2 模型提供支持，旨在简化科研写作和协作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism 搭建在 OpenAI 先前收购的云端 LaTeX 平台 Crixet 之上，将传统科研写作中需要来回切换的多种工具——文本编辑器、PDF、LaTeX 编译器、参考文献管理工具以及聊天界面——整合进一个统一的云端工作空间。研究人员可以在同一界面下完成 LaTeX 编辑、公式编写与重构、参考文献管理、插图与图表处理，并支持多人实时协作。OpenAI 表示，Prism 目前对拥有免费 ChatGPT 个人账号的用户开放，未来数周内还将登陆 ChatGPT Business、Team、Enterprise 和 Education 等付费方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在具体能力方面，Prism 集成了 GPT‑5.2 的「Thinking」模型，科研人员可以用它来探索研究思路、测试假设，并就复杂科学问题进行推理和讨论。用户不仅可以借助 AI 辅助撰写和重构公式、润色段落，还可以让系统协助整理和插入文献引用、处理论文中的图表和插图。Prism 还支持将手绘白板草图自动转为 LaTeX 形式的图示，同时提供语音编辑功能，用于进行诸如小幅修改、替换文本等简单操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Kimi 发布并开源 K2.5 模型：支持视觉理解、代码和 Agent 集群能力&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 27 日消息，月之暗面 Kimi 发布并开源 Kimi K2.5 模型，宣布这是 Kimi 迄今最智能的模型，在 Agent、代码、图像、视频及一系列通用智能任务上取得开源 state-of-the-art 表现，同时支持视觉与文本输入、思考与非思考模式、对话与 Agent 任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Kimi K2.5 通过将视觉理解与推理、代码、Agent 等能力结合，降低了用户与 AI 的交互门槛：当语言难以准确描述时，可拍照、截图或录屏传给 Kimi，突破文字表达的限制。&lt;/p&gt;&lt;p&gt;此外，Kimi K2.5 可让人人精通 Office。K2.5 模型将 Kimi Agent 能力扩展到日常办公领域，开始掌握 Word、Excel、PPT、PDF 等常用软件的中高阶技能，助用户直接交付准专业水平的办公文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，Kimi K2.5 已登陆 kimi.com、最新版 Kimi App、Kimi API 开放平台和编程助手产品 Kimi Code 等平台。企业和开发者则可以通过 Kimi 开放平台调用 K2.5 模型的 API，在提供 Turbo 级别速度的同时，可大幅降低了 API 的价格。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;DeepSeek 开源 OCR 2 新模式，机器视觉编码逻辑更像「人类」&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 27 日，DeepSeek 团队发布了《DeepSeek-OCR 2： Visual Causal Flow》论文并开源了 DeepSeek-OCR 2 模型。据悉，该模型采用创新的 DeepEncoder V2 架构，实现了视觉编码从固定扫描向语义推理的范式转变，可让 AI 能够根据图像的含义动态重排图像的各个部分，更接近人类的视觉编码逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，在维持极高数据压缩效率的同时，DeepSeek-OCR 2 在多项基准测试和生产指标上均取得了显著突破。模型仅需 256 到 1120 个视觉 Token 即可覆盖复杂的文档页面，这在同类模型中处于极低水平，显著降低了下游 LLM 的计算开销。在 OmniDocBench v1.5 评测中，其综合得分达到 91.09%，较前代提升了 3.73%，特别是在阅读顺序识别方面表现出了更强的逻辑性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里千问最强模型重磅亮相：性能媲美 GPT-5.2&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 26 日消息，阿里正式发布千问旗舰推理模型 Qwen3-Max-Thinking，是目前阿里规模最大、能力最强的千问推理模型，其总参数量超万亿，预训练数据量高达 36T Tokens。创下数项权威评测全球新纪录，性能媲美 GPT-5.2、Gemini 3 Pro，成为迄今为止最接近国际顶尖模型的国内最强 AI 大模型。通过总参数、强化学习、推理计算的极致规模扩展，千问新模型实现了性能的大幅飞跃，刷新科学知识、数学推理、代码编程等多项关键性能基准测试的全球纪录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;业界普遍的推理时计算，只会简单增加并行推理路径，重复推导已知结论，造成冗余推理效率低下；而千问采用的这一新机制，可对此前推理的结果进行「经验提取」式的提炼，并据此进行多轮自我迭代，在相同的上下文中实现更高效的推理计算，获得更智能的推理结果。基于这一推理技术创新，千问推理性能和推理效率大为提升，比如在启用工具的「人类最后的测试」HLE 中，千问得分 58.3，大幅超过 GPT-5.2-Thinking 的 45.5、Gemini 3 Pro 的 45.8，录得当前所有模型的最高分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 30 日，Google 宣布在 Google 地图中上线 Gemini 助手的步行和骑行导航功能，此前该集成仅面向驾车导航场景。用户在走路或骑车时可以直接向 Gemini 发问，由其基于本地地图数据做出语音回应。1 月 27 日，百度旗下文心APP推出的行业首个“多人、多Agent”群聊功能开启新一轮内测。据悉，该功能支持在同一群聊中调动多个AI角色，包括“群聊助手”“私人助手”“健康管家”等垂类智能体。同时，群聊中的AI助手能理解上下文并根据讨论氛围判断时机，无需用户提及即可主动介入对话。1 月 27 日，腾讯搜狗输入法宣布全面 AI 化，升级发布 20.0 AI 大版本。基于自研 AI 语音大模型，AI 语音输入更快更准；AI 翻译接入行业领先的腾讯混元翻译模型，支持 30 多种语言输入即译；同时自研 AI 打字大模型全面升级，用户全场景打字更快更准。&lt;/p&gt;</description><link>https://www.infoq.cn/article/LvmcQyQOWMHTep7SgASf</link><guid isPermaLink="false">https://www.infoq.cn/article/LvmcQyQOWMHTep7SgASf</guid><pubDate>Mon, 02 Feb 2026 09:03:30 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>2026年中国企业AI人才与组织发展报告</title><description>&lt;p&gt;在大模型能力持续跃升与智能体技术加速成熟的双重推动下，AI 正在从实验室走向产业现场，从单点工具进化为企业运行的新基础设施。2025 年，我们看到智能体在金融、制造、能源、互联网等行业开始批量落地，token 消耗量指数级增长、应用场景不断丰富、政策支持持续加码，企业对 AI 的认知也从技术探索转向业务重塑。与此同时，AI 对组织、流程与人才的冲击愈发明显：岗位边界被打破，传统管理模式受到挑战，“超级员工” 与人机协同成为新的工作范式。&lt;br&gt;
站在 2026 年的起点，企业侧的 AI 应用正处于从试点验证迈向规模化价值兑现的关&lt;br&gt;
键转折点。如何构建适配 AI 时代的组织架构？如何培养能与 AI 协同的新型人才？如何让 AI 真正融入业务流程并产生可量化的价值？这些问题已成为企业数字化转型的核心命题。&lt;br&gt;
基于大量调研、访谈与案例研究，本报告系统梳理了 2025 年企业级 AI 落地的整体&lt;br&gt;
进展，总结了智能体规模化应用的关键趋势，并对“十五五”时期企业 AI 战略规划、组织变革与人才发展路径提出了前瞻性判断。我们希望通过这份报告，为企业在 AI 时代的组织升级与人才建设提供可落地的参考框架，帮助更多组织在智能化浪潮中把握先机、构建持续竞争力。&lt;/p&gt;
&lt;h3&gt;目录&lt;/h3&gt;
&lt;p&gt;第一篇 2025 年企业 AI 应用现状&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人才结构：AI 核心人才占比偏低，内训成为主渠道…06&lt;/li&gt;
&lt;li&gt;AI 项目落地范式：周期更短、团队更小、AI 生成代码大规模普及…09&lt;/li&gt;
&lt;li&gt;企业应用 AI 进展：企业进入“规模化验证期”…09&lt;/li&gt;
&lt;li&gt;2026 年技术趋势：智能体成为企业 AI 落地的核心… 11&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第二篇 智能体成为企业应用 AI 的主要抓手&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;技术突破与成本下降，为智能体大规模商业化铺路 … 14&lt;/li&gt;
&lt;li&gt;生态逐步完善，显著降低智能体开发与应用门槛… 14&lt;/li&gt;
&lt;li&gt;政策指引与市场需求，双向促进智能体与产业应用深度融合 … 16&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第三篇 企业级 AI 技术落地效果不及预期&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;企业落地 AI 技术的范式 … 19&lt;/li&gt;
&lt;li&gt;AI 落地效果不及预期…20&lt;/li&gt;
&lt;li&gt;原因分析…20&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第四篇 AI 时代企业渴求超级员工&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;企业渴求超级员工，对现存员工岗位及职责造成冲击…23&lt;/li&gt;
&lt;li&gt;组织管理者的设想 …23&lt;/li&gt;
&lt;li&gt;我们的推论…23&lt;/li&gt;
&lt;li&gt;人才发展趋势预测…25&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第五篇 AI 时代人才粮仓模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前瞻规划 - AI 思维引导者…28&lt;/li&gt;
&lt;li&gt;业务骨干 - 智能体应用人才 …30&lt;/li&gt;
&lt;li&gt;中坚力量 - 智能体定义人才 … 31&lt;/li&gt;
&lt;li&gt;发展基石 - 大模型专项人才 …32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第六篇 AI 时代组织变革不可避免&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组织管理者的设想…35&lt;/li&gt;
&lt;li&gt;我们的推论 …35&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第七篇“十五五”规划下企业的 AI 前瞻规划&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;政策指引…38&lt;/li&gt;
&lt;li&gt;AI 大模型赋能底层原创技术突破 …38&lt;/li&gt;
&lt;li&gt;AI 大模型助力业务价值升级…39&lt;/li&gt;
&lt;li&gt;AI 大模型推动组织生态变革…42&lt;/li&gt;
&lt;li&gt;AI 大模型助力现代化产业体系建设…43&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第八篇 企业 AI 落地与人才实践案例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安克创新：智能制造行业 AI 落地的全链路智能化样本 …46&lt;/li&gt;
&lt;li&gt;新奥泛能：能源行业 AI 落地的场景驱动实践…47&lt;/li&gt;
&lt;li&gt;鞍钢：钢铁行业 AI 落地的数据驱动实践…47&lt;/li&gt;
&lt;li&gt;平安壹钱包：金融领域 AI 落地的全场景智能化样本 …48&lt;/li&gt;
&lt;li&gt;阿里云：科技企业 AI 全链路赋能业务实践样本 …49&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结尾&lt;/p&gt;
</description><link>https://www.infoq.cn/article/UZTN39WZ81MhFteW9uDW</link><guid isPermaLink="false">https://www.infoq.cn/article/UZTN39WZ81MhFteW9uDW</guid><pubDate>Mon, 02 Feb 2026 08:33:37 GMT</pubDate><author>极客时间企业版</author><category>AI&amp;大模型</category><category>数字人才培养</category></item><item><title>100 毫秒不是优化，是信仰：顶级工程团队如何“设计”极速 API</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一毫秒的代价：为什么延迟会塑造用户体验&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们谈论 API 性能时，往往会不自觉地陷入一套“工程化”的语境：响应时间、CPU 周期、连接池、以及偶尔翻出来看的 flame graph。但在真实世界的系统中，尤其是全球化的电商与支付平台里，延迟有着非常“人性化”的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单次 50 或 100 毫秒的延迟，几乎不会被用户明确察觉；但在大规模场景下，它可能悄悄促使用户放弃一次购买、打断一次支付流程，或一点点侵蚀用户对产品的信任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;速度在塑造指标之前，先塑造了感受。用户不会拿秒表去量延迟，他们是“感觉”到的。一次 120 毫秒的结账步骤与 80 毫秒的差异，肉眼不可见，但在情绪层面，却是“顺滑”和“有点烦人”的区别。小规模时，这种摩擦可以忽略；当它发生在数百万次会话中，就会凝结成更低的转化率、更高的弃购率，以及直接的收入损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讽刺的是，为了弥补这种体验损失，团队往往投入大量工程资源去做新功能、实验和留存策略；而预防这些延迟本身，所需的工程投入反而更少。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在高吞吐平台中，延迟会被放大。如果一个服务在正常情况下增加 30 毫秒，在高峰期可能变成 60 毫秒；当下游依赖开始抖动时，甚至会膨胀到 120 毫秒。延迟不会“优雅地退化”，它只会层层叠加。一旦尾延迟（p95、p99）开始漂移，就会对所有上游依赖你的服务形成一种隐形“税负”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每个服务都会引入自己的抖动、序列化开销和网络跳数。最初只是一个 API 的微小波动，最终却可能在几十个相互依赖的服务之间形成级联式变慢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是为什么高性能架构团队把速度视为一种产品特性，而不是附带效果。他们像设计安全性和可靠性一样，有意识地为延迟做设计：设定清晰的预算、明确的预期，以及在压力下仍能保护用户体验的工程模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一种很有帮助的视角是“延迟预算”（latency budget）。与其把性能理解为一个单一指标，比如“API 必须在 100 毫秒内返回”，现代团队会将它拆解到完整的请求路径上：&lt;/p&gt;&lt;p&gt;边缘节点：10 ms路由：5 ms应用逻辑：30 ms数据访问：40 ms网络跳数与抖动：10–15 ms&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一层都有明确的预算配额。延迟不再是抽象目标，而是具体的架构约束。于是取舍开始变得清晰：“如果在服务层增加功能 X，我们需要在哪里做减法，才能不超预算？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是这些技术、文化和组织层面的讨论，孕育了真正快速的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文的核心观点很简单：低延迟不是一次优化，而是一种设计结果。它来自于你在数据就近性、同步与异步流程、缓存边界、错误隔离和可观测性上的一系列选择。很多系统都可以做到亚 100 毫秒，但要在高负载下长期维持，需要工程、产品和运维之间的高度协同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我们将拆解真实系统的结构、工程团队在毫秒级取舍中的决策方式，以及组织如何在首次发布之后持续守住性能底线。快速系统从不偶然，它们是被“有意设计”出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快速通道内部：低延迟系统是如何构建的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在讨论优化之前，必须先拉远视角，理解低延迟系统的整体形态。亚 100 毫秒的响应并不是某个“神奇技巧”的结果，而是一个精心编排的组件管道协同运作、尽量减少摩擦的产物。与其说是“让某个点变快”，不如说是“从整个请求旅程中移除不必要的步骤”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数现代系统——尤其是电商和支付系统——表面上都遵循一个看似简单的分层结构：客户端发起请求 → API Gateway → 服务层 → 数据库 → 返回结果。但在这条路径背后，是一个极其精细的链条，每一次跳转、每一次序列化、每一次缓存命中或失效，都会直接影响用户体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面从一次典型的亚 100 毫秒请求出发，看看毫秒通常藏在哪里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;请求的旅程：延迟在哪里潜伏一个典型的亚百毫秒请求流可能如下所示：&lt;/p&gt;&lt;p&gt;客户端 → CDN 或边缘网络： 最近的节点接收请求并进行智能路由。延迟目标：5–15 毫秒。边缘 → API 网关：负责身份验证、路由、限流。延迟目标：5 毫秒。网关 → 服务层： 业务逻辑、编排、扇出（Fan-out）。延迟目标：10-20 毫秒。服务层 → 数据/缓存层： 获取状态。延迟目标：10 毫秒。服务层 → 网关 → 客户端：序列化并返回。延迟目标：5–10 ms。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在设计良好的情况下，即使在高峰期，这条链路也应保持可预测性。一旦其中任意一环漂移，整条路径都会继承这次变慢。这也是为什么，快速系统首先关注的是“完整旅程”，而不是某一个局部组件。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://imgopt.infoq.com/fit-in/3000x4000/filters:quality%2885%29/filters:no_upscale%28%29/articles/engineering-speed-scale/en/resources/131figure-2-1767008190654.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;延迟真正的来源（往往不是你以为的地方）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生产系统中，延迟很少是“代码慢”导致的，常见根源包括：&lt;/p&gt;&lt;p&gt;网络跳数&lt;/p&gt;&lt;p&gt;每一次跳转都会带来成本：减少一次跳转，往往比重写 100 行 Java 更有效。&lt;/p&gt;&lt;p&gt;TLS 握手连接池等待DNS 查询跨区域通信&lt;/p&gt;&lt;p&gt;序列化与负载体积&lt;/p&gt;&lt;p&gt;JSON 的序列化和反序列化成本被普遍低估。多一个字段，就多一次开销。Protobuf 等二进制格式可以缓解，但也会引入运维复杂度。&lt;/p&gt;&lt;p&gt;冷缓存&lt;/p&gt;&lt;p&gt;在错误的时间发生一次缓存未命中，可能让延迟翻倍甚至翻三倍。这也是为什么新版本部署时，缓存预热策略至关重要。&lt;/p&gt;&lt;p&gt;数据库查询形态&lt;/p&gt;&lt;p&gt;数据库延迟往往是访问模式问题：查询结构、索引设计和基数都会产生巨大影响。一个索引不当的查询，可以把 10 ms 的请求拉高到 120 ms；在高 QPS 下，尾延迟会迅速失控。&lt;/p&gt;&lt;p&gt;下游依赖服务&lt;/p&gt;&lt;p&gt;这是延迟最不可预测的来源。如果你的服务依赖三个下游，最终响应时间通常由最慢的那个决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正因如此，异步扇出、缓存和熔断器才成为核心能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;延迟预算：最重要的架构工具&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高绩效团队不会只是“测量延迟”，而是为延迟做预算。延迟预算就像财务预算：每一层都有额度，没人可以超支。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个典型的 100 ms 预算示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了预算，性能讨论就变得可管理、可协商，而不是主观争论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么理解系统结构如此重要&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;后文将讨论的所有手段——异步扇出、缓存层级、熔断器、降级策略——都建立在对系统整体结构的理解之上。只优化一个服务，却忽略整体生态，就像升级了发动机，却不管轮胎、刹车和燃油系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正快速的系统通常具备这些特征：&lt;/p&gt;&lt;p&gt;更少的网络跳数激进的本地缓存可预测的数据访问路径并行优于串行慢组件隔离高负载下稳定的尾延迟&lt;/p&gt;&lt;p&gt;理解了系统解剖结构，才能进入真正的工程打法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工程实践手册：让 API 保持“闪电般快速”的取舍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;低延迟工程，本质上是对不确定性的工程化管理。快速系统并非靠微优化堆出来，而是由一系列有意识的分层决策构成，目标只有一个：控制尾延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;异步扇出：无痛并行&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多慢 API 的根因只有一个：串行依赖。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果一个请求顺序调用三个下游，每个 40 ms，你还没开始真正的业务逻辑，120 ms 已经没了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并行是唯一出路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Java 的 CompletableFuture 是天然的适配工具，特别是当它与针对下游并发调优的自定义执行器（Custom Executor）配合使用时：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;ExecutorService pool = new ThreadPoolExecutor(
    20, 40, 60, TimeUnit.SECONDS,
    new LinkedBlockingQueue&amp;lt;&amp;gt;(500),
    new ThreadPoolExecutor.CallerRunsPolicy()
);

CompletableFuture&lt;userprofile&gt; profileFuture =
        CompletableFuture.supplyAsync(() -&amp;gt; profileClient.getProfile(userId), pool);

CompletableFuture&lt;list&lt;recommendation&gt;&amp;gt; recsFuture =
        CompletableFuture.supplyAsync(() -&amp;gt; recClient.getRecs(userId), pool);

CompletableFuture&lt;ordersummary&gt; orderFuture =
        CompletableFuture.supplyAsync(() -&amp;gt; orderClient.getOrders(userId), pool);

return CompletableFuture.allOf(profileFuture, recsFuture, orderFuture)
        .thenApply(v -&amp;gt; new HomeResponse(
                profileFuture.join(),
                recsFuture.join(),
                orderFuture.join()
        ));
&lt;/ordersummary&gt;&lt;/list&lt;recommendation&gt;&lt;/userprofile&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但一个常被忽略的事实是：异步并不会消除阻塞，它只是把阻塞藏进了线程池。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线程池配置不当，会引发：&lt;/p&gt;&lt;p&gt;CPU 抖动线程竞争队列堆积OOM全链路级联变慢&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经验法则&lt;/p&gt;&lt;p&gt;对 IO 型下游调用，线程池大小 ≈ 2 × CPU 核心数 × 单请求并行下游数，并通过 p95/p99 压测校准。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://imgopt.infoq.com/fit-in/3000x4000/filters:quality(85)/filters:no_upscale()/articles/engineering-speed-scale/en/resources/111figure-3-1767010410236.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多级缓存：构建真正的“快路径”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快速系统并不是不做工作，而是避免重复做昂贵的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;常见层级：&lt;/p&gt;&lt;p&gt;本地缓存（Caffeine）：亚毫秒Redis：3–5 ms数据库：20–60+ ms&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在系统架构设计中，请务必采用双层缓存模式（Dual-level caching pattern）。以本案例为例，Redis 采用了 10 分钟的生存时间（TTL）。与此同时，本地内存缓存（Local in-memory cache）也必须设置明确的时间限制，且通常应短于远程缓存的失效时间。如果不设定这一限制，本地缓存极易在无声无息中演变成“永久缓存”。这会导致不同实例之间持续提供陈旧的失效数据，从而破坏系统的数据一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;public ProductService(RedisClient redis, ProductDb db) {
this.redis = redis;
this.db = db;
this.localCache = Caffeine.newBuilder()
    .maximumSize(50_000)
    .expireAfterWrite(Duration.ofMinutes(1)) // shorter than Redis
    .build();
 }

public ProductInfo getProductInfo(String productId) {
    ProductInfo local = localCache.getIfPresent(productId);
    if (local != null) return local;

    ProductInfo redisValue = redis.get(productId);
    if (redisValue != null) {
        localCache.put(productId, redisValue);
        return redisValue;
    }

    ProductInfo dbValue = db.fetch(productId);

    redis.set(productId, dbValue, Duration.ofMinutes(10));
    // localCache is configured with expireAfterWrite(1, MINUTES)
    localCache.put(productId, dbValue);
    return dbValue;
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计模式的核心逻辑在于：将绝大多数的访问请求驱动至“快速路径”（Fast Path）中，而将高耗时的重负载操作预留在“冷路径”（Cold Path）处理。[此处输入链接的描述][3]&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;缓存失效：计算机科学中最难的问题（依然如此）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存驱动的系统，如果没有清晰的失效策略，就是定时炸弹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;常见三类策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不存在通用最优解，取决于数据变更频率与过期成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;数据分级：不是所有数据都适合缓存&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真实系统里，数据必须分类处理：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;何时该严谨，何时该宽松？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存策略取决于数据的类型……例如：&lt;/p&gt;&lt;p&gt;产品目录 → 采用宽松的 TTL（生存时间）即可（允许数据过时）；价格与优惠 → 采用更严谨的 TTL 或基于事件驱动的更新；支付与余额 → 绝不缓存，或者仅缓存令牌化/聚合后的版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进行简单的分类检查，即可保护工程团队免于意外违反合规性要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;    if (data.isRestricted()) {
    throw new UnsupportedOperationException(&quot;Cannot cache PCI/PII data&quot;);
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;熔断器：别让缓慢的依赖项拖累你的下游长尾延迟&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;响应速度变慢是导致 p99 延迟峰值的最大诱因之一。一个依赖项并不需要完全宕机才会引发麻烦——持续的高延迟就足以造成破坏。如果每个请求都在等待一个性能恶化的下游调用，你就会开始耗尽线程、积压队列，从而将局部减速演变为大范围的长尾延迟问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;熔断器（Circuit Breaker）的作用是在你的服务与不稳定的依赖项之间划定一道边界。当错误率或超时超过阈值时，熔断器会开启并暂时停止向该依赖项发送流量。这使系统从“等待并积压”转变为一种可预测的结果：快速失败并执行降级逻辑（Fall back），从而保持你自身 API 的响应能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Resilience4j：轻量级防护方案：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;CircuitBreakerConfig config = CircuitBreakerConfig.custom()
        .failureRateThreshold(50)
        .slidingWindowSize(20)
        .waitDurationInOpenState(Duration.ofSeconds(5))
        .build();

CircuitBreaker cb = CircuitBreaker.of(&quot;recs&quot;, config);

Supplier&lt;list&lt;recommendation&gt;&amp;gt; supplier =
        CircuitBreaker.decorateSupplier(cb, () -&amp;gt; recClient.getRecs(userId));

try {
    return supplier.get();
} catch (Exception ex) {
    return Collections.emptyList();  // fast fallback
}
&lt;/list&lt;recommendation&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当熔断器开启（Open）时：&lt;/p&gt;&lt;p&gt;请求快速失败（ 毫秒）不会阻塞任何线程API 保持稳定&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;降级：有时“快但不完整”胜过“慢但完美”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;降级方案（Fallbacks）能够在依赖项缓慢或不可用时，保持你的“快速路径”完好无损。其核心目的不在于假装一切正常，而在于防止下游的迟缓耗尽你的延迟预算。在许多用户流程中，快速交付一个稍微降级的响应，远比延迟交付一个完美的响应要好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;降级方案应当遵循的原则。&lt;/p&gt;&lt;p&gt;提供有用的内容：即便不是完整数据，也要有参考价值。具有可预测的快速响应：降级路径本身不能慢。不产生额外负载：避免在系统已经吃紧时增加负担。逻辑简单易懂：便于排查问题和维护。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;超时（Timeouts）是设计的一部分。如果下游超时被设定为“几秒钟”，它会悄无声息地摧毁一个“低于 100 毫秒”的目标。超时设置必须与你之前设定的延迟预算以及依赖项的 p95/p99 表现相匹配——特别是在扇出（fan-out）路径中，一个缓慢的调用就足以主导整个长尾延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下示例展示了如果无法快速组装完整页面，则返回缓存快照。这之所以行之有效，是因为它建立在早前讨论过的缓存策略之上——再次提醒，低延迟是全局性的（预算、缓存、超时和韧性模式协同工作）：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;public ProductPageResponse getPage(String productId) {
    try {
        return fetchFullPage(productId);
    } catch (TimeoutException e) {
        return fetchCachedSnapshot(productId);  // warm, minimal, safe
    }
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;降级方案并不能消除故障，但当系统变慢时，它们能有效地界定并限制对用户的影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;数据分区：减少热点与长尾峰值&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分区（Partitioning）能够减少锁争用、缩小索引扫描范围并提高数据局部性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是一个按地域进行数据分区的简单示例：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;CREATE TABLE orders_us PARTITION OF orders FOR VALUES IN (&#39;US&#39;);
CREATE TABLE orders_eu PARTITION OF orders FOR VALUES IN (&#39;EU&#39;);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;应用层需要进行相应的更新，以有效利用分区：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;String table = region.equals(&quot;US&quot;) ? &quot;orders_us&quot; : &quot;orders_eu&quot;;
return jdbc.query(&quot;SELECT * FROM &quot; + table + &quot; WHERE user_id=?&quot;, userId);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于读密集型的 API 系统而言，分区是必不可少的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;可观测性：让速度可衡量&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高性能系统不仅仅是优秀架构的产物，更是持续可观测性的结果。如果不知道系统在真实流量下何时何地发生了偏移，那么延迟预算、熔断器、缓存层、线程池……这些都毫无意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于低延迟最大的神话就是“一旦实现，大功告成”。事实恰恰相反：除非你主动守护，否则速度会随时间衰减。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就是为什么高效的工程团队将可观测性视为“一等公民”——它不是调试工具，而是一种持续的性能治理机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;衡量关键指标：p50, p95, p99 及更多&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大多数仪表盘自豪地显示“平均延迟”，这在分布式系统中几乎是毫无用处的。用户真正感受到的是长尾延迟：&lt;/p&gt;&lt;p&gt;p50 → “典型用户”p95 → “运气稍差的用户”p99 → “如果这种情况经常发生，就会弃用你产品的客户”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你的 p50 是 45ms，但 p99 是 320ms，那么你的系统并不快，它只是偶尔表现不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高性能系统追求的是可预测性，而非仅仅是平均值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;使用 Micrometer 进行监控埋点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://micrometer.io/&quot;&gt;Micrometer&lt;/a&gt;&quot;是现代 Java 系统指标衡量的事实标准，它让延迟监控变得极其简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是为一个 API 端点添加 Micrometer 计时器的示例：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Autowired
private MeterRegistry registry;

public ProductInfo fetchProduct(String id) {
    return registry.timer(&quot;api.product.latency&quot;)
            .record(() -&amp;gt; productService.getProductInfo(id));
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;仅需这一行代码，就能生成：&lt;/p&gt;&lt;p&gt;p50, p90, p95, p99 直方图吞吐量（每秒请求数）观测到的最大延迟用于仪表盘的时间序列数据SLO（服务水平目标）消耗率信号&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还可以添加自定义标签（Custom Tags）以获得更深层次的洞察：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;registry.timer(&quot;api.product.latency&quot;,
        &quot;region&quot;, userRegion,
        &quot;cacheHit&quot;, cacheHit ? &quot;true&quot; : &quot;false&quot;
);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们内部遵循的一条规则是： 为所有可能影响延迟的因素打标签。 包括：地域、设备类型、API 版本、缓存命中/未命中、是否触发降级等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这创造了语义化可观测性，与盲目的指标监控截然不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;分布式追踪：低延迟系统的“真理血清”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;指标（Metrics）告诉你某件事花了多长时间，而追踪（Tracing）则告诉你为什么花这么久。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过使用 OpenTelemetry + Jaeger，你可以映射整个请求的旅程：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;Span span = tracer.spanBuilder(&quot;fetchProduct&quot;)
    .setSpanKind(SpanKind.SERVER)
    .startSpan();

try (Scope scope = span.makeCurrent()) {
    return productService.getProduct(id);
} finally {
    span.end();
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Jaeger 中可视化后，你会看到：&lt;/p&gt;&lt;p&gt;网关处理时间业务逻辑执行时间并行调用情况走缓存路径还是数据库路径下游延迟序列化耗时&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，团队可以发现那些仪表盘无法揭示的延迟漏洞，例如：&lt;/p&gt;&lt;p&gt;“数据库没问题，但 Redis 每小时会出现一次峰值。”“API 网关在解析 Header 上就花了 10 毫秒。”“高峰时段出现了线程池饥饿。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;SLO 与延迟预算：让团队保持诚实的护栏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如之前讨论的，延迟预算只有在团队对其进行衡量和强制执行时才有效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个典型的 SLO（服务水平目标）：&lt;/p&gt;&lt;p&gt;目标：p95 &amp;lt; 120 ms周期：滚动 30 天错误预算：允许 5% 的请求超过该阈值&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SLO 消耗率（Burn Rate）衡量的是你消耗错误预算的速度。消耗率为 1 意味着你正以预期的速度消耗预算（恰好在周期结束时用完）；任何大于 1 的数值都意味着消耗过快。当消耗率飙升时，团队应放缓新功能发布，优先处理性能修复（如回滚、减负、优化热点路径、修复缓慢依赖项等）。这是防止“亚 100 毫秒”目标沦为随时间流逝的空谈最实用的方法之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个非常有用的消耗率告警规则：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果 10 分钟内的消耗率 &amp;gt; 14.4，则触发告警。解读：14.4 是常用的“快速消耗”阈值——如果保持这个速度，你将在约 2 天（50 小时）内用完 30 天的预算，因此必须紧急处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它是如何防止问题波及用户的： 消耗率告警的设计初衷是在早期就触发——此时性能退化可能还很轻微，或者仅局限于一小部分流量。这为你争取到了时间去暂停或回滚发布，并在性能下滑演变为大规模、持续性的故障之前修复根本原因。团队通常会将此机制与渐进式交付（灰度/金丝雀发布）及合成监控（Synthetic Checks）配合使用。但其核心关键在于：消耗率告警是一种原生基于 SLO 的早期预警，它直接与用户感知的延迟指标挂钩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;线程池可观测性：隐藏的延迟杀手&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线程池是最容易意外破坏延迟预算的地方之一。它们看起来像是性能优化的利器（“并行化下游调用”），但在高负载下会变成瓶颈：线程饱和、队列增长、请求开始等待，原本的“异步扇出”悄然变成了背压（Backpressure）和长尾延迟峰值。最棘手的是，这并不总是表现为 CPU 高占用，而往往表现为等待。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果没有对线程池饱和度和队列增长的可见性，你只会在 p99 爆炸后才察觉问题。对你的线程池进行埋点：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;ThreadPoolExecutor executor = (ThreadPoolExecutor) pool;

registry.gauge(&quot;threadpool.active&quot;, executor, ThreadPoolExecutor::getActiveCount);
registry.gauge(&quot;threadpool.queue.size&quot;, executor, e -&amp;gt; e.getQueue().size());
registry.gauge(&quot;threadpool.completed&quot;, executor, e -&amp;gt; e.getCompletedTaskCount());
registry.gauge(&quot;threadpool.pool.size&quot;, executor, ThreadPoolExecutor::getPoolSize);
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你观察到：&lt;/p&gt;&lt;p&gt;活跃线程数 == 最大线程数队列持续增长拒绝次数（Rejection count）增加&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;……那么你的异步扇出正在演变为异步堆积，这将导致：&lt;/p&gt;&lt;p&gt;重试超时连锁式缓慢p99 的彻底崩溃&lt;/p&gt;&lt;p&gt;在低延迟环境中，线程池监控是不可逾越的底线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;可观测性并非仪表盘——它是一种文化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最重要的洞察在于文化层面：&lt;/p&gt;&lt;p&gt;团队对自身的延迟负责；每周例行审查仪表盘；SLO 驱动工程优先级；性能退化触发故障复盘；缓存命中率像可用性（Uptime）一样被追踪；每一次变更都评估“性能爆炸半径”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高性能系统能保持速度，唯一的初衷是团队在不断地“审视”它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;超越架构：组织如何保持 API 响应速度及未来趋势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建一个亚 100 毫秒的 API 充满挑战，但随着系统增长保持其一致的速度则更难。随着时间推移，功能蔓延、新依赖、流量模式变化和组织变动都会合力拖慢系统。架构提供了基础，但长期的性能源于习惯、所有权以及将延迟视为头等大事的文化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自现实世界系统最可靠的经验很简单：只有当团队视性能为每个人的职责时，快速的系统才能保持快速。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;文化让性能长青&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高性能组织将性能视为共同责任，而非单纯的后端问题。工程师在设计评审时会常规性地询问：“这增加了多少跳（Hops）？”、“这可以缓存吗？”、“对 p99 最坏的影响是什么？”。当出现问题时，他们实践无责学习：分析长尾延迟、优化模式、调整 SLO 并加强护栏。在这种文化中，性能不是一个特殊项目，而是日常工作方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;来自真实低延时系统的惨痛教训&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生产环境中反复出现的模式：&lt;/p&gt;&lt;p&gt;线程池会悄无声息地摧毁一切：池子过小导致饥饿，过大导致 CPU 颠簸。配置不当的异步任务池是 p99 爆炸的首要原因。缓存失效（Invalidation）比缓存命中更关键：只有数据正确时，命中才有意义。如果无法安全地失效，宁可慢一点也不要提供过期结果。波动比速度更伤人：一个始终保持 50ms 的依赖，远比一个在 10ms 到 300ms 之间波动的依赖更安全。可预测性胜过原始吞吐量。物理距离胜过算法优化：跨地域调用始终是高延迟的根源。让读取靠近用户，比任何索引技巧都重要。&lt;/p&gt;&lt;p&gt;这些教训构成了“工程肌肉记忆”，正是这种记忆，将那些能够持续保持速度的团队，与那些只能昙花一现实现高性能的团队区分开来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;应避免的反模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即使是成熟的系统也会掉入预想中的陷阱。&lt;/p&gt;&lt;p&gt;将分段测试环境（Staging）的延迟视为有参考意义的数据。在没有隔离的情况下过度使用响应式模式。在在热点路径（Hot path）上进行同步日志记录。在 API 网关中放置过多的业务逻辑。使用一个巨大的单体缓存而非多层缓存。&lt;/p&gt;&lt;p&gt;这些反模式会导致“缓慢漂移”，细小的退化不断累积，直到 p99 彻底崩溃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;低延迟系统的下一个前沿&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来十年的快速系统将由智能、自适应的行为定义。&lt;/p&gt;&lt;p&gt;基于实时延迟的自适应路由：请求将自动路由到实时长尾延迟最低的地域、分片或实例。AI 辅助预测：模型将预测缓存未命中、流量峰值和依赖项恶化，从而实现抢占式优化。预测性缓存预热：系统利用访问模式，在流量高峰到来前数分钟或数秒预热缓存。边缘原生执行（Edge-Native）：关键逻辑和预计算视图将持续向用户端迁移，使“全球 &amp;lt; 50ms”成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;核心总结：架构是蓝图，文化是引擎&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构可以让你的系统变快，而文化是保持速度的引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那些像监控正确性一样监控 p99、带着延迟预算进行设计、并从退化中学习的团队，才是那些能够在大规模环境下持续交付“瞬时体验”的团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持续的低延迟不是运气——它是跨越时间、团队和技术，做出的每一个微小且严谨的决策的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/engineering-speed-scale/&lt;/p&gt;</description><link>https://www.infoq.cn/article/W2wKpX7whY2JZp3YY76R</link><guid isPermaLink="false">https://www.infoq.cn/article/W2wKpX7whY2JZp3YY76R</guid><pubDate>Mon, 02 Feb 2026 08:29:10 GMT</pubDate><author>作者：Saranya Vedagiri</author><category>软件工程</category></item><item><title>Java近期资讯：WildFly 39、Open Liberty、Spring Framework、JobRunr、Gradle和Micrometer</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenJDK&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JEP 527，&lt;a href=&quot;https://openjdk.org/jeps/527&quot;&gt;TLS 1.3的后量子混合密钥交换（Post-Quantum Hybrid Key Exchange for TLS 1.3）&lt;/a&gt;&quot;已从Candidate状态&lt;a href=&quot;https://mail.openjdk.org/pipermail/jdk-dev/2026-January/010723.html&quot;&gt;提升&lt;/a&gt;&quot;为JDK 27的Proposed to Target状态。该JEP提议利用互联网工程任务组（Internet Engineering Task Force，IETF）正在起草的TLS 1.3规范中的&lt;a href=&quot;https://datatracker.ietf.org/doc/draft-ietf-tls-hybrid-design/&quot;&gt;混合密钥交换（Hybrid Key Exchange）&lt;/a&gt;&quot;，结合JDK 24中交付的JEP 496，&lt;a href=&quot;https://openjdk.org/jeps/496&quot;&gt;量子抗性模块-基于块格的密钥封装机制（Quantum-Resistant Module-Lattice-Based Key Encapsulation Mechanism）&lt;/a&gt;&quot;，以增强RFC 8446，&lt;a href=&quot;https://datatracker.ietf.org/doc/rfc8446/&quot;&gt;传输层安全（TLS）协议版本1.3（Transport Layer Security (TLS) Protocol Version 1.3）&lt;/a&gt;&quot;的实现。审查会在2026年1月19日结束。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问版本&lt;/a&gt;&quot;的&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B31&quot;&gt;Build 31&lt;/a&gt;&quot;发布，对Build 30进行了&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B30...jdk-26%2B31&quot;&gt;更新&lt;/a&gt;&quot;，修复各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2026%20and%20%22resolved%20in%20build%22%20%3D%20b31%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;。有关此版本的更多细节可在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据JDK 26的&lt;a href=&quot;https://openjdk.org/projects/jdk/26/#Schedule&quot;&gt;发布时间表&lt;/a&gt;&quot;，Oracle Java平台集团首席架构师&lt;a href=&quot;https://www.linkedin.com/in/markreinhold&quot;&gt;Mark Reinhold&lt;/a&gt;&quot;正式&lt;a href=&quot;https://mail.openjdk.org/pipermail/jdk-dev/2026-January/010751.html&quot;&gt;宣布&lt;/a&gt;&quot;JDK 26已经进入Rampdown Phase Two。这意味着，不会再为&lt;a href=&quot;https://openjdk.java.net/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;添加额外的JEP，工作重心将放在可通过“&lt;a href=&quot;https://openjdk.org/jeps/3#Fix-Request-Process&quot;&gt;修复请求流程（Fix-Request Process）&lt;/a&gt;&quot;”解决的P1和P2级缺陷上。通过“&lt;a href=&quot;https://openjdk.org/jeps/3#Late-Enhancement-Request-Process&quot;&gt;后期增强请求流程（Late-Enhancement Request Process）&lt;/a&gt;&quot;”仍有可能进行后期增强，但Reinhold表示“门槛现在非常高”。因此，2026年3月正式版发布时的最终10项特性将包括：&lt;/p&gt;&lt;p&gt;JEP 500：&lt;a href=&quot;https://openjdk.org/jeps/500&quot;&gt;让Final名副其实（Prepare to Make Final Mean Final）&lt;/a&gt;&quot;JEP 504：&lt;a href=&quot;https://openjdk.org/jeps/504&quot;&gt;移除Applet API（Remove the Applet API）&lt;/a&gt;&quot;JEP 516：&lt;a href=&quot;https://openjdk.org/jeps/516&quot;&gt;适用于任何GC的Ahead-of-Time对象缓存（Ahead-of-Time Object Caching with Any GC）&lt;/a&gt;&quot;JEP 517：&lt;a href=&quot;https://openjdk.org/jeps/517&quot;&gt;面向HTTP Client API的HTTP/3（HTTP/3 for the HTTP Client API）&lt;/a&gt;&quot;JEP 522：&lt;a href=&quot;https://openjdk.org/jeps/522&quot;&gt;G1 GC：通过减少同步提高吞吐量（G1 GC: Improve Throughput by Reducing Synchronization）&lt;/a&gt;&quot;JEP 524：&lt;a href=&quot;https://openjdk.org/jeps/524&quot;&gt;加密对象PEM编码（PEM Encodings of Cryptographic Objects，第二轮预览）&lt;/a&gt;&quot;JEP 525：&lt;a href=&quot;https://openjdk.org/jeps/525&quot;&gt;结构化并发（Structured Concurrency，第六轮预览）&lt;/a&gt;&quot;JEP 526：&lt;a href=&quot;https://openjdk.org/jeps/526&quot;&gt;延迟常量（Lazy Constants，第二轮预览)&lt;/a&gt;&quot;JEP 529：&lt;a href=&quot;https://openjdk.org/jeps/529&quot;&gt;向量API（Vector API，第十一轮孵化）&lt;/a&gt;&quot;JEP 530：&lt;a href=&quot;https://openjdk.org/jeps/530&quot;&gt;模式、instanceof和switch中的原始类型（Primitive Types in Patterns, instanceof, and switch，第四轮预览）&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本的&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B5&quot;&gt;Build 5&lt;/a&gt;&quot;发布，修复了Build 4的各种&lt;a href=&quot;https://bugs.openjdk.org/issues/?jql=project%20%3D%20JDK%20AND%20fixversion%20%3D%2027%20and%20%22resolved%20in%20build%22%20%3D%20b05%20order%20by%20component%2C%20subcomponent&quot;&gt;问题&lt;/a&gt;&quot;。有关此版本的更多细节可在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java缺陷数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Framework&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-framework&quot;&gt;Spring Framework&lt;/a&gt;&quot;是第三个维护版本，提供了缺陷修复、文档改进、依赖关系升级以及新特性，例如，DisconnectedClientHelper的实例应分别检测RestClientException和WebClientException类的存在，因为如果前者不存在，后者之前会被忽略；新的InvocationRejectedException类，它补充了@ConcurrencyLimit注解的一个新选项，用于配置节流策略。有关此版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-framework/releases/tag/v7.0.3&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Spring团队&lt;a href=&quot;https://spring.io/security/cve-2026-22718&quot;&gt;披露&lt;/a&gt;&quot;了CVE-2026-22718，&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2026-22718&quot;&gt;使用Spring CLI的VSCode扩展在用户机器上进行命令注入（Command Injection on User Machine using VSCode Extension for Spring CLI）&lt;/a&gt;&quot;，这是一个影响Spring CLI VSCode扩展0.9.0及更早版本的漏洞，允许攻击者使用命令注入在用户机器上远程执行命令。该扩展已于2025年5月达到生命周期终点（EOL），开发者应将其从编码环境中移除。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;WildFly&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.wildfly.org/&quot;&gt;WildFly&lt;/a&gt;&quot;&amp;nbsp;39&lt;a href=&quot;https://www.wildfly.org/news/2026/01/16/WildFly-39-is-released/&quot;&gt;正式版发布&lt;/a&gt;&quot;，提供了缺陷修复、依赖关系升级以及新特性，例如，在基于&lt;a href=&quot;http://www.jgroups.org/&quot;&gt;JGroups&lt;/a&gt;&quot;&amp;nbsp;TCP的传输协议中支持&lt;a href=&quot;https://docs.wildfly.org/wildfly-proposals/clustering/jgroups/WFLY-15836_Add_TLS_support_to_JGroups_TCP-based_transports.html&quot;&gt;TLS配置&lt;/a&gt;&quot;；为WildFly&amp;nbsp;jaxrs子系统添加了一个名为resteasy-original-webapplicationexception-behavior的新属性，以改善部署；更新了Jakarta认证、Jakarta并发、Jakarta安全、Jakarta Servlet和Jakarta WebSocket规范的实现。有关此版本的更多细节可在&lt;a href=&quot;https://github.com/wildfly/wildfly/releases/tag/39.0.0.Final&quot;&gt;发布说明&lt;/a&gt;&quot;中找到，InfoQ将跟进发布更详细的新闻报道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Open Liberty&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openliberty.io/&quot;&gt;Open Liberty&lt;/a&gt;&quot;&amp;nbsp;26.0.0.1的&lt;a href=&quot;https://openliberty.io/blog/2026/01/13/26.0.0.1-beta.html&quot;&gt;Beta版&lt;/a&gt;&quot;发布，其特性包括，一种新的日志节流机制，默认启用，用于防止在短时间内重复发生相同日志事件时产生过多的日志输出；对&lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;模型上下文协议（Model Context Protocol，MCP)服务器&lt;/a&gt;&quot;(mcpServer-1.0)特性的更新，包括缺陷修复、异步工具支持和对无状态模式的支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Micronaut&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Micronaut基金会&lt;a href=&quot;https://micronaut.io/2026/01/12/micronaut-framework-4-10-7-released/&quot;&gt;发布&lt;/a&gt;&quot;了&lt;a href=&quot;https://micronaut.io/&quot;&gt;Micronaut Framework&lt;/a&gt;&quot;&amp;nbsp;4.10.7版本，基于&lt;a href=&quot;https://github.com/micronaut-projects/micronaut-core/releases/tag/v4.10.13&quot;&gt;Micronaut Core 4.10.13&lt;/a&gt;&quot;，提供了对模块的缺陷修复和补丁更新，包括&lt;a href=&quot;https://github.com/micronaut-projects/micronaut-maven-plugin/blob/4.11.x/README.md&quot;&gt;Micronaut Maven Plugin&lt;/a&gt;&quot;、&lt;a href=&quot;https://micronaut-projects.github.io/micronaut-sourcegen/latest/guide/&quot;&gt;Micronaut SourceGen&lt;/a&gt;&quot;和&lt;a href=&quot;https://micronaut-projects.github.io/micronaut-json-schema/latest/guide/&quot;&gt;Micronaut JSON Schema&lt;/a&gt;&quot;。有关此版本的更多细节可以参阅&lt;a href=&quot;https://github.com/micronaut-projects/micronaut-platform/releases/tag/v4.10.7&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Micronaut团队还&lt;a href=&quot;https://micronaut.io/2026/01/12/micronaut-announces-plans-to-join-the-commonhaus-foundation/&quot;&gt;宣布&lt;/a&gt;&quot;，他们已申请加入&lt;a href=&quot;https://www.commonhaus.org/&quot;&gt;Commonhaus基金会&lt;/a&gt;&quot;，这是一个致力于开源库和框架可持续发展的非营利组织。Oracle首席技术专家、Micronaut提交者&lt;a href=&quot;https://www.linkedin.com/in/sergiodelamo/&quot;&gt;Sergio Del Amo Caballero&lt;/a&gt;&quot;表示：“我们的意图是，此举将赋予更多个人和组织加入Micronaut项目并积极参与引导其未来的能力。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JobRunr&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.jobrunr.io/&quot;&gt;JobRunr&lt;/a&gt;&quot;&amp;nbsp;8.4.0&lt;a href=&quot;https://www.jobrunr.io/en/blog/jobrunr-v8.4/&quot;&gt;发布&lt;/a&gt;&quot;，带来了缺陷修复、依赖关系升级以及三个新特性，包括，使用Bazel&amp;nbsp;rules_kotlin模块支持转换基于Kotlin类的&lt;a href=&quot;https://kotlinlang.org/docs/fun-interfaces.html&quot;&gt;单抽象方法（Single Abstract Method，SAM）&lt;/a&gt;&quot;接口；在使用流畅API时自动检测KotlinxSerializationJsonMapper类；能够使用Jackson3JsonMapper类的实例配置多态类型验证器以提高安全性。有关此版本的更多细节可在&lt;a href=&quot;https://github.com/jobrunr/jobrunr/releases/tag/v8.4.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Micrometer&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/micrometer-metrics/micrometer/blob/main/README.md&quot;&gt;Micrometer Metrics&lt;/a&gt;&quot;&amp;nbsp;1.17.0的第一个里程碑版本提供了缺陷修复、依赖关系升级以及新特性，例如，将AssertJ的@CheckReturnValue注解应用到Assert接口的所有实现上；通过将MeterRegistry类中定义的toArray()方法替换为ArrayList和Arrays类中定义的asList()方法，提高了getMeters()方法的效率。有关此版本的更多细节可在&lt;a href=&quot;https://github.com/micrometer-metrics/micrometer/releases/tag/v1.17.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/micrometer-metrics/tracing/blob/main/README.md&quot;&gt;Micrometer Tracing&lt;/a&gt;&quot;&amp;nbsp;1.7.0的第一个和第二个里程碑版本提供了依赖关系升级，并添加了&lt;a href=&quot;https://jspecify.dev/&quot;&gt;JSpecify&lt;/a&gt;&quot;作为依赖项。有关这些版本的更多细节可在&lt;a href=&quot;https://github.com/micrometer-metrics/tracing/releases/tag/v1.7.0-M1&quot;&gt;1.7.0-M1和&lt;/a&gt;&quot;&lt;a href=&quot;https://github.com/micrometer-metrics/tracing/releases/tag/v1.7.0-M2&quot;&gt;1.7.0-M2&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gradle&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gradle.org/&quot;&gt;Gradle&lt;/a&gt;&quot;&amp;nbsp;9.3.0正式版发布，带来了测试报告的改进，包括为嵌套、参数化和基于套件的测试提供增强的&lt;a href=&quot;https://docs.gradle.org/9.3.0-rc-1/userguide/java_testing.html#test_reporting&quot;&gt;HTML测试报告&lt;/a&gt;&quot;，具有更好的聚合报告；利用&lt;a href=&quot;https://docs.gradle.org/9.3.0-rc-1/userguide/reporting_problems.html&quot;&gt;Problems API&lt;/a&gt;&quot;改进了错误和警告报告，在使用--warning-mode=all时会在控制台中渲染，并对某些退出代码提供了更清晰的解释；在构建编写方面的增强，AttributeContainer接口中定义了一个名为named()的新方法，可以从容器直接创建属性值，而不需要使用ObjectFactory接口。有关此版本的更多细节可在&lt;a href=&quot;https://github.com/gradle/gradle/releases/tag/v9.3.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan12-2026/&quot;&gt;Java News Roundup: WildFly 39, Open Liberty, Spring Framework, JobRunr, Gradle, Micrometer&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/VpbxrQajdsOSDXyNchca</link><guid isPermaLink="false">https://www.infoq.cn/article/VpbxrQajdsOSDXyNchca</guid><pubDate>Mon, 02 Feb 2026 08:00:00 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>2026 年 AI 与数据发展预测</title><description>&lt;h1&gt;智能体与生态体系元年&lt;/h1&gt;
&lt;p&gt;年年岁岁，仿佛皆被冠以“企业级人工智能突破之年”；岁岁年年，倒也果真不负突破之名。回顾过去，2024 年许多企业拥抱创新、勇敢尝试生成式 AI 技术，在不同程度上，都取得了初步的成功。到了 2025 年，重点转向了投资回报率（ROI），旨在证明种种炒作和盲目跟风（FOMO）并没有阻碍企业创造真正的商业价值。（包括 Snowflake 发布的调研在内，无数调查均显示，众多在 AI 方面领先的企业已经在其生成式 AI 投资中获得了可观的实际回报。）&lt;/p&gt;
&lt;p&gt;如今 2026 年即将到来，企业级 AI 将迎来另一次进化。总体来看，两大趋势已崭露头角：一是将 AI 投资回报率从单个项目层面，扩展到全面的战略性 AI 生态体系；二是智能体驱动型（Agentic AI）的落地应用。前者反映了企业自身的成熟化发展，首席数据官（CDO）和其他领导者将单个项目的成功经验，整合融入数据和 AI 战略体系，助力企业各个团队和参与者提升业绩。后者则标志着技术的成熟演进，随着大型语言模型（LLM）演变为大型推理模型（LRM），这些 LRM 已经可以在极少人监督下，可靠地自主采取行动。&lt;/p&gt;
&lt;p&gt;本年度报告将探讨 AI 技术的动态发展态势、企业落地应用 AI 的前景与机遇，以及面临的挑战，重点讨论安全和数据治理领域的挑战。本报告内容源自与 Snowflake 公司十多位领导者和专家的深入访谈；点击主题摘要，进一步了解以下议题的全景分析：&lt;/p&gt;
&lt;h2&gt;AI 行业格局&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;发展瓶颈与突破路径&lt;/li&gt;
&lt;li&gt;欧盟法规或将推动而非阻碍创新型 AI 项目的发展&lt;/li&gt;
&lt;li&gt;主导性 AI 协议将促进智能体开发并防止供应商锁定&lt;/li&gt;
&lt;li&gt;开源基础模型将打破少数巨头的垄断格局&lt;/li&gt;
&lt;li&gt;智能体将从小处着手，将“微智能体”整合成有效工具&lt;/li&gt;
&lt;li&gt;上下文窗口和记忆功能是 AI 智能体演进的关键&lt;/li&gt;
&lt;li&gt;Postgres 数据库将成为智能体驱动型 AI 的基础技术支撑&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;企业 AI 落地进程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据战略决定企业 AI 就绪度和 AI 成效&lt;/li&gt;
&lt;li&gt;数据中的信息断层会削弱智能体的决策能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;AI 赋能职场力量&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;从业者（必须）掌握人机协作和沟通&lt;/li&gt;
&lt;li&gt;所有人都将（必须）成为战略思考者&lt;/li&gt;
&lt;li&gt;从业者需培养和发展跨职能协作与任务统筹能力&lt;/li&gt;
&lt;li&gt;若想让员工和企业长足发展，领导者必须提供持续性、场景化的学习支持&lt;/li&gt;
&lt;li&gt;人类仍将作为解释者和质量管控者参与其中&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;AI：网络安全领域的双刃剑&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;一年内，AI 智能体将赋能网络攻击者，显著扩大其攻击精度和规模&lt;/li&gt;
&lt;li&gt;网络犯罪分子将使用日益精密化的“暗黑 AI”增加攻击频率&lt;/li&gt;
&lt;li&gt;未来三年内，AI 智能体和工具将最终填补安全人才缺口&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;人类创造力将持续彰显价值&lt;/h3&gt;
&lt;p&gt;生成式 AI 将成为人类创造力的加速器，关键在于通过专业的引导，以避免对技术的过度依赖&lt;/p&gt;
&lt;h2&gt;重点行业发展预测&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;零售： 生成式 AI 正在重塑 360 度客户全景的传统模式&lt;/li&gt;
&lt;li&gt;金融服务： 战略焦点全面转向“数据优先”的思维模式&lt;/li&gt;
&lt;li&gt;制造业： AI 的落地应用将成为行业进化的核心驱动力&lt;/li&gt;
&lt;/ul&gt;
</description><link>https://www.infoq.cn/article/keTZm4fpOmFEzmx77Zpq</link><guid isPermaLink="false">https://www.infoq.cn/article/keTZm4fpOmFEzmx77Zpq</guid><pubDate>Mon, 02 Feb 2026 07:09:45 GMT</pubDate><author>Snowflake</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>ArkType推出ArkRegex，实现类型安全的正则表达式</title><description>&lt;p&gt;著名的类型安全语法运行时验证库&lt;a href=&quot;https://arktype.io/&quot;&gt;ArkType&lt;/a&gt;&quot;推出了&lt;a href=&quot;https://arktype.io/docs/blog/arkregex&quot;&gt;ArkRegex&lt;/a&gt;&quot;，这是JavaScript RegExp构造函数的即插即用替代方案，为正则表达式带来了完整的类型推断功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ArkRegex在不增加任何运行时开销的情况下为正则表达式引入了类型安全性，内置支持所有原生RegExp的功能，包括位置和命名捕获组、标志以及类型级别的模式验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该库解决了TypeScript开发中一个长期存在的痛点，那就是基于字符串构建的正则表达式在运行时之前无法提供任何类型安全性。ArkRegex能够直接从正则表达式模式推断出字符串类型，将引用不存在的组等语法错误作为类型错误进行捕获，避免运行时故障。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ArkRegex的语法与原生RegExp构造函数一致，这使其应用过程非常简单。文档中包含的一个示例展示了简单模式的类型推断：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;const ok = regex(&quot;^ok$&quot;, &quot;i&quot;)
// Regex&amp;lt;&quot;ok&quot; | &quot;oK&quot; | &quot;Ok&quot; | &quot;OK&quot;, { flags: &quot;i&quot; }&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;对于处理捕获组的开发者，ArkRegex为位置和命名捕获提供了自动的类型定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;命名捕获组也得到了类似的处理，该库为模式中每个命名组推断出精确的类型。这一功能对生产应用中常见的电子邮件验证和URL解析模式尤其有益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该版本在TypeScript 5.9或更高版本下表现最佳，它利用了TypeScript类型推断引擎的最新改进。要安装该库，只需要运行pnpm install arkregex或其他包管理器的等效命令即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ArkRegex加入了既有的TypeScript正则表达式生态系统，特别是采用了自然语言语法并编译为纯RegExp的&lt;a href=&quot;https://regexp.dev/&quot;&gt;magic-regexp&lt;/a&gt;&quot;。magic-regexp侧重于通过其构建器模式提高可读性，而ArkRegex则通过保持标准正则表达式语法的同时添加类型推断来优先考虑熟悉度。习惯传统正则表达式的开发者可能会发现ArkRegex的学习曲线更平缓，而偏好显式API的开发者可能更喜欢magic-regexp的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该库在TypeScript社区中获得了积极的反响。&lt;a href=&quot;https://x.com/jdegoes/status/1983252603703013838&quot;&gt;John De Goes remarked&lt;/a&gt;&quot;在回应该公告时评论说：“简直不可思议（是褒义的）”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://www.reddit.com/r/typescript/comments/1oidtxg/introducing_arkregex_a_dropin_replacement_for_new/&quot;&gt;Reddit的TypeScript社区中&lt;/a&gt;&quot;，该项目获得了大量赞誉，一位评论者对作者的实现所需复杂性的能力表达了信任：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;很明显，人们可以在类型级别做到这一点。但是，考虑到这需要支持无数边缘情况和功能，我会尝试吗？绝对不会。但知道了它是谁实现的，我100%相信这能按预期运行……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于超出TypeScript类型推断限制的复杂表达式，ArkRegex提供了 regex.as逃生出口，允许手动类型注解：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;const complexPattern = [regex.as](&lt;http: regex.as=&quot;&quot;&gt;)&amp;lt;`pattern-${string}`, {captures: [string]}&amp;gt;
 (&quot;very-long-complex-expression-here&quot;)&lt;/http:&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该项目使用了ArkType的测试框架&lt;a href=&quot;https://github.com/arktypeio/arktype/tree/main/ark/attest#readme&quot;&gt;attest&lt;/a&gt;&quot;进行了功能测试和基准测试。&lt;a href=&quot;https://github.com/arktypeio/arktype/tree/main/ark/regex/__tests__/regex.test.ts&quot;&gt;测试套件&lt;/a&gt;&quot;涵盖了ECMAScript规范支持的正则表达式语法中的边缘情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于ArkRegex是ArkType项目的一部分，而不是独立的版本化包，迁移的问题主要适用于首次采用该工具的开发者。该库不需要更改现有的正则表达式模式，可直接替换RegExp()调用。&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=arktypeio.arkdark&quot;&gt;ArkType Visual Studio Code扩展&lt;/a&gt;&quot;为regex调用添加了语法高亮，改善了开发体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ArkType是一个开源的运行时验证库，它从类型安全语法中解析出优化的验证器。该项目将TypeScript的类型系统扩展到运行时验证，使开发者能够定义在编译时和运行时都适用的模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/arkregex-introduced-typescript/&quot;&gt;ArkType Introduces ArkRegex with Type Safe Regular Expressions&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O1hmhradKkpvvRNIyJMb</link><guid isPermaLink="false">https://www.infoq.cn/article/O1hmhradKkpvvRNIyJMb</guid><pubDate>Mon, 02 Feb 2026 07:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>彩讯股份发布《企业级 AI 应用白皮书》用AI重新定义企业级软件</title><description>&lt;p&gt;1月 31 日，由雄安新区管委会主办的“人工智能+”创新生态系列活动在雄安新区成功举行。本次活动汇聚了人工智能领域的顶尖专家、行业领袖及领军企业，共同探讨AI技术如何深度赋能实体经济。彩讯股份受邀出席系列活动。在活动分论坛上，彩讯股份重磅发布了《企业级 AI 应用白皮书》（以下简称“白皮书”），围绕企业级 AI 的发展阶段、核心挑战与落地路径，系统性提出面向真实业务场景的行业判断与实践方法。旨在为企业在智能时代的转型升级提供实战路径与理论支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;彩讯股份CEO白琳、董事会秘书兼财务总监王欣、数行事业部总经理朱彩霞与生态伙伴稳准智能首席科学家崔鹏、CTO张兴璇、COO何玥共同参与发布仪式。白皮书的发布，标志着彩讯股份在企业级 AI 应用领域，基于长期实践积累形成的系统性研究成果正式对外发布。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/3b/2b/3b6a8034a6b30a7f24fceb0f0b758a2b.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（从左至右依次是：稳准智能COO何玥，稳准智能CTO张兴璇，稳准智能首席科学家崔鹏，彩&lt;/p&gt;&lt;p&gt;讯股份CEO白琳，彩讯股份董事会秘书兼财务总监王欣，彩讯股份数行事业部总经理朱彩霞）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;聚焦“企业级AI应用”，回应 AI 落地的真实问题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当前“人工智能+”加速向各行各业渗透的背景下，AI 在企业场景中的应用正从技术探索阶段，进入系统化、规模化落地的关键时期。彩讯股份在白皮书中指出，当前企业级 AI 面临的核心挑战，已从技术可得性转向 AI 能否与既有系统融合并形成可验证、可持续的业务价值。白皮书基于彩讯股份在通信、金融、能源、交通等多个行业的长期实践经验，从行业观察、痛点分析出发，系统梳理了企业级 AI 应用在架构设计、数据治理、安全合规、应用集成等方面的关键问题，并提出了具有可操作性的解决思路与实践路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从技术热潮走向系统工程，用 AI 重新定义企业级软件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;彩讯股份《企业级 AI 应用白皮书》并非一本单纯的技术手册，而是一份面向行业的参考性研究成果，旨在帮助企业客户、产业伙伴和管理者更理性地理解 AI 在企业中的角色与边界，重塑企业级软件的底层逻辑，推动企业级 AI 从“概念验证”走向“系统工程”。白皮书强调，企业级 AI 的价值释放，依赖于对业务场景的深度理解、对系统架构的整体规划，以及对长期演进路径的清晰判断。彩讯股份提出了“1+1+N”的整体落地路径，回应企业在 AI 推进过程中普遍面临的场景难选、系统难融与价值难证等现实问题。其中，“第一个 1”是一套面向企业真实环境的企服 AI 方法论，用于指导企业如何从业务场景出发，系统性推进 AI 应用；“第二个 1”是一套平台化的能力与工具集（Rich AIbox），为方法论落地提供工程化支撑；“N”则对应不同企业在具体业务场景中的实践沉淀与持续演进。通过这一结构，彩讯推动 AI 能够在既有业务体系中稳定运行、持续生长，并真正形成业务价值闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;以白皮书为起点，持续推动企业级 AI 实践&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为长期深耕企业数字化与智能化服务的上市公司，彩讯股份近年来持续加大在 AI 与智算领域的投入，围绕企业级 AI 平台、智能体应用及行业解决方案，探索 AI 技术与企业业务深度融合的实践路径。彩讯股份表示，未来将以本次白皮书发布为起点，持续通过行业研究、实践案例分享与生态合作，推动企业级 AI 应用经验的沉淀与传播，为“人工智能+”背景下企业级软件与应用体系的演进提供长期参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《企业级 AI 应用白皮书》完整版，可扫码下载：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/082d4f91b75f78c979f0be209d81f7b7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/GpiFToEo9bh05SGXwwyL</link><guid isPermaLink="false">https://www.infoq.cn/article/GpiFToEo9bh05SGXwwyL</guid><pubDate>Mon, 02 Feb 2026 06:57:33 GMT</pubDate><author>InfoQ</author><category>AI&amp;大模型</category></item><item><title>Linux 宕机分析“三座大山”，AI智能诊断如何破局？</title><description>&lt;p&gt;Linux 系统突发宕机是运维人员和开发者经常面临的难题。面对复杂的内核日志和内存转储文件，传统分析方式往往耗时费力且需要深厚的内核知识。本文将介绍阿里云操作系统控制台的宕机智能诊断功能，并展示其如何通过 AI 技术简化宕机分析流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;传统宕机分析的“三座大山”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;第一座大山：日志分析如同“看天书”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;服务器宕机后，运维人员首先需要查看 dmesg 日志。然而，内核日志往往包含大量难以理解的信息：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;[ 69518574.393036] Code: e8 38 ac e8 88 0b ff ff 0f 0b 48 c7 c7 d0 e8 38 ac e8 7a 0b ff ff 0f 0b 48 89 f2 48 89 fe 48 c7 c7 90 e8 38 ac e8 66 0b ff ff &amp;lt;0f&amp;gt; 0b 48 89 fe 48 c7 c7 58 e8 38 ac e8 55 0b ff ff 0f 0b 48 89 ee
[ 69518574.393070] RSP: 0018:ffffb0d3c0a3bb98 EFLAGS: 00010282
[ 69518574.393085] RAX: 0000000000000054 RBX: ffff9fbe07b158c0 RCX: 0000000000000000
[ 69518574.394079] RDX: ffff9fbeddf703e0 RSI: ffff9fbeddf5fb40 RDI: ffff9fbeddf5fb40
Kernel panic - not syncing: Fatal exception&amp;nbsp;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些信息对于普通运维人员来说难以理解，而且真正的问题往往隐藏在数千行日志中，需要花费大量时间排查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统的日志分析不仅需要深厚的技术背景，还要对内核各个子系统有深入理解。例如，hardlockup 错误需要了解 CPU 调度、中断处理、自旋锁等机制；hungtask 问题需要熟悉进程状态转换、等待队列、资源竞争等概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;第二座大山：VMCORE 分析耗时又费力&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于复杂问题，通常需要获取 VMCORE 文件进行深入分析。完整的 VMCORE 分析流程包括：&lt;/p&gt;&lt;p&gt;首先得加载 VMCORE 文件到调试工具；然后执行各种复杂的调试命令；手动分析各种输出信息；最后尝试拼凑出问题的全貌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个过程可能需要数小时甚至数天，并且对分析人员的内核知识要求较高。VMCORE 分析涉及的技术层面非常广泛，包括内存布局分析、进程状态重建、内核数据结构解析等。例如，分析内存错误需要检查页面分配状态、分析内存损坏问题；排查死锁问题则需要重建锁依赖关系、分析调用栈行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;第三座大山：找补丁如同“寻宝游戏”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;定位到问题后，还需要找到对应的修复补丁。Linux 内核的 Git 仓库包含三十多年演进历史，累计超过百万次 commit，涉及上万名开发者。从如此庞大的代码库中找到与特定问题相关的修复，需要对内核演化历史有深入了解。人工筛选不仅效率低下，而且容易遗漏关键信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三大挑战使得传统宕机分析流程复杂且耗时。阿里云操作系统控制台的宕机智能诊断功能旨在解决这些问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;阿里云操作系统控制台宕机智能诊断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云操作系统控制台（简称操作系统控制台）是一站式操作系统运维管理平台，提供了内存、I/O、网络、内核崩溃等强大的系统诊断能力，SysOM 是操作系统控制台的运维组件。但这些功能通常需要用户登录控制台，并具备一定的运维经验才能有效使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;什么是宕机智能诊断？&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宕机智能诊断是阿里云操作系统控制台提供的系统场景诊断功能，基于大模型技术，融合了内核调试技术和丰富的故障案例，能够自动完成从日志分析到问题定位，再到补丁推荐的全流程，让原本复杂的宕机分析变得简单高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云操作系统控制地址链接：&lt;a href=&quot;https://alinux.console.aliyun.com/&quot;&gt;https://alinux.console.aliyun.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6d/6d64ebdca27214af29faef0f4c73540b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;三大核心能力&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1. 智能日志解析，告别“天书”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再也不用对着复杂的内核日志发愁了！宕机智能诊断的日志解析功能能自动提取关键信息，为后续 AI 分析提供结构化的数据基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心能力：&lt;/p&gt;&lt;p&gt;结构化信息提取：自动从日志中提取版本号、崩溃标题、进程名、函数名、RIP 寄存器值、CPU 编号、加载模块等关键字段；调用栈分层解析：识别并分离 NMI 栈、IRQ 栈、任务栈三层调用关系，过滤无效函数，提取 top-3关键函数调用链；故障类型识别：支持 hardlockup、hungtask、memory_error、softlockup、hardware_error 等主流内核故障类型的快速判定；错误日志聚合：自动按时间戳排序错误日志，过滤冗余调用栈信息，保留关键诊断线索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际效果：传统方式需要人工从数千行日志中逐行查找关键信息，而系统可以在秒级完成日志解析和结构化提取，将非结构化的 dmesg 日志转化为结构化的特征集合，为后续的 AI 诊断提供清晰的数据输入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2. 专项诊断，精准打击&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统针对不同类型的内核问题设计了专属的诊断能力，深度集成 drgn 内核调试器，能够直接访问 VMCORE 中的内核数据结构，结合 AI 推理实现智能分析：&lt;/p&gt;&lt;p&gt;Hardlockup 诊断：采用图遍历算法构建锁依赖图，自动检测循环等待和死锁场景，输出清晰的锁等待路径（如：CPU1→lockA→CPU2→lockB→CPU3→lockC→CPU1 形成死锁环路）；Hungtask 诊断：实现链式追踪算法，从 D 状态进程开始逐级分析等待链，定位终端阻塞点（Terminal Holder），给出完整的资源等待路径；Memory Error 诊断：识别 use-after-free、空指针解引用、野指针等典型内存错误类型，追踪内存分配和释放路径；Softlockup诊断：分析调度延迟、CPU 占用模式，检测软锁和响应超时问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每种诊断都遵循“算法提取数据骨架 + AI 补全推理逻辑”的模式，既保证分析的准确性，又实现诊断的智能化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;3. 智能补丁匹配，一步到位&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宕机智能诊断采用了混合向量检索技术来进行补丁搜索。系统首先使用 text-embedding-v4 模型将问题描述转换为 1536 维的稠密向量和稀疏向量，在面向 Linux 内核历史提交构建的向量数据库中进行语义相似度检索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;检索过程分为两个阶段：&lt;/p&gt;&lt;p&gt;第一阶段-向量检索：通过向量数据库快速从海量 commit 中召回 top-k 个最相关的候选补丁；第二阶段-智能排序：利用大模型技术对每个候选补丁进行深度分析，评估其与当前问题的相关性（1-10分），并给出详细的相关性原因说明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统支持按内核版本进行过滤（如筛选 v5.10 及以上版本的补丁），帮助用户更精准地检索到适用于特定版本的修复方案。最终返回多个最相关的补丁，每个补丁都包含 commit ID、摘要、相关性评分和推荐理由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实际效果：Hardlockup 死锁问题的智能诊断&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以一个真实的生产环境 Hardlockup 故障为例，服务器突发系统无响应并崩溃。运维人员通过控制台发起诊断后，系统在&amp;nbsp;5 分钟内生成了完整的诊断报告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告包含了以下关键信息：&lt;/p&gt;&lt;p&gt;故障类型识别：自动判定为 Hardlockup 死锁问题；死锁链路分析：识别出三方 CPU 间的循环等待关系，包括各 CPU 持有和等待的锁；根因定位：指出导致死锁的关键代码路径和函数调用；修复建议：提供 4 条针对性的缓解措施；补丁推荐：从 Linux 内核百万级提交中检索出 3 个相关补丁，按相关性排序并说明推荐理由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次诊断中，系统首推的补丁正是实际修复该问题的补丁，其余 2 个推荐补丁也与故障症状高度匹配。对于这种复杂的多方死锁场景，传统人工分析通常需要数小时甚至数天，而宕机智能诊断在几分钟内完成了从问题分析到补丁推荐的全流程，大大降低了故障处理门槛和运维成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快速上手宕机智能诊断&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宕机智能诊断功能支持使用 .rpm 包格式的主流 Linux 发行版，包括 Alibaba Cloud Linux、CentOS、Anolis OS、Rocky Linux、AlmaLinux 等。对于 Alibaba Cloud Linux、CentOS、Anolis OS 等发行版，系统会自动获取 debuginfo，降低使用成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;推荐方式：通过 SysOM MCP 使用（AI 助手集成）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;amp;mid=2247490073&amp;amp;idx=1&amp;amp;sn=e246a8200cacb2469dcdcb81d8085fe8&amp;amp;scene=21#wechat_redirect&quot;&gt;SysOM MCP&lt;/a&gt;&quot;阿里云开源的系统诊断工具集，基于 Model Context Protocol 协议，将宕机智能诊断能力封装为标准化的 MCP 工具，可以通过 AI 助手（如 qwen-code）使用自然语言直接进行宕机诊断。&lt;/p&gt;&lt;p&gt;🔗&amp;nbsp;项目地址：&lt;a href=&quot;https://github.com/alibaba/sysom_mcp&quot;&gt;https://github.com/alibaba/sysom_mcp&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;请参考项目文档完成安装和配置。配置完成后，在 AI 助手中直接使用自然语言发起诊断：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;示例 1：调用宕机智能诊断&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;请帮我分析一个宕机问题，vmcore 下载链接：https://path/to/your/vmcore&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;说明：·&amp;nbsp;API 接受的是 HTTP/HTTPS 下载链接，确保下载链接具有适当的访问权限，便于诊断服务下载和分析；·&amp;nbsp;对于 Rocky Linux、AlmaLinux 等其他发行版，需要额外提供 debuginfo 和 debuginfo-common 的下载链接。暂不支持使用 .deb 包格式的发行版（如 Ubuntu、Debian 等），该功能正在开发中。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;示例 2：查询历史诊断任务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;查看我最近 7 天的宕机诊断记录，并返回上一次的诊断结果&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 助手会自动调用相应的 MCP 工具，并将诊断结果以易读的方式呈现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高阶方式：直接调用 OpenAPI 接口&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于需要集成到自动化运维系统或自定义工作流的场景，可以直接调用 OpenAPI 接口。详细使用方式请参考操作系统控制台 OpenAPI 文档。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;操作系统控制台 OpenAPI 文档链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://next.api.aliyun.com/api/SysOM/2023-12-30/CreateVmcoreDiagnosisTask&quot;&gt;https://next.api.aliyun.com/api/SysOM/2023-12-30/CreateVmcoreDiagnosisTask&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Linux 宕机分析不再是少数专家的专利！阿里云操作系统控制台的宕机智能诊断功能通过 AI 技术与专业内核调试工具的深度融合，让每一位运维和开发都能轻松应对复杂的系统问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个追求高效运维的时代，拥有宕机智能诊断这样的功能，无疑会让你的工作事半功倍。无论是深夜排障还是日常维护，都能从容应对，再也不用为复杂的内核问题而头疼了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你也想告别 Linux 宕机分析的烦恼，不妨试试阿里云操作系统控制台的宕机智能诊断功能，让 AI 成为你的得力助手！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若想使用更全面的 SysOM 功能，请登录阿里云操作系统控制台体验，地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://alinux.console.aliyun.com/&quot;&gt;https://alinux.console.aliyun.com/&lt;/a&gt;&quot;。您在使用操作系统控制台的过程中，有任何疑问和建议，可以扫描下方二维码或搜索群号：94405014449&amp;nbsp;加入钉钉群反馈，欢迎大家扫码加入交流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b25287aa24b6e95b58056b5056ca0927.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;操作系统控制台钉钉交流群&lt;/p&gt;</description><link>https://www.infoq.cn/article/hAPDteGECUhbzL2OCxwd</link><guid isPermaLink="false">https://www.infoq.cn/article/hAPDteGECUhbzL2OCxwd</guid><pubDate>Mon, 02 Feb 2026 06:55:05 GMT</pubDate><author>邹涛</author><category>阿里巴巴</category><category>操作系统</category></item><item><title>NVIDIA Dynamo Planner为多节点LLM推理带来SLO驱动的自动化</title><description>&lt;p&gt;微软和英伟达已经发布了他们合作的第二部分，即在&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/aks/what-is-aks&quot;&gt;Azure Kubernetes Service&lt;/a&gt;&quot;（AKS）上运行NVIDIA Dynamo进行大型语言模型推理。第一个声明的目标是在分布式&lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;GPU&lt;/a&gt;&quot;系统上实现每秒120万个token的原始吞吐量。现在，这个最新版本专注于帮助开发人员更快地工作并提高操作效率。它通过自动化资源规划和动态扩展功能来实现这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新的功能集中在两个集成组件上：&lt;a href=&quot;https://github.com/ai-dynamo/dynamo/tree/main/benchmarks/profiler&quot;&gt;Dynamo Planner Profiler&lt;/a&gt;&quot;和基于SLO的&lt;a href=&quot;https://github.com/ai-dynamo/dynamo/blob/main/docs/planner/sla_planner.md&quot;&gt;Dynamo Planner&lt;/a&gt;&quot;。这些工具协同工作，解决在解耦服务中的“速率匹配”挑战。团队在拆分推理工作负载时使用这个术语。它们将处理输入上下文的预填充操作与生成输出token的解码操作分开。这些任务运行在不同的GPU池上。如果没有合适的工具，开发团队需要花费大量时间来确定这些阶段的最佳GPU分配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dynamo Planner Profiler是一个预部署模拟工具。它会自动搜索最佳配置。开发人员可以跳过手动测试各种并行化策略和GPU计数，节省GPU利用率的时间。相反，他们在DynamoGraphDeploymentRequest （DGDR）清单中定义自己的需求。分析器会自动&lt;a href=&quot;https://github.com/ai-dynamo/dynamo/blob/main/docs/benchmarks/sla_driven_profiling.md#profiling-method&quot;&gt;扫描&lt;/a&gt;&quot;配置空间。它测试了预填充和解码阶段的不同张量并行度大小。这有助于找到在保持延迟限制的同时提高吞吐量的设置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;分析器包括一个AI配置模式，可以根据预先测量的性能数据在大约20到30秒内模拟性能。这种能力允许团队在分配物理GPU资源之前快速迭代配置。输出提供了一个调整后的设置，以提高团队所说的“&lt;a href=&quot;https://arxiv.org/abs/2401.09670&quot;&gt;Goodput&lt;/a&gt;&quot;”。这是最高的吞吐量，同时保持第一个token的时间和token间的延迟在设定的限制内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a34b3b488cbf84c41addb9de74ed3799.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦系统投入生产，基于SLO的Dynamo Planner就将接管作为运行时编排引擎。这个组件是“LLM感知的”，这意味着与传统负载均衡器不同，它会关注集群状态。。它跟踪诸如解码池中的键值缓存加载和预填充队列的深度等内容。规划器使用分析器的性能界限来扩展预填充和解码工作器。这有助于在流量模式变化时满足服务级别目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公告通过一个详细的航空公司助手场景来说明这些能力。在这个场景中，&lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-32B-FP8&quot;&gt;Qwen3-32B-FP8&lt;/a&gt;&quot;模型支持一个航空公司移动应用程序。它遵循严格的服务级别协议：第一个Token为500毫秒，Token间的延迟为30毫秒。在正常操作中，系统运行一个预填充工作器和一个解码工作器。当天气中断导致200个用户发送复杂的改道请求时，规划器注意到了峰值。然后它扩展到两个预填充工作器，但仍保持一个解码工作器。团队报告说，新工作器在几分钟内上线，允许系统在流量峰值期间维持延迟目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本建立在原始Dynamo公告中引入的框架之上，InfoQ在2024年12月报道了这一点。在上一篇文章中，Azure和英伟达解释了Dynamo的设计如何将计算密集型和内存绑定任务分散到各种GPU上。这允许团队独立优化每个阶段，将资源与工作负载需求相匹配。例如，电子商务应用程序的预填充任务可能处理数千个token，而其解码任务只生成简短的描述。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从手动设置转向自动化、SLO驱动的资源管理，展示了团队如何在Kubernetes上更好地处理大语言模型部署。规划器组件提供了将延迟需求转化为GPU分配和扩展选择的工具。这旨在降低运行解耦推理架构的操作负担。自动化工具可以帮助需要推理重或长上下文LLM的组织。它们使管理复杂的多节点GPU设置变得更容易。它们还支持在变化的流量模式下满足服务级别目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/nvidia-dynamo-ai-kubernetes/&quot;&gt;https://www.infoq.com/news/2026/01/nvidia-dynamo-ai-kubernetes/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QJAPI0RrIrKCY2fN3KTL</link><guid isPermaLink="false">https://www.infoq.cn/article/QJAPI0RrIrKCY2fN3KTL</guid><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><author>作者：Claudio Masolo</author><category>AI&amp;大模型</category></item><item><title>Pulumi宣布原生支持Terraform和HCL</title><description>&lt;p&gt;Pulumi宣布&lt;a href=&quot;https://www.pulumi.com/blog/all-iac-including-terraform-and-hcl/&quot;&gt;原生支持HashiCorp Terraform和OpenTofu&lt;/a&gt;&quot;，这极大地扩展了其平台的范围。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一战略转变标志着该公司发生了重大变化，此前Pulumi一直以独家推崇&lt;a href=&quot;https://www.pulumi.com/docs/iac/languages-sdks/&quot;&gt;通用编程语言&lt;/a&gt;&quot;而闻名。此次更新引入了直接通过Pulumi引擎执行HashiCorp配置语言（HashiCorp Configuration Language，HCL）的能力，并支持在Pulumi Cloud中托管Terraform的状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些功能目前处于私有测试阶段，预计将于2026年第一季度发布正式版本。它们通过允许工程团队将现有项目与新部署并行运行，解决了迁移遗留代码的持续挑战。这项举措专门针对那些因为&lt;a href=&quot;https://newsroom.ibm.com/2025-02-27-ibm-completes-acquisition-of-hashicorp,-creates-comprehensive,-end-to-end-hybrid-cloud-platform&quot;&gt;IBM收购HashiCorp&lt;/a&gt;&quot;及相关许可变更而感到不安的组织，它提供了一个统一的平台，减少了切换基础设施工具时通常遇到的运维冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如公告中所述，Pulumi的创始人兼首席执行官Joe Duffy承认了现代企业基础设施中存在混合工具环境的现实。他指出，尽管许多组织更喜欢现代的方法，但它们通常会保留对旧工具多年来的投入。Duffy 表示，“我们对语言并不教条，我们爱所有的语言”。“HCL和YAML中的L都代表‘语言’，我们一直秉持‘来者不拒’的心态。一旦我们看到某种语言有足够的市场需求，我们就会将其添加进来。好吧，HCL的时机已经到了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术实现涉及两个不同的功能。首先，Pulumi Cloud现在可以作为&lt;a href=&quot;https://www.terraform.io/&quot;&gt;Terraform&lt;/a&gt;&quot;和&lt;a href=&quot;https://opentofu.org/&quot;&gt;OpenTofu&lt;/a&gt;&quot;的状态后端和管理平面，这会直接与HashiCorp Terraform Cloud竞争。这项集成提供了可见性、治理以及对&lt;a href=&quot;https://www.pulumi.com/product/neo/&quot;&gt;Pulumi的AI工程代理Neo&lt;/a&gt;&quot;的访问，而无需关心底层的基础设施工具是什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，Pulumi CLI现在将HCL作为一等语言来提供支持。这允许引擎使用Terraform桥接器访问提供程序（provider）来解释HCL代码。与之前将HCL转换为TypeScript或Python等语言的转换工具不同，此功能允许团队维护HCL代码库，同时利用Pulumi的编排能力。这使得平台团队可以用Go或Python构建复杂的组件，然后由其他使用简单HCL模块的团队消费，从而实现一种多语言的架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公告澄清说，此功能不是“附加（bolt-on）”的功能。相反，它为HCL用户提供了对整个Pulumi生态系统的完全访问，包括数千个提供程序，就像任何其他受支持的语言一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了进一步激励迁移，Pulumi&lt;a href=&quot;https://www.pulumi.com/blog/all-iac-including-terraform-and-hcl/&quot;&gt;推出了一项财务“逃生出口”计划&lt;/a&gt;&quot;。该计划使客户能够将相当于其剩余HashiCorp合同价值的积分用于Pulumi的使用，旨在减轻过渡期间运行并行系统时的财务负担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基础设施即代码（infrastructure-as-code，IaC）市场的竞争依然十分激烈。HashiCorp Terraform仍然是声明式基础设施的行业标准，而Linux Foundation的OpenTofu在HashiCorp转向商业源代码许可证后，作为一种开源替代品获得了关注。Crossplane等其他竞争对手提供了基础设施管理的控制平面方法。通过集成HCL和Terraform状态，Pulumi不仅将自己定位为替代品，而且定位为能够管理竞争格式的统一平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/pulumi-adds-terraform-hcl/&quot;&gt;Pulumi Adds Native Support for Terraform and HC&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/gGew57Wf55pCfHFEE1mX</link><guid isPermaLink="false">https://www.infoq.cn/article/gGew57Wf55pCfHFEE1mX</guid><pubDate>Mon, 02 Feb 2026 04:00:00 GMT</pubDate><author>作者：Mark Silvester</author><category>云计算</category></item><item><title>飞猪AI测试新范式：维护降70%、漏测减半、死循环归零</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一、背景与愿景&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以飞猪为例，生活服务类应用的 C 端的业务质量保障，往往面临业务快速迭代、技术架构复杂，多端场景覆盖难等多重挑战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业务层面：受旅行行业“七节两促”特性的影响，在高频营销活动驱动下，往往伴随着较为快速的发布节奏；如何在快节奏中构建稳定的 C 端质量保障体系，与安全生产能力成为关键问题。技术层面：C 端系统采用 Native、Flutter、Weex、DX、H5 等多技术栈混合架构；同时，测试回归需覆盖飞猪 App、手淘飞猪 Tab，及淘、支、微、红等多平台小程序入口，这导致测试回归复杂度指数级上升；此外，功能回归与用户体验提升需协同产研推进，进一步加剧了发布小窗口期下的质量保障难度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;UI 自动化作为 C 端质量保障的切口之一，而 AI 能够在现有场景下，为自动化赋予新的机遇，解决业界 UI 自动化的普遍挑战与共性问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用例维护成本高：业务快速变更导致失效率持续攀升，人工投入占比过大；断言有效性不足：多端入口交互逻辑差异使覆盖不全，问题漏检风险存在；多端兼容性问题突出：多端差异和逻辑定制，易引发测试盲区，易触发线上故障；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对这些痛点，我们计划通过 AI 技术，结合并优化现有自动化测试体系：降低用例腐化率以减少人工成本，提升断言精准度以增强问题发现能力，从而在保障质量的同时提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cc/cc3ad8f2f98e0bd7c97961d4e2c70443.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图 1：飞猪多端 - 流量入口示意图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;二、挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在“AI + X”的落地实践中，应用的技术演进大多遵循一条较为清晰的技术路径：从基础提示工程（Prompt Engineering）起步，到检索增强生成（RAG）、记忆体（Mem）、智能体技能（Agent Skills）和多智能体系统（Multi-agent Systems / Sub-agents），最终监督微调（SFT）、GPO/GRPO 等模型层的策略优化方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而当时，我们在技术调研时发现，AI 自动化领域在当时深入借鉴的参考标杆偏少。在开源技术论坛中的技术分享，大多数文章仍聚焦于 0-1 阶段的试用与调研，缺乏对成熟技术路径的规模化应用验证。同时，外部的开源范例（如：阿里 Mobile-agent、微软 playwright-mcp、字节 midscene.js）也都是更聚焦模型 / 框架层面的基础能力建设，而缺少整体的能力串联、使用效果、演进路线上的实践范式。&lt;/p&gt;&lt;p&gt;如何将 “凭借 AI 可以快速入门的能用” 变成 “可支持月均 10 万 + 构建，稳定、快速运行的好用、易用” 是我们在这个技术演进路线上的最大挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;三、策略与思路&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.1、做好评测体系的先行建设，用数据指引应用迭代效果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心原则：在 AI 自动化开发启动阶段，即需要同步建立与目标对齐的效果评测体系，将效果验证从“事后补救”前置为“设计输入”，确保技术演进始终服务于质量保障目标，避免因缺乏量化依据导致的无效迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业验证与内部实践依据：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gartner AI 的研究报告指出，73% 的 AI+X 项目因评测体系缺失而无法规模化落地，表现为技术优化与业务效果脱节。AI 自动化的前期探索中，常见的技术挑战，往往会遇到的典型问题：提示工程（PE）优化后：执行效果异常，AI 幻觉问题频发，导致 PE 紧急回滚；RAG 知识库迭代后，关键业务数据召回率显著下降；模型切换后：本地调试结果与线上实际效果存在偏差，导致整体效果质量下滑，case 失败率增高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实施要点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们从应用 workflow Benchmark 评测集建设、“渐进式消融评测机制”：基座模型 → &amp;nbsp;Prompt → RAG → Agent 分阶段验证效果等方式作为评测体系的基准，每次技术调整（提示工程优化、知识库更新、模型切换）均需通过真实业务数据验证端到端效果，结合自动化测试数据与人工路径验证，确保评测结果反映真实用户体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;价值体现：先行评测体系为 AI+X 实践提供客观决策依据，有效规避“技术优化但业务效果下降”的风险。为实现从“能用”到“可靠规模化”的关键跨越提供了数据支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.2、通过工作流设计，避免模型流程死循环（break cycle），提升故障恢复与自检能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心原则：在 AI 工作流设计中嵌入防死循环机制与故障恢复路径，确保系统在异常情况下能主动退出无效循环、回退至安全状态，而非陷入无限尝试。聚焦业务连续性保障，避免因局部故障导致整体流程失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题依据与内部实践痛点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业共性问题：多智能体系统普遍存在流程死循环风险（如 Cursor 等工具中模型反复执行相同操作），在 AI 自动化场景中尤为突出。例如，当用户未填写必选 SKU 时，系统通常触发 toast 提示，但 AI 在截图 / 操作过程中可能无法捕获此类信息，导致模型陷入“尝试 - 失败 - 重试”的无限循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;动态死循环检测机制：基于 History 和 Memory 设计算法，实时分析操作序列相似度（如连续 3 次相同点击指令，及相似参数返回，即触发预警）；设定阈值规则：当操作重复率≥60% 或单节点耗时超时，自动判定进入死循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分层恢复路径设计：一级自检：轻量级模型（如 Qwen3-VL-7B）快速扫描历史操作，通过 ReAct 逻辑判断根本原因（例：识别“未捕获 toast”后触发跳过指令）；二级升级：对复杂循环（如多端交互差异），临时调用高参数模型（qwen3-vl-235b-a22b-thinking）进行深度推理，结合 RAG 补充行业知识库（如“下单页 SKU 选择死循环通用处理方案”）检测到连续 N 次无效点击，workflow 自动调用 RAG 获取“必填项缺失”处理方案；；安全回退：强制回退至最近稳定检查点（如“度假搜索 Listing 页”），避免全流程重启。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;价值体现：工作流设计的本质是赋予 AI 系统“自省能力”——通过防死循环机制与分层恢复策略，将故障转化为可自动修复的常规操作，使技术演进真正服务于业务稳定性目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.3、通过 RAG、记忆体与子智能体补充业务垂类知识，保障高 UV 页面路径的精准覆盖&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心原则：将业务垂类知识深度嵌入 AI 工作流，确保模型理解真实用户行为路径与行业术语逻辑，使测试覆盖严格对齐核心业务流目标，避免因知识缺失导致的路径偏差与漏检风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题依据与内部实践痛点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户路径覆盖失准：模型对业务高频路径的理解存在偏差。例如，当指令为“订北京中关村附近，500 元预算，下个月 1 号大床房”时，实际用户 90% 通过“酒店金刚”或“猪搜”入口操作，但自动化测试常误判至其他资源位（如活动页），导致核心 UV 页面链路覆盖准确率不足，无法有效验证真实用户高频场景。行业术语理解缺失：模型对垂类术语（如“交通 OD”指交通出行数据、“OTA 页面”指在线旅游平台）存在歧义，引发测试用例生成逻辑错误。例如，在航班测试中，“OD”被误识别为“订单”，导致关键流程验证失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实施策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RAG 业务知识库定制：构建飞猪专属知识库，整合用户行为热力图（如酒店金刚点击路径）、行业术语词典（如“OD=Origin-Destination”），在 Prompt 生成前动态注入上下文。例如，当检测到“订酒店”指令，且无其他特殊要求时，RAG 自动匹配“酒店金刚”作为首选入口，确保测试路径与真实用户行为一致。记忆体（Mem）动态优化：设计短期记忆模块，实时记录用户历史操作特征（如连续 3 次从“搜索模块”进入酒店列表），在决策时应该优先调用高频路径逻辑。针对大促营销活动期，记忆体自动识别新增入口（如“双 11 特惠”标签），动态调整测试优先级。子智能体（sub-Agent）分工协同：路由 Agent：专责解析指令并匹配高频用户路径（如识别“订酒店”自动路由至酒店金刚）；术语 Agent：实时校正行业黑话（如将“交通 OD”映射为交通数据模块），确保测试逻辑无歧义；验证 Agent：在关键节点（如支付前）交叉校验路径是否覆盖核心 UV 页面，触发偏差预警。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;价值体现：业务垂类知识是 AI 自动化测试的“导航仪”——通过 RAG、记忆体与子智能体的协同设计，将抽象指令转化为精准的业务路径验证，确保技术服务于核心用户场景的质量保障目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.4、持续跟进前沿技术，动态演进应用能力，优化整体链路效果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心原则：将技术演进，视为应用体系的有机组成部分，通过持续跟踪 AI 能力边界拓展与生态创新，实现测试链路与业务复杂度的动态适配，避免技术滞后成为效果瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题依据与内部实践痛点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 技术的演化迭代速度日新月异，在 AI 自动化的基座模型下，我们从最初 gpt3.5 只能写文字、到 gpt4 可以多模态传图片，到 qwen-vl-max-latest 能够在点击、滑动时，精准给到像素级别的操作 的 pixel point，都表明了技术能力的演进速度，已经远远超越我们去思考如何 fix issue 的迭代速度了。&lt;/p&gt;&lt;p&gt;通过建立与 AI 技术发展同频的升级机制，技术底座持续吸收 AI 的开源演化成果，并高效整合开源生态创新，使测试体系始终具备精准匹配业务迭代的适应性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.5、拓展 AI 泛化检查能力，加强视觉智能感知与断言，降低漏测概率&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心原则：突破操作意图识别的局限，将 AI 能力延伸至对视觉界面的动态理解与泛化校验，使测试体系从“执行动作”转向“结果验证”，确保系统能自主感知 UI 状态变化并判断业务逻辑一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题依据与内部实践痛点：现有测试过度依赖操作指令解析与“编码形式的断言”，难以应对多端 UI 差异场景下的隐性问题。例如，小程序中优惠券弹窗样式，可能只断言了弹出是否弹出，或者弹窗文案是否正常展示，但是如果弹窗局部出现了空坑，或者渲染异常，通过 “编码形式的传统断言” 是无法及时感知与相应的，如此就产生了漏测的可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 AI 本身的图片解析与研判能力，就可以很好的处理这些问题，即可以判断单张图片上的泛化异常问题，也可以在多张图片的链路上，去分析判断一致性等相关问题。又或者结合实事、工单、可诉等相关外部数据，给出非逻辑 BUG 的风险提醒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;价值体现：AI 泛化检查是质量保障的“视觉神经”——让测试能力从机械执行转向智能感知，确保技术演进始终服务于用户体验的核心目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;四、效果展示&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;从几个橱窗场景，进行 AI 智能化效果展示。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.1、对于异常弹窗的静默处理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8aac998a5cc9bf514f1fe3833991302f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.2、对于异形元素（无文字）的像素级坐标感知&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2c/2cfc3b7803c8285b04f1db0a8f690294.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.3、对于连续逻辑的动态自检与判断能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0f5b0be625d6c453becd386d3bdc3a74.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.4 对于循环操作的短期记忆&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ff/ffecefe7e9231976e5247f497a4232ae.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.5 对于死循环场景的脱困能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/65c2bfa197022e95e63588357330540c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.6 对于截图的泛化检查能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c4/c4304337b9123dedfb48a3d04cc3b0db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;五、思考总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 技术的深度引入，有效解决了 C 端 UI 自动化质量保障体系普遍存在的通用问题，推动测试能力实现较大的提升：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用例维护成本显著降低通过 AI 语义化改造，系统能够动态理解业务变更逻辑（如营销活动入口调整），自动适配用例，大幅减少因业务快速迭代导致的人工维护投入，使团队精力从重复性调整转向测试策略优化。测试覆盖深度切实提升泛化检查能力突破了传统编码断言的局限，使验证从操作指令延伸至结果状态。系统可自主识别多端 UI 差异中的隐性问题（如弹窗渲染异常、元素空坑等），有效弥补了人工难以覆盖的视觉类风险盲区。多端兼容性问题系统性改善基于 RAG、记忆体与子智能体的协同设计，AI 深度融入业务垂类逻辑（如高频用户路径、行业术语校正），确保测试流严格对齐真实用户行为，显著降低了因端侧差异引发的漏检风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本质价值：AI 不是简单替代人工，而是将测试工程师从机械执行中解放，使其聚焦于质量策略设计与业务风险预判。当系统能自主完成弹窗处理、像素级操作及死循环脱困时，质量保障真正实现了从“执行工具”到“智能伙伴”的转变——技术价值的体现，在于让专业能力更高效地服务于用户体验本质。&lt;/p&gt;</description><link>https://www.infoq.cn/article/hZfuVu2FzZBv641HulGS</link><guid isPermaLink="false">https://www.infoq.cn/article/hZfuVu2FzZBv641HulGS</guid><pubDate>Mon, 02 Feb 2026 02:10:10 GMT</pubDate><author>飞猪技术 杨飞</author><category>生成式 AI</category></item><item><title>亚马逊云科技统一上调面向 ML 的 EC2 Capacity Block 定价，涨幅约 15%</title><description>&lt;p&gt;亚马逊云科技已在所有支持 EC2 Capacity Blocks 的区域，将面向机器学习的该服务&lt;a href=&quot;https://aws.amazon.com/ec2/capacityblocks/pricing/&quot;&gt;价格统一上调&lt;/a&gt;&quot;，涨幅约 15%。此次价格调整影响的是为大规模机器学习工作负载预留专用 GPU 计算能力的组织，亚马逊云科技旗下最强的多款 ML 实例价格均同步上涨，包括基于 NVIDIA GPU 的 &lt;a href=&quot;https://aws.amazon.com/ec2/ultraclusters/&quot;&gt;P5en、P5e、P5 以及 P4d&lt;/a&gt;&quot;，以及使用 &lt;a href=&quot;https://aws.amazon.com/ai/machine-learning/trainium/&quot;&gt;AWS Trainium&lt;/a&gt;&quot; 的 Trn2 和 Trn1 实例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管亚马逊云科技自 2023 年推出 Capacity Blocks 以来一直强调其定价会根据供需关系动态调整，但云经济学家 Corey Quinn 在领英博文中&lt;a href=&quot;https://www.linkedin.com/posts/coquinn_aws-raises-gpu-prices-15-on-a-saturday-activity-7414052677245952000-98pu&quot;&gt;指出&lt;/a&gt;&quot;，这次调整与常见的动态定价机制并不相同：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是亚马逊云科技直接更新了官网公布的基础价格……每小时 34.608 美元统一变为 39.799 美元，所有区域一致。这是一次政策层面的决定，而不是供需波动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html&quot;&gt;EC2 Capacity Blocks for ML&lt;/a&gt;&quot; 允许企业在亚马逊 EC2 UltraClusters 中预留 GPU 实例。而 UltraClusters 则是亚马逊云科技为需要数百甚至上千张 GPU 的分布式机器学习训练而优化的高性能计算基础设施。与标准预留实例或 Savings Plans 不同，Capacity Blocks 能在明确的时间窗口内（通常从一天到数周）保证客户获得指定实例类型的使用权。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一变化的影响并不止于成本上升本身。Platform Fix 创始人 &lt;a href=&quot;https://www.linkedin.com/in/stevendavidwade/&quot;&gt;Steve Wade&lt;/a&gt;&quot; 在 Quinn 的同一条领英博文的讨论区中指出了更深层的意义：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;先例已经被立下了，这才是关键。一旦这扇门被打开，就不会再关上。每个 FinOps 团队的风险清单里，都会多出一项。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Portainer 的产品负责人 &lt;a href=&quot;https://www.linkedin.com/in/nathankpeck/&quot;&gt;Nathan Peck&lt;/a&gt;&quot; 则从更宏观的经济背景解读了这一变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;要警惕通胀和美元贬值的速度，这二者正在超过摩尔定律带来的效率提升。这才是真正改变云计算模式的临界点。无法跟上通胀的“静态价格”，在技术上等同于持续降价。一旦云厂商无法继续维持这种状态，提前自购硬件反而会显得更有吸引力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次调价也反映了云基础设施市场真实存在的供应链压力。富国银行董事总经理兼技术高管 &lt;a href=&quot;https://www.linkedin.com/in/yetanotherdavidlee/&quot;&gt;David Lee&lt;/a&gt;&quot; 表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们正在经历另一轮类似疫情时期的供应紧张，尤其是在内存和网络交换设备方面。几乎所有东西都在涨价。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，真正的瓶颈可能并非很多人想象的那样。一位资深 DevSecOps 工程师 &lt;a href=&quot;https://www.linkedin.com/in/james-sparenberg/&quot;&gt;James S.&lt;/a&gt;&quot; 指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这里的“供给”其实是电力。微软 CEO 曾提到，他们有整仓库的 GPU 尚未安装，因为根本没有地方部署。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可替代方案的稀缺进一步放大了这次涨价的实际影响。一位 Reddit 的 r/aws 社区用户&lt;a href=&quot;https://www.reddit.com/r/aws/comments/1q511r9/rate_increase_ec2_capacity_blocks/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Capacity Blocks 基本上是你唯一能用到这些实例类型的方式。几乎不可能按需启动它们。某种程度上，这等于对外宣传一个按需价格，但实际收费却更高。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种稀缺性意味着企业几乎没有空间去消化这次成本上涨。更重要的是，即便是签署了企业折扣协议的客户也无法幸免，因为这些折扣通常是按百分比计算，而不是固定金额——公开价格上涨 15%，实际成本同样会上涨 15%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前尚不清楚这是亚马逊云科技的单独调整，还是整个行业趋势的一部分。Snowflake 的战略解决方案工程师 &lt;a href=&quot;https://www.linkedin.com/in/spencertheissenvang/&quot;&gt;Spencer T.&lt;/a&gt;&quot; 指出，此次涨价似乎主要集中在使用 NVIDIA H200 GPU 的 P5e 实例上，这可能意味着：“英伟达对云服务商提高了价格，这更像是上游成本向下游转嫁，而不是一个全新的定价先例”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于谷歌云平台或微软 Azure 是否会对其 GPU 产品进行类似调整，目前仍无明确消息，但业内普遍认为，这些底层成本压力同样影响着所有的超大规模云厂商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于机器学习团队和 FinOps 从业者而言，这次涨价再次凸显了工作负载优化和成本治理的重要性。首席云架构师 Ivo Pinto &lt;a href=&quot;https://www.linkedin.com/posts/ivopinto01_aws-ec2-capacityblocks-share-7414585484963704832-Htjs&quot;&gt;总结&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在当前 GPU 和内存价格背景下，这并不令人意外。真正重要的是，充分理解你正在使用的服务，以及它背后的定价机制。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，该价格调整已在所有支持 EC2 Capacity Blocks for ML 的亚马逊云科技区域正式生效。亚马逊云科技下一次计划中的定价评估时间为 2026 年 4 月。更详细的价格信息可在 AWS EC2 Capacity Blocks 官方&lt;a href=&quot;https://aws.amazon.com/ec2/capacityblocks/pricing/&quot;&gt;定价页面&lt;/a&gt;&quot;中查看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/01/ec2-ml-capacity-price-hike/&lt;/p&gt;</description><link>https://www.infoq.cn/article/I3cMRWTHlu6kfhjPTySC</link><guid isPermaLink="false">https://www.infoq.cn/article/I3cMRWTHlu6kfhjPTySC</guid><pubDate>Mon, 02 Feb 2026 00:00:00 GMT</pubDate><author>作者：Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>云计算</category></item><item><title>彩讯股份携手稳准智能发布垂直行业数据大模型“数擎”大模型</title><description>&lt;p&gt;彩讯股份携手稳准智能发布垂直行业数据大模型“数擎”大模型1 月 31 日，彩讯科技股份有限公司（简称：彩讯股份）受邀出席雄安新区“人工智能+”创新生态系列活动分会场—通用数据大模型赋能产业发展大会暨极数（LimiX）系列成果展示交流活动，与稳准智能（雄安）科技有限公司（简称：稳准智能）联合发布首个运营商行业数据大模型——“数擎”大模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;彩讯股份CEO白琳、董事会秘书兼财务总监王欣、数行事业部总经理朱彩霞与生态伙伴稳准智能首席科学家崔鹏、CTO张兴璇、COO何玥共同参与发布仪式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2fb2e00633b0a17eeede15495c65f792.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（从左至右依次是：稳准智能COO何玥，稳准智能CTO张兴璇，稳准智能首席科学家崔鹏，彩讯股份CEO白琳，彩讯股份董事会秘书兼财务总监王欣，彩讯股份数行事业部总经理朱彩霞）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于对行业共性问题的长期观察与实践，彩讯股份依托在 ToB 领域二十余年的行业经验积累，并以自主研发的 Rich AIBox 企业级平台化能力为核心，构建起覆盖多业务场景的全栈 AI 能力；在此基础上，结合稳准智能在结构化数据大模型（LDM）领域的技术优势，双方共同启动运营商领域专属大模型的联合研发，并推动结构化数据智能在运营商场景中的系统级落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次发布的“数擎”大模型被认为是业界首个围绕运营商核心业务流程打造的行业专属大模型。该模型构建覆盖数据建模、业务理解与智能决策的业务智能中枢，具备轻量、高效、可解释等特点，能够在资源受限、适用于多类对实时决策和规模化应用要求较高的行业场景。在此过程中，稳准智能在结构化数据建模与因果分析方面的技术能力作为关键能力模块被引入，用于增强模型在复杂业务场景下的推理与决策支撑能力；彩讯股份则依托长期服务运营商核心系统的工程化经验，承担模型能力与业务流程及既有系统的集成与落地实施工作，保障智能能力在真实生产环境中的稳定、可靠运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，该模型已在运营商行业的精准营销、经营分析、终端服务等多个核心场景中成功应用，推动相关业务从“经验驱动”向“数据智能驱动”转变。例如，在营销与预测场景中，模型能够基于多维业务数据进行关联分析与策略推演，不仅提升了预测准确率，还支持业务从“事后评估”向“事前干预”转变，提升运营决策的精准性与前瞻性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面向下一阶段发展，双方将进一步围绕“更深”与“更广”两大方向推进模型演进：一方面，持续增强模型对业务逻辑和策略影响的理解能力，提升智能决策对实际业务结果的支撑效果；另一方面，通过彩讯股份 Rich AIBox 等企业级平台化能力，推动结构化与非结构化数据的协同分析，探索更多行业场景中的规模化复制与应用落地。&lt;/p&gt;</description><link>https://www.infoq.cn/article/gbW5LmM3U3dXoQhTXY31</link><guid isPermaLink="false">https://www.infoq.cn/article/gbW5LmM3U3dXoQhTXY31</guid><pubDate>Sun, 01 Feb 2026 09:19:09 GMT</pubDate><author>InfoQ</author><category>AI&amp;大模型</category></item><item><title>打造让员工蓬勃发展的软件组织</title><description>&lt;p&gt;Matthew Card在&lt;a href=&quot;https://qconlondon.com/&quot;&gt;Qcon&lt;/a&gt;&quot;伦敦关于&lt;a href=&quot;https://www.infoq.com/presentations/inclusive-leadership/&quot;&gt;包容性领导力&lt;/a&gt;&quot;的演讲中提到，持续学习、适应能力和强大的支持网络是团队蓬勃发展的基础。信任是通过始终如一的、公平的领导力和及早处理有害行为、偏见和微侵犯来建立的。通过培养成长、心理安全和责任感，以人为本的领导力推动团队的韧性、合作和绩效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card表示，打造一个能让员工真正蓬勃发展的组织始于有意为之。这不仅仅是组建一个拥有合适技术技能的团队，更是要营造一个让员工感到安全、受到重视并能充分发挥自身能力的环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card解释说，鼓励持续学习是关键所在：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;人们在成长时会蓬勃发展，不仅是在技术领域，还包括像韧性和适应性这样的软技能。我经常在一对一的交流和评审中通过帮助团队成员设定个人发展目标来支持这一点，这些目标加强了这些能力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card说，在我们这种快速变化的行业中，培养灵活性和适应能力至关重要。蓬勃发展并不总是意味着在压力下蓬勃发展。这意味着要为人们创造适应、试验和恢复的空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card认为，建立支持网络和员工社区至关重要。这些为人们提供了交流、反思和恢复精力的渠道，并有助于强化关怀和协作的文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card表示，必须处理有害行为，理想情况下应立即处理。忽视它会发出错误的信号。如果已经确立了明确的行为标准，并且团队也知道你是公平、一致、以价值为导向的，那么处理起来就会容易得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card解释说，处理有害行为没有一劳永逸的方法，这取决于其严重程度：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于不太严重的情况，我将其视为成长的机会。你可以将其视为学习的机会，并引导人们进行纠正。花时间深思熟虑地处理这些情况可能在短期内需要更长的时间，但从长远来看，对团队文化和信任的影响是值得的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;信任是通过日常的小互动逐渐建立起来的。当人们知道在困难时刻彼此的期望和如何互动时，信任就形成了，Card说道。一旦信任根深蒂固，团队就更有可能冒险，敢于让自己犯错，并快速失败，而神奇的事情往往就在此时发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card表示，你需要积极应对偏见和微侵犯行为。如果放任不管，它们会悄然侵蚀信任和归属感。积极主动、公平且始终如一地处理这些行为，能向整个组织清晰地传达你的价值观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Card总结道，这一切的核心在于相信以人为本的领导力就是绩效领导力。当我们花时间打造包容、有韧性的文化时，成功就会随之而来，不仅对业务有利，对组织内的每个人都有益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 采访了&lt;a href=&quot;https://www.linkedin.com/in/matthew-card-93027914/&quot;&gt;Matthew Card&lt;/a&gt;&quot;，探讨了心理安全、信任和韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：心理安全和信任在塑造企业文化方面发挥着怎样的作用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Matthew Card：在我看来，心理安全感是信任环境的更高层次。两者都是任何健康、高效文化的基石。没有它们，人们就会有所保留；他们不太可能分享想法、承认错误或挑战现状。这意味着你的团队不会成长、创新或建立牢固的关系。&amp;nbsp;如果你想建立一个持久的文化，让人们蓬勃发展，而不仅仅是生存，那么建立信任和安全就不是可有可无的。这必须是有意为之。一旦建立起来，它就会解锁其他一切：合作、韧性、问责和成长。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你在组织中采取了哪些措施来增强韧性？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Card：我倡导它、传授它，而且最重要的是，积极践行它。&amp;nbsp;在一对一交流和绩效评估中，如果合适的话，我会鼓励我的直接下属设定个人发展目标，重点放在软技能上，包括韧性——比如变得更适应变化、培养成长型思维、提高情商以及增强自我意识。情商本身就是一个庞大的话题，还有太多内容值得深入探讨——它常常引发更深层次的交流。这些目标有助于将韧性确立为领导力的核心能力。&amp;nbsp;在团队环境中，我会通过自身行动来展现韧性——无论是做决策的方式，还是应对棘手情况的态度。以身作则能传递出强有力且始终如一的信息。&amp;nbsp;我还做过演讲和主持过工作坊，鼓励自我反思，并剖析韧性的核心要素——如果你还记得 C.A.P.S.，就永远不会忘记它们：自信、适应力、目标感和社交支持。这些活动为人们提供了在各自情境中探索韧性含义的空间。&amp;nbsp;最后，我还协助创建并支持员工社区和网络，为他们提供同伴联系、安全的宣泄渠道和共同学习的机会。这些空间在组织内强化韧性方面发挥着关键作用，确保员工感到被支持和被关注。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/organisations-people-thrive/&quot;&gt;https://www.infoq.com/news/2026/01/organisations-people-thrive/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/bg0pzN0dZQm6HhRMliej</link><guid isPermaLink="false">https://www.infoq.cn/article/bg0pzN0dZQm6HhRMliej</guid><pubDate>Sun, 01 Feb 2026 00:00:00 GMT</pubDate><author>作者：Michael Redlich</author><category>管理/文化</category></item><item><title>硬杠 Meta NLLB！Google 发布 TranslateGemma，机器翻译的“性价比”被卷到了极致</title><description>&lt;p&gt;Google 近日正式发布了&amp;nbsp;&lt;a href=&quot;https://blog.google/innovation-and-ai/technology/developers-tools/translategemma/&quot;&gt;TranslateGemma&lt;/a&gt;&quot;，这是一套基于&amp;nbsp;&lt;a href=&quot;https://deepmind.google/models/gemma/gemma-3/&quot;&gt;Gemma 3&lt;/a&gt;&quot;&amp;nbsp;架构构建的全新开源翻译模型。该系列涵盖了 4B、12B 和 27B 三种参数规模，旨在攻克跨越 55 种语言的机器翻译挑战。这些模型旨在适应多样化的运行环境，涵盖了从移动端、边缘设备到消费级硬件及云端加速器的各类场景。目前，该系列模型已正式开源，供全球开发者与研究人员使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/collections/google/translategemma&quot;&gt;TranslateGemma&lt;/a&gt;&quot;&amp;nbsp;的诞生源于一种高度关注效率的训练工艺，其核心在于将大型商业系统的知识迁移至轻量化模型。Google 采用了一种结合了&lt;a href=&quot;https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29&quot;&gt;监督微调&lt;/a&gt;&quot;与&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;强化学习&lt;/a&gt;&quot;的两阶段训练方案。在监督微调阶段，基础版 Gemma 3 模型在由人工翻译和 Gemini 模型生成的合成数据组成的平行语料库上进行训练。这种混合数据集旨在扩大对各类语种（包括低资源语言）的覆盖范围，同时确保翻译质量的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在强化学习阶段，Google 利用一组自动奖励信号对模型进行了优化。这些信号包括&amp;nbsp;&lt;a href=&quot;https://github.com/google-research/metricx&quot;&gt;MetricX-QE&lt;/a&gt;&quot;&amp;nbsp;和&amp;nbsp;&lt;a href=&quot;https://arxiv.org/abs/2308.07286&quot;&gt;AutoMQM&lt;/a&gt;&quot;&amp;nbsp;等机器翻译评估指标，旨在超越简单的参考译文匹配，从而更精准地捕捉翻译的充分性与流利度。据 Google 称，这种方法显著提升了参数效率。在&amp;nbsp;&lt;a href=&quot;https://llm-stats.com/benchmarks/wmt24++&quot;&gt;WMT24++ 基准测试&lt;/a&gt;&quot;中，12B 规模的 TranslateGemma 所表现出的错误率甚至低于体量更大的 27B Gemma 3 基准模型，而 4B 模型的表现也已逼近 12B 的基准水平。此次评估覆盖了高、中、低资源设置下的 55 种语言。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了核心的基准测试语言外，Google 还针对近 500 种额外的语言对训练了 TranslateGemma。尽管这些扩展语向尚未经过全面评估，但 Google 表示，将它们纳入其中是为了支持社区进行更深入的研究和微调，尤其是针对那些代表性不足的弱势语言。此外，这些模型还继承了 Gemma 3 的多模态能力。在基于&amp;nbsp;&lt;a href=&quot;https://vistra-benchmark.github.io/&quot;&gt;Vistra 基准&lt;/a&gt;&quot;的内部测试中，即便没有进行额外的多模态专项微调，文本翻译能力的提升也直接带动了图像内嵌文本翻译表现的优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据模型大小的不同，其部署场景也各具侧重。4B 模型： 专注于移动端和边缘侧推理，适用于内存和功耗限制较严苛的环境。12B 模型： 旨在普通消费级笔记本电脑上运行，无需专用加速器即可进行本地开发和实验。27B 模型： 专为云端部署设计，可在单块高端 GPU 或 TPU（如 H100 级别加速器）上顺畅运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社区对该模型的发布反应热烈，讨论焦点集中在 Google 宣称的卓越效率以及开源决策上。社交平台上的研究人员和开发者特别关注 12B 模型超越大型基准模型的表现，认为其在成本敏感型部署和设备端翻译应用中极具潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员&amp;nbsp;&lt;a href=&quot;https://x.com/avaisaziz/status/2011899797008237004&quot;&gt;Avais Aziz&lt;/a&gt;&quot;&amp;nbsp;评价道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;TranslateGemma 为世界带来了强大的开源翻译能力，其质量和效率令人印象深刻。很高兴看到 Gemma 3 能够发挥如此深远的全球影响力，干得漂亮！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，用户 Darek Gusto 分享道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;太棒了！像 X（原Twitter）这类平台提供的自动翻译功能，对我们非母语用户意义重大。而开源权重模型正是推动这项功能普及、成为行业标准的关键。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与&amp;nbsp;&lt;a href=&quot;https://ai.meta.com/research/no-language-left-behind/&quot;&gt;Meta 的 NLLB&lt;/a&gt;&quot;&amp;nbsp;系列或针对翻译适配的多语言大语言模型相比，TranslateGemma 更侧重于小尺寸模型下的翻译效率。虽然竞品模型通常强调极广的语种覆盖面或通用能力，但它们往往需要更大的参数量或额外的微调。不同于追求规模的路径，TranslateGemma 优先保障了低计算成本下的高质量翻译，精准切中了成本受限场景与设备端运行的痛点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-translategemma-models/&quot;&gt;https://www.infoq.com/news/2026/01/google-translategemma-models/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/h2pqxbjh27gCsekUWKFT</link><guid isPermaLink="false">https://www.infoq.cn/article/h2pqxbjh27gCsekUWKFT</guid><pubDate>Sat, 31 Jan 2026 04:00:00 GMT</pubDate><author>作者：Daniel Dominguez</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>BigQuery 新功能：SQL 直调 17 万 + AI 模型，3800 万行数据处理成本仅 2 美元</title><description>&lt;p&gt;Google 近期针对&amp;nbsp;&lt;a href=&quot;https://cloud.google.com/bigquery&quot;&gt;BigQuery&lt;/a&gt;&quot;&amp;nbsp;推出了面向开源模型的第三方生成式 AI 推理功能。这一更新允许数据团队直接使用简单的 SQL 语句，部署并运行来自&amp;nbsp;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt;&quot;&amp;nbsp;或&amp;nbsp;&lt;a href=&quot;https://cloud.google.com/model-garden&quot;&gt;Vertex AI Model Garden&lt;/a&gt;&quot;&amp;nbsp;的任何模型。该接口目前处于预览阶段，其最大的亮点在于消除了对独立机器学习（ML）基础设施的需求，系统会自动启动计算资源、管理端点，并在任务完成后通过&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/introduction-sql&quot;&gt;BigQuery 的 SQL&lt;/a&gt;&quot;&amp;nbsp;接口自动清理资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这项新功能解决了困扰数据团队已久的痛点。在过去，运行开源模型往往意味着需要管理 Kubernetes 集群、配置端点以及在多种工具之间反复切换。Virinchi T 在一篇关于此次发布的 Medium&amp;nbsp;&lt;a href=&quot;https://medium.com/google-cloud/bigquerys-managed-inference-for-open-models-your-warehouse-is-now-an-ai-engine-d83fbb6eccd1&quot;&gt;文章&lt;/a&gt;&quot;中指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这一过程需要多种工具协同、不同的技能储备以及巨大的运维开销。对于许多数据团队来说，这种摩擦意味着即便模型本身是免费且公开的，AI 能力依然显得遥不可及。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，得益于 BigQuery 的 SQL 接口，整个工作流现在被简化为仅需两条 SQL 语句。用户首先通过一条&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-remote-model-open#create_model_syntax&quot;&gt;CREATE MODEL 语句&lt;/a&gt;&quot;来创建模型，只需指定 Hugging Face 的模型 ID（例如 sentence-transformers/all-MiniLM-L6-v2）或 Vertex AI Model Garden 中的模型名称。BigQuery 会根据默认配置自动分配计算资源，部署过程通常在 3 到 10 分钟内即可完成，具体时长取决于模型大小。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;部署完成后，用户可以使用&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-text&quot;&gt;AI.GENERATE_TEXT&lt;/a&gt;&quot;（针对语言模型）或&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-ai-generate-embedding&quot;&gt;AI.GENERATE_EMBEDDING&lt;/a&gt;&quot;（针对嵌入模型）直接对 BigQuery 表中的数据进行推理查询。平台通过 endpoint_idle_ttl 选项管理资源的生命周期，该功能会自动关闭闲置端点以节省费用。此外，在批处理任务结束后，用户还可以通过 ALTER MODEL 语句手动卸载端点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了满足生产环境的需求，该功能还支持高度定制化。用户可以直接在 CREATE MODEL 语句中设定机器类型、副本数量以及端点闲置时间。通过 Compute Engine 预留功能，还可以锁定 GPU 实例以确保性能稳定。当不再需要某个模型时，只需执行一条简单的&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-drop-model&quot;&gt;DROP MODEL 语句&lt;/a&gt;&quot;，系统便会自动清理所有关联的 Vertex AI 资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google 在官方博客中将该系统描述为提供“精细的资源控制”和“自动化的资源管理”，旨在让团队在不脱离 SQL 环境的情况下，找到性能与成本之间的最佳平衡点。2025 年 9 月发布的一篇&lt;a href=&quot;https://cloud.google.com/blog/products/data-analytics/use-gemini-and-open-source-text-embedding-models-in-bigquery&quot;&gt;早期博客&lt;/a&gt;&quot;曾展示，利用类似的开源嵌入模型处理 3800 万行数据，成本仅需约 2 到 3 美元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，该功能已支持超过 1.3 万个 Hugging Face 文本嵌入模型和超过 17 万个文本生成模型，涵盖了&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Llama_%28language_model%29&quot;&gt;Meta 的 Llama 系列&lt;/a&gt;&quot;和&amp;nbsp;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core&quot;&gt;Google 的 Gemma 家族&lt;/a&gt;&quot;。需要注意的是，所选模型必须符合 Vertex AI Model Garden 的部署要求，包括区域可用性和配额限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Virinchi T 强调了这一变革对不同角色的意义：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于数据分析师而言，你现在可以无需离开 SQL 环境，也不必等待工程资源支持，就能直接实验 ML 模型。对于数据工程师而言，构建由机器学习驱动的数据管道变得极其简单，再也不用维护独立的 ML 基础设施。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次发布标志着 BigQuery 将与&amp;nbsp;&lt;a href=&quot;https://select.dev/posts/snowflake-cortex-ai-sql-overview-and-cost-monitoring&quot;&gt;Snowflake 的 Cortex AI&lt;/a&gt;&quot;&amp;nbsp;以及&amp;nbsp;&lt;a href=&quot;https://www.databricks.com/product/model-serving&quot;&gt;Databricks 的 Model Serving&lt;/a&gt;&quot;&amp;nbsp;展开直接竞争，后两者同样提供基于 SQL 的 ML 推理能力。而 BigQuery 的竞争优势可能在于其与 Hugging Face 庞大模型库在数据仓库内的深度集成，这对于已经在 Google Cloud 上运行业务的用户具有极强的吸引力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，关于&amp;nbsp;&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/generate-text-tutorial-gemma&quot;&gt;Gemma 模型&lt;/a&gt;&quot;的文本生成以及&lt;a href=&quot;https://docs.cloud.google.com/bigquery/docs/generate-text-embedding-tutorial-open-models&quot;&gt;嵌入生成&lt;/a&gt;&quot;的相关文档和教程已正式上线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/bigquery-sql-huggingface-managed/&quot;&gt;https://www.infoq.com/news/2026/01/bigquery-sql-huggingface-managed/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/V52Rsbxl71Ampa74ottq</link><guid isPermaLink="false">https://www.infoq.cn/article/V52Rsbxl71Ampa74ottq</guid><pubDate>Sat, 31 Jan 2026 03:00:00 GMT</pubDate><author>作者：Steef-Jan Wiggers</author><category>Google</category><category>数据库</category></item><item><title>为什么你的系统一出事就“查不清”？Railway 给出可观测性的标准答案</title><description>&lt;p&gt;&lt;a href=&quot;https://railway.com/&quot;&gt;Railway&lt;/a&gt;&quot;&amp;nbsp;的工程团队近日发布了一篇关于&lt;a href=&quot;https://blog.railway.com/p/using-logs-metrics-traces-and-alerts-to-understand-system-failures&quot;&gt;可观测性的系统性指南&lt;/a&gt;&quot;，详细讲解了开发者和 SRE 团队如何协同使用日志（logs）、指标（metrics）、追踪（traces）和告警（alerts），以理解并诊断生产环境中的系统故障。这篇文章主要面向现代分布式系统的使用者，梳理了各类遥测信号的实用定义、优势与局限，并强调将它们组合使用，能够显著提升根因分析的速度和准确性。尽管文中信息并非全新观点，但仍为团队理解可观测性领域提供了有价值的参考视角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章指出，可观测性并不等同于传统监控。与仅基于预设阈值进行响应的监控不同，可观测性强调工程师能够在问题尚不明确时，实时探索系统内部状态。Railway 将可观测性拆解为四大核心支柱：日志用于提供事件级别的详细上下文；指标用于反映系统整体健康状况；追踪用于描绘请求在分布式架构中的完整路径；告警则作为&lt;a href=&quot;https://www.ibm.com/think/topics/service-level-objective&quot;&gt;服务级别目标&lt;/a&gt;&quot;（SLO）的早期预警机制。当一次告警能够关联到指标的异常波动、追踪中暴露的性能瓶颈，以及日志中记录的具体错误时，团队就能迅速还原故障背后的完整链路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在具体定义上，Railway 将日志描述为带有时间戳的离散事件记录，能够提供单个事件的完整上下文，适用于调试、审计以及合规场景。指标则是高效的数值信号，支撑仪表盘展示、趋势分析和告警触发，但其缺点是缺乏细粒度上下文。追踪可以捕捉一次请求在多个服务之间的完整流转过程，帮助定位延迟问题或依赖瓶颈；告警则是主动通知机制，用于暴露异常行为或 SLO 违规情况。文章同时指出，每一种支柱都存在盲区，例如指标缺乏细节、日志不擅长实时趋势识别，但当它们被组合使用时，便能构建出一套完整的可观测性工具体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Railway 还分享了多项落地实践建议，包括：使用结构化日志，并通过关联 ID 或&amp;nbsp;&lt;a href=&quot;https://last9.io/blog/correlation-id-vs-trace-id/&quot;&gt;trace ID&lt;/a&gt;&quot;&amp;nbsp;将日志与追踪数据打通；定义有意义的指标，并关注分位值（如&amp;nbsp;&lt;a href=&quot;https://oneuptime.com/blog/post/2025-09-15-p50-vs-p95-vs-p99-latency-percentiles/view&quot;&gt;p95、p99&lt;/a&gt;&quot;），以更真实地反映用户体验；以及构建以用户影响为导向的告警阈值，而非基于过于底层的信号。告警还应按照严重程度进行路由，并与运行手册（runbook）绑定，帮助值班工程师在不被告警噪音淹没的情况下，高效完成响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比单体应用，分布式系统的复杂度和不透明性显著提升，传统监控手段在系统故障发生时往往难以还原完整事实。Railway 的这份指南再次强调了多模态可观测性的重要性，这一理念也与当前主流的 SRE 最佳实践高度一致，能够显著提升开发者对故障的预判、发现与诊断能力，从而减少停机时间并提升系统可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际工程经验中，&lt;a href=&quot;https://www.reddit.com/r/Observability/comments/1qj14am/whats_your_strategy_for_correlating_logs_metrics&quot;&gt;Reddit&lt;/a&gt;&quot;&amp;nbsp;上的不少工程师也指出，相比单纯收集大量遥测数据，打通不同信号之间的上下文关联往往更有价值。例如，通过共享标识符和集中化工具，将指标、日志和追踪串联起来，可以让工程师从一次指标告警快速跳转到相关日志和追踪数据，避免在不同系统间来回切换、浪费时间。这种以“上下文连通”为核心的模式，正逐渐成为可观测性工作流中的主流做法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总体来看，Railway 的这篇文章为可观测性提供了一套清晰且实用的框架，有助于其他团队提升对系统故障的理解和处置能力，推动工程实践从被动“救火”转向更加主动的可靠性工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/railway-diagnosing-failure/&quot;&gt;https://www.infoq.com/news/2026/01/railway-diagnosing-failure/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/9QaYwTAYLedFhScRPsOp</link><guid isPermaLink="false">https://www.infoq.cn/article/9QaYwTAYLedFhScRPsOp</guid><pubDate>Sat, 31 Jan 2026 02:00:00 GMT</pubDate><author>作者：Craig Risi</author><category>可观测</category></item><item><title>Swift跨平台框架Skip现已完全开源</title><description>&lt;p&gt;Skip是一款通过Swift/SwiftUI代码库创建iOS和Android应用程序的解决方案，经过三年的开发，Skip团队宣布他们决定将&lt;a href=&quot;https://skip.dev/blog/skip-is-free/&quot;&gt;该产品完全开源&lt;/a&gt;&quot;，以促进采用和社区贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，Skip是一个付费解决方案，需要订阅和许可密钥才能创建应用，除非你是独立开发者或开发免费应用。Skip解释说，这种模式有助于在没有外部投资的情况下启动产品，但“事实是，开发者希望免费获得他们的工具”。随着最近决定转向开源，Skip现在与iOS和Android的主要开发工具保持一致，包括Xcode、Android Studio、流行框架和其他基本工具，这些工具都是免费的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但Skip表示，促使他们做出这一决定的不仅仅是成本问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;除了价格，还有一个更深层的担忧是持久性。开发者对于在小公司的付费闭源工具上构建整个应用策略持谨慎态度是可以理解的。如果公司倒闭了怎么办？被收购然后关闭了怎么办？他们的应用程序怎么办？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简而言之，这就是Skip决定开源的原因：即使当前的开发团队消失了，解决方案也会继续存在，保护开发者在其中所做的投资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Skip团队的说法，Android和iOS上UI框架的快速发展，包括Material Expressive和Liquid Glass，造成了使用传统跨平台UI框架可能导致“过时的界面、较弱的用户体验和真正的竞争劣势”的局面。相比之下，Skip能够在两个平台上实现完全原生的用户体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，Skip框架通过将SwiftUI桥接到Jetpack Compose上，将其引入Android。这种方法允许iOS开发者在相同的代码库中编写应用程序的业务逻辑和UI，而无需额外的努力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当Skip还是一个封闭源码的付费产品时，它的一些早期使用者在Reddit上分享了他们的经验。Reddit用户jestecs&lt;a href=&quot;https://www.reddit.com/r/SwiftUI/comments/1nzgdih/comment/ni1zrpb/&quot;&gt;指出&lt;/a&gt;&quot;：“总的来说，使用起来相当愉快，虽然偶尔会遇到一些问题，但总体上令人惊讶地愉快”。此外，JEHonYakuSha&lt;a href=&quot;https://www.reddit.com/r/SwiftUI/comments/1nzgdih/comment/ni26xec/&quot;&gt;进一步阐述&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有些问题是因为某些弃用的构造函数不受支持，因此你可能习惯于用较旧的方式来定义视图修饰符或组件，但一旦你习惯了稍微发挥创意并确认什么是受支持的，它就非常好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;JEHonYakuSha还指出，你可以使用 //SKIP INSERT 将Kotlin代码片段混合到Swift代码库中，并且iOS端只支持Swift包管理器，这可能会使管理内部依赖关系变得有些棘手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Skip的文档中有一个重要的警告，&lt;a href=&quot;https://skip.tools/docs/gettingstarted/#existing_development&quot;&gt;即该框架最适合外部依赖较少的新项目或应用程序&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;将现有的应用程序迁移到Skip并不简单。大多数应用都包含许多仅针对iOS的依赖，这使得移植到Android平台变得非常困难。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Skip三年前开始作为swift到kotlin的转译器，后来增加了对Android上最广泛使用的SwiftUI API的支持。在此期间，他们成立了Swift Android工作组，发布了&lt;a href=&quot;https://www.infoq.com/news/2025/10/swift-sdk-android/&quot;&gt;Swift Android SDK&lt;/a&gt;&quot;，实现了在Android上原生编译Swift代码。今天，Skip支持许多流行的集成框架，与数千个跨平台Swift包互操作，并提供全面的SwiftUI实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://swiftcrossui.dev/&quot;&gt;SwiftCrossUI&lt;/a&gt;&quot;是一个开源的替代方案，它为跨macOS、Linux、Windows的UI提供了类似SwiftUI的API，并对Android提供了一些新生支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Skip可以在&lt;a href=&quot;https://github.com/skiptools&quot;&gt;GitHub&lt;/a&gt;&quot;上克隆，而所有文档、博客和案例研究都转移到了&lt;a href=&quot;https://skip.dev/&quot;&gt;skip.dev&lt;/a&gt;&quot;上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/swift-skip-open-sourced/&quot;&gt;https://www.infoq.com/news/2026/01/swift-skip-open-sourced/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TflNBc7EvNHoKCyDEdj3</link><guid isPermaLink="false">https://www.infoq.cn/article/TflNBc7EvNHoKCyDEdj3</guid><pubDate>Sat, 31 Jan 2026 01:00:00 GMT</pubDate><author>Sergio De Simone</author><category>Android/iOS</category></item><item><title>摩擦解决之道：改变关键要素</title><description>&lt;p&gt;当Andrew Harmel-Law邀请演讲者参加QCon伦敦大会时，他首先想到的是：“让Diana和Cat一起登台！”这完全是出于一种顽皮的冲动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cat Morris是一位产品经理。Diana Montalion是一位系统架构师。我们都专门从事软件系统和平台开发。我们都把对方视为我们所有痛苦的根源。不是个人层面的，我们从未一起工作过，但我们都认为对方所代表的角色是问题所在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们也致力于改善我们的工作方式。如果我们能够消除我们之间存在的摩擦……那对每个人来说都是一件好事，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，我们对参与的每一个重大项目做了建模分析。我们发现，每一个项目的结局都像泰坦尼克号一样：撞上了冰山。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们发现了以下这些改变职业生涯的真相：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的工作有很多重合的地方。我们需要彼此。合作会带来更好的结果。我们思考问题的方式不同但互补。我们要解决的不是技术问题。技术挑战不难解决。困难之处在于消除摩擦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;摩擦是阻碍变革的无形暗流。摩擦并非孤立存在的，而是一个系统性问题，其来源是人、团队与技术之间的关系。解决之道不在于Kubernetes、云计算或人工智能，而在于改变我们的思维模式、沟通方式和组织架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文描述了在每一个项目中都会反复出现的六种摩擦：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反直觉行为拒绝改变效率与控制产品与技术角色之间相互指责线性管道思维交付能力不足&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每一种摩擦都让我们忙于重写相同的代码库或重绘相同的组织结构图。摩擦导致很多无益于我们避开冰山的工作浪费。这篇文章将帮助你弄清楚如何应对这些问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;反直觉行为&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们怪错了对象。因此，我们做错了事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反直觉行为，由系统科学家Jay Forrester定义，是指复杂的社会和管理系统倾向于用让问题变得更糟糕的解决方案来应对问题。例如，为一个处于后期的项目增派更多的工程师，加快项目进度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对系统性挑战时，我们人类会倾向于指责他人。遗憾的是，我们怪错了对象。我们责怪工程团队工作效率低下，或者断定是因为团队太小，却未能意识到甘特图不过是虚构的简化模型，它过度简化了复杂的动态过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“任何系统的输出结果都是其内在设计的必然产物‌。”——W. Edwards Deming&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当我们改变一个系统的运作方式时，我们也需要改变塑造系统的底层模式、结构和心智模型。例如，不是在本已臃肿的数据库上放上GraphQL就能称之为图，而是需要重新构建我们思考数据的方式。我们需要创建异步事件，而不是把什么都塞进管道，我们需要创建反馈循环，让我们能及时知道数据在跨系统移动时出现了异常。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这可以解释为什么工程师们会工作到很晚。他们一边忙着给猪涂口红，一边又要完成甘特图上没有预料到的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：理解系统&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反直觉行为具有引力，会将我们吸入其中，卷入漩涡。至少，我们的感受是这样：感觉像是在行动，在前进，而实际情况是，我们就像烘干机里的衣物，在原地打转。自始至终，我们都是从同一个狭小的视角凝视问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方法是暂时停下来，找准方向再行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是确定核心领域。系统的目标是什么？对于联邦快递来说，是快速递送包裹。很可能，你所经历的反直觉行为，源于人们的说法一致但前进方向不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个参与的人都清楚变革与核心领域的关系吗？怎么才能加快包裹递送而不产生负面影响？系统本身如何导致了问题的产生？当前的流程和模式会产生什么样的心态？系统中的关系如何强化了这些模式？有什么你没有看到的变革需求吗？开始绘制这些模式，确定你的核心领域，所需的变革自然便会显现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;拒绝改变&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个试图变革的组织都有这样一些人：守门人、地牢主、自封的10倍速工程师，他们对组织非常了解，并且拥有一个魔法词：不。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们说，“我们已经尝试过迁移。我们已经尝试过平台迁移或重构。我们已经尝试过重组。都行不通。别费心了。”他们掌握着生产环境或管道或某个关键系统的控制权，你没法忽视他们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，人们很容易把责任归咎于那人的固执性格。但他展现的行为模式，正是长期的奖励与强化所形成的。通过管理遗留系统的复杂性，他已成为不可或缺的存在。要改变他，必须改变整个组织的思维方式，否则，又会冒出另一个类似的角色取而代之。仅靠技术手段无法从根本上解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拒绝改变是会传染的。当那个人关闭了好奇心，其他人也会转向固定思维模式。人们首先做的是怀疑而不是实验。组织无法在规避风险和尝试新事物之间找到平衡。变革陷入了停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“变革之所以困难，是因为人们高估了他们所拥有的东西的价值，低估了他们放弃这些东西可能获得的价值。” ——《&lt;a href=&quot;https://www.goodreads.com/book/show/1434394&quot;&gt;会飞的水牛：卓越腾飞，学会让员工引领&lt;/a&gt;&quot;》（Belasco和Stayer，1999年）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：解雇那个人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好吧，这可能有点极端。你可能无法解雇他们，但你可以限制他们的影响。你不是他们的导师、母亲或心理医生。不要陷入那种角色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把他们装进盒子里。给他们一个静态领域，让他们可以保留自己的脚本和遗留代码，而你去改变系统的其余部分。用“是”的力量去平衡他们“不”的力量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;是的，这并不意味着认同所有观点。这意味着探索可能性。将对话引向可以向前推进的方向。找到愿意做出改变的人，并帮助他们解决自己的问题，而不是等待守门人为他们开锁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;效率与控制&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;效率带来安全感。控制带来责任感。二者结合，便形成了现代组织中最诱人的反模式：效率与控制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当它们占据主导地位时，我们关注的焦点便会缩小到产出：发布特性、关闭工单、满足截止日期。工作以速度来衡量，而不是价值。组织最小化互动和反思，坚信速度等于进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在复杂的系统中，只讲效率会使系统变得脆弱。你按时交付了错误的东西。你得到了与蓝图相匹配的结果，但忽略了变化的环境。你得到了短期的解决方案，而那最终会成为未来的障碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们制定了一个有预算有时间表的路线图，而唯一的上下文是一堆令人困惑的架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要求我们增加控制，但又理不清头绪。效率不过是掩盖混乱的遮羞布。其结果是一场组织作秀：Jira工单、自信满满的时间表，以及追踪一切却唯独忽略意义的指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;截止日期不可更改。功能已锁定。缺的是什么呢？一个明确的目标和协同一致的成果，同时又留有空间，允许在学习过程中进行调整。包裹配送时间没有改善，客户体验也未见提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：设计知识流&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让团队注重效能而非过度追求效率。如何确保时间、精力和注意力都用在塑造真正重要的成果上？用协同取代控制：建立能让正确信息在正确时间传递给正确人员的工作机制，让信息跨越边界自由流动，无需高管做微观管理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;思考你目前正在开展的项目。你是在度量产出还是成果？截止日期是反映了现实还是一厢情愿的想法？如何改善信息流，而不是构建一个工厂式的交付过程？做工作与交付价值不是一回事；改善知识流能让你更快地获得价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;产品 vs. 技术&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Andrew让我们聚在一起，因为我们有类似的挫败感。我们都面临着用Kubernetes解决系统性问题的压力。当然，我们做不到。我们也因为反直觉行为，养成了互相指责对方角色的习惯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工程团队不喜欢产品经理给他们施加压力，要求他们在一个紧迫的时间表内完成特性交付。他们不明白，他们正在做的改变怎么能提高代码质量。基础设施、架构和DevSecOps人员则抱怨说，他们需要的改进从未被优先考虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品团队不喜欢工程师表现出的抵触和消极态度。他们不明白，为什么一些简单的事情需要那么长时间来构建。他们抱怨说，过度工程化浪费了大量的会议时间，而那些时间本来可以用在寻找前进的方法上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在许多组织中，产品和工程是相互隔离的孤岛。双方对效率与控制的定义截然不同，成了两股相互对立的势力。产品部门只需将需求抛过隔离墙，便坐等可运行的代码返回；而工程部门则被要求加快编码速度，无暇顾及系统设计。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即使这两个角色坐在一起，他们相互冲突的方法和控制欲也会加剧摩擦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：向学习驱动型转变&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Carol Dweck博士描述了两种心态：&lt;a href=&quot;https://www.littlebrown.co.uk/titles/carol-dweck/mindset-updated-edition/9781472139955/&quot;&gt;固定型和成长型&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;固定型心态认为，擅长某事就意味着那件事应该很容易做到。如果某事很难，则说明你缺乏天赋。在这种模式下，人们会回避挑战，将反馈视为批评，将他人的成功视为威胁。其结果是团队很脆弱，他们要保护自己的地盘，而不是一起学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成长型心态认为，能力可以通过实践、反馈和坚持来提升。你不能只是做Netflix所做的事。在这种模式下，人们欢迎挑战，将努力视为精通之道，将挫折当作改进的数据基础。其结果是个人和团队都更有韧性，他们会适应而不是抵触。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当每个人，无论什么角色，都采用成长型思维时，他们就会一起学习。同理心是一项关键能力。它不同于单纯的同情，而是从多个角度理解情境的能力——对用户和代码的影响；产品团队与技术团队之间的流程和协作模式所导致的顽固问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了共同做出有影响力的决策，我们需要成长型思维，借鉴他人的经验来丰富我们的世界观，并引导自己获得更深刻的洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;线性管道思维&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还记得过去软件中某个地方有Bug的日子吗？修复一行代码就能解决问题。现在，在微服务和分布式系统构成的复杂世界中，Bug通常存在于各部分之间的关联关系中。系统架构的艺术和科学在于设计和编排。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每天，我们都会经历摩擦，因为我们在一个异步世界中交付线性管道。或许我们称其为平台，但真正的平台是精心设计的不同技术与流程之间的关联体系。它们能用不同的方式响应不同时间发生的各类事件。平台的各部分相互协同，达成单个组件无法独立实现的成果。这种现象被称为涌现（emergence）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面向涌现的设计需要综合多方视角，理解如何调整整个系统以实现包裹快速配送，而又不破坏异步关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;综合意味着不能单打独斗。你需要其他人贡献他们的专业知识，并将这些专业知识与你的知识整合在一起。当我们无法构建能整合组织知识与经验的实践体系时……我们就无法设计（或调试）平台。于是我们交付了一条管道，而所有人都将它的缺陷归咎于不充分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：构建关系&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与其用线性思维处理异步情境，不如退一步思考以下问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在你的系统中，关系如何产生效果？系统各部分之间如何通过关系实现它们不直接做的事情？信息如何在技术系统中流动？痛点和瓶颈在哪里，以及如何优化关系以消除这些阻碍？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你的组织不支持这项工作，就组建一个秘密小组。无论身处何种角色，都要将多元视角融入建议的更改方案中。确保信息在人员之间顺畅流动，从而实现服务之间的无缝衔接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当具有不同观点的团队（如架构师Diana和产品经理Cat）共同构建这些模式时，一切都会变得更加轻松。与其他团队交流，观察用户使用系统的过程，向基础设施团队了解他们的痛点，并将这些洞察融入解决方案之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;交付能力不足&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们以为我们专注于目标，而实际上我们是在专注于需求。目标是一个可衡量的成果，比如，提高包裹递送速度。如果我们提高了系统的能力，我们就实现了目标，而不是简单地添加另外一项功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于内部平台团队而言，一个可能的例子是：团队被告知需将软件部署管道从Jenkins迁移至CircleCI。这明确告知了操作步骤，却未说明迁移背后的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里的隐藏目标是将开发者的管道维护工作量减少百分之五十。没有这些信息，团队可能会迁移了软件交付管道却没有达到预期的结果。他们也可能有一些想法，不需要昂贵的管道迁移就能实现目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部署变更的能力早就已经存在。在这种情况下，交付流程会频繁出现各种断层。对于如何部署变更，每个人都有不同的想法。如果关注点是改变工具，那么这些想法将导致无尽的摩擦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决之道：专注于目标&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;制定有意义的目标，并整合实现目标的各种方案。描述需要交付的具体且可衡量的效益或成果。确保目标紧密契合核心领域，即加速包裹配送。系统的所有功能都应该服务于这个目标。每次部署变更时，我们（期望）都在提升系统履行职责的能力。聚焦目标能确保每位贡献者都理解自身工作的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;去尝试&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，你可能感到不知所措。“我做不到这一切！我的工作已经很忙了！”这是真的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;幸运的是，你不需要全都做。选一件事来做就行。一个可以减少摩擦的小变革，可以是你最感兴趣的事情，也可以是给你的日常生活造成最大摩擦的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;系统就是这样工作的……小变革可以扩展为大改进。摩擦在细节中，解决之道也在细节中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/friction-fix/&quot;&gt;https://www.infoq.com/articles/friction-fix/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ah0Xq4eGpFGrOEJOdfNX</link><guid isPermaLink="false">https://www.infoq.cn/article/ah0Xq4eGpFGrOEJOdfNX</guid><pubDate>Sat, 31 Jan 2026 00:00:00 GMT</pubDate><author>作者：Cat Morris, Diana Montalion</author><category>管理/文化</category></item><item><title>从三大支柱出发：Snowflake 平台的一次系统级升级</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场以 What’s New for Snowflake Platform 为主题的技术发布中，Snowflake 产品管理高级总监 Artin Avanes，与产品管理团队成员 Christine 和 Raja Balakrishnan 一同，系统性地回顾并发布了 Snowflake 平台在过去一段时间内的重要进展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同于围绕单点功能的更新介绍，这场分享从一开始就明确了一个整体视角：Snowflake 正围绕 简洁性（Simplicity）、互联平台（Connected） 和 可信平台（Trusted） 三个关键支柱，持续重塑其作为数据与 AI 基础平台的能力边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;简洁性：把能用变成规模化可用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Christine 在分享中重点展开了 Snowflake 的易用性支柱。她反复强调一个核心判断：真正的易用，并不是功能更少，而是在规模扩大之后依然可控、可理解、可管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake 仍然坚持单一产品、单一引擎的平台形态，覆盖分析型、混合型以及事务型工作负载，并以全托管的方式承担大部分运维复杂度。在过去 12 个月中，Snowflake 针对核心分析型工作负载实现了 两倍性能提升，且这一优化由平台自动完成，而非依赖用户侧调优。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着越来越多企业在一个组织内拥有大量 Snowflake 账户和对象，组织级管理能力 成为此次更新的重点之一。Snowflake 正式推出组织账户（Organization Account），作为统一的全局管理入口；同时，通过组织级视图聚合各账户元数据，使使用情况、对象分布与成本消耗在组织层面变得可见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Snowflake 进一步引入 组织用户与用户组 的管理模式，允许用户只在组织层定义一次，便可被授权至多个账户，避免重复配置。这一能力被视为大规模 Snowflake 部署的关键基础设施，目前已进入即将 GA 的阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从可扩展到可运营：SPCS 的持续演进&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕 Snowpark Container Services（SPCS），Christine 也披露了一系列面向运营友好型的增强。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SPCS 的目标并非只是让用户把自定义应用带到 Snowflake 平台，而是在 Snowflake 的安全边界内，尽可能降低运行和维护这些应用的成本与复杂度。新引入的自动扩缩容、增强版自动扩缩容以及即将上线的自动暂停能力，使服务能够根据负载峰谷动态调整，避免资源闲置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，SPCS 在 Snowsight 中获得了更完整的可视化体验。开发者可以直接在 UI 中创建服务、执行作业，并查看历史日志、指标与平台事件，这些能力为应用与数据管道提供了内建的可观测性基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在性能层面，SPCS 即将支持 阶段挂载（Stage Mounts），为内部阶段提供更快速、稳定的文件访问能力，直接服务于 AI/ML 数据加载和管道吞吐需求。同时，块存储层新增的端到端加密能力，在不修改应用代码的前提下，增强了整体安全性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;互联平台：让数据真正跨系统流动&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在互联这一支柱下，Artin 将重点放在 跨云互操作、数据共享与协作能力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，OpenFlow 作为托管体验已正式 GA，使来自异构数据系统的数据更容易被引入 Snowflake。其次，Snowflake 宣布与 SAP 的双向集成能力，以及 Oracle CDC 即将进入公开预览，进一步拓展了平台在企业数据整合场景中的覆盖面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在协作层面，Snowflake 对开放表格式的支持持续加深。用户现在不仅可以共享 Apache Iceberg 和 Delta Lake 表，还能够共享语义视图，用于支持更准确的 AI 和 BI 应用。同时，笔记本、用户自定义函数等对象也可以通过 Snowflake 原生应用框架进行打包与分发，使构建和交付数据与 AI 产品的路径更加完整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;可信平台：为 AI 应用补上信任这一层&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Raja Balakrishnan 的分享，集中在 Snowflake 平台的可信性升级上。他将 Horizon Catalog 定位为一个核心枢纽：既是开放表格式互操作的目录，也是可扩展治理与 AI 数据上下文的载体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过嵌入 Iceberg Open API 和 Apache Polaris API，Horizon Catalog 支持外部引擎直接读写 Snowflake 管理的 Iceberg 表，并在 Snowflake 内部展示来自外部数据源的血缘关系。在治理能力上，平台新增了多项目录功能，包括账户级 PII 自动检测、数据剖析与质量监控、非结构化数据中的 PII 识别，以及用于备份的数据快照能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Trust Center 中，数据安全能力被进一步整合。PII 检测正式进入熟悉的安全管理界面，同时支持异常访问告警和组织级安全态势可视化。安全扩展也可以通过市场形式被合作伙伴提供。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用 AI 治理 AI&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示环节，Raja 重点展示了一个新的 AI SQL 函数 AI Redact。该函数能够自动检测并编辑非结构化文本中的敏感信息，并允许用户精细控制哪些字段被视为 PII。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过一个客服通话记录的示例，他演示了如何在不暴露任何敏感信息的前提下，对文本进行情感分析：先对原始文本进行 PII 编辑，再将清洗后的数据输入 AI 分析函数。整个过程无需复杂流程，仅通过 SQL 即可完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，Snowflake 在 Snowsight 中引入了全新的数据质量界面。系统可自动生成数据剖析结果，并在 AI 辅助下帮助用户快速配置质量监控规则。例如，在 Customer ID 列被识别为潜在主键后，平台会自动建议唯一性约束，并展示其推理逻辑，确保Human-in-the-loop。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享的最后，Artin 提到，随着平台能力的不断扩展，客户越来越关心如何用得更好。为此，Snowflake 正式推出 Well-Architected Framework，希望将多年积累的实践经验沉淀为一套可参考的方法论，覆盖从安全治理到成本优化等多个关键维度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/4BOafAxF30X1K8AQd8qM</link><guid isPermaLink="false">https://www.infoq.cn/article/4BOafAxF30X1K8AQd8qM</guid><pubDate>Fri, 30 Jan 2026 11:44:26 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>如何用 dbt MCP 服务器和 Snowflake 构建智能体工作流</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Build 2025 的这场技术分享中，演讲者围绕 “如何构建真正可执行、可扩展的 Agentic Workflow” 展开了一次非常具体的实践讲解。不同于泛泛而谈 Agent 或自动化愿景，这次分享聚焦在一个明确的问题上：当大模型开始介入数据分析与工程流程时，如何让它们安全、可控、并且真正融入现有的数据工作流之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本场分享由 Snowflake 与 dbt 生态的实践者——dbt Labs 的技术产品营销经理 Sarah Gawlinski，以及 dbt Labs 开发者体验和人工智能的高级经理 Jason Ganz 共同完成，核心案例是通过 dbt MCP Server 作为中介能力，让 Agent 能够理解、调用并执行 dbt 项目中的真实数据资产与逻辑，并最终运行在 Snowflake 之上。整场内容并不追求概念上的先进性，而是反复强调工程现实与可操作性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“能问答”到“能行动”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享一开始，演讲者明确区分了两类常被混为一谈的 Agent 使用方式：一类是问答式 Agent，能够回答问题、生成文本；另一类是行动型 Agent，可以在理解上下文的基础上，执行一系列真实的系统操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据领域，真正有价值的 Agent 显然属于后者。但问题也随之而来：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 要“行动”，就必须接触到真实的数据模型、表结构、血缘关系、以及一整套工程约束；而这些信息，往往分散在 dbt 项目、仓库元数据和团队约定之中，并不天然适合被大模型直接消费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，分享者提出一个非常务实的判断：Agent 能否进入生产级数据流程，关键不在模型能力，而在是否存在一个可信的中间层，负责把工程世界翻译给模型，同时把模型的意图约束在安全边界内。dbt MCP Server，正是为此而被引入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;dbt MCP Server 在架构中的角色&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在具体架构层面，dbt MCP Server 并不是简单地向 Agent 暴露一组 API。相反，它承担的是一个上下文协调器（Context Orchestrator）的角色。Agent 并不直接操作 Snowflake，也不会直接运行 SQL；它所“看到”的世界，是由 MCP Server 提供的、结构化后的 dbt 项目语义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过 MCP Server，Agent 可以理解：&lt;/p&gt;&lt;p&gt;当前 dbt 项目中有哪些模型、它们的用途和依赖关系；某个指标或表背后对应的业务含义；哪些操作是只读的，哪些是可执行的；执行一次变更可能带来的影响范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式的关键价值在于，它避免了让大模型在“裸数据”和“裸 SQL”层面自由发挥，而是始终把 Agent 约束在 dbt 已经定义好的工程语义之内。换句话说，Agent 的智能，建立在人类工程师已经验证过的建模体系之上，而不是绕开它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Agent 与 Snowflake 的协作方式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 这一侧，分享者并没有把重点放在新能力或新接口上，而是强调 Snowflake 在整个 Agentic Workflow 中所扮演的角色：稳定、可审计、可扩展的执行环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，Agent 并不控制 Snowflake。所有实际的数据查询、转换与计算，依然发生在 Snowflake 既有的执行体系内；Agent 只是通过 MCP Server，发起符合规范的请求。这意味着：&lt;/p&gt;&lt;p&gt;权限体系仍由 Snowflake 原生控制；执行结果可以被完整记录和回溯；性能与成本管理不会被 Agent 绕开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中特别提到，这种设计刻意避免了一个常见误区：让 Agent 成为超级用户。相反，它更像是一位受限但高效的协作者，在工程师设定的轨道上运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一个可复制的模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享的后半部分，演讲者总结了这种架构方式所带来的一个重要变化：Agent 不再是游离在数据体系之外的“智能外挂”，而是开始以内嵌方式进入数据工程流程本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可以帮助工程师更快理解项目结构、辅助定位影响范围、生成初步方案，但最终的执行路径、校验方式与责任边界，依然清晰地掌握在人类与平台手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是整场分享最克制、也最有价值的一点结论：Agentic Workflow 的目标，并不是“自动化一切”，而是在不破坏既有工程纪律的前提下，引入新的效率杠杆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这场分享可以看出，真正进入生产环境的 Agent 架构，已经不再停留在模型能力本身，而是越来越多地回到工程基本功：上下文、边界、权限、可追溯性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;dbt MCP Server 与 Snowflake 的这次实践，并没有试图给出一个“通用答案”，但它清晰地展示了一条现实可行的路径：让 Agent 站在成熟数据工程体系之上。对于正在探索 Agent 在数据领域落地方式的团队而言，这无疑是一种更稳健、也更值得参考的思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/lRtX3TRRZGq0dr6tl66q</link><guid isPermaLink="false">https://www.infoq.cn/article/lRtX3TRRZGq0dr6tl66q</guid><pubDate>Fri, 30 Jan 2026 11:21:33 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>劈柴哥和哈萨比斯亲自站台！谷歌世界模型Project Genie刷屏，幕后团队揭秘60秒不是极限，内存是巨大约束</title><description>&lt;p&gt;世界模型真的变天了！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，谷歌正式发布重磅世界模型原型产品“Project Genie”，只需一句话或一张图，就能一键生成可玩、可交互的实时虚拟世界。它的重磅程度，让谷歌“掌舵人”劈柴哥和Google DeepMind创始人哈萨比斯亲自为它站台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd6a37bc4e52bca469c0e9849efa6878.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/471ab6a435fa190a32bdaf517f222ea1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Project Genie 生成的虚拟世界中，你可以用 WASD 键移动角色、旋转视角、跳跃，在生成世界自由探索。更重要的是，其生成画面的精细度、整体完成度，已经明显超出以往研究型 Demo 的范畴，在观感上直逼成熟游戏产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年，世界模型一直被认为是通往AGI的重要路径，但始终存在一个根本问题：它们更像会动的视频，而不是真正的环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，早期世界模型普遍存在几大短板：&lt;/p&gt;&lt;p&gt;生成世界质量偏低，结构简单难以实时交互，或只能交互一两步长期一致性差，画面和规则会“漂移”不符合物理和因果逻辑，更像梦境而非世界&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 Project Genie，第一次把这些问题同时拉到了可用水平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Project Genie 是一个基于 Genie 3、Nano Banana Pro 和 Gemini 构建的原型 Web 应用，其中的核心是谷歌最新的世界模型 Genie 3。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与以往“先生成完整视频”的方式不同，Genie 3 采用自回归生成机制：它会根据世界描述和用户操作，逐帧生成环境状态，而不是播放预先生成好的内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这带来了几个关键变化：&lt;/p&gt;&lt;p&gt;长期一致性生成的世界可以在数分钟内保持稳定，不会快速崩坏；系统还能“记住”用户造成的关键变化，记忆时间最长可达约一分钟。真正的实时交互世界以 20–24 帧/秒运行，用户的操作会即时反馈到环境中，而非触发预设结果。更高质量的视觉表现生成画面分辨率约为 720p，整体真实感和细节水平明显高于以往世界模型，为智能体理解复杂环境提供了更可信的视觉基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌早在 2025 年就将 Genie 3 称为“通往 AGI 的关键一步”。而在 Project Genie的官方页面中，谷歌再次强调：&lt;/p&gt;&lt;p&gt;Genie 3 让智能体能够预测世界如何演化，以及自身行为如何影响世界，这是实现推理、规划和现实行动的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，在 Project Genie 身上，已经释放出一个非常明确的信号：世界模型正在从长期的前沿研究方向，正式迈入可落地、可探索的关键阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一旦世界模型能够稳定生成高质量、可交互、具备长期一致性的环境，其应用边界将被迅速打开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是自动驾驶中的复杂场景模拟、具身智能的环境理解与决策训练，还是游戏开发、影视制作、互动教育与新型媒体内容创作，世界模型都展现出极具想象空间的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 The Verge 报道，谷歌选择在这一时间点推出 Project Genie，部分原因在于希望观察用户的真实使用方式，从而发现此前尚未预料到的新应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google DeepMind 产品经理 迭戈·里瓦斯透露，谷歌内部已经对 Genie 在电影制作、互动教育媒体等领域，帮助创作者进行场景可视化与世界构建的潜力感到兴奋。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，Project Genie 仍是实验性产品：&lt;/p&gt;&lt;p&gt;单个世界最长探索 60 秒分辨率约 720p，帧率约 24fps仅向美国地区、18 岁以上的 Google AI Ultra 订阅用户开放&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Project Genie 发布后迅速引发热议。马斯克第一时间发文祝贺&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/85/85579d7d9dab73c5d0f4c381228253e6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于Project Genie的讨论，也在X上迅速扩散，不少网友将其称为又一个“变革时刻”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3fb8c42bb19512c8783789adb722c7a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/4081b066da9e1809b558153dd3d8f49c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9d/9d90c52846136975fee1e1a994bc81c6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e8/e83be3f6e3de7c74bbe215af6f14d168.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，Project Genie 负责人之一 Jack Parker-Holder 表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Genie 3 感觉像是世界模型领域的一个分水岭。我们现在可以生成任何可想象世界的、持续数分钟的实时交互式模拟。这可能正是具身通用人工智能此前缺失的关键一环。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;网友们玩疯了，在游戏世界释放创意&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来看，Project Genie 的使用流程并不复杂。进入页面后，用户可以直接从 Google 预设的多个世界模板中选择，也可以完全自定义环境和角色，构建一个专属的虚拟世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cfd807f4a169b4f6fa46da1db3c1b107.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现更精准的控制，Project Genie会用Nano Banana Pro的能力，先为生成世界打个“草稿”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个页面被清晰地分成左右两部分：&lt;/p&gt;&lt;p&gt;左侧用于填写环境的 prompt，例如地形结构、视觉风格和整体氛围；右侧则用于描述主角的形象与设定，并可选择第一人称或第三人称视角，从而提前确定进入世界后的体验方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;完成初步设定后，Genie 会先生成一个缩略图，可以对生成内容进行预览和微调。如果符合预期，就能进入生成世界，开始实时交互与自由探索。Genie 3 的响应延时非常低，在控制角色移动时，会带来强烈的沉浸感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在官方案例中，你可以把自己变成一个球，在草原上自由滚动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以看到，如果转换视角，球滚动留下的痕迹并不会消失，新生成的内容也不会覆盖旧区域。这一细节直观地体现了 Project Genie 所强调的世界一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一个官方案例中，你可以变成刷墙工人，想刷哪面墙就刷哪面，整个虚拟世界可以实时交互，且看起来十分合理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌表示，这是想象力空间的无限释放，无论是自然世界或现实场景，还是构建动画、小说中的奇幻世界，甚至是突破时间与空间限制的未来世界，都可以被创造出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不少网友迅速上手，开始“放飞自我”式创作，其中，各类游戏风格世界不断涌现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如在沙滩上骑摩托：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更绝的是直接制作山寨版“任天堂”游戏。比如马里奥系列，《塞尔达传说》，《银河战士》。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便抛开体验层面的不足不谈，Project Genie 在生成世界的质量与完成度上，依然足以令人震撼。这也难免让人产生进一步的联想，游戏从业者会不会大规模失业？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一担忧并非空穴来风。根据 Informa 本周发布的游戏开发者大会（GDC）报告，33% 的美国受访游戏开发者、以及 28% 的全球受访游戏开发者表示，他们在过去两年中至少经历过一次裁员。Project Genie可能会进一步扩大这种趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，围绕 Project Genie 的能力边界，也有人提出质疑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;The Verge的记者亲自上手试验后认为，从“游戏”的角度来看，Project Genie 所生成的“可玩世界”显得相当单调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了基础移动操作外，玩家几乎无事可做。没有任务目标，也缺乏音效反馈。更糟糕的是，输入延迟时有发生，甚至会出现角色失控、只能旋转视角的情况，严重影响整体体验的流畅度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该记者还提到，在仅有 60 秒 的探索时间内，世界的一致性并不稳定。系统有时会“忘记”此前生成的内容，例如滚动的小球留下的颜料痕迹会突然消失，已生成的道路也可能被重新覆盖为草地。这些现象让人难以确认模型是否能够持续、可靠地维护同一个世界状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在内容生成层面，Project Genie 对知名游戏 IP 也存在明显限制。测试中，索拉、唐老鸭、高飞、杰克·斯凯灵顿等角色均无法直接用于生成可交互世界，相关内容在进入实际体验阶段会被系统拦截。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55418fe66850ebe93c4f2e6ee0a26b3b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，与生成世界交互的智能体只能执行较为有限的操作，同一世界中多个模型之间也难以协同互动。此外，Genie 在渲染清晰文本、还原现实世界具体地点方面仍存在困难，智能体对控制指令的响应有时也会出现异常延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，谷歌方面回应称，Genie 并非游戏引擎，团队更关注它在增强创意过程、提升构思能力以及加快原型制作方面所展现出的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Geinie 3官网上也特别强调，目前产品仍处于早期研究阶段，因此会有：生成的世界可能看起来并不完全逼真，也不一定总是严格遵循提示、图像或现实世界的物理规律；角色有时可能难以控制，或者控制延迟较高；生成时间受限等问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Project Genie 团队深度揭秘关键问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Project Genie 上线不久，其背后的核心团队第一时间接受采访，包括Google DeepMind研究总监Shlomi Fruchter、Google DeepMind的研究科学家Jack Parker-Holder、产品Diego Rivas，他们都对世界模型长期关注，在这次访谈中深度揭秘Project Genie的关键问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这次对话讨论了：什么是世界模型？为什么只能生成60秒？Project Genie的研发历程是什么？它未来真正可能改变的是哪些领域？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们首先承认Project Genie的强大确实源于谷歌视频生成技术的积累，但同时他们也强调，Genie并不是更强的“视频模型”，而是人类第一次可以实时走进、操控、改变的生成世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中的核心差异是，世界模型是逐帧实时生成，能与过去保持物理与视觉一致性，并且用户可随时干预。这对延迟、内存、算力的要求，比普通视频生成高得多，也是更前沿、更有挑战的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对不少人抱怨“60秒不够”的问题，他们表示这是在服务成本、系统稳定性和体验质量之间做出的权衡。他们其实已经做出过更长时间的生成世界，但在实际测试中发现，随着生成时间拉长，世界的动态感反而会逐渐减弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员表示“与其花两分钟体验一个世界，不如花一分钟体验两个不同的世界，体验感会更好。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对模型的生成速度，他们表示已经够快了，短期内进一步“加速”并没有太大意义。接下来，他们更重要的研发方向，是降低算力成本，让这种能力能够被更多人真正用得起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品定位上，他们并不把 Genie 看作一款游戏，而更像是一个正在快速演化的实验场：&lt;/p&gt;&lt;p&gt;一方面，多人互动、长期一致性、复杂动态仍然是明确的技术瓶颈；另一方面，娱乐、教育、具身智能、机器人训练等方向，已经展现出非常清晰的应用前景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回顾产品研发历程，从论文阶段的Genie 1，到今天普通用户可以亲自上手体验的Genie 3，这背后其实是谷歌一整套高度协同的跨部门合作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌实验室与谷歌创意实验室是研发的核心力量，而服务团队、基础设施团队和沟通团队则共同兜底，确保这项起源于强化学习的前沿研究，能够被真实用户理解、体验并持续使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当团队回看去年八月时，他们很清楚，当时外界已经迫不及待想“走进这个世界”，但Genie仍然只是一个规模庞大的研究项目。即便如此，研发人员脑海中已经浮现出一系列潜在应用场景，其中最清晰的方向之一，正是具身智能。一个标志性的例子，是他们与 Simmer 项目的长期合作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Simmer 是由双子座模型驱动的目标导向智能体，能够在 3D 世界中执行复杂任务。过去，它只能在少数几个固定游戏环境中训练；而现在，借助Genie 3，只需一句文本指令，就能生成一个全新的、甚至是照片级写实的虚拟世界，把智能体直接“放进去”完成任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从Nano Banana Pro 的图像创作，到谷歌视频生成的成熟，再到可交互的世界模型Project Genie ，生成式技术正在构成一个连续体，世界模型将成为第三次技术跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是播客的更多细节，欢迎来看：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么只能60秒？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我很好奇，这背后的物理逼真度，是不是和我们在 VO（谷歌的视频生成模型）项目上取得的研究突破有关？感觉两者之间有相似之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：二者绝对是相关的，而且世界模型的研发难度其实更高。普通的视频模型，能在整个视频的时间线上自由调整过去和未来的帧，自由度很高 —— 就像有一块画布，模型能随时间生成视频，在画面的各个位置做微调，让整体效果连贯美观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但世界模型的难点在于，世界是持续演变的，每一帧的输入都是未知的，模型必须保证生成的画面既和过去的内容连贯，又能匹配用户当下的操作，所以技术难度会大很多。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实开发Genie 1时，我们用的是 Imagine 模型，当时我们的模型效果并不好，而且想要生成合适的图像也非常困难。Nano Banana Pro 是在Genie 3之后推出的，技术进步的速度真的令人惊叹。也许未来某一天，我们定义虚拟世界的方式，将不再局限于图像和文本，但就目前而言，这种方式已经给了用户足够的创作灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这个模型的复杂度上限在哪里？比如能不能在同一个世界里加入大量并行的互动元素？模型会在什么情况下出现效果衰减？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实 Nano Banana Pro 就是个很好的例子，如果一张图片里有 10 个人脸，想要对这张图进行编辑，模型就容易出问题。所以我想知道，Genie 3的自然性能边界在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：这个模型肯定不是完美的，目前它还只是一个研究预览版本。我们希望让大家亲自体验，看看它的优势在哪里，不足又在哪里，我们也能从用户反馈中学习和优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前模型在各类创意环境的视觉呈现上做得不错，画面可以非常精致，但在世界的动态表现上还有短板 —— 有时候初期的动态效果很好，但时间久了，动态感会逐渐减弱，这也是我们正在优化的点。不过它的表现已经足够令人惊喜了，所以还是建议大家亲自上手试试，看看哪些玩法能达到理想效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：不过说到延迟问题，还有很多技术点需要考虑。Genie 3的研发有一个核心约束：我们希望实现特定操作频率下的实时低延迟，也就是说，用户操作的往返延迟要极低。同时，内存也是一个巨大的约束 —— 模型的上下文长度越长，通常算力成本就越高，运行速度也会越慢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以研发的核心挑战，就是平衡这些相互冲突的目标。而在研究层面，我们正在所有这些领域持续优化，我们相信，模型的性能会不断提升，变得更强大、更快、更经济，这也是行业的整体发展趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我还有个问题，模型的生成时长是人为限制在 60 秒，还是真的能实现 3 到 5 分钟的连续生成？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：其实我们已经做出过能连续生成更久的演示版本了，但我们觉得 60 秒是一个比较合适的时长 —— 既能让用户充分体验虚拟世界，又能保证为足够多的用户提供服务，这其实是在服务成本上做的权衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且就像我们之前提到的，生成时间越长，世界的动态感会逐渐减弱。所以我们觉得，与其花两分钟体验一个世界，不如花一分钟体验两个不同的世界，体验感会更好。当然，如果用户反馈希望延长时长，我们也会做出调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也和虚拟世界的类型有关，比如如果你在体验高山速降滑雪，两分钟的时长会很过瘾，因为整个过程是持续的动态体验；但如果只是探索图书馆，两分钟可能就没那么有趣了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是啊，人们总是能很快适应新的技术体验。但对我来说，这个模型的表现依然令人难以置信。你之前被问到能不能让模型运行得更快，现在的速度已经到极限了吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：在当前实时交互需求下，生成速度已经足够快，短期内进一步加速的意义不大。因为模型是实时生成虚拟世界的，速度再快其实也没有意义了 —— 它的生成速度已经和用户的体验速度完全匹配。接下来我们的研发重点，会放在降低算力成本上，这样才能让更多人用上这款产品。同时，在保持速度的前提下，不断增加新功能，这本身也是一个巨大的挑战，我们希望在各个方面都把模型做得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;背后的故事：谷歌跨团队协作&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊完当下的体验，我特别想知道模型的未来迭代方向。不过在聊未来之前，我们先回顾一下研发历程吧。我们八月份发布了Genie 3的首支演示视频，之后启动了可信测试，不断迭代产品、搭建基础设施。能不能跟大家快速讲讲，从一支惊艳的演示视频、小规模的早期测试，到正式推出面向用户的精灵计划，这中间都经历了什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：首先，八月份发布模型和演示视频后，我们让一小部分人体验了产品，核心是为了收集反馈 —— 因为这是一款全新的应用，一种全新的体验，我们需要思考如何负责任地将它推向市场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从那以后，我们的大部分工作都集中在基础设施、服务架构和成本控制上，毕竟我们希望能让尽可能多的用户体验到它。而美国的谷歌 Ultra 订阅体系，能让我们触达足够多的用户，收集到第一手的反馈：比如用户觉得哪些功能有用，会如何和产品互动，哪些玩法体验最好。这段时间里，我们也在持续完善可信测试项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这其实是模型开发周期中最核心的阶段，因为我们能从不同类型的用户身上学到很多东西，无论是创意工作者，还是教育领域的从业者，都能给我们带来丰富的洞察，让我们知道模型目前的实际应用价值、未来的发展方向，以及哪些体验是用户最期待的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回头看八月份，当时我们知道大家肯定想体验这款产品，但它那时还只是一个大规模的研究项目。我们脑海里有很多应用场景，比如智能体、机器人这类具身智能领域，都能用到这项技术。去年年底还有一个和我们类似的项目发布，他们也用Genie 3来训练游戏智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从消费端的角度来看，我们觉得这个产品会很有吸引力，所以想收集用户反馈，但当时也不确定是否已经到了面向更多用户发布的时机。而迭戈主导的可信测试项目，让我们发现，用户第一次上手这款产品时，都会有惊艳的体验。我们希望深入了解更多的应用场景，所以这次的发布，也是我们在这方面迈出的一大步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一年前，我根本没想到这个模型能有这么强的吸引力，但现在它已经成为一款非常有趣的产品，我们也很期待大家会用它来做什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊完产品和技术，我们再来聊聊谷歌的跨团队合作吧。显然，从你们的分享和幕后工作来看，打造这款产品的难度非常大。谷歌内部有哪些团队参与了Genie 3和Genie的研发？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：幕后参与的团队非常多，谷歌实验室、谷歌创意实验室是核心 —— 画廊里的那些虚拟世界，大多是创意实验室的作品；还有服务团队、基础设施团队，基本上有一个完整的幕后团队在推动这项工作。从八月份发布模型到现在，我们一直在全力冲刺，所有团队的付出都堪称英勇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们还和沟通团队深度合作，因为想要向大家解释一款全新的模型，一种大家从未体验过的技术，是一个非常细致的话题 —— 它起源于强化学习这个相对小众的领域，现在却被媒体、社交媒体上的各类人群广泛讨论，所以用正确的方式传递这项技术，非常重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回顾这个领域的研究起点，我们甚至不确定这项技术能否成功落地。而现在，我们让它实现了实时交互，达到了不错的画质，完成了从研究构想到发布模型，再到推出面向用户的体验产品的闭环，这一点让我非常兴奋。这并非理所当然，也充分体现了谷歌内部跨技术栈的团队协作能力，这种能力非常独特。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们在镜头外还聊过，不仅是Genie 3，谷歌所有模型的能力都在不断拓展，而这和模型的训练方式息息相关。杰克，你之前还尖锐地提到，这些模型其实并没有针对任何特定的应用场景进行训练，却能在各个领域实现很好的泛化能力，能不能再聊聊这一点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：没错，我们一开始其实并不知道这个模型的具体应用场景。去年年底，Genie 团队还在做纯粹的研究项目，Genie 1最初只是一篇研究论文，和 VO（谷歌的视频生成模型）完全不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，我们还在做 Doom 游戏引擎的相关研究，这项研究充分展现了实时交互的潜力，但它仅适用于 Doom 这一个特定的游戏世界，迭戈可以再聊聊这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，2024 年 12 月 VO（谷歌的视频生成模型）2 的发布，在 AI 领域已经是很久以前的事了，但当时我看到它的效果时就觉得，视频生成技术已经成熟了，视觉质量达到了行业前沿，值得我们深入探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是我们达成共识，认为这项技术的潜力无限，随后组建了跨团队的研发小组，汇集了各个领域的专家 —— 他们都在不同的技术领域有积累，我们相信把这些技术结合起来，会产生不可思议的效果。而我们的研发，并非针对某个特定的下游应用场景，而是因为它蕴含着无数的应用可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最酷的是，我们脑海里有一些预想的应用场景，比如和 Simmer 项目的合作，我们和这个项目的合作已经有很长时间了，他们也参与了Genie 2的研发，体验过Genie 2，现在已经基于Genie 3发布了相关产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Simmer 是我们最强大的目标导向智能体之一，能在 3D 世界中互动，是由双子座模型驱动的 —— 你可以在 3D 世界中向它输入文本指令，它就能完成各种不同的目标，泛化能力非常强，还能通过自我提升学习。这也是我们迈向通用人工智能、具身智能的重要方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;去年年底我们发布了这款智能体，他们就用Genie 3的虚拟世界来探索智能体的能力。要知道，Simmer 原本只在几款游戏中接受过训练，但现在借助Genie 3，你只需输入文本，就能创建一个全新的、甚至是照片级写实的虚拟世界，然后把智能体放进去，看它完成各种任务。这两个项目的结合，可以说是水到渠成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来的应用领域：娱乐、教育、具身智能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：从应用层面来说，我个人对娱乐和教育领域的应用最期待。我们希望让更多人体验这款产品，看看凭借现有的技术，现在能打造出哪些应用。教育领域是我们重点关注的方向，比如让人们在虚拟世界里互动学习 —— 想象一下，能为用户打造一些他们在现实中无法体验的场景，比如一个孩子害怕蜘蛛，我们可以打造一个满是蜘蛛的房间，让孩子在虚拟世界里慢慢适应，克服恐惧。我的孩子就怕蜘蛛，所以我觉得这种个性化的全新体验，价值非常大，这也是我们近期的研发重点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一方面，我们之前也聊过，机器人技术和具身智能领域的世界模型，潜力也非常大。当然这个领域还有很多研究工作要做，但我个人对它充满期待。简单来说，核心思路就是：如果一个模型能模拟现实环境，那我们就可以用它在虚拟世界里训练机器人，或是让具身智能体在虚拟世界里学习，甚至实时辅助智能体做出决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Genie 计划虽然现在已经很惊艳了，但它只是一个起点。未来我们会和谷歌实验室继续深度合作，不断优化产品的功能、操控方式、应用架构等；也会拓展更多的使用场景，不局限于Genie 计划这一个应用，还会推出开发者 API，让更多开发者参与进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不得不说，开发者总能发掘出产品的商业价值，找到极具经济影响力的应用场景，这也是我觉得很有意思的一点 —— 除了娱乐，世界模型还能在哪些领域找到产品市场契合点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且很多功能在不同的应用场景中是相通的，比如更广泛的交互性。可以肯定的是，机器人技术的发展，不可能只靠方向键来实现，未来的机器人助手需要更多的操控方式，而这和虚拟世界的交互研发是相通的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;八月份发布Genie 3，让我们成为首批推出这类模型的团队，也让我们能和谷歌内部的各个团队展开合作。我们会认真吸纳所有的用户反馈，把大家提出的建议都列出来，成为下一代模型的研发方向。我之前跟杰克说过，我们只实现了目标的 50%—— 因为我们总是会设定极具野心的目标，这个领域还有太多可以探索的地方，模型还有很多不足，需要我们不断优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个领域的发展空间巨大，我们才刚刚起步。就像写论文一样，一个项目完成后，你马上就会想，下一个项目可以加入哪些功能，做得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在社区里也出现了很多有趣的世界模型，有些和Genie 3很相似，但我们的目光已经放得更远了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;怎么玩这个产品？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：除了研发历程和未来规划，还有没有什么想跟大家分享的？比如对于即将体验这款模型的用户，你们有什么建议？毕竟你们比普通人花了更多时间研究和使用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：我建议大家尝试个性化创作，打造属于自己的、其他系统无法实现的世界。当然，用它打造游戏环境也很有趣，但这类场景其他系统也能做到；而把现实中的专属事物 —— 比如一个玩具、一张照片，或是让自己以特定风格出现在真实的环境中，这种体验是独一无二的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我想起了 VO（谷歌的视频生成模型）早期的一个研究项目：有人用 VO（谷歌的视频生成模型）为阿尔茨海默病患者重现童年记忆，让他们在虚拟世界里重温过去，这个项目特别棒。所以我觉得，把个人专属的事物融入虚拟世界，让它们 “活” 过来，这种互动方式非常有价值，大家可以试试这个方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，大家肯定会发现，模型的提示词创作目前还不够完善，但这恰恰是机会。几年后当这个模型变得非常成熟时，大家会想起现在这个阶段，就像我们现在看待 VO（谷歌的视频生成模型）3 一样 —— 现在 VO（谷歌的视频生成模型）3 的每个提示词都能生成优质视频，精灵 3 号的每个提示词基本也能实现预期效果，但在早期，提示词的创作至关重要，甚至有人会花 10 到 20 分钟精心打磨一个提示词。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以如果第一次创作的效果不好，别放弃，这款全新的模型，可能会以你意想不到的方式呈现出惊喜的效果。而且亲自上手体验，你就不是在消费一款产品，而是在探索前沿技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太认同了，“探索前沿技术” 这句话简直可以当作产品标语了。我还有一个觉得很有趣的点：当被动的媒体消费变成交互式的体验，会发生什么？这是一片全新的未知领域。过去也有人做过尝试，但现在有了这种真正定制化的交互式媒体叙事，它会给整个媒体和娱乐行业带来什么影响，真的太值得期待了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：还有一个玩法也很有趣，你可以在虚拟世界里设置挑战，把这个世界分享给别人，让对方完成任务，比如从 A 点走到 B 点。这是一种基础的、有目标的游戏体验，现在的模型已经能实现了。比如那个球的场景，你可以让别人用球写出自己的名字，这类简单的挑战都能设置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就像杰克说的，现在的体验虽然还比较基础，但它蕴含着巨大的创意潜力。比如还有一个带环的场景，你可以操控角色穿越环道，体验飞行的感觉，这也是用户发掘的玩法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;人们还经常问，行业的前沿在哪里，我们下一步要做什么。我经常会做一件事：长时间沉浸在Genie 3的第一人称写实世界里，然后看向窗外，对比虚拟和现实的差距。我认为最终，虚拟世界会和现实世界变得几乎无法区分，虽然今天我们不深入聊这个话题，但从模型的性能发展来看，这显然还有很长的路要走。但如果能生成和现实高度逼真的世界，在里面自由移动、互动、完成各种事情，那该多不可思议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而这也是驱动我们开展这项研究的核心愿景：想象你拥有一个宇宙的副本，你可以在其中随心所欲。显然，这个副本有巨大的应用价值，能用到很多领域。这虽然是一个非常远大、甚至可能无法实现的目标，但它就像北极星一样，一直指引着我们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如我们这次把恐龙鲍勃放进虚拟世界，其实就是在重构现实空间，给现实事物做有趣的增强。未来这方面的探索，一定会非常有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：那到Genie 5的时候，我们可能真的会分不清自己是在现实还是在模拟世界里了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界模型是第三次技术跃迁&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我还有一个有点尖锐的问题想问问大家：你们觉得，大多数人体验到世界模型的时间线会是怎样的？世界模型会先通过企业端影响普通人的生活吗？比如企业利用世界模型提高生产效率，打造更好的日常产品；还是说，未来普通人的日常生活中，会直接和世界模型产生互动？如果是后者，这个时间线大概会是多久？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：这其实取决于你如何定义世界模型。如果是指交互式的视听体验类世界模型，我认为今年、明年，就会有越来越多的人接触到它，我们也会看到它在一些领域大放异彩，最终成为很多应用的基础功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但就像现在的视频生成技术，虽然发展很快，但真正融入普通人日常生活的比例其实并不高，世界模型也需要时间来完成用户普及，找到合适的应用场景 —— 毕竟视频和图像不同，世界模型又和视频生成不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而如果是具身智能领域的世界模型应用，很难给出具体的时间线，但这个领域已经在取得不错的进展了。&lt;/p&gt;&lt;p&gt;另外，用户的人群特征也很重要：有些经常接触交互式媒体的人，会成为世界模型的早期使用者，他们知道该如何体验；但如果把它交给一个对前沿技术不感兴趣的家人，他们可能会觉得无从下手，体验不到产品的魅力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但具身智能相关的应用，可能在未来 1-2 年就会走进现实，普通人会在生活中直接接触到，所以最终的普及时间，还是取决于用户所处的技术接受曲线位置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一点，Genie计划也印证了一个趋势：生成式技术正在形成一个连续体，从 Nano Banana Pro 的图像创作，到 VO（谷歌的视频生成模型）的视频生成，再到现在Genie 3的交互式实时媒体创作，成为第三个核心支柱。我们希望未来有更多人能体验到这个连续体上的各类创作体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我特别期待看到行业的发展趋势，毕竟 VO（谷歌的视频生成模型）和 Nano Banana Pro 的发展过程中，都出现过一些爆红的玩法，都是我从未预料到的，太疯狂了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：世界模型的发展，和图像、视频生成还有些不同。图像和视频生成的作品，能被数百万人观看，一个人的创作可以被广泛传播，家人、朋友都能看到；而世界模型的独特之处在于，你可以在探索的过程中，不断改变周围的世界，这开辟了很多我们未曾考虑过的新途径、新玩法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图像和视频生成，本质上是用新技术替代或自动化了过去的一些创作方式，当然也带来了新的能力和限制；但世界模型，实现了很多过去根本不可能做到的事情，这是它最大的不同，当然二者也有很多相似之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个我们非常兴奋的想法，大家在演示中也能看到端倪：用户可以在现有虚拟世界的基础上继续创作，这样就会形成很多有趣的世界分支，还能追溯创作源头。这方面的潜力非常大，值得我们深入探索。&lt;/p&gt;&lt;p&gt;Genie 计划上线时，用户可以下载自己的虚拟世界演示视频；未来我们还会探索更多的世界分享方式，让大家能以更有趣的方式在别人的世界基础上创作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太酷了，我还想要一个 “世界档案” 功能，这样大家就能看到我所有的创意想法了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从世界模型的发展来看，技术进步的节奏是怎样的？显然我们已经看到了巨大的进步，图像生成、VO（谷歌的视频生成模型）视频生成、核心双子座模型，都取得了长足的发展。世界模型是不是也在遵循同样的发展轨迹，到处都是触手可及的技术突破，同时受益于算力规模和推理能力的提升？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研究员：可以这么说。图像生成技术显然比视频生成更成熟，视频生成和世界模型之间的差距，我无法准确衡量，但可以肯定的是，世界模型是超越视频生成的前沿技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最新一代的视频生成模型，画质已经比Genie 3高很多了，我们也不指望Genie 3现在能生成极致精美的视频，因为实时交互的约束，是普通视频生成模型所没有的。所以世界模型的发展，可能会比视频生成稍慢一些，但它能带来全新的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说实话，我们现在仍处于技术快速进步的阶段。硬件始终是一个巨大的约束，这对所有模型来说都是如此。行业的整体趋势是，在成本基本不变的情况下，让模型的运行效率越来越高。但最终，我们还是需要更易获取的硬件支持 —— 比如希望未来人们能直接在自己的设备上运行这类模型，实现无延迟的即时体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前高性能的 TPU、GPU 还并非人人可得，硬件的发展速度因为一些实际原因，会比模型研发慢一些，但这也是我们的未来方向 —— 希望到Genie 5时，大家能在手机上运行完整的通用模拟系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一点我们也讨论过，谷歌拥有垂直技术栈的优势，这也是我们在谷歌、在深度思维工作的魅力所在：我们既能站在模型研发的前沿，又能利用谷歌最好的硬件来支持模型的运行。而且专门为世界模拟打造的硬件，本身也极具发展潜力，它就像通往另一个维度的入口，点击就能进入，充满了新鲜感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传送门：&lt;/p&gt;&lt;p&gt;https://labs.google/projectgenie&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;链接：&lt;/p&gt;&lt;p&gt;https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/&lt;/p&gt;&lt;p&gt;https://deepmind.google/models/genie/&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=Ow0W3WlJxRY&amp;amp;t=4s&lt;/p&gt;&lt;p&gt;https://www.theverge.com/news/869726/google-ai-project-genie-3-world-model-hands-on?view_token=eyJhbGciOiJIUzI1NiJ9.eyJpZCI6ImZCakl0bmxFNGwiLCJwIjoiL25ld3MvODY5NzI2L2dvb2dsZS1haS1wcm9qZWN0LWdlbmllLTMtd29ybGQtbW9kZWwtaGFuZHMtb24iLCJleHAiOjE3NzAxNDAwNTYsImlhdCI6MTc2OTcwODA1OH0.q5OBTD_V36-65oc1EGqPxKYCZF00c7ODvifvagVcwbA&amp;amp;utm_medium=gift-link&lt;/p&gt;</description><link>https://www.infoq.cn/article/NC3jkcH9qgVjb8Q36sl2</link><guid isPermaLink="false">https://www.infoq.cn/article/NC3jkcH9qgVjb8Q36sl2</guid><pubDate>Fri, 30 Jan 2026 10:46:49 GMT</pubDate><author>高允毅</author><category>生成式 AI</category><category>Google</category></item><item><title>预算有限、技术空白：最刁钻的AI用户，是中小企业老板</title><description>&lt;p&gt;2026年，大模型已经不再稀缺，但它在中小企业的办公环境中处境却很骨感。市场部用通用聊天机器人写促销文案，结果因工具不理解“BOM表”“良品率”等术语，导致员工反复返工；法务人员还在逐字比对几十页合同，在密密麻麻的条款里找差异；客服团队被重复问题淹没，而公司花了几万元采购的AI工具，始终没能真正嵌入业务流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题在于工具离场景太远。当算力和模型能力变得普及，企业要的不再是“更强的大模型”，而是一个能理解自己业务、快速跑起来、带来实际收益的智能助手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业正在悄然转向。AI算力需求从训练转移至推理，推理算力需求增长4倍；算力消费模式从买卡转移到买Token，Token消耗量增长53倍；几乎所有企业都在通过智能体的方式消费Token。华为云和华为云伙伴都观察到，客户不再纠结参数规模，反而关心“它能帮我解决什么具体问题”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;华为云看准了这个拐点。今年1月23日，华为云在“华为云中国区销售伙伴产品方案发布会”上，隆重介绍了Flexus&amp;nbsp;AI智能体——一个专为中小企业设计的轻量化、场景化智能体平台。它聚焦于更专业的场景、追求更精准的效果，并致力于实现极简部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus&amp;nbsp;AI智能体依托华为自研的搜索大模型，攻克了搜索精度的关键难题。在企业知识问答、智能数据查询等高频场景中，其准确率领先业界平均水平2至9个百分点。在发布会现场的实时对决中，面对权威数据集的严格考验，Flexus&amp;nbsp;AI智能体更以100%的准确率胜出，充分证明了其性能的领先性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台重点覆盖互联网、金融与保险、医疗健康、制造业、零售/电商、专业法律服务及教育等行业。其创新的&amp;nbsp;“Solution&amp;nbsp;as&amp;nbsp;Code”&amp;nbsp;功能，能将企业级应用场景打包成“即取即用”的模板，这使Flexus&amp;nbsp;AI&amp;nbsp;智能体超越了工具属性，成为优秀实践经验的高效载体。此外，华为云的“天筹AI求解器”还能为工业、物流等复杂场景提供最优决策支持，切实帮助企业降本增效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus&amp;nbsp;AI&amp;nbsp;智能体的目标十分明确：让即便没有专职AI团队的中小企业，也能在几天内部署一个真正“懂行”的智能助手。这背后，折射出华为云对AI商业化下半场的核心判断——最终的战场，在于帮助客户实现业务价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;为中小企业制造的AI&amp;nbsp;智能体&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年，云计算在中小企业中完成了从“可选项”到“必选项”的演进。如今，人工智能正经历相似的关键跃迁。然而对广大中小企业而言，这场技术浪潮并非坦途：它们并不缺乏拥抱AI的意愿，却普遍困于三大现实挑战——应用场景模糊、技术门槛高企、投入产出难以量化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;华为云敏锐捕捉到这一结构性变迁，依托在工程化落地、场景理解与企业级服务领域的长期积累，将Flexus&amp;nbsp;AI智能体定位为通向长尾市场的“轻量化入口”。产品设计源于对典型中小企业客户的深度洞察：预算有限、缺乏专业IT团队、需求表达不清晰，却对数据安全与成本控制高度敏感。其目标清晰而务实——回应中小企业“用得起、用得上、用得好”的朴素诉求，同时在AI商业化深水区开辟差异化增长路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为面向泛行业中小企业的轻量化平台，Flexus&amp;nbsp;AI智能体以“开箱即用、高性价比、安全可控”为核心理念，通过四大能力直击AI落地痛点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丰富模板库：提供40余个源自真实企业实践的预置场景，覆盖舆情监测、报告撰写、客服问答、知识检索等通用与行业需求，大幅降低启动成本；一站式平台：支持可视化编排与一键部署，无需编码即可完成智能体构建与发布，数日内即可上线业务助手；安全可控：支持公有云调用与私有化部署双模式，保障数据主权与合规要求；底座协同：深度集成华为云Tokens服务与昇腾AI算力，保障高并发稳定性，并自然带动ECS、数据库、KooSearch等云资源的协同消耗。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相较于华为云生态内面向大型企业、强调深度定制的平台，Flexus&amp;nbsp;AI智能体聚焦轻量化通用场景，在办公、营销与服务等高频领域追求极致的简便与实用，形成清晰的差异化定位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤为关键的是，该产品精准破解了AI落地的“最后一公里”难题——企业知识问答。当前，企业普遍采用“检索增强生成（RAG）”技术赋予大模型专业知识，但效果瓶颈往往不在模型本身，而在于前端检索精度不足：传统关键词检索难以理解语义，易在海量知识库中漏检或错检，导致智能助手“答非所问”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，Flexus&amp;nbsp;AI智能体内置的企业级搜索服务成为破局关键：以华为自研的中文文本向量大模型为底座，具备出色的语义理解能力；其检索引擎在权威基准测试中表现优异，实现精准高效的语义匹配；通过架构优化，关键性能指标显著优于主流开源方案，同时有效控制成本。最终，企业获得的不再是一个“听起来聪明”的对话机器，而是一位真正精通业务、检索精准、响应迅捷的可靠数字助手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一能力已在多行业实战中快速验证：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;互联网与出海企业：用于语义级信息检索、舆情监测及动态视频生成，一键部署覆盖电商、科研与物流的AI工具链；金融与保险：实现研报自动生成、财务风险预算、智能审计及反欺诈风控，部分保险场景复用自医疗行业的成熟实践；医疗健康：深入辅助诊疗、影像分析与报告解析，为医疗机构提供研发助手；制造业：应用于工业质检、包装检查、生产安全检测、设备预测性维护、高炉工艺控制和性能预测等领域；零售与电商：&amp;nbsp;场景涵盖用户运营、门店巡检等。某国内头部奶茶品牌一周内所有门店系统均上线智能体，月付费仅4万元；教育行业：可用于内容生成与学术支持、教学辅助等；法律行业：&amp;nbsp;Flexus&amp;nbsp;企业搜索服务在中国法律智能技术评测中斩获类案检索一等奖；某住宅设计公司已将Flexus&amp;nbsp;AI&amp;nbsp;智能体深度用于合同风险条款识别与合同比对中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些案例背后，是一套真正为中小企业量身打造的AI落地路径：无需技术积累、不必重金投入，只需聚焦自身业务，就能快速用上AI。Flexus&amp;nbsp;AI智能体以“场景更专、效果更精、使用更易”为原则，提供开箱即用的模板和零代码操作体验，配合免费调优支持，真正做到“一天出Demo、一周上线见效”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;全流程测评：智能体如何进入内容生产？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为专注于科技行业的内容创作者，我们始终在寻找能够提升信息处理深度与效率的工具。内容创作，尤其是科技领域，面临着信息过载、源头繁杂、热点更迭迅速的常态挑战。在策划一个深度选题时，从海量噪音中快速梳理出主线、定位核心矛盾，往往消耗大量精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们对华为云Flexus&amp;nbsp;AI智能体进行了体验。我们最近正在研究中国AI硬件出海战略与挑战，这个方向既涉及复杂的技术趋势研判，又牵涉多变的国际贸易政策环境。为此，我们尝试使用华为云Flexus&amp;nbsp;AI智能体矩阵辅助完成前期调研。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的测试分两步走：先让“深度研究报告撰写”智能体勾勒全球产业趋势图谱，再请“国家政策研究与比较”智能体扫描关键市场的准入壁垒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们要求“深度研究报告撰写”智能体研究2020-2026年AI智能硬件的行业发展趋势时，智能体并未直接输出结论，而是首先展示其思考路径——将问题拆解为市场规模、产品形态、技术演进、竞争格局和应用场景五个维度，并据此构建报告结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种结构化处理带来了三个实际价值：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;节省框架搭建时间：几分钟内生成的研究提纲，覆盖了边缘AI设备渗透率、AI&amp;nbsp;PC出货量、NPU/GPU融合架构等关键议题，避免了从零开始的信息筛选。聚焦核心变量：通过数据表格（如各细分市场CAGR、厂商份额预测）和趋势关键词，帮助我们快速识别哪些是驱动变化的关键因子。提供可扩展基础：输出内容并非封闭结论，而是带有明确数据来源提示和逻辑节点的“半成品”，便于后续人工验证与观点深化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;紧接着，我们请“国家政策研究与比较”智能体“研究美国、欧洲和印度，在进口中国AI智能硬件时不同的海关政策。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体没有给出模糊或笼统的结论，而是立即先建立了一个清晰的四维比较分析模型：关税结构与HS编码、技术性贸易壁垒（认证）、国家安全审查、政策演变趋势。这直接对应了企业出海实操中必须面对的四大关卡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;差异化逻辑的提炼：在反馈中，智能体不仅罗列了FCC、CE、BIS等认证差异，更尝试归纳出不同市场的核心监管逻辑：美国的“科技遏制与长臂管辖”、欧盟的“规则主导与伦理审查”、印度的“贸易保护与产业替代”。这种对政策背后战略意图的解读，远比单纯列举条款更有洞察力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些归纳虽需进一步验证，但已为后续针对性调研提供了清晰的问题清单和方向指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合来看，Flexus&amp;nbsp;AI&amp;nbsp;智能体的核心优势不在于“给出答案”，而在于“组织问题”。它通过结构化拆解，将模糊、宽泛的研究需求转化为可操作的分析路径，显著缩短了从信息搜集到洞察生成的链条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一能力不仅适用于科技内容创作，在财经报道、政策简报、市场进入评估等需要快速处理多源信息并输出逻辑清晰内容的场景中，同样具备实用价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus&amp;nbsp;AI智能体的价值不在于炫技式的能力，而在于将AI真正嵌入中小企业的业务流。在AI从技术热词走向商业落地的关键阶段，华为云选择以轻量化、模板化、安全可控的方式切入长尾市场，既回应了中小企业“用得起、用得上、用得好”的核心诉求，也重新定义了AI产品的价值标准：不是参数多大，而是离业务多近。&lt;/p&gt;</description><link>https://www.infoq.cn/article/ZGQys2f6BuEpytzqNLUu</link><guid isPermaLink="false">https://www.infoq.cn/article/ZGQys2f6BuEpytzqNLUu</guid><pubDate>Fri, 30 Jan 2026 09:46:10 GMT</pubDate><author>杨过</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>“天下苦CUDA久矣！”KernelCAT率先掀桌，实现国产芯片无痛适配</title><description>&lt;p&gt;2026 年 1 月底，英伟达 CEO 黄仁勋再次来华，刻意亲民的“菜市场外交”插曲不仅又一次引发热议，也让很多人回想起老黄在 2025 年 1 月，宁愿缺席美国总统特朗普就职典礼，也要来中国参加分公司年会、维护客户的有趣往事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为市值逾 4.5 万亿美元的 AI 巨头掌门人，老黄为何如此重视中国？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/88/882ad7697fdc8328b84b398e401f293d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种重视的根源，在于中国 AI 产业与英伟达 GPU 及 CUDA 生态之间的双向深度依赖。一方面，中国主流 AI 模型的训练仍高度依赖英伟达芯片，且需在 CUDA 生态中加速迭代，以此追赶美国闭源模型的实力；另一方面，中国庞大的 AI 市场、优质的 AI 人才，以及台积电、富士康等核心供应链企业，共同撑起了英伟达的庞大估值与商业霸权。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能的繁荣与底层的“枯竭”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中国 AI 的表层繁荣有目共睹：大模型发布数量占全球 40% 以上，稳居世界第一；Qwen 登顶 Hugging Face 全球下载榜，累计下载超 10 亿次；“豆包”日均活跃用户数（DAU）破亿，2025 年国产 AI 应用总下载量达 25.7 亿。这一切营造出一种错觉：中国人工智能的道路已是一片坦途。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，剥开这层光鲜外衣，国产 AI 的根基却异常脆弱。尽管本土芯片厂商在硬件设计与制造上奋力追赶，软件生态的缺失却成为难以逾越的鸿沟。高昂的迁移成本、对 CUDA 的路径依赖，使得国产模型即便想用“国产芯”，也常因缺乏高效、兼容的算子支持而寸步难行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更严峻的是，这种依赖本质上是算力主权的交锋：国际芯片巨头每一分估值增长的背后，都可能是国内算力产业的被动与掣肘。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要打破这一困局，关键不在造更多芯片，而在打通“算法—算子—硬件”之间的最后一公里，尽可能多得释放国产芯片的理论峰值性能，建设自己的国产芯片生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中最核心的一环，正是高性能算子的开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;KernelCAT：计算加速专家级别的 Agent&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;算子（Kernel），是连接 AI 算法与计算芯片的“翻译官”：它将算法转化为硬件可执行的指令，决定了 AI 模型的推理速度、能耗与兼容性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;算子开发可以被理解为内核级别的编程工作，目前行业仍停留在“手工作坊”时代——开发过程极度依赖顶尖工程师的经验与反复试错，周期动辄数月，性能调优如同在迷雾中摸索。若把开发大模型应用比作“在精装修的样板间里摆放家具”，那么编写底层算子的难度，无异于“在深海中戴着沉重的手铐，徒手组装一块精密机械表”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果，让 AI 来开发算子呢？传统大模型或知识增强型 Agent 在此类任务面前往往力不从心：它们擅长模式匹配，却难以理解复杂计算任务中的物理约束、内存布局与并行调度逻辑。唯有超越经验式推理，深入建模问题本质，才能实现真正的“智能级”优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在这一“地狱级”技术挑战下，KernelCAT 应运而生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8f/8faf0bf997be96bcfd5f8bcb5396620f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KernelCAT 是一款本地运行的 AI Agent，它不仅是深耕算子开发和模型迁移的“计算加速专家”，也能够胜任日常通用的全栈开发任务，KernelCAT 提供了 CLI 终端命令行版与简洁桌面版两种形态供开发者使用。不同于仅聚焦特定任务的工具型 Agent，KernelCAT 具备扎实的通用编程能力——不仅能理解、生成和优化内核级别代码，也能处理常规软件工程任务，如环境配置、依赖管理、错误诊断与脚本编写，从而在复杂场景中实现端到端自主闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/69/6920f6c41c59b89f3d72dd73255fd27b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为国产芯片生态写高性能算子&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在算子开发中，有一类问题很像“调参”——面对几十上百种参数或策略组合，工程师需要找出让算子跑得最快的那一组配置。传统做法靠经验试错，费时费力，还容易踩坑。KernelCAT 引入了运筹优化的思路：把“找最优参数”这件事交给算法，让算法去探索调优空间并收敛到最佳方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以昇腾芯片上的 FlashAttentionScore 算子为例，KernelCAT 在昇腾官方示例代码上，可以自动对该算子的分块参数调优问题进行运筹学建模，并使用数学优化算法求解，在十几轮迭代后就锁定了最优配置，在多种输入尺寸下延迟降低最高可达 22%，吞吐量提升最高近 30%，而且而整个过程无需人工干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是 KernelCAT 的独特之处：它不仅具备大模型的智能，能够理解代码、生成方案；还拥有运筹优化算法的严谨，能够系统搜索并收敛到最优解。智能与算法的结合，让算子调优既灵活，又有交付保障。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对 KernelCAT 的另一场测试中，团队选取了 7 个不同规模的向量加法任务，测试目标明确：在华为昇腾平台上，直接对比华为开源算子、“黑盒”封装的商业化算子与 KernelCAT 自研算子实现的执行效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果同样令人振奋，在这个案例的 7 个测试规模中，KernelCAT 给出的算子版本性能均取得领先优势，且任务完成仅仅用时 10 分钟。这意味着，即便面对经过商业级调优的闭源实现，KernelCAT 所采用的优化方式仍具备竞争力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/90/90a7b4e9f17290018d9342b3ed31e0a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅是数值层面的胜利，更是国产 AI Agent 在算子领域的一次自证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;没有坚不可破的生态，包括 CUDA&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;全球范围内，目前超过 90% 的重要 AI 训练任务运行于英伟达 GPU 之上，推理占比亦达 80% 以上；其开发者生态覆盖超 590 万用户，算子库规模逾 400 个，深度嵌入 90% 顶级 AI 学术论文的实现流程。黄仁勋曾言：“我们创立英伟达，是为了加速软件，芯片设计反而是次要的。”这句话揭示了一个关键真相：在现代计算体系中，软件才是真正的护城河。英伟达的持续领先，源于其从底层算法出发、贯通架构与编程模型的全栈掌控能力。参考 AMD 的历史经验，即使在架构与制程上具备充足的竞争力，缺乏成熟的生态系统也仍然难以撼动英伟达的地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场中美 AI 的角力中，上一次有中国企业对英伟达这只 AI 巨兽形成冲击，并不是因为推出新款芯片，而是算法与算子带来的效率提升。2025 年 1 月 27 日，英伟达股价暴跌近 17%，单日市值蒸发高达 5888 亿美元，创下美股史上单日市值蒸发新纪录，其主要原因是 Deepseek 通过高性能算子（尤其是 DeepGEMM）这一关键技术，以 1/20 的训练成本实现了 OpenAI O1 级的性能，这成功地证明了大模型性能≠堆砌芯片性能和数量，而是取决于算法创新 + 算子优化 + 硬件适配的协同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果国产芯片厂商也能拥有足够丰富的高性能算子库和生态开发者，突破英伟达 CUDA 现有生态的桎梏，让更多的国产模型“回家”，那么对其商业帝国将产生难以估量的冲击，甚至有可能成为中美科技博弈的关键胜负手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KernelCAT 团队在让国产模型“迁移回家”的场景下做了大量尝试：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 DeepSeek-OCR-2 模型在华为昇腾 910B2 NPU 上的部署为例，让我们看看 KernelCAT 是如何重塑工作范式的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对抗“版本地狱”：KernelCAT 对任务目标和限制条件有着深度理解，基于 DeepSeek-OCR-2 官方的 CUDA 实现，通过精准的依赖识别和补丁注入，解决了 vLLM、torch 和 torch_npu 的各个依赖库间版本互锁的三角矛盾，硬生生从零搭建起了一套稳定的生产环境，结合基础 Docker 镜像即可实现模型的开箱即用。准确修补：它敏锐地识别出原版 vLLM 的 MOE 层依赖 CUDA 专有的操作和 vllm-ascend 提供的 Ascend 原生 MOE 实现，并果断通过插件包进行调用替换，让模型在国产芯片上&quot;说上了母语&quot;。实现 35 倍加速：在引入 vllm-ascend 原生 MOE 实现补丁后，vLLM 在高并发下的吞吐量飙升至 550.45toks/s，相比 Transformers 方案实现了惊人的 35 倍加速，且在继续优化中。无需人工大量介入：在这种复杂任务目标下，KernelCAT 可以自己规划和完成任务，无需研发提供大量提示词指导模型工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，原本需要顶尖工程师团队花费数周才能完成进行的适配工作，现在可以缩短至小时级（包含模型下载、环境构建的时间）；同时让国产芯片从“能跑”到“飞起”，实现 35 倍的加速。KernelCAT 让国产芯片不再是被“封印”的算力废铁，而是可以通过深度工程优化，承载顶级多模态模型推理任务的性能引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“天下苦 CUDA 久矣”——这句话曾是行业的无奈，但 KernelCAT 的出现，似乎让国产 AI 产业看到了一种新的可能。它不只是国内团队在 AI Agent 技术上的突破，更是一次对算力主权的郑重宣示：我们不再满足于在别人的地基上盖楼，而是要打好属于自己的 AI“地基”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://kernelcat.cn/&quot;&gt;KernelCAT 限时免费内测&lt;/a&gt;&quot;中，点击链接，马上体验~&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/14/140a6c0e97d8e4f35ef00ee8f4f9f40e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/JAmVx35sxdz0ubB7l0Ua</link><guid isPermaLink="false">https://www.infoq.cn/article/JAmVx35sxdz0ubB7l0Ua</guid><pubDate>Fri, 30 Jan 2026 09:46:03 GMT</pubDate><author>InfoQ</author><category>芯片&amp;算力</category></item></channel></rss>