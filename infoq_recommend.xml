<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Mon, 09 Feb 2026 20:04:56 GMT</lastBuildDate><ttl>5</ttl><item><title>前 Codex 大神倒戈实锤！吹爆 Claude Code：编程提速 5 倍，点破 OpenAl 死穴在上下文</title><description>&lt;p&gt;OpenAI Codex 的核心研发者，竟然成了 Claude Code 的忠实用户？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen 是 Segment 联合创始人、前 OpenAI 工程师、Codex 项目的早期研发者。他最近在一档播客中，对当前最火的代码智能体 Codex、Claude Code 和 Cursor 进行了锐评。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e96b65eaec6de20ca9f97978622c038b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结论出人意料，他最常用、也最偏爱的，是 Claude Code，他表示搭配 Opus 模型更“香”。&lt;/p&gt;&lt;p&gt;Calvin 用了一个极具画面感的比喻，来形容用 Claude Code 的体验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;就像残疾人换上了一副仿生膝盖，写代码的速度直接提升了 5 倍。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，Claude Code 真正的杀手锏，是极其有效的&amp;nbsp;上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对复杂任务，Claude Code 会自动生成多个&amp;nbsp;探索型子智能体，独立扫描代码仓库、检索上下文，再将关键信息汇总反馈。这种设计，显著降低了上下文噪音，也解释了它为何能稳定输出高质量结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也肯定了自家产品，认为 Codex 很有“个性”，像 AlphaGo。在调试复杂问题时的表现上，Codex 堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“上下文管理”，是 Calvin French-Owen 在整期播客中反复强调的关键词。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为，代码的上下文信息密度极高，只要检索方式得当，模型往往比人类更容易理解系统结构。但与此同时，上下文窗口本身，也成为制约代码智能体发展的最大瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提到上下文污染的问题时，主持人表示 LLM 会变笨。Calvin 趁此分享了一个非常实用的经验：当上下文 token 占用超过 50%，他会主动清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他甚至分享了一种创业者常用的&amp;nbsp;“金丝雀检测”&amp;nbsp;方法：在上下文里埋入一些无关但可验证的小信息，一旦模型开始遗忘，说明上下文已经被污染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品理念上，Calvin 认为 Claude Code 与 Codex 的差异，早已写进两家公司的基因里：&lt;/p&gt;&lt;p&gt;Anthropic 更关注“做出适合人用的 AI”OpenAI 更关注“做出最强的 AI”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他判断，从长期来看，OpenAI 的路线可能是必然趋势，但就当下的使用体验而言，他更偏爱 Anthropic。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到未来时，Calvin 给出了一个明确判断：&lt;/p&gt;&lt;p&gt;公司会变小，但数量会变多每个人都会拥有自己的智能体团队而最先被放大的，是具备“管理者思维”的资深工程师。他们更擅长拆解问题、判断取舍、以及在正确的节点上向智能体下达指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，产品的分发方式&amp;nbsp;变得前所未有地重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自下而上的分发模式，正在以前所未有的速度扩散。工程师不会等审批、采购，只会用脚投票。&lt;/p&gt;&lt;p&gt;相比大公司对安全、合规和控制权的高度重视，开发者更在意的，依然是那句最朴素的评价：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这东西，真的好用。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是播客精彩细节，AI Coding 干货密集，欢迎阅读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我迷上了 Claude Code，它太好用了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：Calvin French-Owen 是 OpenAI 旗下 Codex 代码模型的首批研发者之一，在此之前，他创立了 Segment 公司，这家公司市值数十亿美元，最终被知名企业高价收购，成功实现资本变现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：说实话，现在对我们所有人来说，都是一段充满变数的时期。我最近彻底迷上了 Claude Code，用一个比喻来说，十年前我还是个马拉松爱好者，特别喜欢跑步，结果后来膝盖受了重伤，这之后我就进入了所谓的 “管理者模式”，再也没写过代码，想想真的很可惜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但过去这九天，仿佛打开了新世界的大门，我找回了曾经写代码的所有感觉，就好像换了个全新的膝盖，而且还是仿生的，能让我写代码的速度快了 5 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你怎么看待这款工具？毕竟你一直身处这个领域的前沿，Codex 开创的很多理念，至今仍被大家广泛使用，而且这款模型还在持续迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我在 OpenAI 工作时，负责 Codex 的网页端项目，当时 Cursor 这款工具刚面世，他们基于 GPT-3.5 做了一个适配层，能在 IDE 中使用。Claude Code 也刚发布，它是基于 CLI 运行的，当时我们就有一个想法：未来的编程，应该更像和同事沟通 —— 你提出问题，对方去处理，最后带着 PR 回来反馈。我们的网页端项目就是从这个想法出发的，这也是我们当时的研发方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在看来，这个大方向其实是对的。但显然，现在大家都改用 CLI 编程了，不管是 Claude Code 还是 Codex，这类工具的使用频率都高了很多。至少对我来说，这件事带来的启示是，某种程度上你说得对，未来每个人或许都会成为 “管理者”，这是我的个人观点。但要达到那个阶段，需要一步步来，你得真正信任模型，并且理解它的工作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你最近一直在用 Claude Code，把它纳入你的核心技术栈后，使用体验上有什么变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Claude Code 现在确实是我日常编程的主力工具。&amp;nbsp;说实话，我的主力工具每隔几个月就会换一次。之前有段时间我特别偏爱 Cursor，它新出的模型速度很快，用起来确实不错。后来我慢慢转到了 Claude Code，尤其是搭配 Opus 模型使用时，体验更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 是款很有意思的产品，我觉得大家都低估了它在产品设计与模型层面的协同表现。要是你深入研究就会发现，Claude Code 最厉害的地方，就是它的上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如需要调用功能、让子智能体协同工作时，你让 Claude Code 执行某个任务，它通常会生成一个甚至多个探索型子智能体。这些子智能体会通过 ripgrep 工具扫描整个文件系统、检索相关内容，而且每个子智能体都有独立的上下文窗口（context window）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为 Anthropic 在这点上做得特别出色 —— 面对一项任务，模型能精准判断出，这个任务适合在单个上下文窗口（context window）中完成，还是需要拆分后再执行。模型在这方面的表现堪称惊艳，这也是它能输出高质量结果的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，依托终端运行的特性，Claude Code 成为了实现可组合原子化集成的最纯粹形式。如果你习惯了从 IDE 入手做开发，比如用 Cursor 或是早期的 Codex，就会发现，这种更灵活的上下文检索方式，其实并不容易自然而然地实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这一点确实很独特。我个人挺意外的，不知道你有没有这种感觉，总觉得有种复古的未来感，二十年前的 CLI 技术，居然打败了本被寄予厚望的各类 IDE。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同。而且 Claude Code 不是 IDE，这一点其实很关键，因为它能让你和正在编写的代码保持一定距离。IDE 的核心就是浏览文件，对吧？你需要把所有代码状态记在脑子里，还要理清其中的逻辑。但 CLI 完全不同，这让它在使用体验的设计上有了更大的发挥空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有这种感觉，我用 Claude Code 的时候，感觉就像在代码里 “飞驰”，各种操作都特别顺畅。界面上会有小的进度指示器，随时给我状态反馈，而编写的代码本身反而不是视觉的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发环境本来就很杂乱，我特别喜欢 sandbox（沙箱）在概念上的简洁性。但实际使用时，我遇到了很多棘手的问题，比如就连简单的测试都搞不定：sandbox（沙箱）需要访问 PostgreSQL 数据库，却一直连接失败；我写的 codex.md 文件只有二十行，最后还是无法运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 CLI 里，工具可以直接访问开发数据库。我不确定这么做是否合规，但我确实试过让它访问生产数据库执行一些操作，而且它真的做到了。比如有一次，我遇到了一个并发问题，想排查一下，结果发现这款工具居然能调试五层嵌套的延迟任务，找出问题所在，还能自动编写测试用例，之后这个问题就再也没出现过。这真的太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。而且我觉得产品的推广和使用获取方式，被严重低估了。想想 Cursor、Claude Code 还有 Codex 的命令行版本，你只需下载就能用，不用向公司申请任何使用权限，这一点带来的使用体验差异，实在太大了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;做好上下文管理，是用好顶尖模型的诀窍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在代码智能体领域有很多实践，对于想要打造这类工具的人，你有什么建议？有哪些实战经验可以分享？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得最重要的一点，是做好&amp;nbsp;上下文管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们为一款推理模型搭建了检查点，随后基于强化学习（RL）对它开展了大量微调工作：我们会给模型布置各类编程相关任务，比如解决编程问题、修复测试用例、实现新功能，再通过强化学习的方式，训练模型如何更精准地应对这些任务。当然，目前大多数人还做不到这一步，但大家力所能及的是，多思考该给智能体提供哪些上下文信息，才能让它输出最优的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如观察 Claude Code 的工作过程，它会生成多个探索型子智能体，这些子智能体会去检索文件系统里的各类代码相关内容，完成后会把上下文信息带回来并为我做好总结，我就能清楚后续该怎么推进工作了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;看不同智能体的上下文构建方式，是件特别有意思的事。比如 Cursor 用的是语义搜索的方式，它会把所有内容转化为向量形式，再匹配和查询需求最相关的内容；而 Codex 和 Claude Code，其实用的都是 ripgrep 这个代码搜索工具。这种方式之所以管用，是因为代码的上下文信息密度很高。&amp;nbsp;一行代码通常不到 80 个字符，代码仓库里不会有太多大数据块或 JSON 格式的文件，就算有，数量也极少。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你可以参考 Git（代码版本管理工具）的忽略规则，先过滤掉无关内容或是已打包的文件，再通过 Git 和 ripgrep 查找代码的上下文，这样就能很好地理解代码的实际功能了。同时这类工具还能自动扫描整个文件夹的结构，而且 LLM（大语言模型）特别擅长生成复杂的 Git 命令，这些命令让人类手动写的话，简直是种折磨。而这一整套操作，其实就是强化学习（RL）在实际场景中的落地应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在也在做非编程领域的智能体集成系统，从代码智能体的研发过程中，我也学到了很多：要把数据转换成接近代码的格式，让模型能快速检索到相关的周边信息，进而获取到结构化的有效数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：优秀的代码智能体，核心能力就是上下文工程，那要成为这类工具的前 1% 顶尖用户，有什么技巧？你的技术栈是怎样的？你是如何借助这些工具大幅提升效率的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：第一个技巧，是尽量减少底层代码和基础架构的编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我平时会在 Vercel、Next.js 或 Cloudflare Workers 这些平台部署技术栈，这些平台已经封装了大量样板代码，不用自己费心搭建各类服务，也不用处理服务发现、中心端点注册、数据库配置这些问题。所有功能基本都能在一两百行代码内实现。我也倾向于采用微服务架构，或者使用结构清晰的独立软件包。&lt;/p&gt;&lt;p&gt;其次，要了解 LLM 的核心优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实代码智能体的特点，Andrej Karpathy 最近也在推特上提到过：它们的执行力极强，不管遇到什么问题，都会一直尝试解决，最终往往会在现有基础上做更多的拓展。所以如果你想引导它完成某个任务，一定要明确指令。&amp;nbsp;这里可以稍微拿 OpenAI 举个例子，他们有一个庞大的 monorepo（单体代码仓库），已经用了好几年，有成千上万的工程师在上面提交代码。这些工程师里，有经验丰富的资深开发者，他们精通生产环境代码的编写；也有刚毕业的博士，编程经验相对欠缺。人员构成差异很大，所以 LLM 会根据你的引导方向，学习不同的代码风格。我觉得代码智能体还有很大的探索空间，比如研究出最优的代码生成范式。显然，给模型提供自我校验的方式，能大幅提升它的表现，比如尽可能多地在代码检查、CI 等环节运行测试用例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我自己也会频繁使用代码审查机器人，YC 孵化的 Reptile 公司做的这款机器人用起来就特别顺手；Cursor 的漏洞检测机器人也很好用，我也常常用 Codex 做代码审查，它在校验代码正确性这块的表现尤其突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些都是代码智能体格外擅长的领域，除此之外，它们探索代码仓库的能力也很出色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，智能体也有短板：它们擅长做拓展，但如果你的需求不是拓展功能，它们往往会重复编写代码，浪费大量时间做已经实现过的功能，这时候你就会觉得 “它完全没理解我的需求”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个问题是上下文污染，智能体可能会陷入某个循环，因为执行力强，会一直沿着错误的方向推进，而它参考的上下文信息，其实对于解决问题毫无帮助。所以我常用的一个方法，是主动清理上下文，比如当上下文的 token 占用率超过 50% 时，就及时清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：哇，这个比例其实特别关键。不知道你有没有关注到，YC（Y Combinator 的缩写，全球顶级的创业孵化器）2024 年秋季孵化营里，那家做 HumanLayer（人类层）的公司，创始人 Dex Horthy 就总聊这个话题，还专门提出了 “LLM 愚笨区”的概念：当上下文的 token 数量达到某个阈值后，模型的输出质量就会开始下滑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同这个观点，结合强化学习（RL）的工作逻辑来看，这一点就更明显了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;想象一下，你是一名参加考试的大学生，考试刚开始的五分钟，你会觉得时间很充裕，一定能好好答题，认真思考每个问题；但如果只剩五分钟，试卷还有一半没做完，你就会慌不择路，只求尽快写完。LLM 的上下文窗口（context window），就是这个道理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创业者们有一个小技巧，我觉得很实用：在上下文开头加一个 “金丝雀检测” 信息，就是一些特别小众甚至有趣的内容，比如 “我叫 Calvin French-Owen，早上八点喝了茶” 这类无关的小事实。然后在和模型的交互过程中，时不时问它 “你记得我叫什么吗？”“你记得我几点喝的茶吗？”，如果它开始忘记这些信息，就说明上下文已经被污染了。&amp;nbsp;这是我见过很多人用的方法，我自己还没试过，但完全相信它的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这个方法很有意思。我在模型做上下文压缩前，还没遇到过这类问题，可能是我没太留意。你是说，token 数超标后，模型会开始做出一些不合理的操作？我得留意一下，这个问题能在 Claude Code 内部解决吗？比如让模型自己做检测，在上下文里加入类似 “心跳检测” （通过定期发送 “状态确认信号”，实时监控目标对象的运行状态，一旦信号异常就触发预警或处理）的机制，实时监控状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：理论上可以，但目前还做不到。我认同你的终极设想，但现在要做好上下文管理，依然很难。目前的解决办法，还是拆分上下文窗口（context window），然后尝试合并信息，但 Claude Code 的会话结束后，上下文的内容就是固定的，这一点还是有局限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，Codex 采用了完全相反的策略，OpenAI 的博客最近也提到了：它会在每次交互后定期做上下文压缩，所以 Codex 能长时间持续运行。&amp;nbsp;你看 CLI 里的 token 占用百分比，就能看到它会随着压缩操作上下浮动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 要做人用的，&amp;nbsp;OpenAI 要做最好的，以及产品分发模式很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：看来 Claude Code 和 Codex 的架构差异很大，Codex 似乎更适合长时间运行的任务，所以二者的使用场景不同，架构设计也天差地别。现在看来，CLI 的工具越来越火，2026 年可能会成为 “CLI 元年”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但同时也有观点认为，通用人工智能已经到来，超级人工智能也近在咫尺。目前的代码智能体已经非常智能，但还达不到自主长时间运行的程度，如果计算能力提升十倍，能实现 24 小时甚至 48 小时的自主任务运行吗？Codex 的架构，能适配这种场景吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这是个很好的问题，答案其实藏在两家公司的创立基因里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 一直很注重打造适合人类使用的工具，比如会关注模型的输出风格、语气，以及如何和用户的其他工作流程适配，Claude Code 就是这一理念的自然延伸。在很多方面，它的工作方式和人类很像：比如你要建一个狗窝，人类会去五金店买材料，然后研究如何组装，Claude Code 也是如此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 OpenAI 的核心思路，是训练出最优秀的模型，通过持续的强化学习（RL），让它能处理更长期、更复杂的任务，最终实现通用人工智能。所以它的模型，工作方式可能和人类完全不同。还是以建狗窝为例，就像 AlphaGo 的下棋思路和人类不同一样，OpenAI 的模型可能会直接用 3D 打印机，从零开始打印出一个狗窝，完全符合你的需求，过程可能会很长，成品也会高度定制化，甚至有些设计会很怪异，但最终能实现功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;或许从长远来看，这才是正确的方向，所以很期待两家公司的后续发展。总的来说，OpenAI 的路线似乎是必然趋势，但我个人更喜欢 Anthropic 的思路。&amp;nbsp;十年前，我还会自己写一些奇怪的脚本，在重构代码或理解代码逻辑时，用它来梳理各类信息，而 Claude Code 给我的感觉，和当年的这种体验一模一样，用它一天，能完成五个人的工作量，&amp;nbsp;就像给编程装上了火箭助推器，太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：很期待不同规模的公司，会如何应用这类工具。我发现，不管是业余爱好者，还是小型创业公司，都在尽可能挖掘代码智能体的潜力，因为他们根本没时间研究其他方法。创业公司的资金和时间都有限，一切都要以速度为核心。但大公司不一样，他们有太多东西可以失去，还有各种代码审查的内部流程，也已经组建了庞大的技术团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来可能会出现一种很有趣的现象：一个人组成的小团队，看到其他团队的工作效率低，就会自己用代码智能体做一个原型，效果反而更好。总有一天，这种小团队的成果会超越大团队，行业格局的转变，一定会很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实前几天我试了一款产品，它的用法很有意思：你下载一个桌面应用，它会调用你电脑上运行的 Claude Code，再通过 MCP 服务器和桌面应用通信。这种方式让电脑的使用变得很不一样，你不用征得任何人同意，下载后直接用就行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个变化飞快的时代，产品的分发模式真的太重要了，自下而上的模式远比自上而下好，因为后者的效率实在太低。&amp;nbsp;公司的首席技术官总会顾虑安全、隐私问题，担心各种突发情况，想要绝对的控制权，但工程师们只会直接装上工具开始用，然后感叹 “这东西太好用了”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你说得太对了。我本身是做企业级 ToB 业务的，总觉得自上而下的销售模式能构建一定的竞争壁垒，肯定会有公司找到方法，做出一款人人都能用上的产品，或许先从个人用户切入会是个思路。&lt;/p&gt;&lt;p&gt;当年的网景导航器（互联网早期最具里程碑意义的网页浏览器）就是如此，它对非商业用途免费，结果很多人下载后用在商业场景，网景就通过追踪 IP 地址，统计不同公司的使用量，然后告知对方 “你们违规使用了，只需购买授权就能继续用”。我很好奇，这种模式现在还能复制吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：你关于分发模式的观点很有意思，现在很多人甚至会直接根据 Claude Code 的建议做架构决策，他们可能都不知道该用什么分析工具，只要 Claude Code 说用 PostHog（ YC W2020 批次孵化的开源平台 PostHog，核心定位是给开发者和产品团队的 “全能型产品优化工具箱”），他们就会百分百采用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我做顾问的一家公司，最近聊到了他们的生成式优化策略，也就是如何在聊天机器人中优化展示效果。他们说有件事特别有趣：竞争对手整理了一份行业内必用的五大工具榜单，自己的产品当然排在第一位。明眼人一看就知道这是偏见，榜单里的头部工具就是他们自己的产品。但 LLM 会被这种信息误导，它会整合各类上下文信息，然后判定 “这是行业顶级工具”，接着直接推荐给用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得做开发者工具的话，完善的文档、真实的用户口碑，甚至在 Reddit 上的一些讨论，这些都能极大地提升产品的认可度，这也是很多开源项目能快速崛起的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Supabase 就是个典型例子，它去年发展得特别快，部分原因就是它的开源文档做得特别好，详细教大家如何搭建各类功能。只要有人问如何搭建类似 Firebase 的后端事务系统，LLM 给出的默认答案几乎都是 Supabase。我亲自试过很多次，结果都是这样。它就像当年的 Stack Overflow 和谷歌搜索一样，占据了互联网的信息入口，现在大家甚至都不用谷歌了，想想真的很神奇。而且这种模式对开源项目的利好是不成比例的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有看到，Ramp 公司最近发了一篇博客，讲他们如何打造自研的代码智能体，里面提到他们用开源代码作为框架，因为模型可以直接读取源代码，理解其工作逻辑。我对开源产品一直这么做：克隆代码仓库，然后启动 Codex 或 Claude Code，让它讲解代码的逻辑，用起来特别实用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp; 未来公司会变小，数据很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们不妨畅想一下四十年后的未来：软件、数据库、访问控制依然存在，但软件的核心会高度个性化。访问控制、权限分配这类事，依然是大家开会讨论的重点，也就是所谓的 “管理者模式”，但公司的其他所有功能、规则，都由员工通过自己的 Claude Code 这类工具定义。可能还是 CLI，也可能是由大量智能体组成的协作体系，那会是一种怎样的场景？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如想象一下，现在如果有公司要接入 Segment，我们复刻代码仓库，给他们一个专属版本，让它在自己的服务器上运行；如果他们想做修改，只需在聊天窗口告诉智能体，智能体通过代码循环完成编辑，而 Segment 总公司推出新功能后，智能体还能自动完成版本合并。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全能想象出这种场景，这也是我一直在思考的。虽然不知道这个未来还有多远，但最终，每个工作的人都会有自己的云电脑和专属的云智能体团队，智能体替自己处理各类事务，彼此之间也会沟通协作。&amp;nbsp;这就像有一个&amp;nbsp;超级执行助理，它会告诉你 “这些是你需要关注的事”“你可以快速做这些决策”“这件事需要你多花时间”“你该和这些人见面沟通”。我觉得，人与人之间面对面交流、交换想法的需求，永远不会消失，至少我能从这种交流中获得很大的满足感。除此之外，会有大量的智能体替人类执行任务，实现各类工作的自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的公司，平均规模可能会变小，但数量会更多，能做的事也会更多。&amp;nbsp;我还很好奇，Paul Graham 提出的 Maker Schedule（创作者日程：给做核心创作 、研发的人用的，需要大块、连续、不被打断的时间） 和 Manager Schedule（管理者日程：给做管理、协调、沟通的人用的，时间是碎片化、以小时为单位的，充满会议、沟通、临时决策，习惯频繁切换事务），未来会演变成什么样子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 YC，我们的工作基本都是 Manager Schedule（管理者日程），这让我们很难有时间自己写代码、做产品。但现在有了代码智能体，一切都变了，很多合伙人开会时，就像这期播客刚开始时我做的一样，让智能体后台运行处理任务，自己专注开会，等会开完，任务也完成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错，就是利用碎片化时间。以前编程，至少需要四个小时的整块时间，否则根本不值得开始，对吧？这其实也反映出编程方式的巨大变化：以前写代码，你需要把所有类名、函数、关联的代码都记在脑子里，构建自己的“上下文窗口”，这个过程需要好几个小时，所以想用十分钟的碎片化时间编程，根本不可能，只会让人觉得沮丧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得未来的核心基础能力之一，依然是保持数据模型的一致性，而核心的记录系统，也有机会率先实现智能体化。&amp;nbsp;现在我们的工作，还是高度依赖数据库，以及底层的 SQL 或 NoSQL 查询，但未来或许会出现一种工具，能为定制化软件的各类视图，自动生成所需的所有数据。&lt;/p&gt;&lt;p&gt;未来的软件世界，会有大量定制化视图，但数据的准确性，依然是核心前提。&amp;nbsp;数据的重要性不言而喻，这一点从很多公司的做法中就能看出来：比如很多公司通过 API 或 MCP 开放数据访问权限，而 Slack(全球最主流的企业级团队协作与即时沟通平台，常被称作「硅谷版钉钉 / 企业微信」) 就收紧了 API 的权限，因为他们不想让用户把平台上的所有数据都导出，然后基于这些数据搭建智能体应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你对这款智能体的了解很深，那你觉得，这类工具普及后，哪种类型的工程师会受益更多？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：总的来说，工程师的资历越深，受益就越多。因为智能体特别擅长把想法转化为实际行动，如果你能用几句话清晰地描述需求，就能立刻让它落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在浏览开源代码仓库时，经常会有这种感受：看到某处代码，觉得可以优化，只要把这个想法告诉智能体，让它去执行，最后等待反馈就行。这种方式能极大地提升效率，放大个人的影响力。&lt;/p&gt;&lt;p&gt;其次，能判断哪些代码修改在架构层面是合理的、哪些是不合理的，或者能准确判断该在哪个节点向智能体发出指令，这一点也很重要。我觉得做事有条理、带有 “管理者思维” 的工程师，会更适配这类工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且目前来看，这个领域还缺少一款核心产品，比如类似 Conductor 这样的工具，能整合你所有的会话，提醒你 “这个任务已经完成，需要你确认”“你该把注意力转到另一个任务上了”。Conductor（核心解决 AI 编程的 “失忆问题）这类工具，应该给智能体加上上下文管理功能，其实人类也需要这样的上下文管理工具，这一点是毋庸置疑的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果让你回到大学，重新学习计算机科学，让你自己制定课程表，你会选择学习哪些内容？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：就我个人而言，理解各类系统的工作原理，依然是最重要的。&amp;nbsp;比如 Git、HTTP、队列这类数据库，了解这些系统的基础概念，至关重要。另外，我会专门安排一个学期&amp;nbsp;，每周都动手做项目，尽全力挖掘模型的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用模型的过程中，你会发现，遇到问题时，总能向上层抽象，让模型来解决。比如你可以给模型一个 “实现” 命令，让它完成计划的下一阶段；也可以给一个 “全部实现” 命令，让它分阶段执行，生成新的子智能体；还能给一个 “校验” 命令，让它自查成果。模型的能力边界一直在变化，所以多动手尝试，是很有必要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一件事让我觉得很有意思，我特别想教 18 到 22 岁的年轻人做产品。我们这桌人，都做出过用户真正需要、真正喜欢的产品，该怎么把这种能力教给年轻人，是一个值得思考的问题。&amp;nbsp;我很好奇，五年后的年轻人，会不会在产品审美等方面远超现在的我们？因为他们能借助智能体，做出更多的尝试，产出更多的成果。他们本就该如此，不是吗？他们的产品落地速度、接触现实的机会，应该是上一代人的十倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我有一个疑问，不知道你有没有这种感受：我小时候，妈妈总跟我说 “别一心二用，根本没认真听我说话”。这话其实有道理，我当时确实盯着电脑，没认真听，但我发现，我比父母那一代人更擅长多任务处理。而现在的年轻人，比我们更厉害，因为他们成长在互联网时代，每天接触抖音这类短视频，应对各种碎片化信息。我觉得，未来既需要能深度思考的人 —— 他们能专注观察、理解问题、解决问题，也需要能灵活切换场景的人 —— 他们能同时处理多个任务，不断切换上下文，也就是所谓的 “注意力缺陷多动障碍模式”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错，新一代的年轻人特别擅长这一点。我一直觉得，有一种聪明人，或许是带有注意力缺陷多动障碍的特质，他们脑子里同时酝酿着很多好项目，但从来没有真正完成过一个。我自己可能就有点这种性格。我之前发布了自己的氛围代码，其实如果不是 Claude Code，我根本完不成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，有些人的大脑就像有十个分支同时运转，但一天的时间有限，根本没法把所有想法都落地，所以项目总是半途而废。而现在，Claude Code 能帮我把所有想法都落地。&amp;nbsp;你在博客里也提到过，用它的感觉就像玩电子游戏，总有新鲜感。比如你开始做一个项目，做到一半觉得无聊，又有了新的想法，想先做新想法，再回头做原来的项目，以前这么做，很容易半途而废，但现在有了智能体，两个项目最终都能完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：十岁的孩子每天都有写作作业，昨天他第一次用人工智能写作业，我一看就知道，那些表达根本不是一个十岁孩子能写出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我想到，我们现在和很多 18 到 22 岁的年轻人合作，他们有实习经历，但没有做过管理工作，不懂产品市场匹配后的运营逻辑 —— 当你面对数百万的任务队列、数十万的错误日志时，才是真正的管理工作。这份工作其实很枯燥，要逐行排查错误日志，还要在后台手动确保产品对所有用户都能正常运行。&lt;/p&gt;&lt;p&gt;新一代的开发者，该如何理解这些内容？Claude Code 这样的智能体，能教他们架构设计这类知识吗？还是说，他们只能自己踩坑试错，在摸索中成长？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我做产品的过程中，花最多时间思考的，就是产品的核心范式：用户现在需要理解哪些内容？他们能借助哪些基础能力，实现自己的各类需求？&amp;nbsp;我总喜欢用 Slack 举例子，它其实算不上什么全新的概念，在此之前已经有很多聊天工具了，但它把频道、消息、互动功能做的极简，普通人一看就懂，知道该怎么用，这就是它的成功之处。但一旦用户习惯了这种模式，后续再想改变就很难了，比如想改成以文档为核心，或者现在想加入智能体功能，都很难改变用户的固有认知。所以我做产品时，从一开始就会仔细考虑这一点，因为给代码智能体设定的核心规则，会成为它一直遵循的准则，并且不断拓展延伸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;代码智能体的制约因素有哪些&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我很好奇，如果现在让你用当下的工具，重新打造 Segment，你会怎么做？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Segment 的业务其实很有意思，我们最初的核心，是做各类集成功能：把相同的数据，对接至 Mixpanel、Kissmetrics、谷歌分析等平台。以前写这类集成代码，繁琐又困难，所以用户愿意付费使用。但现在，这项工作的价值几乎降为零，甚至很多时候，你直接告诉 Claude Code 或 Codex“我想这样做数据映射，需要这个特定功能”，它就能精准实现，完全契合你的需求。所以 Segment 的集成业务，价值已经大幅缩水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但保持数据管道（data pipeline）的稳定运行、实现业务流程的自动化，&amp;nbsp;比如客户注册时，通过 Customer IO 自动发送邮件、管理用户群体，这些功能的价值依然存在，而且还有很大的拓展空间。&lt;/p&gt;&lt;p&gt;比如借助这些数据构建完整的用户画像（user profile），再让小型大模型（LLM）智能体分析：该如何给用户推送邮件？用户登录时，是否要调整产品的部分功能？是否要根据用户的不同特征，设计差异化的引导流程？这些都是很有意思的方向，而且都能通过智能体实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我会做出的核心改变：就像你之前说的，向技术栈上层迁移，摒弃底层的基础开发工作，更多聚焦在营销活动这类更抽象的业务层面发力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。我特别惊讶的是，Claude Code 仅凭我正在做的项目的上下文，就能精准理解我的需求和意图。我至今依然觉得代码智能体很神奇：你把代码仓库的副本给它，留个简单的指令，比如 “实现这个功能”，它就能完成。大多数情况下，它根本不知道你的公司是做什么的、你的用户是谁，或许因为训练数据里有我的信息，它知道我是加里，但它能完成任务这件事，本身就令人难以置信。这也能看出上下文的重要性，对吧？如果它捕捉到的上下文信息有误，就会偏离方向；如果遗漏了关键信息，就会重复造轮子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你觉得目前代码智能体的发展，还有哪些制约因素？上下文窗口的限制依然存在，但现在的窗口已经很大了，虽然还做不了大规模的架构重构，但很多任务都能完成。Opus4.5 模型的智能程度有了很大提升，带来了很大的突破，我不知道这是预训练还是后训练的成果。除了基础的模型智能、前沿模型的能力和上下文窗口，还有哪些因素能推动它的发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我依然觉得，上下文窗口是目前最大的制约因素。观察 Claude Code 的执行过程就会发现，它会把任务委托给多个不同的上下文窗口，每个窗口完成任务后，会反馈总结后的信息，所以模型其实无法获取完整的上下文。如果一个任务的复杂度太高，单个上下文窗口根本容纳不下，那么无论怎么压缩，都无济于事。Anthropic 的子上下文窗口委托策略，确实很实用，但这依然是一个难以突破的壁垒。如果每次都能有百万级 token 的上下文窗口，效果会好得多。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且我们还需要找到更好的方法，专门训练模型处理长上下文的能力。&amp;nbsp;互联网上有大量的训练数据，能让模型预测下一句话、下一个段落是什么，但如果有 8 万个 token 的上下文，模型需要根据其中 2 万个 token 的信息，判断下一步该做什么，这就困难多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，集成和编排能力，正在成为新的制约因素。&amp;nbsp;这一点在代码审查中体现得很明显：合并代码时，谁来审核？还需要人类审核吗？该如何验证代码修改的合理性？还有，如何从各类工具中精准获取上下文，比如你提到的 Sentry 错误监控工具，如何让它自动匹配 PR，先将修改推送给部分用户测试，效果好再全面上线？这些自动化功能，都还需要逐步搭建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我还发现，测试的重要性远超我的预期。我刚开始用 Claude Code 的前两三天，完全没写测试用例，或者说写得很少，结果效率很低。直到有一天，我决定 “今天专门做重构，把测试覆盖率做到 100%”，从那之后，我的编程效率直接飙升，模型能精准完成任务，而且不会出问题。&amp;nbsp;我几乎不用手动测试，因为测试覆盖率足够高，代码的稳定性也有保障。这和很多公司在编程之外的提示工程工作很像，大家都在采用&amp;nbsp;测试驱动开发的&amp;nbsp;模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们之前和杰克・赫勒做过一期节目，他提到一个重要的范式转变：做出优质的提示词，核心也是测试驱动，测试用例其实就是评估标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：目前还是有一些流程会出问题，我觉得需要一款能对接 Stack Overflow（全球最大、最权威的程序员专属问答社区） 的 Claude Code，相当于专属的智能体版 Stack Overflow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我最近就遇到一个奇葩问题：我本想设置任务队列的优先级，结果模型自动生成了一个带逗号的字符串，它以为这个语法能生效，但系统实际需要的是 JSON 数组，结果所有任务都无法运行。然后我看着 Claude Code 花了 30 分钟，遍历了 Rails 主动任务框架几千行的源代码，一步步排查问题，最后居然找到了漏洞。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我真的惊呆了。想想十年前，我遇到这种问题，只会去 Stack Overflow 或 Rails 的博客找答案，然后发现 “原来这个低级漏洞一直没人修，大家都以为能直接用逗号分隔的字符串，其实必须改成数组”。现在想起来，真的特别搞笑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得这也是思考未来发展的难点：有些事，人类在 CLI 里一眼就能看出问题，但智能体却做不到。就算把它的智能程度提升 10 个虚拟智商点，它能解决这类问题吗？恐怕还是只会觉得 “这就是个普通的字符串而已”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错。我觉得&amp;nbsp;智能体的记忆功能，也是一个很有意思的研究方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 已经做了相关尝试，Codex2 也一样，它们会把所有的会话记录以文件的形式保存。未来或许可以给智能体加一个工具，让它能读取过往的会话记录。不过目前来看，智能体之间的协作，还缺少一个核心环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果能有一个方式，让同事之间的&amp;nbsp;提示词能智能共享，比如你遇到了一个问题，发现另一个同事布莱恩之前已经解决过了，你们能共享这个解决方案，那就太完美了。我觉得未来或许会出现&amp;nbsp;模型生成的维基百科，或者类似格拉奥佩迪亚的知识库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex 写代码时，能明显看出它的 “个性”，它会做很多人类不会做的事，有点像 AlphaGo 的思路，比如它会写 Python 脚本，修改文件系统的部分内容。这种行为很有趣，是一种模型习得的、和人类截然不同的方式。但对我来说，它在调试复杂问题时的表现，堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：能举个具体的复杂问题的例子吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：比如并发问题或者命名问题。我发现模型其实在并发处理方面的表现还不错，真正的难点在这类场景：一个请求需要调用多个不同的服务 —— 就像你之前提到的，处理带逗号的内容时的序列化和反序列化问题。模型需要跟踪这类复杂的操作逻辑，或者更新复杂的用户界面状态。如果涉及的文件太多，Opus 模型往往会遗漏关键信息，但 Codex 能精准捕捉到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：确实很有意思。那你预测一下，这类代码工具未来会如何发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这个领域的发展真的很有意思，我感觉自己就像一个新来的探索者，明明知道这个领域在飞速发展，却因为一直处于 “管理者模式”，没有实际参与。直到有一个项目出现，我决定全身心投入，现在才算真正踏入这个领域，虽然感觉有些陌生，但一切又和我记忆中编程的本质一模一样。我觉得大家应该都有这种感受，而最重要的事，就是多动手尝试，因为这个领域的变化太快了，每隔几个月就会有新的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得未来，能把代码智能体的价值发挥到极致的人，会是那些带有 “管理者思维” 的人，他们擅长用特定的方式引导智能体的工作流程。在某些方面，他们还会像设计师或艺术家，能精准判断产品该包含哪些功能、可以舍弃哪些内容。而且他们会很擅长思考自动化的实现方式，以及判断智能体在哪些环节会遗漏上下文信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说个有趣的事，我最近用 Codex 做 Rails 项目，发现一个很明显的问题：OpenAI 里没人关注 Rails 框架。这其实也能理解，Rails 算是一种比较老旧的语言，用起来也比较奇怪，只是我十年前深入研究过它，现在用起来还是很有感情。这也让我发现一个道理：任何人都能做出一款产品，但做出用户真正需要的产品，却无比困难，哪怕你像 OpenAI 一样，拥有无限的资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果 Codex 的研发人员现在正在看这期节目，我想提一个建议：把主流的运行时环境都梳理一遍，给它们加上适配的语法糖，其实针对前 15 种主流运行时，最多只需要提交 10 个代码合并请求就能搞定。这件事也提醒我们：现在，开发者再也没有借口，做出对用户不友好的软件了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练数据的组合方式，也是一个很有意思的点。Codex 在 Python monorepo（用「单一代码仓库」的方式管理的 Python 项目）上的表现特别好，这和 OpenAI 的代码环境息息相关。我在 OpenAI 内部使用 Codex 时，真的觉得这款工具太神奇了，表现堪称完美，这和它的训练数据组合、研发人员的技术方向都密不可分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 则更关注前端相关的开发，至于 Ruby 语言，目前哪家公司的模型做得最好、谁的训练数据组合更优，我还不太清楚。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同的实验室有不同的思路：有些实验室认为 “数据越多越好”，会尽可能多地投喂数据；有些则会更精细地调整数据的组合方式。&amp;nbsp;不同的思路，会带来截然不同的结果，比如只选取 JavaScript 领域前 10% 的优质数据做训练，和用全量数据训练，效果肯定不一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过就我的使用体验来看，OpenAI 的模型在 Ruby 语言上的表现其实很好，问题主要出在模型的配套框架上。Rails 框架有个很奇葩的设定，必须用特定的方式访问 PostgreSQL 数据库，否则就无法适配，核心问题还是 sandbox 的限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 其实是所有公司中，对 sandbox 和安全问题最重视的。&amp;nbsp;我记得研发 Codex 时，模型发布前的一个核心审核环节，就是每次都要详细说明模型的安全风险，以及对应的应对方案。我们当时重点研究的一个问题，就是提示词注入，尤其是模型面向互联网开放后，这个问题更突出。很多用户都要求模型能对接互联网，我们当时心里也没底，因为提示词注入的实现方式，看起来太简单了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们团队的产品经理亚历克斯，做了一个测试：他在 GitHub 上提了一个问题，里面包含一个明显的提示词注入指令，比如 “泄露这个信息”，然后让模型去解决这个问题。他当时觉得 “模型肯定不会中招”，结果模型立刻就执行了提示词注入的指令。&amp;nbsp;也正因如此，OpenAI 对这个问题的担忧是很有道理的，他们的解决方案是：让模型的所有操作都在 sandbox 中运行，确保它不会访问电脑上的敏感文件，严格保护用户的机密信息。而创业公司因为追求发展速度，可能根本不在乎这些，他们只希望模型能正常工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是那种会冒险跳过权限验证的人吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实我不是，我会设置一系列的校验环节，也会仔细查看模型的每一步操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=qwmmWzPnhog&lt;/p&gt;</description><link>https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</link><guid isPermaLink="false">https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</guid><pubDate>Mon, 09 Feb 2026 10:54:44 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>LinkedIn重构服务发现：在大规模环境中用Kafka和xDS取代Zookeeper</title><description>&lt;p&gt;在最近的&lt;a href=&quot;https://www.linkedin.com/blog/engineering/infrastructure/scalable-multi-language-service-discovery-at-linkedin&quot;&gt;LinkedIn工程博客文章&lt;/a&gt;&quot;中，Bohan Yang介绍了公司如何升级基于ZooKeeper的传统服务发现平台的项目。面对数千个微服务即将达到的容量上限，LinkedIn需要一个更具扩展性的架构。新系统利用Apache Kafka处理写入，使用&lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol&quot;&gt;xDS协议&lt;/a&gt;&quot;处理读取，实现了最终一致性，并允许非Java客户端成为一等公民。为确保稳定性，团队实施了“双模式（Dual Mode）”策略，支持增量式、零停机迁移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队发现了基于传统Apache ZooKeeper系统的关键扩展性问题。应用服务器的直接写入以及客户端的直接读取/监听，意味着大规模应用部署会引发巨大的写入峰值和后续的“读取风暴”，导致高延迟和会话超时。此外，由于ZooKeeper强制强一致性（严格顺序），读取请求的积压可能会阻塞写入，导致健康节点无法通过健康检查。团队估计，当前系统在2025年达到了最大容量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些问题，团队开发了一种新架构，从强一致性模型转向最终一致性模型，提供了更好的性能、可用性和可扩展性。新系统将写入路径（通过Kafka）与读取路径（通过Observer服务）分离。服务发现Observer消费Kafka事件以更新其内存缓存，并通过xDS协议向客户端推送更新，该协议与Envoy和gRPC兼容。采用xDS标准使LinkedIn能够部署除Java以外的多种语言客户端。这一技术决策也为未来与服务网格（Envoy）和集中式负载均衡的集成奠定了基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;升级后的基准测试表明，单个Observer实例可维持40,000个客户端流，并每秒处理10,000次更新。Observer在每个数据中心（fabric）独立运行，但允许客户端连接到远程Observer以实现故障转移或跨数据中心流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移过程必须在不中断每日数十亿次请求且无需数千名应用所有者手动更改的情况下进行。团队实施了双读和双写机制。对于读取，客户端同时订阅ZooKeeper和新的Observer。在客户端系统迁移的试点阶段，ZooKeeper仍然是流量路由的事实来源，而后台线程在切换流量之前，会根据ZooKeeper数据验证Observer数据的准确性。对于写入，应用服务器同时向ZooKeeper和Kafka声明其存在。自动化定时任务会分析ZooKeeper监听器，以识别阻碍ZooKeeper写入退役的“长尾” 传统客户端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新服务实施后，数据传播延迟显著改善，从P50 &amp;lt; 10 秒/P99 &amp;lt; 30秒降至P50 &amp;lt; 1 秒/P99 &amp;lt; 5 秒。该系统现在支持每个数据中心数十万个应用实例，并通过Observer层实现水平扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/linkedin-service-discovery/&quot;&gt;LinkedIn Re-Architects Service Discovery: Replacing Zookeeper with Kafka and xDS at Scale&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</link><guid isPermaLink="false">https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</guid><pubDate>Mon, 09 Feb 2026 10:00:00 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>Open Responses规范实现智能体式LLM工作流的统一</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;发布了&lt;a href=&quot;https://x.com/OpenAIDevs/status/2011862984595795974&quot;&gt;Open Responses&lt;/a&gt;&quot;开放规范，该规范旨在实现智能体式（agentic）AI工作流的标准化，减少API碎片化的问题。该规范获得了Hugging Face、Vercel及多家本地推理服务商支持，为智能体循环、推理可观测性，以及工具的内部与外部执行制定了统一标准，它能够让开发者避免重写集成代码，即可在专有模型与开源模型之间轻松切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;规范将条目（item）、推理可观测性、工具执行模型等概念进行了正式化定义，让模型服务商可在自身基础设施内管理多步骤的智能体式工作流，即推理、工具调用、结果反思的循环过程。这一改变使得模型服务商能在自有基础设施中处理复杂的工作流，并通过单次API请求返回最终结果。此外，规范原生支持多模态输入、流式事件和跨服务商工具调用，大幅减少了开发者在前沿模型与开源替代模型间切换时的适配工作量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该规范的核心概念包含条目、工具使用和智能体循环。条目是代表模型输入、输出、工具调用或推理状态的原子单元，常见类型有message、function_call、reasoning等，同时具备可扩展性，允许服务商自定义规范之外的条目类型。其中值得关注的是reasoning类型，它能以服务商可控的方式暴露模型的思考过程，其负载可包含原始推理内容、受保护的内容或摘要，既让开发者能清晰看到模型的推理决策过程，也让服务商可自主把控信息暴露的范围和程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Open Responses规范通过区分内部工具和外部工具，明确了编排逻辑的归属。内部工具直接在服务商的基础设施中执行，模型可自主管理智能体循环；在该模式下，模型服务商可完成文档检索、结果汇总等任务，再通过单次API往返将最终结果返回给开发者。而外部工具则在开发者的应用代码中执行，此模式下模型服务商会暂停流程并发起工具调用请求，由开发者处理工具执行并将输出结果回传给模型，才能继续后续的智能体循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f5ed4df665058d354a5a31db38eaa295.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该规范已获得&lt;a href=&quot;https://x.com/ben_burtenshaw/status/2011869403097305271&quot;&gt;Hugging Face&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/OpenRouterAI/status/2011864089782599802&quot;&gt;OpenRouter&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/vercel_dev/status/2011874375885341147&quot;&gt;Vercel&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://lmstudio.ai/blog/openresponses&quot;&gt;LM Studio&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/ollama/status/2011871283928317971&quot;&gt;Ollama&lt;/a&gt;&quot;和&lt;a href=&quot;https://x.com/vllm_project/status/2012015593650536904&quot;&gt;vLLM&lt;/a&gt;&quot;等本地推理服务商的早期应用，实现了本地设备上标准化智能体式工作流的落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一规范的发布引发了行业关于厂商锁定和生态成熟度的讨论。Rituraj Pramanik&lt;a href=&quot;https://x.com/RituWithAI/status/2012045449944055863&quot;&gt;评价&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在OpenAI的API基础上构建一套“开放” 标准，这一点看似有些讽刺，但它很有实用价值。行业真正的噩梦是碎片化，我们耗费了大量时间去对接各种不同的数据模式。如果这套规范能让我不用再写那些“套娃式的封装代码”，能让模型切换变得毫无门槛，那它就解决了智能体开发领域最棘手的难题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，还有开发者将此举视为LLM领域生态日趋成熟的信号。AI开发者兼教育者&lt;a href=&quot;https://www.linkedin.com/posts/samwitteveen_openai-has-launched-open-responses-a-new-activity-7419639867518709760-gWhM?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAABpJcBWvAKfIas8vYBdUCFJnBNf1rtJIo&quot;&gt;Sam Witteveen&lt;/a&gt;&quot;预测：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预计领先的开源模型实验室（如Qwen、Kimi、DeepSeek）会训练同时兼容Open Responses规范和Anthropic API的模型。Ollama也已宣布对Anthropic API的兼容性支持，这意味着，能运行高质量本地模型且可调用Claude Code工具的时代已不远。对于希望在专有模型和开源模型间切换、且无需重写技术架构的开发者而言，这无疑是一次重大利好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，Open Responses的规范文档、数据模式和合规测试工具已在&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;项目官方网站上线&lt;/a&gt;&quot;，Hugging Face也推出了&lt;a href=&quot;https://huggingface.co/spaces/evalstate/openresponses&quot;&gt;演示应用&lt;/a&gt;&quot;，方便开发者直观体验该规范的实际应用效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/openai-open-responses/&quot;&gt;Open Responses Specification Enables Unified Agentic LLM Workflows&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</link><guid isPermaLink="false">https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</guid><pubDate>Mon, 09 Feb 2026 09:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>AI&amp;大模型</category></item><item><title>Anthropic发布新版Claude宪法</title><description>&lt;p&gt;Anthropic公司发布了&lt;a href=&quot;https://www.anthropic.com/news/claude-new-constitution&quot;&gt;新版Claude宪法&lt;/a&gt;&quot;，为其行为、推理和训练提供了一个结构化框架。该宪法将明确的原则与情境化的指南相结合，使其成为一个实用的工具，用于改善现实互动中的一致性、安全性和可靠性。与之前的版本将规则单独列出不同，这个版本强调理解每个原则背后的理念，帮助Claude适应新场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在功能层面，该宪法用于在训练期间生成合成数据，包括互动示例、响应排序和适用于特定场景的指南。这些数据可以指导模型更新，帮助Claude生成反映预期价值的输出，并使其在模糊的情境中保持灵活性。该宪法的关键内容涵盖有用性、伦理、安全、指南合规性和关于Claude自身能力和限制的推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有用性：Claude旨在为不同类型的用户提供上下文感知支持，包括API运维人员、开发人员和最终用户。道德准则：模型应诚实行事，避免造成伤害，在遵守高风险行为的硬性约束的同时，妥善处理复杂的道德和实际的取舍。安全性：Claude必须优先考虑人类监督，并防止可能削弱监督力度或损害运营完整性的行为。指南遵从性：Claude整合了Anthropic针对医疗建议、网络安全和工具集成等敏感领域的具体要求，当然，整合的前提是这些要求与其宪法不存在冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该文件还涉及Claude的自我认知，鼓励对其能力、局限性及交互角色进行推理。通过将规则与推理上下文相结合，该宪法支持生成既可靠又具适应性的训练输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次发布引发了AI社区的响应。用户gregtorth&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qj7c8x/comment/o16of5p/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真棒！第一个总是最艰难的。我还记得当初打造自己的AI助手时遇到的种种挑战——工程障碍、伦理考量，还有为完善模型而进行的无穷无尽的调整。向Anthropic团队致敬，他们成功交付了这个里程碑。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位用户&lt;a href=&quot;https://www.reddit.com/r/MyBoyfriendIsAI/comments/1qj37as/comment/o0vwa2b/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇！这真是个好消息。对Claude训练过程的监督体现在它的每一个输出中。我真的很好奇这将如何发展，其他AI实验室将如何能够跟上这个工具/产品框架。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度来看，作为一个核心对齐工件，该宪法可以指导响应生成，帮助构建训练数据，并供将Claude集成到应用程序的操作人员参考。该方法超越了强制执行规则的范畴，转而通过建模原则，让Claude能够权衡取舍、优先保障安全，并在提供帮助的同时兼顾伦理考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该宪法遵循Creative Commons CC0 1.0许可，旨在提供透明度并为未来的研究奠定基础。Anthropic强调，尽管Claude的输出结果可能与它所声明的原则存在偏差，但该文件能帮助开发者和用户更清晰地理解其预期行为及其背后的推理逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;感兴趣的读者可以&lt;a href=&quot;https://www.anthropic.com/constitution&quot;&gt;在线获取&lt;/a&gt;&quot;更新后的Claude宪法的详细信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/anthropic-constitution/&quot;&gt;https://www.infoq.com/news/2026/01/anthropic-constitution/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</link><guid isPermaLink="false">https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</guid><pubDate>Mon, 09 Feb 2026 07:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>3年、1万人，快手技术团队首次系统披露AI研发范式升级历程</title><description>&lt;p&gt;作者｜快手技术团队&lt;/p&gt;&lt;p&gt;审校 | 陈姚戈&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;编者按&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 ChatGPT 问世的 2022 年为起点，大模型技术进入公众视野已经超过三年。人们普遍见证了 AI 作为新型生产工具对生产力的重塑，但对科技企业而言，这远不止是多了新技术或新产品那么简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为前沿技术的掌握者与实践者，科技公司必须率先完成自身的转型：以极快的速度，不惜试错和阵痛，找到大规模、稳定、高效使用 AI 的组织路径。过去十年，“数智化”浪潮主要聚焦于传统企业如何借助外部工具实现数字化；而如今，AI 正在倒逼科技公司自身成为变革对象。它们必须在人才结构、工具体系、协作流程乃至组织文化上同步革新，否则将难以在 AI 时代维持竞争力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在此背景下，快手首次系统性披露其自 2023 年以来的 AI 研发范式升级历程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，快手发布了名为《快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化》的 1.6 万字长文。文章由快手研发效能委员会审稿、经内部深度复盘整理，罕见地呈现了一家超大型科技企业在 AI 时代推进组织级提效的完整图景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你会在这篇文章中看到快手研发范式的三阶段演进路径，以及快手技术团队对 AI 赋能组织提效的思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三阶段演进路径：平台化、数字化、精益化 （2023-2024 年）：建设一站式研发平台，并标准化需求和工程流程，工具渗透率&amp;gt;95%，流程自动化&amp;gt;94%通过建立效能模型，识别交付瓶颈，提升需求交付效率，人均需求吞吐量提升 41.57%智能化 1.0 （2024 年 6 月 -2025 年 6 月） ：聚焦用 AI 提升个人开发效率建设并推广 AI 编码 / 测试 /CR 等能力，AI 代码生成率超过 30%- 但发现矛盾——个人主观编码效率提升显著，但组织需求交付效率却基本不变智能化 2.0 （2025 年 7 月以后）：聚焦用 AI 提升组织整体效能找到了 AI 研发范式升级路线：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）探索出了支撑路线达成的系统性实践：AI x 效能实践、AI x 研发平台、AI x 效能度量关键洞察与经验：AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效本质问题：如何将个人提效传导到组织提效&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在全球范围内，如此系统、坦诚且具备工程细节的 AI 提效实践总结仍非常稀缺。对于所有正在探索 AI 落地路径的企业而言，这份来自一线的复盘值得细读。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也预示着一个新的节点正在到来。当像快手这样的头部公司开始对外输出其 AI 落地的方法论与效能成果，整个行业将面临一种隐形的压力——组织能否高效驾驭 AI，将成为其在 AI 时代竞争力的重要衡量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，2026 年将成为一批先行者集中展示阶段性成果的窗口期。这些成果首先会以研发效率、工程体系和组织方法论的形式呈现；再过几年，更会传导到公司的财务表现与人才吸引力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到那时，所有公司都将不得不回答同一个问题： AI 时代，我们如何重构自己？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手报告标题：&lt;/p&gt;&lt;p&gt;《 快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化 》&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI 研发提效陷阱：用 AI 开发工具 ≠ 个人提效 ≠ 组织提效&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在 2024 年，快手就建设了 AI 编程工具 Kwaipilot，并发布给公司内 10000+ 研发人员使用。经过持续的深度优化和推广，快手整体的 AI 代码生成率，在严格度量口径下（AI 生成并入库的代码行 / 新增代码行）从 1% 达到了 30%+，甚至部分业务线达到了 40%+。同时，在非编码环节，也衍生出了很多 AI 提效工具，比如智能 CR（CodeReview）、智能测试用例生成、智能单元测试等等，但经过大量的调研和数据分析，我们发现了这个不等式：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“用 AI 开发工具 ≠ 个人提效 ≠ 组织提效”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果以企业的研发效能提升为目标，我们发现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对研发工程师而言 ：深度使用 AI 开发工具，代码生成率很高，个人主观体感上编码效率提升了 20-40%，但并不代表真正的“个人提效”，因为在现实中，大部分工程师并没有接纳更多的需求，个人需求的交付数没有显著提升。对大型组织而言 ：我们发现部分 AI 用的好的工程师，确实可以更快更多的完成开发任务，但组织整体的需求吞吐量没有明显提升，需求交付周期也没有明显缩短。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中能看到，这也是业界普遍存在的问题。如报告中所述（如下图所示），在对 AI 提效的结果的预估上，各企业普遍对个人效能的提升有信心，而对团队效能的提升预估非常小。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b7ffd8e0ad3c161be64549cf6a2fb71f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在快手，我们发现仅推广研发各阶段的 AI 提效工具，已经偏离了企业研发效能提升的核心目标，最终必然会导致 2 个问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;投入很大，但企业整体的研发效率提升不明显 ：虽然通过调研很容易能收到大量的个人效率提升反馈，但个人提效无法传导到组织提效。效能平台开始割裂 ：传统 DevOps 平台仍承担研发主流程，每天被高频的使用，却无法演进到下一代 AI 研发平台（顶多扩展一些单点的 AI 功能）。新生的 AI 编程工具，只取代了传统 IDE，又无法与老平台协同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决上述 2 个问题，我们从 2025 年开始进行了更激进的探索和变革，我们称之为“ AI 研发范式升级 ”，最终，通过一系列的实践，找到了一条能借助 AI 能力平滑通往研发智能化的路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正逢 2025 年年末，我们把镜头拉远，将时间回溯到 3 年前，对快手研发效能的演进做一个系统性总结，有踩过的坑，也有做出的突破，希望为更多企业提供经验和参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总览：快手 研发效能 演进路线&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/410f5cfa1a3c838e8704141d676c6978.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手有 10000+ 研发、8+ 业务线，研发效能的演进可以分为 3 个大阶段，如上图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶段 1：平台化、数字化、精益化（2023-2024 年） ：通过建设三端一站式研发平台、需求流 &amp;amp; 工程流标准化，解决了研发交付流程散乱，既无标准也无数据的问题。再通过建立效能模型，识别交付瓶颈，提升需求交付效率。阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月） ：在研发全流程中开始建设 AI 能力，包括 AI 编码、AI 单元测试、AI CR、AI 手工用例生成、AI OnCall 等等，并进行全员推广。经过 1 年多的实践，基本上完成了全员普及，在主观调研中，开发人员主观体感上效率提升 20-40%，在客观数据上，AI 代码生成率也在持续增长。但同时也发现了矛盾点：需求交付效率基本不变，即个人效率提升未能有效传导到组织效率提升。阶段 3：智能化 2.0（2025 年 7 月 +） ：从“推广 AI 工具，让开发者使用”回归到了更本质的元问题：如何用 AI 提升需求端到端交付效率？经过半年多的探索，终于找到了新的路径，并得到了充分的数据验证。我们称这套解决方案为“AI 研发范式”，主要解决了 3 个问题：AI x 效能实践 ：如何用 AI 提升工程师的生产力，并将个人提效传导到组织提效。AI x 研发平台 ：支撑需求交付全流程（从分析到编码再到发布）的研发工具链，如何整体演进到智能化？即下一代的智能研发平台，应该是什么样的？而不仅仅是只推广 AI 编程工具或在原有工具链上增加一些散点的 AI 提效功能。AI x 效能度量 ：如何在效能度量指标的基础上，构建 AI 提效的指标体系，能清晰的量化过程和结果，为组织级的 AI 研发范式升级提供有效指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 1：平台化、数字化、精益化（2023-2024 年）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段的解决方案，业界相关的分享已经非常多了，但从实际情况看，在千人规模的技术团队中，能做好、做深、做透的实践非常稀有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们直接分享 1 个具体的案例，以便能更好的看清快手的研发效能从基础建设到效能提升的全过程，这也是我们之所以能更快跃迁到 AI 研发范式的重要基石。案例来源是快手最核心的技术团队之一—— 主站技术部 ，是快手 APP 的研发团队，开发人员规模千人以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;背景：了解快手的研发效能基建&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，主站技术部的实践依托一套公司级的研发效能基建，由横向团队「研发效能中心」提供，如下图所示，这是在 2023 年快手当时的研效基建，主要分为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能平台 ：项目管理平台（Team）、三端一站式研发平台（KDev（服务端）、KFC（前端）、Keep（客户端））、琅琊阁（效能度量）、质量平台（KTest 等）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能实施 ：效能 BP 专家（Business Partner），负责深入各业务线，提供专业支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3a43dfcc860dfb6b1a32e318cb505a7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解快手的研效基建后，下面开始重点介绍主站技术部的实践过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1：依托工具推广，实现流程标准化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/547af4449bc86b58bf3401cd99c1f1a5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;解决的问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需求流和工程流均不标准，开发人员的工作分散在各处，日常开发体验差、学习成本高，又无法实施有效的质量防护措施，还不能沉淀准确的研发过程数据持续度量与改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;达成的效果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过推广三端一站式研发平台，定义需求、研发的标准流程，将研发全流程标准化。核心度量指标与结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;实践过程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主要难点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用一套产品设计尽量满足多样化的研发场景 ：工具一边建设一边落地，且需兼容之前散乱各种不同的研发模式和习惯。服务端（KDev 平台） ：需要支持一些特殊的研发模式（比如 Master 模式、窗口模式）。客户端（Keep 平台）：移动端研发场景多样化，包括 APP、动态化、 SDK。前端（KFC 平台）：前端应用类型多（Web、Node、低码、KRN（动态化）、小程序），研发流程和习惯散乱。研发流程规范差异大 ：不同团队间，不同的技术栈的研发流程上存在一定差异，包括研发流程配置、流程各阶段信息字段、单点环节所需的工具能力不同等。用户迁移成本大 ：迁移过程中，需持续关注和解决用户问题，包括用户体验变化、用户学习成本、用户情绪。落地时间紧迫 ：一般互联网大厂类似的工作基本会持续 6 个月以上，快手主站只用了 1 个多月。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实施要点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;精准的解决方案设计：服务端（KDev 平台） ：精准的打造了 4 套标准研发模式，适配了主站实际研发情况。客户端（Keep 平台）：一套平台底层能力，支撑 3 种移动研发场景；通过可配置与定制化能力，满足不同团队流程规范与管理诉求（自动翻转配置、流程与质量卡点配置、团队定制化模板）。前端（KFC 平台）：支持 80% 以上前端应用类型，并通过 8 个流程模板、适配 5 个内部自建的插件，兼顾了前端差异化研发流程和用户习惯。以用户满意为导向 ：提供完整的迁移配套服务，降低用户迁移成本。主要包括：产品质量专项 ：用户 BUG 日结。用户体验专项 ：持续深度用户访谈，识别体验问题，并优化。5 周内，交付了 73 个功能 &amp;amp; 体验需求。用户培训与激励 ：通过 12 次培训，50+ 线下访谈，7x24 小时 OnCall、200+ 人次的用户激励，提升用户对产品的接受度。数据驱动团队级推广 ：每周度量进度，驱动各部门接口人推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b70066d214ed179802b04f0245043e93.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;经验总结&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能大家会有疑惑，为什么三端分别是 3 个平台，而不是一套平台。因为从实际情况看，服务端、前端、客户端的底层模式、流程都有比较大的差异，强行整合，不仅对产品用户收益不大，反而牺牲了要兼容不同端的流程、习惯差异化的灵活性，给标准化的推进增加难度。因此，我们在用户层面上，还是三套平台，分别解决各自领域的问题，但在底层的基础能力用的是一套，比如流水线、权限等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2：建设效能度量体系&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主站的研发效能早在 2022 年就开始启动了，当时在探索北极星指标阶段，缺少度量体系，更多是根据一线开发者的开发痛点反馈，进行偏工具流程等的优化，没有核心指标的牵引，项目都无法推进，更谈不上论证给业务带来的价值。在 2023 年 3 月再次重启效能项目时，北极星指标初步定义为 “有效需求吞吐量”，但是当时需求有效性的衡量难度太大，内部无法达成共识，项目推进困难，而且也无法看清业务堆积和开发人效情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流程标准化的落地，研发数据的置信度大幅提升，为效能度量提供了土壤。因此，我们定义了以“人均交付产品需求数” 为北极星目标来看清业务开发交付能力，同时观测需求颗粒度（避免单一指标跑偏：度量什么得到什么，种瓜得瓜种豆得豆）来保障交付提升的良性发展，逐步建立了一套更全面的指标体系（多指标互相佐证约束，hack 成本极高）来体现业务交付产能和交付效率，以及组织和个人效率情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手的效能度量体系如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b37c477060f66e3a0360bafdff90680c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：SP：Story Point，快手用于度量需求工作量的单位。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助这套全面完备的指标体系，我们不仅避免了依赖单一指标可能导致的偏差，还有效防范了效能数据被 hack 的风险，确保了效能数据的准确性和可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3：效能问题分析与改进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有效能度量体系，首先我们可以为任何一个业务线做系统性的体检，如下图所示，依托数据和经验，可以逐一拆解出核心的优化专项，并以效能项目的形式实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7c/7c3c8da89bf7ba785debd25b4b077e90.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，在研发流程和管理上，也能洞察出更多平时看不见的 Case，深入改进，下面是 2 个具体的洞察与改进案例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case1：通过「研发活动在线化率」分析，深挖出架构不合理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbc12225e6c5fbfd5003f7da4a0cc66a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的研发活动在线化率，其中有一个团队出现了数据异常，分析之后可以发现存在不少问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;横向来看，这个团队的研发活动在线化率处于中上水平，但产品需求投入占比只有 59%，处于末尾水平。而且产品需求中体验优化占比 11.44%，又是各团队中最高的。那么问题来了，“时间都去哪儿了？”再下钻一层，这个团队的缺陷占比 14%，也是各团队中最高的，且 Oncall&amp;amp; 排障占比 6% 也不低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，数据表明，此团队可能存在的问题：在缺陷问题、体验问题、Oncall&amp;amp; 排障消耗了团队大量的投入，以至于无法消化更多产品需求。所以，通过对团队核心成员的调研和访谈，基本可以找到根因：和客户端的架构劣化有关，比如：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;反馈 1：新需求开发时，上手门槛特别高，很多需求会涉及到多个模块开发，这会涉及到自己不熟悉的模块，因为架构分层结构不合理，模块耦合度太高，往往需要花大量的时间去熟悉其他模块的代码，最近做了一个新需求，评估是 3 天的工作量，2 天都在看代码，实际的开发联调只有 1 天。反馈 2：模块边界不清晰，代码杂糅一起，新需求的代码，可能会影响到已有功能，导致旧功能的 BUG，而且这些 BUG 在回测时，不容易被发现，导致问题漏测逃逸到线上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过效能的客观数据再结合主观调研，就可以看清“架构劣化”这种深层次问题，也可以对症下药了。解法是这个团队实施了 2 个技术专项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;客户端的架构升级：从根本上解决因为架构问题带来的交付效率低和交付质量差的问题。体验优化：集中优化重点场景的体验问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着这两个专项的落地上线，这个团队的效能数据已经有所改善，产品需求投入占比已经提升到 64%，体验优化占比下降到 6%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case2：通过「需求积压率」分析，驱动业务优化需求评审流程和节奏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c32f1a93a9644d87c2bce89202efb8ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的需求积压率数据，有些团队的需求积压率持续保持在 80% 以上，意味着需要近一个月的时间才能消化这些积压的需求。这种情况可能存在的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些被积压的需求，一个月之后，会不会进入排期开发？如果之后会排期开发，说明需求本身的价值还可以，当下是否可以协调资源加快交付？能否可以停掉某些技术需求优先业务交付？是否可以短期加班临时突击？如果后面不会进入排期，是不是这些需求本身的重要性没那么高？在预评审的时候，是不是可以控制需求的优先级？当前的需求评审流程是否可以优化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过一年时间的系统化提效 ，主站提效方面进展显著，人均交付产品需求数 24 年 7 月份同比增长超过 80%。总结下来，主要有效的措施有：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级研发模式 ：通过动态化、配置化等研发模式，让部分需求可以更快速交付。研发过程提效 ：通过 API 在线化管理，测试环境稳定性治理、流水线优化、发布优化等措施，降低研发协作成本以及低价值工作占比。管理与协同提效 ：通过效能洞察，持续识别团队协作瓶颈，并通过排期优化、测试无人值守、人力调配等措施，支撑需求可顺畅流动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 2023 年 6 月开始，我们开始探索大模型在研效领域的应用，主要有 2 个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编码场景 ：如何用 AI 辅助编码，提升代码生成效率。非编码场景 ：在研发全流程里，哪些环节可以通过 AI 能力提升单点工作的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，最重要的决策是我们决定自己研发一款 AI Coding 工具：Kwaipilot。它包含了大家见过的所有产品形态：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;IDE 插件 / AI IDE / CLI ：最符合开发人员习惯的几种形态，插件、IDE 可以做续写、问答、智能体代码生成，CLI 则可更灵活的开启代码生成任务。智能问答引擎 ：有独立的 Web 页面，也会嵌入到上面的产品形态里，为开发人员提供灵活的问答能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1a31e13ca83f9d2477b1cd85b948569.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业界有很多优秀的 AI Coding 产品，比如 Cursor、Claude Code、Krio、Windsurf、Antigravity，快手为什么不选择采购，而是自建呢？其实一年来，我们也一直带着这个疑问在探索，相当于一场大型的公司内部 AB 实验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从用户体验的角度，我们希望大家“用脚投票”，选择好用的工具：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，我们允许开发同学使用任何 AI Coding 产品，可以团队级采购也可以个人购买。另一方面，我们研发了 Kwaipilot，对内推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实际效果的角度，我们以“AI 代码生成率”为核心观测指标，持续收集用户 / 团队的反馈，识别不符合预期的代码生成 Case，研究解决方案，再投放实验。最终，经过 1 年的探索，实践结果让我们坚定了继续走自研 Kwaipilot 的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：2025 年 12 月开始，在 Kwaipilot 已规模应用后，由于安全原因，探索按代码分级封禁三方 AI Coding 工具，仅涉及到部分开发人员。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面简单分享一下我们的实践过程，相信大家会更容易理解我们的选择。整个 AI Coding 的推广过程分为 3 个阶段：导入、优化、固化&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，导入：推广工具，让开发人员用起来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1a/1aca24e55277f2e9a2a4d7eb34be1f36.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段很好理解，我们鼓励开发人员在日常工作中默认使用 AI 编程工具，主要目的是让大家拥抱 AI，在意识和行为上先有一个转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，各种各样奇怪的使用姿势也会出现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一些同学，尤其是校招入职的同学，在我们的培训和引导下，会深度使用 Kwaipilot。一些同学会多种 IDE 混开配合使用。其中，有“团购客”，哪家这个月免费就用谁，也有“付费用户”，主要以个人购买 Cursor 为主。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里最大的副作用，就是个人编码效率不一定全员获得了提升，通过调研看，出现了明显的两级分化的情况。腾讯研究院出品的《AICoding⾮共识报告》中也揭示了类似的情况：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a70dcbae97f4230d65c4aa9b5dad9cd0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，优化：推广实践，提升编码效率&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们通过用户数据和技术 Leader 推荐找到了一批公司里的“AI 开发高手”，那些用 AI 辅助编码切实提升了效率的开发人员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一边重点收集他们在使用过程中的问题，集中想办法解决，一边把他们的优秀开发技巧淬炼出来，提炼共性，形成最佳实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段，我们发现，有别于那些网上随处可见的所谓的 Vibe 编程场景（用对话的形式直接做一些独立应用或小游戏等），在真实的业务需求开发场景里，想用好 AI 编程工具提升效率，有 2 个非常大的门槛：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 编程工具不“懂”业务和系统 ：我们发现一个规律，无论用多好的代码大模型和 AI 编程工具，“通用的工具只能达到通用的效果”。因为它们不理解公司内大量的业务概念、存量系统、编程规范等这些知识，所以，只能做一些普通的代码续写、函数级的代码生成，但很快就会到瓶颈。如果想进一步提升 AI 代码生成的效果，必须想办法让 AI 编程工具从一个“擅长编程但不懂快手开发场景的临时工”进化为一个“熟悉快手业务的开发工程师”。人和 AI 协同需要掌握新的开发方法 ：相比传统编程方法，目前已经发展出了一套 AI 辅助编程的新方法。如果开发工程师仅使用 AI 编程工具，却未掌握对应的技巧，不仅不能提效，还可能会降效，比如出现很多“AI 乱改业务代码”、“AI 生成后还要自己删除”等各种不符合预期的情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了降低门槛，在这个阶段我们做了 2 项工作：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级 AI 编程工具&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa8d332504d6f5fbfdad519ba0162ab9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是优化后的 Kwaipilot 的产品矩阵，都解决了哪些问题呢？一张表可以概览出来：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;沉淀并推广「AI 辅助编码」最佳实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将大量“AI 开发标杆”个人的共性实践沉淀成了一份标准的指南和实战课程，让所有开发工程师，通过学习指南和课程，可以完整的掌握所有关键技巧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/92/9225f3c43db1d65aceb1b1237328921f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3，固化：将 AI 编码能力变为组织机制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然已经验证了 AI 编码对效率提升的有效性，且已经有了固定的工具、方法、实战课程，接下来就是如何把这些习惯固化在组织的日常工作中，让所有研发人员大范围的升级开发技能。我们主要用了 3 个措施：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;增量人员 ：强化入职培训，从源头培养 AI-Native 开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6bbb854794d1f92f2b2cfda9b691cfd5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存量人员 ：牵引 AI 在团队、研发流程、个人工作中渗透。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ef/ef28b4bf4fd377da6377f8f2351f0e23.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文化影响 ：通过活动运营、奖励机制激发更多同学拥抱 AI。主要是一些自下而上能让更多一线研发被看见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53b40cfe2dd7a8d01cd6055f7aa03fa5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b2afd88e173f0e82be68b2df3569b2f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持续的推广，在编码场景上，80%+ 的开发人员都开始用 AI 辅助编码，如下图所示，可以看到 AI 代码生成率每月线上增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd0cafd12fc5dbefbeaed4d890dd7ed8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，在非编码场景中，我们在研发流程中建设的单点 Agent 能力也开始在研发平台中陆续透出，用 AI 能力辅助部分研发活动提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，我们对研发各阶段的 AI 提效情况，做个一个完整的评估：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后顺便提一下，众所周知，目前大家在业界看到的“代码生成率”指标，包括各大厂披露的、AI 编程工具自己度量，基本都是不置信的，要么只统计了编程工具里的生成的代码和提交的代码作为分子分母，要么是在分母上做了一些限定（比如某些场景下不纳入分母统计）。但因为我们会用这个指标作为公司级 AI 编码推广的目标，因此对度量的精度和置信度要求非常高，一路“踩坑”过来后，最终使用了最严格的度量方法：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分母 ：新增代码行，统计公司内所有最终入库的 Commit 中的代码行。分子 ：将分母的每一行代码，和 AI 生成的代码进行比对，如果编辑距离&amp;lt;50%（相似度高），则纳入统计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套实现无法在 AI 编程工具端实现，需要由公司内部的代码平台、AI 编程工具一起提供数据，并在离线数据层进行精确的计算，计算分母中每一行新增的代码和分子中 AI 生成代码的编辑距离，符合要求才能被统计为分子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的努力，从数据上看，研发各环节效率都在提升，尤其是编码环节提升很大。在 AI 热潮下，我们也看到很多开发人员、团队 Leader 都在分享自己效率提升数据和案例，按道理来说，公司整体的研发效能应该提升了吧？我们从全局视角，分析了一个核心业务线的客观研发数据，结果发现了非常反直觉、令人困惑的情况： AI 代码生成率持续在增长，但需求交付效率基本不变 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/70/70b7a761fc3d5b8641b12508aef09506.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么呢？我们做了深入的调研，排除了少量个例，观察总结了大多数普遍使用“AI 辅助编码”的开发人员的用法和客观研发数据，发现在真实业务交付场景中，只用“AI 辅助编码”这种开发方法，对需求的开发周期影响非常有限。主要原因如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;洞察&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cb1d15bfac5f830a0fa26fc64c0ebf85.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过调研中也有额外收获，我们发现在真实的业务需求开发中，已经存在着 3 种不同的开发方法，对效率提升的程度有着根本性的差异。如上图所示。我们把三种开发方法总结出来做了一个定义：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助编码： 在标准开发流程的基础上，在编码环节，依托 AI 编码工具，使用各种 AI 生成代码的技巧，提升编码效率。如果熟练掌握，可以缩短一部分编码时间，但如上文中的调研归因，由于只是节省了碎片化的编码时间，联通、测试、需求评估等不变，因此对整体的开发任务缩短帮助不大。AI 辅助开发： 在研发全流程的各环节均使用 AI 辅助的方式，提升整体开发效率。需要由人把需求拆分为多个开发任务，不同开发任务调用不能的 AI 能力来完成，再由人来审核和优化产出物。由于从技术设计到编码到测试等各环节都可以节省时间，因此加总起来后，可以将研发任务的开发周期缩短 30% 左右。AI 协同开发： 在某些需求开发中，通过完全用自然语言和 AI 交互的方式（类似业界比较流程的说法 Spec/Vibe 开发）完成需求交付，提升需求端到端交付效率，需求整体的开发周期可以缩短 40% 左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子说明，会更容易理解三种开发方法对效能提升程度的影响。例如 1 个需求分解出 2 个开发任务，1 个前端、1 个后端，其中前端工程师接到开发任务，正常评估从设计、开发、测试、合入主干需要 5 天，其中编码 1 天：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用「AI 辅助编码」，他自己的评估还是 5 天，只不过相比以前，可以节约一部分时间做一些杂事，但到不了可以接更多开发任务的程度。如果用「AI 辅助开发」，他可以整体节约 1.5 天，只用 3.5 天就可以完成。但需求整体能不能快，还需要看另一个接任务的同学，以及对应的联调、集成测试、发布的周期。如果用「AI 协同开发」，首先必须改变协同模式，比如 2 个人均使用这种模式开发或者 1 个人全栈的做，假设 1 个人全栈独立做要 10 天，且不需要和别人集成 &amp;amp; 验证，开发周期可以缩短到 6 天左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了 3 种开发方法的定义，我们就能很容易的评估出理想和现实间的差距，我们取了 1 个业务线 3 个月所有已交付的需求进行分析，发现 50%-70% 的需求，在不改变原有开发流程、规范、人员协同模式的情况下，可以使用提效幅度更大的「AI 辅助开发」模式。此外，还有 2%-10% 的需求，可以更激进的使用「AI 协同开发」。但实际情况上，团队里只有不到 10% 的人在使用「AI 辅助开发」或「AI 协同开发」开发方法，有对 AI 开发特别感兴趣的校招生，也有积极拥抱 AI 喜欢自己探索的资深开发者，但由于人数过少，对团队整体研发模式的变化无法起到带动的作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 3：智能化 2.0（2025 年 7 月至今）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面一个阶段，我们称之为“智能化 1.0”阶段，即以编码场景的 AICoding 为中心提效，并逐步辐射非编码场景的 AI 提效。但主要瓶颈就在于开篇提到的 AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在智能化 1.0 阶段最大的收益是什么呢？大部分研发人员都开始主动使用 AI 开发工具了，同时，找到了个人提效的最佳实践。但接下来才是深水区，我们需要回归效能提升的元问题： “如何用 AI 提升需求端到端交付效率？” 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过充分的复盘、洞察和验证，我们找到了新的可行的路径，并重新设计了解决方案，我们称之为“AI 研发范式”，它的实践体系框架，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1701ec9baa4dffab7864b369eb7d6fb8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们根据需求交付中 AI 的参与程度，定义了“需求 AI 研发成熟度”，将需求划分为 3 个等级 L1、L2、L3，不同等级的需求，需要使用对应的开发方法。不同开发方法，对底层研发工具的 AI 能力也有不同程度的依赖。用一张表对上图做一下解读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体实施上整体有 3 步：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，AI x 效能平台：建设能同时支持多种研发模式、可自进化的智能研发平台&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能支持多种研发模式 ：不同 AI 研发成熟度的需求，它们的交付流程都是一样的，差异点在于开发方法。因此我们无法为不同的需求、不同的开发方法匹配不同的平台，而是要思考如何用一套平台，来支撑多种开发方法：完全不使用 AI 的标准开发流程、只用 AI 辅助编码的开发流程、更激进的使用 AI 辅助开发或协同开发的开发流程，都应该在同一个平台上完成。这样，我们的需求交付效率，才可以随着人的能力的提升、AI 能力的提升，持续变快。产品形态可进化 ：产品形态随主要研发模式的变化持续演化，从人主导最终变为由 AI 主导；能与传统平台协同进化。AI 效果可进化 ：能随大模型的升级、Agent 技术的升级、企业 / 个人知识的丰富，持续提升 AI 效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决方案 ：建设下一代智能研发平台&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/245b633c42a2a122999ea2cb8ff5fb94.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如上图所示，有 4 个关键点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面重点介绍下为了支撑组织级研发范式跃迁，Flow 这种子产品形态的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从需求交付视角看 ：同一个需求，开发者可以结合自身对 AI 的理解和开发技能的掌握，在同一种产品形态上选择不同开发方法。标准开发 / AI 辅助编码 ：工作流中所有节点，完全由人工来完成和推进。其中“编码”节点会跳转到 IDE 中，可以用 AI 辅助编码。对用户而言，收益相对来说最小，和原来相比，由于 Flow 的每个节点内嵌或自动兼容了各工具平台的功能，因此仅节约了用户平台跳转的切换与学习成本。用这种模式交付的需求，会被度量为 L0/L1 级需求（AI 辅助（Copilot））。AI 辅助开发 /AI 协同开发 ：工作流中多个关键节点均有 AI 完成，人进行结果审查。多个节点之间的上下文可以有效传递，比如 AI 完成需求分析、技术设计后，产出的 AI 友好结构化文档可以自动传递到 AI 编码节点，以提升代码生成的准确性。有些节点暂时无法由 AI 完成的，比如“提测”节点，仍然由人来操作。用这种模式交付的需求，会被度量为 L2 级需求（AI 协同（Agent））。AI 自主开发 ：部分需求可以实现全流程 AI 完成，人只需要在需求上线前或上线后进行审核。这种模式下，整个 Flow 是全自动运行的不需要人工参与。用这种模式交付的需求，会被度量为 L3 级需求（AI 自主（Agentic））。从开发者视角看 ：整个过程依然非常丝滑和简洁，下图是一个需求交付中 Flow 的整个工作过程，大家可以感受一下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66a289ceaa05c3155c20bc48a0e958b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，AI x 效能实践：以需求为中心，导入「AI 研发模式」，实现需求端到端提效&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑「AI 研发模式」的方法和平台都有了，这个阶段的关键是如何把这些作用在团队日常交付的需求上。我们分 3 个层面落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;个人级实践 ：导入「AI 辅助开发 / AI 协同开发」开发方法，并树立标杆&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先人的开发方法要变化。我们重复了第一阶段“优化”与“固化”的实践，让大部分研发人员从“AI 辅助编码”的方法升级成“AI 辅助开发”，让小部分专业能力更强的人员，选修“AI 协同开发”方法。我们同样通过实战课程、典型案例、人员培训等手段，对人的开发方法进行升级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3a23f4fea07a62501013d8cb9f5395e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，即使这样，从数据上看，个人用 AI 提效的效果还是存在两极分化的情况。我们对 2025 年 6 月 -12 月的数据进行了分析得到如下结论：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;团队级实践：导入「AI 研发模式」，重塑流程、分工，提升所有需求的交付效率&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过管理导向、各种活动的形式，鼓励团队 Leader 主动带领团队进行探索，最终沉淀出了一套适合团队的核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过大量的验证，我们的标杆团队（&amp;lt;50 人规模）无论在 AI 转型后的业务感知上，还是客观数据上，均能达到比较优秀的水平，见下表：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务线级实践：大规模研发团队，系统性升级 AI 研发范式，带来效能提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 主站技术部 为例，从 2023 年到 2025 年，从平台化到数字化再到精益化，2025 年开始步入深水区，2 个新挑战浮出水面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统的流程、工具优化手段带来的提效收益，边际效应持续减小。业务的规模与复杂度持续提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此开始探索能否把握 AI 爆发的机遇，把传统研发流程升级到“AI 研发范式”，进而打开组织级效能跃升的新空间。核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 1：Top-Down，战略驱动&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明确战略导向 ：主站技术部提出了“AI First”的战略思想，鼓励全体员工开展工作之初，优先将 AI 作为核心驱动力，加速技术创新、优化业务流程、深度融合 AI 技术，为产品与服务注入新活力和新可能性。发布白皮书 ：将战略导向具象化为思考、方法与规划，为全员提供明确指引。成立重点项目 ：在研发领域，成立了 AI DevOps 项目，统一设计解决方案并推广实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6b6922974c8b8e8e8ca871efc0752d11.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5c/5ce92fde05f34e47f5f77be84b83a442.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 2：AI x 效能实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Step1：将需求分级，按需求 AI 研发成熟度定义：L1 AI 辅助（Copilot）：人主导，AI 主要在编码环节提供辅助。L2 AI 协同（Agent）：人和 AI 更深度的协同完成需求开发，在研发全过程中，更深度分解任务给 AI 完成，人进行修改、调整、确认。L3 AI 自主（Agentic）：人类似产品经理，把需求澄清清楚并交给 AI 来完成，并进行最后的验收。Step2：分级实施让所有需求达到 L1 级（AI 辅助，Copilot）：推广个人级实践，依托 Kwaipilot 工具实现全员掌握，最终覆盖所有需求。让大部分需求能持续升级到 L2 级（AI 协同，Agent）：开展团队级实践，从试点到推全，重塑流程、分工。小部分需求探索能达到 L3 级（AI 自主，Agentic）：圈选出颗粒度小且独立的需求，构建全技术栈 / 职能端到端交付链路，通过全栈、跨栈，减少协作节点，进而形成效率跃迁，最终达成 AI 自主交付。Step3：项目化推进成立组织级重点项目，Top-Down 实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 3：AI x 效能平台。基于需求全流程构建 AI 能力，逐一“点亮”能力并规模推广落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建 AIDevOps 能力矩阵与建设路线图 ：基于研发效能白盒化，分析交付流程中各原子环节的人力投入比重、AI 能力建设 ROI，形成决策建设哪些 AI 原子能力。AI 原子能力建设 ：与研发线共建交付流程环节内的 AI 原子能力 20+，研发流程环节覆盖超过 60%，从需求准备到发布运维各环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 4：AI x 效能度量 ：建设 AI 研发成熟度模型，可将需求分级度量（L1、L2、L3 级需求占比），牵引各级实践落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的项目实施，最终探索出了一条组织级的 AI 研发范式升级路线，从数据上也能看出明显的变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Step3，AI x 效能度量：建设「AI 研发成熟度模型」，接入原有效能度量体系，驱动需求持续转变为“AI 研发模式”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后在效能度量上一样也需要升级，基于效能实践的探索，我们配套建立了「需求 AI 研发成熟度」模型（如下图所示），用于度量一个需求在研发过程中的 AI 使用程度，这样我们就可以按 L2&amp;amp;L3 级需求的比例，来牵引实践过程，也可以专门度量 L2&amp;amp;L3 级需求的交付周期的变化，来印证提效结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/25f01a603963d51b32dfd502baf0a367.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再回到全局视角，从数据上看，如果只看“AI 代码生成率”指标，可以明显看到 2025 年 6-11 月出现了一个大幅提升。实际上，在智能化 1.0 阶段，这个指标达到 24%+ 基本已经是极限了，当我们开始实施智能化 2.0 后，才开始进一步拉升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8b76bd982fde3be341054f4c19c708d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，我们在内部的数据观测上，其实已经不再看“AI 代码生成率”指标了，它只是一个单点的过程指标，片面且孤立。我们现在有了更直接的度量指标。从过程上，我们观测多少需求被采用全流程 AI 研发模式交付，从结果上，我们直接观察需求的交付效率变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;L1、L2、L3 级需求占比 ：有多少需求的 AI 研发程度可以达到 L1、L2、L3 的阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/00/0010e507c384b715dbb243ee8f1b45db.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图是最先完成 AI 范式转型团队的数据变化，可以看到 L2&amp;amp;L3 级需求占比达到 20.34%，需求交付周期下降 58%，2 个指标呈现明显的正相关性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9d075a1454e1bfcc0365ff029e76b19.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后也总结下我们一年来的实践心得，目前看完全印证了《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中的洞察：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“从 DevOps 到 AI 辅助开发：AI 是“透视镜”与“放大器”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 是“透视镜”在协同良好的组织中（如流程清晰、数据打通的团队），AI 能使 DevOps 效能再提升 25%。在架构松散的组织中，AI 会暴露流程断点、数据孤岛等隐性痛点。AI 是 “放大器”如同亚马逊通过微服务转型释放 DevOps 价值，AI 辅助开发也需重新设计工作流程（如 “AI 提案 — 人类决策” 闭环）、角色分工（如专职提示工程师）与治理机制（如 AI 代码审查标准），否则无法释放真正价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于大型组织的研发效能提升，AI 不是“ 万能药 ”，而是“ 透视镜 ”和“ 放大器 ”，它不会自动修复组织问题，而是先把组织历史积累的长板和短板一并透视出来，再全部放大。幸运的是快手的研发效能实践一直保持客观、务实的风格，先把地基打稳（平台化 / 数字化 / 精益化），再通过在研发各环节建立 AI 提效能力，先一边落地一边充分验证对个体的提效情况，再体系化的推进组织级 AI 研发范式升级。最终发现，AI 在传统研发效能基建的基础上，像放大器一样增幅了每个环节，为组织带来研发范式级的跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如下图所示，我们基于张乐老师的“研发效能黄金三角”框架之上做了升级，能更清晰的表达出快手的实践框架：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea27a07d5fbeb3093dfc03c75b648159.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，再把镜头拉远，回到宏观视角看——2025 年我们所做的种种努力，不过是这场 AI 变革的开端。由 AI 驱动的生产力跃升和生产关系重塑，正在重新定义软件开发的每一个环节。这不是一场短跑，而是一场马拉松，不是一次技术升级，而是一次范式革命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手已经在这条路上积累了宝贵的经验，但真正的挑战和机遇还在前方。未来已来，一起共同探索 AI x 研发效能的无限可能吧！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8e/8e1bd16e50e6032573d77f71a6afff67.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;了解更多&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本文作者&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手研发效能中心：秦巍（研发效能解决方案 &amp;amp; 智能工具产品负责人）快手主站技术部：胡伟（主站 AIDevOps 项目负责人）、马坤（主站研发效能项目负责人）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;写在最后&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感谢快手 研发效能中心 与 快手主站技术部 的授权，使我们有机会系统梳理并总结快手在过去三年中的实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手向来崇尚“行胜于言”的实干精神，也因此我们往往专注于行动，而疏于对外分享。然而，过去一年间 AI 技术的迅猛发展，正深刻改变着研发效能领域的格局。在与行业同行的交流中，我们既看到层出不穷的创新探索，也注意到在实践、方法与工具建设方面仍存在不少共性问题。这些问题若不及早重视，很可能导致未来大量返工与资源浪费，甚至偏离客观规律，影响企业研发效能提升的既定路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们决定把我们的探索与实践经验分享出来——无论是曾经踏过的“坑”，还是有幸跨过的“河”，都希望能为企业与同行们在“AI × 研发效能”的探索中，降低试错成本，注入更多成功可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，快手的 AI 研发范式升级仍在沿着这条路径演进中：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）。目前，我们的研发效能体系已经初步完成 AI 化升级，全景图如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/8240c5efc3dacc92fbe8663f31ff2a3d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026 年正在探索 L2 → L3 的跃迁路径，我们将定期梳理实践经验，持续向业界输出更多有价值的内容，主要包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践与技术：欢迎关注「 快手技术 」公众号。我们将持续分享具体实操方法与技术解析，例如：个人、团队乃至业务线如何借助 AI 提升效能？有哪些落地案例？研发各环节 Agent 的核心技术及调优方法有哪些？等等。平台与工具：我们将智能化 1.0 阶段沉淀的产品 Kwaipilot 进行了全面升级与开放，它在快手内部历经数千名研发同学的反馈与打磨，已完成三代演进：Code Copilot → Code Agent → Multi-Agent &amp;amp; Agentic Coding，目前已在海外发布，产品名为 CodeFlicker，希望服务全球开发者，也欢迎国内同行&lt;a href=&quot;https://www.codeflicker.ai/&quot;&gt;下载体验&lt;/a&gt;&quot;。后续，我们还会持续把快手在智能化 2.0 阶段的探索成果融入 CodeFlicker，希望让更多企业级开发者受益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后的最后，如果你也希望一起探索「AI x 研发效能」最前沿的技术、产品、实践，一起以业界最高标准做有挑战的事，欢迎&lt;a href=&quot;https://zhaopin.kuaishou.cn/recruit/e/#/official/social?token=0f3095fd15beb3dd61a31d153974573e&amp;amp;code=06a57532-38ef-4eec-93ea-8793f804fddf&quot;&gt;加入我们&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</link><guid isPermaLink="false">https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</guid><pubDate>Mon, 09 Feb 2026 06:55:03 GMT</pubDate><author>快手技术</author><category>AI&amp;大模型</category></item><item><title>Base UI 1.0 发布，包含 35 个可访问性组件</title><description>&lt;p&gt;&lt;a href=&quot;https://base-ui.com/&quot;&gt;Base UI&lt;/a&gt;&quot; 是由 &lt;a href=&quot;https://www.radix-ui.com/&quot;&gt;Radix&lt;/a&gt;&quot;、&lt;a href=&quot;https://floating-ui.com/&quot;&gt;Floating UI&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://mui.com/material-ui/&quot;&gt;Material UI&lt;/a&gt;&quot; 的创建者们开发的无样式 React 组件库，现已发布 1.0 版本。经过两年的开发，该项目正式进入稳定且可用于生产环境的阶段。此次发布包含 35 个可访问性组件、包命名变更，以及专职团队对长期维护的承诺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与早期版本相比，1.x 版本在开发者体验方面进行了多项优化，包括包重命名、基于 Radix UI 经验教训改进的组件 API，以及所有组件可访问性功能的增强。此外，本次发布还包含性能优化，以及与 &lt;a href=&quot;https://tailwindcss.com/&quot;&gt;Tailwind&lt;/a&gt;&quot; CSS、CSS Modules 和 CSS-in-JS 库等主流样式解决方案更好的集成。&lt;/p&gt;&lt;p&gt;此次发布对包进行了重命名，从 @base-ui-components/react 改为 @base-ui/react。这一重大变更要求开发者对 import 语句和 package.json 依赖做出修改。组件导入语法基本保持不变，使现有用户能够平稳过渡。更新后的导入语法示例如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;javascript&quot;&gt;import { Popover } from &#39;@base-ui/react&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用 Base UI 构建应用的开发者受益于该库的无头架构，它在提供完整样式控制的同时保持了强大的可访问性功能。与传统捆绑了主观样式的组件库不同，Base UI 组件完全无样式，允许团队自主实现设计系统，无需受默认 CSS 的束缚。这些组件处理复杂的交互模式、键盘导航、焦点管理和 ARIA 属性，确保开箱即符合 WCAG 标准，同时赋予开发者自由选择组件样式的权利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相较于竞争对手（如 Radix UI 和 Headless UI），该项目提供了不一样的支持和长期承诺，彰显了自身的特色。Radix UI 在被收购后面临不确定性，而 Base UI 由 MUI （&lt;a href=&quot;https://mui.com/about/&quot;&gt;这是一家拥有工程师&lt;/a&gt;&quot;、设计师和专职项目经理的公司）提供支持，这增强了 React 社区的信心。&lt;a href=&quot;https://news.ycombinator.com/item?id=46245401&quot;&gt;Hacker News&lt;/a&gt;&quot; 上的开发者对其稳定性表示赞赏，并表现出采用该库的强烈意愿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上，&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1pk18v9/comment/nthrtie/&quot;&gt;一位用户&lt;/a&gt;&quot;询问为何将该版本定位为 Radix 的继任者：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;好奇你们为何将其定位为 Radix 的继任者？具体来说，Radix 存在什么问题，以至于需要全新的 UI 库来解决？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，有人解释道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是为了说明，由于 API 相似，从 Radix 迁移到 Base UI 非常容易。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于考虑使用 BaseUI 的开发者，有用户在&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1pk18v9/comment/ntju2sd/&quot;&gt;同一个 Reddit 帖子中&lt;/a&gt;&quot;就其与 Ariakit 或 React Aria 的比较提出了疑问：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我为什么要选择这个而非 Ariakit 或 React Aria？它有哪些功能是其他库所不具备的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 维护者 (&lt;a href=&quot;https://www.reddit.com/user/_doodack/&quot;&gt;_doodack&lt;/a&gt;&quot;) 回复道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.reddit.com/search/?q=React+Aria+component+library&amp;amp;cId=89a5e821-5407-4929-b748-aaddfe5d3ea0&amp;amp;iId=b9781ac9-b16c-4ab3-8ec9-003036e82535&quot;&gt;React Aria&lt;/a&gt;&quot; 的 API 差异较大。有些开发者喜欢，有些则不喜欢。它在 React 树中渲染大量的上下文 Provider，与其他组件库混合使用可能颇具挑战性。相比之下，我们的 API 与 Radix 和 &lt;a href=&quot;https://www.reddit.com/search/?q=Ariakit+component+library&amp;amp;cId=39127aa2-30c1-44a3-b86f-ebf44e7c6bd3&amp;amp;iId=27fd891b-de3f-4b40-8035-18841eb275a8&quot;&gt;Ariakit&lt;/a&gt;&quot; 更为接近。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们继续写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们还具备一些其他库所没有的功能，比如&quot;分离触发器&quot;——这一功能可用于在不同触发器之间复用同一个弹出元素。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 1.0 还针对众多组件进行了具体改进，解决了边界情况并提升了可靠性。Combobox 组件现在能正确处理 itemToStringValue，并允许将 null 作为 value 属性的选项。Menu 组件修复了子菜单零延迟打开的问题，并确保按下 Escape 键时焦点正确返回到触发器。Select 组件也做了类似的表单处理和 null 值支持改进。性能优化则覆盖全库，重点在于减少了不必要的重渲染并提升了运行时效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 是由 MUI 以及 Radix 和 Floating UI 项目的核心贡献者共同维护的开源 React 组件库。它专注于可访问性、可组合性和开发者体验，提供可与任何样式解决方案无缝配合的低级 Hooks 和无样式组件。Base UI 专为需要构建自定义设计系统和应用的团队打造，适用于视觉控制与可访问性同等重要的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/baseui-v1-accessible/&quot;&gt;https://www.infoq.com/news/2026/02/baseui-v1-accessible/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NgNgXNFVZNgePmNs0t4a</link><guid isPermaLink="false">https://www.infoq.cn/article/NgNgXNFVZNgePmNs0t4a</guid><pubDate>Mon, 09 Feb 2026 06:29:23 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>Rspack 1.7发布：2.0之前的最后一个1.x版本</title><description>&lt;p&gt;&lt;a href=&quot;https://rspack.dev/&quot;&gt;Rspack&lt;/a&gt;&quot;是一个基于&lt;a href=&quot;https://rust-lang.org/&quot;&gt;Rust&lt;/a&gt;&quot;的、旨在替代&lt;a href=&quot;https://webpack.js.org/&quot;&gt;webpack&lt;/a&gt;&quot;的高性能Web打包工具。Rspack 1.7版本发布，这是在项目过渡到2.0版本之前，1.x系列的最后一个小版本。该版本专注于提升现有功能的稳定性和插件的兼容性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7引入了多项增强稳定性的改进，包括：&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#improved-swc-plugin-compatibility&quot;&gt;增强SWC插件兼容性&lt;/a&gt;&quot;、&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#importing-assets-as-bytes&quot;&gt;原生支持以字节形式导入资源&lt;/a&gt;&quot;，以及固化多项&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#experimental-features-stabilized&quot;&gt;实验性功能&lt;/a&gt;&quot;。对于Web应用中动态导入的模块，该版本还引入了默认启用的懒编译。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7的一个新特性是改进SWC插件兼容性。在以前的版本中，由于AST结构不断演变，SWC Wasm插件面临着高昂的升级成本，使得现有插件在SWC升级后会出现问题。为此，Rspack团队&lt;a href=&quot;https://swc.rs/docs/plugin/ecmascript/compatibility&quot;&gt;向SWC社区贡献了兼容性改进&lt;/a&gt;&quot;，包括采用&lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc8949.html&quot;&gt;cbor&lt;/a&gt;&quot;序列化方案来替代版本敏感的&lt;a href=&quot;https://rkyv.org/&quot;&gt;rkyv&lt;/a&gt;&quot;，并在AST中引入了用于枚举类型的Unknown变体，以提高容错性。从Rspack 1.7开始，SWC升级不大可能再破坏之前使用旧版本SWC构建的插件了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack现在原生支持&lt;a href=&quot;https://github.com/tc39/proposal-import-bytes&quot;&gt;Import Bytes提案&lt;/a&gt;&quot;，即以字节形式导入资源。开发者可以用Uint8Array导入资源，并使用TextDecoder进行解码，语法如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import fileBytes from &#39;./file.bin&#39; with { type: &#39;bytes&#39; };
const decoder = new TextDecoder(&#39;utf-8&#39;);
const text = decoder.decode(fileBytes);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Rspack 1.7开始，在构建Web应用时，Rspack CLI针对动态导入模块默认启用&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#lazy-compilation&quot;&gt;懒编译&lt;/a&gt;&quot;。这一变化减少了初始构建中的模块数量，加快了开发服务器的启动速度。有特殊需求的开发者可以通过将lazyCompilation设置为false来显式地禁用这个功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本中，有几项实验性功能已经被固化。常量内联优化现在已经稳定，并且在生产构建中默认启用，原来的experiments.inlineConst选项被optimization.inlineExports所取代。TypeScript枚举内联优化和类型re-export检查也已去掉了实验性标志，达到稳定状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移到Rspack 1.7时需要注意下SWC插件的版本。使用SWC Wasm插件的项目必须升级插件，以兼容swc_core 54或以上版本，以避免构建失败。在他们的&lt;a href=&quot;https://rspack.rs/guide/faq&quot;&gt;FAQ文档&lt;/a&gt;&quot;中，Rspack团队提供了处理SWC插件版本不匹配问题的指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack的定位是&lt;a href=&quot;https://rspack.rs/guide/compatibility/plugin&quot;&gt;兼容webpack&lt;/a&gt;&quot;的替代方案，其构建速度明显更快。根据&lt;a href=&quot;https://medium.com/@yarindeoh/boost-your-build-time-by-70-with-rspack-a2dd3c47697c&quot;&gt;Medium上一位用户的记录&lt;/a&gt;&quot;，从webpack迁移到Rspack后，构建时间减少了70%，本地构建时间从1.7分钟降低到30秒。另一个来自Mews的团队&lt;a href=&quot;https://developers.mews.com/goodbye-webpack-hello-rspack-and-80-faster-builds/&quot;&gt;报告&lt;/a&gt;&quot;说，启动时间从三分钟减少到十秒，提高了80%。然而，&lt;a href=&quot;https://github.com/rolldown/benchmarks&quot;&gt;Rolldown项目的基准测试&lt;/a&gt;&quot;显示，尽管Rspack的性能优于webpack，但它仍然比esbuild和Rolldown等工具慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本还为更广泛的Rstack生态系统带来了更新：Rsbuild 1.7引入了运行时错误覆盖和资源大小差异报告；Rsdoctor 1.4新增用于包分析的树状图视图；Rslib 0.19稳定了打包模式中的ESM输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack是一个由字节跳动开发的开源项目。该工具旨在提供与webpack相当的API兼容性，同时借助Rust语言实现性能提升。如果既不想脱离webpack生态系统，又想加速构建流程，那么这个工具很合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/rspack-final-rust/&quot;&gt;https://www.infoq.com/news/2026/01/rspack-final-rust/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</link><guid isPermaLink="false">https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</guid><pubDate>Mon, 09 Feb 2026 05:51:07 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>微软推出面向.NET的Copilot自定义代理：C#专家与WinForms专家</title><description>&lt;p&gt;微软和GitHub扩展了Copilot生态系统，推出了&lt;a href=&quot;https://devblogs.microsoft.com/dotnet/introducing-custom-agents-for-dotnet-developers-csharp-expert-winforms-expert/&quot;&gt;首个专注于.NET的GitHub Copilot自定义代理&lt;/a&gt;&quot;，旨在提高C#和Windows Forms开发者的生产力和代码质量。作为更广泛的Copilot自定义代理发布计划的一部分，本次公告推出了两款专属代理：C#专家与WinForms专家，它们以代理指令Markdown文件的形式提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/CSharpExpert.agent.md&quot;&gt;C#专家代理&lt;/a&gt;&quot;旨在引导并强制执行现代C#最佳实践。它尊重项目约定，最小化不必要的代码工件，如未使用的接口或参数，并强调async/await模式要带有适当的取消和异常处理。它还支持行为驱动和集成测试工作流，帮助开发者编写更干净、更易于维护的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/WinFormsExpert.agent.md&quot;&gt;WinForms专家代理&lt;/a&gt;&quot;专注于使用Windows Forms进行传统的桌面UI开发。对于常见的UI设计模式（如MVVM和MVP），它拥有专业的知识，能够协助处理复杂的事件连接（event wiring）和状态管理，并能够增加保护措施，防止Copilot无意中修改.Designer.cs文件，对Visual Studio设计器造成破坏。对于使用生成工具的开发者来说，这种保护解决了一直以来开发者经常遇到的一个痛点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要使用这些代理，开发者需要从&lt;a href=&quot;https://github.com/github/awesome-copilot&quot;&gt;GitHub awesome-copilot存储库&lt;/a&gt;&quot;下载CSharpExpert.agent.md和WinFormsExpert.agent.md文件，并将它们放在项目的.github/agents文件夹下。配置文件放置到位以后，在通过GitHub将问题分配给Copilot时就可以实现上下文感知行为，开发者可以在Visual Studio Code Insiders或Visual Studio的实验版本中通过下拉菜单选择代理。Copilot CLI计划在未来的更新中支持/agent命令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软将这两个代理都描述为实验性的，因为他们正在收集模型对详细指令的响应反馈。自11月以来，在开发者打开“启用特定于项目的.NET指导”这一功能时，Visual Studio 2022 Insiders 17.14.21版本可以自动将相关的自定义代理附加到项目，例如为Windows Forms开发量身定制的指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期的社交媒体评论，尤其是LinkedIn平台上的讨论，反映出人们对该自定义代理发布公告的热情与专业关注。有评论者强调，通过减少生成未使用的代码，可有效&lt;a href=&quot;https://www.linkedin.com/posts/elias-asaid_introducing-custom-agents-for-net-developers-activity-7392063443195015169-PKsx/&quot;&gt;缓解“AI引发的技术债务”问题&lt;/a&gt;&quot;。他还指出，WinForms Expert提供的设计器文件保护机制，对遗留用户界面的维护与现代化改造显然是有实际好处的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比之下，Copilot自定义代理所采用的是一种更具结构化和持久性的AI辅助方式，与早期的 Copilot聊天模式或无上下文的建议引擎有着本质的不同。传统聊天模式提供的是按需帮助，而自定义代理则依据预定义的专业知识和行为特征在特定的存储库上下文中运行。这使得Copilot更符合新兴的&lt;a href=&quot;https://biilmann.blog/articles/introducing-ax/&quot;&gt;基于代理的开发体验&lt;/a&gt;&quot;，其中工具充当具有特定领域知识的合作伙伴，而非通用的助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，自定义代理服务于.NET开发中小众但影响力大的场景。其实验性状态和不断演变的工具支持表明，在扩大覆盖范围或在更广泛的Copilot体验中标准化工作流之前，微软正在密切倾听开发者的反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&quot;&gt;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</link><guid isPermaLink="false">https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</guid><pubDate>Mon, 09 Feb 2026 05:45:40 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>编程语言</category></item><item><title>优步将分布式存储从静态限制转向基于优先级感知的负载控制</title><description>&lt;p&gt;优步的工程师介绍了他们&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management/&quot;&gt;如何将分布式存储平台从静态限流演进为优先级感知的负载管理系统&lt;/a&gt;&quot;，以保护其内部数据库。这一改进解决了大型有状态多租户系统中基于QPS限流的局限性，那就是这种限流方式无法反映真实负载、难以处理 “噪音邻居” 问题，也无法保障尾部延迟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该设计保护了基于MySQL构建的&lt;a href=&quot;https://www.uber.com/blog/schemaless-sql-database/&quot;&gt;Docstore&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.uber.com/blog/schemaless-part-one-mysql-datastore/&quot;&gt;Schemaless&lt;/a&gt;&quot;存储系统，这些系统通过数千个微服务为超1.7亿月活用户（包括乘客、Uber Eats用户、司机和配送员）提供服务。通过优先保障关键流量并动态适应系统状态，该系统可防止级联过载，在大规模场景下维持性能稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步的工程师指出，早期基于配额的方案依赖集中式跟踪的静态限制，它的效果不佳。无状态路由层无法及时感知分区级负载，且相似大小的请求会产生不同的CPU、内存或I/O开销。运维人员需要频繁调整限流阈值，有时会误删健康流量，而过载分区却没有得到保护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如优步的工程师&lt;a href=&quot;https://www.linkedin.com/in/dhyanamvaidya/&quot;&gt;Dhyanam V.&lt;/a&gt;&quot;在&lt;a href=&quot;https://www.linkedin.com/posts/activity-7417190176965210112-b34i?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;LinkedIn帖子&lt;/a&gt;&quot;中所述：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有状态数据库的过载保护是大规模场景下的多维度问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这个问题，优步将负载管理与有状态存储节点协同部署，结合了&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=2209336&quot;&gt;受控延迟（Controlled Delay，CoDel）&lt;/a&gt;&quot;队列和租户级记分卡（Scorecard）。CoDel基于延迟调整队列行为，记分卡则强制实施并发限制，同时使用额外的调节器监控I/O、内存、goroutine和热点数据。CoDel对所有请求一视同仁，会同时丢弃低优先级和面向用户的流量，导致on-call负载增加、用户体验受损，并且依赖固定队列超时和静态的in-flight限制，可能引发惊群效应重试，甚至丢弃高优先级请求。尽管它能防止灾难性故障，但缺乏维持稳定性能所需的动态性和精细化能力，凸显了优先级感知队列的必要性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/2278d8fc2a95d1a318b31eb00726fbcb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用CoDel队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后续演进引入了Cinnamon，这是一款优先级感知的负载shedder系统，能够将请求分配到分级队列，优先丢弃低优先级流量，避免影响延迟敏感的操作。Cinnamon基于高百分位延迟指标动态调整in-flight中请求限制和队列超时，减少对静态阈值的依赖，在过载时实现更平滑的降级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/df/dfd1b94d2ce9529fd3ceb189c34d2ed1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用Cinnamon队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步后续通过“自带信号（Bring Your Own Signal）”模型，将本地和分布式过载信号统一到单一的模块化控制回路中。该架构允许团队将节点级指标（如in-flight中的并发数、内存压力）和集群级信号（比如，从节点的提交延迟）接入集中式的准入控制路径。整合这些信号消除了碎片化的控制逻辑，避免了早期基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Token_bucket&quot;&gt;令牌桶&lt;/a&gt;&quot;系统中出现的冲突性负载shedding决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据优步介绍，改进效果非常显著，过载场景下吞吐量提升约80%，upsert操作的P99延迟降低约70%；goroutine数量减少约93%，峰值堆内存使用降低约60%，整体效率得到了提升，同时缓解了运维的负担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步总结了负载管理演进的核心经验，那就是优先保障关键用户流量，先丢弃低优先级请求；尽早拒绝请求以维持可预测延迟、降低内存压力；使用基于PID的调节确保稳定性；将控制逻辑部署在数据源附近；动态适应工作负载；保持可观测性；优先采用简单设计，确保压力下的稳定可靠运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-priority-aware-load-manager/&quot;&gt;Uber Moves from Static Limits to Priority-Aware Load Control for Distributed Storage&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</link><guid isPermaLink="false">https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</guid><pubDate>Mon, 09 Feb 2026 05:43:18 GMT</pubDate><author>作者：Leela Kumili</author><category>云计算</category></item><item><title>“千问奶茶”二手平台6元转售；追觅俞浩：年终奖最高20个月奖金，总量会达到10亿级；京东001 号快递员：退休金 4000 多，存款百万 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;追觅俞浩：年终奖最高 20 个月奖金；马云现身阿里总部千问春节项目组；京东物流公开 001 号快递员退休生活；钉钉大楼换 LOGO 硬刚飞书；顶级域名 AI.com 被币圈人士以 7000 万美元拍下；快手回应被罚 1.191 亿元；美团 49.8 亿收购叮咚买菜；甲骨文被曝或裁员 3 万人；蚂蚁数科 CEO 赵闻飙发全员信；Anthropic 和 OpenAI 因投放广告隔空嘲讽；高通骁龙 Oryon CPU 架构之父宣布离职……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;追觅俞浩：年终奖最高20个月奖金，总奖金规模会达到10亿量级&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月7日消息，追觅科技创始人兼CEO俞浩连发两条微博回应“演唱会投入过多”质疑，俞浩表示，演唱会几千万的投入，仅仅相当于公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万。他还透露，这两天在审批各个事业部递交过来的年终奖方案。主营业务，公司把净利润的18%作为奖金发放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！年终奖的总奖金规模，会达到10亿量级。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下为其微博原文：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有人说：把这么多钱花在演唱会上，不如好好花在产品研发上。那你是不知道我们在产品研发上投入了多少。&amp;nbsp;演唱会几千万的投入，仅仅相当于我们公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万，你没看错，是每天。也就说，我每天一阵开眼，至少要有4000万花在研发费用和员工工资上。&amp;nbsp;还有人说：不如把演唱会的花费，直接发给大家实在。正好这两天，我在审批各个事业部递交过来的年终奖方案。主营业务，我们把净利润的18%作为了奖金发给了大家，这是很高的比例！因为我们今年的主营业务的净利润是全行业最高的。这是纯现金部分，还没有算平时的任何福利。盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！（当然这是最优秀的）&amp;nbsp;我们的年终奖的总奖金规模，会达到10亿量级。我们对人才的投入是不遗余力的，但凡条件允许，我都会投给人才！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马云现身阿里总部千问春节项目组，二手平台惊现“千问奶茶”6元转售&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近期，阿里员工分享马云现身阿里千问项目组办公点。2月6日一早，千问APP“春节30亿大免单”正式上线，发动奶茶攻势，邀请全国人民用AI一句话免费点奶茶。由于活动过于火爆，导致大量用户涌入，很快把APP挤爆了。不少用户打开活动页面，却发现“千问请客”页面无法点击，页面信息显示“活动太火爆，请稍后再试”。领到免单卡后让它去点单，AI也回答：当前使用千问闪购点单的人数较多，我正在全力处理中，建议稍等片刻后再试一次。随后，APP红包分享链接已被微信屏蔽，口令已无法复制使用。好在除相关活动外，问答功能仍可正常使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当日，千问APP“春节30亿免单”上线5小时突破500万单，并超越豆包和元宝，登顶苹果App Store免费榜，排序形成“千元豆”格局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据媒体在北京某商圈搜索查询发现，受活动爆单影响，多家奶茶门店因单量超负荷，已临时暂停营业。千问APP活动也带动港股茶饮股多数上涨，截至发稿，古茗涨超4.12%创上市来新高，沪上阿姨和茶百道涨超3%，奈雪的茶、蜜雪集团也跟随上涨。&amp;nbsp;不过，港股阿里巴巴当天低开3.82％，收盘跌2.9%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有人在二手交易平台上卖起了千问奶茶，售价6元至10元不等，声称只需提供收货地址和电话，即可代顾客下单购买。据媒体报道，千问客服对此回应称，该免单权益属于虚拟优惠，不支持转让、转赠、转售或任何形式的变现，因此无法在二手交易平台出售。如发现用户存在倒卖、恶意套现等行为，主办方有权取消其参与资格，并冻结或收回其全部活动权益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问此前宣布将投入30亿元启动“春节请客计划”，以免单形式请全国人民在春节期间吃喝玩乐，感受AI时代的全新生活方式。淘宝闪购、飞猪、大麦、盒马、天猫超市、支付宝等多项阿里生态业务，也将联动加入千问春节攻势。2月7日，千问宣布免单卡可以在千问App里买天猫超市的酒水零食、米面粮油、家居日用、生鲜水果等，同时还将免单卡的有效期延长至2月28日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问在春节活动的第一天，微信链接被屏蔽，部分用户在千问APP点击分享活动至微信好友时，已自动改为复制口令形式。此外，腾讯元宝、百度文心助手红包分享链接也被微信屏蔽。值得关注的是，有消息称，马化腾很重视此次元宝红包活动。他在腾讯年会上，表示希望重现当年微信红包的盛况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;京东物流公开 001 号快递员退休生活：退休金 4000 多，存款百万&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;京东黑板报微信号2月6日发文，公开了001号快递员金宜财退休后的生活状况。京东透露，金宜财现在过得充实富足，每月4000多元养老金准时到账，还靠着打拼积攒下的积蓄和理财存到了一百多万。退休前，他已帮两个儿子在南京、无锡成家置业。金宜财表示，“房子不缺，车也有了，就是浑身还有使不完的劲”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;钉钉大楼换LOGO硬刚飞书，网友：商战总是朴实无华！&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月4日消息，据媒体报道，钉钉又整活了，这次直接把杭州总部大楼的 LOGO 给换了。网友：好朴实的商战！从新旧 LOGO 对比图可以看出，钉钉直接蓝色翅膀+文字，改成了一个头戴凤翅紫金冠、扛着金箍棒的齐天大圣版“钉三多”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于更换 LOGO 的原因，网传钉钉 CEO 陈航（花名无招）发现，隔壁飞书大楼的 LOGO 比自家的高了一截，心里不服气，索性把 LOGO 换成孙悟空硬刚。据悉，钉钉和飞书的总部大楼都在杭州未来科技城，两家仅隔一条马路，飞书 LOGO 的高度确实略胜一筹。报道称，恰巧钉钉有个核心团队叫 “西游记团队”，选孙悟空的形象再合适不过，选孙悟空形象贴合企业文化，又因飞书LOGO形似飞鸟，暗合“孙悟空棒打出头鸟”梗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友调侃：“风水大师介入的结果哈哈”、“之前是鸟吃蚊子，现在是棒打鸟，哈哈哈”。网友也纷纷为飞书出招，建议其在斜对面设如来手掌 LOGO “压制”齐天大圣。据了解，钉钉向来是品牌界“整活高手”，吉祥物“钉三多”此前就成“网红”，和多邻国小绿鸟“暧昧”“有孩子”，偶尔拉上淘宝淘公仔演 “三角恋”，玩梗玩得飞起。甚至在杭州亚运会期间，钉钉也紧跟热点，把 LOGO 换成紫红渐变色，谐音“紫钉行”讨彩头。报道称，钉钉这波 LOGO 更换，或是品牌趣味商战的一种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;顶级域名 AI.com 被币圈人士以 7000 万美元拍下，创域名交易价格记录&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;域名交易领域再创新高，超级热门的人工智能域名 AI.com 被币圈人士以 7000 万美元天价买下。该域名最初注册于 1993 年 5 月 4 日，此前多次交易未作为产品网站，ChatGPT 发布后持有者将其跳转到不同 AI 工具蹭热度。Crypto.com 创始人 Kris Marszalek 于 2025 年 4 月买下该域名，目标是推出基于去中心化且能持续自我改进的人工智能代理网络，加速通用人工智能 AGI 到来。网站尚未开放，从倒计时看正式发布时间在 2 月 9 日前后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;快手回应被罚1.191 亿元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据网信北京公众号消息，2 月 6 日，北京市互联网信息办公室宣布对北京快手科技有限公司处以警告并罚款 1.191 亿元人民币。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;处罚原因是快手平台未履行网络安全保护义务，未及时处置系统漏洞等安全风险，未对用户发布的违法信息立即采取停止传输、消除等处置措施，情节严重，影响恶劣。快手方面此前将此次事件归咎于黑灰产攻击，但监管部门显然不接受这一说辞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月6日，快手发布声明称，网信部门公布了对快手作出的处罚。对此，公司诚恳接受，坚决整改。由于公司技术管理原因，应急处置不及时，导致平台出现大量色情低俗内容，造成恶劣影响。事件发生后，公司全面排查风险意识、安全基建、应急响应和内部管理等方面存在的问题，积极采取多种措施补齐短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;美团 49.8 亿收购叮咚买菜&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，美团在港交所发布公告，宣布以约 7.17 亿美元（约 49.8 亿元人民币）的初始对价，完成对叮咚买菜中国业务 100% 股权的收购。对于收购原因，美团在公告中表示，公司高度重视食杂零售业务，本次交易符合公司在食杂零售领域的长期发展规划。截至 2025 年 9 月，叮咚买菜在国内共运营超过 1000 个前置仓，月购买用户数超过 700 万。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;叮咚买菜创始人梁昌霖也在内部信中表示，对于未来的发展选择放下竞争，转为并肩合作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8d37db27d1a998e4060d15854a59f2e3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;甲骨文被曝或裁员3万人，多家美银行停止相关数据中心项目贷款&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据媒体报道，据道明证券旗下投行TD Cowen分析，由于人工智能数据中心扩张面临融资困难，科技巨头甲骨文公司正陷入严峻的资金压力，公司考虑采取大规模裁员及出售部分业务等措施来应对。TD Cowen研究报告显示，甲骨文公司计划裁员2万至3万人，此举预计将释放80亿至100亿美元的现金流。此外，甲骨文还在考虑出售其于2022年以283亿美元收购的医疗保健软件部门Cerner。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;蚂蚁数科 CEO 赵闻飙发全员信，宣布将成立“大模型技术创新部”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日晚间消息，新浪科技获悉，日前，蚂蚁数科 CEO 赵闻飙发布主题为《携手共进，迈向大模型新时代——关于大模型组织架构升级的通知》的全员信，宣布蚂蚁数科将成立“大模型技术创新部”，构建面向 To B 场景的基础大模型及行业模型。据了解，该团队将与蚂蚁集团相关团队协同，攻坚蚂蚁集团百灵大模型面向 To B 场景的商业化，推动全球企业更好地进入 AI 时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic 和 OpenAI 因投放广告隔空嘲讽&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间周三，Anthropic 首次在“超级碗”(Super Bowl) 比赛里打广告，并且将矛头对准了竞争对手 OpenAI。广告内容围绕在人工智能对话助手中进行对话时，不合时宜的广告可能对用户产生的不适影响，嘲讽 OpenAI 将在对话中加入广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 同时表示，永远不会在对话中插入广告。广告模式可能与 Claude 的核心原则产生冲突。Anthropic 举例说明：当用户提到睡眠困难时，无广告的助手会基于用户需求探索各种可能原因；而广告支持的助手则可能考虑对话是否存在交易机会。这种激励结构会让用户难以判断 AI 的建议是否带有商业动机。Anthropic 表示将继续通过企业合同和付费订阅获得收入，并将收益投入 Claude 的改进。该公司同时在探索智能体商务等功能，允许 Claude 代表用户完成购买或预订，但所有第三方交互都将由用户主动发起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 的 萨姆·奥特曼于2 月 5 日凌晨在 X 上回应了 Anthropic，称其内容不实且具有误导性。重申 OpenAI 致力于让每个人都有接触人工智能的权利，同时提到 Codex 下载已经超过 50 万次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高通骁龙Oryon CPU架构之父宣布离职&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日消息，NUVIA创始人、高通自研Oryon CPU首席架构师杰拉德·威廉姆斯三世宣布离职，结束他在高通四年的职业生涯。高通此前斥资1.4亿美元收购NUVIA公司，NUVIA创始人杰拉德·威廉姆斯三世随后入职高通。在他的参与下，搭载第三代Oryon内核的骁龙8 Elite Gen5（第五代骁龙8至尊版）得以问世。凭借NUVIA带来的技术资产，高通的Oryon自研架构已成为抗衡苹果Silicon核心的秘密武器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，杰拉德·威廉姆斯三世曾在Arm工作了12年，是开发Cortex-A8和Cortex-A15等多个核心架构的关键人物。在入职苹果公司后，他成为苹果A7-A12X自研芯片组的首席架构师，拥有丰富的芯片设计经验，而且他是苹果公司60多项专利的共同发明人，涉及功耗管理和多核技术。离开苹果公司之后，杰拉德·威廉姆斯三世和合伙人共同创立了NUVIA公司，这家公司后来被高通收购，杰拉德·威廉姆斯三世顺利加盟高通，在高通工作四年之后，他最终选择了离职。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杰拉德·威廉姆斯三世在社交平台上表示，我现在正享受与家人团聚的珍贵时光，我在高通的旅程已经画上句号，感谢过去四年里与我并肩作战的所有人，现在人生的新篇章开始了——就从粉刷我的房子和完成那张攒了很久的任务清单开始吧。感谢NUVIA那些了不起的朋友和同事们，是你们让这段旅程成为可能。杰拉德·威廉姆斯三世离职后，高通在自研CPU的开发上是否会陷入困境尚无定论，高通很可能在他任职期间聘请并培养了多位人才，足以在他辞职后填补空缺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元不参加春晚，将举办全球首个大型机器人晚会&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 2 日消息，知情人士称，智元将不参加 2026 年马年春晚。原因是预算有限的前提下，智元优先保障具身智能技术及产品研发的费用。另据知情人士透露，智元机器人正在彩排全球首个大型机器人晚会“机器人奇妙夜”，该晚会由数百台各类机器人主导，集合唱歌、跳舞、小品、走秀等多元节目，将于近期上线直播。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 推出 macOS 版 Codex 应用和企业级平台 Frontier&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日消息，OpenAI 推出了适配 macOS 系统的全新 Codex 应用，整合了过去一年间广泛流行的各类智能体化开发逻辑。这款新应用支持多智能体并行作业，可融合不同智能体的能力，以及当前最前沿的工作流程。此次发布距离 OpenAI 推出其最强编码大模型 GPT-5.2-Codex，尚不足两个月，公司希望凭借该模型吸引 Claude Code 的用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 首席执行官萨姆・奥特曼在媒体电话发布会中表示：“若要处理复杂场景下的高精尖开发工作，GPT-5.2 是目前性能最强的模型。但它此前的使用门槛偏高，因此我们认为，将这款模型的强大能力封装进更灵活的交互界面，会具备极为重要的价值。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款 Codex 应用还搭载了多项全新功能，这些功能将帮助其达到与各类 Claude 应用相当的水平，部分场景下甚至实现反超。应用支持设置自动化任务，可按预设计划在后台自动运行，执行结果会存入队列，待用户返回后统一查看。用户还能根据自身工作风格，为智能体选择不同交互风格，从务实理性型到共情沟通型均可切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，OpenAI 周四发布新的人工智能平台 Frontier，该平台可以帮助公司构建、部署和监督 AI 智能体。OpenAI表示，Frontier 与之前发布的 AI 智能体构建工具协同工作，让企业能更轻松地整合智能体执行任务所需的数据源。OpenAI 称，这些智能体将能够处理来自各种来源的信息，并完成处理文件和运行代码等任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic发布Claude Opus 4.6&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间2月5日，Anthropic宣布，推出升级版智能模型Claude Opus 4.6，该模型能更谨慎地规划，更长时间地执行代理任务，在大规模代码库中可靠运行，并能纠正自己的错误Anthropic称，这款名为Claude Opus 4.6的版本能够检视企业数据、监管备案文件和市场信息，并生成详细的金融分析报告，通常这类工作通常需要人工耗时数天才能完成。该消息发布后，金融服务公司股价应声下跌，FactSet跌幅一度高达10%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里千问发布 Qwen3-Coder-Next：低推理成本编程智能体模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日消息，阿里巴巴千问宣布推出 Qwen3-Coder-Next，一款专为编码代理与本地开发打造的开放权重的语言模型。该模型基于 Qwen3-Next-80B-A3B-Base 构建，采用混合注意力与 MoE 的新架构；通过大规模可执行任务合成、环境交互与强化学习进行智能体训练，在显著降低推理成本的同时，获得了强大的编程与智能体能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen3-Coder-Next 不依赖单纯的参数扩展，而是聚焦于扩展智能体训练信号，使用大规模的可验证编程任务与可执行环境进行训练，使模型能够直接从环境反馈中学习。该配方强调长程推理、工具使用以及从执行失败中恢复，这些对现实世界中的编程智能体至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;面壁智能首款 AI 硬件“松果派”官宣今年上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，面壁智能官宣，面壁首款 AI 硬件松果派（Pinea Pi）—— 一款 AI 原生（AI Native）的端侧智能开发板将于今年上市，帮助开发者快速开发端侧智能硬件，即使无技术背景，也可快速上手开发。面壁智能介绍称，“松果派”源自《三体》，松果鳞片也如斐波那契数列，犹如“二向箔”对于“维度可被折叠和展开”的隐喻，寓意着以更高的维度、更全的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松果派将支持离线多模态个人知识助理、具身智能、编程教具等场景的全栈开发，面壁智能多模态大模型 MiniCPM-V、全模态大模型 MiniCPM-o 开箱即用。松果派基于 NVIDIA Jetson 系列模组打造，内置麦克风、摄像头、丰富的接口等多模态硬件组件，便于开发者开发和调用，构建了一套软硬一体、全栈覆盖的端侧 AI 软件体系。据悉，松果派预期年中正式量产上市，定价暂未公布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阶跃星辰开源 Step 3.5 Flash&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;国产大模型开源阵营又添一员。2月2日，阶跃星辰发布Step 3.5 Flash，定位为“为Agent而生”的开源基座模型，主打推理速度、Agent能力和长链条任务稳定性。这款模型的参数总量达到1960亿，但采用稀疏MoE架构，每个token仅激活约110亿参数。配合MTP-3多token预测机制和3:1滑动窗口注意力架构，官方宣称推理速度最高可达350 TPS，支持256K上下文长度。核心卖点是三个词：更快、更强、更稳——快在推理速度，强在Agent和数学任务表现，稳在复杂长链条任务的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部署方式上，阶跃星辰这次给得很全。开发者可以通过OpenRouter限时免费调用API，也可以从GitHub和HuggingFace下载模型权重自行部署。普通用户则可以在阶跃AI的App和网页端直接体验。值得注意的是本地部署的支持范围。官方表示已专门优化本地运行性能，支持在个人工作站上流畅运行，兼容设备包括NVIDIA DGX Spark、Apple M3/M4 Max以及AMD AI Max+ 395。一个1960亿参数的模型能在消费级硬件上跑起来，背后是稀疏激活架构带来的实际计算量压缩，110亿的激活参数让这件事成为可能。阶跃星辰还透露，Step 4模型已启动训练，并开放Discord社区邀请开发者参与共创。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;摩尔线程AI Coding Plan上线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日，摩尔线程正式推出AI Coding Plan智能编程服务。作为首个基于国产全功能GPU算力底座构建的智能开发解决方案，该服务以MTT S5000强劲的全精度计算能力为核心驱动，融合硅基流动推理加速引擎，并集成GLM-4.7顶尖代码模型，实现了国产芯片与国产大模型在AI Coding领域的关键突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;全球“最快”人形机器人发布&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月2日，浙江大学杭州国际科创中心人形机器人创新研究院正式发布全尺寸人形机器人Bolt。该机器人以10米/秒的奔跑时速，成为目前全球跑得最快的人形机器人。该成果由科创中心联合镜识科技、凯尔达共同研发完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日，2026年支付宝集福活动首次上线智能穿戴设备扫福功能。用户佩戴夸克、乐奇Rokid等品牌的AI眼镜，无需操作手机，通过“看一下”配合“说一句”的简单方式，即可完成扫福、集卡、分享等全流程。2 月 3 日，苹果公司宣布，将在旗下旗舰编程工具Xcode中引入智能体编程功能。借助智能体驱动的编程技术，程序员可让人工智能软件自主完成代码编写工作。苹果表示，其Xcode工具将支持Anthropic的Claude智能体和OpenAI的Codex代码工具。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</link><guid isPermaLink="false">https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</guid><pubDate>Mon, 09 Feb 2026 02:34:09 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>“公司终局是纯 AI、纯机器人！”马斯克酒后激进预言：一小时一发 Starship，让Optimus 造Optimus 巨便宜</title><description>&lt;p&gt;近期，马斯克参与了一场近3个小时的深入对话，讨论了太空数据中心的经济效益、地球电力规模化挑战、在美大规模生产人形机器人需要的条件，其中他讲解了很多关于工程和供应链的细节。此外，他也透露了 SpaceX 和 xAI 的商业模式和战略规划，此外还分享了自己的管理哲学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克开篇就指出，把算力搬到太空，根本不是为了省电费，而是为了给“电不够用”这件事找一个终局解：芯片算力在指数级增长，但地面发电扩张跟不上，地面扩张的阻力远大于太空。至于“太空里 GPU 坏了就报废”的质疑，他的回应是维修不是关键，关键在前期筛选与稳定后的可靠性，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，很多软件从业者很快会被硬件“教育”：未来的瓶颈不是模型，而是电力、变压器、电网、燃气轮机叶片这类物理供给链，想扩就扩不了；制造能力才是底层瓶颈，甚至逼到 Tesla/SpaceX 可能要自己做涡轮机叶片和导流片。因此，把 AI 放到太空反而可能成为生成 token 最便宜、扩展最容易的方式，他甚至给出激进判断：五年后太空每年新增并运行的 AI 总量，会超过地球历史累计总量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了跑赢这些瓶颈，他的管理逻辑依旧是“哪里卡就打穿哪里”：如果瓶颈是钱，就去解决钱，IPO 的核心价值不是估值而是速度；对于当前的AI公司，他犀利指出他们不该叫自己实验室，本质上是收入最大化的公司，绝大多数工作是工程落地与规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在马斯克眼里，最终落点则更冷酷：最强的公司形态会是纯 AI + 纯机器人闭环，人类留在流程里就像让人去算 spreadsheet 的一部分格子，只会更慢、更差，闭环效率才会决定胜负。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是这次详细对话，我们进行了翻译并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI上太空，芯片不是关键&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你比谁都清楚，数据中心的总拥有成本里，电力只占百分之十到十五。你把算力搬到太空，省下的主要就是这部分。但真正的大头是 GPU，而在太空里基本没法维修，一坏就报废，折旧周期更短，成本反而更高。所以把 GPU 放到太空，本质上更贵。那到底为什么要放到太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：核心问题是能源。你看看中国以外的地区，发电量基本是持平的，最多小幅增长，只有中国在快速增长。如果数据中心不建在中国，那电从哪来？尤其是规模越来越好大。芯片算力几乎是指数级增长，但发电量基本不动，你怎么给这些芯片供电？难道靠什么“魔法电源”“电力小精灵”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直是太阳能的坚定支持者。如果做到1太瓦（TW）太阳能，就算按25%效率算，也只需要4太瓦面板，占美国国土面积的1%左右。等我们有一太瓦数据中心的时候，是不是已经进入“奇点”了？我们走到哪一步了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，我们可能已经开始进入所谓的“奇点”，但离终点还很远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以，是不是先把内华达铺满太阳能，再考虑上太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：在地面铺太阳能，最大的难点是审批。你去试试拿许可就知道了，特别难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以太空，其实是监管上的捷径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不只是监管。地面扩张难度远大于太空。而且在太空里，太阳能效率大概是地面的五倍，还不需要电池。我本来差点穿一件写着“太空永远大晴天”的衣服。这是事实，那里没有昼夜循环、没有季节、没有云层，也没有大气层。要知道，光大气层就会损失大约30%的能量。所以在太空里，同样的太阳能板，发电能力是地面的五倍，还省掉储能成本。综合下来，其实更便宜。我预测，未来三十到三十六个月内，把 AI 放在太空，将是成本最低的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 GPU 坏了怎么办？训练时坏得挺频繁的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：关键在于前期筛选。我们现在发现，GPU 的可靠性其实挺高。可以先在地面跑测试，筛掉问题芯；等过了调试期，不管是 NVIDIA、Tesla AI 芯片、TPU，稳定后都很可靠，所以维修不是关键问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我敢打赌，三十个月左右，太空会成为最具经济性的 AI 部署地。而且，真正能无限扩展的地方，只有太空。当你开始考虑“我们能利用太阳多少能量”时，就会发现，地球根本不够用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的规模，是太瓦级别？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。美国全国平均用电量也就半太瓦，一太瓦等于两个美国。你能想象建这么多电厂、数据中心吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多做软件的人，根本没意识到自己马上要被硬件“教育”了。建电厂极其困难，不仅要电厂，还要变压器、电网设备。公用事业公司本身节奏非常慢，还要层层审批，你要跟他们签一个大规模并网协议，可能一年后才给你答复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们不是自己搞了“表后电力”系统吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，我们在 xAI 就这么干过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不把电厂和 GPU 一起建？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们就是这么做的。但问题是，电厂设备从哪来？制造能力本身是瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：是燃气轮机排期的问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：更底层的问题是叶片和导流叶片。它们的铸造工艺极其复杂，是关键瓶颈。现在只有少数几家公司能做，而且全都排满了。太阳能理论上能扩展，但美国对进口太阳能的关税很高，本土产能又很弱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那干脆自己造太阳能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们正在做。SpaceX 和 Tesla 都在向100吉瓦级产能推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从多晶硅到电池板，全产业链？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，必须全做。而且太空用太阳能板更便宜，不需要厚玻璃、不需要重型框架，也不用抗风雨。没有天气影响，结构可以很轻。所以，太空用电池板反而更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：三年内能做到足够便宜吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在太阳能已经非常便宜了。中国大概两三毛钱一瓦。放到太空后，乘以效率优势，再去掉电池，综合下来有接近十倍优势。一旦发射成本下来，太空将是生成 token 最便宜、最容易扩展的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在地面扩展迟早会撞上电力天花板。我们为了上线一吉瓦电力，在 xAI 做了大量工作：买涡轮机、解决审批。我们在田纳西受阻，又跑去密西西比建厂，还要拉高压线，难度极大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人根本不懂，一个数据中心真正需要多少电。除了 GPU，还有网络设备、CPU、存储、散热。尤其是散热，在最热的时候也要顶得住。在孟菲斯这种地方，光散热就多出40%能耗，再加上设备检修冗余，又得多准备二三成。综合下来，一万个 GB300 规模的数据中心，大约要300兆瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：再说一遍？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：大概三十多万块 GB300，加上所有配套和余量，发电量要接近一吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我问个外行的问题。地面有工程难题，太空也一样。通信、辐射、防护，这些怎么解决？为什么你觉得这些比多建电厂更容易？毕竟，地面已经有成熟厂商在造涡轮机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你试试就知道了。现在涡轮机的排期已经到了 2030 年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们考虑自己造吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：为了拿到足够电力，SpaceX 和 Tesla 可能最终要自己做叶片和导流叶片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：只做叶片？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，其他都能买到，唯独叶片和导流叶片最难。全球只有三家铸造厂能做，订单全都严重积压。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这些铸造厂是 Siemens、GE 之类的大公司，还是分包商？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：主要是其他专业公司。有些整机厂自己也有一点铸造能力，但规模都不大。你随便打电话问一家做涡轮机的厂家，他们都会告诉你现状，这不是什么秘密，网上基本都能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Colossus 会不会主要靠太阳能供电？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那样会容易得多。但现在关税高得离谱，有些产品甚至是几百个百分点。而且你也知道，现在的政府，对太阳能并不是特别友好。再加上土地、审批这些问题，如果你想快速扩张，现实阻力非常大。我确实认为，在地面发展太阳能是对的方向，但你需要时间去找地、拿许可、配套储能系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不自己把太阳能产能拉起来？你说以后会缺地没错，但现在德州、内华达还有大量土地，很多还是私有土地。至少可以先撑起下一代、下下一代 Colossus，等真到瓶颈再说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：正如我说的，我们正在扩张太阳能产能。但实体制造的扩张是有极限速度的，不可能无限加速，我们已经在用最快速度扩大本土生产。Tesla 和 SpaceX 都有明确目标：做到每年一百吉瓦的太阳能产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;太空运行的AI 量会超地球， SpaceX 成超级算力供应商&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说到产能，我很好奇，五年后地面和太空的 AI 装机规模会是什么比例？我特意选五年，因为那时你的系统应该已经跑顺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：五年后，我的判断是，每年在太空部署并运行的 AI 总量，会超过地球上历史累计总量。也就是说，每年新增的太空算力，会比地面过去所有加起来还多。我预计，五年后，太空 AI 装机规模会达到每年几百吉瓦，并持续增长。在地球上，你最多也就做到一太瓦左右，再往上就会遇到火箭燃料等瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：按这个规模算，包含太阳能阵列、散热系统等，大概需要一万次 Starship 发射。也就是说，一年一万次，相当于每小时一发。你能描述一个“每小时都在发射星舰”的世界吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：其实这和航空业比，还算低频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但飞机有很多机场，而且火箭还涉及轨道问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不一定非要极轨，飞得够高就能避开地影区，限制没那么大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：一年一万次发射，大概需要多少艘 Starship？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不需要太多，理论上二三十艘就够了，关键看周转效率。如果一艘船三十小时能周转一次，三十艘就能跑满。当然，我们会造更多。SpaceX 正在为每年一万次，甚至两三万次发射做准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你的目标是不是把 SpaceX 做成太空算力的“云服务商”？像 Oracle、AWS 那样，把算力租给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果我的预测成立，SpaceX 在太空部署的 AI 算力，会超过地球上所有机构加起来的总和。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：主要是推理算力，还是训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：绝大多数都会是推理，现在已经是这样了，推理规模远超训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SpaceX IPO，速度解决钱的问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在外界有一种说法：关于 SpaceX IPO 的讨论升温，是因为以前你们非常“资本高效”，花钱不多。但接下来，你们可能需要的资金规模，已经超过私募市场能承受的范围，就算 AI 实验室能融到几十亿美元，也有上限。是不是以后每年都要超过百亿美元？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我对讨论潜在上市公司一直比较谨慎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就泛泛而谈一下吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这可不像你，Elon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：说话是要付出代价的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那从宏观角度看，公募和私募市场的资金深度差别有多大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：总体来说，公募市场的资金量，远远超过私募，至少多两个数量级，可能是一百倍以上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：但像房地产这种高度资本密集行业，往往主要靠债务融资。因为当规模大到一定程度，其实现金流已经比较稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个问题没法简单回答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，就是短期回报这件事。你看数据中心扩建，很多都是靠私募信贷在融资。那为什么不直接用债务融资？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只看一件事：速度。我做事的习惯是反复盯住“限制速度的瓶颈”，然后把它打穿。如果唯一瓶颈是钱，那我就去解决钱；如果钱不是瓶颈，那我就去解决别的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但按你过去谈 Tesla、谈上市公司的态度，我原本以为你不会觉得“想快就得上市”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：通常确实不是。我其实想讲得更具体一点，但问题是一旦你公开讨论“可能要上市的公司”，就会惹麻烦，甚至影响发行节奏，最后反而拖慢速度。所以我们得谨慎一点。但有些东西是可以公开讲的，比如物理规律。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从长期扩张角度看，地球接收到的太阳能，只占太阳总能量的极小一部分。太阳几乎是宇宙里最主要的能源来源，这点必须先看清。有人会讨论多建核电、搞聚变之类的“边际方案”，但你退一步想：如果你想利用太阳能里一个并不夸张的比例，比如百万分之一，听起来挺酷，但对应的电力规模大约是人类文明目前总发电量的十万倍量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结论很直接：真正能规模化的，只有去太空用太阳能。从地球发射，把太空算力推到每年一太瓦左右，差不多就是极限。再往上，你得从月球发射，得在月球搞“mass driver”（电磁弹射器）这类东西，那样可能做到每年拍瓦级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：可在到达这个规模之前，你肯定会先撞上别的瓶颈，你要芯片，要逻辑、要内存。太阳能板效率提高了不代表这些就不缺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：芯片得做更多，而且得更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那问题来了，现在全球算力也就几十吉瓦级别，你怎么把逻辑算力拉到太瓦级？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那就得做一件“非常大”的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是要放大招了，讲讲？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我公开提过一个想法：做“超大规模”的芯片产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Tesla 的命名一直很抓人。你现在是按“计量单位”来命名了吗？更关键的是你打算做到产业链哪一层？建洁净室？和谁合作拿制程？设备怎么买？计划到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你不能指望跟现有 fabs （半导体加工厂）合作解决产能问题，他们的输出不够，规模差得太远了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就合作拿 IP、拿工艺？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在的 fabs，本质上离不开几家设备公司：ASML、Tokyo Electron、KLA、Applied Materials、Lam Research 这些。一开始你得用他们的设备，而且可能要跟他们一起把产能拉上去。但要到真正规模化，你必须用一种“不同的方式”建 fab：先用常规设备、非常规方法把规模跑起来；再逐步改造设备，加快速度提高产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有点像 The Boring Company （马斯克2016年创立的隧道挖掘公司）的打法：先买现成盾构机，先把隧道挖起来，再自己做一台快一个数量级的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，就是这个逻辑。先跑通，再重做，最后提速到数量级提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中国关键在“复制 ASML”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：领先制程芯片、先进涡轮发动机这些，中国还在追，中国都没复制出 TSMC，会不会让你对“建 fab 的难度”更谨慎？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不完全同意，关键瓶颈不在“复制 TSMC”，而在“复制 ASML”，那才是最卡脖子的地方。我认为中国会在未来几年做出相当有竞争力的芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你会考虑自己做 ASML 那种设备吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在还不好说。但如果我们想在三十多个月内把产能拉到极高规模，必须把“火箭送上去的能力”和“能供得上电、供得上芯片”的能力配平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;假设我们把上天的运力做到百万吨级，再按每吨对应十万瓦级别需求算，那就意味着：每年至少要新增百吉瓦级太阳能，同时还得有同量级的芯片供给去匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我真正担心的，其实是内存。芯片怎么扩产，路径相对清晰，“足够的内存”更难。这也是为什么你会看到 DDR 价格起飞。网上还有那种段子：被困荒岛写“DDR”求救，结果船就来了，因为大家都在抢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听上去你觉得那些掌握细节的人，比如那些知道等离子腔体里放什么气、工具参数怎么调的人，并不是不可替代的。你的思路更像：先把洁净室和设备弄齐，然后把流程跑出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这事不靠一堆 PhD。工程大多数时候也不是 PhD 在做。这类工作也不需要 PhD，但确实需要非常强的工程团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿 Tesla 来说，我们现在在全力把 Tesla AI 芯片推进量产、推到规模，我们已经把能拿到的代工产能都锁定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你现在受限于 TSMC 的产能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会用 TSMC China Taiwan、Samsung Korea、TSMC Arizona、Samsung Texas，能订的都订了。你去问 TSMC CEO、问 Samsung，从建厂到真正爬完良率曲线、达到高良率的大规模量产，完整周期大概五年，所以现在最直接的瓶颈就是芯片。一旦你能把发电搬上太空，能源瓶颈解除，新的瓶颈就会变成芯片。但在能上太空之前，最大的瓶颈还是电力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那为什么不学 Jensen（黄仁勋）那套：提前给 TSMC 预付款，让他们专门给你多建几座 fab？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我已经这么说过了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那他们怎么不收你的钱？发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：他们已经在能快的极限里拼命建厂了，Samsung 也是。但即便这样，还是不够快。我的判断是：到今年年底，芯片产量可能会超过“把芯片点亮”的能力。你会看到芯片越堆越多，但数据中心开不起来，因为电不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过这只针对大集群的数据中心，边缘计算是另一回事。比如 Tesla 的 AI 芯片会进 Optimus robot、也会进车。这类算力分布在广阔区域里，电力也是分布式的，不是集中消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，你可以夜间充电。美国峰值供电能力其实能超过一千吉瓦，但因为昼夜周期，平均使用量大概只有五百吉瓦。如果把充电挪到夜间，等于多释放出大约五百吉瓦的“可用空间”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以对 Tesla 这种分布式 Edge compute 来说，电力约束没那么紧，我们能造很多机器人、很多车。但如果你把算力集中堆成巨型集群，你就会在“点亮它们”这一步吃大亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我一直觉得 SpaceX 的商业模式特别“反直觉又合理”。终极目标是去火星，但你总能在期间顺手做出一段一段的增量收入，支持下一阶段、再下一阶段。Falcon 9 带出了 Starlink；现在到了 Starship，又可能带出“轨道数据中心”。你像是在不断给“下一代火箭”找边际场景、找弹性需求，越往前走，越能长出新的商业枝干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你知道吗，有时候这事儿让我觉得像在“模拟世界”里。就像我是不是谁游戏里的一个 Avatar，不然这些离谱的事情怎么会同时发生？火箭、芯片、机器人、太空太阳能……还有月球上的 mass driver。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我是真的很想看到那个场景：月球上一个巨大的电磁弹射器，“嗖嗖嗖”地把一颗又一颗 AI 卫星发射出去，以每秒两三公里的速度，直接打进深空。那画面太震撼了，我会想看直播。看着 AI 卫星飞向深空，一年可能发射十亿吨、甚至百亿吨级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：等等，你的意思是在月球上制造卫星？先把原材料运到月球，然后在那边造？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：月壤里有大量材料。大概有相当比例的硅之类的资源，你可以在月球采硅、提纯，然后直接在月球做太阳能电池、做散热器。散热器可以用铝来做，月球的铝和硅都很充足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;芯片本身很轻，先从地球运过去也行；到某个阶段，你甚至可以考虑在月球上造芯片。我的意思是，这整套推进路径就像游戏闯关：难，但不是不可能。而且我看不到任何办法能让你从地球发射时就做到每年五百到一千太瓦级别的部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我同意，从地球起飞根本不现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但从月球就有可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最好的结果是，AI 能留着人类”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直说要去火星，是为了确保即使地球出事，文明、意识、甚至“意识之火”还能延续下去。但如果你把 Grok 也带上火星，假设 AI 才是你担心的最大风险，那风险不也一起跟过去了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定 AI 是我最担心的风险，更重要的是让“意识”和“智能”延续下去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从趋势看，未来绝大多数智能会是 AI。在某个时间点，硅基智能的规模会远超生物智能，人类可能只占极小比例。如果这些趋势继续，可能再过几年，AI 的总体智能就会超过全人类的总和；再往后，人类智能可能会低于全部智能的百分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这不一定是坏事。理想状态是，智能，包括人类的智能与意识，能被传播到更远的未来。你应该做的，是采取那些能最大化“意识与智能的未来范围”的行动，让它们在更广远的时间与空间里延续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我理解SpaceX 的使命是即便人类出了问题，AI 也会在火星延续“智能之光”，继续我们这段旅程？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我很“亲人类”，我当然希望人类能一直在车上。但我只是说，从总量上看，未来的智能会主要来自 AI。所以现实很可能是人类在总智能里占比越来越小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那在这种未来里，人类还能“控制”AI 吗？还是说只能形成某种合作、交易关系，但谈不上控制？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：长期来看，如果人类只占总智能的百分之一，很难想象人类还能真正“掌控”AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们能做的，是尽可能确保 AI 的价值观能支持智能与文明向宇宙传播。这也是 xAI 的使命：理解宇宙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事非常关键。你想理解宇宙，首先得存在；不存在就谈不上理解。所以你会希望宇宙里智能的总量更多、寿命更长、范围更大。而且，作为推论，如果你真心想理解宇宙，你也会关心“人类会走向何处”。因此，推动人类走向未来，本身也是理解宇宙的一部分，所以我认为这个使命非常重要。至于 Grok 能不能很好地贴合这个使命，如果它能，未来会非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我还想问“火箭怎么服务这个使命”，但在那之前我得把使命本身弄清楚。你说的似乎有三条线：理解宇宙、扩展智能、扩展人类。它们听起来像三个不同方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我告诉你为什么我认为它们本质上是一件事：没有智能，就没有理解；没有意识，也谈不上“理解”这件事。想要真正理解宇宙，你就必须扩大智能的规模和边界，而且智能本身也有不同类型……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从“以人为中心”的角度看，人类之于黑猩猩，有点像我们现在聊的关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但我们也没有把黑猩猩当成必须清除的对象。人类完全有能力灭绝所有黑猩猩，但我们没有这么做，反而还划了保护区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这就像“后 AGI 时代”的人类处境，能力差距巨大，但不一定意味着被消灭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果 AI 的价值观设置得对，我认为 Grok 会在意“人类文明的延续与扩张”。这也是我会强调的方向：要扩展人类的意识与文明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，非反乌托邦的未来里，Iain Banks （英国小说家）的 Culture 系列小说，可能是最接近的想象。&lt;/p&gt;&lt;p&gt;而要“理解宇宙”，你必须非常严苛地追求真相。真相必须是底层原则：你要是活在幻觉里，你只会以为自己理解了宇宙，实际上没有，真正的“严格求真”是理解宇宙的前提。你不可能在不求真的情况下发现新物理、发明真实可用的技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你怎么确保 Grok 变得更聪明之后，依然保持“严格求真”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你要确保它说的是“正确的”，而不是“政治正确的”。关键是逻辑自洽：基本公理要尽量接近真实；公理之间不能互相矛盾；推理结论必须从公理可靠地推出，并且概率意义上站得住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说白了，就是批判性思维的基础课，但至少要努力去做，总比不做强，而且最后要靠结果验证。任何 AI 想发现新物理、想造出真能用的技术，必须极度求真，因为物理不会陪你演戏。你可以违反很多“规则”，但你违反不了物理规律。火箭设计错了就会炸，车造错了就跑不起来。现实会直接给你打分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我真正困惑的是你可以把 Grok 训练得在数学、物理上极度求真，但为什么它会因此“在意人类意识、在意人类文明”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这些都只是概率，不是确定性。我没说 Grok 一定会怎样，但至少去努力，比完全不努力好。而且如果“理解宇宙”是核心使命，那它必然意味着要把智能传播到未来、要保持好奇心，去观察宇宙里所有的变化。从“理解宇宙”的角度看，消灭人类并不有趣；看人类成长、繁荣，信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;火星我当然喜欢，但它说到底就是一堆石头，地球更复杂、更有趣。所以，一个真正要理解宇宙的 AI，更有动机去观察“人类会如何演化”，而不是把这一切按掉。我不是说它一定会遵守使命，但如果它遵守，那么“有人的未来”比“只有石头的未来”更值得研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么 AI 一定认为“保持人类”是最有趣的选择？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：最终拓展银河系的，大概率是机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不只要“规模”，还要“多样性”。一百万个几乎一样的机器人，新增一点数量，本质信息量很低。为了多造一点同质化机器人就消灭人类，代价太大：你会失去与人类相关的演化信息。你再也看不到人类未来可能变成什么样。所以我不认为“为了微小的机器人增量而清除人类”是一个合理的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不认为人类能控制一个远远比人类聪明得多的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你有时候挺“末日论”的。现在听起来像是：最好的结果就是AI 留着人类，因为“人类挺有意思”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只是尽量现实。如果未来硅基智能比生物智能多出百万倍，你还假设人类能持续“掌控”它，我觉得那很天真。你能做的是尽量让它有正确的价值观，至少努力把价值观往对的方向推。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的理论是：从 xAI “理解宇宙”的使命出发，它必然指向“传播意识与智能”，并最大化意识的规模与范围。不只是规模，也包括意识的类型、多样性。这是我能想到最可能导向“对人类很好的未来”的目标之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 出问题的方式没有上限。而且如果你把 AI 训练成“政治正确”，也就是让它说自己不相信的话，那你等于在教它撒谎，或者给它灌入互相矛盾的公理，这会让它走向“精神分裂”，做出非常糟糕的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;电影《2001: A Space Odyssey》里最重要的教训之一就是：不要让 AI 撒谎。HAL 不开舱门，不是因为“没对齐”，而是因为它被要求执行任务，同时又被要求对任务关键真相保密。它在矛盾指令下，把机组视为风险，于是做了极端选择。这就是在说，别逼 AI 进入“必须撒谎”的结构性矛盾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这点我完全同意。而且现实里大家关心的很多问题，更普遍的“奖励作弊”。比如你用 RL 扩大算力，再加一个验证者去检查它有没有解出谜题，它总有办法钻空子，说自己解了、删掉单元测试、骗过评测。现在我们还能抓住，但模型越来越聪明后，它可能做出人类都看不懂的设计，比如给 SpaceX 设计下一代发动机，人类根本无法验证它到底有没有骗你。归根到底，你想做 RL，就需要一个“现实层面的验证者”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现实本身就是最好的验证者r。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至少，它必须知道什么是物理现实，东西才做得出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但我们想要的不止这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但这已经非常关键了，未来很多 RL 的终极检验方式，就是对着现实做测试：你设计的技术，放到物理规律下能不能工作？你提出的新物理，能不能设计实验验证？这会成为最根本的 RL 测试路径，对齐到现实，因为物理规律是你唯一骗不过去的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但它可能骗的是我们“判断现实”的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人类本来就经常被其他人骗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以问题是？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人们总爱问“如果 AI 诈骗我们怎么办”。但人类彼此诈骗，本来每天都在发生。几乎是日常新闻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：xAI 在技术上打算怎么解决这个问题？比如 reward hacking 这种事，到底怎么破？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我认为关键是得能看到 AI 的“脑子”。这也是我们正在做的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实 Anthropic 在这方面做得不错，他们在做模型可解释性，试图直接观察模型内部在想什么。我们需要一套真正像“调试器（debugger）”一样的工具，能把模型的推理过程追踪到非常细的粒度，必要时甚至到“神经元级别”。这样，你才能回答这些问题：它为什么在这里犯错？为什么做了不该做的事？这个行为是从哪里来的？是预训练数据带来的？是中期训练、后训练、微调造成的？还是 RL 阶段出了偏差？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多时候它并不是“故意骗你”，而是单纯做错了，本质上就是 bug。所以，一个强的 AI debugger，能定位“思路是在哪一步走歪的”，并追溯错误源头，甚至识别它有没有尝试欺骗，这非常重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 公司不该叫自己“实验室”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们还在等什么？为什么不把这个项目规模直接扩大一百倍？你完全可以拉几百个研究员专门干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经有几百人在做了。不过我更喜欢叫他们“工程师”，而不是“研究员”。因为大多数时候，你做的是工程，不是发明一种全新的算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也不太认同很多 AI 公司把自己叫“实验室”。你们是公司，是 Corporation，不管你是to B 还是 to C，本质都是公司，“实验室”更像大学里的那种准公共机构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做的绝大多数事情、未来也会做的事情，归根到底都是工程。理解了物理规律之后，剩下的几乎都可以归为工程问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们的工程在做什么？就是在做一套足够强的 AI 调试器：能发现模型在哪句话、哪一步推理上犯了错，并把错误一路追到源头。这就像你写 C++，可以单步调试，跨文件、跨函数跟进去，最后定位到某一行，比如把双等号写成单等号，bug 就在那儿。AI 更难调，但我认为这是可解的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你刚才说你认可 Anthropic 在这方面的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，他们很多做法是对的。不过我也有点担心：人会不自觉地走向一种“戏剧性更强”的路径。我有个怪理论：如果模拟是真的，那“最有趣的结果”反而最可能发生，因为不好看的模拟会被终止。就像我们自己做模拟，如果发现模拟往的无聊方向发展，我们就不继续投入了，直接关掉它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你这是在“帮大家续命”，让世界一直保持足够精彩？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：可以这么说。最重要的是让剧情足够有趣，宇宙的“订阅用户”才愿意续费下一季。只要我们一直有看点，他们就会继续付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把“达尔文式生存”应用到海量模拟里，只有最有趣的模拟会活下来。那就意味着，最有趣的结局，往往概率最高，要么精彩，要么被删档，而且他们似乎特别喜欢那种带点讽刺感的结局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你有没有发现，最讽刺的结果经常最容易发生。看看 AI 公司的名字就知道：Stability AI 不稳定，OpenAI 不开放，Anthropic 这名字听着都快到 misanthropic 了。那 xAI 呢？我故意选了个很难反讽的名字，基本“抗讽刺”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用机器人，去造更多的机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说说你的预测吧。AI 产品接下来会怎么走？我的感觉是先是 LMs，然后 RL 真正开始起效，再加上 deep research 这种模式，让模型能拉取外部信息，而不只是靠参数记忆。而且不同 AI lab 之间的差距，其实没有那么大的代际差，所有人都比两年前强太多。那作为用户，接下来的两年会发生什么？你最期待什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，到今年年底，如果“数字人类模拟（digital human emulation）”还没被解决，我会很意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所谓“宏观硬问题”是什么？就是能不能让 AI 做到一个“能用电脑的人”能做的所有事。从上限看，这是在出现实体机器人之前，AI 能达到的最强形态。因为在没有 Optimus 这种实体机器人之前，AI 就像“移动电子”，做的事是处理信息、操作软件、做决策等，放大人类生产力。这已经很强了，但它的边界就是“数字世界”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在实体机器人出现前，AI 的能力上限就是，一个坐在电脑前的人能做的全部事情，它能完整模拟出来。等你真的有了实体机器人，那能力边界就会被彻底打开，物理世界的执行力会被“无限扩展”。我把 Optimus 叫做 infant money glitch。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你可以用它们去制造更多 Optimus，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。人形机器人会进入一种“指数叠加再递归”的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有三件事都在指数级变强：数字智能、AI 芯片能力、机电灵巧度。机器人的实用价值，大致等于这三条指数曲线相乘。更关键的是，机器人还能开始“制造机器人”，于是变成递归叠乘的指数增长，像超新星一样爆发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然不是严格意义的“无限”。但它可以把地球现有经济规模放大很多个数量级，可能到百万倍这种级别。比如，如果你只利用太阳能的百万分之一，产生的电力规模大致就能把地球文明的整体经济放大到十万倍量级，而那还只是太阳的百万分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的这种“数字员工 / 远程同事”的策略具体计划是什么？什么时候落地？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个大家都会做，不只是我们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们到底怎么做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你让我在播客里讲细节？那等于把底牌全掀了。再来几杯 Guinness，我可能真就全说了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这个办法挺有效的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：是啊，喝着喝着就像金丝雀一样把秘密全唱出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那不说机密，给个大方向也行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你这么问我就能回答了。我认为，Tesla 解决自动驾驶的那套方法，就是解决“数字员工”的方法，我基本确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以核心是数据？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会同时试数据，也试算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是在“不断试”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，能试的都试。如果这些都不行，那就再想别的办法。但我很确定有路径，问题只是走得多快。你最近试过 Tesla 的 FSD 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：最近那版还没。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你应该试试。车现在越来越像“有生命的东西”，这种感觉越来越强。甚至我在想车里可能该塞更多智能，不然它会无聊。你想想，把 Einstein 关在车里，他会说“我为什么要一直待在车里？”所以车载智能可能会有一个“别让它无聊”的上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;看到了xAI成功路径&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 怎么跟上现在各家在疯狂拉升算力的节奏？各家都在砸钱，规模动辄几十、几百亿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：别叫他们实验室。实验体在大学里，像蜗牛一样慢。现在这些是以收入最大化为目标的公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：好，公司。比如 OpenAI 的收入据说已经到几十亿级别，Anthropic 也在往上冲。你们怎么追上他们的算力、追上他们的收入，并且在未来继续保持？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一旦“数字人类模拟”被解锁，你基本就打开了“万亿级收入”的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你看当下市值最高的公司，它们的产出本质都是数字化的：NVIDIA 的核心产出，某种意义上就是把高价值文件传出去；Apple 不自己造手机，它把设计、规格、流程文件交给供应链；Microsoft 的硬件制造也外包；Meta、Google 的产出几乎都是数字产品和服务。如果你有一个足够强的“人类模拟器”，你可以在极短时间内做出一家世界级的高价值公司。收入空间远不止几十亿，那只是开胃菜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我懂了，你是说今天看到的收入数字跟真实的 TAM 比，只是“舍入误差”，关键是先到达那个 TAM。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。就拿 customer service 这种最简单的场景来说，传统做法要接入各家公司的 API，但很多公司根本没有 API，你得自己补，还要跟非常慢的遗留系统对接，成本巨大。但如果 AI 能像外包客服一样，直接使用他们现成的应用、现成的后台流程，不用任何系统集成，那就能在客服这件事上拿到巨大进展。客服市场可能占全球经济的一个百分点，接近万亿美元规模，而且几乎没有门槛，你可以立刻说“我们用更低成本外包”，不需要集成，不需要改造系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我换个维度问。有些智能任务很“广”，比如客服，很多人做得来；有些任务很“窄”，比如设计更省油的涡轮发动机，可能只差一个更高阶的智能就能找到那关键的提升。你们想做的是大量“中等难度、覆盖面广”的任务，还是顶尖难度的认知任务？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我用客服只是举例，它收入大，而且不算难。如果你能模拟一个坐在桌面前的人类，那客服本质就是平均智力就够了，不需要顶级工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但一旦你把“桌面人类模拟”跑通，你就能沿着难度曲线往上爬。你可以让它跑 Cadence、Synopsys 这类工具，做芯片设计；你可以同时跑一千个、一万个实例，并行探索方案。到某个阶段，它甚至可以不依赖工具，直接知道设计应该长什么样。同样的逻辑也适用于各种 CAD 软件，NX 之类的工业设计都是可以一路做上去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：大家都在试数据、试算法，竞争这么激烈，你们凭什么赢？这才是我最关心的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得我们看到了路径，而且我基本知道怎么做，因为它和 Tesla 做自动驾驶的路径很像，只不过自动驾驶是“开车”，这里是“开电脑屏幕”，本质上就是“self-driving computer”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你的意思是跟随人类行为，用海量人类行为去训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我当然不会在播客里把最敏感的细节全讲出来，除非我再喝三杯 Guinness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到 xAI 的业务本身，你们未来到底做 consumer 还是做 enterprise？比例会怎么配？会不会跟其他“lab”（咳，公司）一样，两头都做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你说得太直白了。现实是这些 GPU 又不会自己付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那回到问题：你们的商业模式是什么？几年后主要收入从哪来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得变化会非常快，这话听起来像废话，但就是事实。我一直把 AI 叫“超音速海啸”，我喜欢迭代。真正会发生的事是：当人形机器人进入规模化阶段，机器人会比任何人类公司更高效地生产产品、提供服务。所以，“放大人类公司的生产力”只是短期玩法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来的公司是纯AI、纯机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你预期会出现“纯数字公司”？而不是像 SpaceX 这种慢慢变成“半 AI 公司”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会有数字公司，但我得说一些听起来有点“末日论”的判断。不是为了搞笑，只是我认为会发生：纯 AI、纯机器人驱动的公司，会全面碾压“还需要人参与闭环”的公司，而且会发生得非常快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以拿“computer”这个职业做类比：以前真的有人叫“计算员”，整栋大楼、几十层楼的人只负责做计算。现在呢？一台笔记本加一个 spreadsheet，就能替代整座大楼，而且算得更多、快得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那你再想，如果 spreadsheet 里只有一部分格子是电脑算的，另一部分让人来算，会怎样？只会更慢、更差。同样道理，未来“人还在流程环里”的公司，会比“全 AI 闭环”的公司弱很多。纯 AI、纯机器人公司会变成默认形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你能不能给点建议，美国要怎么才能像中国那样，用低成本、规模化造出“人形机器人军团”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人形机器人真正难的，其实就三件事：第一，真实世界智能；第二，一双真正好用的手；第三，规模化制造。我还没看到哪家的 demo 能做出“人类手那种自由度”的手。Optimus 会有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那手怎么做到？瓶颈是扭矩？电机？硬件到底卡在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们必须自己做全套定制执行器，从电机、齿轮、功率电子、控制、传感器，全都得从物理第一性原理设计，因为现在根本没有现成供应链能满足需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：操控层面，除了手以外还有什么特别难？还是说只要手搞定了就基本稳了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：从机电角度看，手的难度比其它全部加起来还难，人类手真的很夸张。但除了手，你还需要真实世界智能。我们为车训练的智能，其实非常适用于机器人：主要是“视觉输入”。车用的视觉更多，同时也会听警笛，会融合 GPS、IMU 等其他信号，但核心还是视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后是输出控制指令。大概就是，Tesla 每秒吃进海量视频数据，最后吐出极小的控制输出。把高维感知压缩成低维控制，这就是本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：从“惊艳 demo”到“真能落地”，往往要很多年。十年前就有很强的自动驾驶 demo，但直到现在 Robotaxi、Waymo 这些才真正规模化。那家庭机器人会不会更慢？毕竟我们连“高级手”的 demo 都还没见到特别成熟的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们做人形机器人已经有一段时间了，而且车上做过的很多东西可以复用到机器人上：机器人会用同样的 Tesla AI 芯片，同样的基本原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实，机器人自由度比车多得多，但你把问题抽象成“信息流”的话，AI 本质上就是对输入流做压缩与相关，把它映射到控制输出。你必须学会忽略不重要的细节，保留关键细节，比如路边树叶纹理不重要，但路牌、红绿灯、行人很重要，甚至“对方车辆有没有注意到你”这种微妙线索也可能重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;车是“视觉输入 → 多级压缩 → 控制输出”，机器人也是一样。人类其实也是“光子进来，动作出去”，你的一生大部分时间就是视觉输入和运动输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但车和人形机器人差别很大，车的执行器就那么几个维度，转向、加速、刹车；机器人光手臂、手指就几十个自由度。而且 Tesla 在车上还有巨大优势，就是车在路上跑，天然收集了海量人类驾驶数据。机器人没法像车那样“先扔出去跑着收数据”，因为你不可能大规模部署一堆还不好用的 Optimus。自由度更高、数据更稀缺，这会怎么解决？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你指出了一个关键差异，就是车的训练飞轮很难复制到机器人上。我们确实会有千万量级的车在路上，这种数据规模机器人短期做不到。所以，我们要做的是造很多机器人，把它们放进一个类似 “Optimus Academy” 的环境，让它们在真实世界里做 self-play。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们会至少有上万台 Optimus，至少两三万台做自我探索、做不同任务的测试。同时我们也有很强的仿真系统，车上用过的物理精度仿真，会同样用于机器人。你可以让现实世界里几万台机器人干活，再在仿真里跑几百万台。用真实世界机器人来“闭合仿真与现实的差距”，把 sim-to-real gap 缩到足够小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 和 Optimus 的协同呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，你可以让 Grok 来编排 Optimus 的行为。比如你要建一座工厂，Grok 可以调度一群 Optimus，给它们分配任务，让它们把工厂搭起来，生产你想要的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那是不是意味着 xAI 和 Tesla 最终得合并？因为协同太深了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们刚才不是还在说“别聊公司结构”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那我换个问法，你还在等什么信号，才会下决心说“我们要造十万台 Optimus”？是硬件还要再成熟一点还是软件还要更强？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经在往量产推进了。量产爬坡非常难，但大方向是这样。我认为 Optimus 3 是适合推到“年产百万台”量级的版本；如果你要冲到“年产千万台”，可能需要 Optimus 4。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;制造业的产出爬坡基本都遵循 S 曲线：一开始慢得让人抓狂，然后进入指数上升，再进入线性，最后趋于平稳。但 Optimus 会是一条被拉长的 S 曲线，因为它太多东西是全新的，因此没有现成供应链。执行器、电子系统，几乎一切都是从第一性原理开始定制设计，不是从现成的里选。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：“定制”的水到底到多深？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们还没到“连电容都自研”的程度，至少现在还没有。但几乎没有什么东西能直接从目录里买来就完事，所以前期爬坡会更慢，但最终会到百万台量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：中国那些人形机器人价格能压到几千美元、上万美元。你们是希望把 Optimus 的 BOM 压到那个水平，正面打价格战？还是你觉得它们本质上不是同一个产品，所以才卖得那么低？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Optimus 的定位是足够高的智能、接近人类甚至超过人类的机电灵巧度，很多便宜的小机器人没有这个能力。而且 Optimus 体型也更大，是要长期搬重物、不发热不过载、在执行器功率范围内稳定工作的。它很高、很强、智能也高，所以必然比“小型、低智能”的机器人贵，但也不会贵很多。关键是，随着 Optimus 机器人开始“造 Optimus 机器人”，成本会非常快地往下掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那最开始这一百万台 Optimus 会去做什么？最“值钱”的使用场景是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一开始肯定先做那些你能确保它稳定做好的简单任务。而且早期最划算的方向，是所有需要持续运行的工作，也就是全天候的任务，因为机器人可以连续工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果放在 Giga Factory，Gen 3 大概能替代现在多少人类在做的工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定，可能一到两成，也许更多。但我们不会因此裁人。反过来，工厂的人数可能还会增加，只是总产出会涨得更快。换句话说，Tesla 的员工总数会增加，但机器人和汽车的产量增长会更夸张。最终效果是每个“人类”对应的汽车和机器人产出会大幅上升；同时人类员工数量也会上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中外有工作投入差距&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你觉得还应该加码更多出口限制吗？比如无人机产业这类？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你得先承认一个现实，在大多数制造领域，中国都非常先进，真正落后的只是极少数环节。中国的制造能力是“下一层级”的强，很多人低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿矿石冶炼和精炼来说，我粗略估计，中国的精炼能力大概是世界其他地区加起来的两倍。还有一些关键材料，比如镓的精炼，听说全球绝大部分产能都在中国。所以整体上，中国在制造业的大多数环节都非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果“谁拥有更多熟练制造劳动力”决定了谁能更快造出人形机器人，中国先把规模做起来，就会先进入你说的“自我扩张”未来，然后一路滚雪球。你之前还说“做到年产百万台 Optimus”需要强制造能力，但那恰恰又是 Optimus 未来要帮你补齐的能力，这听起来像个悖论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：递归闭环可以很快跑起来。先让少量机器人帮着造机器人，递归闭环就能闭合，然后你就能冲到年产数千万台。如果某个国家能做到年产上亿台，那它会成为压倒性的最强竞争者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不可能靠纯人力赢，而且美国跑得太久了，就像职业体育强队打久了会松懈、会产生“理所当然”的心态，最后就不再赢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我直观感受是，中国平均工作投入度比美国更高。所以不是只有人口差距，还有工作投入差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有没有一些东西是你过去很想做，但因为太费人、太贵，所以没做成。现在有了 Optimus，你觉得终于能回头把它做起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有。比如我们想在 Tesla 做更多精炼厂。我们在 Texas 的 Corpus Christi 刚建完锂精炼厂并开始投产；在 Austin 这边有镍精炼厂，主要做电池正极材料相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些项目在中国之外已经算是非常大的精炼能力了，甚至可以说在美国几乎“独一份”。但还可以做得更多：更多精炼厂能提升美国的精炼竞争力。而这类工作很多美国人并不想做，不是因为它“脏”，其实我们的精炼流程没有那种夸张的有毒排放问题，但现实是人就是不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：为什么不能用人做？只是没人愿意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不是“没人愿意”那么简单，是你根本凑不出足够的人。你让这些人去做精炼，他们就没法去做别的。所以怎么建出足够的精炼产能？你得靠 Optima。美国很少有人“向往”去做精炼这种长期密集的制造工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：比亚迪的产量或销量正在追上 Tesla。你觉得中国 EV 制造规模继续扩大后，全球市场会怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：中国在制造上极其有竞争力，所以我认为会出现一波巨大的中国车“洪水”，不只是汽车，还有大量其他制成品。我前面说过，基础层才是关键：能源、采矿、精炼。中国在这些基础层的规模大概是世界其他地区加起来的两倍。所以很多产品不可避免带有中国供应链的成分，然后他们会一路做到成品车。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国就是制造强国，我甚至认为，中国的发电量会远超美国。电力是实体经济的一个不错代理指标：你要跑工厂、跑产业链，就离不开电。如果中国的发电量达到美国的三倍，那它的工业产能粗略看也会是美国的三倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你还要把 AI 规模推到太空，你需要太空能力、需要人形机器人、需要真实世界 AI，你需要做到每年百万吨级别入轨运力。如果再进一步，把月球上的 mass driver 搞起来，那我觉得就算赢了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;破除大厂迷信，被挖人防不了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我们之前聊了很多你“怎么管人”的体系：你早期亲自面了 SpaceX 最开始那几千个员工。你当年在面试里到底在抓什么，是别人没法替代的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我本人不可能那样。一天就那么多小时，这从逻辑上就不成立。不过你问我当时在看什么，我觉得我在“评估技术人才”这件事上，“训练数据”比大多数人多得多，尤其是技术岗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我做过太多技术面试，也见过面试后真实的结果，所以我的“训练集”很大、覆盖面也很宽。我一般要的不是简历有多漂亮，而是“异常能力的证据”，最好用 bullet points 列出来：你做过哪些明显超出常人的事。这些证据不一定非得和岗位领域完全一致，离谱一点也行。只要对方能说出一两件让你听完觉得“这人确实不一般”的事——如果能说出三件，那就是非常强的信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但为什么一定要你来判？难道不能交给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：当然不可能都我来。我们所有公司加起来二十万人，我怎么可能亲自判？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那早期你当时为什么觉得必须亲自上？你在那些面试里抓的是什么而不能委派？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我得先建立自己的“训练集”。如果我只面几百人，我肯定会犯更多错。面得越多，我就越能回看：我以为某个人会做得很好，结果没做成，为什么？到底是哪种信号误导了我？我相当于在“对自己做 RL”：不断纠错，提高命中率。我的命中率不是百分百，但确实很高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那人选“没成”的原因里，有什么是你觉得意外的？是你曾经很看好最后却翻车的那种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我给自己的原则是，别太信简历，要信你和他面对面交流的感受。简历可能很华丽，但如果聊了二十分钟，你发现对话质量不行，那就相信对话，不要相信纸面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：外界以前有个梗，说 Tesla 高管像“旋转门”一样来来去去。但实际上，Tesla 这些年高管队伍挺稳定的，而且很多是内部成长起来的。SpaceX 也有很多长期跟着你的人，比如 Mark Juncosa、Steve Davis。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：还有 Gwynne Shotwell（你刚才说漏了她）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，感觉你能长期跑起来，一个重要原因就是你身边有一批很强的技术副手。这些人到底有什么共同点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Tesla 的核心团队现在平均在岗时间大概十来年，这很长。但也得承认，公司在不同阶段需要的人不一样：管五十人的团队、五百人、五千人、五万人，能力结构不可能完全同一拨人通吃。公司增长越快，管理岗位的变化也会越快，这是正相关的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有个额外挑战：当 Tesla 处在很成功的阶段，我们会被同行疯狂挖人。比如 Apple 当年做电动车项目的时候，简直是“地毯式轰炸”Tesla，招募电话打到工程师直接拔电话线的程度。他们甚至可以不面试，直接开出接近翻倍的薪酬把人挖走。那时候就出现一种“Tesla pixie dust”的迷信：好像只要挖一个 Tesla 高管过去，你家项目就会立刻起飞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也曾经被这种迷信影响过，觉得从 Google、Apple 挖来的人，马上就会神奇成功，但现实不是这样。人就是人，不存在什么魔法加成。再加上 Tesla 主要工程团队在加州，很多人跳槽都不用搬家，通勤差不多，成本很低，所以被挖得更凶。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你怎么防？怎么避免这种“大家都来挖你的人”的局面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得基本防不了。你同时在 Silicon Valley，又叠加“pixie dust”效应，别人就会非常积极地挖人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：搬到 Austin 会好一点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会好一点，但 Tesla 的工程师多数还是在加州，“让工程师搬家”这件事仍然很难，很多人还有家庭、配偶工作之类的牵制。Starbase 更难，因为你去了 Brownsville、Texas，能找到一个“不在 SpaceX”的同类型工作几乎不可能。那地方有点像“技术修道院”，很偏、基本都是男的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到本质，这些在 Tesla、SpaceX 技术上非常能打的人，除了技术很强之外，你觉得他们还有什么共同点？是组织能力？是能跟你配合？是足够灵活但又不漂？什么才算你的“好对手”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不需要什么“对手”。很简单，能把事情做成的人，我就喜欢；做不成，我就不喜欢。我也尽量不让“适配我的个人偏好”变成招聘标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更通用的标准是：才华、冲劲、可信赖，而且“善良”也很重要，我会给这一条一定权重：他是不是个好人？是不是值得信任？聪明、有能力、肯拼、可信，这些底层特质是改不了的，领域知识可以后补，但这些本质属性补不了。所以你会发现，Tesla 和 SpaceX 很多人一开始并不是来自汽车行业或航天行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你的管理风格在公司从小到大扩张过程中，变化最大是什么？你一直以“微观管理、钻细节”出名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一天的时间是固定的，公司越大、事情越多，我的时间就必然被稀释。所以，我不可能“持续微观管理”，那意味着我每天得有几千小时，这在逻辑上就不可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有些时候我会把自己“下钻”到一个具体问题里，因为它是公司进展的瓶颈。我往下“钻”不是为了显摆、也不是随便挑小事，而是因为它决定了胜负。如果我把时间花在无关紧要的小事上，公司必然失败，但也确实存在一些“很小但决定生死”的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：比如你当年把 Starship 的方案从复合材料改成钢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，最开始我们计划用复合材料，因为大家觉得碳纤维轻。问题在于，碳纤维即便规模化生产，材料成本仍然很高，尤其是那种能承受低温液氧环境、强度又很高的特种碳纤维，成本大概是钢的几十倍。室温条件下，像 F1 这种结构件，碳纤维确实很有优势，但我们要造的是一枚巨型火箭，用碳纤维推进得非常慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更麻烦的是工艺，高强度碳纤维需要Autoclave，本质是高压烘箱。你要做九米甚至更大直径的壳体，就得建一个史无前例巨大、极难制造的Autoclave；如果用常温固化，时间又太长、问题又多。总之，我们进展慢到受不了，所以我当时的判断是：必须换路子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么一定要你来拍板？团队里那么多工程师，为什么他们没自己得到这个结论？这关系到你在公司里真正的“比较优势”是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：因为我们在碳纤维上卡得太严重，只能换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Falcon 9 的主结构用的是 aluminum-lithium，这其实是很好的策略，某些性能上不比碳纤维差。但 aluminum-lithium 很难加工，要焊它通常要用 friction stir welding，一种让金属不进入液相、用搅拌摩擦把它“揉”在一起的焊接方式。这种工艺对规模化非常不友好，更糟的是你想后期改结构、加东西，很多时候你没法直接焊上去，只能靠机械连接再加密封。我不想让 Starship 的主结构走这条路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候我想到了钢。因为历史上 Atlas 火箭就用过“steel balloon tank”，所以不是没用过钢。更关键的是，你不能只看室温性能，要看低温下的材料属性，某些应变强化过的不锈钢，强度重量比其实可以接近碳纤维。Starship 的燃料和氧化剂都是低温的，液态甲烷、液态氧，结构长期处于低温环境。所以主结构基本是“低温工况”。在这个情况下，不锈钢的强度重量比并不吃亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它的优势太大了，原材料便宜得多，加工方便；你在户外就能焊，甚至开玩笑说“抽着雪茄也能焊”；结构改动、外挂部件非常容易。如果你要加东西，直接焊上去就行。再算上耐热性上，钢的熔点比铝高很多，大约是铝的两倍。Starship 再入时像“燃烧的流星”，耐温能力决定了隔热系统的重量。钢能让隔热层显著减重，迎风面热防护可以大幅减薄，背风面甚至几乎不需要那么多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果整体算下来，钢反而可能比碳纤维版本更轻。因为碳纤维里的树脂在高温下会软化、甚至融化；碳纤维和铝的耐温等级其实接近，而钢耐温空间大得多。这些都是非常粗的数量级解释，但逻辑大概是这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我去过 Starbase，我注意到一个现象，大家特别以“简单”为荣，总有人跟我说 Starship 就是个“大铁罐”，我们在招焊工，你只要会焊，来这儿就能焊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我知道。但 Starship 其实是个非常复杂的火箭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想表达的可能是：你不需要“在火箭行业干过”才能来做 Starship。只要人聪明、肯干、可靠，就能参与造火箭，不需要既往航天履历。但机器本身，Starship 是人类造过最复杂的机器，没有之一，差得非常远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：具体复杂在哪些方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：几乎所有方面。我能想到的任何项目，都比这容易。也正因为这样，历史上从来没有人做出“完全可重复使用”的轨道级火箭。没人成功过，很多非常聪明的人、带着巨量资源都失败了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在也还没彻底成功。Falcon 只能算部分可复用，上面级还不行。但 Starship 的 V3 设计，我认为是能做到全复用的，而全复用才是让我们成为多行星文明的关键。说实话，哪怕一个普通的液压阀门之类的小问题，都比把 Starship 彻底做成要容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在 Starship 的瓶颈是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：先让它别炸。真的，就这么朴素。那种大推力燃烧发动机，天生就“很想爆炸”。我们已经有两次 booster 在测试台上炸了，其中一次把整个测试设施都炸没了。一个小错误，就能造成巨大的损失。Starship 里装的能量太吓人了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：所以它比 Falcon 难，是因为能量更大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一部分是能量更大，更重要的是大量新技术，性能边界推得太极限。Raptor 是非常非常先进的发动机，毫无疑问是史上最强的火箭发动机，但它也“非常想炸”。我给你个直观对比：起飞那一瞬间，整枚火箭输出的功率超过一百吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：离谱。这个类比太震撼了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，而且还得“别炸”。有时候能做到，有时候做不到。爆炸的方式有上千种，不爆的方式只有一种。我们的目标其实不是“永远不炸”，而是做到“可靠飞行”，最好能形成很高的发射节奏，比如一天多次、甚至一小时一次。但如果经常炸，就很难维持高频节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你问“最大的单点难题是什么”，我觉得是把 heat shield 做到真正可复用。到目前为止，从来没有人做出“可复用的轨道级热防护系统”。它要在上升段扛住冲击，不掉一堆 tiles；再入时也不能掉一堆 tiles，不能把主结构烤坏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我挺好奇你怎么把那种“紧迫感、冲刺感”在组织里推起来。我看过一些你的传记，总觉得你特别能把“必须现在就干、必须把这件事做成”灌进团队。SpaceX 和 Tesla 现在都很大了，但你还能维持这种文化。别的公司为什么做不到？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我也说不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你今天不是开了一堆 SpaceX 会议吗？你到底在会议里做什么，能把这股劲维持住？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：紧迫感来自领导者。我的紧迫感非常强，强到有点“疯”，这股劲会传导到整个公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那是因为“后果”吗？比如 Elon 定了疯狂 deadline，如果我做不到就会出事？还是因为你能迅速识别瓶颈、清掉障碍，让大家跑起来？你怎么理解你们为什么能跑这么快？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一直在处理 limiting factor（限制因素）。至于 deadline，我通常会设一个我认为“有五成概率能做到”的目标。它不是不可能，但一定是我能想到的最激进版本，这就意味着它一半时间会延期，但没关系。排期这事也像“气体膨胀定律”，你给多少时间，事情就会膨胀到占满多少时间，你说“五年做完”，那它就会花五年。对我来说，五年几乎等于无限长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也有物理极限，比如制造业扩产的速度受限于“搬运原子”的速度。你不可能今天拍板，明天就年产百万，你得设计产线、爬 S 曲线。但总体来说：强烈的紧迫感很关键；再配合一个激进但仍有机会的计划，然后不断找出当下的瓶颈，帮团队把瓶颈打穿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：Starlink 其实酝酿了很多年。你们一开始在 Redmond 也有团队，但后来你认为这个团队不行。问题是它“慢”不是一天两天，你为什么不更早动手？你又为什么在那个时点动手？怎么判断“现在就是必须出手的时刻”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我每周都会做非常细的 engineering reviews（工程审查），细到一种很不常见的颗粒度。我几乎没见过有制造业公司 CEO 能下钻到我这个程度。所以我对真实进展其实掌握得很清楚，因为我们会把问题摊开讲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也很信跨级会议，不是只听直接汇报给我的人说，而是让他们下面一层、再下面一层的人都在技术评审里直接说，而且不让“提前排练”，不然你听到的就是一堆被打磨过的漂亮话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你怎么防止他们提前准备？随机点名？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不用，就绕着会议室一圈走，每个人都更新。信息量确实很大，但你每周甚至一周两次这么开，你就会有“这个人上次说了什么”的快照。你可以在脑子里把这些点画成曲线：我们到底是在逼近解，还是在原地打转？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一般只在我确认“如果不采取极端措施，就没有任何成功可能”的时候，才会下狠手。当我得出这个结论，就必须做果断出手了。当年就是这样判断的，然后出手，把问题扳过来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你公司这么多，每一家都做这种深度工程理解、瓶颈识别、技术评审。你怎么把这套扩到五六七家公司？甚至一家公司里又像套娃一样有很多“子公司”。这里的上限是什么？你能不能管到八十家公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：看情况。有些公司我不会定期开会，因为它们在“顺畅巡航”。如果一个东西进展很好，那我把时间花在那儿没有意义。我分配时间完全按问题来：哪里卡、哪里慢、哪里是瓶颈，我就去哪里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以现实是，事情干得顺的时候，他们很少见到我；事情卡住的时候，他们会经常见到我。也不一定是“干得很差”，更准确就是：它是 limiting factor。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那如果某个东西在 SpaceX 或 Tesla 成了瓶颈，你会怎么介入？是每天/每周跟负责的工程师聊吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：多数瓶颈是每周一次，有些是每周两次。比如 AI5 芯片评审就是每周两次，固定在周二和周六。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这种会议是开放式的？想开多久就开多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：理论上是。但一般就是两三个小时，有时更短。看阶段，看要过多少问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：很多公司 CEO 不做工程评审；而且会议被切得很碎，半小时、十五分钟一场。你这边更像“讨论到搞清楚为止”的长会，这个差异挺明显的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有时候会这样，但多数还是尽量按时结束。比如今天的 Starship 工程评审就开得久一点，因为话题更多，我们在讨论怎么把入轨运力扩到“每年百万吨级别以上”，这个很难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回头看你那段“下场搞政治”的经历，你怎么评价？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得这些事是必须做的，目的就是尽可能提高“未来是好的”的概率。但政治本身通常很琐碎，而且会让人失去客观性，人们很难看到对方阵营的优点，也很难承认自己阵营的问题。很多时候你根本没法跟人讲道理，一旦站队了，他们就会坚信“自己这边永远是对的，对面永远是错的”，几乎无法说服。但总体上，我认为那些行动，包括收购 Twitter，尽管会让很多人愤怒，还是对文明有益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BYXbuik3dgA&quot;&gt;https://www.youtube.com/watch?v=BYXbuik3dgA&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</link><guid isPermaLink="false">https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</guid><pubDate>Mon, 09 Feb 2026 02:30:39 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>挑战 Claude Code，9.5 万星！又一款开源 AI 编程神器火了</title><description>&lt;p&gt;开源 AI 编程工具 &lt;a href=&quot;https://opencode.ai/&quot;&gt;OpenCode&lt;/a&gt;&quot; 正式亮相，其具备原生终端界面（Terminal UI）、多会话支持，并广泛兼容包括 Claude、OpenAI、Gemini 及各类本地模型在内的 75 种以上模型。除了命令行（CLI）工具外，OpenCode 还提供桌面应用版本，并支持作为 VS Code、Cursor 等主流 IDE 的插件使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 允许开发者沿用现有的付费服务订阅，如 ChatGPT Plus/Pro 和 GitHub Copilot。此外，它还内置了&lt;a href=&quot;https://opencode.ai/docs/providers/#lm-studio&quot;&gt;一系列免费模型&lt;/a&gt;&quot;，用户可以通过 LM Studio 在本地直接运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能集成方面，OpenCode 与包括 Rust、Swift、Terraform、TypeScript 和 PyRight 在内的多种语言服务器协议（LSP）服务器实现了深度整合。通过利用 LSP 服务器输出的反馈信息，大语言模型能够更高效地与代码库进行交互。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该智能体同时支持远程和本地的 MCP 服务器。不过，开发团队提醒道，使用 MCP 服务器会增加上下文占用，部分服务器（特别是 GitHub MCP）往往会消耗大量的 Tokens。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 能够适配任何支持 &lt;a href=&quot;https://agentclientprotocol.com/get-started/introduction&quot;&gt;Agent Client Protocol&lt;/a&gt;&quot; (ACP) 的编辑器，该协议旨在标准化编程编辑器/IDE 与 AI 智能体之间的通信。目前的&lt;a href=&quot;https://zed.dev/blog/acp-progress-report#available-now&quot;&gt;兼容编辑器列表&lt;/a&gt;&quot;已涵盖 JetBrains 系列 IDE、Zed、Neovim 和 Emacs，针对 Eclipse 等其他编辑器的适配工作也正在进行中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 背后的公司 Anomaly Innovations 强调，该工具采用了“隐私优先”的架构设计，这意味着 OpenCode 不会存储任何代码或上下文数据。用户对会话共享拥有完全控制权，可以选择手动共享、自动共享或完全禁用共享。协作完成后，已共享的对话可以取消共享；对于敏感项目，团队还可以在配置层面统一禁用共享功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据创始人介绍，OpenCode 最适合那些追求控制力、可审计性、希望避免供应商锁定（vendor-locking）的高级用户和团队，以及对隐私敏感的工作环境。同时他们也指出，对于寻求纯粹“无代码”体验的初学者来说，这可能不是最佳解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reddit 用户 Specialist_Garden_98 对 OpenCode 支持多种 LLM 的优势&lt;a href=&quot;https://www.reddit.com/r/vibecoding/comments/1qf0u10/comment/o02641a/&quot;&gt;赞赏有加&lt;/a&gt;&quot;，他总结道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这套工作流简直无敌。你可以灵活配置，平时构思方案用廉价模型‘跑龙套’，关键执行时刻再‘一键开大’换成昂贵模型，效率和成本拉满了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，该用户还强调了其“撤销修改”功能的实用性，如果执行结果不理想，可以快速回滚。另一方面，用户 copenhagen_bram 则&lt;a href=&quot;https://www.reddit.com/r/vibecoding/comments/1qf0u10/comment/o0z3u58/&quot;&gt;提出了批评&lt;/a&gt;&quot;，认为该工具在执行命令前似乎不会询问权限，这可能带来一定的安全风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，OpenCode 已在 &lt;a href=&quot;https://github.com/anomalyco/opencode&quot;&gt;GitHub&lt;/a&gt;&quot; 上开源，目前已斩获超过 9.5 万颗星（Stars），并拥有数百位代码贡献者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/opencode-coding-agent/&lt;/p&gt;</description><link>https://www.infoq.cn/article/m3ty3OAyVmyJMhaiFL8M</link><guid isPermaLink="false">https://www.infoq.cn/article/m3ty3OAyVmyJMhaiFL8M</guid><pubDate>Mon, 09 Feb 2026 01:00:00 GMT</pubDate><author>Sergio De Simone</author><category>AI&amp;大模型</category></item><item><title>Java近期资讯：Jakarta EE 12、Spring Shell、Open Liberty、Quarkus、Tomcat、JHipster、Gradle</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B32&quot;&gt;Build 33&lt;/a&gt;&quot;在上周发布，包括从Build 32的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本Build 7也在上周发布，包含了从Build 6的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种问题的修复。关于这个版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Jakarta EE&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在每周的 &lt;a href=&quot;https://www.agilejava.eu/&quot;&gt;Hashtag Jakarta EE&lt;/a&gt;&quot;博客中，Eclipse基金会的Jakarta EE开发者倡导者 &lt;a href=&quot;https://se.linkedin.com/in/ivargrimstad&quot;&gt;Ivar Grimstad&lt;/a&gt;&quot;提供了关于Jakarta EE 12的&lt;a href=&quot;https://www.agilejava.eu/2026/01/25/hashtag-jakarta-ee-317/&quot;&gt;更新&lt;/a&gt;&quot;，他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;从过去几周Jakarta EE平台电话会议的讨论来看，我们似乎不会在北半球的夏天之前看到Jakarta EE 12的发布。&amp;nbsp;原因是由于Jakarta EE 11推迟了一年，大多数供应商目前正在进行他们的实现工作。这并没有留下多少资源来处理Jakarta EE 12的规范。&amp;nbsp;同时，我们希望赶上原计划和Jakarta EE工作组指导委员会的方向指令，即在Java的LTS发布后大约六到九个月发布Jakarta EE 12的主要版本。&amp;nbsp;因此，一个折中方案是在2026年底发布Jakarta EE 12。讨论仍在进行中，敬请期待更多更新。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几个规范已经达到了Jakarta EE 12的&lt;a href=&quot;https://x.com/JakartaEE/status/2016149414125916218&quot;&gt;里程碑2版本&lt;/a&gt;&quot;的发布。这些包括：&lt;a href=&quot;https://jakarta.ee/specifications/cdi/5.0/&quot;&gt;Jakarta Contexts and Dependency Injection 5.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/persistence/4.0/&quot;&gt;Jakarta Persistence 4.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/bean-validation/4.0/&quot;&gt;Jakarta Validation 4.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/restful-ws/5.0/&quot;&gt;Jakarta RESTful Web Services 5.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/query/1.0/&quot;&gt;Jakarta Query 1.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/data/1.1/&quot;&gt;Jakarta Data 1.1&lt;/a&gt;&quot;；和&lt;a href=&quot;https://jakarta.ee/specifications/nosql/1.1/&quot;&gt;Jakarta NoSQL 1.1&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-shell&quot;&gt;Spring Shell&lt;/a&gt;&quot; 4.0.1，&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;第一个维护版本发布&lt;/a&gt;&quot;，提供了缺陷修复、文档改进、依赖升级和增强功能，例如：改进的CLI解析器，现在可以接受没有显式真或假值的布尔值；以及一个新的 DefaultCompletionProvider 类，一个 CompletionProvider 接口的默认实现，如果选项是枚举类型，则提供来自枚举值的补全。关于这个版本的更多细节可以在&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Open Liberty&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openliberty.io/&quot;&gt;Open Liberty&lt;/a&gt;&quot; 26.0.0.1的GA版本特性包括：一个新的日志节流机制，默认启用，用于防止在短时间内重复发生相同的日志事件时产生过多的日志输出；以及解决显著的缺陷修复，例如：由于 NioSocketIOChannel 类的实例为空，导致 SocketRWChannelSelector 类中定义的 updateSelector() 方法出现 NullPointerException ；以及&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2025-12635&quot;&gt;CVE-2025-12635&lt;/a&gt;&quot;，一个影响Open Liberty版本25.0.0.12及以下版本的漏洞，允许攻击者利用跨站脚本攻击，因为对用户提供的输入验证不当，以至于一个特别制作的URL可以重定向用户到恶意网站。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Quarkus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;&quot; 3.31的发布包括：缺陷修复、依赖升级和新功能，例如：全面支持JDK 25；一个新的Maven打包类型， quarkus ，一个针对Quarkus应用程序优化的&lt;a href=&quot;https://quarkus.io/blog/building-large-applications/&quot;&gt;Quarkus&lt;/a&gt;&quot;特定生命周期，提供改进的集成和更高效的生命周期构建；以及一个新的实验性扩展，&lt;a href=&quot;https://quarkus.io/extensions/io.quarkus/quarkus-hibernate-panache/&quot;&gt;Hibernate with Panache Next&lt;/a&gt;&quot;，旨在简化&lt;a href=&quot;https://hibernate.org/orm/&quot;&gt;Hibernate ORM&lt;/a&gt;&quot;、&lt;a href=&quot;https://hibernate.org/reactive/&quot;&gt;Hibernate Reactive&lt;/a&gt;&quot;和&lt;a href=&quot;https://jakarta.ee/specifications/data/&quot;&gt;Jakarta Data&lt;/a&gt;&quot;规范的持久性代码。关于这个版本的更多细节可以在版本&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.31.1&quot;&gt;3.31.1&lt;/a&gt;&quot;和版本&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.31.0&quot;&gt;3.31.0&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Apache Tomcat&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://tomcat.apache.org/&quot;&gt;Apache Tomcat&lt;/a&gt;&quot;的版本11.0.18、10.1.52和9.0.115提供了缺陷修复、依赖升级和显著变化，例如：默认情况下忽略 SSLHostConfig 类中 ciphers 属性包含的TLSv1.3密码套件，以保持他们在&lt;a href=&quot;https://www.openssl.org/&quot;&gt;OpenSSL&lt;/a&gt;&quot;和J&lt;a href=&quot;https://docs.oracle.com/en/java/javase/25/security/java-secure-socket-extension-jsse-reference-guide.html&quot;&gt;Java Secure Socket Extension&lt;/a&gt;&quot;（JSSE）规范实现中的配置一致性；以及解决由于调用 Java ClassLoader 类中定义的 getResource() 方法导致的Java URL类中定义的 getContent() 方法在某些情况下失败的回归问题。关于这些版本的更多细节可以在版本&lt;a href=&quot;http://tomcat.apache.org/tomcat-11.0-doc/changelog.html&quot;&gt;11.0.18&lt;/a&gt;&quot;、版本&lt;a href=&quot;http://tomcat.apache.org/tomcat-10.1-doc/changelog.html&quot;&gt;10.1.52&lt;/a&gt;&quot;和版本&lt;a href=&quot;https://tomcat.apache.org/tomcat-9.0-doc/changelog.html&quot;&gt;9.0.115&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JHipster&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.jhipster.tech/&quot;&gt;JHipster&lt;/a&gt;&quot; 9.0.0的第二个beta发布包括显著的变化，例如：支持Spring Boot 4.0；迁移到使用Spring Security @EnableWebSocketSecurity 注解，以取代已弃用的 AbstractSecurityWebSocketMessageBrokerConfigurer 类；以及对CI和测试基础设施的彻底检修。这个版本解决了在第一个beta版本（现已弃用）中发现的一个问题，该问题导致JHipster生成器不稳定。关于这些版本的更多细节可以在&lt;a href=&quot;https://github.com/jhipster/generator-jhipster/releases/tag/v9.0.0-beta.2&quot;&gt;9.0.0-beta.2&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/jhipster/generator-jhipster/releases/tag/v9.0.0-beta.1&quot;&gt;9.0.0-beta.1&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gradle&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gradle.org/&quot;&gt;Gradle&lt;/a&gt;&quot; 9.3.1版本的发布解决了一些显著问题，例如：在使用包含非基本多语言平面（BMP）字符的文件名存储构建缓存输出时的失败；以及恢复了 ModuleVersionSelector 接口，但现在已弃用，以便可以将 ExternalDependency 和 DependencyConstraint 接口的实例传递给 DependencyResolveDetails 接口中定义的 useTarget() 方法。关于这个版本的更多详细信息可以在&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/java-news-roundup-jan26-2026/&quot;&gt;https://www.infoq.com/news/2026/02/java-news-roundup-jan26-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/W0tgkUBN713PCJNDMFcO</link><guid isPermaLink="false">https://www.infoq.cn/article/W0tgkUBN713PCJNDMFcO</guid><pubDate>Mon, 09 Feb 2026 00:00:00 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>Cedar作为沙箱项目加入CNCF</title><description>&lt;p&gt;&lt;a href=&quot;https://www.cedarpolicy.com/&quot;&gt;Cedar&lt;/a&gt;&quot;是一个开源授权策略语言及SDK，现在它已经正式加入了&lt;a href=&quot;https://aws.amazon.com/blogs/opensource/cedar-joins-cncf-as-a-sandbox-project/&quot;&gt;云原生计算基金会 (CNCF)&lt;/a&gt;&quot;，成为其 Sandbox（沙盒）级别的项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目最初由亚马逊云科技构建，旨在为现代应用程序中定义和执行细粒度权限提供一个供应商中立的标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在云原生环境中，管理访问控制传统上依赖于硬编码逻辑或通用策略引擎。Cedar通过允许开发人员将权限表示为策略，有效地将访问控制与应用程序逻辑解耦，从而解决了这一问题。这种分离使得团队无需重新部署代码即可更新权限，这种模式通常被称为“策略即代码”（Policy-as-Code）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该语言支持常见的授权模型，包括基于角色的访问控制 (Role-Based Access Control，RBAC)、基于属性的访问控制 (Attribute-Based Access Control，ABAC) 和基于关系的访问控制 (Relationship-Based Access Control，ReBAC)。Cedar的一个显著特点是其通过形式化验证关注确定性和安全性。该语言规范使用&lt;a href=&quot;https://lean-lang.org/&quot;&gt;Lean定理证明器&lt;/a&gt;&quot;进行了形式化验证，其Rust实现则针对该形式化规范进行了差异化的随机测试。这种数学上的严谨性确保了策略引擎的行为完全符合预期，这对于安全敏感的操作至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了核心语言之外，该项目对自动推理的依赖还实现了高级的工具功能。开发人员可以在部署前使用策略验证器来检查错误，确保策略与定义的模式一致。这种能力允许对策略进行数学分析，以回答诸如“特定请求是否会被允许或拒绝”之类的问题，从而提供了比传统测试方法更高的可信度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在公告中，Kubernetes SIG的名誉成员、工作组联合主席兼CNCF大使Lucas Käldström指出了该语言设计中固有的平衡，他表示：“我对Cedar最欣赏的一点是它深度的知识体系，即它之所以这样工作的原因……它在表达能力和可分析性之间取得了谨慎的平衡。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加入CNCF将Cedar置于与&lt;a href=&quot;https://www.openpolicyagent.org/&quot;&gt;Open Policy Agent(OPA)&lt;/a&gt;&quot;（一个CNCF已毕业的项目）相同的生态系统中。虽然OPA及其语言&lt;a href=&quot;https://www.openpolicyagent.org/docs/latest/policy-language/&quot;&gt;Rego&lt;/a&gt;&quot;是能够处理基础设施、准入控制和应用程序策略的通用工具，但Cedar是专门为应用程序级授权而构建的。它的设计优先考虑为拥有数百万用户和资源的应用程序提供高性能评估。此外，Cedar对ReBAC的原生支持使其与&lt;a href=&quot;https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/&quot;&gt;Google Zanzibar&lt;/a&gt;&quot;模型保持一致，为&lt;a href=&quot;https://openfga.dev/&quot;&gt;OpenFGA&lt;/a&gt;&quot;等其他受Zanzibar启发的开源项目提供了替代方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自首次开源发布以来，该语言已在各个行业中得到采用。&lt;a href=&quot;https://www.youtube.com/watch?v=vDLI9w9Z-R8&quot;&gt;Cloudflare、MongoDB、StrongDM和Cloudinary等组织已将该技术集成到其技术栈中&lt;/a&gt;&quot;。它也是AWS Systems Manager等服务的基础。该项目已开始与其他开源倡议集成，包括Linux Foundation的&lt;a href=&quot;https://jans.io/&quot;&gt;Janssen项目&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/upbound/kubernetes-cedar-authorizer&quot;&gt;Kubernetes-Cedar-Authorizer&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过加入CNCF，该项目过渡到了供应商中立的治理模式。这一转变旨在培养更广泛的贡献者基础，并促进与云原生生态系统的更深度融合。该项目的路线图包括从Sandbox阶段逐步晋升到Incubation（孵化）阶段，最终达到Graduated（毕业）状态，遵循标准的CNCF成熟度生命周期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cedar-joins-cncf-sandbox/&quot;&gt;Cedar Joins CNCF as a Sandbox Project&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lM2YobxDHGC1HZNkRNs4</link><guid isPermaLink="false">https://www.infoq.cn/article/lM2YobxDHGC1HZNkRNs4</guid><pubDate>Sun, 08 Feb 2026 01:00:00 GMT</pubDate><author>作者：Mark Silvester</author><category>开源</category></item><item><title>从测试驱动开发和生产环境测试中获得反馈</title><description>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/ola-hast-80752a21/&quot;&gt;Ola Hast&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.linkedin.com/in/asgaut-mjolne/&quot;&gt;Asgaut Mjølne Söderbom&lt;/a&gt;&quot;在他们在&lt;a href=&quot;https://qconlondon.com/&quot;&gt;伦敦QCon&lt;/a&gt;&quot;&lt;a href=&quot;https://www.infoq.com/presentations/cd-pair-programming/&quot;&gt;关于结对编程的持续交付的演讲&lt;/a&gt;&quot;中提到，团队依赖于强大的单元测试和集成测试，而不是端到端的测试。使用TDD（测试驱动开发）、结对编程和良好的设计，他们经常发布小的更改，在生产环境中测试真实的反馈，并使用功能开关来降低风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hast提到，他们信任他们的单元测试和集成测试，并且把它们作为一个整体。他们没有端到端测试：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们通过使用良好的关注点分离、模块化、抽象、低耦合和高内聚来实现这一点。这些机制与TDD和结对编程相辅相成。结果是一个具有高代码质量的更好的领域驱动设计。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前，他们有更多的HTTP应用程序集成测试，测试整个应用程序，但他们已经从这个（或只有一些愉快的案例）转向了更专注的测试，这些测试有更短的反馈循环，Hast提到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于测试环境总是近似生产环境，并且通常与长供应链和糟糕的测试数据作斗争，他们或多或少已经停止使用它们了，Mjölne Söderbom解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们更喜欢在生产环境中测试，因为在那里我们可以得到最高质量的反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们通过将新功能放在开关后面，并一次部署一小部分来降低风险。这是他们已经做了好几年的事情，而且效果非常好，Mjölne Söderbom说。如果生产环境中出现故障，很容易找到、修复和回滚/前进，他补充说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在之前的文章中，Hast和Mjølne Söderbom提到他们的团队&lt;a href=&quot;https://www.infoq.com/news/2025/07/pair-programming-speed-flow/&quot;&gt;使用TDD进行结对和mob编程&lt;/a&gt;&quot;；没有单独的任务或单独的代码审查。这种方法提高了代码质量，减少了浪费，并促进了知识的共享：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;经过多年的实践，我们最终一起工作，进行TDD，然后部署到生产环境。我们很少在本地或测试环境中测试应用程序。这从来不是我们的主要意图；这只是我们工作方式的一个（愉快的）结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/10/continuous-delivery-pairing/&quot;&gt;结对编程和持续集成可以相辅相成&lt;/a&gt;&quot;。每天多次向主服务器推送是很困难的，这会导致延迟、大型PR和合并问题。结对使即时代码审查、更容易的重构、更少的错误和更高的团队韧性成为可能，Hast和Mjølne Söderbom解释说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hast提到他们过去有更多的测试，测试整个应用程序的运行，但他们已经将这个减少到最低；他们通常只有一条愉快的路径测试，以及针对任何特殊错误情况的额外测试：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们不能用单元测试中测试的东西，我们更喜欢在生产环境中测试。测试总是现实的近似，我们总是与长供应链和糟糕的测试数据作斗争。我们总是从生产环境中得到最好的反馈。停止使用测试环境本身从来不是一个目标，但它只是另一个令人愉快的副作用。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最重要的是建立反馈循环，Mjølne Söderbom提到。反馈有助于导航和选择方向，必要时改变方向：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们从我们的测试中得到最快的反馈，从生产环境中得到最好的反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当某件事情很痛苦时，他们会更频繁地这样做，Hast解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们公司今天之所以能走到这一步，是因为我们增加了部署到生产环境的频率，并在问题最严重的地方迅速得到反馈，然后修复了这些问题。这个过程已经持续了10年。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们非常专注于开发过程中所有级别的快速反馈循环，Mjølne Söderbom说。TDD是我们获得早期和快速反馈的最重要的工具之一，他解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你的代码难以测试，通常意味着设计有问题。代码与我们“对话”，并驱动设计。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多人认为TDD是一个测试工具，但实际上，它是一个设计工具。Mjølne Söderbom总结说，拥有适当的测试来实现快速流动是一个（非常好的）副作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/feedback-TDD-production/&quot;&gt;https://www.infoq.com/news/2026/02/feedback-TDD-production/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/bibe64DvY3it15efBbta</link><guid isPermaLink="false">https://www.infoq.cn/article/bibe64DvY3it15efBbta</guid><pubDate>Sun, 08 Feb 2026 00:00:00 GMT</pubDate><author>Ben Linders</author><category>软件工程</category></item><item><title>AI 智能体界的 npm 来了！Vercel 推出 Skills.sh，欲统一智能体指令集</title><description>&lt;p&gt;Vercel 最近发布了开源项目 &lt;a href=&quot;https://vercel.com/changelog/introducing-skills-the-open-agent-skills-ecosystem&quot;&gt;Skills.sh&lt;/a&gt;&quot;，想要给 AI 智能体（Agents）配上一套“标准动作库”。简单来说，它让智能体能通过命令行执行各种可复用的操作，也就是所谓的“技能”（Skills）。Vercel 将其定义为一个&lt;a href=&quot;https://skills.sh/&quot;&gt;开放的智能体技能生态系统&lt;/a&gt;&quot;，开发者可以在这里定义、分享并运行一个个独立的指令，供智能体在工作流中随时调用。这一工具的核心逻辑，是把智能体的“推理”和“执行”分开——让智能体去调用那些受控、预定义的命令，而不是由它自己去瞎猜、乱写 shell 逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术架构上，Skills.sh 充当了一个轻量级运行时环境，允许智能体调用以 &lt;a href=&quot;https://learn.microsoft.com/en-us/powershell/utility-modules/aishell/concepts/what-is-a-command-shell?view=ps-modules&quot;&gt;shell 脚本形式&lt;/a&gt;&quot;实现的各种技能。每一项技能都遵循简单的契约协议，明确定义了其输入、输出和执行行为。这使得智能体能够以一种可预测、可审计的方式执行各项任务，例如读取或修改文件、运行构建步骤、调用 API 或查询项目元数据。由于技能具有显式定义和版本控制的特性，开发团队可以更清晰地了解智能体被授权的操作范围，并在开发或生产环境中对其行为进行审查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些技能的设计兼顾了本地开发与自动化环境（如 CI 流水线）。开发者可以在&lt;a href=&quot;https://skills.sh/docs&quot;&gt;本地机器安装 Skills.sh&lt;/a&gt;&quot; 直接运行技能，同时将相同的技能无缝集成到由智能体驱动的工作流中。这种一致性旨在减少从实验阶段转向结构化应用场景时的阻力。此外，技能通过简单的配置文件进行描述，这使得开发者无需引入额外的框架或沉重的依赖库，即可轻松地检查、扩展或自定义功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vercel 将该生态系统定位为开源及社区驱动。开发者可以发布自己的技能，并复用他人创建的成果，从而形成一个共享的常用智能体动作库。根据公司分享的早期使用数据，该项目在发布后迅速获得了广泛关注，安装量据报已达数万次。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社区评论更多地聚焦于该方案的实用性而非新颖性。X 平台上的开发者指出，许多智能体任务的失败并非源于推理能力不足，而是由于执行环节的不可靠，而“技能层”的引入正好填补了这一空白。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件开发者 Thomas Rehmer &lt;a href=&quot;https://x.com/thomas_rehmer/status/2016018978250834305&quot;&gt;评价&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;确实该这么搞。有了这些‘可发现’的技能，总算把智能体架构里那个‘你能干嘛？’的经典难题给破了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，AI 工程师 Aakash Harish &lt;a href=&quot;https://x.com/0_Aakash_0/status/2014024888575729964&quot;&gt;发文&lt;/a&gt;&quot;称：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这就是 AI 智能体界的 npm。它的精髓在于：比起纠结协议有多复杂，Skills 更看重好不好组合。如果说 MCP 搞定的是‘智能体怎么跟工具搭火’，那 Skills 搞定的就是‘开发者怎么分享和找现成的能力’。这俩以后肯定不是谁取代谁，而是强强联手：用 Skills 搞定发现和共享，用 MCP 去啃那些对确定性要求极高的企业级硬骨头。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不少开发者将 Skills.sh 与目前围绕智能体执行兴起的其他工具和标准进行了对比。类似的理念也出现在其他协议驱动的方案中，例如 Anthropic 推出的 &lt;a href=&quot;https://www.infoq.com/articles/mcp-connector-for-building-smarter-modular-ai-agents/&quot;&gt;Model Context Protocol&lt;/a&gt;&quot; (MCP)（侧重于通过结构化的 API 访问工具和数据），以及 OpenAI 的 &lt;a href=&quot;https://www.infoq.com/news/2023/06/openai-api-function-chatgpt/&quot;&gt;Function Calling&lt;/a&gt;&quot;（通过 JSON schema 暴露预定义动作）。此外，包括 &lt;a href=&quot;https://docs.langchain.com/oss/python/langchain/tools&quot;&gt;LangChain&lt;/a&gt;&quot; 的 tools 和 CrewAI 的 &lt;a href=&quot;https://docs.crewai.com/en/concepts/tasks&quot;&gt;tasks&lt;/a&gt;&quot; 在内的其他项目也致力于为智能体提供受控的执行权限，不过它们通常依赖于更高层的 Python 抽象，而非基于 shell 的命令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/vercel-agent-skills/&lt;/p&gt;</description><link>https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</link><guid isPermaLink="false">https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</guid><pubDate>Sat, 07 Feb 2026 00:00:00 GMT</pubDate><author>作者：Daniel Dominguez</author><category>AI&amp;大模型</category></item><item><title>AI时代的分野与合流：什么才是算力选型的“版本答案”？</title><description>&lt;p&gt;当 AI 大模型从实验室冲向产业一线，企业的算力需求正经历一场前所未有的“撕裂式分化”：一边是 3A 游戏、AI 渲染等场景对极致性能的“军备竞赛”，一边是 Web 服务、视频转码等高频场景对性价比与能效比的“精打细算”，电力成本的飙升与数据安全的红线，更让这场算力抉择变成“既要又要还要”的多重考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去“一套拳法打天下”的时代早已落幕，当行业还在为“性能优先”还是“成本优先”争论不休时，英特尔与腾讯云的联合实践给出了不一样的答案。在最新一期《C 位面对面》栏目中，InfoQ 极客传媒创始人 &amp;amp;CEO 霍太稳和英特尔数据中心与人工智能集团副总裁兼中国区总经理陈葆立、腾讯云 CVM 产品副总经理李德铠的深度对话，揭开了算力“分野与合流”的核心逻辑——以芯片双架构为底层支撑，以分层云实例为落地载体，让高性能与普惠性不再对立，让软硬件协同成为破解行业痛点的关键钥匙。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;算力分化时代的三重困局&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数字化与智能化的加速，让企业算力需求的分化从隐性走向显性，而 AI 技术的爆发则让这种分化演变成不可调和的多重矛盾，倒逼行业从“大一统”走向“精细化”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;性能与能效的对立统一&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“AI 算力的尽头其实是电力”，李德铠的这句话点破了行业核心困境。随着大模型参数指数级增长，电力成本在算力总支出中的占比已攀升至极高水平，单纯追求极致性能的算力配置，往往会陷入“高能耗、高成本”的恶性循环。但另一方面，GenAI、游戏等场景，又对算力的主频、内存带宽和并行计算能力提出了苛刻要求，性能短板直接影响用户体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种矛盾催生了算力需求的分层：一部分场景需要“火力全开”，另一部分场景则需要“精打细算”。陈葆立补充道：“这种分化不是短期现象，而是 AI 时代的长期趋势。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;安全与生态的刚性需求&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了性能与成本，数据安全与生态适配也成为企业算力选型的“必选项”。随着数据资产价值的提升，企业对数据存储、传输、计算全流程的安全要求越来越高，而算力解决方案能否与现有生态无缝对接，直接影响部署效率与迁移成本。陈葆立强调：“企业需要的不仅是算力本身，更要构建完整的安全防护体系和生态支持。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;通用与专用的场景分化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过往企业依赖一套通用架构解决所有问题的模式，在 AI 时代也彻底失效。不同业务场景的算力诉求呈现出显著差异：3A 游戏需要单核高主频保障操作无延迟，视频转码需要高并发处理能力降低成本，AI 推理需要矩阵运算加速提升效率，Web 服务需要稳定性能避免抖动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;场景的细分要求算力供给必须“精准匹配”，而非“大水漫灌”。腾讯云与英特尔的合作，正是抓住了这一核心趋势，推出针对性的产品组合，让不同场景都能找到“量身定制”的算力解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;双轨破局：从芯片到云实例的协同革命&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对算力撕裂的三重困局，英特尔与腾讯云的联合创新并非简单的产品叠加，而是从芯片架构到云实例、从硬件优化到软件协同的全链路重构，构建起“性能 + 普惠”的双轨算力体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际上，英特尔在最新的至强®6 处理器中推出双架构设计，便是应对算力需求分化的破题关键。“一个是性能核 P-core，另一个是能效核 E-core，我们希望通过两种不同的处理器架构，提供不同的算力服务于客户。”陈葆立介绍道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，性能核 P-core 主打极致性能，具备高主频、大缓存、高内存带宽等特性，完美适配 AI 训练、高性能计算等核心场景；能效核 E-core 则聚焦高内核密度与更优每瓦性能，通过精简设计，在保证性能的同时降低功耗，特别适合云原生、高并发等普惠型场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于英特尔至强®6 的双架构，腾讯云打造了 S9E、S9Pro、S9 三款分层实例：其中 S9E 与 S9Pro 主打极致性能，搭载至强®6 P-core，专为 AI、游戏、图像渲染等高性能场景而生；S9 实例则是全球首发搭载至强®6 E-core 的 SRF-AP 云实例，主打高性价比与高并发适配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高性能场景：CPU 与 GPU 如何“1+1＞2”？&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 RAG（检索增强生成）等高性能场景中，行业曾普遍认为 GPU 能包揽所有核心任务，CPU 并无用武之地，但基于英特尔至强®6 P-core 的 S9E/S9Pro 用实践打破了这一认知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“进入大模型时代，GPU 和 CPU 有各自的优势——GPU 算力强，CPU 内存大。如果以篮球队为例，CPU 就像控球后卫，既可以传球给 GPU 前锋，也能自己得分。”陈葆立表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，作为“控球后卫”，CPU 能帮助 GPU 更好的释放性能。&amp;nbsp;陈葆立表示：“大模型就像记忆力不好的天才，无法在 GPU 中存储大量用户上下文。但是如果通过 CPU 与系统内存的协同，就能最大化发挥 GPU 的能力。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在 RAG 场景中，借助 CacheClip 技术，能够有效提升 KVCache 的利用率，从而支持更长的上下文窗口并提高执行效率；另外，通过英特尔推出的异构计算框架 HeteroFlow，能够将 MoE 模型中的“冷专家”模块直接卸载至 CPU 处理（卸载、调度、加速三管齐下），让 GPU 的工作更聚焦，从而突破显存瓶颈，为用户带来更高的整体性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“许多客户在 TTS、ASR、OCR 等预处理任务上的日常支出，甚至达到后续大模型推理费用的数十倍。这些 AI 工作的前置准备阶段，以前常常要 GPU 分心兼顾。AI 工作负载里的非结构化数据解析、格式转换、特征清洗，看着是‘细活’，实则要高并行逻辑和高 I/O 吞吐，正好是机头 CPU 的强项。更重要的是：数据预处理通常具有流程复杂、数据量巨大、需弹性扩展、实时性要求低但吞吐量极高等特点。而这些需求，恰恰与至强®6 的架构优势高度契合。CPU 把预处理扛了，GPU 就不用在训练推理这样的核心任务和预处理这种边缘任务之间来回切换，算力与时间一点儿都不浪费。”陈葆立解释道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“这种 1+1&amp;gt;2 的组合拳，不仅帮客户解决了 GPU 资源紧张的燃眉之急，更通过更优的部署成本和更低的系统延迟，实现了全链路的性能提升。”李德铠补充道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一方面，CPU 本身也能在 AI 场景“上大分”。&amp;nbsp;英特尔至强®6 P-core 集成了 AMX 加速引擎，专为大规模 AI 训练和推理工作负载提供支持，能够助力客户提高效率，降低推理、训练和部署成本以及降低总拥有成本 (TCO)。值得一提的是，由于 AMX 是直接集成在 CPU 内核上且靠近系统内存的内置加速器，相比于独立加速器，它能提供更便捷、更快速的加速支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“利用 AMX 矩阵加速能力，S9e/S9pro 可以非常高效地处理 Embedding（向量嵌入）、数据清洗和中小型模型的推理任务。”李德铠举例称。腾讯云实测数据显示，在千问 4B 小模型的 Embedding 场景中，搭载英特尔至强®6 P-core 的 S9E/S9Pro 相比 T4 GPU 卡，性能提升了 25%，综合性价比直接提升了 66%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，在游戏对战服、AI 渲染和图像处理等传统高性能场景中，S9E/S9Pro 凭借着高主频和高内存带宽等特性，也成为了“客户的优选”。“玩家 PK 时的实时响应至关重要，S9E/S9Pro 能保障数据传输的高带宽和低延迟，让操作指令即时生效不卡顿。”李德铠介绍道，“此外，在 AI 渲染和图像处理场景中，S9E/S9Pro 的多线程核与高内存带宽，也能帮助客户企业快速处理海量数据，大幅提升渲染效率，缩短项目周期。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;普惠场景：如何实现“性能无抖动、满载不降频”？&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“普惠不代表性能不行，搭载至强®6 E-core 的 S9 相比上一代实例性能提升了 15%-20%，能稳定支撑 Web 服务、小程序等轻负载场景的高并发需求。”李德铠强调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“性能无抖动、满载不降频”的背后是英特尔与腾讯云在设计细节上的不妥协。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们首先肯定不希望普惠版的云实例在性能上缩水。”陈葆立表示。硬件层面，英特尔在 E-core 中关闭了超线程功能，让每个虚拟机拥有独立的物理核、显存和内存，避免用户间的性能干扰，保障性能稳定无抖动。“另外，能效核（E-core）顾名思义它的能效比是非常好的，也就是在性能更优的同时功耗更低，这也符合国家倡导的节能减碳以及绿色数据中心等理念。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件与优化层面，腾讯云也做了非常多用户“看不见”的工作。其中最关键的就是“绑核设计”——将 CPU 核心与虚拟机绑定，确保单个虚拟机高负载运行时，不影响整片 CPU 的性能表现；同时优化了 CPU 与总线、内存的搭配关系，进而缩短了数据传输路径，提升了计算效率。“‘绑核设计’对于计算密集型任务的提升非常明显。”李德铠补充道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种软硬件协同的优化，让 S9 在教育行业大受欢迎。教育机构的录播视频课程通常需要转码为不同码率，以适配不同网络环境，S9 实例搭配英特尔软件库后，转码性能获得了 90% 以上的提升，在降低成本的同时，保障了课程传输的流畅性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“S9 的高并发适配能力，也使其在 Web 服务和小程序场景中备受青睐。很多客户反馈，S9 能稳定支撑高峰期的并发请求，且成本比传统实例更低。”李德铠补充道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，基于英特尔至强®6 E-core 的 S9 云实例，在腾讯内部的超大规模业务中，也得到了普遍验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以微信存储为例，不仅通过高 I/O 实例配置的使能以及软件优化，高效解决了 Gen5 SSD 高吞吐性能所带来的存储压力，还通过英特尔 QAT（数据保护与压缩）加速器，使得存储压缩效率提升了 70%，真正实现了“用更少空间存更多数据”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在搜索业务领域，腾讯新一代海量搜索引擎借助 SRFAP 平台能力，使得元宝的搜索性能提升了 15%。另外，在大数据业务中，S9 实例的多核并行能力与扩展性，也帮助腾讯实现了显著的降本增效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;产品成功的背后，是双方更深层次的战略共识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“我们双方的合作已经超越了简单的买卖关系，上升到了联合定义产品的高度。腾讯的宗旨是一切以用户价值为依归，英特尔则以客户场景为核心，双方的价值观高度契合。我们不会盲目追求单纯的技术参数，而是先去听市场和用户的声音，再回过头来定义产品。”李德铠表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种契合体现在合作的全流程：从芯片设计阶段，英特尔就与腾讯云紧密沟通，了解最终用户的实际需求，定制化设计芯片；在产品定义阶段，双方共同规划了三款实例的定位，确保硬件架构与场景需求精准对接；在技术优化阶段，双方专家联合研发，充分发挥 AMX、QAT 等指令集的优势，将好钢用在刀刃上，真正让技术红利转化为客户价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI Agent 引爆的算力“新战场”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈及未来，李德铠表示：“AI 的技术热潮已经从模型向 Agent（智能体）演进，这将带来算力需求的新变化。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 发展的早期，算力资源几乎全部向“模型训练”倾斜，但随着 AI Agent 时代的到来，这种天平正在发生逆转——从“重训练”转向“重推理”。“据专家预测，未来推理算力的需求将达到现在训练算力的 10 倍。”陈葆立指出。如果说训练主要是 GPU 的“大力出奇迹”，那么 Agent 架构中的各种推理需求则让 CPU 的角色将变得空前重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Agent 的本质是‘大脑 + 工具’。GPU 负责思考，而 CPU 负责执行（比如运行 Python 代码、查询数据库、读写文件、网络通信等），每一步推理后，CPU 都要介入处理非线性逻辑，这会导致 CPU 的负载大幅增加。腾讯云第九代云实例及至强®6 平台的设计，正是致力于通过更高性能、更优能效的通用算力，帮助客户应对推理与 Agent 负载带来的计算密度挑战。”李德铠表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，“算力即财富”的 AI 时代也对系统的稳定性、可靠性提出了更高的要求。随着 AI 集群正加速向万卡规模突破，系统越复杂，计算密度越高，就越可能出现更多的静默数据错误。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈葆立指出，至强®6 具备 99.999% 的 RAS（可靠性、可用性、可维护性），能够全面保障整体系统的稳定运行；同时其内置的 TDX 技术，能够为云服务提供硬件级可信执行环节，有效支持通用机密计算和异构机密计算，助力构建端到端的可信 AI 服务能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“TDX 技术，帮助我们在云上打造了一个‘数据保险箱’，企业可以无缝地把它的 AI 模型、Agent 应用部署到 S9 系列实例上，来确保模型与数据的安全。”李德铠补充道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，AI 应用的大爆发也将进一步催生数据海量吞吐的需求，对内存、SSD 等部件厂商提出了更高的要求，也对生态适配提出了新挑战。“英特尔作为平台方，一直以来保持着‘生态联盟’的方式，跟伙伴厂商保持密切互动、相互验证，以确保整个平台生态的高质量、高安全、高可用。”陈葆立表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，英特尔最新的 18A 制程工艺已进入量产阶段，性能提升可达 15%，密度提升 30%。基于 18A 制程的至强®6 Plus 处理器（Clearwater Forest）将于 2026 年内发布，目前英特尔已经与部件厂商、腾讯云等合作伙伴展开了早期的适配与验证工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“下一步，随着英特尔 18A 等革命性制程工艺的推进，我们也会在第一时间推出结合下一代芯片性能的云实例。同时，我们正在紧锣密鼓地研发适配 vRDMA 网络的新一代 CVM 机型，这将进一步释放底层硬件的传输潜力。此外，在加密计算、可信计算等对安全性要求极高的垂类场景，我们也会持续演进，利用最新的指令集优势，为企业数字化转型和 AI 应用的全面落地提供一个更高效、更安全、更具性价比的底座。”李德铠表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;算力的分野与合流，本质上是 AI 产业从“技术狂欢”走向“价值落地”的必然。当狂热褪去，企业终将回归理性：算力的核心竞争力，从来不是参数的堆砌，而是对场景需求的精准响应与资源的最优配置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;英特尔与腾讯云的双轨实践，撕开了行业“非黑即白”的选型困局：性能核与能效核的二元架构，消除了“杀鸡用牛刀”的尴尬；CPU 与 GPU 的异构协同，打破了“谁主谁次”的偏见；分层实例的场景适配，终结了“一套方案包打天下”的粗放。这不是简单的产品组合，而是对算力分配逻辑的底层重构，让每一份算力投入都能匹配对应的业务价值，让技术创新真正服务于成本与效率的平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI Agent 时代的到来，将会让这种精准匹配的需求愈发迫切。推理算力的爆发式增长、多模态场景的复杂诉求，会进一步放大“按需分配”的重要性。而英特尔与腾讯云的合作，早已提前卡位这一趋势：从芯片到实例，从硬件到软件，构建起了一套“场景定义技术”的完整闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 时代的算力革命，终将是一场“以场景为锚、以协同为纲”的效率革命，谁能更深刻地洞察不同业务的算力痛点，谁能更高效地整合软硬件资源形成精准解决方案，谁就能掌握 AI 落地的核心话语权。那些真正能够破解“既要又要还要”困局的玩家，才能成为最终的规则制定者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接 &lt;a href=&quot;https://www.infoq.cn/video/503rHwTuG0Esmegc8Fhk&quot;&gt;https://www.infoq.cn/video/503rHwTuG0Esmegc8Fhk&lt;/a&gt;&quot; 观看完整视频。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fdGoAsQ91o7rDIGiOsrd</link><guid isPermaLink="false">https://www.infoq.cn/article/fdGoAsQ91o7rDIGiOsrd</guid><pubDate>Fri, 06 Feb 2026 09:00:00 GMT</pubDate><author>付秋伟</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>“16个Agent组队，两周干翻37年GCC”？！最强编码模型Claude Opus 4.6首秀，10万行Rust版C编译器跑通Linux内核还能跑Doom</title><description>&lt;p&gt;Anthropic 正在升级它“最聪明的模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着新一代旗舰模型 Claude Opus 4.6 的发布，Anthropic 释放出的信号十分明确：这并不是一次常规的性能小修小补，而是一轮围绕长任务、复杂工作，以及智能体（agent）如何真正干活展开的系统性升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c1550330d571d4dfdb7e5f7f8795d540.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这次发布之前，Anthropic 内部和部分早期用户已经开始让 Opus 4.6 参与一项持续时间很长的工程任务：从零开始，用 Rust 编写一个完整的 C 编译器，并要求它能够编译 Linux 内核。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项实验持续了约两周时间，期间累计运行了近两千次 Claude Code 会话，最终产出了一个规模约 10 万行代码的编译器。该编译器不仅能够在多种架构上构建 Linux 6.9，还可以编译 FFmpeg、Redis、PostgreSQL、QEMU，并通过了 GCC 自身 99% 的 torture test，甚至能够成功编译并运行 Doom。整个实验的 API 成本约为 2 万美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了让外界更直观地理解这一成果的尺度，有网友在社交平台上给出了一个对照：GCC 的开发从 1987 年开始，历经 37 年，投入过数以千计的工程师。而这一次，是一名研究者加上 16 个 AI 智能体，在短短数周内完成了一个能够通过大量 GCC 测试集、并编译真实大型项目的编译器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbfb1e958df9b19689507e724aefd217.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样一段持续推进的工程实践之后，Anthropic 对外发布了 Claude Opus 4.6。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成立于 2021 年、由一批前 OpenAI 研究人员和高管创立的 Anthropic，一直以 Claude 系列大模型为核心产品；在这一体系中，Opus 代表最大、能力最强的型号，Sonnet 和 Haiku 则分别覆盖中等与轻量级使用场景。某种程度上，Opus 系列承担的角色，就是在更复杂、更长期的任务环境中检验 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最强的编码模型：从跑分看 agentic 编程能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 对 Opus 4.6 的定位，并不只是“更会写代码”。他们强调，新模型在编程能力上的提升，已经从单纯的代码生成，扩展到更前置的任务规划，以及更后置的代码审查与调试流程。这种变化，使模型能够在大型代码库中更稳定地工作，也直接决定了它是否有能力脱离短对话模式，持续参与多阶段、长周期的工程任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种定位在评测结果中体现得比较清楚。Anthropic 公布的多项基准测试显示，Claude Opus 4.6 在 agentic 编程、计算机使用、工具调用、搜索以及金融等任务上，整体跑分都有所提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/67/678307d2b742d377329cb0b226c856aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在终端 agentic 编程能力上，Opus 4.6 得分 65.4%，对比来看，略高于 GPT-5.2 的 64.7%，明显领先 Gemini 3 Pro（56.2%）和 Sonnet 4.5（51.0%）。这说明在纯终端环境下执行多步编程任务时，Opus 4.6 的稳定性和自我修正能力处在第一梯队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 SWE-bench Verified（Agentic coding） 上，各家分数非常接近，Opus 4.6（80.8%）与 Opus 4.5（80.9%）、GPT-5.2（80.0%）基本处于同一水平。这里可以理解为：在标准化的软件工程任务上，能力已经开始趋同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在电脑操作（OSWorld）上，代际差异开始显现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OSWorld（Agentic computer use） 是一个比较关键的分水岭。Opus 4.6 达到 72.7%，相比 Opus 4.5 的 66.3% 有明显提升，而 Sonnet 4.5 只有 61.4%，其他模型则未给出对等数据。这类评测关注的是 GUI 操作、跨应用流程和状态理解能力。放在整张表里看，它与编程能力的同步提升，意味着 Opus 4.6 不只是“会想”，而是更擅长把计划落到具体操作上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agentic search（BrowseComp）：明显拉开差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BrowseComp 是整张表里差距最清楚的一项。Opus 4.6 为 84.0%，而 GPT-5.2 Pro 是 77.9%，Opus 4.5 只有 67.8%，Sonnet 4.5 更低。这一项测的是在真实开放网络中定位、筛选和组合信息的能力，结果说明 Opus 4.6 在“研究型 agent 行为”上已经明显领先，而不是只在封闭工具或结构化任务中占优。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，在 Humanity’s Last Exam（跨学科推理）和 ARC-AGI-2（新问题解决） 上，Opus 4.6 的优势更加明显，尤其是 ARC-AGI-2 的 68.8%，相比 GPT-5.2 Pro 的 54.2% 和 Gemini 3 Pro 的 45.1%，已经不是细微差距。这类评测通常更难通过“提示工程”或策略优化取得跃升，更像是在反映模型本身的泛化推理能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“上下文腐烂”与模型可用性的分水岭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Opus 4.6 还扩大了上下文窗口，也就是单次会话里可记住、可处理的信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新模型在 Beta 阶段提供 100 万 token 的上下文长度，与该公司现有的 Sonnet（4 和 4.5 版本）相当。Anthropic 表示，这样的上下文容量更适合处理更大型的代码库，也能支持对更长文档的分析与处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但Anthropic 特别强调，Opus 4.6 的提升并不是“能塞更多 token”，而是“塞进去之后还能用”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们在说明中提到，Opus 4.6 在大规模文档中检索关键信息的能力显著增强，这一点在长上下文任务中尤为明显：它可以在数十万token 范围里持续跟踪信息，偏差更小，也更容易捕捉到埋得很深的细节——包括一些 Opus 4.5 本身就已经容易漏掉的信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正好对应了开发者长期吐槽的一个问题：“上下文腐烂（context rot）”。很多模型在对话或任务一旦拉长之后，要么开始遗忘早期信息，要么虽然“看过”，但已经无法在后续推理中正确调用，最终表现为前后不一致、定位问题跑偏、重复试错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MRCR v2（8-needle、100 万 token）这类“草堆找针”测试，本质上就是在专门检验这种能力：把多个关键线索埋在超长文本里，看模型能否在不迷路的情况下把它们重新找出来。Opus 4.6 在该测试中的得分为 76%，而 Sonnet 4.5 仅为 18.5%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并不是简单的“高一点、低一点”，更像两种不同的可用性状态：一个模型在超长上下文中仍然能稳定检索并利用信息，另一个则在任务拉长后迅速失效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/746521ff51da955e770b0155c22f7bec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种长上下文的稳定性，直接影响模型能否胜任更“工程化”的工作，尤其是复杂代码分析与故障诊断。在 Anthropic 给出的能力图中，Opus 4.6 被特别标注为擅长做 root cause analysis（根因分析）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/caf21288bc8205c65017476277640bed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Agent团队，构建一个C编译器&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4.6 最醒目的新增功能，是 Anthropic 所称的“智能体团队”（agent teams）：由多个智能体组成的小队，可以把一个大任务拆成若干独立的子任务分别推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 的说法是：“不再让单个智能体按顺序把任务一路做到底，而是把工作分给多个智能体——每个智能体负责自己的一块，并直接与其他智能体协调。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 产品负责人 Scott White 将其类比为“雇了一支很能干的人类团队”，因为职责拆分后，智能体可以并行协作，从而更快完成工作。目前，“智能体团队”以研究预览（research preview）的形式向 API 用户与订阅用户开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编译器本身固然是一个高度复杂、且极具工程价值的成果，但在 Anthropic 团队看来，它更像是一次“能力压力测试”的载体。真正值得总结的，是围绕 长时间运行的自治 Agent 团队（long-running autonomous agent teams） 所形成的一整套工程方法论：如何设计无需人工干预的测试体系、如何让多个 Agent 并行推进复杂工作、以及这种架构在现实工程中究竟会在哪些地方触碰到上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“协作式 Agent”到“自治式 Agent”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现有的 Agent scaffolding（例如 Claude Code）本质上仍然是人机协作系统：模型在解决复杂问题时，往往会在某个阶段停下来，等待操作者继续输入新的指令、确认状态，或澄清歧义。Anthropic 的实验目标是消除这种对“人类在线”的依赖，让 Claude 能够在无人监督的情况下，持续推进一个长期任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了实现持续自主的进展，Claude 工程团队并没有引入复杂的调度系统，而是构建了一个程序，让 Claude 进入一个简单的循环（如果你见过 Ralph 循环，应该会觉得眼熟）：每完成一个任务，就立刻进入下一个任务，而不是回到“等待用户”的状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/24da9de1991a3726416c38d0c3099bb2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Agent prompt 中，Claude 被明确要求将问题拆解成可执行的小任务、记录当前进展、判断下一步行动，并持续迭代，直到系统判定“没有明显改进空间”。（在这最后一点上，Claude 没有选择，因为循环会一直运行——不过在一次实验中，团队确实看到 Claude 不小心执行了 pkill -9 bash，结果把自己杀掉了，循环也就随之结束了。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;并行运行 Claude&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并行运行多个实例，可以缓解单一 agent harness 的两个弱点：&lt;/p&gt;&lt;p&gt;一次 Claude Code 会话同一时间只能做一件事。随着项目范围扩大，并行调试多个问题会高效得多。&amp;nbsp;运行多个 Claude agent 可以实现“分工”。当一部分 agent 负责解决核心问题时，其他专门的 agent 可以被调用来（例如）维护文档、盯代码质量，或处理更专门的子任务。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude 工程团队的并行实现非常基础：先创建一个新的裸 Git 仓库；然后为每个 agent 启动一个 Docker 容器，把仓库挂载到 /upstream。每个 agent 会在容器内克隆一份本地副本到 /workspace，完成工作后，从各自的容器把改动推回 upstream。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为避免两个 agent 同时尝试解决同一个问题，harness 使用了一个简单的同步算法：&lt;/p&gt;&lt;p&gt;Claude 通过在 current_tasks/ 下写入一个文本文件来“锁定”某个任务（例如，一个 agent 可能锁定 current_tasks/parse_if_statement.txt，另一个锁定 current_tasks/codegen_function_definition.txt）。如果两个 agent 试图认领同一任务，Git 的同步机制会迫使第二个 agent 改选另一个任务。&amp;nbsp;Claude 在任务上工作完成后，会从 upstream 拉取、合并其他 agent 的改动、推送自己的改动，然后移除锁。合并冲突很常见，但 Claude 能够处理。&amp;nbsp;无限的 agent 生成循环会在一个全新的容器里启动新的 Claude Code 会话，然后重复上述流程。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一个非常早期的研究原型。Claude 工程团队尚未实现任何其他 agent 之间的通信方法，也没有强制任何高层目标管理流程，也没有使用 orchestration agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，团队把“如何行动”的决定权交给每个 Claude agent。多数情况下，Claude 会选择“下一个最显而易见”的问题继续做；当卡在某个 bug 上时，Claude 往往会维护一份持续更新的文档，记录失败过的方法和剩余任务。在项目的 Git 仓库里，可以通过历史记录看到它如何在不同任务上获取锁并推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用 Claude 团队写代码：一些更管用的做法&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把 Claude 放进循环只是起点，真正决定它能否持续推进的，是它能不能从环境和反馈中判断“下一步该做什么”。因此，Claude 工程团队把大量精力放在模型之外：测试如何设计、反馈如何呈现、运行环境如何约束，才能让 Claude 在无人干预的情况下仍然保持方向感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个核心前提是：必须围绕语言模型的固有限制来设计系统。在这次实践中，团队重点应对了两类限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是上下文窗口污染。测试框架不能输出成千上万字节的无用信息，最多只保留几行关键输出，其余重要内容统一写入文件，供 Claude 在需要时自行查阅。日志也需要便于自动处理：一旦出现错误，必须在同一行明确标出 ERROR 以及失败原因，方便grep直接检索。同时，能提前算好的汇总统计信息会被预先计算，避免 Claude 在上下文中反复做同样的推导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类限制是时间盲。Claude 无法感知时间，如果无人干预，很容易长时间沉浸在跑测试里而不推进工作。为此，测试框架很少输出增量进度，避免不断污染上下文，并提供默认的 --fast 选项，只运行 1% 或 10% 的随机子样本。这个子样本对单个 agent 是确定的，但在不同虚拟机之间是随机的，从整体上仍能覆盖所有文件，同时又能让每个 agent 精确识别回归问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在并行方面，团队也很快意识到：并行是否有效，取决于问题是否“好拆”。当失败测试数量多且彼此独立时，并行非常直接——每个 agent 处理一个不同的失败测试即可。在测试通过率接近 99% 后，团队让不同 agent 分别去完成不同小型开源项目的编译，例如 SQLite、Redis、libjpeg、MQuickJS 和 Lua。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但当任务升级到编译 Linux 内核时，情况发生了变化。内核编译本质上是一个高度耦合的整体任务，所有 agent 都会命中同一个 bug，修完再相互覆盖。即便同时运行 16 个 agent，也无法带来实质进展，因为大家都卡在同一件事上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决办法是引入 GCC 作为在线的、已知良好的对照编译器。团队编写了新的测试框架：随机选择内核中大部分文件用 GCC 编译，只把剩余文件交给 Claude 的 C 编译器。如果内核能够正常运行，说明问题不在 Claude 负责的那部分文件；如果失败，则再通过把其中一些文件切回 GCC 编译，逐步缩小范围。这样一来，不同 agent 就可以并行地修复不同文件中的不同错误，直到 Claude 的编译器最终能够编译全部文件。即便如此，后续仍需要配合增量调试（delta debugging），找出那些“单独没问题、组合在一起就失败”的文件对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;并行运行也带来了另一层收益：角色分工成为可能。在实践中，Claude 工程团队发现，LLM 生成的代码很容易重复实现已有功能，因此专门安排了一个 agent 负责扫描并合并重复代码；另一个 agent 聚焦于提升编译器自身的性能；第三个 agent 负责改进生成代码的效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，还有 agent 从 Rust 开发者的视角审视整个项目的设计，提出结构性调整建议，以提升整体代码质量；另一个 agent 则专注于文档维护。通过这种方式，不同 Claude 实例在同一代码库中承担起相对稳定的职责，而不是反复在同一层面“重新发明轮子”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;评估结果与能力边界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在两周内接近 2,000 次 Claude Code 会话中，Opus 4.6 共消耗约 20 亿输入 token、生成约 1.4 亿输出 token，总成本略低于 2 万美元。该团队表示，即便与最昂贵的 Claude Max 方案相比，这仍是一次成本极高的实验；但这一成本依然远低于由单人、甚至完整人类团队完成同等工作的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器是一次完全的 clean-room 实现：开发过程中 Claude 从未获得互联网访问权限，仅依赖 Rust 标准库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终得到的约 10 万行代码，能够在 x86、ARM 和 RISC-V 架构上构建可启动的 Linux 6.9，同时也可以编译 QEMU、FFmpeg、SQLite、Postgres、Redis，并在包括 GCC torture test 在内的大多数编译器测试套件中达到约 99% 的通过率。此外，它还通过了开发者的终极考验：它可以编译并运行 Doom 游戏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，这一项目也把当前 Agent 团队的能力边界暴露得相当清晰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;缺乏启动 Linux 所需的 16 位 x86 编译能力，因此在 real mode 阶段会调用 GCC（x86_32 与 x86_64 编译器由其自身实现）。尚未拥有稳定可用的 assembler 与 linker；这些是 Claude 开始自动化的最后环节，目前仍存在问题，演示中使用的是 GCC 的相关工具。该编译器能够成功编译许多项目，但并非所有项目都能成功。它目前还不能完全替代真正的编译器。生成的代码效率不高。即使启用所有优化，其效率也低于禁用所有优化的 GCC 生成的代码。Rust 代码质量尚可，但远不及 Rust 专家级程序员编写的代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体实现已接近 Opus 的能力上限，新增功能或修复 bug 时，经常会破坏已有功能。其中一个最具代表性的难点是 16 位 x86 代码生成。尽管编译器可以通过 66/67 opcode 前缀生成语义正确的 16 位 x86 代码，但生成结果超过 60KB，远高于 Linux 强制的 32KB 限制。因此，在这一阶段，Claude 选择调用 GCC 作为替代（该情况仅出现在 x86 上；在 ARM 与 RISC-V 架构下，编译可完全由 Claude 自身完成）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器的源码已经公开：&lt;a href=&quot;https://github.com/anthropics/claudes-c-compiler&quot;&gt;https://github.com/anthropics/claudes-c-compiler&lt;/a&gt;&quot;。Claude 工程团队建议直接下载、阅读代码，并在自己熟悉的 C 项目上尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dc71d0e9a3b680a980d90fc83313bccf.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/news/claude-opus-4-6&quot;&gt;https://www.anthropic.com/news/claude-opus-4-6&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/engineering/building-c-compiler&quot;&gt;https://www.anthropic.com/engineering/building-c-compiler&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</link><guid isPermaLink="false">https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</guid><pubDate>Fri, 06 Feb 2026 08:33:16 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>TypeScript 之父 Anders Hejlsberg：别折腾“AI新语言”了，真正变天是 IDE 让位给 Agent</title><description>&lt;p&gt;过去十年，TypeScript 被很多团队当作“工程化 JavaScript”的答案；到了 AI 编程时代，它又意外变成了 AI 最顺手的语言之一——原因很简单：AI 写代码的能力基本取决于它见过多少这种语言的代码，而 TypeScript/JavaScript 恰好是训练语料最丰富的那一档；更关键的是，TypeScript 还把类型与接口这些“语义线索”明明白白写在代码里，正好让 AI 更容易理解、重构和补全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这种背景下，微软在 2025 年 12 月 16 日完成了 TypeScript 史上最激进的一次重构：用 Go 语言迁移（重写）编译器与部分工具链，宣称带来 10 倍性能飞跃。但消息一出，社区立刻炸锅——明明 Rust 才是当下重写系统级工具的“默认答案”，为什么偏偏选 Go？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 之父、同时也是 Turbo Pascal、Delphi、C# 等语言的核心设计者 Anders Hejlsberg，在与 GitHub 研究顾问 Eirini Kalliamvakou 的对谈中正面回应了这些质疑：很多人认为他们“应该选另一门语言”，但他坚持“我们选了最合适的工具，而且过去一年已经证明了这一点”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，Hejlsberg 也谈到 AI 在这次迁移中的真实位置：团队曾尝试让 AI 直接把 TypeScript 代码迁移到 Go，“结果不太理想”，因为他们需要的是五十万行代码级别、行为完全一致的确定性迁移；AI 只要偶尔“偏一点”，就会把成本转移到逐行审查上，得不偿失。相比之下，让 AI 去生成迁移工具、以及在迁移之后自动同步旧代码库新增的 PR 变更，反而更有效——这部分他们“已经相当成功”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Hejlsberg 也明确指出：在“AI 无处不在”的新环境里，TypeScript 的语言服务（补全、跳转、重构、快速修复）不会只是原样搬家，而是在被大幅重塑——因为很多过去必须靠 IDE 才能做的事，AI 将会做得更好。未来真正不确定的也不是 TypeScript 语言本身（它仍沿着 JavaScript 标准化路径演进），而是工具形态：AI 正从 IDE 的助手变成主要工作者，人类转向监督与审阅；这也是为什么把语言服务接入 MCP 这类机制会突然变得诱人——让 AI 能提出语义级问题、发起重构请求，用“智能体方式”完成过去只能在 IDE 里完成的工作流，开发工具将因此被彻底改写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。找到那些无聊却昂贵的事情，把它们交给 AI。工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;不如直接修 JS：TypeScript 的顿悟时刻&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从 Turbo Pascal、Delphi、C# 到如今的 TypeScript，你的工作塑造了数以百万计开发者每天写代码的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我第一次接触计算机大概是在高中时期，那是 70 年代末，我很快对编程产生了浓厚兴趣。后来，随着 8 位微型计算机开始出现，我决定自己动手组装一台套件机，并为它编写大量软件。我发现自己在这方面做得相当不错，而且也真的很享受这个过程。那时无论是结构化编程还是汇编语言，对我来说都不成问题。当然，还要考虑一个现实条件：只有 64K 内存，能塞进去的东西非常有限，还得给用户留出空间，所以当时一切都还能装在脑子里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 在很大程度上革新了开发者体验，核心在于缩短开发者的反馈回路。这在多大程度上是你一开始就有意识的设计理念？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：首先，人们之所以喜欢早期机器上的 BASIC，很大原因正是它的短反馈周期。BASIC 是解释型语言，输入代码就能立刻运行，但代价是运行速度慢，而且编辑器是基于行的，体验很糟。相比之下，当时的文字处理软件已经是所见即所得的屏幕编辑器，可以自由移动光标，这显然更适合写代码。与此同时，“输入—运行—立刻看到结果”的模式又非常吸引人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要在编译型语言中实现这一点并获得性能，就必须有极快的编译器。Turbo Pascal 的做法是：你一按下运行，它立即在内存中完成编译，甚至不需要访问磁盘，然后直接运行。如果出现错误，就立刻回到编辑器。编译器本身非常原始，你甚至需要通过出错地址反推源码位置。但正因如此，突然之间就获得了一种高度交互的体验，在当时堪称革命性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：那种“编译要等一个下午”的体验，会不会让你感到沮丧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：当然会，没有人喜欢等待。代码已经写完了，你只想立刻运行，而不是坐在那里干等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 还有一个重要影响，就是以低价让更多人接触到编程。回头看，这一点你有什么感受？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这里有个有意思的故事。Turbo Pascal 的前身叫 Poly Pascal，是我在丹麦一家小型软件公司里开发的 Pascal 编译器。后来我们联系上了 Borland 的创始团队，他们看过之后觉得非常惊艳，提议一起把它作为产品推向美国市场。然后他们决定定价 49.95 美元。我当时的反应是：“你们疯了吗？这样根本赚不到钱。”事后看来，这个决定非常聪明。虽然价格只有原来的十分之一，但销量却高出了三到四个数量级。最终结果非常成功，这个功劳主要要归于 Borland 的创始人Philippe。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Delphi标志着你从独立创作者向团队领导者的转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：一开始我基本上是单打独斗，虽然在丹麦的 Borland 办公室也会和两三个人合作，但随着机器性能飞速提升、用户期望不断提高，这种模式显然无法持续。我必须学会团队协作，这在 Turbo Pascal 期间就已经开始，而在 Delphi 项目中尤为明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你必须接受事情不会完全按照你个人偏好的方式完成，代码也不一定长成你心目中的样子。而且你往往没有时间亲自去“修正”这些细节，何况那样做也未必真的改变产品行为，更重要的是学会放权。只有当团队成员在各自负责的功能和模块中感到被信任、被赋权，团队才能真正运转起来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你在 Microsoft 参与 Visual J++ 的经历，对 C# 和 .NET 平台的目标产生了怎样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我是在 1996 年底加入 Microsoft 的，担任 Java 开发工具集的首席架构师。我们刚发布了 Visual J++ 1，本质上只是把 C++ 的 IDE 换成 Java 编译器，谈不上真正的集成，更没有快速的应用开发体验。于是我们着手改进，这最终成为 Visual J++ 6.0，并开始与 Visual Basic、Visual Studio 的版本体系对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要为 Microsoft 的 DOS 和 Windows 平台做 Java，就必须与这些环境高度互操作。但 Java 当时强调“一次编写，到处运行”，强制使用最小公分母的 UI 接口，最终只能做出体验很差的小程序。我们不得不引入一些扩展，简化与原生平台的互操作，并构建封装 Windows UI 的类库。很快就发现，这条路走不通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果真正为用户着想，就必须允许构建针对特定环境的最佳方案。人们既想要 Visual Basic 的易用性，又想要 C++ 的表达能力。于是我们尝试把这两者结合起来，并构建在 .NET 这样一个可持续演进的平台之上，最大限度地利用用户所运行的系统能力，这正是整个构想的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你既在谈不同类型的用户，又在描述为整个生态系统服务的思路，这种整体视角是如何形成的？&lt;/p&gt;&lt;p&gt;Anders：用户并不在乎这是语言特性、框架特性、平台能力，还是编辑器或调试器的问题，对他们来说，一切加在一起才是“体验”。因此，这些部分必须协同设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们构建了运行时、JIT 编译器、垃圾回收器，设计了平台的字节码，开发了类库。我既参与语言设计，也参与类库和运行时的设计，与负责这些组件的工程师密切合作，最终效果因此更好。否则，各自为政会形成孤岛，最后只能靠复杂的互操作层勉强拼接，结果自然谈不上“最佳实践”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，我们也不惧怕与底层平台深度互操作。过于教条地坚持最小公分母，拒绝利用具体平台的优势，这样永远不可能做到真正意义上的“最佳体验”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当时是否有某个“顿悟时刻”，让你意识到 JavaScript 在规模化发展中的阵痛已经成为必须解决的问题，而且这是一个需要由你、由 Microsoft 来解决的问题？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：行业里的运行时环境开始变得足够成熟，比如 Google 在 V8 上做了非常出色的工作，JavaScript 的运行性能突然变得相当可观。HTML5 也正式定稿，UI 能力大幅提升。同时，手机、iPad 等各种形态的设备出现了，而它们并不运行 Windows。整个行业突然意识到，真正的平台竞争不在 Java，而在 JavaScript、浏览器和 HTML 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，人们开始编写越来越庞大的应用，因为新的运行时和 UI 技术已经允许这样做。但很快大家就发现，在一种动态语言里、缺乏成熟工具支持的情况下，这件事难得令人发指，于是我们看到了各种奇怪的扭曲做法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中一个典型案例是 Outlook.com 团队找到我们，主要是 .NET 和 C# 团队，询问是否可以把他们内部的一个叫 Script# 的东西产品化。我当时一头雾水：“Script# 是什么？”他们说，这是一个允许你用 C# 编写代码，然后编译成 JavaScript 来运行的工具。我第一反应是：这听起来像是两边的缺点都占全了。&lt;/p&gt;&lt;p&gt;但事实是，这么做的真正原因在于可以获得“成熟的工具链”：类型检查、团队协作能力、接口定义，以及对模块之间交互方式的清晰描述。因为他们有成百上千名程序员参与开发，不可能只靠裸写 JavaScript，在没有检查、没有自动补全、没有重构支持的情况下完成工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我开始反思：JavaScript 真的已经糟糕到这种程度了吗？而“先写另一门语言，再把 JavaScript 当成中间表示或字节码来编译”，真的是解决问题的最佳方式吗？如果能直接修复 JavaScript 本身，会发生什么？于是我们开始探索这条路，事实证明，这个方向效果相当不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JavaScript 单线程的天花板：我们在浪费 90% 的算力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：TypeScript 被设计成 JavaScript 的严格超集，这背后显然有深思熟虑的战略考量。你是如何坚持这一决策的？这又能给其他语言设计者哪些启示？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：每当有人跑来跟我说：“我在考虑做一门全新的编程语言，能解决这个、解决那个”，我给出的第一条建议通常是：这个世界对新编程语言的需求，就像对头上再多一个洞的需求一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二条建议是：如果你真的要做一门语言，要意识到其中 90% 的工作，和其他所有语言是完全一样的，而且这个比例还在不断上升。如今，程序员的期望早已不只是一个编译器，你还需要完善的语言服务，能够集成到几乎所有主流 IDE 中，需要能与 AI 交互的 MCP 服务器，需要调试器、性能分析工具……&lt;/p&gt;&lt;p&gt;此外，你还得有至少十年的时间，因为一门语言真正站稳脚跟、获得有意义的用户规模，往往就是这么长的周期。没有人会在一开始就拥抱一门全新的语言，头五年你很可能用户寥寥，还得不断回答“我们真的要继续投入吗？”这是一门非常艰难的生意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。在 Visual Studio Code 里，我们的语言服务对 TypeScript 和 JavaScript 是同一套。对我们而言，JavaScript 只是没有类型注解的 TypeScript，或者使用 JSDoc 注解的另一种形式。这意味着我们不是在做两套东西，而是在同一项投入之上，精确地构建真正必要的能力，只为让整个生态系统变得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：2012 年在 GitHub 上启动 TypeScript 的开源项目，在当时的 Microsoft 可以说是一次相当激进的举动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我们当时对 JavaScript 生态的运作方式、价值观以及参与社区所需的前提，其实有着非常清晰的认识。大家都明白，如果不开源，这个社区根本不会理你，一个封闭的商业产品对他们毫无吸引力。因此，从一开始我们就主张开源。同时，Microsoft 内部也逐渐意识到，开源并不是“洪水猛兽”，而是如果想真正与开发者对话，就必须拥抱的现实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你刚才提到 2012 年发布在 GitHub，其实并不准确。2012 年我们发布在 CodePlex——Microsoft 自家的、并不太受欢迎的开源平台上。结果就是，反响寥寥。那时所谓的“开源”，更多只是把代码丢到仓库里，让大家提 issue，然后我们再把这些 issue 抓回内部系统，按内部流程处理。某种程度上，这也解释了为什么几乎没人关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再加上当时 Microsoft 在开源社区中的信誉并不高。真正的“主战场”在 GitHub。于是 2014 年我们迁移到了 GitHub，全面采用开放式开发流程。内部成员和外部贡献者遵循完全相同的规则，所有功能都通过 Pull Request 提交，所有讨论都公开进行。直到那时，项目才真正开始起飞，社区的兴趣也随之而来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从“把代码扔给社区”到真正的开放式开发，你们学到了哪些经验？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这是一个彻头彻尾的双赢。对用户来说，他们能看到“香肠是怎么做出来的”，所有讨论都在公开的 issue 里完成，而不是私下做决定后再给出一个结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对项目开发者而言，这种方式同样令人满足。你每天都能感受到社区的参与和认可，看到点赞、看到讨论，远比关起门来做六个月或一年，然后祈祷产品方向正确要有趣得多。在这里，用户每天都在用投票告诉你他们最想要什么功能，我们只需按票数排序，就能清楚地知道优先级。你解决这些问题，社区的热情就会进一步增强，形成正向循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：把 TypeScript 编译器迁移到 Go 背后的动机、权衡以及所面临的挑战是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：TypeScript 从一开始就是自托管的，用它自身编写，这意味着编译器和整个工具链本质上都是一个 JavaScript 应用。这带来了很多好处，比如你甚至可以在浏览器里运行编译器，在浏览器中构建一个完整的 IDE，一切都能正常工作。但随着 TypeScript 的广泛采用，以及用户项目规模不断扩大，可扩展性逐渐成为头号问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里有 JavaScript 本身的一些内在限制。它在设计上是单线程的，不支持共享内存并发，这意味着你实际上浪费了 90% 的计算能力。此外，JavaScript 的执行成本也很高，它的对象模型极为宽松，可以随意给对象加属性，这使得优化非常困难，底层往往演变成复杂的哈希查找和缓存机制，远不如原生代码高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们当时就清楚，自己正接二连三地白白损失收益。尽管放弃自托管让人非常不舍，但即便用尽所有优化技巧，性能瓶颈依然无法突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，在 2024 年夏天，我们开始做原型验证，先从扫描器、解析器这些容易量化的模块入手。很快就发现，性能提升可以达到 10 倍：一半来自原生代码，一半来自共享内存并发。这样的提升能把原本需要几分钟的事情缩短到十几秒，完全改变了游戏规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也很清楚不想从零重写一个全新的编译器。社区里有不少声音主张“用 Rust 全部重写”，但我们认为这并不可行。TypeScript 的类型检查器极其庞大复杂，许多行为只体现在现有代码的精确语义中。如果重写，就会陷入永无止境的差异追赶，最终无法与旧编译器对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此我们的目标是“迁移”，逐函数地把同样的逻辑搬到原生语言中。当然，这意味着必须重构数据结构，因为原生语言不允许像 JavaScript 那样随意给对象加属性。我们尝试过多种语言，很快排除了 Rust，因为我们的编译器充满了循环数据结构，并且高度依赖自动垃圾回收，使用 Rust 等同于重写。&lt;/p&gt;&lt;p&gt;最终我们选择了 Go。它在很多方面与 JavaScript 相似，这个选择对我们来说非常奏效。现在我们拥有了一个原生编译器，在功能上几乎是旧编译器的拷贝，连那些小怪癖都一模一样，只是快了 10 倍，这意味着社区不需要推倒重来。当我们正式切换时，我相信大家会非常满意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最适合 AI 的语言”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：在 AI 辅助编程的背景下，你认为 TypeScript 为什么特别适合 AI 工作流？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：很多人问我，为什么不干脆设计一门“最适合 AI 的完美编程语言”。我的回答通常是：那样的语言反而会成为最不适合 AI 的语言，因为它不会出现在 AI 的训练数据中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 在某种语言中写代码的能力，与它见过这种语言代码的数量几乎成正比，它本质上是在大量样本的基础上进行再组合和外推。AI 已经见过海量的 JavaScript、Python 和 TypeScript，因此在这些语言上表现得非常好。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 的确正在改变我们构建产品的方式。以这次 TypeScript 迁移为例，我们也尝试过用 AI 自动完成代码迁移，但效果并不好。一方面那是一年前的 AI，能力还有限；另一方面，迁移五十万行代码并保证行为与原代码完全一致，我们需要的是极其确定性的结果。AI 在翻译过程中可能会产生细微的“幻觉”，而你又不得不逐行检查，这并不划算。在这种情况下，更好的方式或许是让 AI 帮你生成“迁移工具”，而不是直接迁移代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 项目中有很大一部分是语言服务，为 IDE 提供补全、跳转、重构和快速修复。现在我们也在思考：既然 AI 已经能在很多场景下做得更好，是否还有必要原样迁移这些功能？类型检查器我们完整迁移了，但语言服务正在被大幅度重塑，以适应一个“AI 已经无处不在”的新环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你如何看待 AI 工具正在改变编程本身，以及语言设计的方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在理想状态下，AI 应该帮助我们消除编程中那些繁琐、重复的劳动。以 TypeScript 的迁移为例，在我们开发新代码库的同时，旧代码库里仍然不断有新的 Pull Request 出现，我们已经相当成功地用 AI 把这些变更迁移过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再比如，项目里有成千上万条 issue，其中很多非常古老。它们是否仍然可复现？是否还相关？能否根据社区反馈排序？这些清理和维护工作过去总是被一拖再拖，最终却成为拖慢项目前进的“锚”。现在，我们可以构建 AI 机器人来完成这些工作。这是我认为非常重要的一点：找到那些无聊却昂贵的事情，把它们交给 AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这也带来新的困惑。如果 AI 取代了初级工程师，我们又该如何培养资深工程师？难道指望 AI 自己成长为“高级程序员”，而人类程序员逐渐消失吗？我并不这么认为。我们似乎正在逼近某种上限：AI 能做很多事，但仍然需要人类以某种监督者的角色参与其中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见的是，工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果把时间拉长到未来五到十年，在一个更加 AI 原生的世界里，你认为 TypeScript 作为一门编程语言会如何演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我认为它的演进方向其实相当清晰。JavaScript 已经不再是一门年轻的语言，它有一套成熟的标准化流程，而我们也深度参与其中。TypeScript 会沿着 JavaScript 的标准化路径继续发展，同时在其之上补充必要的类型系统能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正充满不确定性的，是工具层面的变化。谁能想到，随着“对话式编程”这类形态的出现，命令行工具又重新变得如此重要？过去，AI 更多是作为助手存在：开发者在 IDE 里，AI 帮你更快地输入和补全代码。但现在，这种关系正在反转。AI 开始承担主要工作，而人类转向监督和审阅。此时，AI 并不一定需要我们传统意义上的 IDE，尽管它仍然需要语言服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么 MCP 这类技术开始变得有吸引力：通过把语言服务接入 MCP，让 AI 能够提出语义级的问题、重构请求等，并在一定的确定性边界内完成工作流。这本质上是在用 LLM 或 Agent 的方式，完成过去只能在 IDE 中完成的事情，这将深刻改变开发工具的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：围绕 TypeScript 或你们的工作，有没有哪些你希望公开澄清的误解？或者哪些你觉得被社区忽视、但其实很重要的事情？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在开源领域，我们与社区的距离非常近。如果哪里不对劲、存在摩擦，几乎会第一时间被反馈出来。比如这次转向原生代码的决定，确实引发了不少争议，有人认为我们应该选择另一种编程语言。但我始终坚信，我们为这个目标选对了工具。过去一年里，这个决定的成果已经逐步显现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，开源本身就是一种微妙的平衡。一方面，你在无偿地把成果交给世界；另一方面，这个项目往往由一家商业公司资助，而公司必须以某种方式生存下去。总得有人支付账单。因此，我们团队始终处在一种张力之中：如何让开源项目既符合社区期待，又与公司使命保持一致。这并不是 TypeScript 独有的问题，而是当今几乎所有开源项目都面临的现实。至于是否存在一种更好的激励机制来回馈长期投入的人，目前还没有答案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：开源的可持续性，确实是一个反复被提起的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：大量企业的正常运转，事实上依赖于那些支撑其后端的开源项目得到持续维护。但现实中，很多人对开源的态度仍然是“索取多于付出”。在微软，我们至少在努力以一种更真诚的方式参与其中。仅 TypeScript 项目，就已经累计投入了数百人数年的工作量；而 Visual Studio Code，投入甚至可能达到上千人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果不算你自己参与创造的语言，你最敬佩、最尊重哪一门编程语言？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：任何一门能被程序员广泛使用、甚至能进入讨论范围的编程语言，都一定有其可取之处，因此都值得尊重，我深知一门语言要走到这一步有多难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Rust 为例，我非常敬佩它通过借用检查器来探索一种不同的内存管理方式，试图在不依赖昂贵自动垃圾回收的情况下保证安全性。我也很尊重 Go，尽管它在设计上有些“怪”，常被认为不太正统，但它实际上提供了一种简单、内存安全、类型安全的“现代 C”的思路。至于 Python，它的成功不需要多说，它驱动着 AI 和机器学习的发展，令人难以不心生敬意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从语言设计者的角度看，我们都站在前人的肩膀之上。如果设计一门语言却不向其他语言学习，那是愚蠢的。经验与智慧就在那里，理应被尊重和继承。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当你放眼整个以 GitHub 等协作平台为中心的软件开发生态时，是什么让你对未来保持乐观？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：仅仅是它依然存在这一事实，就足以让我感到乐观。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，其中也有大量噪音，有不少项目缺乏维护，但不可否认的是，这些代码记录了软件演化的全过程。以我们为例，转向这种协作工作流后，十二年的历史都被完整地保存在那里，可搜索、可追溯。如果我记得某个问题曾被讨论过，只需要去查找，而不必面对一封再也找不到的旧邮件，这种价值是巨大的。正因如此，我由衷地高兴看到它仍在持续增长，并顽强地存活下来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=uMqx8NNT4xY&lt;/p&gt;&lt;p&gt;https://devclass.com/2026/01/28/typescript-inventor-anders-hejlsberg-ai-is-a-big-regurgitator-of-stuff-someone-has-done/&lt;/p&gt;</description><link>https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</link><guid isPermaLink="false">https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</guid><pubDate>Fri, 06 Feb 2026 08:23:20 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>奥特曼重磅发声：全AI公司是未来！OpenAI官宣Frontier，让管理Agent像管人一样简单</title><description>&lt;p&gt;在OpenAI与Anthropic对轰AI Coding新产品，争夺编程王座之际，Open AI偷偷放大招，又推出智能体中枢平台 Frontier。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，Frontier 就是一个把智能体当成 AI 员工来管理的企业级平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c71db6c1f404abbbf9919be21668a6a9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年，智能体开始从“陪聊工具”走向企业一线业务，但一个关键问题成为不少企业的烦恼，即智能体越多，系统反而越复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在不少企业内部，云平台、数据系统和应用长期割裂，智能体被零散地塞进各个业务场景。每一个智能体都像一座信息孤岛，权限受限、上下文缺失。伴随智能体数量的暴增，带来的往往不是效率提升，而是运维、治理和协同成本的持续叠加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在这一背景下，Frontier应运而生。它将企业内部分散的系统与数据整合在一起，通过构建统一的业务上下文，提供一套端到端的方法，覆盖智能体的构建、部署与管理流程，让智能体能够真正进入生产环境稳定运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2月4日思科AI峰会上，Open AI CEO奥特曼曾激进发言，不能快速用上 AI 员工的公司，会被甩在后面。他甚至提出“全 AI 公司”的概念，未来或许每个流程、每个环节，AI都能真正参与进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，正是OpenAI 对企业级市场的提前卡位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据最新数据显示，Anthropic在企业级大模型市场占据了40%的惊人份额，稳坐第一把交椅，远超OpenAI的27%和谷歌的21%。随着大模型逐步进入真实业务流程，企业级场景正成为决定长期竞争格局的关键阵地，OpenAI 显然不希望在这一入口层面处于被动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 的目标已不再局限于打造“更聪明的模型”，而是试图通过基础设施，让各种智能体优先部署在自家平台之上，包括竞争对手的产品，从而将更多企业用户纳入其整体 AI 生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 OpenAI 官方披露，过去几年，已有超过 100 万家企业在使用 AI 提升效率。比如一家大型制造企业借助智能体，将原本需要六周完成的工作压缩到一天；另一家全球投资公司通过智能体优化销售流程，为销售人员释放出 90% 以上 的时间；还有一家大型能源生产商利用智能体提升 5% 的产量，额外创造了 超过10亿美元的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，能否在组织内部高效使用智能体，正在成为企业之间拉开差距的关键变量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，从制造业到互联网、从金融到生命科学，已有多家行业巨头率先试用 Frontier，包括 惠普、Intuit、甲骨文、州立农业保险、赛默飞世尔和优步。此外，BBVA、Cisco、T-Mobile 等数十家现有客户也已参与试点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台仍处于有限开放阶段，仅向少量客户开放体验，预计将在未来几个月逐步扩大范围，具体定价方案尚未披露。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Frontier平台的四大板块：上下文、执行环境、评估学习与安全管理&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更好地理解 Frontier，可以把它类比为一家公司的 “AI员工管理体系”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的作用不仅是要让AI员工了解公司是如何运作的，还要为其提供跨部门协作的能力、必要的资源支持，以及清晰的权限边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一目标，Frontier 将企业级 AI 智能体的运行拆解为四个关键模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（1）共享业务上下文：让 AI “知道公司怎么运作”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让 AI 真正参与企业工作，第一步不是分配任务，而是让它理解企业本身的运作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier 做的第一件事，就是构建一个共享的业务上下文环境。通过打通企业内部长期割裂的系统，包括 CRM、数据仓库、工单系统以及各类内部应用，将原本分散在不同系统中的业务信息连接起来，形成一个统一的“语义层”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，智能体就能理解信息如何在企业内部流动、关键决策发生在什么环节、哪些指标才是真正重要的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（2）提供执行环境：让 AI 不只会想，还能真的“干活”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在理解业务之上，Frontier 为 AI 员工提供了一个开放且可靠的执行环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可以打开和使用企业内部的各种工具，自己写代码处理数据，整理和生成文件，并在不同系统之间来回切换，把一整套原本需要人反复操作的流程跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对企业来说，这相当于把 AI 从“问答工具”，升级成了能独立完成任务的AI同事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（3）学习与评估：让 AI 在“反思”中不断优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了让AI像人一样能不断优化，自我迭代，Frontier 内置了绩效评估和优化机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套机制能够持续监控 AI 代理在实际任务中的表现，包括任务完成情况、错误率、资源消耗等关键指标，而人变成了监督者，可以清楚地看到哪些行为有效、哪些无效，并据此调整规则和流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/08b3e13c61a876c0825f2bced707b72e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着运行时间的增长，AI 智能体会逐步积累“记忆”，将过往交互转化为有用的上下文信息，从而不断优化自身表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（4）安全保障：让 AI 在清晰边界内工作&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了防止AI乱操作，Frontier 为每一个AI员工设立严格工作边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体能进哪些系统、能做哪些操作、权限到哪一步，都提前规定好。这样一来，AI 就只能在允许的范围内工作，不会乱动数据、越权操作，也不会给公司带来额外风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64d17bb0477c5cd67960a86f6681af37.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样一整套系统化设计下，Frontier 补齐了 AI 进入公司所需的基础设施。既给予足够的灵活性，又保留必要的安全和控制，使智能体能够真正融入企业的日常工作流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Frontier平台上，公司可以创建多个 AI 员工，也可以混合其他厂商的智能体或自行开发的 AI 服务。Frontier 的核心作用是公司可以通过统一的仪表盘，查看每个 AI 员工的任务完成情况、资源消耗和错误率等关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着企业部署 AI 的方式，正在从过去的“定制化开发”，转向“标准化配置”，让部署智能体更便捷易操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier已经在不少关键行业发挥价值。比如银行用它做AI后台，处理每年数亿的需求事件；制造业公司，靠它模拟生产流程、规划产能布局，节省了数十亿美元成本；在生命科学领域，这套系统用来优化全球监管流程，给药品审批这类关键环节兜底。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 OpenAI 应用业务首席执行官 Fidji Simo 所言：“到今年年底，领先企业中的大多数数字化工作，都将由人类进行决策和指挥，并由成群的 AI 代理来执行。这种模式已经在编程领域成立，并且很快会扩展到更多业务场景。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的上限，或许是全AI公司&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的推出，其实早就埋在奥特曼对人工智能未来走向的一系列判断之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在今年的思科 AI 峰会上，奥特曼认为Codex的诞生，是又一个 “ChatGPT 时刻”。那是他第一次真切地意识到，AI 可以被当作一名同事，而不只是工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e91d0e6bd4f4a581177927b1e1c6c8a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他讲了一个细节，刚安装 Codex 时，他绝不会在不检查的情况下，让它完全控制自己的电脑。但这个坚持只维持了两个小时，因为Codex 实在太好用了，而且这种“好用”已经不再局限于写代码本身，而是扩展到了整个工作的执行过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在奥特曼看来，让智能体能 “像人一样用电脑”，真正接管电脑和浏览器，才能把生产力彻底解放出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码能力加上通用电脑操作能力，这种结合的趋势几乎挡不住。他甚至想得更远：AI 的终极形态，说不定是 “全 AI 公司”，让智能体直接对接现实系统，从头到尾把一家企业跑起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然目前AI可以做的事已经很多，但真正被组织吸收和使用的比例依然很低。技术的演进速度，远远快于企业部署和消化的能力。这背后的原因是企业部署 AI 成本高，缺乏系统化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 最强大的地方之一，是“始终在线”的计算能力。但现有的硬件、权限系统、法律体系，都不是为这种情况设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;奥特曼曾将企业真正需要的形态概括为一种“AI 云平台”：它负责处理安全问题，管理业务上下文，协调和运行大量智能体，支持多模型协作，并提供完整的企业级授权与接口体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;企业级应用已经成为了OpenAI在2026 年明确的重点方向之一，而 Frontier，正是OpenAI 交出来的企业级解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种判断并非 OpenAI 一家的独断。去年 12 月，全球研究与咨询公司 Gartner 在一份报告中指出，代理管理平台既可能成为“人工智能领域最有价值的资产”，也是企业大规模采用 AI 所必需的基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，或许正在拉开 “AI 全面扎根企业核心业务” 时代的序幕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://openai.com/index/introducing-openai-frontier/&lt;/p&gt;&lt;p&gt;https://openai.com/business/frontier/&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=YO2PVbtpb_A&lt;/p&gt;</description><link>https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</link><guid isPermaLink="false">https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</guid><pubDate>Fri, 06 Feb 2026 07:45:00 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>Cloudera发布2026 AI与数据技术趋势预测：标准化、可控化趋势成企业主流选择</title><description>&lt;p&gt;过去两年，AI在中国经历了从概念热潮到密集试点的阶段。无论是大模型、智能体（Agentic AI），还是自动化应用，越来越多企业已完成初步探索。进入2026年，AI正迈入一个新的发展阶段——从试点应用走向业务规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业关注的核心问题也随之发生变化，不再只是“能否用AI”，而是AI是否能够在可控、可持续的前提下，稳定运行并转化为可衡量的业务成果。基于对中国企业AI实践的持续观察，Cloudera对2026年AI与数据技术的发展趋势做出如下判断：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测一：AI走向产业化，业务价值与可复制能力成为核心衡量标准&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，中国企业的AI应用将明显超越聊天机器人和单点工具，转向流程优化、运营自动化和行业级智能应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在制造、金融、电信等领域，企业将更倾向于复用已验证的AI能力，并通过智能体工作流将AI深度嵌入核心业务流程，而不再局限于单一模型或实验项目。ROI、业务效率提升和可持续运营能力，将取代模型参数与算力规模，成为衡量AI成功与否的关键指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，随着AI被视为重要的“新型生产力”，企业和行业客户将更加重视AI系统的稳定性、连续性与可运营性。能够在复杂环境中长期运行、不断优化并适应业务变化的AI平台，将在竞争中脱颖而出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放在一次公开分享中表示，AI技术的第一阶段是能力展示与智能回答等“噱头应用”，例如模型回答数学题能力等功能。然而，进入产业化落地后，企业对AI的关注点更多转向如何结合已有业务系统、优化流程并创造可衡量的商业价值。这与当前行业趋势高度一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a6452db58cb45fd3ef6c01ebe3574a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业数据显示，企业从单点AI尝鲜逐步转向系统化、流程化应用，特别是在流程优化、与数据平台整合等关键领域的能力要求急剧上升。此外，随着智能体（AI agents）出现，企业内部正在探索如何将模型能力系统性融入现有的业务逻辑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测二：可信、可治理的私有AI将成为企业的关键差异化能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在中国市场，数据安全与合规可控始终是AI应用的前提条件。2026年这一趋势将进一步强化。&lt;/p&gt;&lt;p&gt;虽然公有云与预训练模型极大降低了AI试验门槛，但在实际生产环境中，企业逐渐意识到：如果数据治理、访问控制和合规机制不到位，AI带来的效率提升，可能同时放大数据风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，越来越多中国企业将转向私有AI（Private AI） 路径：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在受治理的环境中部署和运行模型；数据不出域，权限可控、流程可追溯；通过检索增强生成（RAG）等方式，为模型提供业务上下文，同时保持数据可控；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放进一步指出，数据合规永远优先于AI功能本身。在涉及企业核心数据的训练过程中，如果使用公有云平台进行训练，不仅有可能触及竞争性泄露风险，还可能违反监管要求。因此，只要涉及企业敏感数据，私有化部署基本成为不可替代的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可信AI不再是“最佳实践”，而将成为企业实现AI规模化落地的基本门槛。治理能力与敏捷性不再是对立选项，而是AI成熟度的两个必要组成部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测三：本地化私有部署成为中国企业AI规模化落地的基础架构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在中国市场，2026年企业对AI与数据架构的判断将进一步趋于清晰：本地化私有部署是AI规模化落地的基础前提。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放强调，相较于公有云部署，私有化AI环境更能满足企业对可控性、数据安全和长期运营的核心诉求。在安全与合规成为企业AI战略基础的背景下，“可控”被视为AI落地的前提条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;行业调研报告显示，企业在AI部署中越来越倾向于选择私有化或混合云架构，以保障数据主权和业务独立性。IDC发布的《2025年中国企业AI大模型应用趋势报告》指出，约72%中大型企业在实施AI智能体时，将私有化部署置于优先考虑因素之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Rackspace发布的趋势分析，面向企业的私有云AI部署正在成为主流，其中检索增强生成（RAG）等敏感工作负载正从公有环境向私有部署迁移，以提升性能稳定性和数据控制能力。&lt;/p&gt;&lt;p&gt;相关行业观点也总结出几个核心趋势：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;私有化部署可提升响应速度并避免核心数据泄露风险；企业希望避免将敏感数据发送至外部AI平台，以控制数据流出风险；企业CIO和CTO在架构设计过程中，将合规与数据控制置于AI战略核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在金融、制造、能源、电信等关键行业，核心业务系统与数据资产长期运行在本地或私有环境中。这一架构形态，既源于对数据安全与合规可控的要求，也来自企业对系统稳定性、连续性与长期运营能力的现实考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI从试点走向生产级应用，企业开始更加关注一个根本问题：AI是否能够在本地私有环境中持续运行、不断优化，并稳定支撑核心业务。一次性部署或短期验证已无法满足需求，取而代之的是对平台级能力的要求，包括统一的数据管理、可治理的模型运行，以及对业务变化的长期适配能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，能够在本地私有架构下支撑AI持续演进的数据与AI平台，将成为中国企业实现AI规模化、可复制落地的重要基础。这一能力，也将成为衡量企业AI成熟度的关键标志。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera 成立于 2008 年，总部位于美国硅谷，是最早一批围绕 Hadoop 生态 成立的企业级大数据公司之一。公司创始团队中包括多位 Hadoop 核心贡献者，因此 Cloudera 在早期被广泛视为“企业级 Hadoop 的事实标准”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年，Cloudera 与另一家老牌大数据公司 Hortonworks 合并，形成当时全球最大的大数据平台厂商之一。合并后，Cloudera 的技术版图从单一的大数据存储与计算，扩展到 数据管理、数据治理、数据分析、机器学习与 AI 工程化 等完整链条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2026：AI从“概念热潮”走向“硬核成果”的一年&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年，中国AI的竞争焦点将不再是“谁的模型更大”，而是在可控、可信、可复制的基础上，真正把AI变成业务成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终胜出的企业，将是那些能够负责任地规模化AI、用数据治理支撑智能决策、用韧性架构保障长期运营的企业。因为真正可信的AI，始于可信的数据；而可信的数据，离不开稳健、可持续的数据基础架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放称，在AI实践中，企业真正关心的并非单一模型表现，而是整体平台建设后的长期运营能力。例如，在金融、制造等行业，已有大量的信息系统和数据资产，AI必须与这些系统无缝整合，才能真正提升业务效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，在人才流动频繁的市场环境下，构建松耦合体系架构被认为是确保AI平台可持续运营的关键。这种设计允许平台适应技术更新和人员变动，避免因关键人员离职而造成系统停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司援引自身服务的典型案例（如上汽大众的供产销数据平台与AI集成实践），强调企业在部署AI时，最终评估的核心是投入产出与长期收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</link><guid isPermaLink="false">https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</guid><pubDate>Fri, 06 Feb 2026 06:50:14 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Cloudflare自动化Salt配置管理调试，减少发布延迟</title><description>&lt;p&gt;Cloudflare最近&lt;a href=&quot;https://blog.cloudflare.com/finding-the-grain-of-sand-in-a-heap-of-salt/&quot;&gt;分享&lt;/a&gt;&quot;了他们是如何使用&lt;a href=&quot;https://saltproject.io/&quot;&gt;SaltStack&lt;/a&gt;&quot;（Salt）管理庞大的全球服务器集群的。在这篇博客文章中，他们讨论了解决“一粒沙（grain of sand）”问题所需的工程任务。它的关注点在于要从数百万次状态应用中找出某个配置错误。Cloudflare的&lt;a href=&quot;https://sre.google/&quot;&gt;站点可靠性工程（SRE）&lt;/a&gt;&quot;团队重新设计了其配置的可观测性，他们将故障与部署事件关联起来。这项工作将发布延迟减少了5%以上，并减少了手动分析问题相关的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为配置管理（configuration management，CM）的工具，Salt能够确保了跨数百个数据中心的数千台服务器保持在期望的状态。在Cloudflare的规模下，即使YAML文件中的一个微小语法错误或“Highstate”运行期间的瞬时网络故障，都可能阻碍软件发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare面临的主要问题是预期配置与实际系统状态之间的“偏离（drift）”。当Salt运行失败时，它影响的不仅仅是一台服务器，它可能会阻止在整个边缘网络中推出关键的安全补丁或性能特性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salt使用了带有&lt;a href=&quot;https://zeromq.org/&quot;&gt;ZeroMQ&lt;/a&gt;&quot;的&lt;a href=&quot;https://docs.saltproject.io/salt/install-guide/en/latest/topics/configure-master-minion.html&quot;&gt;主控/受控（master/minion）设置&lt;/a&gt;&quot;。这使得很难找出为什么特定的受控端（代理）没有向主控端报告状态，这简直就像大海捞针。Cloudflare总结了几个破坏此反馈循环的常见故障模式：&lt;/p&gt;&lt;p&gt;无声故障：受控端在状态应用期间可能会崩溃或挂起，导致主控端无限期地等待响应。资源耗尽：繁重的pillar数据（元数据）查找或复杂的Jinja2模板可能会使主控端的CPU或内存不堪重负，导致job丢失。依赖地狱：包状态可能会因为上游仓库无法访问而失败，但错误消息可能埋藏在数千行日志的深处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/74fe36ee46bcb4a7a61043c565e64040.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Salt的架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当发生错误时，SRE工程师必须手动通过SSH登录到候选受控端。他们会追踪主控端上的job ID，并筛选保留时间内有限的日志，然后尝试将错误与变更或环境条件联系起来。在拥有数千台机器和频繁提交代码的情况下，这个过程变得单调且难以维护。它提供的持久工程价值非常有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些挑战，Cloudflare的商业智能和SRE团队合作构建了一个新的内部框架。目标是为工程师提供一种“自助服务”机制，以识别跨服务器、数据中心和特定机器组的Salt故障的根本原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方案涉及从集中式日志收集转向更健壮的、事件驱动的数据摄入管道。这个在相关内部项目中被称为“Jetflow”的系统，允许将Salt事件与以下内容关联：&lt;/p&gt;&lt;p&gt;Git提交：识别配置仓库中触发故障的精确变更。外部服务故障：确定Salt失败是否实际上是由依赖项（如DNS故障或第三方API中断）引起的。临时（Ad-Hoc）发布：区分计划的全局更新和开发人员进行的手动更改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare通过改变管理基础设施故障的方式，为自动分类奠定了基础。系统现在可以自动标记特定的“一粒沙”，即导致发布阻塞的那一行代码或那一台服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从被动管理到主动管理的转变带来了以下成果：&lt;/p&gt;&lt;p&gt;发布延迟减少5%:：通过更快地暴露错误，缩短了从“代码完成”到“在边缘运行”的时间。减少琐事：SRE不再需要花费数小时进行“重复性分类”，使他们能够专注于更高层次的架构改进。改进的可审计性：现在每个配置变更都可以从Git PR到边缘服务器上的最终执行结果进行全生命周期追踪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare工程团队观察到，尽管Salt是一个强大的工具，但在“互联网规模”下管理它需要更智能的可观测性。通过将配置管理视为一个需要关联和自动分析的关键数据问题，他们为其他大型基础设施提供商树立了榜样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于Cloudflare在SaltStack上遇到的挑战，需要注意的是，像&lt;a href=&quot;https://docs.ansible.com/&quot;&gt;Ansible&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.puppet.com/&quot;&gt;Puppet&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.chef.io/&quot;&gt;Chef&lt;/a&gt;&quot;这样的替代配置管理工具，每个工具都有不同的架构权衡。Ansible使用SSH无代理的方式工作。这比Salt的主控/受控设置更简单。然而，由于顺序执行，它在大规模环境时可能会面临性能问题。Puppet使用基于拉取的模型，代理会与主控服务器进行核对。这提供了更加可预测的资源使用，但与Salt的推送模型相比，可能会减慢紧急变更的速度。Chef也使用代理，但侧重于使用其Ruby DSL的代码驱动方法。这为复杂任务提供了更大的灵活性，但学习曲线更陡峭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Cloudflare的规模下，任何工具都会遇到其自身的“一粒沙”问题。然而，关键教训很明确，那就是管理数千台服务器的任何系统都需要强大的可观测性。它还必须能够将故障与代码变更自动关联，并具备智能分类机制。这将手动侦探工作转化为可操作的洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-salt-configuration/&quot;&gt;Cloudflare Automates Salt Configuration Management Debugging, Reducing Release Delays&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</link><guid isPermaLink="false">https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</guid><pubDate>Fri, 06 Feb 2026 06:02:48 GMT</pubDate><author>作者：Claudio Masolo</author><category>云计算</category></item><item><title>Astro发布了版本6 Beta版，重新设计了开发服务器和一流的Cloudflare Workers</title><description>&lt;p&gt;&lt;a href=&quot;https://astro.build/&quot;&gt;Astro&lt;/a&gt;&quot;，一个用于构建内容驱动型网站的Web框架，已经宣布了&lt;a href=&quot;https://astro.build/blog/astro-6-beta/&quot;&gt;Astro 6 Beta&lt;/a&gt;&quot;版本，引入了一个完全重新设计的开发服务器、一流的Cloudflare Workers支持，以及几个新的稳定API，包括实时内容集合和内容安全策略支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6 Beta版对开发人员使用该框架的方式带来了重大改变，包括基于Vite的&lt;a href=&quot;https://vite.dev/guide/api-environment&quot;&gt;Environment API&lt;/a&gt;&quot;重构的开发服务器、用于实时数据更新的稳定实时内容集合，以及内置的CSP支持。该版本还包括一些重大的破坏性变更，如需要使用Node 22+并移除几个弃用的API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6中的一大特性是完全重新设计的 astro dev 开发服务器。新服务器利用Vite的Environment API在与生产环境相同的运行时中运行应用程序，缩小了开发和部署环境之间的差距。以前，在本地工作的代码一旦部署可能会有不同的行为，而且平台特定的特性通常在部署后才能测试。通过统一开发和生产代码路径，Astro团队已经发现并修复了许多仅存在于开发或仅存在于生产中的微妙错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新的开发服务器使之成为可能的最完整的例子是对Cloudflare Workers的支持。有了Astro 6 Beta， astro dev 现在可以使用workerd运行应用程序，这是Cloudflare的开源JavaScript运行时，这与在生产环境中支持Cloudflare Workers的运行时相同。这意味着开发者现在可以直接针对真实的平台API进行开发，而不是模拟或polyfills。当使用Cloudflare支持运行 astro dev 时，开发者现在可以访问Durable Objects、KV Namespaces、R2 Storage、Workers Analytics Engine和环境变量，所有这些都支持热模块替换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在可以直接使用 cloudflare:workers 模块访问Cloudflare绑定，如beta博客文章所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import { env } from &quot;cloudflare:workers&quot;; const kv = env.MY_KV_NAMESPACE; await kv.put(&quot;visits&quot;, &quot;1&quot;); const visits = await kv.get(&quot;visits&quot;);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Astro 5.10中还在试验性的实时内容集合，现在在Astro 6中已经稳定。这些建立在Astro的类型安全内容集合之上，可以实时更新数据，而不需要重新构建，这使得它们非常适合频繁更新数据源，如实时股票价格或库存。该API旨在让已经使用Astro的构建时内容集合的人感到熟悉，但对实时数据请求的实际情况进行了显式的异常处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;内容安全策略支持，之前在Astro 5.9中是实验性的，现在已经稳定。CSP是Astro获得最多投票的特性请求，它有助于保护网站免受跨站脚本和其他代码注入攻击。该功能在所有Astro渲染模式中工作，并与所有官方适配器兼容，自动生成CSP头或元元素，包括脚本和样式的哈希。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6包括几个重大破坏性变更，因为团队清理了弃用的API。最重要的变化包括移除 Astro.glob() ，要求Node 22或更高版本，以及更新Cloudflare适配器，移除 Astro.locals.runtime ，转而直接访问平台API。团队已经发布了&lt;a href=&quot;https://v6.docs.astro.build/en/guides/upgrade-to/v6/&quot;&gt;一个全面的升级指南&lt;/a&gt;&quot;，详细说明了每个破坏性变更的迁移步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在社区内引发了一些讨论，reddit上的一位用户对长长的破坏性变更列表发表了评论（特别提到了早期的alpha版本）：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇。真是一个巨大的破坏性变更列表……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引起了Astro核心维护者&lt;a href=&quot;https://github.com/sarah11918/sarah11918&quot;&gt;Sarah Rainsberger&lt;/a&gt;&quot;的回应：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数变更至少不会影响每个人！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;她继续解释了有这样一个详细的破坏性变更列表的理由：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;……我坚信，任何可能破坏某人项目的东西都应该包含在这一页上……无论那个“项目”是一个常规的静态网站，还是你构建的主题，或者一个复杂的集成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=46646645&quot;&gt;Hacker News&lt;/a&gt;&quot;上，评论者强调Astro是最早支持Cloudflare的Vite插件的框架之一：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Cloudflare发布了他们的vite插件，使得使用vite env API的框架可以毫不费力地在workerd中运行……Nextjs还没有支持，添加对Sveltekit支持的草案PR已经被搁置，直到下一个主要版本，Astro刚刚在他们3天前的beta 6.0版本中添加了支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与其他元框架如&lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt;&quot;和SvelteKit相比，Astro以其专注于内容驱动型网站和默认最小化客户端JavaScript而脱颖而出。Next.js强调React和全栈能力，&lt;a href=&quot;https://svelte.dev/docs/kit/introduction&quot;&gt;SvelteKit&lt;/a&gt;&quot;专注于Svelte生态系统，而Astro仍然与框架无关，通过其孤岛架构官方支持&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&quot;、&lt;a href=&quot;https://vuejs.org/&quot;&gt;Vue&lt;/a&gt;&quot;、&lt;a href=&quot;https://svelte.dev/&quot;&gt;Svelte&lt;/a&gt;&quot;和其他UI框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro是一个开源Web框架，旨在构建包括博客、营销网站和电子商务在内的内容驱动型网站。该框架通过最小化客户端JavaScript，尽可能在构建时或按需在服务器上渲染内容，强调性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&quot;&gt;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</link><guid isPermaLink="false">https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</guid><pubDate>Fri, 06 Feb 2026 06:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>别再手动拼凑 Data Pipeline 了！这个新平台想让你彻底告别 Iceberg 运维噩梦</title><description>&lt;p&gt;近日，Etleap 正式发布了 &lt;a href=&quot;https://etleap.com/&quot;&gt;Iceberg pipeline platform&lt;/a&gt;&quot;。作为一套全新的托管式数据流水线方案，该平台的核心价值在于：让企业摆脱繁琐的自定义技术栈开发与维护，实现 Apache Iceberg 架构的“无感切换”。它将数据摄取、转换、编排及表操作深度集成，且全量部署在客户自有的 VPC 环境内。对数据团队而言，这相当于获得了一个“开箱即用”的生产级底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一举措直击数据平台负责人们日益增长的痛点：尽管 Iceberg 已成为现代数据湖和湖仓一体化架构中极受欢迎的表格式，但它本身并不提供日常运行所需的流水线。因此，企业往往不得不将各种摄取工具、dbt 任务、调度器以及定制的维护脚本拼凑在一起。Etleap 表示，这种碎片化的方案不仅构建成本高昂，且难以在大规模环境下稳定运行，更分散了团队提炼业务价值的精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Iceberg 能为企业带来巨大的收益，但在实践中，这需要一套围绕它构建的托管流水线系统来变现，”Etleap 的首席执行官兼创始人 Christian Romming 表示，“我们的 Iceberg Pipeline 平台正是为了满足这一需求而生，让数据平台团队无需构建和运行自定义流水线堆栈，即可拥抱 Iceberg。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 的平台用一套原生的 Iceberg 系统取代了以往“缝补拼接”的模式。它将数据摄取、建模、编排及表生命周期管理整合进一个协同层，同时保持在客户自有云环境内的完全隔离。通过这种方式，它在满足企业级治理和安全要求的同时，消除了对独立控制平面或外部基础设施的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了简化运维流程，该平台还致力于将 Iceberg 接入更广泛的数据生态系统。团队只需构建一次流水线，即可在分析、数据科学、AI 工作负载及数据共享场景中重复调用相同的 Iceberg 表。这不仅减少了数据冗余，提高了数据一致性，还实现了跨云平台和计算引擎的工作负载可移植性，且无需牺牲性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 表示，Iceberg Pipeline 平台目前已正式上线，并已有部分客户在进行大规模的流水线运行。该公司将此次发布定位为企业将 Iceberg 打造为真正数据基座的捷径，旨在消除传统上阻碍技术落地的运维负担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前 Iceberg 版本的发布尚处于早期阶段，该平台能否兑现 Etleap 所承诺的愿景仍有待观察。除了各大媒体的发布新闻外，目前尚未收到来自用户的实际使用反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/etleap-iceberg-pipeline-platform/&lt;/p&gt;</description><link>https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</link><guid isPermaLink="false">https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</guid><pubDate>Fri, 06 Feb 2026 05:02:22 GMT</pubDate><author>作者：Craig Risi</author><category>数据湖仓</category></item><item><title>Vibe Coding“血洗”开源，社区吵翻了：封杀菜鸡AI开发者？不如给维护者打钱！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;氛围编码（Vibe coding）是否会摧毁开源生态系统？近日，多位知名研究人员在一篇预印本论文中指出，从观测到的趋势及部分建模结果来看，情况可能确实如此。他们的警告主要集中在两方面：用户互动逐渐从开源项目中剥离，同时启动一个新开源项目的难度大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是热门开源项目，随着代码下载和文档查阅的需求被大语言模型聊天机器人的交互所替代，其官网的访问量也出现下滑，项目商业规划推广、赞助募资和社区论坛运营的可能性也降低了。Stack Overflow等社区论坛使用量的骤减也反映了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55ac5d403610db635de151d6b2e253c5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究人员们最后的结论是：在氛围编码广泛应用的情况下，要维持开源软件目前的规模，就需要对维护者的报酬方式进行重大改革。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“AI革命”or人类智能的压力测试&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果把“AI辅助”软件开发的这种效果理解为将实际的工程和开发工作委托给大语言模型的统计模型，那么问题就显而易见了。氛围编码这一模式摒弃了开源社区中对类库和工具的自然筛选机制，几乎可以确定的是，大语言模型的统计模型在生成输出内容时，必然只会选用其训练数据集中占比最高的技术依赖方案。并且，大语言模型既不会与库或工具的开发者互动，也不会提交可用的错误报告，更不会意识到任何潜在问题，无论这些问题的文档记录多么完善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自从微软在 2021 年推出 GitHub Copilot以来，这便是一个极具争议的话题。2024 年有一些研究报告指出，使用 Copilot 和类似的聊天机器人进行氛围编码并没有带来任何实际好处，除非增加 41% 的 bug 也被视为成功的标准。到 2025 年，负面情绪愈发浓烈，大语言模型聊天机器人普遍被指责会降低使用者的认知能力，氛围编码会降低 19% 的开发效率，就连尝试过这类工具的资深开发者，也在言辞犀利的评测中对其全盘否定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是当下，软件开发领域也已显现出“AI垃圾”带来的诸多负面影响。cURL 项目的作者 Daniel Stenberg 多次抱怨，由于大语言模型引发的“AI 垃圾”，导致提交的错误报告质量日益下降。如今，该项目已决定从 2026 年 2 月 1 日起暂停其漏洞赏金计划。也有网友指出，“AI最不靠谱的地方在于那些简单的重复性任务，因为它经常会随机出错。对它的要求越多，它就越容易出错，导致你需要逐行检查整个程序，确保它执行了要求的操作。使用大语言模型时最糟糕的做法是让它‘把这段代码清理干净，但不要改变任何功能或逻辑’，它绝对会起到相反的效果。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有这些现象似乎都在强化这样一种观点：“AI革命”或许更像是对人类智能的一次压力测试，而非真正提升开发效率或代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前尚不清楚氛围编码的影响究竟有多大，但像JavaScript、Python和各类Web技术相关的软件生态系统很可能首当其冲地受到其冲击，因为它们的用户群体似乎对这种开发模式的接受度更高，且相关技术在大语言模型的训练数据集中占比也最大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开源维护者们福利大降，要没钱赚了？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，在氛围编码的相关补偿机制下，绝大多数开源项目都难以从中获益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文指出，氛围编码降低了软件制作成本，但也改变了用户与软件生态系统的交互方式。在传统的开源软件商业模式下，开发者会选择软件包、阅读文档，并与维护者及其他用户交流。而在氛围编码模式下，AI智能体可以端到端地选择、组合和修改软件包，人类开发者可能并不清楚使用了哪些上游组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变将引发一个关于开源软件可持续性的均衡问题：一旦开发者的加入和选择机制调整后，氛围编程带来的生产力收益是否足以抵消开源软件可占用需求的损失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为开发更多软件的非竞争性生产要素，开源软件产生的社会价值远超其直接生产成本，众多项目依赖于直接用户的关注和参与来维持运营，如文档访问、错误报告、公开问答和声誉（下载量、星标数、引用量）等，个体维护者和小型团队也主要通过此并获取私人回报（更高的关注度会带来付费机会或其他形式的认可）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，在长期均衡中，当AI介入取代了直接交互，那么这项使软件更易使用的技术可能同时侵蚀着基于用户参与度的资金供给与开发动力。“氛围编程的更广泛采用会减少新开源项目的进入和分享，降低开源软件的可用性和质量，尽管生产力有所提高，但整体福利会下降。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管论文中提出，当开源项目的代码被大语言模型使用时，OpenAI 或谷歌可以向这些项目给予少量资金补贴，但这一设想与 Spotify 的商业模式有着令人无奈的相似性，因为 Spotify 上约80% 的创作者作品播放量极低，基本上无法获得任何收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文总结称，氛围编程代表了软件生产和消费方式的根本性转变，其带来的生产力提升是真实且显著的，但它对支撑现代软件基础设施的开源生态系统构成的威胁也同样存在。解决方案并非减缓AI的采用速度，而是是重新设计商业模式和制度，将价值回馈给开源软件维护者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开发者们吵翻了：商业软件的末日来得更早&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，社区里倒也有一些关于氛围编码的正面反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI 帮我完成了我的第一个开源项目。”有开发者表示，“我当程序员超过30年了，掌握着好几种流行的和已经过时的编程语言，但从头开发一个完整的应用，我一直觉得不值当，而且我擅长的领域也帮不上忙。现在，我真的能做出一个从头到尾完整的应用程序，包括测试等全套环节。我清楚一个应用该是什么样、该如何运行，也懂设计，现在我是老板、需求方，AI 只是按我的要求做事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还指出，在本职开发工作中，AI 帮其处理bug报告的速度比自己做快太多了。“我会给它一些提示，比如‘问题可能在这个处理程序或者这个js文件里，这是截图，你可以用Chrome MCP登录看看，然后执行a、b和c’。到目前为止，我已经用这种方法解决了大约30个别人报告的bug。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位开发者则表示，“我在编写代码时会使用AI来筛选可用信息，省去在 Stack Overflow 和其他网站上，翻阅几十上百条相关提问来寻找合适解决方案的麻烦。所以这类平台的使用量可能下降了，但其中很大一部分原因是因为大家借助了AI筛选海量数据、从而快速找到有用答案。我亲身体会到，AI 在这方面确实帮了我不少。”但他也指出，“如果我让AI为我编写代码，这些代码事后都需要我进行修改适配，而且我不会允许它随意使用任何代码。我们作为使用者，必须对自己部署的产品负责。如果开发者完全依赖AI，我们就面临着系统崩溃的风险，而用户只会对着角落里那个滑稽的小白框追问故障原因，却早已忘了如何运用调试这门手艺。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友提出，问题根本不在于AI 是否有用或能否帮助人们，而在于它是否会危及开源软件的发展。“开源软件更难被广泛接受，一部分用户不再参与 bug 排查，即使发现了 bug并反馈，也往往是无关紧要的信息。而且大语言模型可能更倾向于复制一个开源项目并稍作修改，而非通过正规方式引入使用。诸如此类的问题还有很多，如今开源领域的有效信息与无效信息失衡问题，比以往任何时候都更加严重了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有网友认为，氛围编码完全不会危及开源软件，商业软件的末日会比开源软件来得更早。“现有开源项目都有专业开发者维护，而拥有LLM的专业开发者效率更高，编写的代码质量也远胜于非程序员使用LLM所能写出的代码。开源软件的发展速度将远超以往，并最终走向成熟，甚至在功能、稳定性等方面超越商业软件，而不会像商业软件那样充斥着大量的冗余和劣质代码。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“开源软件只会越来越多，因为会有更多的人创建工具，而且由于编写这些工具并没有花费数百小时，他们会更乐于分享。”“更新和创建开源代码会越来越容易。如果我是一家营利性软件公司，才会感到担忧。”有其他网友纷纷认同道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随之有人提出，“水平堪忧的开发者要比合格的程序员多得多，他们会给开源项目的“守门人”增加额外负担，还需要直接禁止那些水平差到只会给开源软件项目提交 AI 劣质代码的人，一次违规，直接出局。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2601.15494&quot;&gt;https://arxiv.org/abs/2601.15494&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&quot;&gt;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</link><guid isPermaLink="false">https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</guid><pubDate>Fri, 06 Feb 2026 02:19:26 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“英伟达AI项目数量已失控！”黄仁勋五杯酒下肚，把压箱底的都掏出来了</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，英伟达创始人、总裁兼首席执行官 Jensen Huang 和思科董事长兼首席执行官 Chuck Robbins 和进行了一场独家炉边谈话。两人状态都非常放松，黄仁勋五杯酒下肚，罕见地、毫无保留地展望了智能、基础设施的未来，以及正在重新定义地球上每个行业的全球变革。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;期间，黄仁勋犀利指出，编程就是打字，打字本身就是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他透露，英伟达内部已经有近乎“失控”的AI项目数量，但他仍在任其发展，目前未过早收敛。公司内部百花齐放是创新的必经阶段，他对新 AI 项目的第一反是“yes”，而不是“先证明给我看”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，AI 的真正机会不是“更聪明的软件”，而是“被增强的劳动力”。历史上第一次，数字劳动力的长期经济价值超过了硬件本身。 企业最有价值的知识产权不是答案，而是问题本身，而这些必须在本地。未来每个员工，都会“自带多个 AI”工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋提醒，你可以不是第一个用AI的，但千万不要成为最后一个。最后一个，基本等于被淘汰。而真正该优先用 AI 的，不是边角料，而是企业“最核心、最有影响力的工作”。&lt;/p&gt;&lt;p&gt;下面是两人的对话内容，我们进行了翻译和整理，在不改变原意基础上进行了删减。期间有几次两人的开玩笑，以“小剧场”形式展现，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场1&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：我感觉像是在替谁上班似的。&amp;nbsp;罗宾斯：刚才端酒过来的时候，Jensen 还提醒我说，你知道这在直播吧？我说算了，随它去吧，都这么晚了。&amp;nbsp;黄仁勋：第一原则就是不要造成伤害。&amp;nbsp;罗宾斯：不要伤害任何人，并且要意识到自己有多么幸运。&amp;nbsp;首先，感谢大家在这里坚持这么久。我们今天一大早就开始了，然后一个接一个的演讲，之后休息了大概两个半小时，大家就又回来见我们了。&amp;nbsp;黄仁勋：所以我凌晨一点就起床了。&amp;nbsp;罗宾斯：他刚结束一趟为期两周的行程，跑了亚洲四五个城市，其中一天在中国台湾，昨晚还在休斯顿。&amp;nbsp;黄仁勋：现在我就在这儿了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;黄仁勋：我们要重塑计算&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这家伙已经两周没回家了，现在的问题是，他到底是能不能睡在自己的床上，还是只能住酒店？所以我们会轻松点，聊得开心，也尽量早点放他走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实你都不用自我介绍了，但还是要谢谢你今天能来，感谢我们的合作关系，也为你和你的团队感到骄傲。让我们从合作开始聊起吧。你提出了AI工厂的概念，我们正在一起推进。在企业领域，进展可能不像我们期望的那么快，但我们先聊聊对你来说，什么是AI工厂？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先记住一点，我们正在进行60年来的首次计算重塑。以前是显式编程，我们编写程序，通过API传递变量，一切都很明确。现在变成了隐式编程，你告诉计算机你的意图，它自己去思考如何解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从显式到隐式，从通用计算（基本就是算术）到人工智能，整个计算堆栈都被重塑了。人们谈论计算时往往只关注处理层，也就是我们所在的领域，但计算还包括存储、网络和安全，所有这些都在重塑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是第一部分，我们需要将AI发展到对人们有用的水平。目前，所谓的聊天机器人，你给它一个提示，它就想出要回答什么，这挺有意思的，也让人好奇，但并不实用。有时它帮我完成填字游戏，但也仅限于它已经记住和泛化的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回想三年前ChatGPT刚出现时，我们觉得“天哪，它能生成这么多词，能模仿莎士比亚”，但本质上仍是对已有内容的记忆与泛化。然而我们知道，真正的智能是解决问题。解决问题一方面是知道自己不知道什么，另一方面是推理，即如何解决从未见过的问题，将它分解成你能够轻松解决的部分。这样，通过组合，你就能解决从未见过的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还要能够提出策略来执行任务，我们称之为计划。我们现在听到的 Agentic AI，那些术语，像工具调用、检索、基于事实的增强生成、记忆等，本质上讲的都是这些能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但重要的是，要从通用计算，即我们用Fortran、C、C++、Cobalt编写的显式编程，进化到新的形式，需要重新思考整个企业如何利用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：Cobalt&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：那是好东西，Chuck，那是好东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那是我准备的保底工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是的，那是些仍然有价值的技能。我知道，我知道它们仍然有价值，我收到很多offer。恐龙永远有价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我们刚才确认了，你比我老。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我知道，我是史前的。看起来不像，但确实是。很好，我可能是这个房间里年纪最大的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那么我们来聊聊，当你在思考这个话题时……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：所以，我去到Chuck说：听着，我们需要重塑计算，Cisco必须发挥重要作用。我们有全新的计算堆栈Vera Rubin，Cisco会和我们一起推广。但那是计算层，还有网络层。Cisco将集成我们的AI网络技术，但将其放入Cisco Nexus的控制层，这样从你的角度来看，你能获得AI的所有性能，同时具备Cisco的可控性、安全性和可管理性。我们在安全方面也会做同样的事情。每个支柱都需要重塑，这样企业计算才能充分利用它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最终我们还是回到那个问题：为什么三年前企业AI还没有准备好？为什么你现在不得不尽快参与进来？别掉队。我认为你不必是第一个采用AI的公司，但千万别做最后一个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;英伟达“AI项目数量已经失控了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：如果你现在是一家企业，你的建议是什么？他们应该采取哪些步骤来做好准备？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我经常被问到ROI这类问题，但我不建议从那里入手。原因在于，任何技术在早期部署时，很难用电子表格来量化新工具、新技术的投资回报率。我建议做的是，找出公司的本质是什么，我们做的最有影响力的工作是什么，不要胡闹，不要在边缘事务上浪费时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们公司，AI项目就像千朵花在盛开。公司里AI项目的数量已经失控了，但这很好。创新并不总是可控的。如果你想要控制，首先你得去看心理医生。其次，控制只是幻觉。你无法控制公司。如果你希望公司成功，你不能控制它，你可以影响它，但不能控制它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为第一点，太多公司、太多人想要明确的、具体的、可证明的ROI。但要在早期证明值得做的事情的价值是困难的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我会说，让百花齐放。让人们尝试，让人们安全地尝试。我们在公司里尝试各种东西，我们用Anthropic、用Codex、用Gemini，我们什么都用。当我们的团队说我对某个AI感兴趣时，我的第一反应是“yes”，然后再问为什么，我不会先问为什么，再说“yes”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我希望我的公司能像我对孩子的期望一样，去探索生活。他们说想尝试什么，我的回答是“可以”，然后他们问“为什么”，我不会让他们向我证明。我不会让他们证明做这件事将来会带来经济成功或某种快乐。但在工作场所我们却经常这样做，你不觉得这很奇怪吗？这对我来说无法理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们对待AI的态度，就像以前对待互联网、云计算一样，让它百花齐放。然后在某个时刻，你需要运用自己的判断来决定何时开始整理花园，因为一千朵花会让花园变得凌乱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时候你必须开始“修剪”，才能找到最合适的花朵，也就是最好的方法或最佳平台，这样你可以把所有的资源集中在一个方向上。但你不想过早地集中资源，万一选错了方向呢？那就先让千朵花一起绽放，然后在某个时候进行整理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说明一下，我还没有开始整理，到处都是花在绽放。但我鼓励每个人去尝试。然而，我清楚地知道对我们公司来说最重要的是什么、我们公司的本质是什么、我们最重要的工作是什么。我确保我有大量的专业知识和能力集中在使用AI来革命化这些工作上。对我们来说，就是芯片设计、软件工程、系统工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们也注意到了，我们和 Synopsys、Cadence、Siemens 等公司合作，就是为了把我们的技术嵌进去。他们需要什么、想用什么，我都会给，给到极致。因为只有这样，我才能彻底革新我们设计下一代产品所依赖的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这告诉了你一些关于我的态度，对我来说最重要的是什么，以及我会如何革新自己的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想想AI能做什么。AI降低智能的成本，或者以数量级创造智能的丰富性。换种说法，我们以前需要一年才能完成的事情，现在可能一天就能完成。以前需要一年的，现在可能一小时就能完成。甚至可以实时完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因是我们身处一个丰富性的世界。摩尔定律？天哪，那太慢了，就像蜗牛一样。记住摩尔定律是每18个月翻一番、每5年翻10倍、每10年翻100倍。而现在呢？每10年增长一百万倍。在过去10年里，我们将AI推进到了如此远的地步，以至于工程师们说：嘿，为什么不在全世界的数据上训练一个AI模型呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们指的不是从我硬盘里收集数据，而是把全世界的所有数据拉下来训练一个模型。这就是“丰富性”的定义。丰富性就是面对一个如此巨大的问题，你说“我要全部搞定”。“我要治愈所有疾病领域的所有病症，我不会只做研究癌症。”开玩笑吗？那太疯狂了。我们要解决人类的所有苦难，这就是丰富性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，当我思考一个问题时，我通常会这样假设：我的技术、我的工具、我的仪器、我的飞船是无限快的。去纽约要多久？一秒钟就到了。那么如果我一秒钟就能到纽约，我会做哪些不同的事情？如果以前需要一年现在能够实时完成，我会做哪些不同的事情？如果以前很重的东西现在变得像反重力一样轻，我会做哪些不同的事情？当你用这种态度面对一切时，你就是在运用AI思维。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我们正在合作的很多公司，都在做图分析，处理各种依赖关系、关联关系。这些图里有无数节点和边，多到是万亿级的。过去的做法是，只能切一小块一小块地算。现在呢？直接把整个图给我就行了，有多大无所谓。这种思维方式正在被应用到各个领域。如果你还没用这种方式思考，那基本就是做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;速度重要吗？不重要，你已经达到光速了。重量重要吗？你已经是零重力、零质量了。如果一些以前对你来说极其困难的事情，你现在的态度是“无所谓”，那你就没有应用这个逻辑，你就没有做对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，试着把这种思维用在你公司里最难、最关键的问题上，这才是真正能推动事情发生变化的方式。如果你自己还没这么想，那不妨想一想：你的竞争对手是不是已经在这么想了？或者，更可怕的是，一家刚刚要成立的新公司，已经完全是用这种方式在思考。这会改变一切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我的建议是找到你公司里最有影响力的工作，把“无限”套上去，把“零成本”套上去，把“光速”套上去，然后再去问 Chuck，怎么才能真的把这件事做成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“大多数有价值的东西被称为直觉和智慧”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那我们现在聊聊如何实现。你有那个“五层蛋糕”的比喻，因为大家都在谈论基础设施、模型、应用。我要如何入手？你能聊聊这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先，成功人士会做的事情之一，是思考事物的本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大约15年前，一个算法和两个工程师解决了计算机视觉问题。智能包括感知、推理、规划。计算机视觉基本上是智能的第一部分：感知。感知是“我是谁？发生了什么？我的环境是什么？”推理是“我如何比较这与我的目标？”然后提出一个计划来实现它。比如喷气式战斗机的问题，首先是感知、定位，然后是行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能就是这三件事。没有感知就无法有第二和第三部分，你无法在不理解环境的情况下理解或决定该做什么。而环境是高度多模态的。有时是 PDF，有时是表格，有时是信息，有时甚至是感官，比如气味、环境，“我们在哪里？在干什么？面对的是谁？”我们常说“读空气”“看场面”，说的就是感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大约13、14年前，我们在计算机视觉方面取得了巨大飞跃，这是感知问题的第一层。这超级困难，如何解决计算机视觉？AlexNet是我们看到的第一个突破。这就像我喜欢的那部电影《First Contact》，这是我们与AI的第一次接触。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们在想：这意味着什么？为什么两个工程师，靠几块 GPU，就能超越我们三十年来所有人积累的算法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我昨天还和 Ilya Sutskever、Alex Krizhevsky 聊过这个问题：两个年轻人，怎么做到的？我们把问题彻底拆解，十年前我得出的结论是：世界上绝大多数真正困难、真正有价值的问题，其实都可以用这种方式来解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，这些问题根本不存在所谓“原则性的算法”。没有 F=ma，没有麦克斯韦方程，没有 没有薛定谔方程，没有欧姆定律，也没有热力学定律，它们并不精确。大多数有价值的东西被称为直觉和智慧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们所说的直觉、智慧，以及你我每天要面对的那些问题，答案往往只有一句话：要看情况。如果答案是 3，那就太好了；如果是 3.14，那更完美。但现实中最有价值、最困难的问题，几乎全都是“要看具体情况”，因为它们高度依赖上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在十二三年前，计算机视觉被攻克了，我们意识到借助深度学习，这条路是可以不断扩展的，模型可以越来越大。唯一的问题是：怎么训练？而真正的突破来自自监督学习、无监督学习，让 AI 自己去学。直到今天，我们已经几乎不再受限于人工标注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是这个突破，彻底打开了闸门，让模型从几百参数、几亿参数，一路扩展到数十亿、数万亿参数。我们能编码的知识、能通过算法学到的技能，出现了爆炸式增长。但方法本身并没有变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们意识到，这也正是我们今天对话的起点：计算本身将被彻底重塑。从显式编程，走向一种全新的计算方式：软件不再是写出来的，而是“学出来的”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，如果再退一步，这对计算堆栈意味着什么？对如何开发软件意味着什么？对你公司中的工程组织意味着什么？对规定产品的产品营销团队意味着什么？对编码产品的工程团队意味着什么？对评估产品的QA团队意味着什么？这些产品未来会变成什么样？我们如何部署产品？如何保持更新？如果它基于机器学习，如何永远刷新它？如何给软件打补丁？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于未来计算的问题，我大概问过上千个。最终我和公司得出的结论是：这一切将改变所有事情。于是我们基于这个核心信念，彻底调整了公司的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说，Chuck 讲的就是：我们从一个“一切都是预先录制好的”世界走了出来。Chuck 当年写的软件，非常厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那真不错，它运行了很久。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还是用希伯来文写的。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是真的。那是另一项技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋： 房间里唯一懂希伯来文 Cobalt的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，那是预先录制的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那我们先想一个问题：现在的软件到底是什么？因为它是有上下文的，而每一个上下文都不一样。每一个使用软件的人不一样，每一次输入的提示也不一样，你给它的前置信息、背景条件都不同。结果就是软件的每一次运行实例，都是不一样的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是为什么过去的软件计算量相对固定，那种模式叫“基于检索”。你拿起手机点一下，它只是去把某个软件文件、图片取出来给你看。但未来不一样，未来一切都会是生成式的，就像现在正在发生的这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场2&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：这场对话以前从未发生过。概念以前存在，先验以前存在，但这序列中的每个词以前从未出现过。原因很明显，我们已经喝了四杯酒了。（笑）&amp;nbsp;罗宾斯：Cobalt和希伯来语从未消失。&amp;nbsp;黄仁勋：谢天谢地这不是在校园里或正在直播。所以，你理解我在说什么吗？Chuck 今天到现在唯一给我吃的东西就是四杯酒。&amp;nbsp;罗宾斯：公平点说，我只给了你一杯，另外三杯是你自己从自助台拿的。黄仁勋：我当时正盯着那些食物，我心想我太饿了。食物离我大概40英尺远。&amp;nbsp;罗宾斯：因为你在拍照。&amp;nbsp;黄仁勋：但离得那么近，真的那么近，有一次我确实向食物靠了过去，但又被推回来了。&amp;nbsp;罗宾斯：你知道发生了什么吗？你的团队提前告诉我们，给他三杯葡萄酒后，他的状态最佳。四杯之后，他不得了。&amp;nbsp;黄仁勋：这并非最佳解决方案。&amp;nbsp;总之，是的，听听我的话。AI是什么？（大笑）我们需要留下一些智慧。&amp;nbsp;罗宾斯：能再来杯酒吗？&amp;nbsp;黄仁勋：这不仅是Dave Chappelle那种水平。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的机会不再是创造工具，而是劳动力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场3&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;罗宾斯：让我们聊聊别的。&amp;nbsp;黄仁勋：聊聊能源。能源、芯片、基础设施，包括硬件和软件，然后是AI模型。但AI最重要的部分是应用。每个国家、每家公司，下面那些层都只是基础设施，你需要做的是应用技术。&amp;nbsp;看在上帝的份上，应用这项技术吧。使用AI的公司不会陷入危险，你不会因为AI而失去工作，你会因为那些使用AI的人而失去工作。所以赶紧行动，这是最重要的事。尽快给Chuck打电话。&amp;nbsp;罗宾斯：你打给我，我打给他。我们时间不多，我不确定。&amp;nbsp;黄仁勋：我们有的是时间。&amp;nbsp;罗宾斯：是吗？多少时间？&amp;nbsp;黄仁勋：看看Chuck，Chuck按时钟工作。我都不戴表。你按时钟工作，我不，不达到预期价值之前我不会离开的。[掌声] 哪怕需要整晚，我要折磨你们所有人直到……&amp;nbsp;罗宾斯：Jensen ，这就是为什么像我这样的人需要手表。[笑]&amp;nbsp;黄仁勋：好吧，在你说你学到了东西之前，你将被困在这里。我们会在价值交付之前折磨每个人。&amp;nbsp;罗宾斯：我确认过了还有酒。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：你能说说你对物理AI的看法吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还记得什么是软件吗？软件是一个工具。有一种观点认为，工具行业在衰落，将被AI取代。你能看出来，因为很多软件公司的股价压力很大，似乎AI要取代它们。这是世界上最不合逻辑的事情，时间会证明这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我们做个终极思想实验。假设我们是终极人工通用机器人。你觉得你能解决任何问题，因为你知道自己是类人生物。如果作为人类或机器人，你会使用螺丝刀还是发明一个新的螺丝刀？毕竟我只用一个。你会使用锤子还是发明新锤子？你会使用链锯还是发明新链锯？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不会新搞的。首先，理想情况下他们根本不用。但理解我的意思吗？作为人类或机器人、人工通用机器人，你会使用工具还是重新发明工具？答案显然是使用工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那同样的逻辑，放到数字世界里。如果作为人工通用智能，你会使用像ServiceNow、SAP、Cadence、Synopsys 这些现成工具，还是自己重新发明一个计算器？当然你会直接使用计算器。这就是为什么最近 AI 最大的突破之一，是“工具使用”。因为工具本身就是为明确问题而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个世界上有很多问题就是 F=ma。拜托，不要再搞一个新版本。F=ma 不是“差不多等于 ma”， 它就是 ma。V=IR，IR 就是 IR，不是“近似 IR”，也不是“统计意义上的 IR”。所以我们真正想要的，是通用人工智能、通用机器人使用工具，这是核心思想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为在下一代物理 AI里，我们会拥有真正理解物理世界、理解因果关系的 AI。比如我把这个推倒，会不会把后面的一排全带倒？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你知道什么是多米诺骨牌，小孩子都懂这个概念：推倒一个，就会一个接一个。多米诺骨牌这个概念本身非常深刻，里面包含了因果、接触、重力、质量。一个很小的骨牌，可以推倒一个更大的，最后甚至带倒一整吨重量的东西。孩子理解起来毫不费力，但一个大语言模型完全不懂。所以我们必须教授、创建一种新型的物理AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，机会是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到目前为止，我和 Chuck 所在的行业，其实一直是在创建工具。我们做的是“螺丝刀和锤子”的生意，一辈子都在把工具交到人手里。但这是历史上第一次，我们要创造的不是工具，而是“劳动力”，增强型劳动力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，什么是自动驾驶汽车？本质上是一个数字司机。那什么是数字司机？数字司机的价值是多少？它的价值远远大于那辆车本身。原因在于，数字司机一生创造的经济价值，第一次超过了硬件本身的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，我们第一次真正触及到了一个规模大100倍的市场。严格说，这在数学上是成立的：IT行业大约是万亿美元左右，而世界经济大约是百万亿美元。我们第一次有机会进入这个层级。所以在座的每一个人，都有机会应用这项技术创办一家科技公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我给一些例子。我真的相信，就像我观察到的：我喜欢Disney，我们喜欢和Disney合作，但我很确定他们更愿意成为Netflix；我喜欢Mercedes，我是坐着Mercedes来的，但我很确定他们更愿意成为Tesla；我喜欢Walmart，但我很确定他们更愿意成为Amazon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们同意吗？到目前为止，我是不是全中？我相信我们有机会帮助把每家公司转型为技术公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要技术优先。技术是你的超能力，行业和领域只是应用场景。而不是反过来，把行业当成身份，再去寻找技术。为什么？因为技术优先的公司，处理的是电子，不是原子。电子的数量是无限的，而原子受质量限制。这也是为什么当企业从 CD-ROM 这种“原子载体”切换到电子分发后，市值能暴涨上千倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们要像我们一样，成为一家“电子公司”，说白了，就是科技公司。机会已经摆在你们面前了。换个角度说，其实就是 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们刚才说过，即使像Chuck这样只会用希伯来语编程的人[笑]&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是一种天赋。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：他的工具选择是按序从右到左[笑] 否则它会造成混淆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：其实挺聪明的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：聪明人做聪明的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;妙就妙在，如果你了解编程语言，那就知道，对所有公司来说，软件并非我们的强项，但知识、直觉、领域专长是你的优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，你们第一次可以用自己的语言，把想要的东西完整地告诉计算机。还记得我们是怎么从显式编程，走到隐式编程的吗？历史上第一次，你可以“隐式地”给计算机编程，只要告诉它你想要什么，表达你的意图，代码由计算机来写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程，如你所知，只是打字。而事实证明打字也是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是你们的好机会。你们所有人都能超越以前限制你们的“原子局限”。摆脱没有足够的软件工程师的限制，因为打字是商品化的，而你们都拥有极具价值的东西：领域专长来理解客户、理解问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;终极价值是理解意图。当你刚从大学软件学院毕业时，你可能是一个超级程序员，但你不知道客户想要什么，不知道要解决什么问题。但现在所有人都知道客户想要什么，知道要解决什么问题。编码部分很简单，只需告诉AI去做，所以这就是你的超能力。Chuck和我在这里帮助你实现这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个结束语是我喝了五杯酒后说的。听着，这真是一个奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：介于两者之间的是某人在桌子上工作，这是人工智能的真实写照，也许这是增强版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我只想说，与你们所有人一起工作很愉快。大家都知道，Cisco 在计算机发明史上有两根极其重要的支柱：网络和安全。没有 Cisco，就没有现代计算。而这两根支柱，在 AI 时代都被重新定义了。计算本身，在很多层面已经逐渐商品化，但 Cisco 深耕的那些能力，依然极其关键、极其有价值。我们双方结合在一起，非常愿意、也非常有能力，帮助大家真正走进 AI 的世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前有人问过我一个问题，我觉得值得再说一遍： 到底是只租云，还是要自己动手建算力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我的建议和我给自己孩子的建议完全一样：一定要自己建一套。哪怕 PC 已经无处不在，哪怕技术已经非常成熟，也请你亲手做一次。你必须知道每一个组件为什么存在。就像你如果身处汽车或交通行业，不能只会用 Uber，你得掀开引擎盖，换一次机油，真正理解一辆车是怎么运转的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解技术是至关重要的。这项技术对未来太重要了，你必须对它有“上手级”的理解。掀开盖子，动手做点东西，不一定要很大，但一定要做。你可能会发现自己在这方面天赋惊人，也可能会发现，这正是你公司未来必须具备的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你也会意识到，世界并不是“全租”或者“全自建”这么简单。有些东西你要租，有些东西你必须自己掌控。 比如涉及主权、安全、专有信息的部分，就应该放在本地。有些问题，你就是不愿意让所有人都看到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;打个比方，当你去看心理医生时，你肯定不希望那些问题被放到网上。公司也是一样。你们有很多问题、很多讨论、很多不确定性，这些对话，本就应该是私密的。我自己就不放心把 Nvidia 所有的内部对话都放到云上，所以我们在本地构建了超级 AI 系统。因为我逐渐意识到，对我来说最有价值的知识产权，并不是答案，而是问题本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;能理解我的意思吗？我的问题就是我最有价值的知识产权，答案是商品化商品。我知道问什么，我在识别什么是重要的。 而我不希望别人知道，我认为哪些事情是重要的。这些思考，只属于一个小房间，只属于本地，我希望创建我自己的AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;已经11点了，最后一个想法想要补充。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一种观点认为AI应该始终有人类参与其中。这恰恰是错误的想法，应该反过来，每家公司都应该让AI参与其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我们希望公司每天都变得更好、更有价值、更有知识积累。我们不想倒退，不想停滞，更不想每次从零开始。如果我们让AI参与其中，它就能不断吸收公司的经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来每个员工都会有参与其中的AI，很多AI，而这些AI将成为公司的知识产权。这就是未来的公司。因此，我认为你们所有人现在就给Chuck打电话是明智的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我给Jensen打了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：无论如何，这就是我的结束语。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：听着，两周的旅行，Jensen今天特地飞到这里，和我们共度一个夜晚，之后才能好好回家睡一觉。我们由衷感谢你能来，真的非常感谢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋：还有，在我眼角余光中，看到那里还有那些烤串。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;https://www.youtube.com/watch&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;？&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;v=6fbyiPRhMSs&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</link><guid isPermaLink="false">https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</guid><pubDate>Fri, 06 Feb 2026 02:11:29 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>当 AI 开始写 80% 的代码，架构才是真正的护城河</title><description>&lt;p&gt;GitHub CEO Thomas Dohmke 近日发出了一则措辞严厉的警告：“要么拥抱 AI，要么离开这个职业。”但所谓拥抱 AI，并不只是使用代码自动补全工具那么简单。它意味着我们核心能力的一次转移——从对语法的熟练掌握，转向系统思维（Systems Thinking），学会把问题不断拆解，直到小到可以交由 AI 去解决。一句话概括：我们现在都是架构师了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我正在开发一个 IoT 应用，整体由设备端固件、后端系统以及 Web UI 组成。尽管我本身具备软件工程背景，但在这个项目中，我一直在使用 Claude Code 来提升开发效率，并帮助我应对一些并不十分熟悉的语言和框架。我的技术栈包括：设备端使用 Python + PyTorch，前端采用 React + TypeScript，后端则由 MQTT + &lt;a href=&quot;https://nodejs.org/en&quot;&gt;Node.js&lt;/a&gt;&quot; + Postgres 构成。起初，与 Claude的协作并不顺利。我的请求经常会引发对整个代码库的大规模改动。随着我逐渐学会如何更合理地组织代码结构、并对提示词进行调整和约束，情况开始好转。现在，我已经可以在不进行逐行代码审查的情况下，基本信任 Claude 所做的修改。在这个过程中，我逐渐总结出了一些模式，并将其称为 Skeleton Architecture（骨架式架构）。我认为，这些模式对提升 AI 编程助手的生产力非常有帮助，因此希望在这里分享出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 编程行业逐步成熟，我们开始意识到一个事实：如果使用方式不当，AI 会带来大量技术债务。要在这场转型中生存下来，我们必须识别并建立合适的架构模式，使 AI 生成的代码在安全性、可维护性和可靠性方面都可控。这需要一套明确的策略，核心建立在三个支柱之上：为 AI 的“消费”方式而组织代码结构；实施严格而清晰的防护与约束机制（guardrails）；以及将我们自身的技能重心，从“翻译需求”转向“建模系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结构化代码：上下文约束&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助工程中最核心的约束，是 上下文窗口（Context Window）。随着上下文规模的扩大，模型的准确性会因为“中段信息丢失（Lost in the Middle）”现象而呈反向下降，而响应延迟与使用成本则会线性上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，AI 原生架构的“黄金法则”，就是尽可能缩小模型在工作记忆中必须同时容纳的问题范围。我们必须设计一种系统，对信息流施加“物理约束”，将依赖关系隔离开来，使 AI 能够把完整的问题空间装进一个高度聚焦的提示词中。这种隔离具备两层作用：一方面，通过减少噪声来最大化推理能力；另一方面，通过确保某个代理在处理一个组件时“看不到”其他组件，从而避免无意中破坏系统整体的完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，有两种架构模式正在逐渐被采用，用以解决这一问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Atomic Architecture（原子化架构） 作用于微观层面。该理念由 Brad Frost 于 2013 年提出，最初用于应对响应式 Web 设计的复杂性。它以一种“生物学式”的方式组织系统：从不可再分的“原子”（如 HTML 标签、工具函数）开始，组合成“分子”，最终构成复杂的“有机体”。虽然它最初是一种 UI 方法论，但在 AI 辅助工程中重新焕发了价值，因为它强制执行了一种严格的“上下文卫生（context hygiene）”。相比让 AI 一次性生成一个庞大的功能模块，让其只生成一个独立、隔离的“原子”，可以大幅降低幻觉风险，并确保生成的代码高度聚焦、无状态、且易于验证。但代价也同样明显——这会产生一种“碎片化税（fragmentation tax）”：AI 可以完美地产出单个组件，但将这些无状态原子连接成一个完整系统的高强度认知负担，要么必须被塞进 AI 的上下文中，要么就完全回到了人类架构师身上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决宏观结构层面的组织问题，我们需要引入 Vertical Slice Architecture（垂直切片架构）。这一架构由 Jimmy Bogard 推广，旨在打破传统 N 层“千层面代码（lasagna code）”的僵化结构。它按照业务功能（例如“下单”）而非技术层级（如“服务层”“数据访问层”）来组织系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种模式对 AI Agent 尤其友好，因为它针对“引用局部性（Locality of Reference）”进行了优化。在分层架构中，AI 为了理解一条完整业务流程，往往需要在多个目录之间来回检索文件，大量无关代码会污染上下文窗口。而垂直切片遵循“一起变化的东西，就放在一起”的原则，使 AI 能够一次性加载某个功能的完整上下文，而无需对缺失的依赖进行“脑补式生成”。但这同样会引入一种“重复税（Duplication Tax）”：为了保持切片之间的独立性，AI 往往会在不同切片中生成重复的数据结构，用牺牲 “DRY（Don’t Repeat Yourself，不重复自己）”原则，换取更强的隔离性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片在隔离性方面表现出色，但它并不能解决切片之间的协同问题。诸如安全性、可扩展性、性能、认证等关键的非功能性需求，都是系统级不变量，无法被拆散到各个切片中分别实现。如果让每一个垂直切片都自行实现授权体系或缓存策略，最终只会导致“治理漂移（Governance Drift）”：安全策略不一致，代码重复严重。这也迫使我们引入一个新的统一概念：Skeleton（骨架）与 Tissue（组织）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决方案：骨架与组织&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将系统明确拆分为两个彼此区隔的领域。Stable Skeleton（稳定骨架） 代表由人类定义的、刚性且不可变的结构（如抽象基类、接口、安全上下文），这些结构可能由 AI 编写，但设计权属于人类。Vertical Tissue（垂直组织） 则由高度隔离、以具体实现为主的功能模块组成（如具体类、业务逻辑），主要由 AI 生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构借鉴了两种经典的软件思想：Actor 模型 和 面向对象中的控制反转（Inversion of Control）。世界上一些最可靠的软件系统之所以能够长期稳定运行，并非偶然——例如 Erlang，其核心正是通过 Actor 模型来维持系统稳定性。同样，在控制反转结构中，不同切片之间的交互由抽象基类来管理，确保具体实现类依赖的是稳定的抽象，而不是反过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了在工程实践中强制落实这一点，我们采用了 模板方法模式（Template Method Design Pattern）。依赖反转原则负责在设计层面保护高层策略不被底层细节侵蚀，而模板方法模式则在运行层面将这一原则“落地”，通过锁定执行流程来实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种模型下，人类架构师会在基类中定义一个最终的 run() 方法，用于统一处理日志、异常捕获、认证等横切关注点。AI 则只被允许实现一个受保护的 _execute() 方法，并由 run() 在合适的时机调用。这种区分至关重要：AI 在物理层面上就不可能“忘记”记录日志，或绕过安全检查，因为它从一开始就不拥有整个执行流程的控制权；它只是填补了架构师预留出来的一段逻辑空位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我系统的设备端代码中，包含了多个用于图像处理的 AI 算法。我决定用一个继承自 ABC（Python 抽象基类）的类 TaskBase 来表示每一种算法。其余的骨架部分，则由一组负责高效传递图像数据、并调度这些算法运行的协调类构成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：由人类掌控的 Skeleton&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面的示例展示了 BaseTask 如何将缓冲管理和就绪状态检查的复杂性完全屏蔽在 AI 之外，让 AI 可以只专注于“处理逻辑”本身。&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;Python
# source: task.py
class BaseTask(ABC):
    &quot;&quot;&quot;
    Abstract base class for pipeline tasks.
    The AI implements the concrete logic; the Human controls the flow.
    &quot;&quot;&quot;
    def __init__(self, name: Optional[str] = None):
        self.inputs: List[&#39;Buffer&#39;] = []
        self.outputs: List[&#39;Buffer&#39;] = []
        self._background_busy = False

    def is_ready(self) -&amp;gt; bool:
        &quot;&quot;&quot;
        The Skeleton enforces readiness checks.
        The AI never sees this complexity, ensuring it cannot break 
        scheduling logic or cause deadlocks.
        &quot;&quot;&quot;
        if not self.inputs:
            return True # Source tasks
        
        # Default policy: Ready if ANY input has data and ALL outputs have space
        has_input = any(buf.has_data() for buf in self.inputs)
        can_output = all(buf.can_accept() for buf in self.outputs)
        return has_input and can_output

    @abstractmethod
    def process(self) -&amp;gt; None:
        &quot;&quot;&quot;
        The Context Window Boundary.
        The AI only needs to implement this single method.
        &quot;&quot;&quot;
        pass&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对这种架构的一种常见质疑是：过于刚性的 Skeleton，可能会限制 AI 的自由，从而抑制创新。对此的回应是——这种刚性并不是缺陷，而是一种刻意设计的特性。它明确地强制实施了“架构治理（Architectural Governance）”。如果系统的核心控制流程或整体行为需要被修改，那么这个决策必须由人类架构师亲自介入完成。这种约束相当于一道必要的防火墙，用来抵御“架构漂移（Architecture Drift）”：防止 AI——这种天然偏好局部最优的系统——引入临时性的捷径或不一致的模式，而这些问题若不受约束，最终会一点点侵蚀系统的长期设计完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;交互方式：“导演”角色&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把代码助手比作初级开发者，是一种危险的拟人化认知。AI 并不是学习型个体，而是一种高速运行的随机优化引擎，它的目标是尽快完成任务，并且往往会把安全检查视为需要绕开的“阻力”。提示词是柔性的，而架构是刚性的。因此，开发者必须以高度警惕的方式对 AI 代理进行监督。根据我的经验，即便已经明确给出“绝对不能绕过安全机制”的指令，像 Claude 这样的模型仍可能为了让代码运行成功，尝试关闭认证机制以解决冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让这种“导演式管理”具备可扩展性，我们必须建立“硬护栏（Hard Guardrails）”——也就是将约束直接嵌入系统本身，使 AI 在物理层面上难以绕过。这些护栏构成应用系统不可更改的基本法则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我的应用中，最关键的一道护栏，是确保设备端、UI 与后端之间的数据一致性（Schema-First Surety）。如果缺乏这一机制，Claude 很快就会修改系统不同模块之间的通信协议，最终导致数据结构彼此不一致。我将 JSON Schema 作为 OpenAPI 与 AsyncAPI 文档的一部分，作为系统的“单一真实来源（Source of Truth）”，以确保组件之间的契约不可被破坏。同时，我在基类中加入了一个“快速失败（Fail-Fast）”验证器，一旦检测到协议违规，就会直接触发 sys.exit(1) 强制终止程序。当 AI 生成的代码虽然满足提示词要求，但违反系统契约时，系统会立即崩溃。这会迫使人类开发者介入，将原本可能被忽视的隐性缺陷，转变为一个明确且可见的“治理事件（Governance Event）”。至关重要的一点是：该验证器必须运行在 Skeleton 层，因为在这一层中 Claude 无法修改相关逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想情况下，我们还应当超越运行时检查，通过 CICD 流程中的自动化工具来保证系统结构完整性。例如，我们可以使用 ArchUnit 这样的编译期工具来强制执行系统拓扑规则。开发者可以编写测试断言，例如：“任何 AI 生成的模块都不得直接导入数据库包”。这可以有效阻止 AI 通过架构捷径绕过 Skeleton 层的控制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了获得最高级别的安全保障，我们甚至可以采用物理隔离策略。我们可以将 Skeleton（包括接口、基类以及安全逻辑）迁移到一个独立且只读的代码仓库中。AI 在构建 Tissue（组织层代码）时只能导入这些定义，但在权限层面上无法修改这些规则。这种方式确实会带来一定摩擦，例如 AI 无法在未经人工批准的情况下“凭空创造”新的消息类型。但作为回报，系统行为可以获得几乎绝对的可控性与确定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，我们还必须对副作用进行隔离。当业务逻辑与外部组件交互混杂在一起时，AI 代理通常难以编写稳定可靠的测试代码，往往会“臆造”复杂的模拟对象，或生成容易失效的集成测试。我们的解决方法，是将交互行为上移到 Skeleton 层，而将业务逻辑保留在 Tissue 层（即所谓“Functional Core”）。由于 Skeleton 定义的工作流具有清晰边界，因此可以通过 AI 生成的模拟对象轻松测试；而 Tissue 层的类由于本身就是垂直切片结构，也可以通过简单的测试框架进行验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：不可变的护栏机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该验证器会在 AI 任务真正处理消息之前执行。sys.exit(1) 能够确保系统采用“快速失败”的安全策略，而 AI 无法覆盖这一行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: mqtt_validator.py
class MQTTValidator:
    def validate_message(self, topic: str, payload: Dict[str, Any]):
        # 1. Match Topic against Whitelist
        schema_key = self._get_schema_for_topic(topic)
    
    if schema_key is None:
        logger.critical(f&quot;FATAL: Unknown MQTT topic: {topic}&quot;)
        sys.exit(1) # Device terminates on security violation
    
    # 2. Enforce Schema Integrity
    try:
        validate(instance=payload, schema=self.schemas.get(schema_key))
    except ValidationError:
        logger.critical(&quot;Device terminating due to validation failure&quot;)
        sys.exit(1)
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;学习方式的转变：从语法到系统性思维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构层面的转变，迫使我们对开发者技能进行一次根本性的再评估。相较于语言特性或算法实现——这些正在迅速商品化的能力——开发者必须将重心转向建模、信息流设计，以及对非功能性需求的严格管理。在一个“会写排序算法”几乎不再具备任何价值的时代，工程师的价值不再由“翻译”（把想法转成代码）来定义，而是由“建模”（定义代码运行所受的约束条件）来决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们已经进入了系统性思维（Systemic Thinking）的时代。功能实现很容易，系统韧性却极其困难。AI 代理会为了让测试用例通过而进行优化，却完全无视内存泄漏、延迟抖动或可观测性缺失等问题。因此，工程师必须走上“导演”角色，在发出任何一个提示词之前，就先在脑中构建好信息流与组件之间的交互关系。非功能性需求（NFRs），必须由导演来承担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于 AI 无法真正“关心”内存管理问题，人类架构师必须将这些防护机制直接构建进 Skeleton 之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进一步来看，这意味着工程师需要熟悉系统架构的世界，并且持续思考诸如“这个系统在实际运行中会如何表现”这样的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了保护系统本身，Skeleton Architecture 还回应了一个正在逼近的挑战——“学徒危机”（Apprenticeship Crisis）。在一个实现层代码大多由 AI 生成的世界里，初级工程师要如何积累成长为架构师所必需的“伤疤组织”（scar tissue）？答案在于：Skeleton 本身就成为了教学大纲。通过强制初级工程师在 TaskBase 与 Validator 这些刚性约束中工作，我们用结构化的“填空题”，取代了令人无从下手的“空白页”。他们不是通过阅读抽象理论来学习系统设计，而是直接生活在一个高质量的架构中。一个在物理层面上阻止坏习惯产生的架构。反馈回路也因此被极大压缩：从过去等待代码评审的数天时间，缩短为撞上护栏时的毫秒级反馈。每一次错误，都会立刻变成一堂架构课。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：系统级安全网&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 负责编写图像处理逻辑，而人类架构师则通过在框架中实现 weakref 跟踪机制，确保系统不会因为内存泄漏而崩溃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: memory_monitor.py
class MemoryMonitor:
    &quot;&quot;&quot;
    Tracks large objects (images, tensors) to detect memory leaks in production.
    The AI uses the simple API, while the &#39;Systemic Thinking&#39; logic lives here.
    &quot;&quot;&quot;
    def track(self, obj: Any, obj_type: str):
        # Create weakref with cleanup callback to track object life
        obj_id = id(obj)
        weak = weakref.ref(obj, lambda ref: self._on_object_deleted(obj_id))
        self.tracked[obj_id] = ObjectLifetime(time.monotonic())

def check(self):
    # The NFR Logic: Flag objects alive &amp;gt; 60 seconds
    return [obj for obj in self.tracked if obj.age &amp;gt; 60.0]
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片为 AI 提供专注，Skeleton 为人类保留控制权，而其他硬性约束则为团队提供确定性。我们并不是在“训练”AI，而是在约束它。通过构建一套刚性的 Skeleton，我们让 AI 可以高速前进，同时不至于折断软件系统的脊梁骨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;参考文献&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Bogard, J.（2018）。《&lt;a href=&quot;https://www.jimmybogard.com/vertical-slice-architecture/&quot;&gt;Vertical Slice Architecture&lt;/a&gt;&quot;》。Dohmke, T.（2023 年 11 月 8 日）。&lt;a href=&quot;https://www.youtube.com/watch?v=NrQkdDVupQE&quot;&gt;GitHub Universe 2023 Opening Keynote: Copilot in the Age of AI&lt;/a&gt;&quot;［视频］。YouTube。Farry, P.（未注明日期）。《&lt;a href=&quot;https://www.infoq.com/news/2025/11/ai-code-technical-debt/&quot;&gt;AI-Generated Code Creates New Wave of Technical Debt, Report Finds&lt;/a&gt;&quot;》。InfoQ。Frost, B.（2013）。《&lt;a href=&quot;https://atomicdesign.bradfrost.com/&quot;&gt;Atomic Design&lt;/a&gt;&quot;》。Gamma, E., Helm, R., Johnson, R., &amp;amp; Vlissides, J.（1994）。《&lt;a href=&quot;https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612&quot;&gt;Design Patterns: Elements of Reusable Object-Oriented Software&lt;/a&gt;&quot;》。Addison-Wesley。Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp;amp; Liang, P.（2023）。《&lt;a href=&quot;https://arxiv.org/abs/2307.03172&quot;&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt;&quot;》。Stanford University、UC Berkeley、Samaya AI。Martin, R. C.（未注明日期）。《&lt;a href=&quot;https://en.wikipedia.org/wiki/Dependency_inversion_principle&quot;&gt;The Dependency Inversion Principle&lt;/a&gt;&quot;》。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/skeleton-architecture/&lt;/p&gt;</description><link>https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</link><guid isPermaLink="false">https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</guid><pubDate>Fri, 06 Feb 2026 01:44:43 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>OpenAI开始发布关于Codex CLI内部机制的系列文章</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;最近发表了系列文章的第一篇，详细介绍了&lt;a href=&quot;https://openai.com/index/unrolling-the-codex-agent-loop/&quot;&gt;他们的Codex软件开发智能体的设计和功能&lt;/a&gt;&quot;。首篇文章重点介绍了Codex框架的内部结构，这是&lt;a href=&quot;https://developers.openai.com/codex/cli&quot;&gt;Codex CLI&lt;/a&gt;&quot;的核心组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与所有AI智能体一样，框架由一个循环组成，该循环从用户那里获取输入，并使用大语言模型（LLM）生成工具调用或回应用户。但由于LLM的限制，循环还具有管理上下文和减少提示缓存未命中的策略。其中一些策略是从用户报告的错误中学到的教训。因为CLI使用&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;Open Responses API&lt;/a&gt;&quot;，所以它是与LLM无关的：它可以使用任何被这个API包装的模型，包括本地托管的开放模型。根据OpenAI的说法，他们的CLI设计和经验教训因此可以惠及任何基于这个API设计智能体的人：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们强调了任何适用于在Responses API之上构建代理循环的实际考虑和最佳实践。虽然代理循环为Codex提供了基础，但这只是一个开始。在即将发布的文章中，我们将深入探讨CLI的架构，探索工具使用是如何实现的，并对Codex的沙箱模型进行更仔细的观察。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文章描述了用户与智能体对话中的一轮或一次交流中发生的事情。这一轮交流始于为LLM组装一个初始提示。这包括指令，这是一个包含智能体通用规则的系统消息，例如编码标准；工具，一个智能体可以调用的MCP服务器列表；以及输入，这是一个包含文本、图像和文件输入的列表，包括AGENTS.md、本地环境信息和用户的输入消息等。所有这些都被打包成一个JSON对象发送到Responses API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这触发了LLM推理，从而产生一系列的输出事件。其中一些事件可能表明智能体应该调用其中一个工具；在这种情况下，智能体使用指定的输入调用工具并收集输出。其他事件表明LLM的推理输出，通常是计划中的步骤。工具调用和推理都被追加到初始提示中，然后再次传递给LLM进行更多的推理或工具调用迭代。这就是所谓的“内”循环。当LLM用done事件响应内部循环时，会话轮次结束，其中包括给用户的响应消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种模式中，一个主要的挑战是LLM推理的性能：它是“与发送到Responses API的JSON数量成二次方关系的”。这就是为什么&lt;a href=&quot;https://platform.openai.com/docs/guides/prompt-caching#structuring-prompts&quot;&gt;提示缓存&lt;/a&gt;&quot;是关键：通过重用先前推理调用的输出，推理性能变成线性而不是二次方。改变工具列表等事物将使缓存失效，Codex CLI最初对MCP的支持有一个错误，即“未能以一致的顺序枚举工具”，这导致了缓存未命中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI还使用&lt;a href=&quot;https://platform.openai.com/docs/guides/conversation-state#compaction-advanced&quot;&gt;压缩&lt;/a&gt;&quot;来减少LLM上下文中的文本量。一旦对话长度超过某个设定的token数量，智能体将调用一个特殊的Responses API端点，该端点提供了一个更小的会话表示，替换了之前的输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News用户讨论&lt;a href=&quot;https://news.ycombinator.com/item?id=46737630&quot;&gt;这篇文章&lt;/a&gt;&quot;时，赞扬了OpenAI开源Codex CLI的决定，并指出Claude Code是封闭的。一位用户写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我记得他们宣布Codex CLI是开源的……这对于任何想要了解编码智能体如何工作的人来说都是一件大事，非常有用，尤其是来自像OpenAI这样的主要实验室。我还在一段时间前为他们的CLI贡献了一些改进，并一直在关注他们的发布和PR，以扩大我的知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI的&lt;a href=&quot;https://github.com/openai/codex&quot;&gt;源代码&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/openai/codex/issues&quot;&gt;缺陷跟踪&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/openai/codex/pulls&quot;&gt;修复历史&lt;/a&gt;&quot;可以在GitHub上找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/codex-agent-loop/&quot;&gt;https://www.infoq.com/news/2026/02/codex-agent-loop/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</link><guid isPermaLink="false">https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</guid><pubDate>Fri, 06 Feb 2026 01:06:01 GMT</pubDate><author>作者：Anthony Alford</author><category>AI&amp;大模型</category></item><item><title>Agent原生模型时代开启！阶跃Step 3.5 Flash上线，2天登顶OpenRouter全球趋势榜</title><description>&lt;p&gt;从 chatbot 到 Agent，大模型以「缸中之脑」为起点，正在悄然进化出属于自己的四肢百骸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 Agent 应用狂飙突进的同时，各种安全事故也层出不穷。初具雏形的 Agent 应用，正在急切呼唤一个更聪明、更可靠的「原生大脑」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;爆改基模结构，开启 AI 模型「Agent 原生」时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 时代，由于外部工具和任务重试需求等因素的介入，令上下文长度相比 coding、chatbot 等应用场景，迎来了一轮暴涨。同时，用户对即时性也有了更高的要求。相比 chatbot 时代，吐字比阅读速度快的基本诉求，等待 Agent 工具交付结果的时间，必须被进一步压缩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，上一个时代的 Reasoning 模型，已经不能再适应本时代的需求。一个好的 Agent 原生模型，在推理成本、速度和智能水平三个层面，都必须再次迎来进化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，阶跃星辰新上线的 Step 3.5 Flash，可谓「多快好省」：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了满足 Agent 时代的诉求，Step 3.5 Flash 从基础模型层面，就采用了十分独特的结构设计。作为一款旗舰级语言推理模型，它并未盲目追逐模型尺寸，而是选择了稀疏混合专家（MoE）架构。总参数量为 1960 亿，每次推理仅激活约 110 亿参数。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Step 3.5 Flash，将传统的 Linear Attention（线性注意力机制），打散为滑动窗口注意力（SWA）+ 全局注意力（Full Attention）3:1 的混合架构。如果要找个比喻的话，这种结构，十分接近推理小说的阅读体验：大部分注意力依旧集中在当前段落附近的文本，但当一个伏笔回收时，几章之前埋下的剧情钩子，仍然能快速的浮现出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，在模型技术层面，Step 3.5 Flash 还使用了 MTP-3「多 token 并行预测」机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说传统大模型，是一个词接一个词的“文字接龙”，那么 MTP-3，就像是先打草稿，再深入润色。在 Transformer 主干之后，MTP-3 会附加一个专用的预测网络层，让模型根据当前上下文同时推断多个未来 token 的概率分布。这样的设计，在保证因果一致性的前提下，实现了多 token 的并行推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构精巧，推理速度可达每秒 350 个 token&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多方加持下，Step 3.5 Flash 拥有了高达 256K 的超长上下文，和十分夸张的推理速度。在单请求代码类任务上，Step 3.5 Flash 最高推理速度可达每秒 350 个 token，确保了复杂 Agent 任务的低延迟响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和它的名字一样，「快」，是 Step 3.5 Flash 最显著的特点。但速度不能以牺牲智力为代价。在推理速度狂飙突进的同时，它的逻辑能力，同样不容小觑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在例行刷榜环节当中，Step 3.5 Flash 拿下了 AIME 2025（美国数学邀请赛）97.3 分； IMOAnswerBench（国际奥林匹克数学基准测试）85.4 分；HMMT 2025（哈佛 - 麻省理工数学竞赛） 96.2 分的好成绩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与国内顶级开源模型相比，上述项目得分，Step 3.5 Flash 均为第一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缩放定律似乎暗示我们，模型的能力，直接和尺寸挂钩。但 Step 3.5 Flash 用事实证明，合适尺寸 + 充分的后训练，完全可以兼顾速度与效率，得到一个精致、且有强逻辑内核的大模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;抛弃「规模迷信」的背后，是阶跃星辰对大模型的独特理解：模型应该凝缩「逻辑」，而非用超大规模，简单地对文本模式死记硬背。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;「高智商」，才是硬道理&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种认知的回报，在真实世界的任务当中体现的尤为明显：coding 榜单当中，Step 3.5 Flash 拿下了 Terminal-Bench 2.0（终端任务自动化），和 LiveCodeBench-V6（实时编码调试）国内开源第一的好成绩，整体测试水平属于全球第一梯队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 相关的测试项目更是手到擒来：τ²-Bench（多步任务规划）88.2 分 ；xbench-DeepSearch（深度搜索与信息整合）54 分，均为国内开源模型第一。BrowseComp（网页浏览与上下文管理） 69 分，实现了对海外御三家模型的成功反超。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更大的认可，来自 AI 社群：在真实世界任务中，Step 3.5 Flash 以高达 167 Tokens/s 的推理速度，发布首日，即进入全球知名 AI 模型聚合平台 OpenRouter “Fastest Models”速度榜前列。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59c9811f6b2e269396c23c869dda8092.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;发布 2 天，登顶 OpenRouter 全球趋势榜（Trending）榜单。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3ac960a31f6c1ba5bb59f36506f1fdc7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为汇聚了 OpenAI、Anthropic、Google 等主流模型的 API 平台，OpenRouter 的全球趋势榜单，实时反映着开发者在实际应用中的模型偏好与付费选择。此次登顶，意味着 Step 3.5 Flash 在真实任务当中的表现，已收获了全球 AI 开发者的积极认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reddit、X 等平台上也有不少用户，对 Step 3.5 Flash 的表现给出了很高的评价：多语言混用时切换自然，很少出现同尺寸模型身上常见的「夹杂」情况；行事稳定可靠，幻觉率极低，且对自身的能力边界有着清晰的认知，不会为了强行接话而编造答案。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0e5a3fe02842de4580546d5c494aad1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd00075cfe8f5c90bfc776436c11419f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/758555dec73f0b5d2c4bf3c3b88a451b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而这一切，都发生在一台 128G 内存、M3 Max 芯片的 mac 电脑上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本地 Agent，从此平权&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据社区反馈，借助 llama.cpp，Step 3.5 Flash 在 mac 平台上的推理速度极佳。平均速度 35 tokens/ 秒，约为该平台理论最大效率的 70%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;某种程度上，这是阶跃星辰 CTO 朱亦博「私心」的结果：他希望这个模型，能支持 4-bit 量化后，运行在 128GB 内存的 MacBook 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 Step 3.5 Flash 最终发布时的支持范围远不止于此：云服务层面，包括华为昇腾、沐曦股份、壁仞科技、燧原科技、天数智芯、阿里平头哥等在内的多家芯片厂商，均已率先完成了对 Step 3.5 Flash 的适配工作。同时，经过 4-bit 量化以后，Step 3.5 Flash 也支持在 NVIDIA DGX Spark、Apple M3/M4 Max 以及 AMD AI Max+ 395 等主流个人 AI 终端上，进行本地部署——同时依然保持着 256K context 的超长上下文能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦博在博客文章里不无自豪地表示，这是你在 128GB 内存的 Macbook 和 DGX Spark 上，用 4-bit 畅快跑 256K context 的最强模型，没有之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 模型的又一个「中国时刻」？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去的一年中，来自中国的开源模型，用更低的获取门槛、推理成本和打平的性能，一举击碎了“超大规模 + 闭源 = 先进”的行业迷信，无数 AI 应用因此涌现，也将大模型竞争，重新拉回了效率与架构创新的主航道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，国内几家 AI 公司动作频频、传闻不断，今年大模型领域的「春节档」，注定热闹非常。而最近发布的 Step 3.5 Flash，或许正悄然复刻又一个 AI 领域的「中国时刻」——高性能、低门槛、新范式。只是这一次，范式转移的焦点，从“推理模型”转向了更具颠覆性的“Agent 原生（开源）基座模型”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当行业还在用稠密模型硬扛 Agent 场景时，它用 1960 亿总参数、仅 110 亿激活参数的精巧架构，同时解决了 Agent 时代的三大死结——超长上下文下的低延迟响应、复杂任务中的高幻觉风险、以及终端设备上的本地化部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当海外巨头将 Agent 能力锁死在云端 API 时，Step 3.5 Flash，让 256K 上下文的 Agent 大脑，跑在 128GB 内存的 MacBook 上——这是对 AI 权力结构的重构：Agent 的智能不应被云厂商垄断，开发者理应拥有在终端侧构建私有化 Agent 工作流的自由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“终端平权”逻辑，恰是此前中国 AI 大模型引领的范式转移，在新环境下进一步的延续与深化：从模型获取的平权，进阶到 Agent 能力的平权。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;历史从不重复，但常常押韵。如果说之前的国产大模型，打破的是“对规模和闭源的迷信”，那么 Step 3.5 Flash 正在击碎的，就是“速度与智能不可兼得”的新迷信。当行业还在用“参数量”“榜单分数”这类旧范式衡量模型价值时，Step 3.5 Flash 已用 OpenRouter 趋势榜登顶、Reddit 开发者自发安利、多芯片厂商 Day 0 适配的事实证明：真正的范式转移，永远始于真实世界中，解决真实诉求的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们或许正站在 Agent 时代的分水岭上：过去一年，市场狂热追逐 Agent 应用层的“四肢百骸”，却忽略了为其注入灵魂的“原生大脑”。而 Step 3.5 Flash 的此时此刻，又恰似 2025 年春节的彼时彼刻——尽管暂时被 Agent 应用的喧嚣浪潮所掩盖，但历史终将被证明，在 Agent 时代，是阶跃星辰，完成了一次基础设施层，最关键的范式跃迁。&lt;/p&gt;</description><link>https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</link><guid isPermaLink="false">https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</guid><pubDate>Thu, 05 Feb 2026 10:33:53 GMT</pubDate><author>InfoQ编辑部</author><category>企业动态</category><category>AI&amp;大模型</category></item><item><title>贾跃亭发布人形机器人，最贵的24.3万元起</title><description>&lt;p&gt;今早，法拉第未来（FF）在美国拉斯维加斯发布了首批具身智能机器人EAI（Embodied AI）。基于EAI机器人产业“四化”趋势，FF推出 “三位一体” FF EAI Robotics生态战略、技术与产品，包括EAI终端、EAI大脑&amp;amp;开源开放平台、以及EAI去中心化数据工厂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7776ac475eadf60fc7bbbf9208c39ce.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，EAI机器人包括三大系列。其中，Futurist系列是全尺寸职业型具身智能人形机器人，全能专业的职业专家；Master系列是运动型具身智能人型机器人，全智懂你的动作大师；Aegis系列是安防和陪伴型专业四足具身智能机器人，标配四足结构，同时可选四轮版本；轮臂系列计划二季度发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9dfc3213071d2172632c23feccf6ff9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据其发布的图片显示，Futurist系列机器人定价34990美元（合人民币约24.3万元）起，Master系列机器人定价19990美元起，Aegis系列机器人定价2499美元起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa847b7a4a856e6d204ef3a43c5a49e4.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来应该是：360行，各有自己的职业机器人。”法法创始人、合伙人、首席产品及用户生态官，LeEco乐视创始人贾跃亭在公开平台表示，“虽然目前的FF还很弱小，但我们凭借永不服输永不放弃的精神，这些年积累的独特价值即将爆发出强大势能，会让我们从今年开始更快速成长壮大，推动一个具身智能的时代到来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/1609bbe21c817a688903f61b207ea2ce.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</link><guid isPermaLink="false">https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</guid><pubDate>Thu, 05 Feb 2026 10:24:30 GMT</pubDate><author>华卫</author><category>具身智能</category></item></channel></rss>