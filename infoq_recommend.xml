<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Fri, 23 Jan 2026 02:07:25 GMT</lastBuildDate><ttl>5</ttl><item><title>“AI工程师”已上岗！微软 CEO 曝正尝试新学徒制模式：内部工程师的顶级实践全变</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近的达沃斯论坛上，科技领袖们纷纷出来发表观点。当 Google 的 Demis Hassabis 和 Anthropic 的 Dario Amodei 在讨论更宏观的 AGI 话题时，微软 CEO Satya Nadella 与英国前首相 Rishi Sunak的对话，更聚焦在了AI应用的话题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya以自己参加达沃斯的准备工作变化为例，来说明在企业内部，AI 正在打破传统层级架构，让信息流实现扁平化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“自从我1992年参加以来，直到几年前，流程都没什么变化：我的现场团队会准备笔记，然后送到总部进一步提炼。但现在我直接找Copilot说，“我要见xxx，给我一个简介”。它会给我一个全方位的视角。”“我做的是立即把这个简介分享给所有部门的同事。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，企业 AI 应用呈现出明显的 “杠杆效应”：初创公司能从零开始构建适配 AI 的组织，落地速度更快；大型企业虽手握数据、资源优势，但传统工作流程与组织惯性带来的变革管理挑战更大。而无论大小企业，都需经历 “思维转变 — 技能培养 — 数据整合” 的艰苦过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人才方面，他认为全球 AI 技术人才与初创公司的质量已无显著差异：“雅加达、伊斯坦布尔的人才技术水平并不逊色于西雅图、旧金山。”真正的差距在于大规模应用的推进力度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya表示，判断 AI 是否存在泡沫，关键也在于落地应用：若仅停留在科技公司的技术讨论，泡沫风险确实存在；但当 AI 加速药物临床试验、提升农业生产效率、优化公共服务时，技术就已转化为实实在在的经济价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今天，Satya 参加 All-In Podcast的采访也发布了，这次谈话与Rishi那次比，有部分话题重合，但也更微观一些。他谈到，科技行业每十年换一批竞争对手是好事，能倒逼企业保持竞争力，科技产业蛋糕会持续变大，绝非零和博弈。而微软与OpenAI合作的核心逻辑：不押注单一模型，而是打造算力+应用服务器层的平台，兼容多模型生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还提到，公司内部全球网络团队已用AI Agent（数字员工）自动化处理光纤挖断、设备故障等DevOps重复工作，完全是自下而上的落地实践。此外还将LinkedIn等团队各角色合并为“全栈构建者”，重构AI产品工作流。现在，微软正在尝试新学徒制模式：由资深IC工程师带一组应届生，借助AI加速新人生产力爬坡，以适配AI时代的人才培养方式，新人仍需持续进入职场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;国际竞争方面，他认为，美国技术栈的核心优势是生态效应（平台之上生态收入远超自身收入），而非单纯市场份额，技术“扩散”是做大全球蛋糕，而非抢蛋糕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们翻译并整理了这次访谈内容，并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;移民政策下的一段“奇妙经历”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：今天非常高兴，能请到重量级嘉宾 Satya Nadella，Microsoft 的第三任 CEO，和我们的 AI 与加密领域负责人 David Sacks 来一场即兴炉边对话。Satya 出生在印度，大学毕业后来到美国，这一路经历本身就很传奇。你在书里写过，为了把太太接来美国，还专门“折返”了一趟。能不能简单和大家讲讲当时是怎么回事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：这件事其实是美国移民政策下的一段“奇妙经历”。我和太太在印度读的是同一所大学，后来我来美国读研究生，我们结了婚。我拿到了绿卡，但问题是由于我们是结婚后才申请，她反而不能直接过来。结果就是，我不得不放弃已经拿到的绿卡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最有意思的是，我去新德里的美国使馆，问工作人员：“请问放弃绿卡要排哪一队？”他们直接说：“没有这种队伍。”在九十年代，主动放弃绿卡绝对算是件“疯狂”的事。但为了让她能以 H1 签证过来，只能这么操作。好在最后一切都解决了，现在想起来更像是一段久远但有点荒诞的回忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：我想聊聊 Copilot。你们最早在 GitHub 上推出 Copilot，后来做到桌面端，再到直接把它放进 Windows，这对 Microsoft 来说是个非常大胆的决定。我每天都在用。但老实说，在它还不能真正理解文件系统、也没法和应用深度交互之前，市场反应不温不火。不过现在你们明显在持续加码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，面向知识工作者，AI 正在走向三种形态：一类是 Elon 在 xAI 做的那种“人类模拟器”，据说直接把“虚拟员工”塞进聊天和邮箱系统；一类是 Claude 刚发布的协作型Agent，强得离谱，很多人已经被震住了，我自己连续玩了四十多个小时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 Microsoft 的愿景是什么？知识工作者究竟该怎么真正把这些东西用起来？现在大家更多还是在“玩 ChatGPT”，这和真正创造商业价值之间好像还有一道鸿沟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：要理解这些不同形态，最好的切入口其实是编程，代码工作几乎是最典型的知识工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回头看这条演进路线：最早是“Next Edit Suggestions”，也就是智能补全。老实说，我对这一代 AI 技术真正建立信心，就是从早期 Codex 那一代模型开始的。那还是 GPT-3.5 之前，但补全已经相当准确了。后来我们有了chat交互，再往后是可执行的actions，现在则是全自主Agent。这些Agent既可以在前台，也可以在后台；可以在云端，也可以在本地运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，这些形态今天在编程中都有，而且你会全部用到，而非只选其中一种。比如我在 CLI 里，可以有前台Agent、后台Agent，同时直接在 VS Code 里改代码，这些全部并行进行。这说明了不同形态是可以组合的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这套放到知识工作上也是一样。我们是从chat开始，带推理的chat不只是一问一答，你能看到它完整的思考过程；现在到了actions阶段，通过模拟电脑操作、Skill和Agent 调用调用来执行任务，这就是Copilot 如今的状况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，其实需要一个新的“隐喻”来理解 AI 时代的计算机。Jobs 当年形容 PC 是“思维的自行车”；Bill Gates 说过一句我很喜欢的话：“信息触手可及”。但在 AI 时代，我们需要新的说法。我很喜欢 Notion CEO 的一个比喻：“无限思维的管理者”。这个说法非常形象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：确实是个很棒的产品。不过你们还没收购它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：还没有（笑）。但这个比喻点中了关键：你同时在和大量Agent协作。我自己还常用两个词：宏观委派和微观引导，即你把一整块工作交出去，同时在执行过程中不断给细节指令。写代码其实已经是这样了。这正是今天 Copilot 的真实状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一种我特别期待的形态，很快你们就会看到：开发者并不是只待在自己的 repo 里。我们要开会、写设计文档、实现别人写好的规格说明，还要保证代码和这些内容一致。这就意味着，Copilot 需要能通过 MCP Server 之类的方式，把我的工作流、待办事项、上下文全部拉进来。这才是真正的知识工作“组合”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;安全领域也是一样。一个安全工程师面对的是海量日志：把日志放进文件系统、用代码分析、生成仪表盘，这些都是 AI 能大幅放大的知识工作场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数字员工如何进入企业&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那“数字员工”“数字同事”这种概念呢？是不是也在你们的规划里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：核心问题其实是“身份”。我们推出了 Agent 365，就是把今天给人用的身份体系、终端防护体系，扩展到Agent身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：也就是说，你可以“克隆”一个我，让他在 HR 或市场部里工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：没错。在 Office 体系里完全可以做到。这里有两种模式：一种是，每个知识工作者都拥有“无限个大脑”；另一种是，创造完全独立于你个人身份的Agent。而身份这件事非常关键，权限、决策、责任追溯等全都依赖它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：说到底，就是搞清楚“谁对谁做了什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：正是如此。对任何组织来说，最重要的问题之一就是：工作是谁完成的、怎么完成的、来源是什么、能不能追溯，所以要么是“人 + 一堆Agent”，由人来做宏委派、微观引导，要么就是一个完全独立的身份在运作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：过去几年，Microsoft 的员工数量基本没变，但收入多了 900 亿美元，利润还翻了一倍。你们也像 Alphabet、Meta 一样，削掉了不少中间管理层。这是因为自动化？还是以前人确实有点多？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：你抓住了一个非常关键的问题。我认为，这是自 PC 普及以来，知识工作最大的结构性变化。想想 PC 之前，一家跨国公司怎么做预测？传真、内部备忘录满天飞，最后凑出一份结果。后来 PC 成了标配，Excel + Email，让流程和产出物全变了，今天正在发生同样级别的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，在 LinkedIn，我们以前有产品经理、设计师、前端工程师、后端工程师，后来我们把前面这些角色合并、扩大职责范围，统一成“全栈构建者”。这是结构性的调整，它改变了工作本身，也改变了工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：沟通成本一下就下来了，速度自然更快，一个人就能“vibe coding”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：没错，而且 AI 产品本身也有一套全新的工作流：从评测、到科学建模，再到基础设施。评测和产品由新的“全栈型 PM / Builder”完成，系统工程师负责支撑后端科学和基础设施，这是一个全新的闭环，必须从组织结构上去适配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，对 Microsoft 来说，我们不可能只活在未来。现在，我们要一边把 Windows 的热补丁做好、质量做到位；一边还要持续提升 Copilot 的评测体系和质量，这两件事都必须是第一优先级的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“每十年换一批竞争对手”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：这大概是你职业生涯里最具挑战性的阶段吧？过去 Microsoft 在很多领域是双寡头甚至垄断，但现在面对的竞争完全不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：确实非常激烈。但我一直觉得，每十年换一批竞争对手，其实是好事，它能让你保持“体能”。我 1992 年加入 Microsoft，那时最大的对手是 Novell；现在是 2026 年，环境完全不同。竞争很残酷，但从 GDP 占比来看，五年后科技产业一定更大，这不是一个零和游戏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：蛋糕在变大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：而且会大得多。整个技术栈对社会的影响会极其深远。最终的问题是Microsoft 的品牌定位是什么？客户期待我们提供什么？有时候我们会误以为，所有客户对所有厂商的期待都是一样的，但真正重要的是弄清楚客户“希望从你这里得到什么”。这其实是 Peter Thiel 那个观点的另一种表达：不是逃避竞争，而是通过理解客户，找到你真正不可替代的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这次在达沃斯，既有不少国家领导人，也有大量《财富》世界五百强公司的 CEO。昨晚晚宴上，有人问你一个问题：他们该如何看待 AI，怎样才能真正把 AI 用好。我记得你当时提到了“扩散（diffusion）”这个词，这一点和我最近参与的一些政策研究高度契合。能不能展开讲讲你的想法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：当然可以。事实上，你们一直在做一件非常重要的事，就是确保以美国为代表的技术栈，能在全球范围内被广泛采用、并且被信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回过头来看，技术本身只是起点，真正的价值来自于被大规模、深入地使用。我一直很喜欢一项研究，是 Diego Comin 做的，研究的是工业革命时期各国是如何实现领先的。结论其实很简单：那些把最新技术引入本国，并在此基础上做价值叠加的国家，最终跑得最快。说白了，不要重复造轮子，而是先用最先进的，再在上面持续创新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是“扩散”的意义所在。像 AI 这样的通用型技术，关键在于能不能真正铺开。就拿美国来说，技术我们已经有了，但问题是：它有没有进入医疗？有没有进入金融？有没有进入所有行业？不只是大企业，也包括中小企业和公共部门。如果看不到这种广泛而密集的应用，就谈不上真正的成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在我们正处在这样一个阶段：AI 正在更快地“扩散”。你们做的那些政策层面的工作其实非常关键。好消息是，技术已经成熟了，云计算和移动互联网这些“基础设施轨道”早就铺好了，这让 AI 的传播成为可能。现在真正的问题不在算力能不能拿到，而在于具体的应用场景是什么，以及组织如何管理随之而来的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在达沃斯，还有一个常被提起的问题：发达国家之外，全球南方怎么办？我反而觉得这里蕴含着巨大的机会。在很多全球南方国家，公共部门在 GDP 中的占比非常高。想象一下，如果 AI 能显著提升政府把纳税人资金转化为公共服务的效率，哪怕只提升一点点，那可能就是几个百分点的 GDP 增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我非常乐观，我认为会形成一种强烈的拉动力，而美国也应该把我们已有的技术栈，推动在欧洲、亚洲、南美、非洲等地广泛落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我经常被问到一个问题：这场 AI 竞赛，怎么判断谁在赢？或者美国是不是领先全球？我给出的答案很直接：看市场份额。如果几年后我们放眼全球，看到美国公司的技术占据了绝大多数市场，那说明我们做对了；如果看到全球到处用的都是中国的芯片和模型，那可能就意味着我们输了。说到底，使用情况才是最真实的检验标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我同意。但你也在 Microsoft 工作过几年，应该记得 Bill Gates 对“平台”的理解。对我来说，除了市场份额，更重要的是生态效应。美国一直以来的优势，不只是本国公司的收入规模，而是围绕平台形成的完整生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 Microsoft 学到的一点是，每次去一个国家访问，最先看的不是我们卖了多少软件，而是围绕 Microsoft 平台，在当地创造了多少就业岗位。比如有多少渠道伙伴、多少 ISV、多少相关的 IT 从业者。我们有一整套指标，衡量一个国家的生态是如何围绕平台建立起来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是美国技术栈过去在全球，包括在中国，能够被广泛采用的原因：当地公司能在上面构建自己的产品和业务。这种事情还会再次发生。所以你们推动“扩散”的工作，本质上不是在抢蛋糕，而是在把蛋糕做大，增强对平台的信任，从而带来真正的经济机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：你这么一说，我确实想起了一些往事。那还是十多年前，Yammer 被 Microsoft 收购，我们并入了 SharePoint 团队。当时产品经理们非常自豪的一点是：围绕 SharePoint 的生态收入，即非 Microsoft 的咨询公司、实施伙伴创造的收入，其规模是 Microsoft 自身软件收入的好几倍。Bill 也说过一句话：只有当平台之上的收入，显著超过平台自身的收入时，你才算真正拥有一个生态。所以，当我们谈“扩散”，希望美国保持领先地位，并不意味着这对世界其他地方是坏事。恰恰相反，其他国家和公司可以在这个平台之上创造出更大的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：完全同意。这一点非常关键。这不是“美国技术、美国收入”的问题，而是在用一个新平台在全球范围内创造机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我90年代做数据库产品时，和 SAP 有过深度合作。SQL Server 和 R/3 的结合，对双方都是巨大的成功。大家常提 Intel 和 Microsoft，但对我个人成长影响很深的一件事其实是和一家欧洲软件巨头的合作。放到今天也是一样，谁知道下一个伟大的 AI 应用会出现在哪里？我始终相信，即便基于美国的技术栈，世界各地都可能诞生顶级的科技公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;与OpenAI合作背后：所有公司、应用会同时用多种模型&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：你不仅是技术领袖，也是一位非常出色的并购操盘手，这一点其实被外界低估了。你和 Sam Altman、OpenAI 的合作，被认为既高明又充满争议。有人说，这笔交易可能让 Microsoft 获得巨额回报，但也有人质疑：你是不是亲手培养了一个未来最强的竞争对手？尤其是考虑到 Microsoft 过去错过了移动互联网浪潮，你们为什么不自己做一个 Gemini、xAI 或 Claude？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我理解这种疑问。很多人问我：你们自己的基础模型在哪里？从知识产权角度说，我们确实拥有相关能力，但更重要的是，Microsoft 现在的战略有几个层面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，我们要把“算力工厂”做好。Azure 是我们最大的业务之一，而随着 AI 的发展，它的市场空间会变得极其庞大，这要求我们在异构基础设施管理、软件调度和资源利用率上做到极致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，是应用服务器层。未来，每个人都在构建Agent，有强化学习环境、有评测体系，就像每一代平台都会有自己的应用服务器一样。我们现在在做的 Foundry，就是这个定位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一层里，有一点已经非常清楚：任何应用、任何公司，最终都会同时使用多种模型。为什么不用呢，甚至在一个具体任务里，编排多个模型协同工作，效果往往比单一的前沿模型更好。我们在医疗领域做过一个“决策编排”的实践，仅仅通过给模型分配不同角色再进行协同，就能显著提升结果质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那是不是可以理解为，你其实看好开源模型，认为大模型本身会逐渐商品化，真正的价值不在这里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我更愿意把它类比成数据库市场。最早大家觉得数据库就是 SQL，后来才发现并不是。关系型、文档型、NoSQL，各种数据库层出不穷，甚至出现了大量开源项目和围绕它们建立的公司。模型也会是类似的演进路径，会有闭源的前沿模型，也会有达到前沿水平的开源模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来一个非常重要的方向是：企业能否把自身的隐性知识，真正嵌入到自己掌控的模型权重中。有人问我未来会有多少模型，我的回答是：可能和世界上有多少家公司一样多。这听起来极端，但在我看来，这正是“知识经济”向“AI 经济”转变的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那你有没有在 Windows 桌面上，悄悄推进一个本地运行的大模型？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：其实已经在发生了，现在已经有完全驻留在本地、基于 NPU 和 GPU 的模型。高性能工作站正在回归，这本身就是一件非常有意思的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 明白了。所以 Microsoft 当然会重视 PC，这毕竟是你们的主场，有完整的桌面生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 是的，本质上这是个商业问题。我们一直认为“形态”非常重要。我常开玩笑说，我的职业生涯是从命令行开始的，说不定最后也会回到命令行。但不管怎样，形态一直在演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 你当年起步时用的是 Sun 那种最早的工作站，价格五千到一万美元。你能想象有一天，你会向客户推荐一台一万到两万美元的桌面机，里面内置 LLM 和强悍硬件吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 完全有可能。你可以插一张 DGX 卡，做出一台非常强的机器。其实在模型架构上，我们可能只差一次关键调整就能实现某种分布式模型架构，比如真正能自我调度的 MoE 架构。这类突破会彻底改变“混合 AI”该是什么样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但不管怎样，我们非常明确：PC 必须成为本地模型的最佳载体。本地模型可以承担大量 prompt 处理，再按需调用云端能力。这里面还有大量工作空间，这也是我们正在坚定推进的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David： 云与本地的协同已经证明了，能直接访问本地文件系统，本身就非常有价值。这让我想到 Yammer。很多人可能不知道 Yammer 当年最大的特点，是用消费级增长打法去攻企业软件。站在今天去看企业 AI 的采用，你觉得未来一年会怎么“扩散”？现在好像正处在一个关键点：会是自上而下，由 CEO 拍板、搞战略转型、走 RFP；还是自下而上，由一批 AI 原生员工先用起来，把工具带进工作中，做出惊人的成果？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 说实话，我觉得两种都会发生。自上而下的原因很简单：在客服、供应链、HR 自助这些场景里，AI 的 ROI 非常清晰，IT 和 CXO 很容易拍板，这也是目前最先落地的一波真实 AI 应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但最终真正改变组织的，一定是自下而上的力量。回看 PC 的历史也是这样：最早是律师把 Word 带进公司、财务把 Excel 带进来，后来有了邮件，最后才变成标配。现在正在重演这个过程。比如说 Agent，现在几乎所有人都在做 Agent，本质是在重构工作流，把大量重复、枯燥的事情自动化掉，这正是自下而上转型的起点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，我最兴奋的也是这种变化。以 Microsoft 为例，我们在全球管理着五百多个光纤运营点，尤其在亚洲。我自己以前都没意识到，这些所谓的 DevOps，其实很大一部分是物理资产：光纤会被挖断、设备会出故障。所谓 DevOps，很多时候就是在不停地发邮件问“这张光纤卡怎么了”“怎么修”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在负责全球网络的同事，已经构建了一批“数字员工”，本质就是 Agent 在自动处理这些 DevOps 工作。这完全是自下而上的：工具已经在那里了，我就用它来做自动化，减少重复劳动，提高效率和质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这些能力最终能不能规模化，关键不在“学会没有”，而在“用不用”。所谓技能提升并不神秘，就是在实际使用中完成的。工具扩散、工具被真正用起来，这才是最重要的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们在尝试新的学徒制模式”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 正因为如此，现在用这些工具去赋能现有员工，比招人、培养新人要容易得多。站在今天看，如果 Microsoft 规模不变，三、四十年后谁会接我的工作？你们是典型的技术优先公司，理论上已经没有太多理由继续增加员工数量，这几年你们也基本没扩张，只是在内部结构上做了调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那你怎么看下一代？对那些现在还没拿到 Microsoft offer 的应届生，你会给什么建议？以前你花了很多精力去培养这群人，但现在好像没那么“奢侈”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 这是个好问题。现在确实有争论：职业早期会发生什么变化、校园招聘还重要吗？我依然坚定相信校园招聘，因为 AI 会彻底改变一个人掌握代码库、建立熟练度的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，新人进团队的爬坡期很长；现在不一样了，有文档、有技能库，还可以直接问 Agent，本质上就像身边有一个极其强大的导师帮你快速上手代码。换句话说，应届生的生产力曲线会比以往陡得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也在尝试新的学徒制模式：让一位资深 IC 工程师带一组应届生一起工作，因为这本身就是一种全新的工作方式。以前大家进 Microsoft 后会去读 Dave Cutler 的代码，理解什么是顶级工程实践；而现在，顶级实践更多体现在十倍、百倍工程师是如何借助 AI 打造高质量产品的。对于这些经验，新一代毕业生会学得更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对 Microsoft 这样的公司来说，这是好事。毕竟只要人类还没解决“永生”问题，我们就需要新人进入职场、在 Microsoft 成长。所以我们依然会积极投入，只是会确保岗位的边界和内容，让其既符合现有员工的期望，也符合新入职者的追求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=5nCbHsCG334&lt;/p&gt;</description><link>https://www.infoq.cn/article/9ZTy7eW64XZMNOkZJV1r</link><guid isPermaLink="false">https://www.infoq.cn/article/9ZTy7eW64XZMNOkZJV1r</guid><pubDate>Thu, 22 Jan 2026 10:38:46 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>马斯克的底裤要被扒光了！超级爆料一个多小时， xAI 工程师被火速解雇</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Sulaiman Ghori 在一期播客中，用了一个多小时详细讲述了他在 xAI 的经历。他说，在那里“从来没有人对我说不”，每个人都被充分信任去做正确的事；只要是好想法，当天就能落地、当天就能得到反馈。他还提到，马斯克愿意被证明是错的，只要你能拿出实验数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他也坦言，在上一家公司，很多事情也许他一个人能做得更快；但在 xAI，整体反而更快，因为几乎没有官僚流程。这些话，听起来都是对公司的认同和马斯克的赞扬，实际上他还说自己是马斯克粉丝。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后，播客发出来后第3天，他被解雇了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;外界猜测是因为他说了太多敏感信息。节目中，他透露了利用闲置特斯拉汽车驱动的人类模拟器AI代理的计划、还有马斯克如何快速构建Colossus超级集群、xAI在模型策略上的核心决策，曝光了公司内部部署测试的AI虚拟员工等，还有xAI也被完全曝光。他坦率地谈到了激进的时间表、马斯克亲自参与的Cybertruck奖金计划、内部文化和运营方式以及一些非公开的策略，这些言论引发了外界的强烈反响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/728e59e8a2fe868f89affab28208d2d9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经历被玩梗：如何在1小时内毁掉你的一生，对应了最近x的爆文“如何在1小时内修复你的一生”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman自 2019 年起持续创业。在德国上大学一个月后退学，为了实现童年创办航天公司的梦想，在自家后院亲手制造过一台液体燃料火箭发动机。创业失败后，他进入xAI。对于他的经历，有网友表示，“这位兄弟跑去上播客，没拿到明确授权，就顺手把一堆内部敏感信息抖出来，这就是纯纯的新手行为。可以说，这是职业生涯级别的大忌。任何一家严肃的公司都会立刻把你原地开除，更别说是像马斯克这样的人。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们翻译并整理了他这期“超级爆料”的播客对话，并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在xAI，事情永远是“昨天就该完成”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：今天我很高兴能和 Sulaiman Ghori 坐下来聊聊，他是 xAI 的一名工程师。我从 2023 年马斯克刚开始搞 xAI 的时候就一直很关注这家公司，感觉它可能是史上增长最快的公司之一。你能不能跟大家讲讲，现在 xAI 到底在发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，我们几乎没有所谓的 deadline，永远都是“昨天就该完成”。基本没有什么人为障碍。马斯克一直强调要“追根溯源”，找到最底层、最根本的东西，不管是物理层面的还是其他的。我们通常会非常快地深入到那个层面，能多快就多快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在软件行业其实挺有意思的，因为你平时不太会把硬件这件事放在心上，但我们确实花了很多精力去考虑这些。而且严格来说，我们现在也不完全算是一家纯软件公司了，毕竟基础设施的建设占了很大一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，现在明显是被硬件限制住的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没错。硬件可能是我们最大的优势之一，因为在部署能力上，几乎没有其他公司能接近我们。不过，软件方面的人才密度也高得惊人，我从来没在任何地方见过这样的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得马斯克有一点特别厉害：他很擅长提前判断未来几个月、甚至几年后会出现什么瓶颈，然后从那个未来的瓶颈反推，确保自己现在就站在一个很好的位置上。这种思维方式在日常工作中是怎么影响普通工程师、AI 开发者的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常我们一旦要快速启动一个新项目，不管是我们还是他自己，都会先定一个指标。这个指标一般都非常核心，要么直接关系到财务回报，要么关系到硬件层面的产出，有时候两者都有。之后，所有事情都会围绕着这个指标来推进。而且我们不太接受那种“这事本来就不可能”的说法，就算真有极限，那也必须是一个扎根在最底层的、本质性的限制，而不是人为的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件行业里，尤其是过去十年做 Web 开发的人，往往会默认、接受很多所谓的限制，比如速度、延迟之类的。但实际上这些限制很多都是假的。技术栈里有大量没必要的开销和“蠢东西”，如果你能把这些清掉，很多系统都能直接提升 2 到 8 倍，至少是那些相对比较新的东西。当然，也有些老东西确实不好动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最近一次真正感受到“传统认知被彻底打碎”的经历是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：最近一次就是我们在 Macrohood 上做模型迭代。我们同时在做几种全新的架构，而且是并行推进的。现在我们几乎每天都会出新版本，有时候一天不止一次，有些甚至是从预训练阶段就开始重新来。这在业内其实非常少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这背后有几个原因：第一，我们有一支非常强的超算团队，他们解决了很多训练过程中常见的障碍。即便我们的硬件环境变化很大，但通常一个机架搭好后，一天之内就能开始训练，有时候甚至几个小时就可以。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这真的很不正常，一般不是都要好几天吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：甚至好几周。过去十年里，大多数人都是把这些事情抽象掉，交给 Amazon、Google 去管，他们给你多少算力你就用多少。但在 AI 时代，这种方式是行不通的。要么你死掉，要么你自己把这些东西建出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;入职初体验：没人管，做模型和产品默认资源到位&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：当初为什么加入 xAI，以及前几周入职体验怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我当时刚搬到湾区，在做自己的创业项目。那段时间，xAI 的联合创始人之一 Greg Yang主动联系了我。他真的很会招人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一开始收到邮件的时候还以为是垃圾邮件，因为那时候我经常收到那种“嘿，想聊聊吗”“我很欣赏你做的事情”之类的邮件。正准备删掉的时候，看到发件人的域名是 xAI，我一下反应过来：等等，这不是那帮人吗？当时他们大概成立了八个月左右，我就答应先聊聊。我们聊了好几次，我本来还想再试试别的机会，但后来发现时机不太对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个项目最后也没做下去，原因很明显：用一百万美元是不可能把 Macrohard 这种东西做出来的，但想法本身是对的。接下来六七个月，我基本是在烧钱，做各种航天相关的小项目，还试过一个“空气空间”相关的概念，后来也发现大概率行不通，但至少试过了。于是，我又给 Greg 发邮件，说能不能再聊聊。他直接回我：要不要明天面试？我说“好”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面试还算顺利，我周一就搬家，直接入职了。第一天真的没人管我，就给了我一台电脑和工牌。我当时想：那现在怎么办？我去找 Greg，说我连团队都没有，也没人告诉我该干嘛。他当初招我进来，更多是因为他认可我之前做的事情，也觉得和 Macrohard&amp;nbsp;的长期方向相关，但那时候 Macrohood 甚至还算不上一个正式项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来正好 Ask Grok 要启动，做和 X 的集成，他们问我能不能帮忙，我说当然可以。第一周我基本就是和另外一个人一起干活。但我很快意识到，在 xAI，你坐在工位上，甚至站起来一看，就能指着某个东西说：哦，这是那个人做的。这种感觉非常酷。而且我连固定工位都没有，就坐在当天没来的人桌子旁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那时候公司里人其实也不多吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，大概也就几百人，工程团队一百来号人。基础设施团队具体多少人我也说不太清，因为有些人是从其他团队慢慢转到我们正式编制里的。但整体规模确实比其他实验室小个数量级。当时我们刚做完 Grok 3。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：真的很酷。我特别喜欢的一点是，xAI 从成立到现在的速度实在太夸张了。我记得马斯克一开始还说，不确定在别人已经领先好几年的情况下能不能成功。结果你们第一个 Colossus 数据中心 122 天就建完了，这在行业里几乎是不可想象的。这种速度塑造了一种怎样的文化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：他让我们在做模型和产品的时候，可以默认资源是到位的。事实也确实如此，我们并没有被资源严重卡住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我们还是会把资源用到极限，但那是因为同时在推进二三十个、甚至更多事情。有大量训练任务并行跑着，通常是由少数几个人在推动。这也是为什么我们在模型和产品迭代上能这么快。而且这种速度让我们可以更长期地去思考。比如 Grok 4、Grok 5，其实在我加入之前、甚至 Grok 3 落地之前，规模和预期就已经设计好了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，至少提前一年在规划？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，而且你能感觉到，这些预期大概率是能实现的，因为团队整体非常可靠。这就极大地解放了你的思维，让你不用老是纠结“我会不会做不到”。举个例子，我们之前假设的最低延迟，其实比真正需要的高了大概三倍，而基础设施的建设让我们可以做到这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是什么意思？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们在做的一种新架构，如果没有足够高的实验频率，基本是不可能推进的，因为它完全不建立在现有研究基础之上。你需要全新的预训练体系，也需要新的数据集。这本身并不完全受制于硬件资源，虽然也有一些因素，比如 Tesla 计算平台的问题。这个其实已经是公开的了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在在想一件事：如果我们用 Macrohard 去做“人类模拟器”，那要怎么部署？如果要部署一百万个“人类模拟器”，就需要一百万台计算机，这怎么可能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果两天后答案就出现了：Tesla 车载计算机。它的资本效率非常高，我们可以在上面跑模型，甚至跑一个完整的人类工作环境，成本比在 AWS、Oracle 的虚拟机上，甚至直接买 Nvidia 硬件都要低得多。这让我们可以假设：我们能以更快的速度、在更大的规模上部署。所以我们也相应调整了预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，你们基本上可以直接利用汽车网络?&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：所以这其实是一种潜在的解决方案。简单来说，我们想要一百万个虚拟人（VMs）。仅在北美，就已经有大约 400 万辆特斯拉汽车。假设其中有三分之二，或者哪怕一半，已经配备了 Hardware 4。而且在 78% 到 80% 的时间里，这些车基本都是停在那里，要么闲置、要么在充电。那我们完全可以付费，让车主把车的算力时间“租”给我们。车本身已经有网络、有散热、有电力。我们可以直接在车上运行一个“人类模拟器”，也就是 Digital Optimus。这样一来，车主的租赁费用能被覆盖，我们这边则得到一个可以投入工作的完整人类模拟器。整个过程几乎不需要额外的基础设施建设，基本就是一个纯软件层面的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，这个资产本来就放在那里，你们只是把它用起来了，太厉害了。那从宏观层面看，这种“人类模拟器”规模化到几百万个，它的目的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：其实核心概念非常简单。Optimus 就是把人类能做的任何物理任务，让机器人自动完成，成本更低，而且可以 24×7 全天候运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在做的，是把这个逻辑复制到“数字世界”。凡是人类需要通过键盘、鼠标、看屏幕、做决策来完成的数字化工作，我们都可以直接去模拟人类的操作过程。完全不需要软件方做任何适配，也不需要改系统。只要现在有一个岗位是人类在用电脑做的，我们理论上都可以直接部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：挺有意思的。那具体会怎么推进、怎么落地呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们还没公开详细的落地计划，整体来说会是先慢后快。对我们来说在于，要么基础设施已经建好了，要么我们可以直接用特斯拉的网络，或者自己扩数据中心、测试算力。实际上，从一千个“人类模拟器”扩展到一百万个，差别对我们来说并没有想象中那么大，这反而不是最难的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克一个电话“救火”，个人“生死自负”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：马斯克最擅长的一件事，就是在公司里不断“救火”，哪里有问题就冲到哪里把问题解决掉。你有没有见过那种，本来是个大问题，但被他非常快地解决掉的情况？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有，最典型的就是基础设施建设，这是最大的一个。模型这边也有过一些小波折，但整体还算顺利。在模型侧，因为涉及很多非常底层、非常具体的算子，每一代 ASIC、CPU 都是为特定操作优化的，当我们引入新硬件，比如从 Nvidia 或其他厂商拿到新产品时，往往不是所有东西都能直接跑起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年年初有几次内部会议，他听到这些问题之后直接打了一个电话，第二天软件团队就给我们交付了补丁。我们几乎是并肩作战，直到问题解决，然后就能很快在新硬件上跑模型或训练任务，否则这种来回沟通可能要拖上好几周。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以很多这种“卡点”，真的就是一个电话就解决了。要么是我们主动提出来，要么他自己会问。经常在会议快结束、或者讨论暂时停顿的时候，他会突然来一句：“我能怎么帮忙？怎么能把这件事再加快一点？”然后就有人把问题抛出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道你们在并行做很多不同的产品，这在一定程度上是必须的。但在大多数组织里，同时推进多个目标，其实很难保持专注。你们是怎么做到多线并行还能高效执行的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，很多时候，是在全员会议或者大家私下聊天时，我们才真正搞清楚每个人在做什么、各个项目进展到哪一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如，我们当时做语音模型和语音部署，其实很多极低延迟的端到端能力早就已经在系统里了，从数据包发到客户端那一整套链路都准备好了。后来只是把正确的开关打开、解决一些冲突，延迟就直接降了两三倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种情况非常常见：在软件或硬件某个角落里，存在一个“很蠢”的问题，而恰好已经有人想好了方案。你可能是在翻代码库的时候发现，或者随口问一句，有人就会说：“哦，这个 XYZ 已经搞定了，你去找他就行。”基本不需要花太多时间对齐、同步、请示。提出一个想法，反馈要么是“这想法不行”，要么是“那为什么还没做完？”然后你就直接去做，事情就这么推进了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在马斯克的公司里，好像你只要主动要责任，就得“生死自负”。事情做成了就担负更多责任，做不成可能就出局。你的体验是这样吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，基本就是这样。我参与过很多不同的项目，大多只是因为有人找我帮忙，我就一直帮下去。结果到最后，我就成了某个模块、甚至一大块系统的负责人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对所有人来说都是这样。如果你在某个领域有经验，或者能非常快地推进事情，几天之内，这个组件就归你负责了。从“正式流程”上看其实挺混乱的。我在 HR 系统里可能还是挂在 voice 和 iOS 名下，安全系统甚至还以为我在做 X 的集成，从来没人更新这些信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，你进公司时并没有一个非常清晰的工作方向，就是先开始干活，然后不断在不同项目之间流动，谁需要你你就去哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：差不多是这样，会有很多重叠和流动。入职之后，我通常同时参与两三个项目，哪个最紧急、或者我能帮上最多忙，就会占用我大部分时间。然后项目之间会像瀑布一样自然切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那从入职到现在，你大概都做过哪些项目？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：一开始我做的是 Ask Grok以及相关集成，也和后端团队一起处理过可靠性和扩展性问题，当时系统规模增长得很快；之后我独立承担了桌面端套件的开发，把它做到内部可用的完整状；接着又被拉去帮做 Imagine 的发布，以及 iOS 相关工作。说真的，iOS 团队小得离谱，和用户规模完全不匹配，你绝对猜不到有多少人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：五个？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：三个。当时推出时，我正好是第三个。但大家都非常强。这是我第一次感觉到，自己必须拼命跑才能跟上整体的节奏和人才密度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你第一次真正感觉到“自己被充分使用”的时刻是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：肯定是Imagine的那次发布。我们基本是 24 小时一个迭代周期：晚上收到反馈，当晚就改；第二天早上再看新一轮反馈，接着马上修 bug、加大家想要的新功能。模型这边有新变化，我们也立刻跟进。整个节奏非常快，那可能是我连续每天都在办公室待着时间最长的一段时期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那段时间持续了多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：大概两三个月。那段时间几乎没有周末，但我反而挺开心的，也算验证了自己能扛住这种强度。之后我就被调去做 Macrohard 产品了，当时那边只有另一个人，一开始就我们俩。我从项目启动一直做到现在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;疯狂推进度，马斯克直接送Cybertruck&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：关于 Colossus 的建设，我不知道你了解多少。早期 xAI 团队为了把 Colossus 跑起来，在供电、算力、各种基础条件上都做了很多“疯狂”的事。到现在，其实还是到处是瓶颈，总觉得还需要更多芯片、更多 GPU、更快的速度。你当时的感受是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这一路上有太多“战争故事”，也下过不少赌注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：挑几个讲讲吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：好。我记得 Tyler 当时和马斯克打了个赌。我们在上新机柜的时候，具体是哪一代 GPU 我都忘了。马斯克说，“如果你能在 24 小时内用这些 CPU 跑起来一次训练，我今晚就送你一辆 Cybertruck。”结果那天晚上我们真的把训练跑起来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：他拿到了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：拿到了。现在从我们午餐的窗户望去就能看到那辆车，马斯克人挺酷的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到供电，其实我们必须和市政、电力公司还有州一级的电力机构高度协同。因为当他们那边负载飙升时，我们就得立刻切断公共电网，全部切到自备电源上——大概是八十台，甚至可能更多，用卡车拉来的移动发电机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整个切换过程必须无缝完成，不能影响任何正在跑的训练任务。你要知道，那些训练极其不稳定，GPU 和硬件的功耗可以在毫秒级别上下波动，动辄就是几兆瓦。这件事本身就非常夸张。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那是不是也是为什么你们会把巨型电池组直接放在数据中心旁边？这样负载上下波动就能更快响应？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。没有电池的话，很难这么快地调整负载，发电机毕竟是物理设备，你是在让一个真实旋转的东西加速或减速，它天然就有时间延迟，电池的反应速度要快得多。从物理层面看，整个链路是：本地电容、数据大厅侧的电容、电池、发电机，最后才是公共电网。当然，这套架构我们现在可能也在不断调整，尤其是散热这块，反应速度必须非常快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你还有没有那种“本来不可能，但最后居然成了”的故事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有。比如我们这块地的租约，从法律意义上讲其实是临时的。这样做是为了最快通过审批、尽快开工。我猜以后会转成永久的，但现在确实是短期租约。对数据中心来说，这是目前能把事情推进得最快的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：他们是怎么允许这种操作的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：算是一种地方和州政府层面的特殊豁免。你只是“临时”改造这块土地，类似嘉年华那种用途。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以 xAI 本质上就是个要来的“嘉年华”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：差不多就是这个意思（笑）。但正因为这样，事情推进得特别快。内部规划加建设，全程不到一个月就搞定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：规模接下来肯定会继续疯狂扩张。马斯克 也说过，能源会是最大的瓶颈，其次才是芯片。在这种很难预测未来一到两年项目和资源需求的情况下，你们是怎么做规划的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们会尽量从“杠杆率最高的目标”倒推。先想清楚：在某个时间点之前，我们最值得做的事情是什么。比如，如果我们想在某个日期前做到一千万甚至一亿美元收入，那从经济和系统设计角度，最有效的事情是什么？然后再倒推：需要什么软件、什么物理基础设施，最后一步步拆解。所以我们几乎不会从“硬件需求”开始，那通常是最后才考虑的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那是不是也有一套类似 SpaceX 的“让事情发生”的算法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：你是说那种“先删掉，再加回来”的逻辑？那确实一直都在用。我们经常先把某个东西砍掉，等确认必须要的时候再加回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最近一次这么干是什么时候？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：今天（指录制当天）。Macrohard 上部署大量变化极快的物理硬件，让测试变得很难，所以我们尽量减少下游的“特殊情况”。比如，我们要让三十年前的老显示器到最新的 5K Apple 显示器，全都跑在同一套技术栈上，结果发现并不是所有系统在任何时候都能愉快地配合。比如视频编码器，在某些层级上就得反复调。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我之前不知道，后来才发现，有些编码器对“最大像素数”是有硬上限的。所以我们一开始删掉了多编码器的特殊分支，后来在 5K 分辨率上撞墙了，又不得不把这个特殊逻辑加回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“优秀的人太多了，反而变得很难判断”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在你看来，xAI 本身有哪些特别值得讲的地方？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：首先是人，这里的角色非常多样；其次是我们的招聘方式也挺“怪”的。有些我原本觉得很蠢的做法，结果发现居然行得通，那我们就直接试。比如搞 hackathon，如果能从五百个人里挑出五个顶级选手，这件事就非常划算。他们未来给公司带来的预期价值，远远高于这次活动的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们前几天还算了一笔账，现在主仓库里，每一次 commit 的“价值”大概是 250 万美元。我今天提交了五次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你今天直接加了差不多一千两百万美元？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：轻轻松松的一天（笑）。确实不错，杠杆效应非常强。你用更少的努力和时间就能做更多事，因为身边的人和内部工具都很棒。还有我的老板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那什么样的人会想来这里工作？我听你描述，感觉第一天来的人就已经准备好周末、熬夜、全天候投入了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：大家刚来的时候都非常兴奋，非常有热情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：使命感驱动？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，但野心的类型不一样。有些人想往管理层走，看有多少人向自己汇报；也有人想“拥有”一大块技术栈。比如现在，我们在重构核心生产 API，基本上是一个人+20个 Agent 在做，而且做得非常好。你完全可以独立拥有代码库中的很大部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有点像 X 被收购之后那样，人很少，但每个人负责的范围巨大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：除了 hackathon，你们在招聘上还有什么不太常规的做法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们在 Macrohard 上推得非常猛。有两、三周的时间，我每周面试20多个人。有的只聊十五分钟，有的就是一整小时的技术面。优秀的人太多了，反而变得很难判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你怎么判断？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我有一道自己解决过的、非常具体的问题，是几年前在创业时遇到的一个计算机视觉问题。我会给候选人半小时去实现解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个问题本身其实很简单，但“简单得很有欺骗性”，大多数人都会想复杂。我特别看重一点：你能否不过度思考，给出一个朴素但有效的方案。因为我们的系统要跑在跨三、四十年的各种硬件、操作系统上，如果不保持简单，下周代码量就能膨胀到一千万行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你还会看重哪些杠杆能力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我喜欢会质疑需求、也会质疑我的人。这个方法我从 Chester Ford 那里学来的。&lt;/p&gt;&lt;p&gt;他在招聘时，常常会故意在题目里塞一个错误的需求、不可能的条件，期待候选人指出来。如果对方没发现，他就不招。我现在也这么干，效果非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们的节奏真的快到离谱。你自己也在做很多不同的事情，面对新任务时，怎么最快上手？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：要看具体是什么。如果是代码多，那就老老实实读代码，反复跳转定义，很快就能摸清楚。很多时候，实现代码比想象的要少。只有在高度活跃开发的模块里，才会同时存在二十个版本，你根本不知道哪个是主线，这时候就只能去问人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我惊喜的是，这里的人都非常开放、友好。我原本以为大家会很聪明、也很傲慢，但事实是：大家都很聪明，而且非常乐于帮忙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不怎么写文档，因为写文档的速度跟不上开发速度（笑）。现在我们也在尝试用 AI 自动生成文档。好处是，我们有几乎无限的算力和很聪明的 AI，可以大胆试各种“蠢办法”。在别的创业公司，这可能要烧掉几十万、上百万美元，但我们几乎是零成本。结果就是：实验更多、失败更多，但成功也更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克极限压缩时间，“办法总会有的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在实验这件事上，你们是怎么最大化“尝试次数”的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常都会有时间限制。我们经常在模型侧同时跑两、三个实验。有时候不是因为时间紧，而是因为两周后某个前置条件才会就绪：可能是硬件，也可能是数据。但今天你必须上线一个东西，那就先跑几种方案，看哪个今天就能交付、能产生收入或客户效果，两周后条件成熟了再切换。这种做法在 Macrohard 里是常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你有没有遇到过这种情况：按理说一个项目的周期应该拉得很长，但你们却压缩后提前了好几周甚至几个月完成？这种事经常发生吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：每次都是这样，无论是跟马斯克的会议，还是内部讨论，只要有人强力推动一件事，或者有外部的人——哪怕他并不对这件事负责——提出了新的需求、要求你把某件事做出来。我们一开始都会觉得，这个时间要求太离谱了。通常会花两分钟想一想、抱怨几句，然后剩下的时间就全部用来想：怎么在这个时间内把事情做完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到底，对完成时间的预估，永远建立在一堆假设之上。一旦时间被压到原来的二分之一、甚至十分之一，你就会回头看这些假设并问自己：这些假设对时间的影响到底有多大？然后你要么把它们砍掉，要么调整掉。这样一来，时间线立刻就能快一倍。你多做几次这样的优化，基本上任何要求都能满足。当然，最终还是会撞上物理极限，但一开始的时候，你离那个极限其实远得很。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道像完全自动驾驶、SpaceX 的火箭也是类似的情况。马斯克给的时间线通常都比实际要长得多，所谓的 “马斯克时间” 可能只有真实周期的四分之一或者一半。但正因为一开始把时间线定得这么激进，事情反而真的快了好几倍。xAI 这边是不是也差不多？虽然现在更多是软件，但哪怕在数据中心这类硬件侧，感觉进展也快得离谱，而且基本都落在他最初说的那个时间范围内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我觉得他自己也在不断校准他的时间判断。毕竟现在马斯克已经在大规模部署各种各样的硬件了，所以他的估算明显比以前准很多。而且他更新时间线的频率也更高了，有时候甚至每天都在变。他会跟我们不断沟通，根据不同的参数来调整进度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些变化甚至是他那边直接带来的，尤其是在基础设施层面。比如某个交易提前敲定了，或者某批设备可以提前排进生产，那就可能直接省下一个月、两个月，甚至更多，具体要看部署的情况。软件这边其实也是一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他一直说的一句话是：你完全可以试着用一个月去做一件原本要一年才能做完的事，最后你可能两个月就搞定了，但那也已经快得多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得在 SpaceX 的早期，有一种内部共识：马斯克说每拖延一天，就相当于损失一千万美元的收入。我不知道在 xAI 是什么感觉，你心里会不会也有一种直觉：如果今天没有再 push 一点、没有把事情往前拱一步，就等于损失了多少本可以创造的价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有的。至少在Macrohard这个项目上，我们确实有一些非常明确的收入目标。具体数字我不能说，但在我脑子里，只要一件事被延迟或者被加速，我几乎立刻就能算出来：我们刚刚是多赚了多少钱，或者少赚了多少钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也太夸张了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，数字会非常大。一方面是因为预期回报本身就极高，另一方面是时间线实在太短了。所以哪怕只是几天的变化，按比例来看，对收入的影响都已经非常可观了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：马斯克一直以“快速下重注”闻名。有没有那种在一次会议里，就做出了投入巨大资本、时间或者承诺的决定？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有一个非常典型的决定，就是在Macrohard 上，我们选择了一条路线：模型的速度至少要比人类快 1.5 倍，而现在看起来，实际速度远远不止如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在其他实验室，类似“人类模拟器”的尝试，更多是走“更强推理能力、更大的模型”这条路。但我们当时的这个决定，几乎是完全走在了和所有人相反的方向上。之后我们做的几乎所有事情，基本都是这个决定的下游结果。虽然不能说百分之百，但它影响了绝大多数事情，而且这个决定是在非常早期就定下来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在某种程度上也是一种共识，尤其是类比完全自动驾驶就很容易理解。没有人会等电脑花十分钟去做一件自己五分钟就能做完的事。但如果电脑十秒就能搞定，那我愿意为此付出任何价格。这其实是个非常直观的判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正常情况下，我们这些工程师可能会站出来反对，有二十个理由说明事情不能这么做。但当一个决定已经被拍板了，你只能从结果倒推路径，办法总会有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;没有AI研究员， 就是工程师&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得马斯克之前说过一次，好像是在 YC的活动上，他和 Gary Tan 做问答。Gary 提到 AI 研究员这件事，结果马斯克说不存在什么 AI 研究员了，现在全都是 AI 工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，我们跟他开过一次关于招聘的会，也有人提到过类似的话题，比如岗位描述之类的。然后他大概讲了十分钟，核心就一句话：工程师，就是工程师，别的都不重要。只要是好工程师，本质上是个会解决问题的人就行。不管你以前是做哪一块的，用过什么架构、做过哪种基础设施，这些都不重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：为什么“工程师”这么重要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：因为这样边界就被拉得很宽。意味着我们可以从很多不同背景的人里招人，现实中也确实是这样。AI 领域可能还不算特别明显，但 SpaceX 有很多这样的故事：有人来自你完全想不到的背景，按传统眼光根本不可能进来，但最后却在工程上做成了非常大的事情。所以定义宽一点，就等于给这些人留了一条路，也能帮助我们整体跑得更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“没人指挥你干这个、干那个”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那对你个人来说，在那工作最有意思的地方是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没人管我。真的，没人指挥你干这个、干那个。如果我有个好想法，通常当天就能自己动手把它做出来，然后拿去展示。看看合不合理，跑个评估，或者直接给客户看，给马斯克看，给相关的人看，一般当天就能知道这个方向对不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没有冗长的讨论，也不用等各种流程和官僚审批，我特别喜欢这一点。说实话，我从非常小的创业公司来更大的公司，本以为会牺牲一些自由度。我加入时公司 100 人,是我之前公司的 10 倍。但对马斯克的公司来说算小的，确实感觉很小的公司，没有什么繁文缛节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你进去之前，有没有什么特别大的预期，结果后来发现完全不是那么回事的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我原来以为会更“自上而下”一些，结果发现有一些，但不多。管理层级非常少，基本就三层：最底下是 IC，中间是联合创始人和一些新晋的经理，再往上就是马斯克，没有了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在每个经理下面的人都很多，事情反而很少是自上而下推动的。通常是我们自己先想出解决方案，跟经理对一下，马斯克点头，就直接干了。有反馈就再调整。整体比我想象中要“自下而上”得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：感觉就是在刻意设计一种状态，让所有人都在做东西，管理者更少，真正的“建造者”更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。我刚加入的时候，几乎所有经理都还在写代码。现在有些人下面管着上百号人，写得少了一点，但总体上，大家还是工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得第一周，有天吃晚饭，一个人坐我旁边。我就随口问他在哪个团队。他说他是做销售的，主要负责企业客户。我当时还想，“哦，原来是销售。”结果，他接着跟我讲他最近在训练的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没错，销售也是工程师。销售团队全是工程师，几乎每个人都是工程师。那会儿公司里，可能真正不算工程师的人不到八个。即便如此，大家也都是在为同一台“机器”做贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以是不是更像这样：一个工程师负责一个项目，可以直接面对客户，理解他们的问题，然后快速实现解决方案？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，而且层级越少，信息损失就越小。本质上是信息压缩的问题。语言本身就是有损的。如果信息要从客户脑子里变成语言，再进销售脑子，再变成语言、再到经理、再到工程师，每过一层，就像传话游戏一样丢一大截。如果你能尽量减少层级，那就只剩下一次压缩：客户直接告诉你他们要什么、体验是什么，然后工程师直接去解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有没有什么你以前在别的公司从没见过，但 xAI 在做的事情，能让事情推进得特别快？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：最让我意外的是团队之间、职责之间的“模糊性”。这在其他大公司，甚至规模差不多的公司里，都很少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如我要修虚拟机基础设施的一个问题，我就直接修，修完给负责那块的人看一眼，对方说 OK，马上合并、上线。几乎没有那种严格的边界，大家基本都可以改任何东西。当然，危险的操作还是有检查的，但总体上，公司是信任你的，默认你会把事情做对。这种感觉真的很不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得之前马斯克在搞 DOGE 的时候，删掉了一些防控措施然后又很快加回来了。在这种高速试错的过程中，有没有什么东西被删掉、又重新做回来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：几乎没有那种不可逆的破坏。我想不起来有什么东西是真的被永久性毁掉的。但像你说的，删掉、移除某个东西，然后有人说“我需要这个”，这种情况非常常见。可能一个小时后就回滚了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有那种情况，一个项目做了好几个月，依赖某块基础设施，结果等你真要上线的时候，那块基础设施已经被重构过三次了。那就再适配一次，继续往前走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得工程团队人这么少是件好事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：绝对是。人越多，反而越慢。一个人能做完的事，两个人来做，往往要花两倍时间，这在任何规模下都成立。尤其是现在，你已经不需要像以前那样写那么多代码了，更多是在做决策、做架构设计。每个人都可以是架构师，不需要那么多“手”，一个大脑能做的事情多得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前自己也尝试过创业，做过很多不同的项目。是什么让你决定来这里？使命感也好，文化也好，哪一点真正打动了你？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，我一直是马斯克的粉丝。小时候第一次看到猎鹰火箭回收着陆，那种震撼真的忘不了。我后来还专门跑去看了 星舰的第五次发射，那次是第一次成功“接住”，真的值回票价，是我这辈子见过最酷的事情。所以只要能参与任何跟这些事情沾点边的东西，对我来说就已经非常有吸引力了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你当初为什么选择这家公司，而不是 SpaceX 或特斯拉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：主要还是因为我骨子里就是个创业者吧。xAI 是这几家公司里规模最小、也最新的一家。我当时的一个判断，就是在这种体量的公司里，个人能产生的杠杆和改变会最大，事实也基本验证了这一点。因为从比例上看，你在公司里的“占比”更大。不是说其他公司不酷、或者个人不重要，而是这种比例带来的影响力不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，对决策产生影响的可能性要大得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：甚至不只是决策，而是从想法到落地、到看到结果，速度都非常快。我之前以为很多事情自己单干会更快，比如自己做某个功能、跑某个实验。但现实是，在 xAI 反而更快，因为已经有现成的基础设施和团队，很多我本来要手动完成的步骤，他们早就做过了，而且基本没人会对你说“不”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;内部AI虚拟员工&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前提到，公司里不同人、不同事情之间的边界其实挺模糊的。那你能不能随时去找其他同事帮忙？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：经常啊。基本就是走到别人桌前，直接说：“我有个问题。你现在在做什么？我能不能帮你一点？你能不能帮我这个？”大家都在同一栋楼里，这种事非常自然。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;挺有意思的是，我们后来在公司内部测试“虚拟员工”（human emulator），有时候甚至没提前告诉大家，所以就会出现这种情况：有个真人员工在干活，突然有人找他说“你能不能帮我做这个”，虚拟员工就回：“行啊，来我工位吧。”结果那人真的走过去，发现什么都没有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好几次我收到消息说：“组织架构里这个人向你汇报，他今天是不是没来？”但其实他是个 AI，是虚拟员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过整体来说，大家默认都是在同一栋楼、随时能联系到的。所以互相求助这件事非常频繁。我可以找别人帮忙，别人也经常来找我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在这些过程中，最容易“翻车”或者最让你意外的点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：主要发生在“人类行为模拟”这块，尤其是和客户一起做的时候。我们会尽量全面地理解客户的工作内容：先聊天、访谈，让他们讲，或者写下来他们是怎么做这份工作的。再过一周，我们回头看虚拟员工犯的错误，发现它总是在某些特定场景出问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候我们就去观察真人是怎么做的，结果发现真实流程里其实有二、三十个步骤，对方之前完全没提。我们一问，他们就说：“哦对，这一步我们是这么做的，刚才忘了说，不好意思。”这种情况太常见了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多事情在人脑里是默认存在的，全靠“自动驾驶模式”在跑。就像你开车开了一小时，完全不记得自己刚才是怎么开的。人类对任何重复性的工作都是这样，而我们想解决的正是这些问题：把人类现在反复做、其实根本不需要人来做的“蠢活”，全部替掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你是怎么决定“先解决哪一类问题”的？除了开车以外，人类还有哪些事情是天天在做、但其实没必要继续做的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：只要是电脑上的重复性工作，基本都在这个范围内。比如客服就是一个特别典型的场景：不断接收各种格式、各种内容的用户输入，然后把它们转化成一个标准化的处理流程。这样人类就可以去做更有创造性、更需要大脑的事情。&lt;/p&gt;&lt;p&gt;这&lt;/p&gt;&lt;p&gt;和编程领域发生的变化几乎是完全平行的：以前你要把同样的实现写二十遍，现在你用三句话描述一下，它就帮你搞定了，这是一次巨大的“压缩”。我们做的，其实就是把这种“压缩”，应用到所有数字化工作流上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在公司内部推这些“虚拟员工”的时候，除了“人不存在但被叫去工位”这种情况，还有什么让你觉得意外的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：意外的一点是，它的泛化能力比我们预期的强很多。有很多测试案例，模型根本没针对这个任务训练过，但表现却非常完美，远远超出我们的预期。因此，可以很确定地说，泛化效果真的比想象中好，而且我们现在还处在非常早期的阶段，之后只会越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这点其实和完全自动驾驶很像：有些场景并不在训练数据里，但车就是能正确应对。这本质上是一个“权重效率”的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克给反馈，要么宏观、要么细节&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你参加过几次和马斯克的会议？那种会议一般是什么样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话都挺简单的，而且我运气不错，大多数都进行得很顺利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 SpaceX 这种地方，成本和零部件细节特别重要。但在你们这里，他给反馈时会不会不太一样？比如不会去抠每个流程的细节？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：他的反馈通常要么非常宏观，要么非常微观，很少停在中间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;宏观层面上，可能是产品方向、客户判断，比如“只专注这个细分市场”“这件事完全不要做”。微观层面，尤其是算力效率、延迟这些问题，他往往会给出非常具体的建议，比如“试试这个方案”。而且他是愿意被证明错的，但前提是要有证据，必须做实验、看结果，而不是靠观点对喷。有些实验的结果甚至会出乎所有人的意料，然后我们就顺着那个方向继续走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以你们后来选择小模型，而不是一味堆大模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，小模型在算力效率上的选择，带来了很多改进。有些是直接的，有些是间接的。最直观的当然是响应更快。但更重要的是，特斯拉在自动驾驶上也发现了同样的事：模型小了，迭代速度就快得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不仅模型对环境反应更快，部署新版本的速度也快了。以前可能四周一次，现在一周一次。这又反过来影响了实验方式：为什么我们能同时跑二十个实验，其实就是源于这个早期决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那一开始的设想，是不是想直接上大模型？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：算是吧。我们确实想比所有人都快，但后来发现，“快”这件事的效果，被放大了很多倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“war room”真实存在&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：维基百科一直被诟病有偏见，马斯克也很关注构建一个“更接近真实”的替代体系。那你们怎么看待清理互联网来找到真相这件事?&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这是个极其困难的问题，因为互联网本身往往并不是所谓的“事实真相”。我们能做的，是尽可能往“底层原理”去钻，但这本身也很难。比如你问“宪法在物理意义上的底层原理是什么”，这其实很难有人真正给出一个严谨的答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但思路是类似的：尽量往下挖，再从那里往上构建。问题是真正这样写、这样做的资料并不多。比较接近的一个例子，是 James Burke 的《Connections》系列，他会把看似完全不相关的概念，通过物理和发明串联起来，非常有意思。我们想做的，其实是类似的事情，只不过这条路还很新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们是怎么找到更好的数据的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：数据并不是决定结果的唯一因素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我有时候会在 X 上看到有人贴出 Grok 的输出，说“这明显不对”，然后马斯克直接回复说“我们会修”，接着可能过了十二个小时、一天，他又说“好了，已经修好了”。这种事情发生时，内部一般是怎么运作的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常是他把哪里出问题了直接指给我们看，然后当时还醒着的人就会马上拉一个线程开始解决问题，一般先是个人处理，如果需要就再拉几个人。之后我们会做一次复盘，把到底哪里出了问题、以后怎么避免都讲清楚。原则上，犯一次错是可以接受的，但同样的错误犯第二次就很严重了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 SpaceX 的历史里，包括特斯拉，其实有过很多这种“冲刺时刻”。比如马斯克半夜突然出现，发一封全公司邮件，说大家都来公司干活。你们也有这种情况吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这种更多发生在做大模型的时候。就 Macrohard 这个项目来说，我们已经在“作战室”里连续干了四个月了，基本一直就是这种状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们门口是不是还真挂着一块牌子写着“war room”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，真的。最早那个作战室后来扩张了，我们就把东西全搬走了。有一次马斯克走进作战室，发现里面空无一人，就问“人呢？怎么回事？”然后他又走到我们现在待的地方，其实就是健身房，我们把健身器材全清掉，把人都塞进来了——然后他就在那儿开始一连串追问到底发生了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在那种很多事情被打乱又被迅速推进的夜晚，或者经历那种大规模冲刺时，是什么感觉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我最近正好看到 xAI 的一位联合创始人 Igor 发的一条内容。他人特别好，我也很喜欢跟他一起工作。他以前在StarCraft AI 工作，大概十年前吧，是我高中时尝试复现过的最酷的机器学习项目之一，难得要命，所以后来能和他一起共事真的挺神奇的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他说的一句话我特别有共鸣：有些时间里，感觉只过去了几天；但有些夜晚里，仿佛发生了几个月的事情。那天晚上就是这样。说“几个月”可能有点夸张，技术结果我们本来也可能几周内做到，但一晚上把它搞出来，冲击感非常大，而且真的熬了一个通宵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有没有那种情况，大家连续五天、甚至一整周都没怎么离开过办公室？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有的。模型冲刺的时候，经常会有很多人直接在公司过夜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：之前提到你们有五、六个睡眠舱，大家轮着用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，有睡眠舱，现在还有一些上下铺，条件差点，但至少能睡。后来帐篷那张照片传出来后，很多人都发给我。我只能说确实有帐篷，但我从没见过一次搭那么多。反正……确实挺极端的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成长经历：从小不服权威&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道你小时候做过很多不同的项目，好像还做过指尖陀螺。可能是在你房间里搞的？这种折腾、动手的心态，对你现在的工作影响大吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：影响挺大的。我很小就开始学编程，大概十一岁的时候，我爸给我买了一本书。我一开始觉得还行，但真正开始喜欢是在我意识到它能赚钱之后。我在网上认识了一些人，他们给游戏写脚本、外挂，然后卖一点钱。对我来说，能在网上赚到几百美元已经是天大的事了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：第一次有人给你钱，那种感觉真的很奇怪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：太疯狂了。我还记得当时得让我爸帮我弄一个 PayPal 的托管账户之类的，然后钱真的打进来了。对我来说，那简直是世界上最酷的事情。我干了几个月，攒了点钱，当时我对3D 打印特别着迷，RepRap 那套体系正火。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那其实就是一群大学生搞的项目，目标是造一台能打印出自己大部分零件的机器，所以才叫 RepRap。他们在不同大学里搞了一些实验室，从一台打印机开始，让它打印下一台的零件，一步步扩展。当然，这里面问题很多，他们也一直在解决，但那确实推动了后来的 3D 打印浪潮。我当时特别痴迷，就照着他们的零件清单，在阿里巴巴上把东西全买齐了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：然后呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：一个月后东西到齐了，我一晚上把它装起来，但过程其实挺惨的。我在拆电源的铜线，那是个非常不靠谱的电源，结果真的着火了。铜线全散开，有一根直接扎进我拇指里，大概有五厘米深。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：去医院了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没有。那是个上学的夜晚，已经凌晨了。我十三岁，动手能力也不行，在卫生间用镊子折腾了一个小时也没拔出来，最后我干脆把露在外面的剪掉了。接下来几周，它一点点往外长，我每天早上再剪一点。现在想想还挺离谱的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过打印机最后还是装好了。那时候正好赶上指尖陀螺爆火。我从中国买了一千个滑板轴承，在自己卧室里搞了个小工厂。晚上每隔两个小时起来一次清理打印平台，重新打印一批陀螺。白天上学前，我在车库里装轴承、喷漆、晾干，然后跑去其他学校的公交站，把货卖给“分销商”，其实就是别的学校的学生。他们白天卖，我放学后收钱，线上也卖、发货。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;生意做了两个月，最后被叫停了。官方理由是，学校餐饮公司有独家销售权，不能在校园里卖东西。但我觉得，他们主要是不爽我一边分散大家注意力，一边还赚钱。这事让我学到了一种“健康的不服从权威”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这种对权威保持距离，好像一直贯穿你的经历。你提到你不太信任机构，这种态度是怎么形成的？在你的人生里具体体现在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我从很小就知道，我想要的是一种不寻常的结果，而走一条常规路径，基本不可能得到。于是我本能地抗拒一切“惯例”，而机构的本质就是维护惯例。我觉得，几乎所有真正有创造力、有意思的成果，都是来自自由的人。至少在我看到的世界里是这样。所以，忠于这一点，对我来说才是正确的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢 John Carlson 的一个观点：所有东西都这么难造、难实现。看看周围，世界就是充满人们的激情项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，完全就是个奇迹。每一样东西背后都有故事，比你想象的要多得多。我记得以前读过 YKK 拉链的故事。你会发现，全世界真正做得好的拉链厂商就两、三家。拉链看起来很便宜，但机械结构其实挺复杂的。之所以能这么便宜、这么可靠，是因为有极少数公司、甚至可以说是极少数人，花了几十年把这件事做到极致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这几乎适用于所有东西。任何特别具体、又能大规模生产的东西，背后通常只有几家公司、甚至几个人在做。就像有时候你会听说，德国某个不起眼的小公司一停产，大众汽车整条产线都得停。疫情期间这种事就更明显了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在我们见面之前，你还做了一个液体燃料火箭发动机，我记得很小一个，你说是临时起意，二十四小时内点火的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：整个项目其实前后做了大概四周。一开始我就是买了一堆教材，研究火箭发动机的设计原理。和软件完全不一样，软件你可以上 GitHub 看别人的代码，但火箭没有现成文件。你得搞清楚材料特性、化学性质、怎么加工、参数怎么定，推力怎么估算，怎么避免超压。还有喷注器的设计，这个特别难，大概占了一半时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是最难的部分吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，喷注器最难，也是最后问题最大的地方。我花了三、四周时间，找中国工厂加急做了很多零件。那时候正好感恩节，我准备飞回东海岸看家人。我当时想，要么今晚把它装好、点火，要么就拖两周，然后我决定不能拖，就现在干。我早上灌了很多咖啡，一整天都在干活，搭测试架、装发动机，当晚就点火了。当然，为了能当晚完成，做了不少妥协。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我真的觉得特别好笑，你当时离它其实就几步远？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。我其实设计了远程点火，但问题是，用来给板载计算机供电的电源还没到，只能用笔记本通过 USB 供电。而我最长的 USB 线只有一米多，所以我只能站在旁边点火。我心里估计，大概有三成概率它会炸，或者喷得到处都是火。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;视频里其实能看到，我的外套着火了。因为喷注器设计不好，产生了很多超压，没完全燃烧的乙醇直接喷出来，溅到我身上就点着了。那件烧焦的外套现在还留着，当纪念品了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8jN60eJr4Ps&amp;amp;t=41s&quot;&gt;https://www.youtube.com/watch?v=8jN60eJr4Ps&amp;amp;t=41s&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0ds6wQgBpN7W2N6EL1iM</link><guid isPermaLink="false">https://www.infoq.cn/article/0ds6wQgBpN7W2N6EL1iM</guid><pubDate>Thu, 22 Jan 2026 10:35:34 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Salesforce将1,000多个EKS集群迁移到Karpenter，以提高扩缩速度和效率</title><description>&lt;p&gt;&lt;a href=&quot;https://www.salesforce.com/eu/?ir=1&quot;&gt;Salesforce&lt;/a&gt;&quot;已完成对1000多个Amazon &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;Elastic Kubernetes Service&lt;/a&gt;&quot;（EKS）集群从Kubernetes Cluster Autoscaler到&lt;a href=&quot;https://karpenter.sh/&quot;&gt;Karpenter&lt;/a&gt;&quot;的分阶段性迁移，Karpenter是AWS的开源节点配置和自动伸缩解决方案。这次大规模转型旨在减少扩展延迟，简化操作，降低成本，并为公司广泛的Kubernetes团队内部开发人员提供更灵活自助的基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对基于自动伸缩组的自动伸（Auto Scaling）和集群自动伸缩（Cluster Autoscaler）的限制，包括扩展速度慢、跨可用区利用率低以及成千上万的节点组的激增，Salesforce的平台团队构建了自定义工具来安全、可靠地自动化和管理迁移。这种方法结合了精心编排的节点转换和自动化，尊重Pod中断预算（&lt;a href=&quot;https://www.cloudbolt.io/kubernetes-pod-scheduling/pod-disruption-budgets/&quot;&gt;Pod Disruption Budgets&lt;/a&gt;&quot;，PDBs），支持回滚路径，并与公司的CI/CD配置管道集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移之旅始于2025年中期的低风险环境，并在2026年初投入生产之前经历了测试和验证阶段。Salesforce的工程师开发了一种内部Karpenter转换工具和补丁检查，可以处理节点轮换、亚马逊机器镜像（&lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html&quot;&gt;Amazon Machine Image&lt;/a&gt;&quot;，AMI）验证和优雅的Pod驱逐，从而实现了跨不同节点池配置的可重复和一致的转换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过这次转型，团队解决了操作上的挑战，例如配置错误的PDBs阻止了节点替换、&lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;&quot;标签长度限制导致自动化失败，以及Karpenter高效打包需要调整以防止单副本应用程序中断的工作负载模式。这些见解导致了改进的实践，包括主动策略验证和工作负载感知中断策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce报告了迁移后可衡量的操作和成本改善。通过采用Karpenter的动态配置模型，集群扩展延迟从分钟减少到几秒，通过更智能的打包提高了节点利用率，并显著减少了对静态自动伸缩组的依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于自动化流程取代了手动节点组管理，使开发人员可以自己声明节点池配置，操作开销减少了大约80%。这种加速的采用减少了对中央平台团队的依赖。此外，初步结果显示，2026财年成本节省了约5%，预计随着Karpenter的打包和现货实例利用率继续优化资源，预计2027财年将进一步减少5-10%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce的迁移突出了大规模Kubernetes操作中的更广泛趋势，其中传统的自动伸缩机制难以跟上动态工作负载和异构基础设施需求的步伐。Karpenter的实时决策、对异构实例类型的支持（包括GPU和ARM）以及与云API的更紧密集成，与集群自动伸缩器相比，实现了更快的响应和更高效的节点使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他从传统的Kubernetes自动伸缩向Karpenter等更动态的解决方案进行大规模过渡的组织面临了许多与Salesforce记录的相同结构性挑战。例如，&lt;a href=&quot;https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/pro/PRO303_Scaling-to-new-heights-Coinbase-migrates-to-Amazon-EKS-and-scales-10x.pdf&quot;&gt;Coinbase&lt;/a&gt;&quot;公开描述了其向Karpenter的转变，以处理具有变化需求模式的复杂混合工作负载集群，提到了扩展延迟和资源效率的改善，同时减少了静态节点组引起的操作摩擦。同样，&lt;a href=&quot;https://aws.amazon.com/blogs/industries/transforming-the-bmw-connected-vehicle-backend-with-karpenter/&quot;&gt;宝马集团&lt;/a&gt;&quot;分享了在其汽车平台上采用Karpenter如何更好地利用现货实例和工作负载感知调度，实现了更快的开发人员反馈循环和降低基础设施成本波动。这些案例呼应了Salesforce的观察，即集群自动伸缩器依赖于预定义的自动伸缩组和较慢的决策路径，可能会阻碍具有多样化和突发工作负载的环境的响应能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce迁移的不同之处在于其规模和自动化工具：转换超过1000个不同的EKS集群需要定制工具来处理策略验证、Pod中断预算限制、Kubernetes标签限制和舰队级别的增量推出自动化。其他公司报告了在个别集群或较小舰队中从Karpenter中的获益，但Salesforce的方法强调了在企业规模上可重复、自动化的转换，集成了回滚和合规性保障。在实践中，这意味着不仅要替换自动伸缩逻辑，还要协调工作负载模式、治理控制和全球平台上开发人员的自助服务期望。虽然这些迁移的最终目标是更快的扩展、更好的利用率和减少手动开销，但Salesforce的蓝图突出了将这些好处带给大型、生产关键环境所需的操作规程和自定义自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着企业越来越多地采用Kubernetes来支持关键任务服务，Salesforce的经验为其他考虑类似转型的组织提供了一个蓝图，证明了自动化、联合自动扩展可以带来性能、成本效率和开发者速度的显著提升——前提是必须有周密的规划和工具支持来支撑这一变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/salesforce-eks-karpenter/&quot;&gt;https://www.infoq.com/news/2026/01/salesforce-eks-karpenter/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/MJlz0Dv7QQPqf782oCPJ</link><guid isPermaLink="false">https://www.infoq.cn/article/MJlz0Dv7QQPqf782oCPJ</guid><pubDate>Thu, 22 Jan 2026 08:02:00 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category><category>大数据</category></item><item><title>每周工作100小时！谷歌DeepMind CEO揭秘：中国对手是字节跳动，断言谷歌是AI领域唯一全栈巨头</title><description>&lt;p&gt;“没有，从来都没有安心的时候。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 2026 年达沃斯世界经济论坛，DeepMind 创始人、Google DeepMind CEO 德米斯·哈萨比斯，用这句话形容过去三到四年的谷歌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;外界一度流行的“谷歌慢半拍”的言论，在他看来是一个彻底的误解。事实上，在这段时间里，谷歌的 AI 团队几乎一直处于红色警报状态。他本人长期保持着每周 100 小时、一年 50 周的工作强度，把一家万亿美元体量的科技巨头，硬生生拉回到创业公司的战时节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0fc6727cdbb957df06f1b507fb492a99.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这样的状态下，谷歌迎来了 Gemini 3 的发布，被哈萨比斯视为“重回行业最前沿”的关键节点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在接受彭博社记者 Emily Chang 的专访时，他罕见地系统性拆解了当下几乎所有 AI 世界的核心争议：&lt;/p&gt;&lt;p&gt;谷歌是否真的掉队？中国 AI 是否构成威胁？Transformer 和大模型是否已经走到尽头？AGI 会在什么时候到来？当工作不再必要，人类该如何寻找意义。&lt;/p&gt;&lt;p&gt;在哈萨比斯看来，过去十年，现代人工智能产业所依赖的关键突破，比如 Transformer 架构、深度强化学习、AlphaGo 背后的技术体系，几乎都诞生于谷歌与 DeepMind。他高度赞扬谷歌深厚的技术积累，他认为&amp;nbsp;谷歌是唯一真正具备 AI 全栈能力的公司，其真正的问题在于能否把研究、算力、数据、硬件和产品，整合成一个统一体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还高度赞扬了谷歌的科学研究氛围，认为这正是他当初选择谷歌作为 Google DeepMind 归宿的原因。他还透露了他与拉里・佩奇、谢尔盖・布林如何高效分工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在访谈中，哈萨比斯还反复提到一个关键词：物理 AI（Physical AI），他承认物理 AI 确实正处于突破的临界点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他的设想中，Gemini 从一开始就不是“聊天模型”，而是一个理解现实世界的多模态系统，是通往物理 AI 的入口。未来 Gemini 只会走向两个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随身的通用 AI 助手（眼镜、手机）真正能干活的机器人&lt;/p&gt;&lt;p&gt;当然，他也给出了冷静判断，距离物理 AI 跨过临界点还有&amp;nbsp;18 个月到两年的时间，在算法、数据、硬件等方面，都还差最后一段路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈到中国 AI，哈萨比斯的态度异常冷静。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并不认为 DeepSeek 构成真正意义上的“危机”，也直言西方舆论夸大了其算力效率优势，这背后仍依赖西方模型蒸馏。在他看来，中国公司极其擅长追赶，但是否能率先打开下一代技术前沿，仍有待时间验证。而&amp;nbsp;现代人工智能行业所依赖的约 90% 的突破性技术，都是谷歌研发的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他特别表扬了&amp;nbsp;字节跳动，给出了一个极具分量的评价：字节跳动距离技术前沿，大约只差 6 个月，而不是 1–2 年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位把 AGI 当作毕生使命的科学家型 CEO，几乎反驳了 马斯克、杨立昆和伊利亚·苏茨克维的核心判断，同时给出了一个异常冷静 AGI 的时间表：2030 年，有 50% 的概率实现通用人工智能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯对 AGI 有自己一套严格的标准，即必须具备完整的人类认知能力，尤其是科学创新能力，不仅能解决问题，还要能提出真正重要的问题&amp;nbsp;。&amp;nbsp;这其中还有不小的差距。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为距离 AGI，还需要一两项，最多不超过五项突破性技术，这可能体现在世界模型、持续学习的能力、稳定性表现、更强的推理能力或更长远的规划能力等方面。他高度认可现有的模型成就，认为在现有方法的基础上进行优化并扩大规模，或许就能实现 AGI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在访谈的最后，话题不可避免地走向未来社会：人工智能是否会取代人类的工作？围绕这一问题，哈萨比斯提出了一个有趣的概念&amp;nbsp;“后稀缺时代”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，AI 带来的变革，无论规模还是速度，都会是工业革命的十倍，取代部分人类工作几乎是不可避免的结果。但他厘清一个概念，即人工智能本质上是一种终极的科学研究工具，就像更先进的望远镜和显微镜一样，是为科学服务的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在哈萨比斯的设想中，真正重要的并不是“谁被取代”，而是人类将因此获得前所未有的自由，把注意力转向那些更根本的问题。例如能源危机，如何实现核聚变，如何发现全新的材料体系。这些长期困扰人类的难题，或许正是在人工智能的加持下，才第一次显露出被彻底解决的可能性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅是一场技术竞赛，更是一场文明级实验。真正的风险，在于当人类不再需要通过工作来定义自身价值时，我们是否已经准备好回答那个更深层的问题“为什么而活？”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在那个时代，人类或许需要的不只是更强的工程师，而是伟大的哲学家，去重新书写意义的来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是哈萨比斯访谈实录，更多的谈话细节，欢迎来看：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谷歌的红色警报期与“王者归来”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：和你上次来达沃斯相比，今年的感受有什么不同吗？Gemini 3 已经发布了，相关的消息我们也都听说了。我在内部甚至把这段时间称作“红色警报”。你觉得谷歌已经找回曾经的状态了吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不太确定这是不是该由我来评价，但我确实认为，过去这一年我们做得非常出色。我们付出了极其艰苦的努力，几乎是全力以赴，才让我们的技术和模型重新回到行业最前沿。&lt;/p&gt;&lt;p&gt;尤其是 Gemini 3，以及我们在视觉和成像系统方面取得的一些关键突破，都在这一过程中起到了决定性作用。同时，我们也逐渐适应了如今这种节奏极快、需要迅速将成果推向市场的行业环境，让整个团队重新焕发出一种更接近初创公司的活力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你认为人们是否低估了谷歌，或是对谷歌有误解？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：或许是吧，我不确定。我的意思是，我们一直都拥有站在这个领域前沿的所有必备条件，显然我们在这方面有着悠久的积淀。&lt;/p&gt;&lt;p&gt;我认为在过去十年里，谷歌和 Google DeepMind（谷歌深度思维）联手，创造出了现代人工智能行业所依赖的大部分突破性技术。比如 Transformer 架构，还有最知名的阿尔法狗背后的深度强化学习技术，这些都是我们的成果。&lt;/p&gt;&lt;p&gt;我们还有覆盖数十亿用户的优质产品矩阵，从搜索引擎、电子邮箱到谷歌浏览器，这些产品天生就适合融入人工智能技术。&lt;/p&gt;&lt;p&gt;问题只是如何将所有这些资源整合起来，以正确的方式统筹规划。&amp;nbsp;过去几年我们已经做到了这一点，当然还有大量工作要做，但我们已经开始看到努力带来的成果了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果你认为谷歌具备优势，你觉得这个优势有多大？能持续多久？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：在我看来，一切都始于研究。尤其是模型，要在各类基准测试中都保持行业前沿水平。这也是我们整合谷歌和 Google DeepMind（谷歌深度思维）后，首要聚焦的工作。双子座系列模型的进展，我们感到非常满意，当然这方面还有很多工作要推进。&lt;/p&gt;&lt;p&gt;但我认为，我们是唯一一家拥有全栈能力的机构，从技术、战术、流程体系，到硬件、数据中心、云业务、前沿实验室，再到一众天生适配人工智能的优质产品，我们一应俱全。&lt;/p&gt;&lt;p&gt;所以从根本的结构层面来说，我们本就该有出色的表现，而且我认为我们未来还有很大的提升空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我想知道，作为前沿模型研发的负责人，日常工作状态是怎样的。我看到有报道说，你大多在凌晨一点到四点进行深度思考。确实是这样吧？谷歌内部的工作状态是否一直处于红色警报级别？你有没有感到安心的时候？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：没有，从来都没有安心的时候。我们设定红色警报级别，本是针对特殊情况的，但过去三四年，工作强度一直大到难以想象。每周工作一百小时，一年工作五十周，这已经是常态。&lt;/p&gt;&lt;p&gt;在这个技术发展速度极快的领域，要想保持前沿，就必须这样做。行业的竞争异常激烈，可能是科技领域有史以来最白热化的阶段，而且背后的利害关系重大。通用人工智能的研发，无论从商业还是科学角度，都有着深远的意义。&lt;/p&gt;&lt;p&gt;再加上我们正做的事情本身就令人振奋，而我的热情就是用人工智能探索科学难题，推动科学发现的进程。这是我一直以来的梦想，我毕生都在为人工智能发展的这一刻而努力。所以常常会因为有太多工作要做而难以入眠，但同时，也有太多令人兴奋的事情值得我们去探索、去推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊聊谷歌目前的内部文化吧，你们既要在这场竞争中取胜，又要保证研发的方向正确。拉里・佩奇和谢尔盖・布林 现在的参与度如何？你和他们沟通的频率高吗？他们现阶段的工作重点是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：他们的参与度非常高。&lt;/p&gt;&lt;p&gt;拉里・佩奇更多负责战略层面的工作，我会在董事会会议上见到他，去硅谷时也会和他碰面。&lt;/p&gt;&lt;p&gt;谢尔盖・布林则更多参与具体工作，他甚至会亲自参与双子座研发团队的编码工作，尤其专注于算法细节方面。&lt;/p&gt;&lt;p&gt;他们能对当下的人工智能研发充满热情，这对我们来说是好事，毕竟这是计算机科学发展史上一个无比重要的时刻，单从科学角度来看，这也是人类历史上的重要时刻，所以所有人都想亲身参与其中，这一点非常好。&lt;/p&gt;&lt;p&gt;而对于我来说，我正努力融合各方优势，既保留初创企业快速推出产品、敢于冒险的活力，这一点我们已经看到了成效；又充分利用大企业的资源优势，同时还为长期研究和探索性研究保留空间，而非只聚焦于三个月内就能落地的产品相关研究，我认为只做短期研究是不明智的。&lt;/p&gt;&lt;p&gt;我正努力平衡这些因素，过去一年，各项工作的推进都很顺利，而且我认为今年我们能做得更好。我对目前的发展态势非常满意，谷歌的技术提升和研发进展速度，在业内应该是最快的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;物理 AI 的奇点时刻，还有 18 个月到两年的时间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我知道你一直把重点放在推动科学进步上，比如发现新材料。我们也看到，现在 Gemini 已经被整合进人形机器人系统中。那么你觉得，人工智能在真实物理世界中的应用，是否即将迎来一个类似 AlphaFold 那样的突破性时刻？如果是的话，这个“突破”会以什么形式出现？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：是的，过去一年我花了大量时间深入研究机器人技术。我确实认为，我们正处在物理 AI 取得突破性进展的临界点。&lt;/p&gt;&lt;p&gt;但我还是觉得，距离实现这一突破，我们还有 18 个月到两年的时间，还需要开展更多研究。&lt;/p&gt;&lt;p&gt;不过我认为，双子座这样的基础模型，为我们指明了方向。从一开始，我们就将双子座设计为多模态模型，让它能够理解物理世界，背后有多重原因。&lt;/p&gt;&lt;p&gt;其一，是为了打造通用智能助手，这种助手或许会搭载在&amp;nbsp;智能眼镜或手机&amp;nbsp;上，能够理解周边的现实世界。&lt;/p&gt;&lt;p&gt;其二，当然就是为了应用在&amp;nbsp;机器人领域。那么人工智能在物理世界的突破性时刻，究竟会是怎样的？我认为，那就是让机器人能在现实世界中稳定地完成各类有实际价值的任务。&lt;/p&gt;&lt;p&gt;目前，仍有一些因素制约着这一目标的实现。&lt;/p&gt;&lt;p&gt;一方面，算法还不够完善，需要提升鲁棒性，而且相较于实验室中仅处理数字信息的模型，机器人相关算法能依托的数据量更少，合成这类数据的难度也远高于数字数据。&lt;/p&gt;&lt;p&gt;另一方面，硬件方面也仍有一些难题尚未解决，尤其是机械臂和机械手的研发。其实深入研究机器人技术后，你会对人类的手部结构产生全新的敬畏之心，至少我是这样。进化的设计精妙绝伦，人类的手在稳定性、力量和灵活性上的表现，很难被复刻。所以在我看来，要实现这一突破，还有不少环节需要完善，但目前已有很多令人振奋的进展。&lt;/p&gt;&lt;p&gt;我们刚刚宣布与&amp;nbsp;波士顿动力&amp;nbsp;展开深度合作，他们研发的机器人非常出色，我们正将人工智能技术应用到汽&amp;nbsp;车制造领域。&lt;/p&gt;&lt;p&gt;接下来一年，我们会先推出&amp;nbsp;原型机&amp;nbsp;进行测试，或许一两年后，我们就能展示一些令人印象深刻的成果，并实现规模化应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;DeepSeek并不是重大危机，特别表扬字节跳动&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：一年前，DeepSeek 模型的发布在西方引发了不小的震动，很多人把它视为一场潜在的危机。但一年过去了，局势似乎逐渐平稳下来，中国方面的节奏看起来也有所放缓。你对中国人工智能领域整体竞争格局的看法，有没有发生变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：没有，其实并没有改变。一开始我就不认为这是一场真正意义上的危机，我觉得西方当时的反应多少有些过度了。&lt;/p&gt;&lt;p&gt;DeepSeek 的确是一个令人印象深刻的模型，它清楚地展现了中国科技公司的实力。&lt;/p&gt;&lt;p&gt;如果看头部企业，比如字节跳动，我认为他们的能力非常强。在技术前沿的跟进速度上，他们可能只落后大约六个月，而不是一到两年。DeepSeek 正是这一点的体现。&lt;/p&gt;&lt;p&gt;当然，围绕它的一些说法也被夸大了。比如关于&amp;nbsp;算力使用效率的说法，并不完全准确，因为他们在研发过程中借鉴并依托了部分西方模型，也对顶尖模型的输出结果进行了微调，而不是完全从零开始独立训练。&lt;/p&gt;&lt;p&gt;另外，还有一个关键问题目前仍然没有答案：那就是中国公司是否能够在跟进前沿的基础上，真正实现原创性的突破并引领下一代技术。&amp;nbsp;他们在追赶方面确实非常擅长，而且能力正在快速提升，但到目前为止，还没有证明自己能够率先打开新的技术前沿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AGI 的时间表：2030 年，有 50% 的可能实现 AGI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是你为通用人工智能给出了定义，你也曾说过，到 2030 年，我们有 50% 的可能实现通用人工智能。&amp;nbsp;这个时间规划是否依然不变？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：通用人工智能对你而言，依然是一个有价值的研发目标吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为是的，这个时间规划在我看来很合理，而且相较于一些人的预期，这个时间其实更充裕。&lt;/p&gt;&lt;p&gt;但我对通用人工智能的评判标准非常高，它指的是一个具备人类所有认知能力的系统，显然我们目前离这个目标还有很大差距。&amp;nbsp;这意味着，这类系统需要拥有&amp;nbsp;科学创新能力，不仅能解决科学领域的猜想和难题，更要能率先提出研究假设和问题。&amp;nbsp;任何一名科学家都清楚，找到正确的问题，往往比找到答案难得多。&lt;/p&gt;&lt;p&gt;目前的人工智能系统显然还不具备这种能力，未来能否拥有，还未可知，我们也仍未明确实现这一能力需要哪些技术突破。比如&amp;nbsp;持续学习能力，也就是在线学习能力，让系统能突破训练的局限，在现实世界中自主学习；还有&amp;nbsp;稳定性，目前的系统在不同领域的表现参差不齐，而通用智能系统不该有这样的短板。在我看来，要打造通用人工智能系统，还有不少关键能力亟待突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们来聊聊技术和未来的发展趋势。Meta 首席科学家&amp;nbsp;杨立昆（Yann LeCun）&amp;nbsp;认为，仅凭 Transformer 架构和大模型，无法实现通用人工智能。你是否认同这一观点？如果这些技术走到了尽头，我们的研发方向会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同，我认为说这些技术走到尽头的观点显然是错误的，因为它们目前已经展现出了巨大的实用价值。但在我看来，这是一个实证问题，也是一个科学问题，仅凭这些技术是否能实现通用人工智能，尚无定论。&lt;/p&gt;&lt;p&gt;我认为有 50% 的可能，只需在现有方法的基础上进行优化并扩大规模，就能实现通用人工智能，&amp;nbsp;这是有可能的，而且我们也必须这样做。在我看来，这项研究是有价值的，因为至少这些大模型会成为最终通用人工智能系统的核心组成部分，唯一的问题只是，它是否是唯一的组成部分。&lt;/p&gt;&lt;p&gt;我能想象，从现在到实现通用人工智能，我们还需要一两项，最多不超过五项突破性技术。&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;世界模型，这是我一直提及的，我们也正在研发，目前我们的 GENI 系统就是最先进的世界模型（GENI 是 DeepMind 、Google 内部正在研发的一类世界模型（World Model）系统），我也直接参与了这项研发，我认为它至关重要。&lt;/p&gt;&lt;p&gt;还有&amp;nbsp;持续学习能力，以及打造&amp;nbsp;性能稳定的系统，让系统不再出现这种领域间的表现失衡，真正的通用智能系统，不该有这样的问题。&lt;/p&gt;&lt;p&gt;所以在我看来，人工智能还缺乏更强的&amp;nbsp;推理能力、更长远的规划能力&amp;nbsp;等多项关键能力。目前尚未确定的是，实现这些能力，是否需要新的架构或突破性技术，还是只需在现有基础上继续优化。而谷歌和 Google DeepMind（谷歌深度思维）的做法是，双管齐下，既全力研发新的技术，也持续优化并扩大现有技术的规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：OpenAI 联合创始人兼前首席科学家伊利亚・苏茨克维（Ilya Sutskever）认为，依靠扩大模型规模实现技术提升的时代即将结束。你是否认同这一观点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同。他的原话大概是 “我们重回研究的时代”，我和伊利亚・苏茨克维是很好的朋友，我们在很多问题上的看法都一致，但在这一点上，我并不认同。&lt;/p&gt;&lt;p&gt;我的观点是，我们从未离开过研究的时代，至少谷歌和 Google DeepMind（谷歌深度思维）一直如此。&amp;nbsp;我们始终在研发领域投入巨资，而且我认为，整合后的谷歌和 Google DeepMind（谷歌深度思维），拥有业内最深厚、最广泛的研发团队。&lt;/p&gt;&lt;p&gt;过去十年，现代人工智能行业所依赖的约 90% 的突破性技术，都是我们研发的，当然最知名的是 Transformer 架构，还有深度强化学习、阿尔法狗背后的各类强化学习技术，这些都是我们开创的。所以如果未来实现通用人工智能需要新的突破性技术，我相信，就像过去一样，我们依然会是这些技术的研发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题，埃隆・马斯克说我们已经进入了技术奇点，你是否认同？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：不认同，我认为这一说法为时过早。在我看来，技术奇点其实就是实现完全的通用人工智能，而我之前已经解释过，我们目前离这个目标还相去甚远。我相信我们最终能实现这一目标还有五年的时间，从实现通用人工智能的角度来看，其实并不长，但在那之前，我们还有大量的工作要做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人工智能就像更先进的望远镜和显微镜&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是诺贝尔奖得主，我知道你一心想让人工智能推动科学研究的发展。如果未来人工智能本身取得了足以获得诺贝尔奖的科研发现，这个奖项该颁给谁？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为还是该颁给人类。当然，这取决于人工智能是否是完全独立完成这项发现。&lt;/p&gt;&lt;p&gt;目前来看，人工智能依然只是工具，在我眼中，它是终极的科学研究工具，就像更先进的望远镜和显微镜。&amp;nbsp;人类一直都在制造工具，让自己能更好地探索自然世界，人类本质上就是会制造工具的物种，这也是人类与其他动物的区别，而工具也让人类拥有了超越自身的能力，计算机当然也属于这类工具，人工智能则是这种能力的终极体现。&lt;/p&gt;&lt;p&gt;所以在我看来，人工智能一直都是推动科学研究的终极工具，而且在可预见的未来，科学研究都将是顶尖科学家与人工智能的合作成果：科学家提出富有创意的想法和研究假设，而人工智能作为强大的工具，助力提升数据处理、模式识别的效率，推动科学探索的进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 是否会取代人？我们将迎来后稀缺时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：谷歌是 Anthropic 人工智能公司的主要投资方，Anthropic 联合创始人兼 CEO 达里奥・阿莫迪 (Dario Amodei) 今天早些时候也来到了达沃斯。他预测，未来五年内，人工智能会取代 50% 的初级白领岗位，你是否认同这一观点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同，我认为这一过程会耗时更久。今年，我们或许能看到这一趋势的初步显现，比如初级岗位和实习岗位可能会受到影响，但要实现大规模取代，我们还需要解决人工智能系统的稳定性问题。&lt;/p&gt;&lt;p&gt;我把目前人工智能的这种不均衡表现称为&amp;nbsp;“锯齿型智能”，在某些领域表现出色，在另一些领域却不尽如人意。如果想将一整项工作完全交由人工智能代理完成，而非像现在这样，仅让其作为辅助工具，就需要让系统在各方面都保持稳定的表现。如果一个系统完成一项工作的成功率只有 95%，那是远远不够的，必须能圆满完成整个任务，才能让人放心地将工作交托给它。&lt;/p&gt;&lt;p&gt;所以在出现这种大规模的岗位变革前，我们还有大量工作要做，但&amp;nbsp;这种变革最终一定会到来。当然，一旦实现通用人工智能，整个经济体系都会发生改变，这早已超出了岗位变革的范畴。如果我们能打造出真正的通用人工智能，而且方向正确，我们或许会进入一个后稀缺时代，解决世界上一些根本性的难题，比如能源问题。借助人工智能，研发出全新的清洁、可再生的近乎免费的能源，比如实现核聚变。还有新材料的研发，我认为在实现通用人工智能后的五到十年，我们会进入一个彻底改变的世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：不过，在进入后稀缺时代之前，人们对这一过渡阶段充满了焦虑。我是一位母亲，我知道你也有孩子。你最担心孩子们未来会面临什么？你会和他们聊些什么？会告诉他们未来即将到来的变化吗？我听到很多人说，大学毕业生未来的就业会非常困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我倒不这么认为。我觉得我们即将进入一个变革的时代，就像工业革命那样，或许变革的速度会是工业革命的十倍，甚至难以想象。准确来说，变革的规模和速度都会是工业革命的十倍，影响力会是百倍。&lt;/p&gt;&lt;p&gt;但我想对所有人说，变革的背后，蕴藏着巨大的机遇。而且我始终坚信人类的创造力，我们的适应能力极强，因为人类的思维具有极强的通用性。&lt;/p&gt;&lt;p&gt;人类的大脑无比强大，我们的祖先以狩猎采集为生，而我们凭借这样的大脑构建了现代文明，所以我相信我们能再次适应新的时代。当然，这次的变革是前所未有的，因为它的速度太快了。以往，这样的重大变革往往需要一两代人的时间才能完成，而这次人工智能技术的变革，规模和影响力都极为巨大。&lt;/p&gt;&lt;p&gt;但对于如今的孩子，我会鼓励他们熟练掌握这些新工具，像使用母语一样运用它们，这些工具几乎能赋予他们超能力。比如在创意艺术领域，借助人工智能，一个人或许能完成过去十个人的工作。这意味着，如果你富有创业精神，在游戏设计、电影制作等创意领域有想法，就能完成更多工作，也能比以往更容易地跻身这些行业，成为新锐人才。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：一些人主张暂停人工智能的研发，让监管政策跟上技术发展的步伐，也让社会有时间适应这些变化。如果在理想情况下，所有企业、所有国家都同意暂停研发，你是否会支持这一做法？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我会支持。我也曾公开表达过我的期望，这也是我十五年来的梦想。我接触人工智能研究已有二十五年，我一直希望，当我们接近实现通用人工智能的这一关键节点时，全球的科研人员能展开科学层面的合作。&lt;/p&gt;&lt;p&gt;我有时会设想，成立一个类似欧洲核子研究中心的国际人工智能研究机构，让全球最顶尖的人才携手合作，以极为严谨的科学方式，推进通用人工智能研发的最后阶段，同时让全社会参与其中，不仅是技术人员，还有哲学家、社会科学家、经济学家，共同探讨我们希望从这项技术中获得什么，以及如何让它造福全人类。这才是我们当下的核心议题。&lt;/p&gt;&lt;p&gt;但显然，这需要国际社会的通力合作，因为即便只有一家企业、一个国家，甚至整个西方世界决定暂停研发，倘若没有全世界的共同参与，没有制定统一的最低标准，这一做法也毫无意义。而目前，国际合作面临着不小的阻碍，所以如果想以严谨的科学方式推进通用人工智能的最后研发，就必须改变当下的国际合作现状。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果到 2030 年我们实现了通用人工智能，而相关的监管政策尚未出台，我们是否注定会面临困境？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我依然乐观地认为，全球顶尖的人工智能研发机构会充分沟通，至少在安全和安保协议等方面展开合作，目前这方面的合作已经有了不少进展。比如我们和人工智能公司 Anthropic 在这些领域的合作就十分紧密。&lt;/p&gt;&lt;p&gt;如果国际层面的合作难以推进，这种行业内的同行合作就尤为必要。我和其他顶尖人工智能实验室的负责人关系都很不错，我认为，当利害关系足够重大时，大家会意识到问题的严重性和潜在的风险，而在未来两到三年，这一点会变得更加清晰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你当初本可以把 Google DeepMind（谷歌深度思维）卖给任何一家企业，而如今，这些研发人工智能的企业都在寻求大众的信任。尤其是在监管政策难以跟上技术发展速度的情况下，历史经验也证明了这一点。我们为什么该信任你？为什么你认为谷歌，也是你内心所认可的，是最值得我们信任的企业？毕竟人工智能的研发存在不小的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为，评判一家企业，要看它的实际行动，也要看参与相关研发的领导者的初衷。&lt;/p&gt;&lt;p&gt;我选择谷歌作为 Google DeepMind（谷歌深度思维）的归宿，有多个原因，最主要的是，谷歌的创始人创立谷歌的初衷，是打造一家以科学研究为核心的企业。&amp;nbsp;很多人都忘了，谷歌最初其实是一个&amp;nbsp;博士研究项目，是拉里・佩奇和谢尔盖・布林 的研究成果。所以我和他们一见如故。&lt;/p&gt;&lt;p&gt;拉里・佩奇主导了 Google DeepMind（谷歌深度思维）的收购，而谷歌的董事会成员也都是各行各业的顶尖人才，比如董事会主席约翰・轩尼诗是图灵奖得主，弗朗西斯・阿诺德是诺贝尔奖得主，这样的阵容在企业董事会中并不多见。所以谷歌的整体环境充满了&amp;nbsp;科学氛围，企业的发展以科学研究和工程技术为核心，这一文化早已根深蒂固。而追求最高水平的科学研究，就意味着&amp;nbsp;做事要严谨、深思熟虑，在所有领域都践行科学方法。&lt;/p&gt;&lt;p&gt;我认为这不仅适用于技术研发，也适用于企业的运营管理。所以我们始终努力做到深思熟虑、负责任，尽可能掌控我们推向市场的技术。当然，我们不可能做到尽善尽美，因为人工智能是一项全新、复杂且具有变革性的技术，但如果出现问题，我们会尽快调整修正。&lt;/p&gt;&lt;p&gt;最后我想说，谷歌想要为世界做的事情，也是我当初选择谷歌的原因之一。&amp;nbsp;谷歌的使命是整合全球信息，让人人皆可访问并从中受益，我认为这是一个非常崇高的目标。而&amp;nbsp;Google DeepMind（谷歌深度思维）的使命是破解智能的奥秘，并利用智能解决其他所有问题，这两个使命高度契合。人工智能与整合全球信息的工作本就相辅相成，谷歌的各类产品，从谷歌地图、电子邮箱到搜索引擎，都是对世界有实际价值的产品，人工智能能很自然地融入这些产品，为所有人的日常生活提供助力，我认为这是一件造福世界的事，能为此贡献力量，我感到很荣幸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：试想一下，在后稀缺时代，人们不再需要工作，当你实现了所有的技术目标后，你个人打算如何度过时间？毕竟到那时，科研工作本身或许也能实现自动化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：如果真的到了那个阶段，我想利用人工智能探索物理学的极限。&lt;/p&gt;&lt;p&gt;上学时，我最感兴趣的就是那些终极问题：现实的本质是什么？意识的本质是什么？费米悖论的答案是什么？（费米悖论是宇宙学和天体生物学中最经典的未解之谜，由美籍意大利物理学家、1938 年诺贝尔物理学奖得主恩里科・费米（Enrico Fermi） 在 1950 年提出，核心是 “理论上的地外文明存在性” 与 “人类实际观测证据为零” 的尖锐矛盾 ，其最经典的表述就是费米的一句反问：“他们都在哪儿呢？”）时间是什么？引力是什么？&lt;/p&gt;&lt;p&gt;我很惊讶，很多人每天忙于生活，却从未思考过这些重大问题，而这些问题一直萦绕在我心头，迫切想要找到答案。我想借助人工智能，去探索所有这些问题，或许还能在人工智能的助力下，利用新的能源和材料技术，实现星际旅行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果人们不再需要工作，我们还能找到生活的意义和目标吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：说实话，这一点比经济层面的问题更让我担忧。经济层面的问题，更多是一个政治问题：当人工智能为我们带来巨大的效益和生产力提升时，我们能否确保这些成果为全人类共享，这也是我一直坚信的理念。&lt;/p&gt;&lt;p&gt;但更核心的问题是，很多人从工作和科研中获得生活的意义和目标，在新的时代，我们该如何找到这些？我认为，我们需要&amp;nbsp;新一代伟大的哲学家，来帮助我们思考这个问题。或许未来，我们的艺术创作会更加精妙，我们的探索之旅会更加深远，就像如今我们所做的极限运动等非经济目的的事情一样，未来或许会有更多更小众、更有深度的这类活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：在场的所有人都想知道，自己该如何应对人工智能带来的变革。比如现在坐在达沃斯的会场里，十年后该如何自处？你认为，在场的人在看待人工智能这件事上，最容易犯的重大错误是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我想从两个方面来说。&lt;/p&gt;&lt;p&gt;第一，对于年轻人和我们的孩子而言，唯一可以确定的是，未来会发生巨大的变化。所以在学习技能方面，要做好持续学习的准备，学会学习，才是最重要的能力。要能快速适应新环境，利用现有工具吸收新信息。&lt;/p&gt;&lt;p&gt;第二，对于在场的企业首席执行官和商界人士而言，当下最重要的是，目前市场上有很多顶尖的人工智能模型和服务提供商，未来还会更多。要选择那些以正确方式研发人工智能的合作伙伴，与这些企业携手，共同打造我们所期望的人工智能未来。&lt;/p&gt;</description><link>https://www.infoq.cn/article/0TByYFFwWJi9u0xLobuU</link><guid isPermaLink="false">https://www.infoq.cn/article/0TByYFFwWJi9u0xLobuU</guid><pubDate>Thu, 22 Jan 2026 07:41:31 GMT</pubDate><author>高允毅</author><category>Google</category><category>生成式 AI</category></item><item><title>AI撞到“数据天花板”，一场革命正悄悄上演</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;撰稿：李文朋&lt;/p&gt;&lt;p&gt;编辑：王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两年AI发展很快，很多企业遇到的瓶颈也在变化：不再是“算力不够”，而是“数据跟不上”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月，IDC在《边缘进化：从核心到边缘驱动成功》报告中提到：已经部署生成式AI的企业里，超过60%的“实时交互类应用的响应延迟比预期高”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多时候，这种延迟不一定是模型慢，也不一定是算力不够，而是数据散在企业内部各处，口径不统一，质量也不稳定，关键时刻更是“找不到、拿不出、对不上、流不动”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;金融行业感受特别明显。一位城商行做数字化建设的负责人公开表示：“我们目前不缺算力，也不缺模型。缺的是能让模型真正跑起来的数据。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型训练成本在下降，但把数据整理好、清洗好、能实时用起来的成本反而越来越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年初，这个问题已经不只是“体验不好”，甚至会影响商业项目的成败。IDC在FutureScape里提醒称：今年，50%的AI驱动应用将会因为数据基础薄弱，达不到原定ROI目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，数据的重要性远不止如此，更长远一点看，甚至会关系到AI到底能走多远。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年云栖大会上，阿里巴巴集团CEO吴泳铭谈过一个判断：AGI大概率会出现，但只是开始。真正的下一步，是走向能自我迭代、持续变强的ASI。他把过程分成三段：先学会推理，再学会使用工具辅助人类，然后连接现实世界的数据，能自己学习、自己迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说得更直白一点：未来AI更像一个“持续在线的系统”，它得不断吃到最新的数据，并把这些数据变成新的能力。数据是否能高效、持续地进入系统，变得愈加重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正因如此，很多基础设施厂商开始关注“更适合AI使用的数据”方向。数据库不再是“存数据”，而是要让数据更容易被统一管理、被实时取用、被不同类型的模型和应用调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月20日，阿里云在2026 PolarDB开发者大会上发布了AI就绪（AI-Ready）云原生数据库新标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它想解决的事情其实很简单：让数据系统不仅能存储、查询多模态数据，还将直接驱动AI智能决策，让数据进入模型与业务的路径更短、更稳定，以及更安全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b084376de02531226517c5dabe52a7ec.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云资深副总裁、数据库产品事业部负责人李飞飞表示：“未来，AI原生数据库是技术演进的必然方向。从云原生到Al就绪，再到Al原生，PolarDB将持续深化AI与数据库的融合创新，加快走向超级人工智能时代。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业视角看，数据库已不只是业务系统的底座，开始逐渐变成智能应用能不能跑顺的关键部分。围绕“数据怎么被组织、被使用、被转化”的变革，已悄然上演。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第一部分：数据困境的背后：是新旧时代的“不兼容”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去很多年，企业做数据治理的“沉淀逻辑”只有一个：让人更容易做决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业务员、分析师、管理层要看的数据，通常得“对得上”“能解释”“表格整齐”。于是传统数据团队投入大量成本做ETL（清洗、转换、加载），把数据整理成一张张看起来清楚、口径一致的报表。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题是，现在数据的“主要使用者”变了：很多数据不是给人看，而是给模型用。这就会出现一种情况：对人很友好的数据，不一定对模型有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个常见例子是风控。在传统的数据整理过程中，为了让报表更稳定、更好讲，分析人员往往会把极端交易、可疑行为当成离群点删掉，觉得它们会影响整体判断。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但对模型来说，删掉这些样本的结果是：正常样本越来越多，异常样本越来越少；并导致欺诈、极端风险这些关键模式识别，几乎无法归纳学习。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，在AI时代，“干净数据”并不等于“高质量训练数据”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天很多企业说“数据资产不少，但模型效果一般”，背后往往是同一类问题——现有数据的组织方式，跟模型所需要的对不上——本质就是“兼容”问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在结构方面，企业现有数据多数是二维表格，字段清晰，适合报表和人工分析。但很多模型更需要的是向量、图结构、时间序列这些形式，用来表达关系、上下文和变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据的维度也不够。传统指标体系更强调“少而精”，字段要能解释能展示，但模型训练往往靠大量稠密特征。很多特征单看没什么意义，要组合起来才有价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据更新速度也慢。很多系统按天、按周更新数据，这对复盘、报表够用，但推荐、风控、运营决策这类应用，往往希望输入尽量接近实时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据格式也较为分散，不少业务系统以结构化数据为主，图像、音频、视频、传感器流等数据通常分散在各自系统里，管理不在一起，调用也不在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是看上去数据资产很多，但真正能直接拿来训练、推理的数据，比例并不高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家越来越接受一个现实：2026年，数据本身将决定AI模型的能力天花板。为了缓解上面的这些“对不上”问题，“AI就绪数据”（AI-Ready Data）应运而生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它想表达的不是一个新概念，而是一件很具体的事：数据要经过专门的整理、特征化和组织，以更小的工程成本直接用于训练、推理和决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI就绪数据，通常会包含几类要求：首先，特征要够用，不是“有数据”就行，而是要有足够细的维度，让模型有东西可学。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如做用户行为建模，只保留“总次数”“总金额”通常不够，还需要时间分布、品类偏好、渠道差异、设备类型等细节等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，标签也要准，需要监督学习的场景里，标签相当于“题目答案”。标签粗、标签不一致，都会拉低模型上限。这就要求，图像分割、文本抽取都要尽可能精确。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，样本要尽可能覆盖真实世界，因为现实业务不会只落在“平均值”上。所以实践中会强调覆盖长尾：高峰期、极端天气、罕见故障、少数群体、低频行为等。这些数据从报表角度不一定好看，但对泛化能力很重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，数据也要能跟着变化更新，很多传统的数据质量体系把数据当“静态资产”，但用于智能应用时，数据要像“动态输入”。常见要求包括：按合适频率引入新样本；对明显过时的数据标记或降权；根据线上表现迭代数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两年，很多企业在数据库和数仓之外，再搭特征平台；要实时就接流计算；要多模态就加向量库、图系统；最后再用调度、同步、API网关把这些拼在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种做法在试点阶段通常能跑起来，但场景一多、频率一高、数据类型一复杂，架构复杂度和运维成本就会上去。因此，越来越多的方法论开始强调：与其在旧框架上不断加组件，不如从底层重新规划面向智能应用的数据底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品层面，一些云数据库厂商正在调整定位：不只做“关系型数据库”，而是把自己当作智能应用的数据基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如阿里云云原生数据库PolarDB的产品理念，就强调在云原生架构上，配合湖库一体等能力，去支撑结构化、半结构化以及非结构化数据的统一管理，为“AI就绪数据”提供底层能力等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b40c88968d48f02667384403e292f2aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB还首次系统定义了“AI就绪数据库”的4大核心支柱，分别是：多模态AI数据湖库、高效融合搜索能力、模型算子化服务，以及面向Agent应用开发的后端服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是通过将多模态存储、搜索、推理和后端开发套件深度集成到数据库内核，满足企业多模态搜索、问答、数据处理、标注等需求，将复杂的异构架构简化为统一的智能化底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，AI就绪数据会越来越像企业的“基础配置”：这不是为了追趋势，而是为了让后面的应用能更智能、更高效、更安全地跑起来、跑下去。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第二部分：行业正想尽办法，让数据处理实现加速&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说“AI数据就绪”解决了数据能不能用，那么“数据处理速度”则决定这些数据能否“实时”产生价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过不少实践后，大家慢慢形成一个判断：同一份信息，发生在“刚刚”和“昨天”，对业务价值可能不是一两倍的差距，而是会差一个数量级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以淘宝为例，数据显示电商运营数据的实时监控能够让决策效率提升40%以上。某头部淘宝店铺通过自主搭建实时数据采集和分析系统，将数据延迟控制在1-5分钟后，运营效率和业绩直接提升30%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;风控领域的收益更明显。一次异常交易判断窗口往往只有秒级：秒级识别，损失只是几百元；第二天发现，可能已经数百万。对金融机构来说，实时数据不是“体验优化”，而是成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题在于：今天大多数企业的传统数据链路，并不是为“实时”设计的。最典型数据处理路径就是：从业务数据库，到ETL，再到特征平台处理，进行特征缓存，最后供模型调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这条链路长、环节多，每一步都会带来延迟。所以这两年行业里出现一个变化：大家开始关注能不能少搬点数据，少绕几道弯。因为数据在系统之间来回搬运、复制、同步，本身就是时间和复杂度的来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，很多数据“新架构”绕来绕去，其实想解决的是同一件事：让数据尽量留在一个更统一的底座上，把处理、检索、计算尽量在同一套体系里完成，把链路缩短简化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB这次讲的“AI就绪云原生数据库”，基本就是沿着这个思路在做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年企业反复提“湖仓一体/湖库一体”，说白了是因为两套系统各有短板：数据湖便宜、能存很多、数据类型也更杂，数据库查询强、事务能力好，可一旦规模大、成本就上来了，对大规模非结构化数据也不友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果就是数据经常搬来搬去：为了分析，把业务数据抽到湖里；为了在线服务，又从湖里挑一部分加工后装回库或特征仓。每搬一次，就多一次复制、多一次同步、多一段延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次，PolarDB发布的—AI数据湖库（Lakebase）解决方案，就是专为实现“湖库—体”架构而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI数据湖库尝试把结构化、半结构化，以及非结构化数据，都放在同一个平台里统一存取和处理，减少来回同步，让链路变短。与此同时，它还配了缓存加速能力，针对不同场景做I/O和带宽的加速，让海量数据在底座里流转得更顺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让数据从“产生”到“能用”的时间缩短，很多场景能从小时级压到分钟级，甚至更低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是加速的第一步：少搬数据。但湖库一体更多解决的不止是“搬运成本”，还有个更隐蔽、也更容易被忽略的卡点：推理路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统架构里，数据库只负责存储和查询，推理模型是独立的外部服务。这样做的结果是：应用需要先从数据库取特征，再送给推理服务推理，最后把结果写回或返回业务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一步看起来都不慢，但数据序列化、网络传输、排队等待加起来，延迟就会暴增。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB这次的思路不太一样：它不是把推理当成“外挂”，而是希望把推理内化为数据库的原生能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它的做法是，通过多模态引擎与独有In-DB模型算子化的深度集成，开发者可以在PolarDB库内直接完成语义检索与推理加工，在效率显著提升的同时，确保数据不出域，保障隐私合规。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体方面，通过LLM SQL接口封装阿里云百炼各类模型构建PolarDB模型算子，开发者在SQL里可以直接调用推理能力——不用数据出库，不用中间转换，一条查询就完成&quot;找数据→检索语义→推理加工→返回结果&quot;整个流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了支撑这套库内推理，PolarDB还对底层做了分层优化，创新性地融合了KVCache、图数据库与向量技术，构建了兼顾长短期记忆与低算力消耗的检索方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，AI数据湖库不再只是提供&quot;看数据接口&quot;，而是变成&quot;数据和模型直接对话的场所&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，要让推理少绕路，还有个前提：数据库要顶得住Agent的高频访问。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent在执行任务时，可能会发起大量查询来验证和规划，如果数据库是“存储和计算绑在一起”，高频查询的计算压力会直接拖垮存储稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/65648598c912b6fedcaf65301862e20b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云原生数据库PolarDB的设计是通过存算分离来解决这个问题：计算节点独立扩缩，高并发查询主要消耗计算资源，不会拖垮存储。遇到Agent高峰期的访问洪峰，可以独立扩计算而不用扩存储，成本和效率都会提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了架构分离，PolarDB还在应用和功能层做了专门设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB新增AgentMemory能力，提供长短期记忆表结构模板，自动管理对话历史和上下文。开发者不需要自己拼SQL、维护索引，Agent每一轮对话都被自动记录，下一轮查询时自动成为上下文的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行层，PolarDB提供自然语言工具调用（NL2SQL自动解析与执行），Agent可以用&quot;问问题&quot;的方式检索复杂知识。同时支持多模态数据融合，让Agent能在一次查询里实时融合文本、向量、图关系的检索结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结合基于Supabase的Agent统一部署与托管，PolarDB为企业提供工业级Agent开发框架。从多租户隔离、Serverless自动扩容、到运维自动化，所有工程复杂度都被打包进框架里，开发者只需专注定义Agent的行为和目标即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样一来，开发者收获很明确：存算分离让高并发和性能更容易同时拿到，AgentMemory+NL2SQL+多模态融合让Agent的记忆、检索、推理更像是数据库原生支持的事；工程上的托管和Serverless减少了部署、扩容、监控这些杂事难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体看下来，数据行业的这轮&quot;加速&quot;并不只是把某个指标做快，而是在做一件更底层的事：让数据少移动，让推理少绕路，让Agent的高频快速访问有专门架构支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;链路短了，实时能力才更容易稳定下来，也更容易规模化，不至于每个场景都要重新搭一套。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第三部分：当AI反哺“数据”，AI-Native成为可能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业看，2026年很可能会成为多Agent协同大规模落地的起点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不是因为单个Agent的能力突然跃升，而是因为多个Agent协同工作能够产生涌现效应——它们可以相互验证、相互纠正、共同规划复杂任务，从而完成单一模型难以胜任的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当Agent大规模走向自主决策与协作时，可能在一秒内对数据库发起成千上万次查询——先查一遍，根据结果修正假设，再查一遍，调整策略，反复循环，直到找到满意的答案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要承载&amp;nbsp;Agent&amp;nbsp;这种近乎“暴力”的访问模式，就必须引入一种全新的数据库形态——AI-Native&amp;nbsp;数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI-Native数据库也需要从根本上改变与Agent的交互方式。最核心的转向是：从SQL的&quot;精确匹配&quot;扩展到&quot;语义级检索与推理式访问&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着数据库不再仅仅回答&quot;这个值是什么&quot;的问题，而是要回答&quot;这个值意味什么&quot;、&quot;这条数据与另一条数据在语义上有什么关联&quot;、&quot;基于这些信息，下一步应该怎么做？&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而要做到这一点，AI相关的数据能力不能只做成外挂，而要成为数据库的“内生智能”。例如在存储层支持向量索引，在查询层支持相似度检索，在优化层针对向量查询做专门优化等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大会上，PolarDB提出“AI就绪的云原生数据库”的概念，就是为了推动数据库实现从“外挂式”集成AI到“内生智能”的进化，这也是走向AI-Native的过渡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于AI-Native数据库，另一个同样重要、却常被低估的变化，是对数据动态性的重新认知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在AI时代，高质量数据并不是一次性定义出来就能长期使用的：今天仍然有效的数据集，可能因为新的应用场景或模型路线，变得不再匹配。这需要Agent持续学习、持续适应新环境，相应的数据特征也会随之变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很显然，传统数据仓库“每天一次、每周一次”的更新节奏明显跟不上，AI-Native数据库需要支持更实时、更持续的数据优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;好的一面是：被数据“喂养”的AI，正在获得反过来“反哺数据”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去的数据清洗、整理与验证高度依赖人工：工程师写脚本，分析师定规则，QA定期抽检，流程慢且容易遗漏。现在，具备推理与决策能力的Agent已可以把一部分治理工作自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，让Agent获得对数据库的“写权限”：把自己的思考过程、决策日志写入数据库，沉淀为训练样本；把推理中得到的新知识、新规律固化到数据层。更进一步，当Agent在执行任务时发现脏数据、明显错误或不一致，它可以自动触发修正流程，而不是等人工排查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当这些机制形成闭环，数据库就能更快产出“最新、可用、被校正过”的数据，并把反馈链路压到更短的延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/11/11e22833ed83a41f2566b86065a84483.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以想象一个场景：某个Agent在做客户风险评估时，发现了一类新的可疑交易特征。它把该特征写入数据库并触发检测规则；规则自动回扫历史数据，标注出相似交易；评分模型读取新标签，更新客户风险等级。整个流程自动闭环，同时数据一致性仍然受到约束与保障。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从更宏观的角度看，这意味着AI+Data正在形成一个自循环系统：AI消费数据、理解数据、改写数据，数据再反过来塑造AI的行为与能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的超级智能（ASI）将不再是一个孤立模型，而更像是一个持续运转的系统：它既是数据的使用者，也是数据的生产者和优化者。数据不再只是被存放的资源，而是一种被不断加工、更新的运行态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个循环的速度越快、效率越高，整个系统的智能水平就越高。而承载这个循环的核心基础设施，一定是AI-Native的数据库系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到PolarDB大会发布的一系列能力：AI数据湖库（Lakebase）减少数据搬运，多模态多引擎融合扩展可管理的数据类型，模型算子化把推理拉回数据库内部，以及面向&amp;nbsp;Agent&amp;nbsp;应用开发的托管能力。它们看起来是分散功能，但放在一起更像一套完整路径——让数据库在AI时代重新站到系统中心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着一次更深的范式转移：从2025到2026，数据库产品、数据架构与AI应用之间的边界在变得模糊。企业IT也可能从“多个专用系统拼装”转向“围绕一个AI-Native数据库组织数据、计算与决策”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个背景下，未来谁能更快完成从云原生到AI原生的迁移，谁就更有机会在下一轮基础设施竞争中占据优势。&lt;/p&gt;</description><link>https://www.infoq.cn/article/vAZrlAQMJ6rvaq6oWypv</link><guid isPermaLink="false">https://www.infoq.cn/article/vAZrlAQMJ6rvaq6oWypv</guid><pubDate>Thu, 22 Jan 2026 06:47:00 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>数据湖仓</category></item><item><title>Android Studio Otter 优化代理工作流程，增强 LLM 灵活性</title><description>&lt;p&gt;Android Studio Otter 的最新版本引入了多项新特性，&lt;a href=&quot;https://android-developers.googleblog.com/2026/01/llm-flexibility-agent-mode-improvements.html&quot;&gt;使开发者可以更轻松地将 AI 驱动的工具集成到他们的工作流程中&lt;/a&gt;&quot;，包括选择使用哪个大型语言模型（LLM）、通过设备交互实现增强型代理模式、支持自然语言测试等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 灵活性是指开发者可以选择使用哪个 LLM 为 Android Studio 中的 AI 功能提供支持。虽然 IDE 默认包含一个 Gemini 模型，但开发者现在可以集成一个单独的远程模型，包括 OpenAI 的 GPT 和 Anthropic 的 Claude，或者使用 LM Studio 或 Ollama 等运行一个本地模型。谷歌表示，本地模型特别适合那些“互联网连接受限、数据隐私要求严格或希望尝试开源研究成果”的开发者，不过它们需要大量的本地 RAM 和硬盘空间才能有效运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;偏好 Gemini 的开发者现在可以使用自己的 Gemini API 密钥访问更高级的版本，以及扩展后的上下文窗口和配额，在使用代理模式进行长时间编码会话时，这可能很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio Otter 还通过让代理“看到”并与应用程序交互来增强代理模式。这包括在设备或模拟器上部署和检查应用程序，通过捕获屏幕截图和分析屏幕内容来调试应用程序 UI，以及检查 Logcat 以查找错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio Otter 的另一个主要特性是通过 “Journey” 支持自然语言测试，这使得开发者可以用简单英语定义用户 Journey 测试，Gemini 会将这些指令转换为可执行的测试步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这不仅使你可以更轻松地编写测试，而且编写出来的测试更容易理解。它还使你能够定义复杂的断言，让 Gemini 根据在设备屏幕上“看到”的内容进行评估。因为 Gemini 会推理如何实现你的目标，所以这些测试能更好地应对应用程序布局的微妙变化，在面对不同应用程序版本或设备配置时显著减少测试结果的不稳定性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该 IDE 专门提供了一个基于 XML 的编辑器（管理这些 Journey ）以及一个测试面板（显示每个动作的屏幕截图以及 Gemini 执行每个步骤的原因）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio 现在还支持模型上下文协议（MCP），允许 AI 代理连接到 Figma、Notion 和 Canva 等远程服务器。例如，通过连接到 Figma，代理模式可以直接访问设计文件，生成更准确的 UI 代码，减少了在不同的工具之间手动复制粘贴上下文的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，本次更新引入了一个专门的 UI ，用于审查编码代理编辑过的每个文件。它允许开发者查看代码差异，并选择保留或单个或全部撤销更改。此外，它现在可以管理多个聊天线程，使不同的任务（如 UI 设计和 Bug 修复）可以同时执行，而不会丢失上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Otter 的特性更新比这里提到的要多许多，如经过改进的应用链接助手、 Logcat 自动回溯等。要了解完整的特性更新信息，请查阅发布公告原文。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/android-studio-otter-llm-flex/&quot;&gt;https://www.infoq.com/news/2026/01/android-studio-otter-llm-flex/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hGq63lTTN44RtAmwA128</link><guid isPermaLink="false">https://www.infoq.cn/article/hGq63lTTN44RtAmwA128</guid><pubDate>Thu, 22 Jan 2026 06:40:00 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category><category>Android/iOS</category></item><item><title>Cloudflare在R2 SQL中推出聚合功能，提升数据分析能力</title><description>&lt;p&gt;最近，Cloudflare宣布&lt;a href=&quot;https://blog.cloudflare.com/r2-sql-aggregations/&quot;&gt;在R2 SQL中支持聚合功能&lt;/a&gt;&quot;。这是一个新特性，使开发者可以通过SQL查询存储在R2中的数据。这一功能增强使得R2 SQL不再局限于基本的过滤功能，而是可以在不依赖单独的数据仓库工具的情况下，更好地满足分析工作负载的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://developers.cloudflare.com/r2-sql/&quot;&gt;R2 SQL&lt;/a&gt;&quot;现在支持SUM、COUNT、AVG、MIN和MAX，以及GROUP BY和HAVING子句。这些聚合函数使开发者可以直接在R2上通过R2数据目录运行SQL分析，快速汇总数据、发现趋势、生成报告以及识别日志中的异常模式。除了聚合之外，本次更新还引入了模式发现命令，包括SHOW TABLES和DESCRIBE。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare资深软件工程师&lt;a href=&quot;https://www.linkedin.com/in/jeromeschneider/&quot;&gt;Jérôme Schneider&lt;/a&gt;&quot;、高级软件工程师&lt;a href=&quot;https://www.linkedin.com/in/nikitalapkov/&quot;&gt;Nikita Lapkov&lt;/a&gt;&quot;和高级产品经理&lt;a href=&quot;https://www.linkedin.com/in/marc-selwan-088a7342&quot;&gt;Marc Selwan&lt;/a&gt;&quot;总结道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;无论是生成报告、监控大量日志中的异常，还是仅仅试图发现数据中的趋势，现在你都可以在Cloudflare提供的开发者平台上轻松完成所有这些工作，而无需管理复杂的OLAP基础设施或将数据从R2中移出。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudZero研究主管Jeremy Daly在他的新闻资讯中&lt;a href=&quot;https://offbynone.io/issues/349/&quot;&gt;评论&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;通过在R2 SQL中支持聚合，Cloudflare继续将数据推向边缘，扩展了开发者可以实际在边缘运行的工作负载类型。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/200e9b573823a72d742963d9b8e1b640.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：Cloudflare博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Schneider、Lapkov和Selwan阐述了他们如何使用scatter-gather和shuffling策略构建分布式GROUP BY执行，以便直接在R2数据目录上运行分析：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;不包含HAVING和ORDER BY子句的聚合查询可以用和过滤查询类似的方式执行。对于过滤查询，R2 SQL会选择一个节点作为查询执行的协调者。这个节点会分析查询并查看R2数据目录，以便确定哪些Parquet行组可能包含与查询相关的数据。每个Parquet行组代表单个计算节点可以处理的相对较小的工作量。协调节点将工作分配给多个工作节点，收集结果后返回给用户。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare还单独宣布，&lt;a href=&quot;https://developers.cloudflare.com/changelog/2025-12-18-r2-data-catalog-snapshot-expiration/&quot;&gt;R2数据目录现在支持Apache Iceberg表的快照自动过期&lt;/a&gt;&quot;，完善了&lt;a href=&quot;https://developers.cloudflare.com/r2/data-catalog/table-maintenance/&quot;&gt;自动压缩&lt;/a&gt;&quot;——通过将小数据文件合并成比较大的文件来优化查询性能。Selwan&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407493096638386176/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这两者是相辅相成的，因为快照过期所带来的一系列元数据清理/管理操作能够提高这些聚合查询的执行效率，在启用了压缩功能的情况下更是如此。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这家超大规模云服务商最近发布了一篇深度解析文章，详细阐述了其&lt;a href=&quot;https://blog.cloudflare.com/r2-sql-deep-dive&quot;&gt;分布式查询引擎的工作原理&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于R2 SQL仍处于公测阶段，所以支持的SQL语法可能会随着时间的推移而变化。文档页介绍了&lt;a href=&quot;https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/&quot;&gt;当前存在的限制和最佳实践&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-r2-sql-aggregations/&quot;&gt;https://www.infoq.com/news/2026/01/cloudflare-r2-sql-aggregations/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/233savw2C6lrQCPaUbMX</link><guid isPermaLink="false">https://www.infoq.cn/article/233savw2C6lrQCPaUbMX</guid><pubDate>Thu, 22 Jan 2026 05:58:00 GMT</pubDate><author>Renato Losio</author><category>大数据</category></item><item><title>如何利用 Snowflake ML 实现电商个性化 ｜ 技术实践</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今竞争激烈的电商领域，为客户提供个性化体验已不再是奢侈选项，而是驱动成功的关键要素。运用人工智能驱动分析、数据科学与机器学习的企业正日益超越竞争对手。消费者越来越期待定制化推荐与动态购物体验——这正是 Snowflake ML 的用武之地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Snowflake ML，开发者和分析师可直接在 Snowflake 平台中使用标准 SQL 实现以下功能：&lt;/p&gt;&lt;p&gt;加载与整合数据构建客户细分画像训练并部署机器学习模型生成个性化评分将结果输送到应用与实时工作流中&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文将深入探讨 Snowflake ML 如何为现代电商体验提供简洁、基于 SQL 的个性化解决方案。您将了解如何将客户数据接入 Snowflake，根据行为模式划分客群，并利用 Snowflake ML 构建预测高价值客户的智能模型。无论您是构建个性化工作流的开发者，还是提升营销效果的分析师，这些实践步骤都将助您快速入门。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;请首先登录您的 Snowflake 账户（访问 Snowflake 网页控制台）。若尚未拥有账户或需测试环境进行学习，可在此免费注册体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤1：加载并准备数据&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将首先创建一个客户订单的小型模拟数据集。&lt;/p&gt;&lt;p&gt;请在 Snowflake SQL 工作表中完整运行以下代码块：&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 1.0: Create a database and schema

CREATE OR REPLACE DATABASE DATACLOUDDISPATCHSI;
USE DATABASE DATACLOUDDISPATCHSI;
CREATE OR REPLACE SCHEMA ECOMMERCE;
USE SCHEMA ECOMMERCE;

-- Step 1.1: Create a customer orders table

CREATE OR REPLACE TABLE CUSTOMER_ORDERS (
  CUSTOMER_ID  NUMBER,
  ORDER_ID     NUMBER,
  ORDER_DATE   DATE,
  ORDER_VALUE  NUMBER(10,2),
  PRODUCT_ID   NUMBER
);

-- Step 1.2: Insert sample order data

INSERT INTO CUSTOMER_ORDERS (CUSTOMER_ID, ORDER_ID, ORDER_DATE, ORDER_VALUE, PRODUCT_ID) VALUES
(1001,50001,&#39;2023-01-15&#39;, 89.99,201),
(1001,50022,&#39;2023-03-02&#39;,120.49,305),
(1002,50110,&#39;2023-05-11&#39;, 45.00,110),
(1003,50155,&#39;2023-02-19&#39;,239.00,402),
(1003,50190,&#39;2023-05-22&#39;,130.00,233),
(1003,50201,&#39;2023-06-01&#39;, 99.99,110),
(1004,50333,&#39;2023-01-05&#39;, 19.99,502),
(1001,50390,&#39;2023-11-11&#39;,301.00,900),
(1005,50400,&#39;2023-12-12&#39;, 67.50,702);

-- Step 1.3: Verify data
SELECT * FROM CUSTOMER_ORDERS ORDER BY ORDER_DATE;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该数据集包含重复的客户购买记录、多样化的订单金额以及用于后续客户分群和机器学习建模的实用字段，足以支持基础建模需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;使用 Snowflake Workspace&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若您倾向于通过可视化界面而非 SQL 加载数据，Snowflake Workspace 支持将文件（包括 Excel 和 CSV 格式）直接拖放至环境中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;在 Snowflake 左侧导航栏中进入 Projects。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;点击下拉菜单中的 Workspaces（如图所示）。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;创建并打开一个新的 Workspace。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;在 Workspace 内点击+ Worksheet 以新建 SQL 工作表。&lt;/p&gt;&lt;p&gt;5.&amp;nbsp;运行 SQL 代码前，请确保工作表已设置正确的角色、仓库、数据库与模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/03741f3890ab0c93ba25027a62dbd661.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本教程步骤 1 至 3 中的所有 SQL 命令均需在此 SQL 工作表中粘贴并执行。Snowflake 虽提供 Workspace、Notebook 等多种项目工具，但本教程全程使用标准 SQL 工作表完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤二：使用 SQL 构建客户细分模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake 支持集成机器学习模型，用于预测客户行为、推荐产品及定制促销策略。开发人员可通过 Python 或 R 语言，结合 Snowflake 的 Data Science Workspace 部署模型，该模型可输入客户数据并输出个性化推荐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一种基础的个性化策略是基于客户历史行为进行识别，我们将计算以下指标：&lt;/p&gt;&lt;p&gt;购买频率客单价（AOV）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;USE DATABASE DATACLOUDDISPATCHSI;
USE SCHEMA ECOMMERCE;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 2.1: Create customer segments
CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS AS
SELECT
  CUSTOMER_ID,
  COUNT(ORDER_ID)  AS PURCHASE_COUNT,
  AVG(ORDER_VALUE) AS AVG_ORDER_VALUE
FROM CUSTOMER_ORDERS
WHERE ORDER_DATE BETWEEN &#39;2023-01-01&#39; AND &#39;2023-12-31&#39;
GROUP BY CUSTOMER_ID;

-- Step 2.2: Inspect customer segments
SELECT * FROM CUSTOMER_SEGMENTS ORDER BY PURCHASE_COUNT DESC;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由此构建的 CUSTOMER_SEGMENTS 表将成为机器学习模型的基础数据层。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤三：训练与部署机器学习模型（基于 Snowflake ML 的纯 SQL 实现）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake ML 支持直接使用 SQL 训练模型，无需依赖 Python 或外部工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将完成以下任务：&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;标记“高价值客户”（购买次数 ≥3次）&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;训练分类模型&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;对全部客户进行评分&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.1：创建训练表&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 中训练机器学习模型前，需为模型提供学习样本。这意味着需要构建一个包含以下内容的表：&lt;/p&gt;&lt;p&gt;特征（模型学习的输入变量）目标标签（模型需预测的结果）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本例中，我们的目标是识别高价值客户。因此，需要在历史数据中创建一列，明确标注哪些客户属于高价值客户。训练表的作用正在于此——它基于步骤二生成的客户分群，新增目标标签列。随后，Snowflake ML将利用此标注表学习高价值客户的特征模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.1: Add a target label for modeling
CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS_TRAIN AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    IFF(PURCHASE_COUNT &amp;gt;= 3, 1, 0) AS TARGET_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

SELECT * FROM CUSTOMER_SEGMENTS_TRAIN ORDER BY PURCHASE_COUNT DESC;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.2：使用 Snowflake ML 训练分类模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在获得已标注的训练表后，即可训练 Snowflake ML 识别高价值客户的潜在特征。通过训练分类模型，Snowflake 将学习：&lt;/p&gt;&lt;p&gt;应从哪些输入特征中学习规律（如购买次数与平均订单金额）需要预测的目标结果（即高价值标签：0或1）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.2: Train the classification model

CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION HIGH_VALUE_MODEL (
    INPUT_DATA     =&amp;gt; SYSTEM$REFERENCE(&#39;TABLE&#39;, &#39;ECOMMERCE.CUSTOMER_SEGMENTS_TRAIN&#39;),
    TARGET_COLNAME =&amp;gt; &#39;TARGET_HIGH_VALUE&#39;
);
Snowflake automatically trains and tunes the model based on your training table.
(Optional) View metrics:
CALL HIGH_VALUE_MODEL!SHOW_EVALUATION_METRICS();&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.3：使用模型对客户进行评分（SQL）&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型训练完成后，即可用于预测。在此步骤中，模型将根据每位客户的购买行为（购买次数与平均订单金额）判断其是否为潜在高价值客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下 SQL 命令将每位客户的特征输入模型，并返回预测结果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.3: Score customers

SELECT
    s.CUSTOMER_ID,
    s.PURCHASE_COUNT,
    s.AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, s.PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, s.AVG_ORDER_VALUE
        )
    ) AS MODEL_OUTPUT
FROM CUSTOMER_SEGMENTS AS s;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;MODEL_OUTPUT 是什么？&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 将模型的预测结果以 VARIANT 类型（一种结构化对象）返回。您无需运行或执行它——它仅仅是 Snowflake 所展示的结果！&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了使预测结果更易于使用，您可以只提取预测类别（0或1）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 表示模型将客户识别为高价值客户0 表示非高价值客户&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提取预测类别的语句为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这将为您提供一个清晰的 0/1 指标，用于判断客户是否被视为“高价值客户”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;&amp;nbsp;步骤3.4：持久化个性化评分（可选）&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至此，您已通过在查询中直接使用模型生成预测，这非常适合探索性分析——但在实际场景中，您通常需要将这些预测存储到表中，以便供仪表板、应用程序、营销活动等重复使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下 SQL 语句创建一个名为 CUSTOMER_VALUE_SCORES 的新表，其中包含每位客户、其购买行为以及模型的预测结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE OR REPLACE TABLE CUSTOMER_VALUE_SCORES AS

SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;
SELECT * FROM CUSTOMER_VALUE_SCORES ORDER BY PREDICTED_HIGH_VALUE DESC;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在您已拥有一个可用于下游个性化流程的数据表。您可以持续引用这些评分来定位高价值客户、触发个性化优惠、提供推荐内容等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤四：实时个性化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;获得每位客户的预测评分后，即可结合实时行为数据提供更智能的个性化推荐。实时行为数据包括：&lt;/p&gt;&lt;p&gt;最近浏览的商品购物车中新增或移除的商品浏览或会话事件实时库存更新&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对更高级的用例，Snowflake 支持在线特征存储，允许应用程序（如网站或推荐引擎）在毫秒级延迟内获取最新的客户特征——包括近期点击行为、会话历史或模型生成的评分。这对于需要在应用用户体验中实现实时个性化（而非依赖批量调度）的场景尤为理想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 可通过 Kafka、Kinesis 或 Event Hubs 等工具接收此类流式数据，从而根据客户行为变化持续更新推荐结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为保持个性化数据的时效性，您还可以通过 Snowflake 任务定期更新推荐表。以下示例展示了一个每小时运行并刷新热门商品推荐的简化任务：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;--示例：定期更新推荐数据&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE OR REPLACE TASK PERSONALIZE_RECOMMENDATIONS
WAREHOUSE = COMPUTE_WH
SCHEDULE = &#39;USING CRON 0   UTC&#39;
AS
MERGE INTO LATEST_RECOMMENDATIONS tgt
USING (
    SELECT CUSTOMER_ID, PRODUCT_ID, SCORE
    FROM ECOMMERCE.RECOMMENDATIONS_STREAM
    WHERE SCORE &amp;gt; 0.8
) src
ON tgt.CUSTOMER_ID = src.CUSTOMER_ID AND tgt.PRODUCT_ID = src.PRODUCT_ID
WHEN MATCHED THEN UPDATE SET SCORE = src.SCORE
WHEN NOT MATCHED THEN INSERT VALUES (src.CUSTOMER_ID, src.PRODUCT_ID, src.SCORE);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此方案使您的应用程序能够始终查询最新、最相关的推荐结果，从而实现完全动态的个性化购物体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;个性化推荐现已不再局限于手动规则或外部机器学习流水线。借助 Snowflake ML，您可以在 Snowflake 平台内直接驱动端到端的电商个性化推荐。本教程展示了如何：&lt;/p&gt;&lt;p&gt;将全部电商数据整合至统一的单一平台完全使用 SQL 构建客户细分模型通过 Snowflake ML 训练机器学习模型——无需 Python 环境完成客户评分并生成个性化洞察利用实时数据流和任务机制保持推荐结果动态更新&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最关键的是，所有操作均在 Snowflake 内完成——无需数据迁移、无需配置 Python 环境、无需依赖外部服务。这使得开发者、分析师和数据团队能够以前所未有的便捷度，提供高度个性化的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;注：本教程使用 SQL 和 Snowflake ML 进行演示，但 Snowflake 还提供更多人工智能与智能增强功能，可助力规模化扩展个性化应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;想要一键复制代码以便跟随操作吗？&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是您可以粘贴到 SQL workspace 中的分步最小可复现工作流程：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- ============================================================

-- E-COMMERCE PERSONALIZATION QUICKSTART (SQL-ONLY)

-- End-to-end example:

--  1. Create database &amp;amp; schema
--  2. Load sample orders data
--  3. Build customer segments
--  4. Prepare training data for ML
--  5. Train Snowflake ML classification model
--  6. Score customers &amp;amp; optionally persist scores

-- ============================================================

----------------------------------------------------------------

-- (Optional) Step 0: Choose a warehouse

----------------------------------------------------------------

-- Uncomment and replace &lt;your_warehouse&gt; if needed:
-- USE WAREHOUSE &lt;your_warehouse&gt;;

----------------------------------------------------------------

-- Step 1: Create database, schema, and sample CUSTOMER_ORDERS

----------------------------------------------------------------

CREATE OR REPLACE DATABASE DATACLOUDDISPATCHSI;
USE DATABASE DATACLOUDDISPATCHSI;
CREATE OR REPLACE SCHEMA ECOMMERCE;
USE SCHEMA ECOMMERCE;

-- Create the orders table

CREATE OR REPLACE TABLE CUSTOMER_ORDERS (
  CUSTOMER_ID  NUMBER,
  ORDER_ID     NUMBER,
  ORDER_DATE   DATE,
  ORDER_VALUE  NUMBER(10,2),
  PRODUCT_ID   NUMBER
);

-- Insert sample e-commerce data

INSERT INTO CUSTOMER_ORDERS (CUSTOMER_ID, ORDER_ID, ORDER_DATE, ORDER_VALUE, PRODUCT_ID) VALUES
(1001,50001,&#39;2023-01-15&#39;, 89.99,201),
(1001,50022,&#39;2023-03-02&#39;,120.49,305),
(1002,50110,&#39;2023-05-11&#39;, 45.00,110),
(1003,50155,&#39;2023-02-19&#39;,239.00,402),
(1003,50190,&#39;2023-05-22&#39;,130.00,233),
(1003,50201,&#39;2023-06-01&#39;, 99.99,110),
(1004,50333,&#39;2023-01-05&#39;, 19.99,502),
(1001,50390,&#39;2023-11-11&#39;,301.00,900),
(1005,50400,&#39;2023-12-12&#39;, 67.50,702);

-- Quick preview of raw orders

SELECT * FROM CUSTOMER_ORDERS ORDER BY ORDER_DATE;

----------------------------------------------------------------

-- Step 2: Build customer segments (frequency &amp;amp; average order value)

----------------------------------------------------------------

-- Aggregate behavior to create one row per customer

CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS AS
SELECT
    CUSTOMER_ID,
    COUNT(ORDER_ID)  AS PURCHASE_COUNT,
    AVG(ORDER_VALUE) AS AVG_ORDER_VALUE
FROM CUSTOMER_ORDERS
WHERE ORDER_DATE BETWEEN &#39;2023-01-01&#39; AND &#39;2023-12-31&#39;
GROUP BY CUSTOMER_ID;

-- Inspect segments

SELECT * FROM CUSTOMER_SEGMENTS ORDER BY PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 3: Prepare training data for Snowflake ML
-- Add a label indicating whether a customer is “high-value”
-- (in this example: 3 or more purchases)

----------------------------------------------------------------

CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS_TRAIN AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    IFF(PURCHASE_COUNT &amp;gt;= 3, 1, 0) AS TARGET_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

-- View training data with target

SELECT * FROM CUSTOMER_SEGMENTS_TRAIN ORDER BY PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 4: Train a classification model with Snowflake ML
-- This learns to predict TARGET_HIGH_VALUE from the features
-- PURCHASE_COUNT and AVG_ORDER_VALUE.

----------------------------------------------------------------

CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION HIGH_VALUE_MODEL (
    INPUT_DATA     =&amp;gt; SYSTEM$REFERENCE(&#39;TABLE&#39;,&#39;ECOMMERCE.CUSTOMER_SEGMENTS_TRAIN&#39;),
    TARGET_COLNAME =&amp;gt; &#39;TARGET_HIGH_VALUE&#39;
);

-- (Optional) Inspect training metrics
CALL HIGH_VALUE_MODEL!SHOW_EVALUATION_METRICS();

----------------------------------------------------------------

-- Step 5: Score customers with the trained model
-- This returns the predicted class (0 = not high-value, 1 = high-value).

----------------------------------------------------------------

SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS
ORDER BY PREDICTED_HIGH_VALUE DESC, PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 6 (Optional): Persist personalized scores for downstream use
-- This creates a reusable table that other teams, dashboards,
-- and applications can query.

----------------------------------------------------------------

CREATE OR REPLACE TABLE CUSTOMER_VALUE_SCORES AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

-- Final scored output

SELECT * FROM CUSTOMER_VALUE_SCORES
ORDER BY PREDICTED_HIGH_VALUE DESC, PURCHASE_COUNT DESC;&lt;/your_warehouse&gt;&lt;/your_warehouse&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.linkedin.com/pulse/how-leverage-snowflake-intelligence-e-commerce-personalization-60fhc/?trackingId=SamHZTb8T76gKESH2PP2SA%3D%3D&quot;&gt;https://www.linkedin.com/pulse/how-leverage-snowflake-intelligence-e-commerce-personalization-60fhc/?trackingId=SamHZTb8T76gKESH2PP2SA%3D%3D&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QY60NJrEuZq8um5fiHO7</link><guid isPermaLink="false">https://www.infoq.cn/article/QY60NJrEuZq8um5fiHO7</guid><pubDate>Thu, 22 Jan 2026 03:24:12 GMT</pubDate><author>Snowflake</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>揭秘Uber跨区域数据湖与灾难恢复机制：350PB数据、数百万事件、单一系统</title><description>&lt;p&gt;Uber构建了&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync/&quot;&gt;HiveSync&lt;/a&gt;&quot;，这是一个分片式批量复制系统，能够使Hive和HDFS数据在多个区域之间保持同步，它每天处理数百万个Hive事件。HiveSync确保了跨区域数据的一致性，实现了Uber的灾难恢复策略，并消除了由次要区域闲置而导致的低效问题——此前次要区域需承担与主区域一样的硬件成本，而HiveSync在维持高可用性的同时彻底解决了这一问题&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync基于开源项目Airbnb &lt;a href=&quot;https://github.com/airbnb/reair&quot;&gt;ReAir&lt;/a&gt;&quot;构建并做了一些扩展，包括实现了分片、基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;&gt;DAG&lt;/a&gt;&quot;的编排以及控制平面和数据平面的分离。&lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt;&quot;作业现在只在主数据中心执行，而HiveSync处理跨区域复制，实现了近乎实时的一致性，保持了灾难应对能力和分析访问权限。分片功能允许将表和分区划分为独立的单元，从而实现并行复制和细粒度容错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync将控制平面（负责编排作业和管理关系元数据存储中的状态）与数据平面（执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hadoop&quot;&gt;HDFS&lt;/a&gt;&quot;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hive&quot;&gt;Hive&lt;/a&gt;&quot;文件操作）分离。Hive Metastore事件监听器负责捕获DDL和DML变更，将它们记录到&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&quot;中，并触发复制工作流。任务以有限状态机的形式呈现，支持任务重启与健壮的故障恢复机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/70/70a87b9fb2fc1939cfd347a9b44c9890.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;HiveSync架构：控制平面和数据平面分离（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync有两个主要组件：HiveSync复制服务和数据修复服务。复制服务使用Hive Metastore事件监听器实时捕获表和分区变更，将它们异步记录到MySQL中。这些审计条目被转换为异步复制作业，以有限状态机的形式执行，为确保可靠性，状态会被持久化。Uber使用了混合策略：规模比较小的作业使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_procedure_call&quot;&gt;RPC&lt;/a&gt;&quot;以提高效率，而规模比较大的作业则利用&lt;a href=&quot;https://yarnpkg.com/&quot;&gt;YARN&lt;/a&gt;&quot;上的DistCp。DAG管理器强制执行分片级的排序和锁定，而静态和动态分片技术则实现了水平扩展，确保复制过程一致且无冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5e/5ed30f7fc7494e66967a1812cc9107e8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;HiveSync复制服务（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据修复是一个持续检测异常的服务，如缺失的分区或非预期的HDFS更新，恢复数据中心1（DC1）和数据中心2（DC2）之间的一致性，从而保证数据的正确性。HiveSync保证了每四小时一次的复制SLA，99&lt;a href=&quot;https://en.wikipedia.org/wiki/Percentile&quot;&gt;百分位&lt;/a&gt;&quot;的延迟大约为20分钟，并支持一次性复制，用于在切换到增量复制之前，一次性地将历史数据集导入新区域或集群。Uber的数据修复服务会扫描DC1和DC2，检测异常（如缺失或多余的分区），并修复任何不匹配的情况，从而确保跨区域的一致性，目标是准确性超过99.99%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5dd2b6aac7fb4dd6b5b67670c5d45d79.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据修复服务分析和解决数据中心之间的不一致性（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync的规模很大，管理着80万个Hive表，总计约300PB的数据，单表数据量从几GB到数十PB不等，单表分区数从几百到一百万多不等。每天，HiveSync处理超过500万个Hive DDL和DML事件，跨区域复制约8PB的数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望未来，随着批量分析和ML管道迁移到谷歌云平台，Uber计划将HiveSync扩展到云端复制场景，进一步利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Shard_(database_architecture)&quot;&gt;分片&lt;/a&gt;&quot;、编排和数据一致性技术来高效地维护其PB级数据的完整性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-hivesync-data-lake/&quot;&gt;https://www.infoq.com/news/2026/01/uber-hivesync-data-lake/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O7T47Q680HHi6rbdBEPr</link><guid isPermaLink="false">https://www.infoq.cn/article/O7T47Q680HHi6rbdBEPr</guid><pubDate>Thu, 22 Jan 2026 03:06:30 GMT</pubDate><author>作者：Leela Kumili</author><category>大数据</category><category>安全</category></item><item><title>架构彻底重构！DeepSeek新模型代码曝光，要来的V4让国内外都坐不住了？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek V4马上要来了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正值 DeepSeek-R1 发布一周年之际，DeepSeek 的官方 GitHub 代码库意外曝光了代号为“MODEL1”的全新模型线索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而综合泄露代码片段中呈现的架构调整、硬件优化与全新处理机制来看，“MODEL1”似乎绝非简单的版本迭代，而是一次全方位的架构重构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次 DeepSeek 在 GitHub 代码库的提前部署，在时间线上与业内疯传的“其新模型再次在春节期间发布”的消息高度吻合。本月初，也有外媒爆料称，DeepSeek 将在今年 2 月中旬农历新年期间推出新一代旗舰 AI 模型 DeepSeek V4。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;新模型曝光，代码揭露全新架构能力&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，DeepSeek 陆陆续续给其在GitHub上的 FlashMLA 代码库做了一系列更新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e5d5348f3efe211a0e3e5f1ba7eb4e5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而刚刚，有开发者发现，114 个文件中有 28 处都提到了未知的“MODEL1”大模型标识符。而且，在代码逻辑结构中，该标识符与现有模型“V32”（即 DeepSeek-V3.2）是并列且作为独立分支出现的。也就是说，“MODEL1”很可能代表一个不同于现有架构和技术路径的全新模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/37bd822e39d17249fa980e897d6855d9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;网友们也纷纷猜测，这个“MODEL1”很可能就是 DeepSeek 即将发布的新模型V4的内部开发代号或首个工程版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据代码片段中披露的技术规格，这个新模型有重大架构变更，或在KV Cache（键值缓存）布局、稀疏性处理及FP8解码支持等方面改变了策略和机制，还包括参数维度切换至 512 维以及针对英伟达下一代 Blackwell GPU 架构的专项优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 FP8 解码路径上，该模型有多处针对性的内存优化调整。测试脚本中同步新增了test_flash_mla_sparse_decoding.py与test_flash_mla_dense_decoding.py两个文件，这一改动证实“MODEL1”具备稀疏与稠密计算并行处理的能力。在稀疏化实现方案中，键值缓存存储采用 FP8 精度，而矩阵乘法运算则使用 bfloat16 精度，以此保障计算准确性。这种混合精度设计表明，“MODEL1”通过在推理阶段对部分数据进行选择性稀疏化处理，有效降低内存占用压力，从而具备处理超长上下文窗口的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f8/f8d9b8a3d386e1644e188a9293322bd6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在csrc/api/common.h文件内的代码显示，“MODEL1”的注意力头参数维度被配置为 512 维，与上一代产品 DeepSeek V3.2 采用的 576 维参数设置形成显著差异。这一架构调整意味着，DeepSeek已对其多头隐式注意力（MLA）结构进行了重新设计。此前的 V3 系列采用非对称设计方案，将 128 维旋转位置编码（RoPE）与 448 维隐层维度相结合。此次转向标准化的 512 维参数配置，或许是为了更好地适配硬件性能，也可能是在隐层压缩率方面实现了技术突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0889979be9eaef4b84e9d7e73cb3ce6a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代码更新记录还显示，DeepSeek研发团队已围绕英伟达 Blackwell 架构开展了大量优化工作，预示着DeepSeek正为“MODEL1”量身打造下一代硬件适配方案。代码中新增了一批专门面向 Blackwell 指令集的接口，包括FMHACutlassSM100FwdRun；相关文档明确指出，该模型若要在 B200 GPU 上运行，需依赖 CUDA 12.9 版本环境；内嵌的性能指标数据显示，即便在未完全优化的状态下，稀疏化 MLA 算子在 B200 硬件平台上的运算性能仍可达到 350 万亿次浮点运算每秒（TFLOPS）。在当前主流的 H800 GPU（基于 SM90a 架构）上，稠密型 MLA 算子的吞吐量则能达到 660 万亿次浮点运算每秒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管本次代码提交的内容主要聚焦于算子层面的实现，但调度逻辑中仍提及多项新增功能。从代码仓库的结构可以推断，“MODEL1”集成了价值向量位置感知（VVPA）技术，这项技术有望解决传统 MLA 架构在长文本处理场景下存在的位置信息衰减问题。代码注释中还提到了一种名为 “记忆印记（Engram）机制” 的技术，但在已公开的代码提交记录中，相关实现细节尚不完整。从该机制在分布式处理模块中的部署位置推测，其功能大概率与分布式存储优化或高级键值压缩技术相关，旨在满足“MODEL1”对高吞吐量的性能需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前不久，DeepSeek 研究团队刚发布了 Engram 的技术论文。当时，就有业内观察者认为，Engram 模块可能会成为 DeepSeek V4 的重要组成部分，并预示 DeepSeek 下一代模型会在记忆和推理协同上实现架构级提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些优化能够表明，“MODEL1”在推理效率上可能有更好的表现。此前也有爆料称，DeepSeek V4的代码表现已超越 Claude 和 GPT 系列，并且具备处理复杂项目架构和大规模代码库的工程化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;国内外万众期待，“中国 AI 站起来了”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“DeepSeek刚刚泄露了一个模型，这可能会再次改变整个AI行业的格局。”在国内外的各大社交平台及社区，针对 DeepSeek 新模型的上线猜测、能力预测的期待帖子已大量涌现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“中国 AI 站起来了。”昨日，全球最大的 AI 开源社区 Hugging Face 以“距离DeepSeek 时刻一周年”为题专门发文，复盘了 R1 发布这一年来对中国开源社区及其对整个AI生态系统的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这是中国研发的开源模型首次跻身全球主流榜单。此后一年间，每当有新模型发布时，R1 都会被当作重要的参照基准。该模型迅速登顶 Hugging Face 平台历史最受欢迎模型榜单，而这一平台上最受青睐的模型，也不再以美国研发的产品为主导。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他们看来，R1 的真正价值在于降低先进AI能力的门槛或者说障碍，并提供了清晰的模式。&lt;/p&gt;&lt;p&gt;技术障碍。通过公开分享其推理路径和训练后的方法，R1将此前被封闭API锁定的高级推理转变为可下载、提炼和微调的工程资产。许多团队不再需要从零开始训练庞大的模型来获得强大的推理能力。应用障碍。R1 以 MIT 许可证发布，使其使用、修改和再分发变得简单。依赖封闭式模型的公司开始直接将R1投入生产。蒸馏、二次培训和领域特定适应成为常规工程工作，而非专门项目。心理层面。当问题从“我们能做到吗？”转变为“我们如何做好？”时，许多公司的决策发生了变化。对于中国 AI 社区来说，这也是罕见的持续全球关注时刻，对长期被视为追随者的生态系统意义重大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“在 R1 模型发布一年后的今天，我们看到的不仅是一大批新模型的涌现，更见证了一个富有生命力的中国 AI 开源生态的加速成型。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/FlashMLA?tab=readme-ov-file&quot;&gt;https://github.com/deepseek-ai/FlashMLA?tab=readme-ov-file&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment&quot;&gt;https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://chinabizinsider.com/deepseeks-mysterious-model-1-surfaces-in-github-code-sparking-speculation-about-next-generation-ai-system/&quot;&gt;https://chinabizinsider.com/deepseeks-mysterious-model-1-surfaces-in-github-code-sparking-speculation-about-next-generation-ai-system/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/XISEq5cHfv4FARpZMBgZ</link><guid isPermaLink="false">https://www.infoq.cn/article/XISEq5cHfv4FARpZMBgZ</guid><pubDate>Wed, 21 Jan 2026 10:18:11 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>理想主义者们，没能阻止AI进入伊朗“战场”</title><description>&lt;p&gt;撰稿&amp;nbsp;|&amp;nbsp;陈姚戈、高允毅&lt;/p&gt;&lt;p&gt;编辑&amp;nbsp;|&amp;nbsp;王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一场从线下蔓延至线上的舆论战争，正发生在伊朗。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线下，伊朗当局正在组织“反骚乱”集会；线上，断网、媒体管制和信息封锁同时发生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在网络被关闭的时段，伊朗国营媒体几乎成为唯一的信息源。信息真空之中，大量影像只能在社交平台上传播，却很难被证实或证伪。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网民和非营利组织通过自发事实核查发现，伊朗官方发布了使用后期编辑和&amp;nbsp;AI&amp;nbsp;生成影像，刻意营造了“反骚乱”的舆论氛围。这类内容在&amp;nbsp;X&amp;nbsp;平台上获得了数万次观看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/229bbf0e7d84ba6134ce9060c6aadf50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，另一方同样出现大量&amp;nbsp;AI&amp;nbsp;生成内容。一段被广泛转发的视频显示，有人从建筑物上扯下国旗，发布者称有人撤下了伊朗国旗。这段视频经过反向图片搜索后，被证实拍摄于&amp;nbsp;2025&amp;nbsp;年&amp;nbsp;9&amp;nbsp;月尼泊尔抗议活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/84af1869924c53814f241423b3de06cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非营利组织&amp;nbsp;WITNESS“技术威胁与机遇”项目副主任&amp;nbsp;Mahsa&amp;nbsp;Alimardani&amp;nbsp;&lt;a href=&quot;https://www.theatlantic.com/international/2026/01/iran-disinformation-ai-protests-doubt/685608/&quot;&gt;指出&lt;/a&gt;&quot;，在传播过程中被&amp;nbsp;AI“增强画质”的现场照片，反而被当局用来否定影像本身的真实性，对抗议事实进行整体抨击。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真假在这一过程中被同时稀释，AI&amp;nbsp;让“知晓真相”这件事变得更难了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并非人们第一次意识到&amp;nbsp;AI&amp;nbsp;的风险。近几年，从《&lt;a href=&quot;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&quot;&gt;要求暂停更强模型训练的公开信&lt;/a&gt;&quot;》，到《&lt;a href=&quot;https://superintelligence-statement.org/zh&quot;&gt;针对超级智能的联合声明&lt;/a&gt;&quot;》，理想主义者反复呼吁放慢脚步、建立约束。但现实是，这些警告几乎没有改变产业的整体方向，也未能阻止更强模型和更激进应用的持续推出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤其是在战争中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从俄乌冲突、以伊冲突，再到今天在伊朗发生的舆论战，包括生成式&amp;nbsp;AI&amp;nbsp;在内的技术被广泛采用，而战场也正成为前沿&amp;nbsp;AI&amp;nbsp;技术和武器的“实验场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《&lt;a href=&quot;https://www.news.cn/mil/2024-04/24/c_1212356149.htm&quot;&gt;中国航空报&lt;/a&gt;&quot;》指出，乌克兰战事加速了&amp;nbsp;AI&amp;nbsp;在实战中的应用落地，如自主导航、目标识别和交战以及情报处理等。根据《&lt;a href=&quot;https://qnck.cyol.com/pc/content/202502/28/content_407550.html&quot;&gt;青年参考&lt;/a&gt;&quot;》，大量军事科技初创企业和国防创新企业在乌克兰聚集，使乌克兰逐渐演变为相关技术的重要孵化地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更深层的变化在于，科技公司、金融资本与国家战争机器之间，正在形成紧密绑定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月，美国国防部长&amp;nbsp;Pete&amp;nbsp;Hegseth&amp;nbsp;公布新一轮国防采购改革，明确指出原有国防体系已难以应对新的战争形态，并宣布启动新的“作战采购系统”，以缩短交付周期、提升灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;美国国防部试图引入硅谷的投资和迭代逻辑，重塑军备采购体系，让军队像科技公司一样快速试错、快速部署、快速扩张。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资本迅速跟进。今年1月，a16z&amp;nbsp;宣布新一轮募资超过&amp;nbsp;150&amp;nbsp;亿美元，其中明确投向国防科技领域的资金超过&amp;nbsp;11&amp;nbsp;亿美元。与此同时，a16z&amp;nbsp;还与美国陆军参谋长CTO&amp;nbsp;Alex&amp;nbsp;Miller&amp;nbsp;和美国海军部&amp;nbsp;CTO&amp;nbsp;Justin&amp;nbsp;Fanelli等美国军方要员共同推出播客和专栏，教初创企业如何拿下国防部订单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以色列政府通过初创公司加速器计划&amp;nbsp;Innofense、增加对本土初创企业的采购额等，系统性地推动私营技术进入军事和安全体系。围绕这一政策环境，近几年集中涌现出一批专注国防科技的初创公司和投资机构，“Patriotism&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service”成为了以色列创投圈的时髦概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;类似的转向也正在欧洲发生。2024&amp;nbsp;年，欧盟投资银行放宽了对军民两用技术项目的投资限制，并参与设立规模约&amp;nbsp;1.75&amp;nbsp;亿欧元的国防股权基金，以吸引更多社会资本进入相关领域。《&lt;a href=&quot;https://www.news.cn/globe/20251124/ec98edac049d4f879924d9a3f1d0ff7d/c.html&quot;&gt;环球&lt;/a&gt;&quot;》杂志指出，这些政策为初创企业提供了关键的早期订单和市场入口；同时，在技术、市场、资本与战略因素的共振下，欧洲初创企业大力进军军工产业，欧洲军工创业投资正迎来爆发式增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;世界正处这样的时刻：AI&amp;nbsp;的能力已被大规模引入战争中最敏感的场景，而大型科技公司缺乏主动约束自身的动力；本应推动规则协调与共识形成的国际组织，在关键议题上的作用仍然有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;伊朗现场，正在发生的&amp;nbsp;AI&amp;nbsp;信息战&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一段“万人上街支持政府”的航拍视频，在&amp;nbsp;1&amp;nbsp;月&amp;nbsp;12&amp;nbsp;日突然刷屏社交平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;画面中，伊朗记者坐在一架直升机敞开的舱门边，一边俯瞰地面“集会人群”，一边对着镜头解说：伊朗民众自发走上街头，支持本国政府，对抗美国和以色列的干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/caf6efe11878be722651ce5d8a2634de.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;镜头掠过，整条街道被伊朗国旗铺满，整齐庞大的队伍，看上去就是一场“全民拥护政府”的壮观场面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很快，这段视频就被贴上了另一个标签：AI&amp;nbsp;造假。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伊朗政策分析师&amp;nbsp;Behnam&amp;nbsp;Gholipour&amp;nbsp;公开质疑画面真实性，并谴责这是人工智能生成的虚假信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d82290fe01a54ec5caf5e74456a0c05b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有网民对画面细节提出质疑，并逐帧分析指出：记者坐在直升机舱门边，却未见任何安全防护；衣着与面部状态未呈现高速气流下的正常反应；手部动作存在异常形变；街道背景中还出现了已被烧毁的建筑……&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;质疑声越滚越大，IRIB&amp;nbsp;很快放出第二段“证据视频”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;画面里，记者坐在电脑前，播放完整的集会录像，试图证明，先前那段航拍并非伪造。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b3f0e10709366b4b727d5bc0f73a3c17.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但马上有人发现新录像存在前后矛盾之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/749ce8e117dd4d98ce3cfe8048d92efc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而另一张广泛流传的关于集会的图片，有眼尖网民放大画面，发现有人“长”在伊朗国旗上，上半身悬在空中，下半身则直接消失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a1/a17a809bdb148f8cb27a50ecc3cba6f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社交平台的评论画风逐渐一边倒，IRIB&amp;nbsp;不仅在用可疑的视频讲述“盛大集会”，还在用同样粗糙的方式掩盖伪造。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网民对伊朗官方的愤怒，很快堆积在评论区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有人开始恶搞那位直升机记者，用各种&amp;nbsp;AI&amp;nbsp;工具生成新的“伪造视频”和恶搞图。“既然你用&amp;nbsp;AI&amp;nbsp;篡改现场，那我们就用&amp;nbsp;AI&amp;nbsp;把你变成梗。”这种“以梗对梗，以&amp;nbsp;AI&amp;nbsp;反制&amp;nbsp;AI”的创作，在社交平台上快速扩散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/997976329d8201813164900890e854ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI玩梗是“技术抵抗”和消解意义的一种方式。但以AI对抗AI终不是种解法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非营利组织&amp;nbsp;WITNESS&amp;nbsp;“技术威胁与机遇”项目副主任&amp;nbsp;Mahsa&amp;nbsp;Alimardani&amp;nbsp;在网络欺骗、审查和监控领域有超过&amp;nbsp;15&amp;nbsp;年的研究经验。她在最近发表的文章《&lt;a href=&quot;https://www.theatlantic.com/international/2026/01/iran-disinformation-ai-protests-doubt/685608/&quot;&gt;怀疑如何在伊朗成为一种武器&lt;/a&gt;&quot;》中指出：“AI&amp;nbsp;对信息的操纵，以及围绕这种操纵产生的怀疑，本身都会成为掩盖真相的工具。”（AI&amp;nbsp;manipulation,&amp;nbsp;and&amp;nbsp;the&amp;nbsp;very&amp;nbsp;suspicion&amp;nbsp;of&amp;nbsp;it,&amp;nbsp;serves&amp;nbsp;those&amp;nbsp;who&amp;nbsp;have&amp;nbsp;the&amp;nbsp;most&amp;nbsp;to&amp;nbsp;hide.）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mahsa&amp;nbsp;Alimardani&amp;nbsp;回忆称，集会自12月28日爆发后仅数小时，伊朗当局相关账号就开始将抗议现场的真实影像贴上“AI&amp;nbsp;伪造”的标签。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个典型案例发生在抗议爆发后的第二天：一段在德黑兰拍摄的低清视频中，一名抗议者坐在街道中央，面对安保力量。该事件已被多方核实确认属实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但随着视频在网络上不断传播，出现了经过&amp;nbsp;AI&amp;nbsp;增强画质的版本。BBC&amp;nbsp;波斯语记者&amp;nbsp;Hossein&amp;nbsp;Bastani&amp;nbsp;发布了这段清晰版视频，但未注意到其已被&amp;nbsp;AI&amp;nbsp;处理。支持伊朗官方的相关账号随即抓住这一点，将&amp;nbsp;AI&amp;nbsp;修图留下的痕迹当作“证据”，以此否定这张照片和其他抗议影像的真实性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/197aea919a2a038d1d66ff3e6e16aa9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/744e24b3b6994950405c283f527ddb02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图为低清原视频；下图为&amp;nbsp;AI&amp;nbsp;增强后的版本。Hossein&amp;nbsp;Bastani&amp;nbsp;已就未注意到其&amp;nbsp;AI&amp;nbsp;增强特性而道歉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Alimardani&amp;nbsp;认为，深度伪造让&amp;nbsp;AI&amp;nbsp;贴上“欺骗工具”的标签，但实际上很多常用的图片编辑工具都带有生成式&amp;nbsp;AI&amp;nbsp;的能力，公众很难分辨出哪一种是善意修图、哪一种是恶意伪造。正因为如此，伊朗不仅可以利用&amp;nbsp;AI&amp;nbsp;本身，还可以利用公众对&amp;nbsp;AI&amp;nbsp;的怀疑，把这种怀疑变成一种“加速剂”，进一步压制和否定抗议信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI&amp;nbsp;如何改变现代战场&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在战场中，AI不仅影响人们理解战争，更近一步参与战争本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;俄乌冲突与以伊冲突，为观察&amp;nbsp;AI&amp;nbsp;如何介入舆论战与实际作战提供了清晰案例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欧盟资助的虚假信息意识与韧性项目团队（&lt;a href=&quot;https://www.eeas.europa.eu/delegations/ukraine/results-pro-russian-information-manipulation-and-disinformation-monitoring-targeting-ukraine-eu_en?s=232&quot;&gt;DARE&lt;/a&gt;&quot;），在调研俄乌冲突时的信息操纵时发现，相关舆论操纵活动已明显呈现出自动化、规模化特征。调查显示，水军账号不再主要依赖人工运营，而是借助&amp;nbsp;AI&amp;nbsp;工具批量生成虚假社交身份，并模拟真实用户的行为轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以&amp;nbsp;Meliorator&amp;nbsp;为代表的&amp;nbsp;AI&amp;nbsp;软件包，可以自动生成包含头像、兴趣与互动历史的账号资料，并通过技术手段规避平台的异常检测机制，使这些账号在短时间内融入正常的信息流。同时，AI&amp;nbsp;生成的图像与视频被用于构建情绪指向明确的叙事，削弱受众对信息真实性的判断能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种变化在中东地区的冲突中表现得更加直观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以伊冲突期间，一张“伊朗击落以色列F-35战斗机”的图片在社交平台迅速传播。图片中，一架喷气式战机坠毁在沙漠中，残骸周围挤满围观民众。这一画面一度让外界误以为伊朗在空中对抗中占据上风。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f5095d33f9165e34063a51a232e12c0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但图像本身的物理逻辑存在明显漏洞。现场人物与车辆比例失衡，沙地上也缺乏高速坠毁应有的冲击痕迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据澎湃新闻旗下事实核查栏目“澎湃明查”梳理，在&amp;nbsp;2025&amp;nbsp;年伊以冲突期间，基于&amp;nbsp;AI&amp;nbsp;生成的虚假视频和图像数量显著上升，规模甚至超过了俄乌冲突初期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些内容往往画面粗糙、叙事夸张，甚至直接截取自游戏画面，却频繁被用于“重构”战斗场景，成为信息战的重要组成部分。凡是包含武器、废墟或宗教符号的影像，都可能被抽离原有背景，重新拼接成一个看似连贯、实则失真的“中东战场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说舆论战主要作用于认知层面，那么从俄乌冲突开始，AI&amp;nbsp;已逐步进入直接参与作战的阶段，战场也成为&amp;nbsp;AI&amp;nbsp;技术快速试验和迭代的环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月的“蛛网”行动，集中体现了&amp;nbsp;AI&amp;nbsp;与无人机系统结合后所展现出的作战能力。在这次行动中，乌克兰国家安全局策划并实施代号为“蛛网”的特种作战，出动约&amp;nbsp;150&amp;nbsp;架远程无人机，对俄罗斯境内&amp;nbsp;5&amp;nbsp;座空军基地发动袭击，损坏包括&amp;nbsp;Tu-160、Tu-22&amp;nbsp;和&amp;nbsp;Tu-95&amp;nbsp;在内的&amp;nbsp;41&amp;nbsp;架战机。乌方称俄方损失约&amp;nbsp;70&amp;nbsp;亿美元，而单架无人机的成本不足&amp;nbsp;1000&amp;nbsp;美元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随技术升级，战争的参与结构也在发生变化。商用武器、AI技术和军事需求的结合，正在塑造一个由政府和企业共同参与的作战生态。这一模式部署灵活、更新迅速，但相应的监管与约束机制尚未同步建立。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，私营商业科技公司开始进入更核心的位置。乌克兰在冲突中广泛使用由美国民用软件公司&amp;nbsp;Palantir&amp;nbsp;提供的信息分析系统，对多源战场数据进行整合与研判，为指挥决策提供支持。相关系统能够在短时间内处理光学影像、雷达数据与火力分布信息，从而提升行动效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Palantir&amp;nbsp;是由&amp;nbsp;PayPal创始人&amp;nbsp;Peter&amp;nbsp;Thiel&amp;nbsp;创立的国防科技公司，已经成为多国国防部的供应商。就在今年1月，Palantir&amp;nbsp;与乌克兰国防科技集群&amp;nbsp;Brave1&amp;nbsp;启动&amp;nbsp;Dataroom&amp;nbsp;项目。该平台允许工程师利用大量经实战验证的数据训练和测试&amp;nbsp;AI&amp;nbsp;模型，目标之一是开发新一代自主拦截无人机，使其在缺乏人工干预、且&amp;nbsp;GPS&amp;nbsp;与通信受干扰的环境下，仍能完成探测、分类与拦截任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;科技企业正主动嵌入战争的运行机制之中。技术开始按市场与投资逻辑被快速设计、部署和迭代，战争由此进入一套新的商业-政治结构，对既有国际规则形成持续挤压。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当一段伪造影像就足以影响大规模公众判断，当低成本无人系统能够在复杂环境中自主锁定并打击高价值目标时，如何为&amp;nbsp;AI&amp;nbsp;的军事应用划定清晰边界，已成为无法回避的现实问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;当科技、资本和政治形成AI联盟&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正在美国和以色列发生的事情，为我们提供了一种更现实的视角：当科技公司、金融资本与国家安全机器深度绑定，战争的技术形态、节奏与激励机制都会随之改变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个共同趋势正在显现——私营科技公司再次被系统性地拉入国防体系核心。它们不再只是为军方提供工具的外包商，而是直接参与战争工具的设计、部署，甚至作战本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从资金流向上看，这并非零星现象。根据&amp;nbsp;PitchBook&amp;nbsp;数据，全球防务科技的风险投资在过去十年持续抬升，并在近两轮战争节点出现明显跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是交易金额还是交易数量，在俄乌冲突、加沙战争这些时间点，“发战争财”都变得异常活跃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d1/d1ce8f6c04d8cb06d736c8a32ef15dff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;硅谷回到五角大楼&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在美国正在发生的事情是，硅谷与五角大楼关系的重新加温。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然硅谷的诞生与美国国防技术的发展息息相关，但过去二十年中，风险投资企业对国防科技的关注度，从未像今天如此之高。风险投资机构们纵使不出于道德考虑，也因为昂贵的硬件、未经证实的商业路径以及传统国防承包商的垄断，一直徘徊在五角大楼门外。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这个平衡正在被打破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，政策环境发生变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://federalnewsnetwork.com/federal-insights/2026/01/military-acquisition-reform-has-important-backing/&quot;&gt;特朗普&lt;/a&gt;&quot;通过一系列行政命令和《FoRGED法案》等立法支持，对传统军工承包商施加严格的财务与绩效惩罚，同时大幅放松采购监管，以扶持高增长的科技企业。并推动一套得到两党支持的采购改革方案，核心逻辑只有一个：让军队像科技公司一样采购、迭代和部署技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月&amp;nbsp;7&amp;nbsp;日，美国国防部长&amp;nbsp;Pete&amp;nbsp;Hegseth&amp;nbsp;&lt;a href=&quot;https://media.defense.gov/2025/Nov/10/2003819439/-1/-1/1/TRANSFORMING-THE-DEFENSE-ACQUISITION-SYSTEM-INTO-THE-WARFIGHTING-ACQUISITION-SYSTEM-TO-ACCELERATE-FIELDING-OF-URGENTLY-NEEDED-CAPABILITIES-TO-OUR-WARRIORS.PDF&quot;&gt;正式公布&lt;/a&gt;&quot;新一轮国防采购改革，目标是缩短装备交付周期，为长期僵化的采购体系引入更大的灵活性。在面向国防与科技行业高管的演讲中，他直言原有的“国防采购系统”已经走到尽头，并宣布启动全新的“作战采购系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，五角大楼发布《采购转型战略》及配套指令，明确三项改革重点：一是整体转向作战采购体系；二是推进对外军售（FMS）与直接商业销售（DCS）的现代化；三是重塑联合需求审查流程。国防部释放出的信号十分明确——现有规则不再适配新的战争形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一方面，资本明确进场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就在今年&amp;nbsp;1&amp;nbsp;月&amp;nbsp;9&amp;nbsp;日，a16z&lt;a href=&quot;https://a16z.com/why-did-we-raise-15b/?campaign_id=4&amp;amp;emc=edit_dk_20260112&amp;amp;instance_id=169150&amp;amp;nl=dealbook&amp;amp;regi_id=297464440&amp;amp;segment_id=213529&amp;amp;user_id=bb34479d05a7b8849830c7aced0df618&quot;&gt;宣布&lt;/a&gt;&quot;新一轮募资超过&amp;nbsp;150&amp;nbsp;亿美元，金额占&amp;nbsp;2025&amp;nbsp;年美国所有风险投资总额的&amp;nbsp;18%&amp;nbsp;以上。新基金的领域的金额包括：&amp;nbsp;American&amp;nbsp;Dynamism（11.76&amp;nbsp;亿美元）、Apps（17&amp;nbsp;亿美元）、Bio&amp;nbsp;+&amp;nbsp;Health（7&amp;nbsp;亿美元）、Infrastructure（17&amp;nbsp;亿美元）、Growth（67.5&amp;nbsp;亿美元）和其他风险投资策略（30&amp;nbsp;亿美元）。其中&amp;nbsp;“American&amp;nbsp;Dynamism”&amp;nbsp;明确指向国防与国家安全相关产业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;a16z&amp;nbsp;的&lt;a href=&quot;https://a16z.com/defense-reform/&quot;&gt;官网&lt;/a&gt;&quot;，你可以看到这样两行露骨的文字——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“a16z&amp;nbsp;致力于推动动态的国防科技改革，以重建美国的国防工业基础。以创新保障安全。是时候行动了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/eb/eb75e2407086bdf99d281a6024c25ee0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“美国——这个创新者和建设者的国度——已经因为官僚主义和中央计划而失去了国防工业基础。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/43/43600e99c56cc8896d502ad65c870706.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月，美国陆军在官网&lt;a href=&quot;https://www.army.mil/article-amp/286317/army_launches_detachment_201_executive_innovation_corps_to_drive_tech_transformation&quot;&gt;宣布&lt;/a&gt;&quot;，正在组建第&amp;nbsp;201&amp;nbsp;分队“陆军高管级创新团”。来自&amp;nbsp;Meta、OpenAI、Palantir&amp;nbsp;和Thinking&amp;nbsp;Machines&amp;nbsp;Lab的&amp;nbsp;4&amp;nbsp;位高管，以高级顾问身份兼职宣誓加入陆军预备役。陆军在公告中表示，通过引入私营领域的专业能力，第&amp;nbsp;201&amp;nbsp;分队正为包括陆军转型计划在内的多个项目提供支持，目标是让军队变得更加精简、智能和高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca90c9df8ad5acce173b57666b0e7428.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月&amp;nbsp;13&amp;nbsp;日，美国陆军参谋长Randy&amp;nbsp;A.&amp;nbsp;George&amp;nbsp;为四名新任美国陆军中校主持宣誓就职仪式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Randy&amp;nbsp;A.&amp;nbsp;George&amp;nbsp;对面从左至右分别是Meta&amp;nbsp;首席技术官&amp;nbsp;Andrew&amp;nbsp;Bosworth、Thinking&amp;nbsp;Machines&amp;nbsp;Lab&amp;nbsp;顾问和OpenAI&amp;nbsp;前首席研究官Bob&amp;nbsp;McGrew、Palantir&amp;nbsp;首席技术官&amp;nbsp;Shyam&amp;nbsp;Sankar、OpenAI&amp;nbsp;for&amp;nbsp;Science副总裁&amp;nbsp;Kevin&amp;nbsp;Weil。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硅谷不再只是为战争“提供工具”，也开始参与战争体系的设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从金融到科技公司，以色列的“全民皆兵”模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比美国，以色列并不缺乏军队和科技企业融合的历史，大量科技公司，如Palo&amp;nbsp;Alto&amp;nbsp;Networks、Wix&amp;nbsp;的创始人都来自&amp;nbsp;8200&amp;nbsp;情报部队。8200&amp;nbsp;情报部队的退伍军人还组成了非盈利组织“8200&amp;nbsp;校友”，为青少年提供编程培训、为创业公司提供服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但真正的变化起源于&amp;nbsp;2019&amp;nbsp;年之后。当时，以色列前参谋长&amp;nbsp;Aviv&amp;nbsp;Kochavi&amp;nbsp;发起了&amp;nbsp;Tnufa&amp;nbsp;五年计划，旨在将以色列国防军（IDF）转型为一支更致命、数字化的多域作战力量。与此同时，以色列国防部（IMoD）、研发局（MAFAT）与民间机构合作成立初创企业加速器Innofense，寻找和集成能够改变战场游戏规则的军民两用技术，并为企业提供早期资金支持，加速其产品化进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2023年的10/7事件后，虽然&amp;nbsp;Tnufa&amp;nbsp;计划宣告破产，但以&amp;nbsp;Innofense&amp;nbsp;为代表的军队与初创公司合作的模式被保留下来。除Innofense&amp;nbsp;之外，以色列政府和军队还&lt;a href=&quot;https://ddrd-mafat.mod.gov.il/en/mafat-for-startups&quot;&gt;大力推进&lt;/a&gt;&quot;“绿色通道计划”（Green&amp;nbsp;Lane&amp;nbsp;Track），为初创企业和年收入不超过&amp;nbsp;2500&amp;nbsp;万新谢克尔&amp;nbsp;(NIS)&amp;nbsp;的小型公司提供精简流程，使其能够注册成为国防部的正式供应商。结果是，与标准国防采购相比，该通道大幅缩短了反馈响应时间，并放宽了合同条件，为初创企业简化了采购流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/ce/21/cee2e753de9772d789dd5bc90d3c6721.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;10/7事件，是指2023年10月7日，在哈马斯袭击以色列之初，从加沙地带潜入以色列的哈马斯武装分子对在雷姆基布兹附近参加诺瓦音乐节的平民发动大屠杀。这个事件被认为是以色列的“911”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上文提到的Tnufa计划中，以色列军队为了追求“高效、灵活”削减了一些传统的地面部队规模，导致以色列边境常规驻军过少且缺乏随时可用的预备役动员方案，增援部队花费了数小时甚至十数小时才到达受袭社区。2026年，现任以色列国防军总参谋长&amp;nbsp;Eyal&amp;nbsp;Zamir&amp;nbsp;宣布了新的多年计划Hoshen，以替代Tnufa。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片为以色列国家图书馆推出的“10月7日纪念墙”，展示了2023年10月7日以来遇难的平民、以色列国防军士兵的照片和姓名。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据公开信息，截止2025年12月，以色列国防部与超过&amp;nbsp;300&amp;nbsp;家初创公司合作，其中三分之一直接参与战争相关项目，大多为军民两用技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;10/7事件也直接影响了许多以色列金融和科技经营的投资和创业逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“过去18到24个月内成立的这批国防科技初创公司，绝大多数都是在‘10/7’事件之后才诞生的。它们源于真实的军事需求、作战需求，甚至是个人切肤之痛，并且已经经过实战验证、正在发挥作用。”Aurelius&amp;nbsp;Capital&amp;nbsp;创始人&amp;nbsp;Alon&amp;nbsp;Lifshitz&amp;nbsp;在&lt;a href=&quot;https://www.youtube.com/watch?v=Q-ZOAoTRKbo&quot;&gt;最近的&lt;/a&gt;&quot;对谈播客中表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成立于&amp;nbsp;2024&amp;nbsp;年的&amp;nbsp;Kela&amp;nbsp;是这一代公司的代表。其目标是“帮助西方防务体系快速、无缝整合商业与军事系统”，已从红杉资本、Lux&amp;nbsp;Capital&amp;nbsp;以及&amp;nbsp;In-Q-Tel&amp;nbsp;筹集&amp;nbsp;1&amp;nbsp;亿美元资金，最新一轮估值约&amp;nbsp;2&amp;nbsp;亿美元。值得一提的是，In-Q-Tel&amp;nbsp;虽为非盈利机构，其资金却来自&amp;nbsp;CIA&amp;nbsp;，它也是&amp;nbsp;Palantir的早期投资者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aurelius&amp;nbsp;Capital&amp;nbsp;则代表了这批公司背后，以色列投资机构的新风向。它成立于2025年1月，专注以色列国防领域投资，目前已经完成首轮约&amp;nbsp;5000&amp;nbsp;万美元的募资。&amp;nbsp;创始人&amp;nbsp;Alon&amp;nbsp;Lifshitz&amp;nbsp;曾经在采访中表示，他此前创立的&amp;nbsp;Haneco&amp;nbsp;Venture&amp;nbsp;因&amp;nbsp;LP&amp;nbsp;限制无法涉足国防领域，而10/7事件直接促使他与妻子另起炉灶，成立一家明确服务于国防方向的新基金。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种转向甚至开始被包装为一种“以色列爱国主义”投资叙事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在以往频繁讨论&amp;nbsp;Platform&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service、Model&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service&amp;nbsp;的以色列投资界，出现了“Patriotism&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service”的说法。以色列风投机构&amp;nbsp;TLV&amp;nbsp;Partners&amp;nbsp;在10/7事件后公开提出这一理念，并表示国防领域将成为其投资生态的重要部分。&amp;nbsp;TLV&amp;nbsp;Partners&amp;nbsp;投资的&amp;nbsp;AI&amp;nbsp;视觉识别公司&amp;nbsp;Airis&amp;nbsp;Labs，试图将日常数字影像转化为可直接用于任务的情报资产，服务于国家安全、公共安全、边境管理和应急响应等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Airis&amp;nbsp;Labs&amp;nbsp;在官网介绍，传统情报工具并非为去中心化、多模态的信息环境而设计，而以色列的对手正日益利用用户生成内容进行协调、招募和传播。借助Airis&amp;nbsp;Labs的&amp;nbsp;User-Generated&amp;nbsp;Field&amp;nbsp;Intelligence，任何来源的媒体内容都可以被转化为可计算、可调用的情报资产。短短几句的描述，已经为我们勾勒出一个《疑犯追踪》中大规模、定制化监控系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从资金规模看，以色列国防相关部门和公司的合作已经具有明显规模效应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据多方公开信息梳理，与以色列国防部研发局（MAFAT）合作的国防科技初创企业，在&amp;nbsp;2025&amp;nbsp;年通过融资和并购已吸引超过&amp;nbsp;10&amp;nbsp;亿美元资金。报道同时指出，2024&amp;nbsp;年虽然也是国防领域融资金额创纪录的一年，但全年融资规模仅约&amp;nbsp;1.5&amp;nbsp;亿美元；在&amp;nbsp;2025&amp;nbsp;年之前，该领域初创企业历年来累计融资总额约为&amp;nbsp;4.22&amp;nbsp;亿美元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多人可能已经忘了互联网开始于军用网络。而今天这些科技公司、金融资本与国家安全机器在“国防”领域的合作，无疑都在提醒我们，一个把科技当作美好创新代表的时代已经结束了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是我们今天讨论&amp;nbsp;AI&amp;nbsp;治理，无法回避的现实背景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;失效的AI治理&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，AI&amp;nbsp;治理正同时经历着道德共识、国际机制与企业自律的三重失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自&amp;nbsp;AI&amp;nbsp;行业引领者的警告一直从未缺席。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;10&amp;nbsp;月，非营利组织未来生命研究所发起了《&lt;a href=&quot;https://superintelligence-statement.org/zh&quot;&gt;针对超级智能的联合声明&lt;/a&gt;&quot;》，包括人工智能先驱杰弗里·辛顿、苹果公司联合创始人史蒂夫·沃兹尼亚克等多位知名人士参与签署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这份声明没有激起什么讨论的水花。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也许你还记得2023年3月，科技界曾发起《&lt;a href=&quot;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&quot;&gt;要求暂停更强模型训练的公开信&lt;/a&gt;&quot;》，呼吁所有人工智能实验室立即暂停训练比&amp;nbsp;GPT‑4&amp;nbsp;更强大的模型，暂停时间至少&amp;nbsp;6&amp;nbsp;个月，并建议在企业不配合的情况下由政府强制介入。结果是，没有任何一家关键公司或实验室真正停下，包括签署公开信的埃隆·马斯克本人。2023&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月，xAI&amp;nbsp;正式推出&amp;nbsp;Grok‑1&amp;nbsp;的抢先体验版本——很难相信这是一场“暂停”之后的产物。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杰弗里·辛顿频繁公开演讲、不断签署声明，但这些努力并未改变产业的集体行动方向，并未改变和他一样的聪明头脑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些公开信之后的“缺乏行动”无疑反映出，大型科技公司缺乏主动约束自身的动力，行业内部也未能形成真正可执行的治理共识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说道德呼吁无法转化为行动，本应承担“共识塑造”与规则协调角色的国际组织，同样未能填补这一真空。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;军事领域负责任人工智能峰会（REAIM），是少数能够聚集全球近一半国家和地区代表，专门讨论军事人工智能治理的国际平台。2024&amp;nbsp;年，该峰会形成了一份“行动蓝图”，提出了关于“负责任使用军事人工智能”的最低共识，例如强调人工智能应用应符合伦理、以人为本，人类仍需对人工智能的开发和使用承担责任；同时明确指出，人工智能技术应接受法律审查，并遵循包括国际人道主义法和国际人权法在内的适用国际法框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便是这样一份最低限度的原则文件，在会议期间仍未获得完全认可，约有&amp;nbsp;30&amp;nbsp;个政府代表拒绝接受相关表述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直到&amp;nbsp;2025&amp;nbsp;年&amp;nbsp;8&amp;nbsp;月，联合国才正式设立具备明确职能和常设架构的&amp;nbsp;AI&amp;nbsp;治理机制，包括“人工智能独立国际科学专家组”和“全球人工智能治理对话平台”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这份好不容易到来的“治理机制”并不试图建立一套具有强制力的普适规则，而是强调在部分议题上促进协调与共识，同时有意避开高度敏感的领域。尤其值得注意的是，独立政策研究机构&amp;nbsp;Chatham&amp;nbsp;House&amp;nbsp;&lt;a href=&quot;https://www.chathamhouse.org/2025/09/can-uns-new-ai-governance-efforts-weather-ai-race&quot;&gt;观察&lt;/a&gt;&quot;到，人工智能在军事领域的应用，被明确排除在联合国讨论议程之外，这也直接引发了对“军民两用技术”将如何被监管的广泛疑虑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在国际治理尚未就AI在军事中的应用达成广泛共识之前，AI&amp;nbsp;企业自身已率先调整了边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2024&amp;nbsp;年&amp;nbsp;1&amp;nbsp;月，OpenAI&amp;nbsp;在其服务条款中&lt;a href=&quot;https://www.cnbc.com/2024/01/16/openai-quietly-removes-ban-on-military-use-of-its-ai-tools.html&quot;&gt;删除了&lt;/a&gt;&quot;明确禁止人工智能用于“军事和战争”应用的条款，转而采用更模糊的措辞，要求用户不应“利用我们的服务伤害自己或他人”，包括“研发或使用武器”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同年&amp;nbsp;11&amp;nbsp;月，Meta&amp;nbsp;&lt;a href=&quot;https://about.fb.com/news/2024/11/open-source-ai-america-global-security/&quot;&gt;宣布&lt;/a&gt;&quot;将向政府机构提供其&amp;nbsp;Llama&amp;nbsp;生成式人工智能模型用于“国家安全应用”，并与国防承包商&amp;nbsp;Anduril&amp;nbsp;合作，开发军用&amp;nbsp;AR/VR&amp;nbsp;头戴设备和训练系统。TechRadar&amp;nbsp;&lt;a href=&quot;https://www.techradar.com/pro/meta-is-letting-the-us-military-use-its-llama-ai-model-for-national-security-applications&quot;&gt;评论&lt;/a&gt;&quot;称，这一行动与&amp;nbsp;Llama&amp;nbsp;之前的可接受使用政策存在显著差异——该政策原本禁止模型用于“军事、战争、核工业或间谍活动”，并明确禁止武器开发和宣扬暴力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年，Google&amp;nbsp;&lt;a href=&quot;https://www.wired.com/story/google-responsible-ai-principles/&quot;&gt;修改《AI&amp;nbsp;原则》&lt;/a&gt;&quot;，删除“不开发武器”“不用于监视”等明确限制条款，转而采用更模糊的表述，强调技术应用需服务于“国家安全、民主与防卫”。这打破了2018年谷歌的承诺。当时&amp;nbsp;Google&amp;nbsp;因参与五角大楼&amp;nbsp;Project&amp;nbsp;Maven&amp;nbsp;项目引发员工抗议，随后发布《AI&amp;nbsp;原则》，明确承诺不将技术用于武器开发或特定监控用途。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伦敦国王大学讲师&amp;nbsp;Nick&amp;nbsp;Srnicek&amp;nbsp;在其新书《&lt;a href=&quot;https://www.wired.com/story/book-excerpt-silicon-empires-nick-srnicek/&quot;&gt;硅谷帝国：人工智能的未来之争&lt;/a&gt;&quot;》中，描述了AI巨头们卷入美国军事行动的故事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他观察到&amp;nbsp;，科技巨头正借助“竞争威胁”的叙事抵制监管，并与国家安全体系深度绑定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年中，关键人物的立场已发生明显转变：Sam&amp;nbsp;Altman&amp;nbsp;从呼吁中美合作，转向强调“美国领导的志同道合国家联盟”；Anthropic&amp;nbsp;首席执行官&amp;nbsp;Dario&amp;nbsp;Amodei&amp;nbsp;也从担忧竞赛风险，转向主张美国必须在&amp;nbsp;AI&amp;nbsp;竞争中取胜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Srnicek&amp;nbsp;总结，这标志着“硅谷共识”的瓦解——曾以全球化与开放为目标的技术秩序，正在被技术民族主义和阵营对抗取代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;投资机构以及进入“国防”领域的初创公司，则进一步借助“安全困境”理论为自身行为提供正当性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;国防科技公司&amp;nbsp;Anduril&amp;nbsp;与投资机构&amp;nbsp;Founders&amp;nbsp;Fund&amp;nbsp;的创始人&amp;nbsp;Trae&amp;nbsp;Stephens&amp;nbsp;曾发表过一篇广为流传的文章《&lt;a href=&quot;https://medium.com/@traestephens/the-ethics-of-defense-technology-development-an-investors-perspective-45c71bf6e6af&quot;&gt;国防科技发展的伦理：一个投资者的视角&lt;/a&gt;&quot;》，为私企和资本加大对“国防”技术的投入“正名”。这篇文章的核心观点是，战争应当是“万不得已的最后手段”，对国防技术的投资恰恰是为了避免和慑止战争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，他强调更高科技的武器有可能带来更少的伤害：高度精确、由&amp;nbsp;AI&amp;nbsp;驱动的打击手段，有可能减少无辜平民的伤亡，并降低大规模、无差别攻击发生的概率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;是的，技术有可能做到这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但现实是，“精准打击”往往不顾及平民伤亡。根据冲突检测机构&amp;nbsp;Airwars，2023年10月，以色列通过&amp;nbsp;AI&amp;nbsp;赋能的监听技术锁定哈马斯指挥官Ibrahim&amp;nbsp;Biari后，对他所在地区展开空袭，袭击中超过125名平民丧生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当这群世界上“最聪明”“最有野心”的人聚集在一起，不断提高武器创新和部署的效率时，很难相信“威慑”仍是他们唯一的动机——当从战争中公开获利变得越来越容易，又有什么理由真正去阻止战争的发生？&lt;/p&gt;</description><link>https://www.infoq.cn/article/RuJMiEsSJguUXza6HsrM</link><guid isPermaLink="false">https://www.infoq.cn/article/RuJMiEsSJguUXza6HsrM</guid><pubDate>Wed, 21 Jan 2026 09:47:10 GMT</pubDate><author>陈姚戈,高允毅,王一鹏</author><category>工业</category><category>AI&amp;大模型</category></item><item><title>从数据到决策：AI 驱动的 Quick BI 架构设计与实践</title><description>&lt;p&gt;在生成式 AI 重构数据生产力的时代，BI 工具正从&quot;被动响应&quot;走向&quot;主动洞察&quot;。在 2025 年 4 月 InfoQ 举办的&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/schedule&quot;&gt; QCon 全球软件开发大会（北京站）&lt;/a&gt;&quot;上，阿里云智能集团瓴羊高级技术专家王璟尧分享了“&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6359&quot;&gt;从数据到决策：AI 驱动的 Quick BI 架构设计与实践&lt;/a&gt;&quot;”，他介绍了阿里云 Quick BI 如何通过技术架构跃迁、结合大模型的突破实现从传统 BI 到 AI 驱动的智能 BI 的跨越式进化。并重点解析领域大模型与 BI 引擎的协同设计、NL2SQL 算法调优与架构演进、AI + BI 在场景落地实践过程中的技术权衡，为行业提供可复用的技术范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：将于 4 月 16 - 18 召开的 QCon 北京站策划了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1908&quot;&gt;AI 重塑数据生产与消费&lt;/a&gt;&quot;」专题，将深入探讨如何系统化地运用大模型与智能体技术，重塑数据全链路的每一个环节。内容涵盖引擎与架构优化、数据治理、开发与运维提效、下一代 BI 与数据工具，以及智能的取数与分析等多个方向。如果你也有相关方向案例想要分享，&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;欢迎提交&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;BI 领域的技术演进及趋势&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;传统 BI VS 大模型驱动 BI&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期 BI 是在数据仓库和数据库不断发展演进后形成的需求场景：数据仓库会将各类数据融合到一起进行数据清洗和分析，随后业务人员对自助分析产生了一系列诉求，早期的 BI 工具便应运而生，像 90 年代的 Business Object 就是一个比较有代表性的例子，具有一定限度的自助式分析能力。当时其实还没有“商业智能”这一概念，随着自助分析能力要求的不断提高，可视化、自助式可交互的分析需求也越来越强烈，于是敏捷 BI 应运而生：基于可视化的自助式、可交互的分析，以 Tableau/Qlik 为代表，其最大特点是可通过拖拉拽、按钮点击等简单操作就能完成报表的搭建工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入大模型时代，大模型强大的语言分析和生成能力以及更接近人类思维的推理方式，让 BI 领域进行了重新定位：即从一个单纯的工具到数字助手的进化，模型能力的突破让正在的商业“智能”可以从 DEMO 和实验室走向实际应用。Quick BI 也在各类大模型技术发展的时代洪流中逐步演进，不断成熟与发展，实践落地智能小 Q 系列，给用户带来全新的、端到端到产品体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据分析存在局限性，它几乎主要聚焦于报表平台的工作流程：业务方提出需求，产研团队加工执行，然后制作简单的报表，以辅助管理仪表板。传统数据分析主要面向一线业务和老板，工具即使再敏捷，交付物本质上也是以固定式报表作为承载。尽管敏捷 BI 的诞生缓解了部分问题，但业务团队和数据团队之间难以融合贯通的问题依然无法避免。而在 AIGC 时代，大模型加持的对话式分析可以对自然语言灵活响应，简单、自动地完成需求，或许“人人都是数据消费者”及“数据民主化”不再仅仅是一句口号了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2a/2a52e4a61cae7cd1415b5e932d284c5d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型驱动的业务落地方向&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于用户的实际需求和大模型 Agent 技术发展，我们对大模型驱动的业务落地演进方向做了大致判断。从执行到思考，从智能到智慧，难度系数逐步增加。大模型刚出现的初期，大家都在做 Copilot（即搭建助手）：用户通过 Copilot 用简单指令或描述就能辅助搭建报表，从而降低 BI 工具的使用门槛和成本。然后是 Chat BI，理论上它会改变整个分析流程，用户像和人类对话一样向系统提问，由系统即时理解并返回准确的分析结果，所有人都可以随时随地的获取数据，降低传统 BI 报表和仪表板出现的必要性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再接着是洞察分析：基于数据、业务知识，利用机器学习算法、数据挖掘技术的融合，叠加上大模型的语言理解和推理能力，让使用传统算法的洞察分析脱胎换骨，实现更精准的总结、诊断、归因，能够自动发现数据中隐藏的价值。第四阶段可能还为时过早，很多厂商将其称为 DI（即决策智能 Decision Intelligence）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着数据量爆炸式增长和分析技术进步，如多模态、多元信息整合、多 Agent 技术等，我们可能不再满足于单个功能，产品形态会演变成分析平台主动在海量数据中发现价值，通过完整数据报告或主动 Feeds 流方式推送给我，不仅能给出“发生了什么”，还能进一步解释 “为什么会发生”、“未来会怎么样”，为用户提供更高阶的决策支持，这是也许是目前能看得到的数据分析领域的理想态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/f3/f372e926c5bffa7946edcb3b96e12d6d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于对业务落地的判断，企业级智能 BI 分析离不开 BI 工具、大模型和企业私域知识这三者的有效融合。首先，BI 工具作为核心框架，凭借强大的数据分析和可视化能力，将规模庞大和复杂的数据转化为直观易懂的图文报表，为企业搭建洞察业务的桥梁。要最大化的发挥 BI 工具本身的作用，如高性能分析引擎、可视化、安全管控、开放集成能力、协调办公能力等。其次，BI 工具并非孤立的存在，大语言模型的加入为其注入了灵魂，通过大模型理解自然语言指令，精准理解用户意图，大大降低数据分析门槛。此外，随着多模态、multi-agent 等技术的成熟，大模型的记忆、推理、规划、反思、工具使用能力反过来推动大模型在各领域的丰富应用，包括数据分析产品。最后，绝大部分企业级的智能数据应用都离不开私域数据，作为大模型应用的根据，只有将企业数据、企业内部知识、行业知识深度整合，才能让 BI 分析更具针对性和业务价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大模型落地 QuickBl 全景&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Quick BI 是阿里云上的一款 SaaS 的 BI 产品，连续 6年入选 Gartner 的商业智能和数据分析魔力象限，也连续 6 年作为国内唯一入选榜单的国产 BI 产品，承载其智能化能力的产品叫智能小 Q。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型重塑整个 BI 分析流程&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型时代，有句话说得非常到位：所有产品都值得用大模型技术重做一遍，BI 产品也不例外。传统的 BI 产品，其分析流程在模式上相对比较固定，从数据到结果，基本要经历从数据连接、到数据建模、到数据分析、到数据可视化、再到数据协同和消费的整个流程。这个流程离不开业务人员的人工搭建操作，对用户的模型理解和配置技能有较高要求。而在大模型时代，这个流程的每一个环节都有可能被重塑。例如，在数据连接环节，我们可以对数据准备的 ETL 任务进行辅助开发，对连接的数据源进行数据探查和校验；在建模上，可以对字段质量进行评估，实现计算字段生成优化和 SQL 诊断；在数据分析阶段，对报表一键美化、洞察归因和自然语言生成报表；在消费端，有 Chat BI 智能问数；最终的消费态则可以有智能决策和数据解读报告这样的形态。当我们打开思维去尝试探索后会发现，这里面的发挥空间会非常大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/6d/6d782088d629c02e2d9036390b5859bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;BI Copilot&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BI Copilot 的具体形式就是智能搭建。分析师在原有搭建报表的流程时，用自然语言替代繁杂的功能寻找、拖拉拽按钮和配置，直接完成用户想要的操作。在这个领域，我们更多瞄准的是那些高频多步的、或者强依赖分析师经验的功能。例如：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;金额大于 3000 的标红 - 就是个典型的条件格式场景；帮我美化下这个报表……&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从技术流程图可以看出，我们将原本强耦合在底层产品内部的一些能力做了解耦和开放，在渲染引擎、搭建引擎和用户会话之间构建了一整套指令系统和前端 API 层。大模型作为稳定的“中介”，负责对接会话层和指令系统，将用户自然语言意图转换成底层引擎能识别的“API”指令。在这个部分，我们基于基座模型微调了适合 QBI 搭建的增强指令识别系统，即带有指令 CMD 和参数 Params 的 NL2API。初级 API 进过指令系统的复杂处理，如依赖检测、指令调度、执行等，最终会调用暴露出来的 API 层，最终在渲染引擎和搭建引擎的加持下，完成一整套动作。不过，在这层面上 NL2API 系统相对封闭，因为大模型本质上主要是为 Quick BI 或自身应用系统内部业务服务的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/6e/6e4de6fa7f1d55dbe147f932227bbb32.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BI copilot 的另一个重要应用是数据洞察。用户对洞察的期望通常是：看懂图表 -&amp;gt;补充信息 -&amp;gt;分析和解释数据现象 -&amp;gt;定位问题 -&amp;gt;支撑行动决策。这几个步骤里，任意一个步骤想要做好都需要天时地利人和的：算法够优秀、支撑数据够多、流程组织够清晰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ac/accc0c29996ae53b73bfdae0e6676048.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前我们在洞察领域做了如下三方面的探索：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一是&amp;nbsp;内置洞察算法，这部分主要使用经典统计计算模型，毕竟智能化并不能完全等于大模型。例如，关注指标变动是否正常，若不正常，是哪些维度造成异常，本质上是参考历史数据、行业经验及其他关联数据，寻找对业务目标最具解释力的维度，这就是内置洞察算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;二是&amp;nbsp;大模型的洞察解读，将报表数据和背后所在数据集的数据以及配置元数据等信息组合，利用通用模型在数据解读、语义理解等方面的优势，通过 Prompt 工程 +Multi-Agent 的方式完成的解读方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，QuickBI 具备外置 Agent 接入能力（如 Dify 或百炼等），让客户特定的工作流和业务逻辑对接到小 Q 对话流里。作为一款通用工具型产品，一定没法满足所有用户的个性化定开需求，这算是一种体验很好的曲线救国方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;Chat BI&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今的商业智能（BI）领域，Chat BI 这一概念正逐渐成为焦点。Quick BI 已经成功落地了智能问数这一场景，这在国内国际都引起了众多企业的浓厚兴趣。目前，众多厂商的 Chat BI 产品都在致力于实现类似的功能，技术路线也呈现出多样化，如 NL2DSL（自然语言到领域特定语言）、NL2Python、NL2SQL 等，可谓是百家争鸣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 QuickBI 的智能问数为例，一个智能问数的用户旅程大致如下：用户首先输入一个问题，系统在前置处理阶段会进行权限管控和流量管理等操作。随后，我们利用经过训练的大模型领域模型对问题意图进行判断。如果该任务需要多步才能完成，系统会将其拆分为多个子任务；若单步即可返回结果，则直接进入核心流程。通过一系列召回算法，我们将元数据、知识库等信息组合起来，输入到大模型中。最终，大模型以 DSL 和 SQL 的形式将结果传递给 BI 底层的查询引擎。查询引擎负责方言翻译、高级计算下推等复杂流程，并最终以图表形式呈现结果。这些图表在呈现过程中仍然可以进行交互和调整。整个流程的关键在于，我们能够清晰地梳理从上到下的所有字段血缘关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b1/b1903ea8ab65df1c29917ca7c8809871.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;通用大模型与自研领域模型的混合流程设计&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在通用大模型与自研领域模型的流程设计中，我们秉持着开放的态度。本质上，用户的自然语言通过大模型转换为代码，代码再通过我们工程内部的方式转化为技术逻辑，最终在产品中体现为具体的展现形式。BI Agent（在某些场合被称为 AI 中间层），它通过组织编排各种大模型的输出和流程代码，实现自然语言与代码的有效连接。BI Agent 会定义一些 API 和 DSL，为了让模型能够与应用系统有效交互，我们正在通过 MCP 的方式将其能力开放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/d8/d80ee76c749d433cc3b556aabe028e6b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BI Agent 与中间层的控制中心配合，经过业务上下文处理、意图识别、任务拆分等步骤，使 BI 系统能够理解模型的返回结果，并据此进行进一步的操作。此外，在处理复杂任务时，我们还会按照正确的顺序执行编排的子任务，确保任务的成功完成。举个例子，需要获取“X 部门的经营分析报告”，LLM 本身是不会直接总结的，它需要调用取数工具先获取每个月的销量情况，再基于各种拆解数据做归纳分析，这里的“取数工具”就作为最原子化的 BI Agent 存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/67/67a637b0638a44662732cf978a3158d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;工程架构设计&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在设计工程架构的最初阶段，我们在最初阶段经历了诸多纠结与讨论。我们曾反复思量，是要打造一个代码助手，还是 SQL 插件，又或是增强版的 BI 工具呢？无论最终选择何种形态，技术路径的抉择都必须着重考量几个关键问题。首先，该系统是否具备企业级的特性。这关乎到权限管理、租户隔离等诸多方面，以及它能否与现有的业务场景无缝兼容。其次，系统的前端界面是否交互便捷、易于使用，这至关重要。再者，系统是否拥有开放集成的能力，能否提供 API 接口，是否能够嵌入到自有系统，或是接入自有知识库和数据源。此外，多场景的适应性也不容忽视。在早期，我们发现许多开源的 Demo 项目，它们仅用几行代码就跑起来一个 Chat BI，虽在特定场景下效果显著，但难以在企业级的真实场景中落地应用。最后，系统的未来拓展性也是必须考量的因素之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;智能小 Q 分层架构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图展示了智能小 Q 的整体技术架构。从图中可以清晰地看到，智能小 Q 从上至下依次为应用层、AI 中间层、自研领域大模型和通用模型层，以及 BI 基座引擎层。AI 中间层处于上层应用与大模型之间，主要承担任务分发与协同的职责。我们通过构建 API 和 DSL，实现了 Agent 与算子的有效对接，让大模型应用更具确定性，避免以前通过自然语言输入的应用表达不稳定，使得在 BI 领域的大模型的应用编程变成确定性应用编程。作为支撑小 Q 的关键部分，基座的 BI 引擎确保了数据分析的强复用性。分析引擎涵盖了从数据连接建模到复杂分析的全方位能力，而渲染引擎则承载着图表可视化及交互的重任。整套系统都是在 Quick BI 已有的能力基座上进行开发的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/fa/fa0d3347586e0f028e9fe1fed02a780e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;自研模型在 SQL 语义生成的可控性&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在自研模型的 SQL 语义生成技术路线上，目前&amp;nbsp;主流的有两种方式：Text to SQL 和 Text to DSL。我们对这两种技术路线进行了长期且深入的对比分析。Text to SQL 是直接将文本转换为 SQL 语句，直接在物理数据源上进行查询；而 Text to DSL 则是先经过一层抽象的语法，再分发到数据源进行查询。从业务特性来看，Text to SQL 在门槛较低的情况下，能够充分利用大模型的泛化能力，简化数据分析过程。然而，它也存在一些局限性。由于缺乏数据模型的抽象定义，直接对标物理表，使得大模型生成 SQL 的过程变得异常复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，大模型不可能被训练去了解市面上众多数据源的方言。以 Quick BI 为例，它支持四五十种方言，如果要对私域数据进行私有模型训练，成本将难以控制。而且，即使是同一数据源的不同版本，如 MySQL 5.7 和 8.0，它们支持的函数也有差异（如开窗函数），这对大模型来说并不友好。从技术限制角度而言，DSL 的灵活性相对较弱，其查询能力受限于 BI 引擎的能力边界。从适用场景来看，Text to SQL 更适合门槛较低、没有复杂业务分析要求的场景；而 Text to DSL 则更适合业务场景明确、面向大型团队和企业级应用的场景。对于 QuickBI 来说，技术路线从纯 Text to DSL 到 Text to SQL to DSL，再到混合模式，可谓是吸收各个路线的优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;一个问数问题的工程链路&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们从工程链路的角度剖析一下问数。用户问了一个相对复杂的问题后，经过模型的复杂链路处理，包括 Agent 路由、各种实体召回，由自研 BI 大模型生成 DSL，经由工程端的查询参数构造后，发给查询引擎进行取数。查询分析引擎会处理复杂计算字段（如 LOD 函数、自定义抽象函数）、注入用户的行列权限等，最终翻译成物理 SQL 和内存计算进行取数处理。从下图的例子可以看到，相对抽象的 LOD 函数会被稳定转义成适配不同数据源方言的 JOIN, 大大降低了模型生成 SQL 的难度和稳定性。在这种工程架构下，可以解决传统 NL2SQL 面临的三大关键问题：1）保证可用，具备企业级的管控能力，如完备的权限能力、开放集成能力等；2）保证可信，BI 引擎的引入降低模型生成原生 SQL 的难度，对取数的每个链路都做到逻辑有迹可循，查询元数据血缘透出，提升结果的可信度；3）结果可交互，复用了 BI 丰富的可视化能力，在生成的图表后链路上可修改并进行二次查询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e8/e8ec17c7c9c5b507ea17dd03322930ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;工程架构设计&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;NL2SQL 算法的挑战&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9f/9f45f7869153af23822b3a2fd766194c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与通用模型处理的其他类型问题相比，NL2SQL 算法领域面临的挑战主要集中在以下三个部分：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义的模糊到精确：自然语言天然是非精确的，同样一个意图可以有多种不同方式的表达，而 SQL 代码及执行是精确的，用户对结果的正确性的容忍度非常低。因此 NL2SQL 天然属于模糊到精确的多对多映射问题。语言结构化：SQL 是“结构化查询语言”，而对比与 Python/C++ 等其他编程语言是过程化的语言。这里有什么区别呢？过程化语言可以做片段化的逻辑生成，对模型推理的要求偏低，但结构化语言需要结构和逻辑整体正确，难度相对大些。在问数任务的绝大部分场景下，用户的问题只提供了信息的局部，只是回答信息必须上下文的很少一部分。有更多信息以企业内部约定俗称、表元数据的简称、数据的具体内容形式存在。这点相信在这个领域的各位应该也有比较深刻的体会。事实上，有大量的 Query 之外的“隐藏信息”需要补全，而这又无形中对系统的配套设施提出了更高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在训练模型前，对要达到的效果做了定义和预演。即什么样的数据分析助手是“好”的助手？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先在风格和调性上来说，主要有以下几点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有效性，模型必须保证准确和稳定，即单位 token 的有效信息密度高，不啰嗦；准确性，在最小可用的数据和关联拓展之间做一个平衡。用户问数往往是看一系列数据，不是单个数据，我们会在某些场景下主动给一些关联数据，实践下来带来会给用户一些“小惊喜”；在复杂任务上的表现，过程中逐步规划、反馈，通过多个简单任务组合解决复杂任务。当然，这里如何拆分子任务、子任务的粒度也是另外一个较大的话题了（过于原子化和过于抽象都是有问题的）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次对于大模型能力，主要有以下几点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基础能力稳定性高：在问数基础、高频场景下需要稳定且高准确度的表现，避免过多的过程性解释；在数据分析场景下的专业性：模型训练能够对数据分析师、业务常用的分析思路有理解，能给出专业的数据建议。比如用户问单个指标的时候，同时看一下指标趋势也没有坏处；规划、矫正能力：具备将复杂任务拆解为用户易于理解。易于干预的子任务，能根据不同的上下文、用户提示，矫正复杂任务的执行规划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;一个问数问题的大模型旅程&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从算法的视角来重新看问数的链路：大模型在生成抽象 SQL/DSL 的过程中，经历了元数据选取、上下文添加、问题改写、完整 Prompt 构建、输出、转译等步骤。。这其中最重要的一步就是领域模型的训练，领域模型训练需要足够的信息来进行正确推理，这些信息主要包括任务描述和通识能力，例如大模型不知道今天是哪天，我们需要将当前时间戳加入其中。其次是表和字段信息，这是非常关键的，如果没有表的字段信息和维度枚举值，对于 NL2SQL 来说将是一场灾难。再者是私域知识库，相关的知识条目以及是否做强制改写等，都会给大模型提供启发。另外，参考样例也很重要，什么是好、什么是坏的，我们在产品上通过点赞、点踩等方式给大模型提供真实的 few-shot 示例。经过各种选表问题改写 SQL 等流程后，最终生成的 Prompt 会交给领域模型完成推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2c/2cb526347e2d724d85714b6306cf32a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;基于 BI 引擎的 NL2SQL 算法演进&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面有讲到，最初我们定义了特定的查询语言 DSL, 用于表达对于不同查询参数的描述，由大模型直接学习并生成 DSL，再通过中间层将抽象的 DSL 在元数据和知识库的召回后实例化，转换成实际 QBI 的查询参数执行真实的取数；这里的几个好处，比如 SQL 方言屏蔽、高级计算能力复用等等。但随着支持的问数能力越来越多，问数的意图千变万化，要准备这套 DSL 语义的样本成本在逐渐增大，毕竟 DSL 是我们自定义的，通用模型训练并不含这部分内容。同时，各类通用基座模型本身对意图转简单 SQL 确是有大量积累的。于是我们在单表查询的标准 SQL 基础上拓展了抽象函数和高级计算符，变成增强 SQL 语言，以训练基座模型对于增强 SQL 的生成来提升对复杂意图的理解准确度，然后通过自研语法解析器来改写成 DSL 映射。也就是说，增强 SQL 和 DSL 是可以稳定转换的。这样既能巧妙利用通用模型的能力，又能大大降低训练样本的准备成本。至此，复杂查询意图到取数流程就被串联起来了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ff/ffe928a1440dabdaaf1591e03a0db2fd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;Text2DSL：丰富的算子和函数&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于上述架构的最终选择，有两个重要因素支撑才能成立。第一个是丰富的算子和函数，得益于 Quick BI 内置大量逻辑函数，如聚合数值处理、文本处理，以及 LOD 函数、时间算子等。例如，计算环比对于 SQL 来说可能很复杂，我们会将大量复杂分析场景定义封装在这些算子和函数里，大模型在生成增强 SQL 时不需要感知这些复杂内容，它只需要知道如何使用这些算子和函数即可，这有点像现在流行的 Agent 方式。第二个是完善的数据模型，我们作为 BI 系统本身就支持很多关联模型，包括自定义 SQL 模型，如单表星形、雪花星系等经典 BI OLAP 模型。实际上，我们会将复杂的多表关联合并和嵌套查询下推到数据建模层，这些信息对大模型来说是透明的。大模型不需要感知这些，因为 Chat BI 不仅仅是 NL2SQL 算法的炫技，更重要的是解决实际客户的问题。有时我们会将复杂建模放在前面，对于整个大模型来说，它只是一个单表的、带有各种复杂函数的 SQL 生成逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/0a/0a3460dccd2dad346c52d7e66689266e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个重要的计算逻辑是多步计算。多步计算是为了解决一些纯 NL2SQL 无法处理的问题，转而通过 NL2Python，或者说 NL2Python Agent 的方式来解决。举个简单例子，询问销售金额日环比超过 40% 的用户有哪些？我们可能只能算日环比，超过 40% 对于人来说很简单，但实际上这是一个多步计算的解决方案。在这个流程中，大模型会进行多次推理，这里的 Python Agent 会触发大模型在当前输入上进行二次推理。通过合理的任务拆解，可以降低整个复杂问题的解决难度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/bc/bc167a6e6542bbbc1403cebeb4e95533.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，关于领域模型的训练，我们这边的训练主要分为三个部分：继续预训练、微调和 GRPO。简单来说，在预训练阶段，我们会把 Query 质量不高但有大量抽象 SQL 的东西作为预训练的一部分。在微调阶段，我们会把高质量的 query 和 SQL 对应关系放到微调中进行训练。最终通过强化学习 GRPO 的方式把整个模型训练好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/d3/d3d49cf81959a339e1570032bbb74186.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型与好数据：训练数据准备&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型是离不开好数据的，只有大量 + 优质的训练数据加持，模型才有可能突破。下图是我们数据准备的一个数据飞轮：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面我们依赖了人工构造，我们有一只专门的数据团队去构造、收集复杂的训练数据。其次，利用模版、AI 去生成，各类大小模型的结合提升训练数据的质量和覆盖。然后是数据蒸馏，对一些复杂问题，我们会通过大模型训练小模型的方式。与此同时，我们还会在数据准备过程中利用若模型生成一些有价值的错误，这些样本随后可以在执行引擎的协助下执行验证并进行错误归纳，相当于反例的标注，这对于训练非常有必要。我们实践下来发现，有价值的错误在整个训练过程中是非常有必要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/6e/6e0ee67d5c397156b183879312d2fda9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;业务价值与展望&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;智能小 Q 客户实践场景&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以零售品牌为例，它们直接利用我们的 SaaS 产品来推进功能演进。还有快消品客户，更是将我们的小 Q 直接嵌入到他们的业务系统中。实际上，在客户那里，我们进行了一些实践和调优工作。作为一个通用的 BI 工具，无需任何配置的前提下想要直接达到 90% 以上的准确率是不现实的。事实上，脱离具体应用场景谈准确率，多少有些不切实际。这里有一个案例，一开始我们完全没有介入时，准确率仅为 65%。通过交付过程中的介入，引导用户如何使用、如何提问，以及让用户通过点赞、点踩的方式参与，我们的模型可以自动进行 SFT。在这种场景下，最终将模型强化后的问数准确率提升到了 92%。主要提升点在于指标维度的扩充、指标覆盖等，让用户尽可能多地提供信息，针对复杂问题进行自动拆解。很多时候，客户的问题并非单纯的问数问题，比如他们可能会让你去分析一下某个情况，这就需要进行问题拆解。此外针对无法回答的问题，提供用户提示，即拒识方面的引导优化也是非常有必要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/5c/5c196793e974597f9349b7c64d245bd2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;Bl 未来发展&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前大模型擅长的包括：语义理解、代码生成、分析思路、文本生成、任务编排… 我认为大模型在以下几个方面会有长足进步，具体进步包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;动态推理能力：包括任务拆分、逻辑推演、冲突解决；多模态感知能力：未来可能会整合跨平台的报表数据、根据截图、报告来挖掘出更有意义的数据科学部分；模型能力本身的持续进化、自我反思机制的增强：在整体水位线上能让智能来的更加真切；自主决策能力：非预设路径的行动生成和决策。在更高的角度来看，随着大模型这些能力的持续进化，我相信将会推动智能 BI 从任务执行者向决策主体跨越，进而让整个领域在交互模式和能力边界上都有相应的变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/83/83a33a3b44a3853047da61d0166a2d02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王璟尧，毕业于浙江大学信电系，10 年数据产品建设和技术架构经验。现任阿里云智能集团高级技术专家，Quick BI 数据智能研发负责人，负责 BI 平台架构、新一代智能 BI 建设等工作，在元数据管理、BI + AI、大模型应用等领域上有丰富经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从基础设施、推理与知识体系，到研发与交付流程，再到前端、客户端与应用体验——AI 正在以更工程化的方式进入软件生产。2026 年&amp;nbsp;QCon 全球软件开发大会（北京站）将以&amp;nbsp;「Agentic AI 时代的软件工程重塑」&amp;nbsp;作为大会核心主线，把讨论从&amp;nbsp;「AI For What」，走向真正可持续的&amp;nbsp;「Value From AI」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/01/016a6e281287b30423b759d9f9056ef8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xK8RMrfsu3070hI90ij4</link><guid isPermaLink="false">https://www.infoq.cn/article/xK8RMrfsu3070hI90ij4</guid><pubDate>Wed, 21 Jan 2026 09:45:29 GMT</pubDate><author>Kitty</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>Node.js之父宣判“手写代码时代结束”！DHH明确反对“结束论”：大模型还差口气，手写更有竞争力</title><description>&lt;p&gt;Ryan Dahl 在 1 月 20 日给软件工程下了结论：“人类写代码的时代已经结束。”留下的工作里，不包括继续手写语法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/946fc5bc57726ec280668a561ab00ce0.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果这话出自某个科技网红，大概刷过去就算了。但 Ryan Dahl 不一样——他不仅写出了 Node.js，后来还“推倒重来”做了 Deno。你可以把他的意思理解为：写代码这部分会越来越自动化，而人的价值会更多落在判断、取舍和责任上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在 Ryan Dahl 这次“宣判”之前，1 月 3 日，Ruby on Rails 作者 DHH 也在 X 上连发多条，语气罕见地偏“乐观派”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“别让那些粗制滥造和尴尬翻车，遮住你对 AI 的惊叹。自从我们把计算机连上互联网以来，这是我们让计算机做到过的最令人兴奋的事。如果你在 2025 年一直对 AI 悲观或怀疑，不如在 2026 年的开端，用一点乐观和好奇再试试看？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，社区里迅速冒出一种更夸张、但传播力极强的解读：“DHH 都松口了。”“连最不买账的人都开始给 AI 站台——你还有什么理由不用？”甚至有人干脆把它说成：“DHH 也扛不住了，最终还是向 AI 屈服低头了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你真去听 DHH 的原话，会发现所谓“DHH 屈服论”，并不是那么回事儿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/9821fb2526cb52cda4d0c35374638be5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在最新一期播客中，他说在37signals，AI 没有在写真实产品，更谈不上“从零写出什么东西”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他在用 AI，而且每天都用，但更多是做那种“一发入魂”的小实验；一旦进入真工程：要持续演进、要迭代、要打磨，他就会觉得：“这在浪费我的时间，到这一步我自己写更快。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以他们的新产品Fizzy 里 95% 的代码，还是人类亲手敲出来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还补了一句：我们离那种“AI 让一切始终更好、更快、更省心”的明显拐点，还差一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“就现在而言，我仍然在意代码的样子。我在意它的美感。我在意打磨、推敲、润色。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更关键的是，他不是在怀旧。他明确说：“手写代码依然有竞争力。”“至少在此时此刻，这是一个仍然有竞争力的选择。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且他的判断正好和 Ryan Dahl 相反：“我们并没有到 AGI，没有到那种‘人类写代码的时代死了’的程度。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;挺好玩的是，DHH还说要远离 Anthropic 的 CEO：他一听到那种“再过五分钟就不需要程序员了”的口吻就火大，直接开喷：“你们到底用的啥模型啊？”反正他自己用的是 Opus 4.5（或当下版本），但在他的体验里，这种“程序员马上下岗”的说法完全不符合现实——尤其是那些要长期维护、持续迭代、不断演进的真实工程，离“五分钟结束”差得十万八千里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是 DHH 播客整理全文翻译：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“如果浏览 Web 的不再是人类”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：欢迎大家来到《Next Token》。今天这期节目对我来说有点特别，可能要追溯到 25 年前。很高兴请到 DHH——David Heinemeier Hansson。欢迎你。&lt;/p&gt;&lt;p&gt;DHH：很高兴来，谢谢邀请。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我猜你可能是刚从赛车里下来（笑）。&lt;/p&gt;&lt;p&gt;DHH：现在是休赛期，正好歇一歇。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人（Torsten）：那我就先来点“热血沸腾”的话题。我从 2010 年左右就开始关注你，你可能是对我影响最大的前五位程序员之一。如果没有你，我可能不会走到今天。我职业生涯中有七八年都在写 Rails，看了你所有的书、博客。我们其实从没见过面，但有一次“交集”让我印象极深——我发过一条关于 Cookie Banners 的吐槽推文，那是我人生中传播最广的一条推文。那天中午我被 Cookie Banners气疯了，随手发了一条，然后彻底炸了。第二天你转推并评论说：“这就是为什么人们不再浏览 Web，而是开始用 ChatGPT。” 所以我想直接问你：欧盟最近说要“取消 Cookie Banners”，你觉得这真的能改善什么吗？还是说——已经太迟了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我认为 Cookie Banners是 Web 体验变得糟糕的一个主要原因。它们几乎比早期那种弹窗广告还要糟糕——你知道的，“打地鼠”“打猴子”那种 2000 年初的弹窗。当年浏览器还能通过技术手段封杀弹窗，但 Cookie Banners没有一个统一、有效的技术解决方案。我知道有插件能挡，但大多数人不会装。结果就是：Cookie Banners成了互联网的一场瘟疫。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我是丹麦人，所以我觉得我有资格狠狠吐槽欧盟。Cookie Banners最初的出发点是“高尚的”——限制数据收集、提高透明度。但这套东西在第一个 Cookie Banners出现 5 分钟后，就已经被证明是失败的。可欧盟花了整整 15 年，才开始承认这个问题。现在他们说要“移除”Cookie Banners。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但“移除”是什么意思？你以为这就能抹掉你对互联网造成的破坏吗？不可能。接下来 30 年，仍然会有大量网站继续保留 Cookie Banners——因为删掉它比留着更麻烦，或者网站早就没人维护了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一件非常悲哀的事。当然，我并不是说：如果没有 Cookie Banners，人们就不会去用 ChatGPT。 那不现实。但它确实在可测量的层面上伤害了 Web，让浏览体验变得远比必要的程度更糟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦你已经在用户体验上制造了第一道伤口，后面再多来几刀，心理成本就低多了。Cookie Banners把“底线”拉得太低了，以至于很多 Web 设计师会觉得：再多放点广告、再恶心一点，好像也没那么糟。 这就像“破窗理论”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在 Cookie Banners把 Web 搞成这样之后，你觉得互联网浏览的未来会走向哪里？&lt;/p&gt;&lt;p&gt;如果未来主要“浏览 Web 的不再是人类”，那这些问题还重要吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：这是一个好问题。我觉得现在有很多聪明的人都在试图搞明白这件事，我们也在尝试各种不同的做法。某种意义上，这真的很像上世纪 90 年代中后期——当时我们在摸索互联网的第一个版本：这一切究竟会怎么运作？谁会掌握权力？谁会成为平台？谁又会成为把关者？所有这些问题，如今再次被抛回到空中，悬而未决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不管我个人怎么看它最终会走向哪里，我都觉得这是一件令人兴奋的事情。互联网和计算技术，已经很久没有像现在这样让人感到兴奋了——上一次有这种感觉，还是在 2007 年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那是 iPhone 刚刚问世的时候，我们迎来了一个全新的形态。随后经历了很长一段时间：好，一切都转向移动端了。而现在，我们又站在另一次巨大的转折点上——这一次，不只是“移动”不再以同样的方式重要了，它不再是你思考和构建产品时的那个主导视角。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，还有大量没有答案的问题。如果人类不再亲自阅读互联网内容，因此也不再阅读广告，那究竟是谁在为互联网写作？谁还会去生产那些美好的内容？当我们摆脱了 cookie 弹窗，重新拥有一个“干净体面”的门面，这件事真的还重要吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果这件事本身已经不再重要，如果人们不再想为互联网写作，那 AI 又将从哪里获取它所需要的信息？我觉得现在有太多悬而未决的问题，以至于没有任何人哪怕稍微知道，最终的解决方案会是什么样子。而这，恰恰是活在这个时代最令人振奋的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我毫不怀疑，将来我们回头看今天这个时刻时，会说：“好吧，这里发生了一次决定性的变化。”而且，这种变化在当下的可感知程度，甚至比前两次都要更明显。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;互联网的出现，花了五六年的时间才真正渗透进社会，对整个社会产生巨大影响。后来是手机，速度快了一些，但也没有快到哪里去——iPhone 本身也经历了好几代迭代，我们一开始甚至都没有 App Store，这些东西都是慢慢才出现的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 AI 不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 的出现，在当下这一刻就已经非常明显。任何一个用过第一版 ChatGPT 的人，都会立刻意识到：哇，这完全是一个全新的东西，它将重写规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，在这三次巨大的技术变迁中——互联网的诞生、移动时代的到来，以及现在的 AI——这是第一次，我们在实时发生的过程中就清楚地知道：世界一定会变得完全不同，而我们却不知道最终会变成什么样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，我觉得你能做的最好的事情，就是接受三点：第一，我们不知道答案；第二，这真的令人兴奋；第三，赶紧上车，狠狠干脆坐稳了，看看它会把我们带到哪里去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为还有另一种冲动，过去在互联网时代出现过，在移动时代也出现过：那就是一部分人会说，“我更喜欢以前的样子。我喜欢变革发生之前的一切。我不喜欢 AI。我不喜欢也许会被整个互联网重新中介化。我不喜欢这些东西。我们能不能把一切都倒回去？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不，不能。你没有这种权力。你无法把这些东西倒回去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你当然可以在个人层面选择：我不用生成式 AI，或者我不买任何包含 AI 方案的产品。但这种想法，本质上是一种“阿米什式”的思维方式——而在任何时代，这都只是非常小众的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果这就是你，如果这就是你想与世界互动的方式，那很好，祝你一切顺利。我们有时候确实需要一些“疯子”来提醒我们：事情也可以用完全不同的方式来做。但这，并不会改变历史前进的轨迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“这真的是一个无比令人兴奋的时代”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你的兴奋更多来自哪里？是因为规则被打乱、棋盘被掀翻？还是因为你真的想用 AI 做事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：首先也是最重要的一点，我热爱计算机。我喜欢看到计算机做出以前做不了的新事情。说实话，让我觉得非常惊讶的是：有这么多在科技行业工作的人，其实并不怎么喜欢计算机——甚至包括那些每天都要和计算机打交道、让计算机“跳舞”的程序员，也并不是所有人都真的喜欢计算机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我不一样。我爱计算机。我真的爱计算机本身，爱的是它作为一台机器的纯粹性。我并不是只把计算机当成一种“工具”，不是只想用它来完成某个目的。确实有一大类人，把计算机仅仅视为通往某个结果的手段。但不是这样，对我来说，这要更深得多——我就是单纯地热爱计算机这个东西本身，也热爱看到它去做全新的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现在发生的这件事，是计算机在我这一生中做过的最令人兴奋的新事情之一，至少可以和当年“计算机连上网络”这件事相提并论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那时我们从 Commodore 64、Amiga 时代走过来，突然“砰”地一下就上网了，用小小的调制解调器拨号，连接世界各地的 BBS，听着它唱出那种刺耳却又美妙的声音——那同样是一次巨大的转变，也彻底改变了我和计算机之间的关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而现在，很可能是第二次这样规模的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一件让我感到兴奋的，是棋盘被彻底翻转了。尤其是我们已经形成了一些根深蒂固的格局。比如 Apple，我和那家公司有过不少摩擦。我非常期待看到 Apple 通过 App Store 以及整个移动生态所建立的那种“封闭控制”，被彻底掀翻，因为它也许将不再以同样的方式重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我也并不天真到以为：只要棋盘一翻转，接下来就会迎来一个人人和谐共处的“涅槃世界”，一切都会变成开放平台，没有任何人占据主导地位。这显然不可能发生。不管最终的主导者叫 OpenAI、xAI、Google，还是别的什么名字，某种形式的集中和垄断，迟早都会出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但至少在现在，我们还处在“尚未整合”的阶段。有这么多公司同时在追逐前沿模型，却没有任何一家明显胜出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在五秒钟前，整个科技行业还准备给 Google 判死刑——“他们错过了浪潮”，“早期研究是他们做的，《Attention Is All You Need》那篇论文也是他们团队出的，但后来落后了整整九个月”，当时大家已经在谈论 Google 的衰落了。而现在，他们也许又重新回到了领先位置，至少在某些领域确实如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种不确定性本身就让人兴奋——我们并不知道，最终谁会占据主导地位，甚至都不确定“主导地位”这种东西是否一定会出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事也很有意思。就在几周前，我还在推特上说，跑本地模型这件事有点“奇怪”。因为我之前试过一些本地模型，说不上什么时候，总之那时体验一般。但就在这周，我又开始重新跑本地模型，然后我心里想：“靠，我之前说的话，保质期也太短了吧。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现实变化的速度已经快到：三个月前说的任何一句话，现在看起来都可能有点傻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我真的被本地模型现在的水平震惊到了。它们当然还比不上最前沿的模型，但如果再往前看两年呢？有没有一种可能，根本不会出现一个“唯一的赢家”？赢家反而会是开放模型？最终的局面，会不会类似开源软件对后端软件世界造成的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去我们是有绝对主导者的。我们有过 Sun，有过 IBM，在某种程度上也有过 Microsoft。但这些都已经不存在了。整个后端世界——从 Linux 到各种数据库，再到 Ruby、Rails，以及所有这些东西——几乎全都是开源的。你再也看不到那种一家独大的绝对统治。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在另一边，在前端世界，尤其是移动端，我们却看到的是彻底的垄断：只有两个赢家，Google 和 Apple。他们对平台拥有完全的控制权，而且还在不断收紧螺丝。我们唯一的希望，似乎只剩下立法或监管，而说实话，我对这条路也已经越来越悲观了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以现在的局面真的很令人兴奋——它可能朝两个完全不同的方向发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们很可能还是会走向某种形式的垄断，因为这是面向用户的界面层。而在历史上，我几乎想不起有哪个时代，这种层面没有被“征服”过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但也有另一种可能：这些开放模型会好到一个程度，以至于“谁占据商业主导地位”这件事根本不重要，你甚至不需要那种商业上的统治。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这真的是一个无比令人兴奋的时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们的产品也试过 AI 功能，但最后都没上线”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这挺有意思的——你正好是在这个变动时期推出新产品。HEY 大概是五年前发布的，然后最近 Fizzy 也上线了。我们特别想知道：37signals 内部现在到底在发生什么？你们到底怎么用 AI？你们做 Fizzy 的时候，用没用 AI？用到什么程度？我很想听点“细节层面的现实”，AI 在 37signals 具体怎么落地、怎么被用起来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：哦，用的，当然用。我们每一个开发者都在某种程度上使用 AI。我自己每天也在用 AI。&lt;/p&gt;&lt;p&gt;但我也得先加一句前提：我虽然对我们即将进入的新现实非常兴奋，但我每天处理的仍然是“此时此刻真实存在的东西”。你必须学会在“ hype 的列车”和“现实的列车”之间保持平衡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在我的“现实列车”里，AI 没有在写 Fizzy（一个Kanban工具）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 也没有从零写任何东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我确实用过 AI 做过各种“一发入魂”的实验——但它们通常都只停留在“一发入魂”。因为只要我进入真正的细节：要持续演进、要迭代、要打磨，我就会想：“嗯，这就是在浪费我的时间。到这个阶段，我自己写反而更快。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，AI 在另一些方面确实能大幅加速。我们在做这些产品时，也在一定程度上使用 AI。但我们并没有大量用 AI 来写 Ruby 代码。如果用 AI 写 Ruby，通常也只是“机械式翻译”——比如：“这里有个我们知道已经存在的东西，你能把它用 Ruby 版本写出来吗？” 它能给出一个初稿，有时候会稍微帮点忙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 更有价值的地方是在我们的一些 Go 代码上，因为那里面“样板代码”更多，收益更明显。&lt;/p&gt;&lt;p&gt;但即便是 Ruby 和 Go 这两块，也谈不上“改变游戏规则”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正改变游戏规则的是：&lt;/p&gt;&lt;p&gt;你想学习一个新 API你想理解一个新概念模型或者我们做实验，直接用 AI 去尝试构建“能真正带来价值”的 AI 功能&lt;/p&gt;&lt;p&gt;在这些方面，收益更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我们离那种——某些 CEO（比如 Anthropic 的 CEO 那种语气）说的——“再过五分钟我们就不需要程序员了”还差得远。我就想问一句：你们到底用的是什么模型？我用的是 Opus 4.5（或者现在的版本），但那种说法完全不符合现实——至少对于“持续演进”这类工作来说，是完全不成立的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我仍然保持开放心态，我也能看到那种承诺。我记得互联网在 1994、1995 年那会儿是什么状态，我当然能做外推：我们也许真的会走到那一步。也许我们会到一个阶段：人类不再编写大多数代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但如果你看 Fizzy：95% 的代码，是人类亲手敲出来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有意思。真的？你们内部也这样认为？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：你回头看 Fizzy 的整个开发历史，会更有意思。我们在 Fizzy 里做过一堆 AI 功能实验：我们试过做一个 AI 驱动的命令行，用来和卡片（cards）交互；我们也试过 AI 摘要，给一些内容自动做总结。但最后这两项我们都没有发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Basecamp 也是一样：我们实验过很多不同的 AI 功能，但没有一个能达到“明显更好、用户会一直爱用”的标准，所以都没进最终版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我仍然相信未来这会改变。只是我们现在还没到那个时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也见过其他地方做得更成熟的案例。比如我在 Shopify 董事会，Shopify 做的 Sidekick（他们的 AI agent）——用来帮助商家搭建店铺、优化店铺——真的很不可思议。那里面有一些非常具体、非常可触达的收益，我觉得几乎无可争辩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们仍然处在一个阶段：距离“AI 让一切始终更好、更快、更省心”那种明显的拐点，还差一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为还没到那个拐点，所以才会出现一些反弹——我认为其中不少反弹甚至是合理的。&lt;/p&gt;&lt;p&gt;因为很多人用了所谓“AI 功能”之后会觉得：“这玩意儿太烂了。”“不更好，也不更快，甚至很蠢。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如摘要。我们刚刚还提到 Apple。Apple 对新闻、短信之类的摘要，我真不知道有多少人真喜欢开着它。它在很多情况下都离谱地糟糕、离谱地错误。连 Apple 这种体量的公司都做不对，那你基本可以合理推测：很多别的公司也同样做不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过我也想强调：最近我们确实找到了几个非常好的 AI 用例。其中一个是我们的安全漏洞赏金项目（通过 HackerOne 运行）。我们会收到海量的报告——某个研究员声称在我们的应用里发现了漏洞。我们必须处理这些报告，而现实的数学非常残酷。我们大概会收到……可能一个季度 300 份报告之类的数量。但真正“靠谱、有效、值得修”的——大概只有 3 份。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也就是说，真正有价值的比例大概只有 1%。而这个 1% 非常重要，因为它们可能真的指出了一个严重问题，我们必须修。但为了抓住这 1%，你必须花巨大精力去验证剩下 99% 的垃圾——这对团队来说是巨大的麻烦、巨大的时间黑洞、巨大的烦躁来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 在这件事上简直太厉害了：它能在报告进来时就先处理一遍，给我们一个初步判断——“这到底是扯淡，还是不扯淡？”然后还会帮我们写回复邮件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而写回复其实才是痛点的一半：当 99% 的提交都是彻头彻尾的狗屎，写这些狗屎的人还常常—— 根本不懂自己在说什么，却又特别理直气壮，还特别不耐烦，甚至还一副“你必须立刻给我 5000 美金赏金”的态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候让人类程序员保持冷静、不直接对他们开喷，是很难的。真的，你会很想直接骂人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 就完全没这个负担。它特别乐意用一种非常冷静的语气写一大段回复：“为什么你这个东西不成立。”它帮我们省了大量时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有意思。所以 AI 是拿到报告之后，去看你们代码库，然后判断它到底对不对？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：对。没错。就是这样。把这两件事结合起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来需要一点技巧：拿到安全报告，很多是垃圾，但到了某个层级，你确实得打开代码去确认“这到底是不是真的”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：以前要看 100 份报告，现在可能只要看 5 份——这就是真实的生产力提升。就算你最后要看 10 份、20 份，只要你能把原本 100 份的工作压缩到 20 份，这就是 AI 承诺的生产力收益。如果我们能把这种压缩能力用到业务的其他方面——那简直太好了。这也是为什么我们一直在尝试把 AI 用在一些具体环节上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个我们断断续续尝试了好几年的方向是客服支持（support）。但 support 很微妙：如果你只能 90% 正确，那其实很糟糕。因为这意味着你会有 10% 的概率把事情说错——而且是对着客户说错。你如果给客户一个完全错误的答案，让客户体验很差，客户可能就直接流失了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那这个客户的终生价值是多少？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你以为 AI 带来的那点“节省成本”，可能瞬间就被一次流失抵消得干干净净。我们上一次认真测试让 AI “做完整客服链路”，大概是 18 个月前左右。效果不太行。但一切都在飞速变化。我知道 Intercom 有一个叫 Finn 的 AI agent，采用得很好，看起来我们也确实该再试一次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这又回到我最初的那种兴奋：一切变化太快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些人会觉得这很让人迷失方向，我觉得这也是很多焦虑的来源。但如果你像我一样，只是单纯喜欢看计算机变得更强大——那现在真的就是一场大戏。坐在第一排，实时看它发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们从“那个吃意大利面的人”——看起来像噩梦一样的生成图——走到了今天这种几乎不可区分的输出。接下来，我们很可能会在更多领域看到同样的跃迁。你得保持一种“敬畏感”和“惊奇感”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你此刻身处这个行业，和计算机打交道——你的“惊奇感”就是你的安全绳。它能对冲焦虑，对冲不确定性，让这一切变得可承受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我们并不能消除不确定性和焦虑。比如：我的工作三个月后还存在吗？这种焦虑非常合理。但你可以用惊奇感来对冲它：“这些硅做的小东西也太聪明了吧。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI时代，为什么你发布的产品别人看不见？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：它们真的很神奇。这就引出了一个更大的问题：软件商业模式的未来到底会怎样？这确实很神奇，但也真的太不一样了。你能不能展开讲讲：创业公司会走向哪里？软件产品会走向哪里？软件工程师会走向哪里？未来到底会怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：有一点我现在非常确定：今天发布一个新产品，从“把它做出来”的角度看，是史上最容易的。AI 让构建更容易；工具史上最好；Ruby 和 Rails 也从未如此成熟。对所有人来说，这都很棒。结果就是：市场被海量新产品发布淹没了——永无止境的“上百万、上亿级别”的新发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是你现在要面对的现实。门槛被降低了。而我不确定所有人都会在“轮到自己发布时”还为门槛降低而兴奋——因为你一发布，可能就是一片寂静，连个回响都没有。我们刚发布 Fizzy，算是一次不错的发布，但它并没有像我们历史上某些发布那样“声量巨大”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这当然不只是 AI 的原因，还有社交媒体算法的原因。以前，我在 X（Twitter）上有粉丝，他们就能看到我发的东西。但现在，你会发现：X 上正在发生 Facebook 在 2010 年左右发生过的那一幕——你有粉丝，但你触达不了他们，除非你付钱给平台“买触达”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在甚至都不只是“付钱”这么简单。问题变成：我甚至都看不到我合伙人 Jason 的推文了。除非他发了一条“爆款（banger）”，爆到病毒式传播，否则他的内容就不会出现在我的 For You 页面里。一切被压缩成了“你能不能发出爆款”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拥有大量粉丝这件事的价值，被严重稀释了。我在 X 上有五十多万粉丝——这在我发一些犀利观点、能引起传播时依然好用。但当我想发“右勾拳”（也就是营销、转化）的时候，它不再提供过去那种收益。当然，这种变化也不全是坏处。现在小账号也可能爆：就算你只有 10 个粉丝，只要你发了一条爆款，算法也可能把你推上去。算法选赢家和输家的方式，反而让那些没有花 20 年积累粉丝的人受益。但这真的好吗？我大概发了 7 万条推文——这真是离谱。但 18 年下来，这些投入几乎没有“可积累的剩余权益”（residual equity）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不确定这是不是我们长期想要的生态。但可以确定的是：对我们的营销方式、产品发布方式来说，这已经是一个全新的世界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们公司现在的阶段是：我们能承受“靠一靠、观望一下”，说一句“挺有意思”。但如果你还处在“必须打出名气”的阶段，你肯定会更焦虑。因为以前那套打法，已经不像过去那样奏效，你得发明新的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，这种认知直接影响了 Fizzy 的发布策略：我们承认——你不能再用老办法发布产品了。你手里的名单、你已有的受众，不可能再用“传统方式”被激活。你需要持续不断的“滴灌”：一滴、一滴、一滴。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果我们希望 Fizzy 这个品牌能在用户心里留下印象，以至于当他们遇到我们要解决的问题时，会想起它、会去 fizzy.do，我们就必须设计一种策略，让我们能一直这样做下去。这也部分解释了为什么我们从一开始就把 Fizzy 开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把 Fizzy 从发布第一天就开源——&lt;/p&gt;&lt;p&gt;对所有想学习“生产级 Ruby/Rails 应用如何构建”的人来说，这是一个巨大的礼物；同时，对我们来说，它也给了我们一个“更频繁谈论 Fizzy 的许可”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在社交平台上，纯商业化的转化号召（call-to-action）越来越推不动。以前它传播力也一般，但好歹还能“硬塞”一下——那就是所谓的“右勾拳”。现在右勾拳打不出去，你就得换一种卖法。我目前觉得最管用的策略，是把“给价值”和“求转化”合成一拳：轻击（jab）和右勾拳（right hook）不再分开打，而是同一条内容里同时完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如我会发：“Fizzy 里有个很酷的小功能——可能是我们做的，也可能是社区做的，或者我只是想提醒你注意到它。”这条对开发者有用；与此同时，我也顺势把品牌名反复露出来：Fizzy、Fizzy、Fizzy……品牌就是靠重复进入脑子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键是：重复仍然有效，但必须绑着价值一起出现。光当“慷慨的好人”持续免费输出已经不够了——你得把输出和你正在做的产品强绑定。这就是我们现在的打法。当然规则也可能随时被改写，但就此刻来看，这就是现实的游戏规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你说“现在你只要把东西做出来就行”，这句话听起来很有趣，因为我觉得你以前不会这么说。你从一开始就很重视营销——从最早的 Rails demo、到各种“挑衅”、到你如何推销愿景……你一直都在想怎么卖、怎么讲故事。但现在市场被淹没了，好像营销反而变得更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更巧的是，我们内部也在聊类似的事。我们在做 AMP（我们在做一个 coding agent），我们内部一直说：现在外界没有太多“强烈的 OTE”（那种外溢式的注意力/势能）。我们想做的是：用一个故事把人“拉着走”——告诉他们我们在这个动荡的时代学到了什么，让他们产生一种感觉：“如果你跟我们走，门是开着的；如果你跟我们走，我们会分享我们学到的东西。”这不是那种“社交媒体上再来 10 个小贴士”的套路，而更像是：“我们一起干这件事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而你刚刚说的，正好对应了很多人最近在讲的： “爆款发布（big launch）这套已经不灵了。”Product Hunt 死了。Hacker News 的 launch 也……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我认识 Fizzy，就是因为 Jason 一直在 X 上做这些小 screencast：“现在进展到哪了”、“这里出了一些 X 问题”、“这里哪里又崩了”。我会偶尔刷到它们，可能是 Grok 或者算法觉得我会喜欢。但我的感觉是：我被“拉着走”了——像在跟着你们一起把产品做出来。所以我后来才注意到：噢，原来它上线了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：你说得对，这确实是我们这个时代发生的巨大变化之一。我记得我们在 2006 年写《Getting Real》（那本书）时，我们谈过“爆款发布（blockbuster launch）”这套模型：先放 teaser（预告），再放 trailer（预热视频），最后来一个 blockbuster launch（大爆发）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套模型已经死了。爆款不再发生。因为我们已经没有共享文化了。没有共享的事件。我们只有每个人各自的个性化信息流——正如你说的，算法之神决定：今天给你投喂哪一小块“刚好合适”的东西。所以，一方面，你必须“灌满渠道”（flood the channel）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一方面，也有个有意思的反面：以前我会更克制，比如提醒自己别发太多推。有时候我会突然进入那种“多条意识流同时开喷”的状态，但在过去你会想：“哎，我今天已经发第七条了，会不会太多？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在这种限制不存在了。你一天发 100 条都没关系。因为你不会“淹没”任何人的 For You 页面——算法会替你处理。而你发得越多，你就越有机会让一些小种子落地、生长、发芽。你还需要更长的周期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;爆款发布以前的核心逻辑是：“就在这一天，我们发布，然后所有人都在这一天关注。”现在不会了。大家不会在同一天关注同一件事。但随着时间推移，如果你把“发布”理解为：一整个季度、或者一年、甚至某些情况下是一整个十年——你依然可以做“分步骤的搭建”，依然能起作用。因为营销的底层真价值仍然成立：口碑传播、故事激活、好产品、好钩子——这些依然有效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;只是，它变得慢得多。你不会再看到那种巨大峰值，然后被“发布日的高潮”爽到。某种意义上，现在的发布没有那个“超级尖峰”了。当然，很多人本来也从来没有过“超级尖峰”，因为大多数发布都什么也不会发生——失败一直是常态。但我现在更强烈地觉得：你越来越难“工程化制造一个爆款”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个夏天我又学到（或者说被提醒）了一点。我在做一个项目叫 Omarchy——一个 Linux 发行版。我做得很开心。当我推进它时，我从营销角度体会到：如果你不断分享项目进展、再配合一个疯狂的发布节奏，价值非常大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得第一个月我做了大概 40 次发布？简直离谱。节奏快得惊人，整个过程一直都充满了不确定性，所以特别刺激、特别带劲。这让我可以连续三个月“轰炸”所有人的信息流。更有意思的是：人们明明意识到自己在被轰炸，却仍然无力抵抗。我收到过无数条推文，大意都是：“行行行，我第 17 次听说 Omarchy 了，我服了，我试一下。”“我投降，好吧，我装。”这又回到了营销最本质的东西：重复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个老的经验法则（我也不知道现在是不是过时了）：你需要听到一个品牌 七次，它才会在你遇到问题时被激活——你才会想起它能解决什么。所以我当时就是在努力让尽可能多的人“听到七次”。同时我也在做 Jason 说的那个：enthusiasm transfer（热情迁移）——把创作者的兴奋感转移给别人。这一直是营销的一部分，但现在比以前更重要，因为营销越来越“人格化”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还发现：社交平台从来就不怎么喜欢公司账号，但现在它们几乎把公司账号都“幽灵化”了。我们公司账号发什么都没用：从 37signals 发，没人理；从 Basecamp 发，也没人理。一片寂静。然后我看到一些“巨型媒体账号”——几百万粉丝那种——表现也一样惨。这就是算法：它现在真的讨厌品牌账号。除非你是那种“神级品牌账号”——有账号运营团队，能自己成为内容源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但另一部分也让我们意识到：这游戏即便对我们而言仍然很残酷——而且很耗人。这种耗人让我想起我听一些 YouTuber 讲过的东西：如果你是 influencer（网红）、content creator（内容创作者）——这俩词简直是现代词汇里最让我厌恶的词之一——你就会被迫持续生产内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你维持曝光的方式只有一个：不停输出、不停输出、不停输出（chop chop chop）。以前还有一种“喘息”：你做完 teaser、trailer、爆款发布，然后你还能休息五分钟。现在不行了。那种节奏不存在了。所以一切的速度被推到一个夸张的程度。说实话，我很庆幸我现在不需要“去攒人生的第一桶金”了（笑）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们最近也在高频发东西：过去 10 天我们写了 8 篇 release post。这和你做 Omarchy 的方式很像：你需要重复。但那种 5 年前的“空洞重复”已经不行了——比如：“两天前我们大发布，记得吗？”“一周前我们大发布，记得吗？”这种完全没效果。你必须一直有新内容，否则算法不推。节奏太夸张了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在我们这个做 AI agents 的领域，你还会被大模型厂商不断“催更”——他们两天发一个新模型，用户两天后就来问：“你怎么还不切？怎么还没上新？”所以现在疯狂的事情特别多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的问题是：你写过《It Doesn’t Have to Be Crazy at Work》（工作不必这么疯狂），但现实已经如此——这在实践中到底怎么改变软件开发？你一直是小团队、小公司路线的拥护者。 但现在如果你想让产品成功，你好像必须把一天切成两半：一半写代码，一半发推、做内容、做传播、分享进展。你觉得这会怎么影响未来的软件开发者/软件公司？营销和软件是在融合吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我一直都说：这些东西本来就是一回事。“Marketing is everything（营销就是一切）”——这是《Rework》里的一章。而“everything”真的就是一切：软件、发布、客服、那些乱七八糟的推文、写作、播客……全都是。我们这么干已经 25 年了。但我同意：现在的节奏、算法的胃口，确实到了一个“无底洞”的程度，这种感觉以前没有这么强烈。不过我也觉得：这可能就是竞争加剧的样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当年我们做 Basecamp 的时候，行业比现在小太多了。那时做 Web 产品的团队少得可怜，以至于我们能关注到每一次发布。后来进入 Product Hunt 时代，你至少还能“一天看一个新东西”。现在结束了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;甚至 OpenAI 发一个新模型——那可能烧了 4 亿美元——它也只能获得几个小时的峰值关注与兴奋。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，它在很多方面变得更难了。可另一方面，基本面依然没变，你得小心别被这些压力带着跑偏。做有趣的东西、做值得讲的东西——这带来的杠杆还在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你要“脱颖而出”的难度变大了，因为参与者更多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但只要你真的突出，注意力仍然在那里。注意力并没有从系统里被抽走。甚至可以说：注意力比以往更多，因为参与系统的人更多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这有点像 Spotify。你总听音乐人抱怨 Spotify 付得太少，但你再看数据：音乐产业的规模依然很大，甚至更大，而且在很多情况下，更多收入是直接流向音乐人（因为他们不再必须签那些苛刻的发行合约）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以一部分现实就是：我们在抱怨“事情太美好了”，但又没有人真的开心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有个段子讲得很好：“一切都很棒，但没人开心。”我觉得这确实说中了某种人性。事情确实很棒：越来越多人能更快地做出东西。而这自然会带来更多竞争。资本家最讨厌的一件事是什么？是竞争。这就是那个系统最大的讽刺。我们都在拼命挖“护城河（moat）”。但护城河是用来挡谁的？不是挡“龙”（Not dragons）——是挡竞争对手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;竞争对手，这才是护城河真正要挡的东西。这个隐喻本身也很有趣：你会想，那它把谁“圈”在里面？客户？你在护城河里放鳄鱼，让客户别游出来？这个隐喻挺自利，也挺资本家叙事的。但无论如何，我玩这个游戏，也乐在其中。同时我也很高兴——现在我比过去任何时候都更清楚地知道：我对“什么真正有效、什么无效”的确定性变少了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一直以来，很多东西本就是谜。比如我们 2004 年发布 Basecamp，它一路成了现象级成功，今天仍然成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我经常会想：为什么？为什么偏偏是 Basecamp？在我 25 年的职业生涯里，我做过很多东西，但没有任何一个产品层面的命中，能像 Basecamp 这么“正中靶心”。我至今也不完全明白原因。尤其是现在，Basecamp 所在的领域竞争者多得多。但每周仍然有成千上万的人注册一个新的 Basecamp 账号。每周我都会想：这怎么可能？怎么会每周都有几千几千人来注册？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一直是个巨大的谜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得这种谦逊非常重要——无论你在做产品、还是在做营销，你都要记住：你不可能了解一切。你不可能确切知道什么有效、什么无效。你能做的，是去尝试很多东西，然后得到一些迹象、一些推力、一些暗示：市场想要什么、算法想要什么、客户想要什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你不可能制定一套“主战略”，并指望它具备可重复的复刻性。即便是在一个高度“爆款驱动”的行业——比如我刚刚提到的音乐行业——也没人真正搞明白。的确，有些人比别人更擅长做出爆款，但也没有谁掌握一套公式：“照着这套流程，我们就能稳定生产爆款。”商业也是一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;只是现在曲调又变了。你可以因此沮丧：“我以前那套把戏不灵了。”也可以因此兴奋：“什么？那我更迫不及待想学习——现在到底什么才有效！”我也接受一个现实：我不可能永远拥有过去拥有的一切。世界不是这样运作的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“独立开发者”之梦没变：核心还是“一个人也能干”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我感觉我们好像回到了 2004 年。我记得你发布 Basecamp 的时候，你在 YC 还是哪里做过一个演讲，你当时大意是说：如果你有个想法，然后能找到 1000 个客户，每人每月付你 25 美元，你的人生就彻底不一样了。那次演讲就是我决定辞掉 Web 开发工作、去做 Dropsend 的起点——也开启了我整个职业生涯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得我们又回到了那种状态：现在你真的可以有一个想法，甚至可能是“一人团队”。所以，我们现在是不是就处在这个阶段？还是说，所有 indie hackers（独立开发者）最终都会被“吃掉”？这难道不是好事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我也觉得这是好事。而且这里还有个讽刺点：我 20 多年来一直在讲——开发者生产力真的重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是 Ruby 和 Rails 的核心前提：你不需要一个八人团队，你一个人也能做出来。Rails 从一开始就试图成为“单人开发者的框架”，而且我认为它在这件事上比几乎所有框架都做得更成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我们今天对 AI 兴奋的原因也一样：我们对小团队能获得的杠杆感到兴奋，因为 AI 能做很多事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个根本事实没变：当你降低实验成本、降低构建一个“值得做的东西”的生产力成本时，你就会有更多“射门次数”（shots on goal）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ruby + Rails 能做到这一点；AI 也能做到；甚至更好的是：AI + Ruby on Rails 一起做到。&lt;/p&gt;&lt;p&gt;但我不确定游戏的本质在这点上发生了根本变化，也许只是变得对更多人可及了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得这大概率是好事——不，只能说：这就是好事。我们应该从“对人类整体有什么分类级别的好处”来理解：对全人类而言，难道不是更好——我们有更多实验吗？即便最终“命中并变成可持续商业”的人，可能比例更低（我甚至不确定这是否属实，但先这么假设）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而作为一个文明整体，我们最终仍然会在更多类别、更多细分领域里，更快地获得更好的软件。问题的一部分在于：无论是 Web 开发圈，还是独立开发者（indie hacker）圈，很多讨论都过于短视地集中在那些我们一直反复折腾的“通用大类”上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如待办事项应用。好吧，我职业生涯里大概已经做过七个了，而全球可能已经有二十亿个同类产品。最后真正成功的，可能也就那么几个，剩下 99% 都失败了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你知道吗？你有没有试过给美发沙龙做软件？他们可没有一万种选择。有时候，他们甚至几乎没有任何选择，除了那些“狗屎一样”的系统。那种三十年前做出来的烂软件，出自一些对“好软件”毫不在意的人之手。所以，如果你愿意跳出这些吸引了绝大多数人的大而泛的领域，其实机会依然多得很。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;颇具讽刺意味的是，我自己长期以来恰恰以“不去碰这些方向”为傲——只解决我自己的问题。因为我觉得那样更简单，而且也确实如此：当你解决的是自己的问题时，你立刻就能判断你做出来的软件到底好不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并不意味着它一定会成功，但至少你有了第一道过滤器。如果让我去给美发沙龙做软件，我其实并不知道什么是好、什么是坏，我得不停地去问别人：“你们怎么看？你们给我什么反馈？”老实说，我不确定自己是否适合为了正在构建的软件，去进行这么多和他人的互动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我认为，对那些愿意这么做的创业者来说，机会是非常多的，而这其实也是大多数人。只要我们稍微把视野放宽一点，不要总是说：“天啊，现在再做一个新的待办事项应用太难了。”因为这个领域在过去三十年里，已经被来来回回地“薅”了大概五十亿次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你往外看——就只要离开它五米远——到处都是一大片未被开发的绿地。真的，到处都是。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;DHH 说 95% 代码是手写的，但他又天天用 AI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：David 你说 Fizzy 95% 的代码还是手写的，对吧？你每天都在用 AI。但对我来说，今年正好相反：我现在大概 90% 的代码都是 AI 写的。所以我的疑问是：如果你说你不怎么用 AI 写代码、或者 AI 不替你写代码——那生产力提升到底从哪里来？尤其对一家小公司来说，比如给美发店做软件，它不需要庞大的客服团队，也不需要很多外围部门，核心就是把软件做出来、交付出来。所以你觉得 AI 让软件开发更快的关键在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我说说我自己的体验——从这波 AI 开始我就一直在用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的生产力提升，主要来自：它让我更强、更聪明、更快——&lt;/p&gt;&lt;p&gt;更快上手新 API、新技术更快理解新概念（我会让 AI 解释给我听）更快找到“为什么这个 bug 会这样”的正确线索&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如 Omarchy 这个项目，如果没有 AI，它就不会存在。我不会有耐心去 Linux 论坛里翻半天，去解读那些晦涩的错误信息到底是什么意思。这对我来说不可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 带来的巨大提升，是给了我一个地方，把错误信息贴进去，然后得到比那种居高临下、还过时三年的 Stack Overflow 回答更好的线索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;收益巨大。真的巨大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有我需要读某个东西时、学习某个东西时，它也很有帮助。举个快例子：我们最近把 Rails 的 CSRF 防护机制改了——从以前“把 token 放进 cookie”的方式，改成使用现代浏览器的新特性：通过一个 header 来做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我可以直接问 AI：“那个 header 是什么？”“什么时候开始支持的？”“具体有哪些细节？”这些答案我当然也能手动查：去 caniuse.com、看历史、查 RFC……全都能做。但 AI 能把这些东西一盘端上来，整合在一起，省事又快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“AI只是让我变聪明了”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我能更快学到更多东西。而这正是我真正喜欢的地方：不是让 AI 替我做事，而是用 AI 让我更聪明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，这种模式未来未必会成为主流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就像你说的，你已经让 AI 写很多代码，甚至多数代码。我完全准备好在某个时点，我也会进入那种状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但就现在而言，我仍然在意代码的样子。我在意它的美感。我在意打磨、推敲、润色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这可能是一种“奢侈”，有点像现代的马鞍匠：他会在意字母压得是否刚好、针脚是否完美。你可以说：“但你已经不是交通运输的主力生产体系了。”我会说：那又怎样？只要我还享受，我就会继续做我手写代码的“马鞍”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我也意识到：这种模式目前仍然是有竞争力的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 37signals，我们并不觉得自己在产出能力、发布能力、改进能力上落后。因此我对一些说法保持怀疑：“AI 已经强到可以把标准 SaaS 公司的一半程序员裁掉，还能跑得更快。”我没看到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我当年也用同一套“根本测试”来审视云计算：“我们能不能用更少的人、花更少的钱，做更多的事？”我们几年前退出云，就是因为这个测试没有通过。而且我也不太听说这个测试在别处通过过。云计算并没有让你把运维团队砍半、把基础设施预算砍半。很多时候恰恰相反：上云之后团队规模翻倍，账单翻四倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们切换之后是不是省了类似每月一百万美元？很夸张的数字？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我们现在大概是一年省 200 万美元。我们云预算峰值大概是 340 万美元，现在的持续成本在 100 多万美元左右。所以在成本上，节省非常巨大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这和 AI 有一些相似之处——不完全相同，但有相似之处：我觉得现在很多人在用 AI，脑子里觉得自己“好高产”，但他们其实交付更少、做出来的东西更少，甚至理解得更少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“Vibe Coding”的风险：能力会从指尖流走&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DHH：AI 还有另一个因素：当我尝试“氛围式写代码”（vibe coding）的时候——尤其在一个我还没完全内化的新领域——我能明显感觉到我的能力在从指尖滴走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我刚开始做 Omarchy 时，写了很多 bash。我以前从没系统写过大量 bash，最多就是命令行里用用。然后我发现自己一次又一次问 AI：“某个 if 条件到底怎么写？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时你就会想：“为什么我没有内化这件事？我没内化，是因为我把它外包给 AI 了。”那这样更好吗？我现在更划算了吗？还是说，我跟当年那些老师一样天真：他们以为有了计算器，学生就不需要背乘法表了？不对。如果你不能迅速在脑子里算出 7×7，你真的会把自己变成傻子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你有没有形成一种直觉：该在哪里划线？你不可能知道一切，对吧？你也会把你不会的事交给信任的同事去做，你不会因为让同事设计某个东西就觉得“能力在流失”。你能接受：“这事我不需要会 / 我不想会”。那在 2025 这样疯狂的一年里，你有没有更清晰的边界：哪些你想自己掌握、哪些你可以忽略？比如 bash。为了推进 Omarchy，你觉得 bash 该学到什么程度？又有哪些可以不学？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：我觉得我得会几乎全部，除了怎么在 bash 里搞数组（笑）。因为 bash 里数组那玩意儿复杂得离谱，简直反人类。但我其实认为：人类大脑是个很惊人的器官，它不会像 LLM 那样“容量到顶就装不下”。我们用得越多，记忆和能力的“配额”会增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我真正担心的趋势是：随着时间推移，我知道得更少、我变得更不胜任。我需要一条向上增长的移动平均线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不需要把所有领域都吞进去——我不需要什么都懂。但一年结束时，我应该在更多领域懂得更多。如果我不在这种上升轨道上，我会无聊。我无聊就会没动力。没动力我就什么也不干。这也是 AI 讨论的一部分：我们得想清楚，我们真正享受这套方程式里的哪一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人不享受当项目经理。我会做——而且不止偶尔——因为我想要“组织一群人”能产出的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但当我看 AI 这件事时，我不想当一群 AI agent 的项目经理。那不是我想要的状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我喜欢写代码。而至少在此时此刻，这是一个仍然有竞争力的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，这可能三个月后就变了；下周就变了；随时都可能变。但 AI 公司那些领袖已经预言“再过五分钟就结束了”预言了很久了——现在也没结束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你看 AI 公司自己，它们也还在招聘大量程序员。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们并没有到 AGI，没有到那种“人类写代码的时代死了”的程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并不否认你说的：有些程序员已经觉得自己大多数代码都让 AI 写了。但至少在市场上——按我看到的情况——还没有出现那种“压倒性差距”，就像：一个公司用马车送啤酒，另一个公司用卡车送啤酒。那种经济差距会非常快把前者淘汰。我还没在 AI 身上看到这种情况。也许数据有滞后；也许已经发生了——我仍然怀疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便我在长期上是极度“AI 乐观派”，但就当下，我没看到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;有时神得离谱，有时烂得没法维护&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：而且原因之一是：我每天都在“盯”着它。我一直在问 AI：你能给我写这段代码吗？&lt;/p&gt;&lt;p&gt;它会写。然后我会想：“不，我不喜欢这个。”“我甚至不想维护它。”“它做得还不如大多数初级程序员会被要求做到的水平。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但偶尔，它也会给出另一种答案：我问它一个东西，它拼出来的结果让我震惊：“它怎么知道的？它怎么能把这些全部串起来？”那真的很惊人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我感觉它像一个闪烁的灯泡：你在完全黑暗里，它突然一闪——你觉得“我什么都看见了”。两秒后，啪，又全黑。如果你能让这个灯泡稳定下来、一直亮着——那对人类当然是巨大的福音。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;顺便说一句，我很喜欢美国的一点就是：美国把这个“闪烁灯泡”当成一种信仰——相信我们能把它变可靠，能到 AGI。现在大家就是一场巨大的押注：押注这一定会发生。即便我这么 AI 乐观，我仍然会对这种规模的“集体确信”感到惊叹：一个经济体一起说： “不管花多少代价，100 万亿、1000 万亿，我不在乎，我们一定能到那里。”我会想：这也许就是为什么它会成为“第一名”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：确实是个令人兴奋的时代。就像你说的——能活在此时此刻本身就是一种奇迹。我们也差不多到一小时的时间上限了。今天能和你重新连上线真的很开心，感谢你抽时间来。你现在也在忙 Fizzy。要不你简单跟大家说说：Fizzy 是什么？在哪能了解更多？然后我们就收尾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DHH：当然。Fizzy 在 fizzy.do。它是对 Kanban（看板）的一个全新诠释。这里还有个小故事：Jason 特别擅长解释“为什么值得回头重新解决一个问题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kanban 这个概念来自 50 年代，是丰田为了管理生产线提出来的。后来我们把它做成了软件。第一代软件化的版本大概是 2000 年初。再后来 Trello 出现，把这个领域彻底带火、带爆。但我们还是回到这个领域，说：“你知道吗？我觉得我们还能做一个更好、更舒服的版本。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人很难理解软件这件事：明明一个问题领域已经有很多玩家了，为什么你还要进去？原因可能只是：你想做得更好、更有趣、更轻量、更丰富多彩、更令人愉悦、功能更少——这些带着“爱”的细节，我们都烘焙进了 Fizzy。而且我们把它定价得很便宜：1000 张卡片免费，之后是 每月 20 美元。同时我们也把整个代码库开源了：如果你想自托管（self-host），你可以免费用。服务器我们不替你付，你自己折腾就行。你也可以贡献代码，也可以从中学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做 Fizzy 是一件很快乐的事，而且它也像一个实验室。我们现在正在做 Basecamp 5。我们在 Fizzy 上尝试了很多新技术——不管是编程层面还是产品层面——我们会把最好的想法带回 Basecamp 5。如果你关心我对这些话题（或任何话题）的观点，你可以去 dhh.dk，我的东西都在那。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：太棒了。很高兴你来做客，也迫不及待想看未来会发生什么。感谢你的时间，我们下期再见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uWqno4HM4xA&quot;&gt;https://www.youtube.com/watch?v=uWqno4HM4xA&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/ClaudeCode/comments/1qhiicv/the_creator_of_nodejs_says_the_era_of_writing/&quot;&gt;https://www.reddit.com/r/ClaudeCode/comments/1qhiicv/the_creator_of_nodejs_says_the_era_of_writing/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NJFBOuIqvvyUjc4BQhk5</link><guid isPermaLink="false">https://www.infoq.cn/article/NJFBOuIqvvyUjc4BQhk5</guid><pubDate>Wed, 21 Jan 2026 08:26:45 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>XAML Studio 开源：一款老牌原型工具，如何在 WinUI 时代重获生命力</title><description>&lt;p&gt;Microsoft 已正式将&amp;nbsp;&lt;a href=&quot;https://devblogs.microsoft.com/ifdef-windows/xaml-studio-is-now-open-sourced/&quot;&gt;XAML Studio 开源&lt;/a&gt;&quot;，并将其纳入 .NET Foundation 体系。XAML Studio 是一款轻量级的快速原型工具，面向基于 XAML 的 UI 开发。该工具最初作为 Microsoft Garage 项目的一部分，通过 Microsoft Store 发布，如今其&amp;nbsp;&lt;a href=&quot;https://github.com/dotnet/XAMLStudio&quot;&gt;GitHub 仓库&lt;/a&gt;&quot;已向社区开放，欢迎开发者参与协作与贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;XAML Studio 的起源可以追溯到 2017 年的一次黑客松。当时诞生的首个原型名为 XamlPad+，目标是重振 WPF 时代的经典工具，例如 XamlPad、XamlPadX 和 Kaxaml。最初它只是一个业余项目，但在随后的数年中不断演进，最终发展为一款专注于 WinUI 和 UWP 原型设计的独立应用。在开源之前，其开发主要由内部团队推进，仅有部分组件通过&amp;nbsp;&lt;a href=&quot;https://github.com/CommunityToolkit/Windows&quot;&gt;Windows Community Toolkit&lt;/a&gt;&quot;&amp;nbsp;对外共享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次开源版本包含了面向 2.0 版本的一系列重大更新，这些改动目前集中在&amp;nbsp;&lt;a href=&quot;https://github.com/dotnet/XAMLStudio/tree/dev&quot;&gt;dev 分支&lt;/a&gt;&quot;中。更新内容包括：基于 Fluent 设计原则的大幅 UI 重构、对 WinUI 3 的更深度集成，以及实时数据上下文、属性检查器、布局对齐工具等新功能。新版还重新引入了一些最初原型中已有、但未能进入 XAML Studio 1.0 的功能，例如文件夹支持。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/0b/d2/0b6bb184920dbefce94238a120af3cd2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;XAML Studio v2（开发中）的屏幕截图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将 XAML Studio 开源，标志着该工具在维护和开发模式上的重要转变。作为 .NET Foundation 的&lt;a href=&quot;https://github.com/dotnet-foundation/projects?tab=readme-ov-file#project-levels&quot;&gt;种子项目&lt;/a&gt;&quot;，它被寄予长期社区共建的期待。GitHub 上的公告也提到，团队早在 2017 年 8 月就有开源的想法，但直到现在项目在成熟度上才真正适合面向公众开放开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要注意的是，虽然当前代码库已经开放，但 2.0 版本尚未被视为稳定版本。探索该仓库的开发者应当了解，其中部分功能仍处于实验阶段，相关文档也在持续完善中。对传统 UWP 工作流的兼容仍然有限，与 Visual Studio 的集成目前也仅支持外部预览，而非完整的设计器替代方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便如此，此次发布仍有望为 WinUI 和 XAML 开发者带来实际价值，尤其是那些希望获得快速反馈、又不想承担完整项目脚手架成本的场景。同时，开源也为后续改进奠定了基础，使社区能够参与功能贡献、问题跟踪以及路线图讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感兴趣的开发者可以访问该项目的&amp;nbsp;&lt;a href=&quot;https://github.com/dotnet/XAMLStudio&quot;&gt;GitHub 仓库&lt;/a&gt;&quot;，查看最新&lt;a href=&quot;https://github.com/dotnet/XAMLStudio/issues/34&quot;&gt;路线图&lt;/a&gt;&quot;，并通过讨论区或 Pull Request 参与其中。Microsoft 的&lt;a href=&quot;https://devblogs.microsoft.com/ifdef-windows/xaml-studio-is-now-open-sourced/&quot;&gt;官方博客&lt;/a&gt;&quot;以及 GitHub 上的讨论帖，也提供了更多关于项目历史与发展愿景的&lt;a href=&quot;https://github.com/dotnet/XAMLStudio/discussions/26&quot;&gt;背景信息&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/xaml-studio-open-source/&quot;&gt;https://www.infoq.com/news/2026/01/xaml-studio-open-source/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/JCw7RzpyL4DUPvC0kHwE</link><guid isPermaLink="false">https://www.infoq.cn/article/JCw7RzpyL4DUPvC0kHwE</guid><pubDate>Wed, 21 Jan 2026 08:00:00 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>开源</category></item><item><title>飞猪搭建系统演进：从人工运营到多Agent协同 搭投生产</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;飞猪搭建体系介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可视化页面搭建技术在业界已相当成熟，几乎每个前端团队都会建设一套页面搭建系统，它显著提升研发效率与交付速度，已成为业务快速迭代与规模化生产的关键基础能力。飞猪移动端页面搭建最早诞生于营销大促场景，不同于常规的业务开发，营销大促业务具有以下特点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;迭代速度快：飞猪大促项目频次高且节奏紧凑，传统瀑布式研发模式难以适配业务快速迭代诉求。需沉淀一套面向活动场景的低代码平台，将页面公共能力和通用组件进行标准化、平台化，以降低重复开发与跨团队协作成本，支撑快速迭代与高频发布，在有限周期内兼顾交付效率与稳定性；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面数量多：同期活动通常涵盖多个会场、频道、主题页及域外落地页等，上百个活动页面并行推进，且页面结构差异较大，高频、紧急的营销节奏叠加海量页面需求，使研发交付压力显著增加；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面变阵频繁：一场完整的大促活动通常要经历「预热、预售、现货」等多个阶段，不同阶段的核心目标与主推商品 / 权益会递进变化，运营工作人员需要实时盯盘，根据流量、转化与库存等数据频繁调整页面内容与商品坑位布局且要求快速生效；&lt;/p&gt;&lt;p&gt;基于上述特性，飞猪移动端页面搭建体系应运而生，我们围绕可视化搭建、数据配置与动态渲染三大核心能力，打造了一套面向高频迭代与规模化交付的页面搭建体系。该体系从初代寄生于阿里集团搭建平台下的一个服务站点起步，先后经历了数据投放服务自建、平台能力自建、搭建服务与渲染底座自研。最终形成了当前将数据投放与页面搭建深度融合的一体化“搭投”体系，能够灵活支撑营销大促、日常频道、互动玩法、机酒汽等行业场景、商业化以及小程序等多场景业务需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建体系遵循 PMT 模型规范，核心概念包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Page（页面）：用户可访问的完整页面载体，由页面容器聚合多个业务模块完成运行时渲染，页面容器提供通用基础能力，包括：模板引擎渲染、协议数据的转换与分发、组件依赖与运行时注入、以及埋点与日志等公共服务；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Module（模块）：可复用的业务组件单元，封装组件的展示与业务逻辑并定义数据绑定规则，支持独立配置、按需组合编排，快速拼装成不同页面结构；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Tag（资源位）：模块内用于承载内容投放的最小单元，用来挂载与管理投放数据，一对多关联具体数据投放配置，并支持个性化规则定投；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e9df347677d2499dc2009e48327ff04.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建平台&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;现状与可优化空间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从寄生站点到自研体系的全面回迁，标志着飞猪搭建体系逐渐成熟，但是依赖传统人工选搭投的模式没有得到本质化的改变。下一阶段的重点，是在既有能力之上进一步释放效率红利，围绕“搭建自动化、配置智能化、保障体系化”持续优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运营侧提效：页面搭建链路仍有较大提效潜力，当前运营在模块选择、配置理解、跨平台联动配置上投入时间较多，且在日常使用中对咨询支持的需求仍然较旺。与此同时，运营团队更新节奏快，新人从熟悉到独立产出仍需要一定学习成本，存在进一步降低门槛、增强引导与工具化的空间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设计侧降本增效：页面交付对设计资源的依赖仍然较重，尤其在大促等峰值场景，设计需求集中、节奏紧，容易成为影响整体交付效率的关键路径。日常场景采用计件和众包模式也带来一定的成本与协同开销，因此在设计素材智能生成等方面有进一步优化空间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;产研侧提效：答疑提效：尽管已有答疑平台与知识库沉淀，但是在自主排查方面仍可继续增强，高频场景可进一步沉淀标准化诊断与一键解决方案，减少人工介入；技术保障提效：大促会场通常需要多轮预演，上线后也需要持续巡检以应对业务调整带来的体验波动。随着平台迭代和营销活动频次提升，可通过自动化预检和巡检等方案持续降人工保障投入；研发提效：搭建模块遵循特定规范与约束，新人从上手开发到独立交付仍需一定学习成本；同时由于容器和渲染等核心链路的黑盒特性，仍需平台负责人员提供一定协同支持与保障。可引入 AI Coding 辅助研发，并完善 AI 自助诊断与排障工具，提升代码研发与问题排查效率，降低对平台协同支持的依赖；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 化升级的目标&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今正是 AI 蓬勃发展的时代，问答推理、物料生产、流程编排等方案已经成熟，当下的命题应当是思考搭建 + AI 如何深度结合，打造智能化搭建系统，从人工运营向 AI 辅助运营转型，最终实现无人值守自动化运营。为解决选搭投当前的问题正式启动 AI 智能搭投升级专项：深度融合图文素材生产系统、大模型调度编排等能力，将 AI 模式注入招商选品、页面搭建、数据投放等核心系统，构建从需求理解 → 智能编排 → 自动执行的搭投新范式，为未来全域智能运营奠定技术基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提升运营配置效率：打通“招选搭投”体系 AI 基础能力，通过推荐模板页面、智能搭建页面、智能助理运维等能力辅助业务快速完成页面搭投，覆盖飞猪页面搭建全场景、提升运营配置效率、减少页面配置时间；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;减少设计投入成本：全面推广 AI 文生图和创意合图能力，实现计件众包模式向业务自助产图转型，减少设计费用实现流程提效；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;减少人工答疑成本：AI 赋能答疑场景，提升答疑助手拦截率，常见配置问题自动定位并引导解决，缩短问题解决耗时；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提升模块研发效率：模块研发全面拥抱 AI 模式，完善搭建 IDE Rules + MCP 生态，探索 D2C 在搭建模块开发应用场景，提升模块开发效率；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;产品结构设计&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能选搭投体系 = AI 搭建 + AI 投放 + AI 素材 + AI 助理 + AI 答疑 + AI Coding&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/9596ea9d0ce02a0fc1a9e751967ce524.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;产品结构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;技术实现细节&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能搭建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模板检索&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了保证 AI 生产的页面尽可能贴合飞猪营销页面规范且保证转化效率，以及让运营人员有体感预期，我们采用模板预览的形式作为平台入口，基于用户描述推荐场景相似且业务效果较好的模板，用户可进行预览并确认后进入下一步生产流程。此外我们对页面模板进行了分类，不同的页面类型会走到不同的 Agent 完成生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/95332b8521a4a258094f7ea64dbf036b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;模板预览&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么这么多页面模板从哪里来呢，总不能人工维护吧？答案是基于历史页面作为模板即可，毕竟运营人工搭的页面一定是符合预期效果的。通过跑定时任务批量对历史页面进行回溯，再交给多模态 LLM 分析页面内容打标，作为检索关键词进行向量化并落库，检索时进行相似度计算排序即可，实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面回溯：处理埋点日志找到页面 UV 最大的日期，基于该日期进行页面结构和投放数据 mock 穿越处理，并忽略页面下线重定向逻辑，即可回溯到该页面历史线上投放效果；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;页面快照：基于无头浏览器对页面进行截图，生成页面截图快照；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内容打标：基于多模态 LLM 进行页面内容分析，生成页面内容维度标识，包括：页面介绍、关键词、行业标、主题标、质量评分、效率评分等，作为相似度计算的依据；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;向量持久化：基于 multimodal-embedding-v1 等多模态向量模型将快照截图和内容标识转换成浮点向量，存到关系型数据库即可（如果数据量较大推荐使用向量数据库）；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;检索召回：实时检索时先进行简单的 SQL 检索过滤，再将用户关键词进行向量化，与数据库中的向量字段进行余弦相似度计算，最后排序返回即可；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd9dd57f5ddc9a1964954806a5fc6955.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;向量检索链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;页面生产&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在页面生成链路，我们梳理了飞猪现存搭建页面类型，大致可分为“搭建页”、“频道页”、“图文页”、“文本页”四类，不同页面类型有着各自的应用场景，需要针对性设计不同的技术方案：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b8/b8fc57dbebae74784333556f3c090dd8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终生产的页面效果如下，完全达到线上可投放标准：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;搭建页&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统搭建页主要通过各行业通用模块搭建而成，不同搭建模块配置项各异且繁琐，新人理解学习成本非常高，即便是运营老司机走完配置流程也需要花不少时间，另外在配置过程中还需要填写大量图片素材依赖设计团队产图，页面搭建过程并不丝滑，一整套页面搭建下来往往需要花费大半天甚至几天时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此我们设计了智能搭建系统，基于用户选择的页面模板复制页面和模块，LLM 推理用户需求总结归纳为页面搭建方案，批量调用智能投放链路（详见后文）完成模块配置，再打通后续排期发布和页面发布工程链路。整套流程只需要 3 min 即可完成，用户描述需求后全程无感，并且支持退出页面后台异步生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/7866d2e35925e36b2ca9d373022abcf8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;搭建页面生产效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体走大模型编排链路，链路如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd9dd57f5ddc9a1964954806a5fc6955.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;搭建页生产链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各流程节点实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e7/e7afd05a4bd82e85cbb9b05b663588e1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图文页&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统搭建模式难以有效支持图文页面场景，难点在于图文页偏设计化、页面结构较复杂，一直以来我们都依赖设计出视觉稿再切图投放，该模式设计计件费用昂贵并且流程较长，对于短平快的营销场景来说是属于历史糟粕亟需革命。我们尝试了以下方案，结果总是差强人意：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/18f41b8189282c089586e2a098ea298a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们总是在讨论 AI Coding 如何对程序员提效，那么能否直接绕过 “中间商赚差价”（拿起剪刀剪自己辫子），让运营工作人员直接基于 AI 生产页面呢？答案是可以的，抛开复杂的页面交互逻辑，如果仅是产出静态图文页面，完全可行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们设计了一套 Agent 模拟 “前端页面开发” 流程，由 LLM 担任产品、PM、设计、开发等多角色，协同完成图文页面开发任务，最终产物为页面 HTML 代码。再将 HTML 代码渲染至左侧面板提供预览能力，支持文本、图片等 DOM 元素可视化编辑以及选中元素后 AI 微调等能力供运营人员二次调整。页面调整完成后会按页面片段进行图层切分，基于 html2canvas 库将 HTML 片段代码转换成 PNG 图片，导入图片模块并借助 LLM 完成自动圈选热区，最终生成可点击图文页面，或者直接 HTML 代码注入模块实时渲染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd8ceef920132f7f26f89f6e482517ad.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图文页生产效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体采用 Manager–Worker 架构设计智能体，拆分分析器、调度器、设计师、工程师等多个角色协同完成长图文生产任务，单 Agent 内部实现完整的 ReAct 范式确保输出最优，使用任务队列模式存储执行进程和上下文，由 AI 自主完成监督调度。在设计 Agent 时需要严格约定出入参数据格式以便调度器准确调度执行，另外还需要设计纠错机制和重试机制保障 Agent 节点在意外抖动时整体链路能够稳定运行。图片页生产链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/68/684db72e34d22f332d1a1078b57bf1c5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心 Agent 实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/abc34476e860b5d5effcdbfb3b685c2d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能投放&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建体系遵循 PMT 规范，其中资源位数据投放也是搭建链路中重要的一环，我们自建了数据投放系统，支持静态数据源（模块配置）、标准数据源（商品宝贝、酒店日历房）、插件数据源（服务端接口）三类数据类型，涵盖了几乎所有场景的数据内容投放。在搭建 AI 化升级过程中我们对数据投放链路同步进行了升级，衍生出了智能投放链路。智能投放目前支持了图片素材、模块配置、商品选品、二方平台四类场景一句话快速投放，并且支持全类目定投策略智能关联能力，如：时间周期、人群、设备等。我们还对智能投放原子化能力进行封装以支持页面级批量调用，通过 LLM 分析页面主题归纳页面搭建方案，各模块基于搭建方案自主生产投放数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能投放技术链路如下图，首先 LLM 会根据用户描述、页面和模块信息进行推理，生成符合页面主题的模块推荐配置，引导用户快速选择。用户进一步描述后，LLM 针对不同物料类型生成所需的内容素材，包括：配置项（JSON）、文案、图片等，并对素材内容整合转换成模块配置入参，若语义识别到用户有定投诉求会自动进行配置，最后调工程链路完成排期发布和钉钉消息通知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/98/988c1321516b6e635ed666f8a4972413.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;智能投放链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能投放各节点详细方案和效果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能助理&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在页面搭建过程中，运营人员除了搭建模块和数据投放外，通常还需要配置页面容器、插件，以及后续投放过程中的渠道加参和转码转链等操作。这类配置内容比较分散，甚至跨多个平台，对于新人运营人员有一定学习过程。因此我们结合 AI 能力推出了「智能助理」，涵盖：页面修改、转码转链、热点分析等多种能力，支持语义识别用户的诉求自动调用相应的工具，辅助用户提高运营效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;技术链路上比较简单，通过 LLM 理解用户的操作意图，转换成工具约定的出入参并调用底层平台能力完成对应的操作，功能实现细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/75bc75038e89f8f622e754bf02862aa0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能素材&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外我们还提供了原子化素材创意生成能力，帮助有匠心的运营人员快速微调和优化页面素材内容：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 答疑&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建直面一线运营人员，我们没有专业的技术支持人员，答疑工作只能由产研人员“兼职”。每天面对大量的答疑咨询工单，让本就繁忙的研发工作雪上加霜。在 LLM 普及后我们也是第一时间接入答疑场景，提供平台和群答疑两种交互模式，日常常见问题全部交由 AI 处理，技术人员只需处理疑难杂症即可。将答疑工作量减至原来的 1/6，拦截率达到 91.4%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 答疑技术方案比较简单，纯调 LLM 没什么好说的，核心需要丰富的知识库来提高答疑准确率。我们扒了近几年来飞猪搭建相关的技术 &amp;amp; 产品文档，根据常见问题进行分类，修订为搭投提示知识库。有了知识库还需要对 RAG 进行优化，主要通过文档分段打标实现，我们通过三方接口召回了集团答疑工具近年来所有人工答疑记录，作为语料支持对现有知识库进行打标，文档缺失部分将该 Case 作为 FAQ 进行补充。基于存量知识库答疑总是具有局限性，随着飞猪搭建体系不断地迭代发展，所以还需要让 AI 自我学习最新知识，我们将 AI 对话和人工答疑对话落库，跑定时任务召回对话并进行向量化落库。后续实时检索时历史对话也会作为知识库的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a0cfd6bbd758495a4840ffef74018400.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;AI 答疑 RAG 链路&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI Coding&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;搭建模块开发：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI Coding 方面我们没有选择造 “重复且低质量的轮子”，而是在 Qoder、Cursor 等成熟 IDE 基础上，通过修订 Rules 方式增强 AI 编码能力。我们编写了 8 类搭建模块开发特殊规则，并接入飞猪代码仓库知识库、Ftech、Figma 等 MCP 服务实现组件库代码片段召回、PRD 理解、D2C 视觉稿转码等能力。有助于模块开发提效，对于新人入门非常友好，整体编码提效 80%+，实测模块功能实现和业务逻辑近乎完美只需要微调样式，规则目录如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0857f208d280df51caada65922d2728b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;schema 编写助手&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪搭建模块配置表单遵循类 JSON Schema 规范，我们开发了 Schema 编写助手，辅助开发快速生成表单配置，支持 mock 数据和 schema 配置互转，减少新人学习成本、提高模块开发效率、减少研发答疑成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/de/de2b26a4d5dee44fb56cde7e91dc0870.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;schema 助手&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总 &amp;nbsp; &amp;nbsp;结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过对选品、搭建、投放、素材等平台全面整合，我们对飞猪搭建体系进行全面的 AI 化升级，面向运营、设计、产品、研发等全角色构建了一体化的智能搭建平台。在产品形态上，智能搭建体系已经形成智能创建、智能投放、智能素材、智能助理、AI 答疑、AI Coding 六大支柱，为未来全域智能运营奠定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发一套 AI 体系远比想象中困难，它不像写代码那样有明确、可复现的确定性结果，几乎每天都在对着“人工智障” 挠头。最棘手的问题之一就是大模型的幻觉：自始至终都在困扰着我们，哪怕只是一个很细微的偏差，在多轮任务的链式执行中也会被不断放大，最终产出就可能变得不可用。在不讨论训练 / 微调模型这类高阶手段的前提下，我认为提升 Agent 准确率无外乎以下几种手段：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构设计：Agent 开发需要有 “把不确定性关进笼子” 的架构设计，实践中可以结合具体场景参考网上的经典设计案例，把任务拆解为可控步骤并引入观测、校验、重试、回滚等机制，将大模型的自由生成约束在可验证和纠偏的闭环中以降低幻觉；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;工程实现：多数场景下，工程代码在执行性能与稳定性上更具优势，对于链路中规则清晰、可验证的环节，优先用工程代码实现，能有效降低不确定性；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Prompt 调优：Prompt 是一门学问，调好了能让大模型稳定地发挥能力，总结来说就是结构化、多约束、评估标准、分层书写。网上有许多教程和黑科技，不妨先快速学习一轮，实践中灵活尝试运用，磨刀不误砍柴工；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上下文管理：对于多 Agent 体系中，上下文管理尤为重要，一股脑往里塞必然导致 “上下文污染” 越聊越偏。因此需要设计合理的 memory 结构，为上下文标注来源和边界，只使用可信内容、临时内容及时销毁；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型升级：终极大招，可能你调了半天发现还没换个模型见效快，那就等待 AI 科技的发展进步吧。但是别忘了，基建架构是不可逆的，设计好了就是锦上添花，设计烂了就是雪上加霜；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一阶段的关键在于把 AI 搭建从 “提效 Copilot” 升级为 “可控、可评估、可闭环的生产系统”，通过约束式架构、自动诊断调优等手段不断提升 AI 的自我思考能力并减少幻觉，推广到日常活动、频道运营、大促活动等方方面面，支撑飞猪更高频、更复杂、多场景的全域智能运营。以下是建设过程中的一些思考以及后续规划：&lt;/p&gt;</description><link>https://www.infoq.cn/article/T96C1HNWw5IEiwmMj7It</link><guid isPermaLink="false">https://www.infoq.cn/article/T96C1HNWw5IEiwmMj7It</guid><pubDate>Wed, 21 Jan 2026 05:51:20 GMT</pubDate><author>飞猪技术 丁兆杰</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>零售进入 Agent 时代：Google 联合 Walmart、Shopify 推出 UCP</title><description>&lt;p&gt;Google 正式发布&amp;nbsp;&lt;a href=&quot;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&quot;&gt;Universal Commerce Protocol&lt;/a&gt;&quot;（UCP，通用商业协议），这是一项开放标准，旨在支持“代理式商业”，也就是由 AI 驱动的购物代理可完成从商品发现、下单结算到售后管理的全流程任务。UCP 同时兼顾零售商与消费者需求，在整个购物旅程中始终以“客户关系”为核心，从最初的商品发现到购买决策乃至购买之后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;UCP 在&lt;a href=&quot;https://nrfbigshow.nrf.com/&quot;&gt;美国全国零售联合会&lt;/a&gt;&quot;（National Retail Federation，NRF）年度大会上正式公布。该协议为 AI 代理与商业生态中的后台系统建立了一种安全、标准化的连接方式。企业可以通过 UCP 对外暴露自身能力，并在此基础上扩展诸如折扣等功能；AI 代理则可以通过企业资料动态发现可用服务与支付选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在支付设计上，UCP 将支付工具与支付处理方进行解耦，支持多个支付服务提供商。通信层面，协议支持标准 API、&lt;a href=&quot;https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/&quot;&gt;Agent2Agent&lt;/a&gt;&quot;&amp;nbsp;以及&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_Context_Protocol&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;&amp;nbsp;绑定。Google 还提供了示例实现，包括一个&amp;nbsp;&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/samples/blob/main/rest/python/server/README.md&quot;&gt;Python 服务器&lt;/a&gt;&quot;以及包含商品数据的&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/python-sdk&quot;&gt;软件开发工具包&lt;/a&gt;&quot;，用于展示 AI 代理如何发现商业能力并执行结算流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;American Express 在 LinkedIn 上&lt;a href=&quot;https://www.linkedin.com/posts/american-express_innovation-agenticcommerce-activity-7416134344714715136-6dbm?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;发文&lt;/a&gt;&quot;，强调了该协议在简化商业流程方面的潜力：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Google 推出的 Universal Commerce Protocol（UCP）是一项面向代理式商业的全新开放标准，想减少支离破碎的购物体验，帮零售商与消费者建立更顺畅的连接。UCP 很快将为 Google 搜索的 AI Mode 以及 Gemini 应用中的全新结算体验提供支持。像这样的开放标准，对于构建更安全、更可信的商业体系至关重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 Google 在一篇深入解析的&lt;a href=&quot;http://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/&quot;&gt;技术博客&lt;/a&gt;&quot;中所介绍的，UCP 定义了一系列核心商业能力，包括商品发现、购物车管理、结算流程以及售后工作流。AI 代理可通过查询企业资料来识别可用服务并协商支持的功能，从而减少定制化集成的需求。这种方式既能让企业继续掌控价格、库存和履约逻辑，又能让 AI 代理实现更高程度的自主运行。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/53/59/530774d2061823652c198576a014f059.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;UCP 高层架构示意图&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在安全架构方面，UCP 通过凭证提供方对支付和身份信息进行令牌化处理，而具体的交易处理则由支付服务提供商完成。这种分离设计使 AI 代理在无需接触原始支付信息或个人数据的情况下即可完成交易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，UCP 具备传输层无关性，既支持标准 API 交互，也支持面向代理的绑定形式，适用于对话式界面、AI 助手和自动化工作流。目前，UCP 的早期实现已出现在 AI 驱动的搜索与助理平台中，符合条件的零售商可以在不跳转至外部网站的情况下，直接向用户提供结算体验。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/7d/b4/7d59865678aa463b7252d18a6ccd19b4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;基于 AI 代理的购买流程演示&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;UCP 由 Google 与 Shopify、Etsy、Wayfair、Target、Walmart 共同开发，并获得了来自商业生态中 20 多家合作伙伴的支持与背书，其中包括&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Adyen&quot;&gt;Adyen&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/American_Express&quot;&gt;American Express&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Best_Buy&quot;&gt;Best Buy&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Flipkart&quot;&gt;Flipkart&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Macy%27s&quot;&gt;Macy’s&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Mastercard&quot;&gt;Mastercard&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Stripe,_Inc.&quot;&gt;Stripe&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Home_Depot&quot;&gt;The Home Depot&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Visa_Inc.&quot;&gt;Visa&lt;/a&gt;&quot;&amp;nbsp;以及&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Zalando&quot;&gt;Zalando&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在未来规划方面，UCP 的路线图着眼于构建一个覆盖全球、超越单笔交易的统一商业标准。相关计划包括多商品结算、购物车管理、会员与忠诚度计划、订单后流程，以及个性化的交叉销售与追加销售能力，同时继续确保核心商业逻辑掌握在企业自身手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，UCP 的早期版本已支持美国部分符合条件的零售商在 Google 产品界面内直接完成结算。接下来，该协议还将拓展至印度、印度尼西亚以及拉丁美洲等市场。Google 与合作伙伴也正在广泛&lt;a href=&quot;http://github.com/Universal-Commerce-Protocol/ucp?tab=contributing-ov-file&quot;&gt;征求反馈&lt;/a&gt;&quot;，以不断完善这一协议，共同塑造可互操作、由 AI 驱动的未来商业形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-agentic-commerce-ucp/&quot;&gt;https://www.infoq.com/news/2026/01/google-agentic-commerce-ucp/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相关报道：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9&quot;&gt;“商业版 HTTP”来了：谷歌 CEO 劈柴官宣 UCP，Agent 直接下单，倒逼淘宝京东“拆家式重构”？&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Rqjcx2J5ZbOSjlNJ9BUz</link><guid isPermaLink="false">https://www.infoq.cn/article/Rqjcx2J5ZbOSjlNJ9BUz</guid><pubDate>Wed, 21 Jan 2026 03:23:42 GMT</pubDate><author>作者：Leela Kumili</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>Agent 不再各自为战：GitLab Duo 构建可编排的研发智能体系</title><description>&lt;p&gt;&lt;a href=&quot;https://about.gitlab.com/releases/2026/01/15/gitlab-18-8-released/#gitlab-duo-agent-platform-now-generally-available&quot;&gt;GitLab 18.8&lt;/a&gt;&quot;&amp;nbsp;带来多项新功能，包括 GitLab Duo Planner Agent、GitLab Duo Security Analyst Agent、自动忽略不相关漏洞等。随着本次发布，用于帮助组织统一编排 AI 代理的 GitLab Duo Agent Platform 正式达到全面可用（General Availability，GA）状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;GitLab 表示，GitLab Duo Agent Platform 能够帮助团队在软件开发的各个阶段协同 AI 代理，从规划、构建到安全防护和最终交付。GitLab 认为，如果 AI 只停留在写代码阶段，价值依然有限。Duo Agent Platform 的思路，是让 AI 参与到整个研发流程中，帮助团队应对代码评审积压、安全漏洞、合规检查以及后续修复等现实问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现这一目标，GitLab Duo Agent Platform 将代理式聊天（agentic chat）、面向具体任务的代理、工作流自动化以及企业级管控能力整合在一起，使组织能够在整个软件开发生命周期中部署和管理 AI。&lt;/p&gt;&lt;p&gt;该平台提供了一个集中的 AI Catalog，团队可以在其中发现、管理并在组织内部共享各类代理和流程。内置的基础代理（如 Planner、Security Analyst 和 Data Analyst）可在关键决策节点处理结构化工作；同时，可定制的流程能够在开发工作流中自动执行多步骤的代理任务，覆盖从 Issue 到 Merge Request、CI/CD 迁移、流水线故障排查以及代码评审等多个场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者可以使用 Planner Agent 来创建、更新和分析 GitLab 中的工作项，例如进行待办事项分析、基于 RICE 或 MoSCoW 等框架进行优先级排序，并识别哪些问题需要人工直接介入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Security Analyst Agent 将漏洞管理从仪表盘和脚本中“解放”出来，工程师只需在 GitLab Duo Agentic Chat 中进行对话，就能完成漏洞分流、评估以及修复指导等工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过漏洞管理策略，安全团队可以自动忽略那些并不适用于自身组织的漏洞，从而减少噪音，让开发者专注于真正的安全风险。被自动忽略的漏洞会在 Merge Request 中被明确标注，并在漏洞报告中记录，以满足审计需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了上述由 AI 驱动的新能力之外，GitLab 18.8 还包含多项其他改进，例如升级后的 GitLab Runner、多容器扫描、集中式凭据管理 API 等。更多细节可参考 GitLab 官方发布公告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要指出的是，GitLab Duo Agent Platform 并非市场上唯一的代理编排平台，其竞争对手还包括&amp;nbsp;&lt;a href=&quot;https://www.infoq.com/news/2024/05/github-copilot-workspace-preview/&quot;&gt;Copilot Workspace&lt;/a&gt;&quot;、&lt;a href=&quot;https://cloud.google.com/gemini-enterprise?hl=en&quot;&gt;Google Gemini Enterprise&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.microsoft.com/en-us/microsoft-agent-365&quot;&gt;Microsoft Agent 365&lt;/a&gt;&quot;&amp;nbsp;等产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/gitlab-18-8-duo-agent-platform/&quot;&gt;https://www.infoq.com/news/2026/01/gitlab-18-8-duo-agent-platform/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7PsJIm4r5EKqUasq3xYt</link><guid isPermaLink="false">https://www.infoq.cn/article/7PsJIm4r5EKqUasq3xYt</guid><pubDate>Wed, 21 Jan 2026 03:16:40 GMT</pubDate><author>Sergio De Simone</author><category>AI&amp;大模型</category></item><item><title>Agent Skills 落地实战：拒绝“裸奔”，构建确定性与灵活性共存的混合架构</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;摘要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 Anthropic 开源 skills 仓库，&quot;Code Interpreter&quot;（代码解释器）模式成为 Agent 开发的热门方向。许多开发者试图采取激进路线：赋予 LLM 联网和 Python 执行权限，让其现场编写代码来解决一切问题。但在构建企业级“智能文档分析 Agent”的实践中，我们发现这种“全托管”模式在稳定性、安全性和可控性上存在巨大隐患。本文将分享我们如何摒弃激进路线，采用 Java (确定性 ETL) + DSL 封装式 Skills + 实时渲染 的混合架构，在保留 LLM 灵活性的同时，确保系统的工业级稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一、 背景：当文档分析遇到“复杂生成”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我们的“文档处理 Agent”项目中，基础的问答功能（RAG）已经解决得很好。但随着用户需求升级，我们面临了新的挑战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这是 2024 和 2025 年的两份经营数据报表，请对比 DAU 和营收的同比增长率，并生成一个 Excel 表格给我。另外，把总结报告导出为 PDF。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类需求包含两个特征：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;逻辑计算：需要精确算术（LLM 弱项）。文件 IO：需要生成物理文件（LLM 无法直接做到）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;引入 Skills（让 LLM 调用 Python 代码）似乎是唯一解。但在具体落地时，我们走了一段弯路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;二、 弯路：激进的“纯 Skills”路线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;起初，我们参考了开源社区做法，采用了 完全的 Code Interpreter 模式。我们将 requests、pandas、reportlab 等库的权限全部开放给 LLM，并在 Prompt 中告诉它：“你是一个 Python 专家，请自己写代码解决所有问题。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“裸奔”模式在生产环境中遭遇了三次暴击：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;输入端不可控：LLM 对非结构化数据（如无后缀 URL、加密 PDF）的处理极其脆弱，经常陷入报错死循环。输出端崩坏：让 LLM 从零绘制 PDF/Word 是灾难。经常出现中文乱码、表格对不齐、使用了过期的库 API 等问题。安全黑洞：数据流完全在沙箱内闭环，Java 主程序失去了对内容的控制权，无法拦截敏感词或违规数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;三、 变革：Java 主控 + DSL Skills 的混合架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决上述问题，我们重构了架构。核心思想是：收回 LLM 的“底层操作权”，只保留其“逻辑调度权”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们制定了新的架构分工：Java 负责确定性的数据流转与安检，LLM 负责意图理解与代码组装，Python 沙箱 负责在受控环境下执行具体计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.1 架构设计概览&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将系统重新划分为四个逻辑层级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ETL 层 (Java)：负责下载、MIME 识别、OCR、敏感词检测。这是“确定性管道”。Brain 层 (LLM)：负责阅读纯文本，进行逻辑推理，并生成调用代码。Skills 层 (Python Sandbox)：提供高度封装的 SDK（DSL），而非裸库。Delivery 层 (Java)：负责将 Markdown/HTML 实时渲染为 PDF/Word。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1f/1f672bd3d0df517c2c226761992e53f9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.2 输入侧：回归 Java 流水线 (ETL)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们不再让 LLM 去下载和解析文件。所有输入文件，先经过 Java 的 DocPipeline。利用 Apache Tika 进行精准解析，并立即进行敏感词检测和文本截断。这一步保证了喂给 LLM 的数据是干净、安全、标准化的纯文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.3 中间层：DSL 封装模式 (The Wrapper Pattern)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我们对 Skills 实践最大的改进。我们禁止 LLM 直接写 import pandas 进行底层操作，而是预置了一套高度封装的 DSL。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Python 端封装 (excel_tool.py)：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;import pandas as pd
import os

def create_excel(data_list, filename=&quot;report.xlsx&quot;, output_dir=&quot;/workspace&quot;):
    try:
        df = pd.DataFrame(data_list)
        save_path = os.path.join(output_dir, filename)

        # 【封装价值体现】自动处理格式、列宽、引擎兼容性，屏蔽 LLM 的幻觉风险
        with pd.ExcelWriter(save_path, engine=&#39;openpyxl&#39;) as writer:
            df.to_excel(writer, index=False, sheet_name=&#39;Sheet1&#39;)
            
            # 自动调整列宽 (LLM 很难写对的工程细节)
            worksheet = writer.sheets[&#39;Sheet1&#39;]
            for idx, col in enumerate(df.columns):
                max_len = max(df[col].astype(str).map(len).max(), len(str(col))) + 2
                worksheet.column_dimensions[chr(65 + idx)].width = min(max_len, 50)
            
        return save_path
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skill 说明书 (SKILL.md)：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在 Prompt 中通过“接口契约”强行约束 LLM 的行为，明确了何时该写代码，何时该纯输出文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;markdown&quot;&gt;# File Generation Skill (Standardized)

你拥有生成专业格式文件（Excel, Word, PDF）的能力。
沙箱中已预装了封装好的 `excel_tool` 库。

**核心决策树**：
1. 如果是 **统计数据/表格** -&amp;gt; 必须生成 **Excel** -&amp;gt; **写 Python 代码**。
2. 如果是 **分析报告/文档** -&amp;gt; 必须生成 **Word/PDF** -&amp;gt; **禁止写代码**，走渲染路径。

---

### 场景 1：生成 Excel (.xlsx)
**规则**：禁止使用 `pandas` 底层 API，必须调用封装函数。
**数据结构**：必须是【字典列表】，每个字典代表一行。

**Python 调用示例**：
```python
import excel_tool

# 1. 准备数据 (从文档中提取)
data = [
    {&#39;年份&#39;: &#39;2024&#39;, &#39;DAU&#39;: 1000, &#39;营收&#39;: &#39;500万&#39;},
    {&#39;年份&#39;: &#39;2025&#39;, &#39;DAU&#39;: 1500, &#39;营收&#39;: &#39;800万&#39;}
]

# 2. 调用封装函数 (自动处理样式、列宽)
excel_tool.create_excel(data, filename=&#39;analysis.xlsx&#39;)
```

---

### 场景 2：生成 Word / PDF (.docx / .pdf)
**规则**：**严禁编写 Python 代码**（如 `reportlab` 或 `python-docx`）。
**执行动作**：
1. 请直接输出内容丰富、排版精美的 **Markdown** 文本。
2. 在 Markdown 的**最后一行**，务必添加对应的动作标签，系统会自动将其渲染为文件。

**输出示例**：
# 2024 年度经营分析报告

## 一、 数据概览
本季度营收同比增长 20%...

| 指标 | Q1 | Q2 |
| :--- | :--- | :--- |
| DAU | 100w | 120w |

... (此处省略 2000 字内容) ...

&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;
&lt;/action:convert|pdf&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3.4 输出侧：渲染与交付的分离&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于不同类型的文件，我们采取了截然不同的交付策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Excel（强结构化）：走 Skills 路线。LLM 组装数据 -&amp;gt; 调用 excel_tool -&amp;gt; 沙箱生成物理文件。Word/PDF（富文本）：走 渲染路线。严禁 LLM 写代码生成。LLM 只输出高质量的 Markdown 并在末尾打上 &amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt; 标签。Java 后端拦截该标签，利用 OpenHTMLtoPDF 或 Pandoc 将 Markdown 实时转换 为精美的 PDF/Word。&lt;/action:convert|pdf&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;四、 硬核代码实现 (Spring AI)&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是我们在 Spring AI 体系下实现这套混合架构的关键逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.1 动态技能注入 (SkillManager)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们实现了一个 SkillManager，支持按需加载技能。为了提升性能，我们设计了 Session 级的“防抖机制”，确保同一个会话中只需上传一次 Python 脚本，避免重复 IO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Service
public class SkillManager {
    // 缓存技能脚本: 技能名 -&amp;gt; { 文件路径 -&amp;gt; 内容 }
    private final Map&lt;string, map&lt;string,=&quot;&quot; string=&quot;&quot;&gt;&amp;gt; skillScripts = new ConcurrentHashMap&amp;lt;&amp;gt;();
    // 防止重复注入的防抖 Set
    private final Set&lt;string&gt; injectedSessions = ConcurrentHashMap.newKeySet();

    /**
     * 核心逻辑：根据需要的技能列表，动态注入脚本到沙箱
     */
    public void injectToSandbox(String sessionId, List&lt;string&gt; neededSkills) {
        // 1. 防抖检查：如果该 Session 已注入，直接跳过，避免重复 IO
        if (injectedSessions.contains(sessionId)) return;

        // 2. 注入 Python 包结构 (__init__.py)
        sandboxService.uploadFile(sessionId, &quot;/workspace/skills/__init__.py&quot;, &quot;&quot;);

        // 3. 批量上传该技能所需的 DSL 脚本
        for (String skillName : neededSkills) {
            Map&lt;string, string=&quot;&quot;&gt; scripts = skillScripts.get(skillName);
            if (scripts != null) {
                scripts.forEach((path, content) -&amp;gt; 
                    sandboxService.uploadFile(sessionId, path, content)
                );
            }
        }
        injectedSessions.add(sessionId);
    }
    
    // ... 省略加载 Resource 的代码 ...
}
&lt;/string,&gt;&lt;/string&gt;&lt;/string&gt;&lt;/string,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.2 业务调度与意图分流 (Handler)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;串联 Java ETL、LLM 推理和最终的交付分流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Service
public class DocumentAnalysisRequestHandler {

    public Flowable&lt;response&gt; processStreamingRequest(Request req) {
        // 1. 【Java ETL】确定性解析与安检
        // 无论 URL 还是文件，先转为纯文本，并做敏感词过滤
        List&lt;parseresult&gt; parsedDocs = etlPipeline.process(req.getUrls());
        
        // 2. 【技能注入】
        List&lt;string&gt; neededSkills = List.of(&quot;file_generation&quot;);
        skillManager.injectToSandbox(req.getSessionId(), neededSkills);

        // 3. 【LLM 执行】Context Stuffing
        String prompt = buildPrompt(parsedDocs, skillManager.getPrompts(neededSkills));
        
        // 调用 LLM，挂载 ToolContext 以实现多租户隔离
        Flowable&lt;agentoutput&gt; agentFlow = chatClient.prompt()
                .system(prompt)
                .user(req.getUserInstruction())
                .toolContext(Map.of(&quot;projectId&quot;, req.getSessionId())) 
                .stream()
                .content();

        // 4. 【结果分流】
        return agentFlow
                .toList() // 收集完整回复
                .flatMap(this::handlePostGenerationAction);
    }

    /**
     * 核心分流逻辑：决定是返回沙箱文件(Excel) 还是 调用Java渲染(PDF)
     */
    private Single&lt;agentoutput&gt; handlePostGenerationAction(List&lt;string&gt; rawChunks) {
        String text = String.join(&quot;&quot;, rawChunks);

        // 分支 A：检测到 Python 生成了 Excel (Skills 产物)
        // 格式：[FILE_GENERATED: /workspace/report.xlsx]
        if (FILE_GENERATED_PATTERN.matcher(text).find()) {
            String path = extractPath(text);
            return Single.just(new AgentOutput(path, OutputType.FILE));
        }

        // 分支 B：检测到转换指令 (渲染产物)
        // 格式：&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;
        if (text.contains(&quot;&amp;lt;&amp;lt;&lt;action:convert|pdf&gt;&amp;gt;&amp;gt;&quot;)) {
            // Java 侧实时渲染：Markdown -&amp;gt; PDF
            // 优势：完美控制字体和样式，避免 Python 生成乱码
            String pdfPath = docConverterService.convertAndSave(text, &quot;pdf&quot;);
            return Single.just(new AgentOutput(pdfPath, OutputType.FILE));
        }

        // 分支 C：普通文本
        return Single.just(new AgentOutput(text, OutputType.TEXT));
    }
}
&lt;/action:convert|pdf&gt;&lt;/action:convert|pdf&gt;&lt;/string&gt;&lt;/agentoutput&gt;&lt;/agentoutput&gt;&lt;/string&gt;&lt;/parseresult&gt;&lt;/response&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbfaaffa67b67e9643771a9f5640c283.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;4.3 拦截与交付 (SandboxTools)&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Tool 执行层做最后一道防线：输出内容的二次安检。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;@Component
public class SandboxTools {

    @Tool(name = &quot;execute_command&quot;, description = &quot;在沙箱中执行 Shell 命令&quot;)
    public String executeCommand(ExecuteCommandRequest req, ToolContext context) {
        String projectId = (String) context.getContext().get(&quot;projectId&quot;);
        
        try {
            // 1. 执行 Python 脚本
            Map&lt;string, object=&quot;&quot;&gt; result = sandboxMcpService.executeCommand(projectId, req.command());
            String stdout = (String) result.get(&quot;stdout&quot;);

            // 2. 【关键】输出侧安检
            // 防止 LLM 通过代码计算出违规内容，绕过输入侧检查
            if (banwordService.hasBanWords(stdout)) {
                log.warn(&quot;Banword detected in sandbox output!&quot;);
                throw new BanwordException(&quot;敏感内容阻断&quot;);
            }

            // 3. 超长截断 (防止 LLM 上下文爆炸)
            if (stdout.length() &amp;gt; MAX_TEXT_LENGTH) {
                return stdout.substring(0, MAX_TEXT_LENGTH) + &quot;\n[SYSTEM: TRUNCATED]&quot;;
            }

            return stdout;
        } catch (Exception e) {
            return &quot;Execution Error: &quot; + e.getMessage();
        }
    }
}
&lt;/string,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;五、 总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skills 技术让 LLM 拥有了“手”，但这双手必须戴上“手套”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这次架构演进，我们得出的核心经验是：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不要高估 LLM 的 Coding 能力：它是一个优秀的逻辑推理引擎，但在工程细节（排版、库依赖、环境配置）上非常糟糕。DSL 封装是必须的。不要丢掉 Java 的确定性：解析、下载、格式转换、安全检查，这些传统代码擅长的领域，不要交给概率性的 LLM 去做。架构分层：Input: Java (Standardization &amp;amp; Security)Thinking: LLM (Reasoning)Action: Python (Calculation via DSL)Output: Java (Rendering &amp;amp; Delivery)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种混合架构，既保留了 Agent 处理复杂动态需求的能力（如自定义计算涨跌幅），又守住了企业级应用对稳定性与合规性的底线。&lt;/p&gt;</description><link>https://www.infoq.cn/article/MUo1faBPQqOwxVDWA9vB</link><guid isPermaLink="false">https://www.infoq.cn/article/MUo1faBPQqOwxVDWA9vB</guid><pubDate>Wed, 21 Jan 2026 02:42:14 GMT</pubDate><author>仇智慧</author><category>生成式 AI</category></item><item><title>Pinterest的Moka：Kubernetes如何重写大数据处理规则</title><description>&lt;p&gt;数字公告板提供商Pinterest发布了一篇文章，&lt;a href=&quot;https://medium.com/pinterest-engineering/next-gen-data-processing-at-massive-scale-at-pinterest-with-moka-part-2-of-2-d0210ded34e0&quot;&gt;解释了其新平台Moka在大规模数据处理方面的未来蓝图&lt;/a&gt;&quot;。该公司正在将核心工作负载从老化的Hadoop基础设施迁移到基于Kubernetes的系统上，该系统运行在亚马逊EKS上，以Apache Spark作为主要引擎，并即将支持其他框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一个包含两篇文章的博客系列中，Soam Acharya、Rainie Li、William Tom和Ang Zhang描述了Pinterest大数据平台团队如何考虑下一代大规模数据处理平台的替代方案，因为现有的基于Hadoop的系统（内部称为Monarch）的局限性变得越来越明显。他们将Moka作为搜索的结果，以及基于EKS的云原生数据处理平台，该平台现在运行的生产负载达到了Pinterest的规模。&lt;a href=&quot;https://www.infoq.com/news/2025/07/pinterest-spark-kubernetes/&quot;&gt;该系列的第一部分关注整体设计和应用层&lt;/a&gt;&quot;。相比之下，第二部分转向作者所说的“Moka的基础设施重点方面，包括经验和未来方向”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0a/0adbf9dae170c8d0b8fd96d9f0c2d79f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章从实际角度描述了向Kubernetes的转变。它展示了一个全行业的转变，即大型技术公司现在将Kubernetes视为数据的控制平面，而不仅仅是无状态的服务平台。在大数据社区日益增长的受欢迎程度和越来越多的采用的鼓励下，团队探索了基于Kubernetes的系统，作为Hadoop 2.x最有可能的替代品。任何候选平台都必须满足可扩展性、安全性、成本以及托管多个处理引擎的精确标准。Moka是如何在不放弃现有Spark投资的情况下现代化Hadoop时代的数据平台的一个例子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二篇文章的核心主题是如何在Kubernetes上以非常大的规模运行Spark。作者解释了他们如何围绕Moka添加日志、指标和作业历史服务，以便工程师可以在不了解底层集群拓扑的情况下调试和调整作业。他们使用Fluent Bit对日志集合进行标准化，并使用OpenTelemetry和Prometheus兼容的端点发布统一指标。这为基础设施和应用程序团队提供了系统健康的一致视图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pinterest还投资于通过基础设施即代码的方式使平台可重复使用。在文章中，团队概述了他们如何使用Terraform和Helm创建EKS集群、配置网络和安全以及部署支持组件，如Spark历史服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Pinterest的工程师还讨论了处理不同的硬件架构。他们描述了他们如何构建多架构镜像，以便他们的数据工作负载在Intel和基于ARM的实例上运行良好，包括AWS Graviton，并将此与集群规模的成本和效率目标联系起来。InfoQ编辑Eran Stiller在LinkedIn&lt;a href=&quot;https://www.linkedin.com/posts/estiller_from-hadoop-to-kubernetes-pinterests-scalable-activity-7356213210661744640-17Ee/&quot;&gt;上对该项目中的总结指出&lt;/a&gt;&quot;，Moka“提供了容器级别的隔离、ARM支持、YuniKorn调度，并通过整合工作负载和跨实例类型的自动扩展实现了显著的成本节省”。这些细节将工作置于云用户寻求在不牺牲性能的情况下削减基础设施成本的更大趋势之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于处理引擎的更广泛的行业对话为Pinterest的故事增添了细微差别。在另一篇&lt;a href=&quot;https://www.linkedin.com/posts/soamacharya_next-gen-data-processing-at-massive-scale-activity-7350931757694640128-vCg0/&quot;&gt;LinkedIn&lt;/a&gt;&quot;帖子中，Acharya写道：“虽然Spark是我们的主要主力，但Moka的成功意味着Pinterest的其他用例也在效仿：Flink Batch已经投入生产，Apache Ray紧随其后，Flink Streaming也将在今年晚些时候推出”。通过对Spark和Flink技术的深入探讨，我们可以了解到这一点的重要性。强调Spark仍然非常适合大型批处理和交互式分析工作负载，而Flink是“为实时、有状态的流处理而构建的”，具有严格的逐事件处理。团队将Moka呈现为一个灵活的基础，可以根据特定工作负载的需求添加不同的引擎，而不是一个只支持spark的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;外部观察者从Pinterest案例中吸取了教训。&lt;a href=&quot;https://ml.blaze.email/archive/ml-engineering-newsletter-5558/&quot;&gt;ML工程师&lt;/a&gt;&quot;通讯将Moka文章描述为“在Kubernetes上部署EKS集群、Fluent Bit日志、OTEL指标管道、镜像管理和Spark的自定义Moka UI”的例子，将其与其他现代数据基础设施案例研究并列。这些反应表明，Moka被视为一类云原生数据系统的参考架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，团队确实将他们的迁移工作呈现为一个正在进行的旅程，而不是一个已经完成的项目。在博客和进一步的&lt;a href=&quot;https://www.linkedin.com/posts/pinterest-engineering_next-gen-data-processing-at-massive-scale-activity-7373833079498371072-EREu/&quot;&gt;LinkedIn&lt;/a&gt;&quot;帖子中，Pinterest作者讨论了“经验和未来的方向”，并描述了早期概念验证如何导致随着对新堆栈的信心增长而逐步远离Hadoop的迁移。Acharya指出，“最好的问题出现在规模上”，构建平台涉及“解决难题”，因为团队转移了实际工作负载。对于其他组织来说，这种经验可能是最重要的教训。复制围绕Kubernetes、EKS和Spark的技术选择相对简单，但从遗留系统中解耦并投资于可观测性、自动化和多引擎支持的过程可能是未来真正的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/pinterest-kubernetes-bigdata/&quot;&gt;https://www.infoq.com/news/2026/01/pinterest-kubernetes-bigdata/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/DksL1xE1iKPIIzqOorEw</link><guid isPermaLink="false">https://www.infoq.cn/article/DksL1xE1iKPIIzqOorEw</guid><pubDate>Wed, 21 Jan 2026 01:44:32 GMT</pubDate><author>Tim Anderson</author><category>云计算</category><category>大数据</category></item><item><title>模力工场 029 周 AI 应用榜：AI 生图文字不再“开盲盒”，GLM-Image 凭精准登顶榜首！</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260120infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场作为官方生态合作伙伴，诚邀您共赴产业前沿盛会——「逐梦 AI ·天使筑基」2026 中关村早期投资论坛暨 AI 新场景产业创新大会。本次大会汇聚政策、资本与产业领袖，深度聚焦机器人、智能体、大模型应用等前沿赛道，共同把脉 AI 趋势、破解落地难题，为您提供决策的一手洞察。1月28日，北京中关村，期待与您共筑未来！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/b0/56/b06d552a0853ba4148d32160df9fb556.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;029 周榜单总介绍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260120infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 029 周 AI 应用榜来袭！本周共有 23 款应用上架，我们从榜单中精选出十款代表性应用与大家分享。本期榜单应用多为近期热门或美国 CES 参展应用，整体呈现“软硬结合、多领域并进”的特点，涵盖大模型应用、智能硬件、生活方式工具及 AI 基础设施等多个方向。从中可以看出，当前 AI 应用正朝着更实用、更集成、更富交互感的方向演进，硬件创新与场景化服务正成为推动 AI 走向普及的关键动力。以下为本周精选的十款应用简介：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/glmimage?utm_source=20260120infoQ&quot;&gt;GLM-Image&lt;/a&gt;&quot;（智谱 AI）: 图像设计、AI Infra 类，开源图像生成模型&lt;a href=&quot;https://agicamp.com/products/qwenx?utm_source=20260120infoQ&quot;&gt;千问App&lt;/a&gt;&quot;: AI 搜索问答、生活方式类, 阿里最强模型官方 AI 助手&lt;a href=&quot;https://agicamp.com/products/rayneo?utm_source=20260120infoQ&quot;&gt;雷鸟 AI 眼镜&lt;/a&gt;&quot;（RayNeo）: AI 硬件类，想象万千，终于一见&lt;a href=&quot;https://agicamp.com/products/immo?utm_source=20260120infoQ&quot;&gt;影目 GO3&lt;/a&gt;&quot; （IMMO）: AI 硬件类，AI 眼镜美学标杆&lt;a href=&quot;https://agicamp.com/products/Lynxring?utm_source=20260120infoQ&quot;&gt;Lynx Ring&lt;/a&gt;&quot;（云康宝）: AI 硬件类，小巧智能戒指，24 小时健康监测随身管理&lt;a href=&quot;https://agicamp.com/products/Boujour?utm_source=20260120infoQ&quot;&gt;Bonjour 数字名片&lt;/a&gt;&quot;: 生活方式类，Bonjour！创意工作者的 Portfolio&lt;a href=&quot;https://agicamp.com/products/agibot?utm_source=20260120infoQ&quot;&gt;智元机器人 AgiBot A2&lt;/a&gt;&quot;: AI 硬件类，业内首个规模化商用的全尺寸人形机器人&lt;a href=&quot;https://agicamp.com/products/loona?utm_source=20260120infoQ&quot;&gt;Loona（可以科技）&lt;/a&gt;&quot;: AI 硬件类，具备情感反馈的家庭 AI 宠物陪伴机器人&lt;a href=&quot;https://agicamp.com/products/xh-lanyun?utm_source=20260120infoQ&quot;&gt;蓝耘星河&lt;/a&gt;&quot;: AI Infra、新媒体创作、营销增长类，蓝耘星河以智能，驱动增长&lt;a href=&quot;https://agicamp.com/products/Tunee&quot;&gt;Tunee&lt;/a&gt;&quot;: AI Agent、音频语音、视频多媒体类，Tunee！The smartest AI music agent.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周必试应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/glmimage?utm_source=20260120infoQ&quot;&gt;GLM-Image&lt;/a&gt;&quot;（智谱 AI）&lt;/p&gt;&lt;p&gt;关键词：开源图像生成模型 ｜ 复杂视觉文本生成 ｜ 长文本渲染&lt;/p&gt;&lt;p&gt;模力小A推荐：GLM-Image 在中文长文本准确性与小字脚注生成上表现突出，尤其适合法律文书、产品说明等对文字保真度要求极高的场景。此外，其价格仅为 Nano Banana Pro 的一半左右，性价比显著。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;上榜冷门但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/qwenx?utm_source=20260120infoQ&quot;&gt;千问App&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：阿里官方出品｜多场景智能问答｜搜索增强｜生活助手&lt;/p&gt;&lt;p&gt;模力小A推荐：如果说之前的千问还是一位“聊天伙伴”，那么现在的它，已经进化成了能真正帮你“办事”的智能管家。随着1月15日新版本的发布，千问 App 全面接入了淘宝、支付宝、高德等阿里核心生态，这意味着你可以直接通过和千问对话完成点外卖、买机票、订酒店等一系列操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本周的 AI 趋势呈现出清晰的双线演进：软件正变得更深、更实用，而硬件则在变得更轻、更自然。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件：从“能说会道”到“能干实事”&lt;/p&gt;&lt;p&gt;近期两个标志性进展值得关注。其一，GLM-Image 登顶 Hugging Face 榜单，证明了国产模型能在专业场景（如法律文书、产品说明）中精准生成文本和图像，同时还具备显著的成本优势，让专业级 AI 工具变得触手可及。其二，千问 App 全面接入阿里生态，意味着 AI 已从单纯的“问答对话”进阶到“办事调度”阶段——用户可以通过自然对话直接完成点外卖、订机票等操作。AI 正从一个聊天对象，转变为串联现实服务的智能中枢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硬件：从“技术秀场”到“场景适配”&lt;/p&gt;&lt;p&gt;从 CES 的趋势来看，AI 硬件近期正在褪去“极客玩具”的标签，转向更务实的设计哲学：不刻意刷存在感，但需要时总在身边。&lt;/p&gt;&lt;p&gt;入口更轻了：新一代 AR 眼镜（如雷鸟、影目）不再追求取代手机，而是通过更轻巧的设计，专注做好“信息提示”“即时导航”这类“抬头即用”的场景，成为生活中的“第二块屏幕”。类似地，像 Loona DeskMate 这样的产品，让用户闲置的旧手机成为桌面机器人的“面孔”，以几乎零成本的方式，把熟悉的设备变成了桌面上可互动、可陪伴的 AI 伙伴。陪伴更久了：以智能戒指为代表的健康设备，正变得像首饰一样无感佩戴。竞争的关键不再是“能测多少项”，而是能否让用户愿意长期佩戴，从而获得持续、有价值的健康数据。同样，人形机器人（如智元 AgiBot A2）也迈入了新阶段：能量产了。接下来的核心问题，是它能在工厂、商场等具体场景中解决什么实际工作，创造什么经济价值。表达更活了：AI 也开始赋能个人形象展示。像 Bonjour 数字名片这样的工具，让个人主页从静态的“电子名片”变成了可动态展示作品、风格乃至个性的“互动橱窗”，帮助用户在社交与职场中更生动地呈现自己。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体而言，当前 AI 的发展更加注重与真实场景、既有习惯的衔接。无论是软件的能力延伸，还是硬件的形态演进，都体现出同一种思路：在用户需要时提供恰到好处的支持，而非刻意强调技术本身的存在。或许只有当技术彻底融入行为日常，才是其真正成熟的标志。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后再介绍一下模力工场的上榜机制和加入榜单的参与方式，欢迎大家继续积极参与提交 AI 应用：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模力工场 AI 应用榜并非依靠“点赞刷榜”，而是参考以下权重维度：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;评论数（核心指标，代表社区真实反馈）收藏与点赞（次级指标）推荐人贡献（注册推荐人可直接为好应用打 Call）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;加入榜单的参与方式：&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你是开发者：上传你的 AI 应用，描述使用场景与核心亮点；如果你是推荐人：发现好工具，发布推荐理由；如果你是用户：关注榜单，评论互动，影响榜单权重，贡献真实声音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One More Thing，对于所有在模力工场上发布的 AI 应用，极客邦科技会借助旗下各品牌资源进行传播，短时间内触达千万级技术决策者与开发者、AI 用户：&lt;/p&gt;&lt;p&gt;InfoQ 全媒体矩阵AI 前线全媒体矩阵极客时间全媒体矩阵TGO 鲲鹏会全媒体矩阵霍太稳视频号&lt;/p&gt;</description><link>https://www.infoq.cn/article/1CO7VkWwPjB94w95xxfF</link><guid isPermaLink="false">https://www.infoq.cn/article/1CO7VkWwPjB94w95xxfF</guid><pubDate>Tue, 20 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>MAKE IT SNOW ｜ 2025-2026 Data+AI 年度时刻</title><description>&lt;p&gt;&lt;strong&gt;嘉宾介绍：&lt;/strong&gt;&lt;br&gt;
杨    扬   Snowflake 亚太及日本地区解决方案工程副总裁&lt;br&gt;
郭多娇   Snowflake 中国区市场总经理&lt;br&gt;
王一鹏   InfoQ  极客传媒总经理、总编辑&lt;br&gt;
郭    炜   白鲸开源 CEO&lt;br&gt;
史少锋    Datastrato VP of Engineering&lt;br&gt;
李   飞    数势科技 AI 负责人&lt;br&gt;
朱亦非    罗氏中国 Snowflake 数据平台技术负责人&lt;br&gt;
陈砚琳    西门子数据分析师&lt;br&gt;
高   杰    蔚来汽车人工智能研发负责人 &amp;amp; 高级总监&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;内容简介：&lt;/strong&gt;&lt;br&gt;
InfoQ 携手 Snowflake，邀请数位来自智能制造、智慧医疗、智驾等行业的全球头部企业的数智化专家和大数据专家围炉对谈，深度复盘 2025 年三大认知突破，围绕 Data 与 AI 战略发起“年度十问”，共同见证企业数智化变革的关键时刻。&lt;/p&gt;
</description><link>https://www.infoq.cn/article/uqAQNcO0Ct5oJxoC1ARK</link><guid isPermaLink="false">https://www.infoq.cn/article/uqAQNcO0Ct5oJxoC1ARK</guid><pubDate>Tue, 20 Jan 2026 11:11:38 GMT</pubDate><author>王玮</author><category>企业动态</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>刚刚，基于Grok的X推荐算法开源！专家：ROI 过低，其它平台不一定跟</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;时隔近三年，马斯克再次开源X推荐算法&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚，X 工程团队在X上发帖宣布，正式开源X推荐算法，据介绍，这个开源库包含为 X 上的“为你推荐”信息流提供支持的核心推荐系统，它将网络内内容（来自用户关注的帐户）与网络外内容（通过基于机器学习的检索发现）相结合，并使用基于 Grok 的 Transformer 模型对所有内容进行排名，也就是说，该算法采用了与 Grok 相同的Transformer 架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开源地址：&lt;a href=&quot;https://x.com/XEng/status/2013471689087086804&quot;&gt;https://x.com/XEng/status/2013471689087086804&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e05f1688362e07e6ea6ad9cc07a62290.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;X 的推荐算法负责生成用户在主界面看到的“为你推荐”（For You Feed）内容。它从两个主要来源获取候选帖子：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你关注的账号（In-Network / Thunder）平台上发现的其他帖子（Out-of-Network / Phoenix）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些候选内容随后被统一处理、过滤然后按相关性排序。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，算法核心架构与运行逻辑是怎样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;算法先从两类来源抓取候选内容：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关注内的内容：来自你主动关注的账号发布的帖子。非关注内容：由系统在整个内容库中检索出的、可能你感兴趣的帖子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一阶段的目标是“把可能相关的帖子找出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;系统自动去除低质量、重复、违规或不合适的内容。例如：&lt;/p&gt;&lt;p&gt;已屏蔽账号的内容与用户明确不感兴趣的主题非法、过时或无效帖子&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样确保最终排序时只处理有价值的候选内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次开源的算法的核心是系统使用一个 Grok-based Transformer 模型（类似大型语言模型/深度学习网络）对每条候选帖子进行评分。Transformer 模型根据用户的历史行为（点赞、回复、转发、点击等）预测每种行为的概率。最后，将这些行为概率加权组合成一个综合得分，得分越高的帖子越有可能被推荐给用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一设计把传统手工提取特征的做法基本废除，改用端到端的学习方式预测用户兴趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1724e34ba728f55ce4ad205423b0ce46.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不是马斯克第一次开源X推荐算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早在2023年3月31日，正如马斯克收购Twitter 时承诺的那样，他已将 Twitter 部分源代码正式开源，其中包括在用户时间线中推荐推文的算法。开源当天，该项目在 GitHub 已收获 10k+ 颗Star。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，马斯克在 Twitter 上表示此次发布的是“大部分推荐算法”，其余的算法也将陆续开放。他还提到，希望“独立的第三方能够以合理的准确性确定 Twitter 可能向用户展示的内容”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在关于算法发布的 Space 讨论中，他说此次开源计划是想让 Twitter 成为“互联网上最透明的系统”，并让它像最知名也最成功的开源项目 Linux 一样健壮。“总体目标，就是让继续支持 Twitter 的用户们最大程度享受这里。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/85/85d48494461692b0c3a13e7225fdd9e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今距离马斯克初次开源X算法，过去了近三年的时间。而作为技术圈的超级KOL，马斯克早已为此次开源做足了的宣传。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月11日，马斯克在X上发帖称，将于 7 天内将新的 X 算法（包括用于确定向用户推荐哪些自然搜索内容和广告内容的所有代码）开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此流程将每 4 周重复一次，并附有详细的开发者说明，以帮助用户了解发生了哪些变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今天，他的承诺再次兑现了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9eb159196c1721b87fc4e3bee3768ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克为什么要开源？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当埃隆·马斯克再次提到“开源”时，外界的第一反应并非技术理想主义，而是现实压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去一年里，X因其内容分发机制屡次陷入争议。该平台被广泛批评在算法层面偏袒和助长右翼观点，这种倾向并非零星个案，而被认为具有系统性特征。去年发布的一份研究报告就指出，X 的推荐系统在政治内容传播上出现了明显的新偏见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，一些极端案例进一步放大了外界的质疑。去年，一段涉及美国右翼活动人士查理·柯克遇刺的未经审查视频在 X 平台迅速传播，引发舆论震动。批评者认为，这不仅暴露了平台审核机制的失效，也再次凸显了算法在“放大什么、不放大什么”上的隐性权力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这样的背景下，马斯克突然强调算法透明性，很难被简单解读为一次纯粹的技术决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d9/d9e7cb72307acd0f513610a4cdfbfefd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友怎么看？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;X推荐算法开源后，在X平台，有用户对推荐算法机制做了以下5点总结：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;1. 回复你的评论。算法对“回复+作者回应”的权重是点赞的75倍。不回复评论会严重影响曝光率。2. 链接会降低曝光率。应该把链接放在个人简介或置顶帖里，千万不要放在帖子正文中。3. 观看时长至关重要。如果他们滑动屏幕略过，你就不会吸引他们。视频/帖子之所以能获得高关注，是因为它们能让用户停下来。4. 坚守你的领域。“模拟集群”是真实存在的。如果你偏离了你的细分领域（加密货币、科技等），你将无法获得任何分销渠道。5. 屏蔽/默不作声会大幅降低你的分数。要有争议性，但不要令人讨厌。&amp;nbsp;简而言之：与你的受众沟通，建立关系，让用户留在应用内。其实很简单。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d0/d025a7d0c966f45b1a2850e960f470d4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也有网友发现，虽然架构是开源的，但还有些内容仍未开源。该网友表示，此次发布本质上是一个框架，没有引擎。具体少了啥？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;缺少权重参数 - 代码确认“积极行为加分”和“消极行为扣分”，但与 2023 年版本不同的是，具体的数值被删除了。隐藏模型权重 - 不包含模型本身的内部参数和计算。未公开的训练数据 - 对于训练模型的数据、用户行为的采样方式，以及如何构建“好”样本与“坏”样本，我们一无所知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于普通X用户而言，X的算法开源并不会造成太大影响。但更高的透明度可以解释为什么有些帖子能获得曝光而另一些则无人问津，并使研究人员能够研究平台如何对内容进行排名。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么推荐系统是必争之地？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在大多数技术讨论中，推荐系统往往被视为后台工程的一部分，低调、复杂，却很少站在聚光灯下。但如果真正拆解互联网巨头的商业运转方式，会发现推荐系统并不是边缘模块，而是支撑整个商业模式的“基础设施级存在”。正因如此，它可以被称为互联网行业的“沉默巨兽”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公开数据已经反复印证了这一点。亚马逊曾披露，其平台约 35% 的购买行为直接来自推荐系统；Netflix 更为激进，约 80% 的观看时长由推荐算法驱动；YouTube 的情况同样类似，大约 70% 的观看来自推荐系统，尤其是信息流（feed）。至于 Meta，虽然从未给出明确比例，但其技术团队曾提到，公司内部计算集群中约 80% 的算力周期都用于服务推荐相关任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些数字意味着什么？如果将推荐系统从这些产品中移除，几乎等同于抽掉地基。就拿 Meta 来说，广告投放、用户停留时长、商业转化，几乎都建立在推荐系统之上。推荐系统不仅决定用户“看什么”，更直接决定平台“如何赚钱”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，正是这样一个决定生死的系统，长期面临着工程复杂度极高的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统推荐系统架构中，很难用一个统一模型覆盖所有场景。现实中的生产系统往往高度碎片化。以 Meta、LinkedIn、Netflix 这类公司为例，一个完整的推荐链路背后，通常同时运行着 30 个甚至更多专用模型：召回模型、粗排模型、精排模型、重排模型，各自针对不同目标函数和业务指标进行优化。每个模型背后，往往对应一个甚至多个团队，负责特征工程、训练、调参、上线与持续迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种模式的代价是显而易见的：工程复杂、维护成本高、跨任务协同困难。一旦有人提出“是否可以用一个模型解决多个推荐问题”，对整个系统而言，意味着复杂度的数量级下降。这正是行业长期渴望却难以实现的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大型语言模型的出现，给推荐系统提供了一条新的可能路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 已经在实践中证明，它可以成为极其强大的通用模型：在不同任务之间迁移能力强，随着数据规模和算力的扩展，性能还能持续提升。相比之下，传统推荐模型往往是“任务定制型”的，很难在多个场景之间共享能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，单一大模型带来的不仅是工程简化，还包括“交叉学习”的潜力。当同一个模型同时处理多个推荐任务时，不同任务之间的信号可以相互补充，随着数据规模增长，模型更容易整体进化。这正是推荐系统长期渴望、却很难通过传统方式实现的特性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 改变了什么？其实是改变了从特征工程到理解能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从方法论层面看，LLM 对推荐系统最大的改变，发生在“特征工程”这一核心环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统推荐系统中，工程师需要先人为构造大量信号：用户点击历史、停留时长、相似用户偏好、内容标签等，然后明确告诉模型“请基于这些特征做判断”。模型本身并不理解这些信号的语义，只是在数值空间中学习映射关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而引入语言模型后，这一流程被高度抽象。你不再需要逐条指定“看这个信号、忽略那个信号”，而是可以直接向模型描述问题本身：这是一个用户，这是一个内容；这个用户过去喜欢过类似内容，其他用户也对这个内容有正反馈——现在请判断，这条内容是否应该推荐给这个用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语言模型本身已经具备理解能力，它可以自行判断哪些信息是重要信号，如何综合这些信号做出决策。在某种意义上，它不只是执行推荐规则，而是在“理解推荐这件事”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种能力的来源，在于 LLM 在训练阶段接触过海量、多样化的数据，使其更容易捕捉细微但重要的模式。相比之下，传统推荐系统必须依赖工程师显式枚举这些模式，一旦遗漏，模型就无法感知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从后端视角看，这种变化并不陌生。就像你向 GPT 提问，它会基于上下文信息生成回答；同样地，当你问它“我是否会对这条内容感兴趣”，它也可以基于已有信息做出判断。某种程度上，语言模型本身已经天然具备“推荐”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;专家解读：工业界可参考，对学术价值不大&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果 X 的方向真是“让 Grok 成为算法本身”，那么这次开源事件的意义就不止是透明度提升，更像是把一场大模型化推荐的系统级改造公开摆到台前，接受开发者与行业的持续检视与解读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借此机会，我们邀请到了搜推广资深算法专家，生成式推荐模型OnePiece作者，《业务驱动的推荐系统：方法与实践》作者傅聪，为大家解读这次开源事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从代码层面看，X 这套推荐系统中，大模型是否是已经进入核心决策环节？这与传统“LLM + 规则 / 特征管道”的推荐系统相比，最大的结构性变化是什么？是否只是替换了部分模块？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：从系统整体设计层面看，开源的代码依然遵从 recall -&amp;gt; rank这样的多阶段漏斗筛选架构。新的post推送会从数亿 候选集合中 以传统的 双塔 向量召回，合并排序、去重等等环节，最后送给用户。grok没有参与中间过程，只是给post做排序的模型采用了类似grok 的模型架构，但远小于grok的参数量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最大的结构变化在于他们用了一种纯transformer（类grok）的模型结构去做排序，其它差异不大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从能力边界看，该如何看待“每日处理上亿条内容、并进行实时多模态理解”这一目标所带来的系统挑战？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：需要极其充足的GPU算力以及高并发的处理引擎，尤其是视频内容，其token消耗量巨大，因此计算量巨大。此外，模型还需要一个可以高速访问的大型文件系统，保证大量视频可以暂存、传递给Grok模型。而实际上x并没有真的让grok来做这个事情，应该是处于成本考虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：传统推荐系统采用轻量级启发式算法，成本效益高，而Grok方法需要大量计算资源，那么您怎么看待成本和用户体验提升之间的收益比？在算力、成本和基础设施约束下，这种方式是否注定只属于极少数平台？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：Grok消耗的算力是数千倍于传统的推荐系统的，这部分成本往往不能被平台的收益覆盖。尤其是X这样的平台，其收入核心来源是广告。只有做到延迟、体验都能对标原有系统，其广告收入才可以持平。但因为投入成本过高，这个ROI过低，目前来看只X自己也没有真的以这种规模使用grok。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：如果 Grok 真要“把帖子都读一遍、把视频都看一遍”再来做匹配，这是不是相当于把推荐系统推到了更强的“内容级监控”？平台不只是记你点过什么，还能在语义层面猜到你可能会被什么吸引，是否会带来新的以前没有的问题？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：Grok读过并不一定会记忆。很多数据并不一定会被Grok用来训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：另外，传统推荐系统的信息茧房问题，语义理解方式是否能解决？是否更“中立”？（此前的争议有一部分在于认为X平台偏向马斯克个人账号和一些党派言论）。从系统机制上看，它最可能在哪些环节反而更容易固化偏好、放大偏差？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：大语言模型有它自己的bias，以大语言模型为核心的推荐系统会根据它的语言偏好构建新的信息茧房。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：从开源意义看，在推荐系统这种高度复杂、长期被视为“黑箱”的领域，这种“持续、周期性开源”代码的方式，实现起来的难度在哪里？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：难度在于只开源代码，不开源所有配套的系统和训练数据，就无法复现它的效果。这种开源，对学术研究价值不大，对工业交流有一定参考意义。但目前其架构来看，可参考的新东西不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;InfoQ：您如何看待这次开源的影响？如果 Grok 这套思路跑通，这次开源是否会迫使其他内容平台跟进，从而引发推荐系统的一轮“范式迁移”？在这种趋势下，行业会不会弱化对行为数据（包括历史数据）的依赖，甚至调整数据收集与画像方式，进而重塑整个推荐系统生态？对广告行为的影响会是什么样的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪：即使Grok跑通，其它平台也不一定会跟进。第一其他平台没有属于自己的Grok，第二，其它大部分平台不会在这里投入这么多算力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;行业也不会弱化对用户行为和画像的依赖，经验证明，用户历史行为才是实现个性化的数据根基，缺少这部分信息输入的推荐系统很难千人千面，而容易做成千篇一律。从开源代码看，ranking模型依然在使用用户行为历史进行预测，这一点也符合预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;嘉宾简介：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;傅聪，搜推广资深算法专家、生成式推荐模型OnePiece作者，《业务驱动的推荐系统：方法与实践》作者，《生成式推荐系统算法与实践》作者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/xai-org/x-algorithm&quot;&gt;https://github.com/xai-org/x-algorithm&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/XEng/status/2013471689087086804&quot;&gt;https://x.com/XEng/status/2013471689087086804&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/BlockFlow_News/status/2013510113873813781&quot;&gt;https://x.com/BlockFlow_News/status/2013510113873813781&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2lb8A2IuImbvpMI1tR7D</link><guid isPermaLink="false">https://www.infoq.cn/article/2lb8A2IuImbvpMI1tR7D</guid><pubDate>Tue, 20 Jan 2026 10:30:00 GMT</pubDate><author>Tina,李冬梅</author><category>生成式 AI</category></item><item><title>当前关于 Vibe Engineering 的所有认知都会在 1 个月内严重过时</title><description>&lt;p&gt;过去几周，我对于 Vibe Engineering 的实践有了更多的体会, 今天再次总结一下。其实也能看出来我避免使用 Vibe Coding 这个词，是因为当下的重点已经不再是代码，而是一些更高维度的东西。另外，本文的 AI 含量我会尽量控制在 5% 内，可以放心阅读😄。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前我提到的我开始的 TiDB Postgres 重写项目已经不再在是个玩具。在前几天出差的路上, 因为长途飞行没有网络, 我仔细 review 了一下这个项目的代码, 虽然一些地方略有瑕疵, 但是总体质量已经很高, 我认为已经是接近生产水平的 rust 代码，和以前我理解中的早期原型的定义很不一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;顺便提一句, 我认为这个项目从一开始就选择 rust 是一个无比正确的决定, rust 的严谨性让 AI 能写出更接近 bug free 的 infra code (对比我另一个项目 agfs 的 shell 和它自带的脚本语言 ascript，由于这项目使用 python，项目变大后，可维护性就大大降低，但此时重写已经很困难，只能捏着鼻子慢慢重构)，所以现在已经是 2026 年了， 如果你要再启动一个新的 backend infra 项目, rust 应该是你的第一选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;验证差不多后，我也邀请了几位我团队内的几个顶尖的 vibe coder 加入项目, 看看 100% 的 AI Native 研发模式能在多快把这个项目推进到何种程度，无论如何都很想看看，应该会很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面说说自己最近的一些感受。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当前关于 Vibe Engineering 的所有的认知都会在 1 个月内严重过时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并非危言耸听，哪怕我正在写的这篇文章，如果你是 2026 年 2 月看到，那么很遗憾，本文聊到的东西很可能已经过时，这个领域发展的太快，很多今天的 SOTA 也许下个月就过时了。而且很有意思，过去很多对 Vibe Coding 嗤之以鼻的大佬，例如 DHH，Linus，Antirez 等，在 2025.12 月开始纷纷改口，我觉得这是相当正常的，去年 12 月开始，AI 编程工具和头部的模型突然有一个跳跃式的进步，突然对于复杂任务和大型项目的理解，以及写出代码的正确率有了极大的提升。这进步大概来自于两个方面：&lt;/p&gt;&lt;p&gt;一方面头部模型在长上下文（&amp;gt;256K) 的支持，尤其是关键信息的召回率提升惊人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如上面是 GPT-5.2 在长上下文的召回表现和 GPT-5.1 对比很明显，要知道对于 Agent Coding 的场景来说，通常是多轮次推理 + 长上下文（因为要放更多的代码和中间推理结果）才能更好的有大局观，大局观的正确是对于复杂项目起到决定性因素。在这种场景下，你可以做一个简单的计算，一个模型（类似 GPT-5.1) 每轮的召回率 50%，大概 3 轮后，正确的召回率就会降低到 12.5%, 而 GPT-5.2 仍然能保持 70% 以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外一个进步是主流的 Vibe Coding 工具的 Context Engineer 实践日益成熟，例如 Claude Code / Codex / OpenCode。从用户体验到最佳实践，肉眼可见的越来越好，例如对于 Bash 的使用，Subagent 等，这方面越来越多的资深 Engineer 的重度使用和经验分享会对这些工具的进化提供数据飞轮，尤其是 AI 也在深度的开发这些工具，迭代速度只会更快。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实这个进步也并不是去年 12 月那个时间点的突然什么黑科技爆发，其实前几个月一直在进步，不过还不能长时间离开人工干预，更像是那个时间点，主流 Coding Agent 的质量超过了一个临界点：100% 的无人工干预下完成长时间的 Agentic Loop 成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Hire the best (model)，否则就是在浪费生命&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面所有提到的进步，我个人感觉只反映在了最顶尖的闭源头部模型中。我听到很多朋友和我反馈到：“我感觉 AI 编程还是很傻啊？并没有你提到那么聪明”，我首先会反问，你是不是只是用着 $20 一个月那种入门模型？如果是的话，那先去用一阵 $200 以上的 Pro Max 档次的，也许有惊喜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我个人认为，目前主流的模型，即使并非头部那档，作为 chatbot 处理大多数普通人的短上下文的日常工作是完全足够的，哪怕是 GPT-4 在和你讲人生道理的时候也已经足够把你说得一愣一愣了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为人来说，我们的直觉或者是一些简单的 CRUD Demo 已经无法评估这些模型之间的智商差距了。但是在复杂的项目的开发中，这个差距是极端明显的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据我个人的实践来说，当下能用来进行大型 Infra 项目（数据库，操作系统，编译器等）开发的模型大概就两个：GPT-5.2 (xhigh) + Opus 4.5，还有半个算是 Gemini 3 Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大概上个月我主要用着 opencode + oh-my-opencode + Opus 4.5 但是最近两周转向到了 codex + gpt-5.2 的组合，下面分析一下这几个模型的一些脾气和调性，仅仅是个人感受，而且是在后端 Infra 软件开发这个领域，仅供参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Opus 4.5 的风格是速度很快，是个话唠，由于 Sonnet 4 有严重 reward hacking 问题，例如是在解决不了 bug 的时候会偷偷的构造作弊的测试然后糊弄过去，所以导致很长一段时间我都不太敢用 Sonnet 系列模型干复杂的事情，但是这点在 Opus 4.5 中解决得很好，即使在模型冥思苦各种尝试想都搞不定的情况下也没有选择作弊，让我放心不少，但是 Opus 的问题是 reasoning 和做 investigation 的时间太少，动手太快，以至于发现不对的时候，又返回头确认假设和研究，这样的特性催生了像 ralph-loop 这样的奇技淫巧。比方说，同样的一个 prompt 在 Claude Code 结束后又通过 stop hook 重新调用，再完整走一遍流程，不断地逼近最终的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，GPT-5.2 更像是一个更加小心谨慎、话不多的角色。我最开始用 Codex 的体验其实不算太好，因为我一直觉得它有点太慢了。主要是因为我习惯用它的 xhigh 深度思考模式，在真正开始写代码之前，它会花很长时间去浏览项目里的各种文件和文档，做很多准备工作。可能也是因为 Codex 的客户端不会告诉你它的计划和大概需要多久，所以就显得过程特别长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有时候一些复杂的任务，它前期的调查可能就要花上一到两个小时。但是经过长时间思考后它完成的效果通常是更好的，尤其是在一个项目的大体框架已经稳定，Codex 考虑得更周全，最终也体现出更少的 bug 和更好的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于第三个顶级模型，也就是 Gemini 3 Pro。虽然我也知道它的多模态能力非常吸引人，但就复杂任务的 Coding 场景而言，至少从我个人的体验来看，它的表现并没有 Opus 4.5 和 GPT-5.2 那么强。不过它确实针对一些快速的前端项目 Demo 和原型制作做了一些优化，再加上它的 Playground 模式，让你在需要一些炫酷的小 Demo 或前端项目时能更快实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实一个比较反直觉的事情是，过去我们经常说 Vibe Coding 只能搞一些比较简单的事情，比如上面那些小 Demo 或 CRUD 项目，你会看到网上各种各样的 KOL 其实都在做这种小原型，反而大家觉得对于一些像后端这种核心的基础设施代码，当前 AI 还是搞不定的。我以前也这么想，但从去年 12 月份开始，这个结论可能需要修正了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里面的原因是，其实这类基础设施的代码通常是由顶级工程师长期精雕细琢而成，它们有清晰的抽象、良好的测试，甚至代码本身经过多轮重构后也相当精炼。所以当 AI 具备足够的上下文空间 + 更好的推理能力 + 更成熟的 Agentic Loop + 高效的工具调用时，这类 Infra 代码的开发和维护反而是能最有效地利用这些顶尖大模型的智商的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际的工作中，我经常会让多个 Agent 互相协作，或者使用一些复杂的工作流来把它们编排在一起，并不会让一个模型来完成所有的事情。后面我会再分享一些我自己实践中的具体例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;人在什么时候进入？扮演什么角色？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面提到了，这些顶级模型再配合主流的 Vibe Coding 工具，基本上已经能超越大多数资深工程师的水平了。这不仅体现在能写出更少 bug 的代码，也体现在在 review 中能发现更多人类工程师可能看不到的问题，毕竟 AI 真的会一行一行仔细看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以人在这个过程中扮演什么样的角色，哪些阶段只有人才能做？根据我自己的实践来说，第一当然是提出需求，毕竟只有你才知道你想要啥，这很显然，但是有时确实也挺难的，毕竟人很难从一开始就准确描述自己想要什么，这时候我会用一个偷懒的办法：让 AI 来角色扮演，比方说，我在开发 PostgreSQL 版本的 TiDB 时，我就让 AI 假设自己是一个资深的 Postgres 用户，从开发者的视角告诉我有哪些特性是非常重要、一定要实现而且 ROI 比较高的，让它列出 N 个这样的功能点，然后 AI 就会根据它的理解生成一个需求列表，接下来你再和 AI 对这些需求逐个打磨，这其实是一个高效冷启动的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二是在需求提出后，现在的 Coding Agent 大多都会和你有一个规划阶段（Planning），会反复确认你的需求。在这个过程中其实有一些技巧，比如不要给 AI 太具体的方案，而是让 AI 来生成方案，你只需要关注最终你想要的结果；提前告诉 AI 有哪些基础设施和环境的问题，让它少走弯路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我通常会在提出需求的第一阶段就要求 Agent 做的一些关键动作。比如无论接下来做什么，都要把计划和 todo 列表放在一个 work.md 或 todo.md 这类文件里。还有，每完成一个阶段的工作，就把上一阶段的经验教训更新到 agents.md 里。第三点是当一个计划完成并且代码合并后，把这个工作的设计文档添加到项目的知识库中（.codex/knowledge）。这些都是我会在一开始提需求时就放进去的内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个阶段就是漫长的调查、研究和分析的阶段。这个阶段其实基本上不需要人做什么事情，而且 Agent 的效率比人高得多，你只需要等着就好。唯一需要注意的就是在 Research 的过程中，我通常会告诉模型它拥有无限的预算和时间，尽可能充分地进行调研。另外，如果你的模型有推理深度的参数的话，我建议在这个阶段把它们全部调到 xhigh 的级别。虽然这会让过程变慢，但在这个阶段多烧一些 token、做好更好的规划、了解更多上下文，对后续的实现阶段会更有帮助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实现阶段没什么特别好说的，反正我现在基本不会一行行去看 AI 的代码。我觉得在实现阶段唯一要注意的就是，要么你就让 AI 完全去做，要么你就完全自己做，千万别混着来，我目前是倾向于完全零人工干预的模式效果更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四个阶段人就变得非常重要了，那就是测试和验收结果的阶段。其实在我个人和 AI 开发项目的过程中，我 90% 的时间和精力都花在了这个阶段：也就是如何评估 AI 的工作成果，我觉得在 Vibe Coding 时：There&#39;s a test, there&#39;s a feature，你只要知道如何评估和测试你要的东西，AI 就一定能把东西给你做出来。另外值得注意的是，AI 在实现过程中会自动帮你添加很多单元测试，但说实话，这些单元测试在微观层面基本都能通过，毕竟 AI 写这种局部代码时已经很难出 bug。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 AI 不擅长的是集成测试、端到端测试。比如在开发一个 SQL 数据库时，哪怕每个细节的单元测试都没问题，但整合到一起时集成测试可能会出错。所以我在完成大目标前，我一定会先和 AI 一起做一个方便的集成测试框架，并提前准备好测试的基础设施，收集和生成一些现成集成测试的用例，尽量一键能运行那种，这样在开发阶段就能事半功倍，而且关于如何使用这些测试的基础设施的信息，我都会在正式开始前就固化在 agents.md 里，这样就不用每次沟通的时候都再告诉它该怎么测试了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于测试从哪来的问题，我自己的经验是你可以让 AI 帮你生成，但一定要告诉它一些生成的逻辑，标准和目的，另外就是千万不要把生成测试的 Context 和实际进行开发工作的 Agent 的 Context 混在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第五个阶段是重构和拆分。我发现当前的 Coding Agent 在面对单一模块复杂度超过大约 5 万行代码之后，就开始很难在 1-shot 里把问题一次性解决掉（但反过来这也意味着，只要任务复杂度控制在这个阈值之下，在一个足够好的 first prompt 驱动下，很多事情确实可以做到 1-shot AC），Agent 通常不会主动去做项目结构和模块边界的治理，你要它把功能做出来，它恨不得把所有东西都写进几个几万行的大文件里，短期看似很快，长期就是债务爆炸。我自己在这个阶段的做法通常是先停下来，用自己的经验进行模块拆分，然后在新的架构下进行 1～2 轮的重构，之后又可以高并发度的进行开发了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多 Agent 协同编程的一些实践&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面提到我现在使用 Coding Agent 的时候，通常不会只用一个，我自己的工作流会尽量让多个 Coding Agent 同时工作。这也是为什么有时候在一些项目上会花掉好几千美金，因为你必须把并发跑起来。当然，并发和吞吐是一方面，但另一方面我觉得让不同的 Agent 在不共享上下文的前提下互相 Review 工作，其实能显著提高质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就像在管理研发团队时，你不会让同一个人既当运动员又当裁判。相当于 Agent A 写的代码交给 Agent B 来 Review，往往能发现一些 A 看不到的问题。通过这样的循环往复，你就会更有信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，我在实际工作中现在用得比较好的一个工作流是这样的：首先让 GPT-5.2 在 Codex 下生成多个功能的设计文档，做出详细的设计和规划，第一阶段把这些规划文档都保存下来。然后在第二阶段，依然用 Codex 根据这些需求文档一个一个去实现功能。在实现的过程中，就像我前面提到的那样，记录 To-Do、经验教训，并在接近完成的时候，在代码通过测试并准备提交之前停下，把当前的工作区交给另一个 ClaudeCode 或 OpenCode，在不提供上下文的情况下，让 ClaudeCode 来 Review 当前还未提交的代码，根据设计提出修改建议。然后再把这些建议发回给 Codex，让 Codex 来评论这些建议，如果有道理就修改代码。改完之后，再让 ClaudeCode (Opus 4.5) 那边再次 Review，直到双方都觉得代码已经写得很不错了，再提交到 Git 上，标记这个任务完成，更新知识库，然后进入下一个功能的开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外在一个大型项目中我会同时开多个 Agent (in different Tmux) 并行开发多个功能，但我尽量让它们负责完全不同的模块。比如一个 Agent 修改内核代码，另一个 Agent 做前端界面，这样就能分开进行，如果你需要在一份代码上做一些彼此不太相关的工作时，可以利用 git 的 worktree 让多个 Agent 在不同的 git 分支上各自工作，这样也能快速提升吞吐量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;未来的软件公司和组织形态&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的软件公司会是什么形态呢？反正从我自己的实践和与一些朋友的交流来看，至少在当下，团队中用 Coding Agent 的 token 的消耗呈现出一个非常符合二八定律的分布，也就是说，最头部的用 AI 用得最好的工程师，他们消耗的 token 可能比剩下 80% 的工程师加起来还要多，而且 Coding Agent 对于不同工程师产出（质量，吞吐）的增益是不一样的，这个方差非常大，也就是对于用的最好的一群人，他们的增幅可能是 10x，但是普通人可能也就是 10%，而且唯一的瓶颈是人工的 code review 和一些无法被自动化的线上运维工作（我觉得也很快了）而且这样的特点能够让这些头部的工程师在 AI 的协助下可以无边界的工作，也就是会有越来越多的 one-man army 出现，只是目前我认为和 token 消耗是正相关的，你能花掉多少 token，大致代表你能做得多好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外我发现一个很有趣的现象，同样是 10x 的工程师，他们各自的 Vibe Coding 工作流和最佳实践其实并不相同。也就意味着，两个顶尖的 Vibe Coder 是很难在一个项目中（的同一个模块）协作。这种工作方式更像是头狼带着一群狼群（Agents），在一片自己的领地里面耕耘，但是同一片领地里很难容纳两匹头狼，会造成 1+1 &amp;lt; 2。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的组织形态下，我觉得传统意义上的“团队协作方式”会被重新定义。过去我们强调的是多人在同一个代码库、同一个模块里高频协作，通过评审、讨论、同步来达成共识；但在 Vibe Engineering 这种模式下，更有效的方式反而可能是强解耦的并行。管理者要做的是把问题切分成足够清晰、边界明确的“领地”，让每一个头部工程师带着自己的 Agent 群，在各自的领域里做到极致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从管理的角度看，这其实是一个挺大的挑战。因为你不能再用统一流程、统一节奏去约束所有人。对顶尖的 Vibe Coder 来说，过多的流程和同步反而会显著拉低效率，甚至抵消 AI 带来的增益。管理者更像是在做“资源调度”和“冲突隔离”：确保不同头狼之间尽量少互相干扰，同时在必要的时候，能够通过清晰的接口、契约和测试来完成协作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为上面的种种，AI-Native 的研发组织其实很难自底向上从一个非 AI-Native 的组织中生长出来，因为大多数开发者面对变革的时候的第一反应其实并不是拥抱，而是回避和抵触，但是时代的进步不会因为个人的意志转移，只有主动拥抱和被动拥抱的区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大概就写到这里吧，总的来说，在这样一个大环境下，对个人而言意味着一场深刻的转变，就像我之前在朋友圈里提到的，我身边最好的工程师们有一些已经陷入了或多或少的存在主义危机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但是作为具体的 Builder 的我来说是兴奋的，因为造物，在当下，门槛变低了许多，如果你能从造物中能获得成就感和找到人生的意义，那恭喜你，你活在一个最好的时代。但反过来，作为一个抽象的 “人” 来说，我又是悲观的，人类是否准备好面对这样的工具？以及这样工具带来的对于社会和整个人类文明的冲击？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我不知道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/k05gRzGFhz4QerCz4ARl</link><guid isPermaLink="false">https://www.infoq.cn/article/k05gRzGFhz4QerCz4ARl</guid><pubDate>Tue, 20 Jan 2026 10:13:12 GMT</pubDate><author>黄东旭</author><category>生成式 AI</category><category>数据库</category></item><item><title>OpenAI 广告续命遭全网骂，用户要跑路Gemini！需烧400 亿，18个月破产预警</title><description>&lt;p&gt;近日，OpenAI在其官方网站及官方社交媒体公告中表示，公司计划在“未来几周内”开始在ChatGPT对话界面中测试广告投放，这些广告将首先面向美国地区的免费版用户以及新推出的低价订阅层级“ChatGPT Go”用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;广告内容的展示形式预计主要是在ChatGPT生成的回答底部以清晰标注的独立模块形式出现，与AI生成内容严格区分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/1681e00cb156e94baedbc5de08e78cfa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI强调，广告不会影响ChatGPT的回答逻辑，也不会向广告商分享用户对话内容。付费订阅用户（如Plus、Pro、Business 及 Enterprise层级）仍将享受无广告体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据官方发布内容及多家外媒消息，OpenAI此举是为了进一步拓展营收来源，以缓解高昂的研发与基础设施支出压力，同时扩大服务的可持续性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司管理层表示，即便公司业务规模庞大，依靠订阅收入仍难覆盖巨额算力成本，而广告收入是补充营收的一种必要尝试。OpenAI同时承诺，广告不会改变AI应答过程，并且将在敏感话题如健康、政治等领域避免投放广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI此举引发了社区热议，但批评声音居多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Hacker News上，有用户表示，由于他们加了广告，很多用户已经转向了Gemini，所以长远来看这种行为是得不偿失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“OpenAI 广告的一个问题是，用户已经开始转向 Gemeni，而 Gemeni 并不投放广告。&amp;nbsp;ChatGPT 大多数情况下也比 Gemeni 差（或许如此），而且没有像 Gemeni 那样严格的速率限制。因此，他们已经开始流失用户，并且产品体验也比竞争对手更糟糕。&amp;nbsp;OpenAI 当然能从广告中赚到一些钱，但这能弥补他们巨额的亏损吗？我觉得不太可能。他们真的需要像微软那样，被一位财力雄厚的“金主”收购，才能玩转这种游戏。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ab/ab04c6eea33ee463df39c99f4c6de4b5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有用户表示即使他们加入了广告，也不会向谷歌和Facebook 那样赚大钱，只是赚一些小钱罢了。该用户评价道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“就我所了解的广告市场而言，像谷歌和Facebook这样的公司之所以能赚得盆满钵满，主要是因为它们在广告市场的垂直整合中占据了绝对优势。而 OpenAI 整合广告的方式在我看来，似乎只是想分得蛋糕里最小的一块——一个投放广告的地方——这意味着，我估计他们的用户收入更接近于报纸网站，而不是最大的社交媒体网站，或者更接近于推特或 Tumblr 这类从未实现过巨额盈利的公司。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c6/c6bd1d9ddfc915c46dc857f7023a2a78.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e8/e80fac779975b727b1b89c12981bac1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明知加广告会被骂，OpenAI为什么还要这么做？那就要从OpenAI的财务状况说起。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;营收增长10倍，但算力投入扩大9.5倍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI的财务状况与算力投入呈现出高度协同的增长态势，过去三年，二者均实现了累计十倍左右的扩张，印证了“算力决定营收上限”的核心逻辑。这种强关联不仅是业务发展的结果，更成为OpenAI规划未来投入、平衡供需关系的重要依据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在OpenAI最新一期博客中，公司首席财务官 Sarah Friar 透露了 OpenAI 的财务细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从算力投入来看，OpenAI的扩张速度堪称惊人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年，其算力规模为0.2 吉瓦（GW）；2024年，迅速提升至0.6 吉瓦；2025年，进一步增至约1.9 吉瓦，三年累计扩大约9.5倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为保障未来算力供应，Sarah Friar 称 OpenAI已与微软、英伟达、AMD、甲骨文等企业签署数千亿美元的合作协议，同时从单一供应商转向多云、多芯片的多元化布局，在高端训练任务中采用最新硬件，在大规模推理场景中使用低成本基础设施，平衡效率与开支。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，算力投入存在显著的时间差，当前的投入需提前规划至2028～2030年的需求，这也意味着OpenAI需要稳定的长期收入来覆盖前置成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;营收方面，OpenAI同步实现了三倍速年度增长，与算力扩张节奏高度匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年，其收入达到20亿美元，2024年增至60亿美元，2025年预计突破200亿美元，三年累计增长约十倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种增长并非依赖单一业务，而是构建了多元化的收入结构：一是订阅收入，涵盖个人用户的ChatGPT Go、Plus、Pro档位及企业订阅服务，满足不同层级用户需求；二是API服务收入，为开发者和企业提供模型调用能力，支出与交付成果直接挂钩；三是广告与电商收入，依托免费用户流量开辟新增长曲线；未来还将探索授权许可、知识产权合作、结果导向定价等模式，进一步丰富收入来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从运营效率看，OpenAI的算力投入与营收的强相关性，验证了其商业模式的可行性。但当前仍面临算力缺口的核心挑战——由于算力不足，诸多潜在产品与功能无法落地，尚未充分释放价格弹性杠杆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也解释了为何OpenAI持续加码算力投资，同时通过广告等业务拓宽收入渠道，本质上是为了打破算力瓶颈，释放更多商业价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;加入广告，也会坚守三大原则&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从成本端看，算力是OpenAI发展的核心命脉，且需求近乎无限。但Sarah Friar在博客中表示，即便在模型中加入广告，也会“死守”三大底线。他表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“大家普遍会疑惑，广告会对产品本身和公司运营产生怎样的影响？要回答这个问题，我们可以从当前的用户结构说起：如今，我们消费端平台上 95% 的用户都在使用免费版本。这恰恰契合了我们的使命 —— 研发通用人工智能是为了造福全人类，而非仅仅服务于有能力付费的群体。因此，保障用户的访问权至关重要。&amp;nbsp;从广告业务的角度出发，我认为有三点原则必须坚守。第一，我们要让所有人都清楚：模型给出的永远是它能提供的最佳答案，而非付费推广的结果。很多其他平台正是在这一点上栽了跟头，导致用户无法判断看到的内容是付费广告还是真实的最优推荐。而我们的核心准则就是，模型始终以提供最优答案为导向。&amp;nbsp;第二，广告本身可以具备很高的实用价值。我们会明确标注广告内容，让用户一目了然。举个例子，如果用户搜索 “圣地亚哥周末短途旅行”，那么一条爱彼迎的广告可能会非常有帮助。用户甚至可能愿意在 ChatGPT 的对话场景中，与广告商展开深度交流 —— 前提是他们清楚这是广告环节。这正是我们需要创新的方向，要打造出与平台生态深度融合的广告形式，而非简单地把传统的横幅广告生搬硬套过来。&amp;nbsp;第三，也是最后一点，我们必须保留无广告的服务层级，让用户拥有选择权和控制权。同时，我们对用户数据的保护始终保持高度谨慎。此前推出医疗健康功能时，我们就明确告知用户，相关数据会被隔离存储，不会用于模型训练。信任是 OpenAI 的立足之本，即便在广告业务上，我们也会坚守这些原则。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sarah Friar还表示，其实不只是OpenAI，未来，消费者很可能会订阅多款人工智能服务，就像现在大多数人都会订阅不止一个流媒体平台一样，这一消费行为模式可以作为很好的参考。不同的人会根据自身需求做出不同选择，包括免费选项 —— 毕竟也有广告支持的免费流媒体服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且即便是同一项服务，也会同时提供付费版和免费版两种选择，未来的市场格局会呈现出丰富的多样性。不过，用户切换不同平台时也会面临一个问题 —— 迁移成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sarah Friar还表示模型的记忆功能也是值得探讨的问题。他还进一步表示OpenAI不会垄断整个市场：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“未来的模型是会实现跨平台统一记忆，还是会分平台独立记忆？其实即便是基于同一个模型，不同服务商也会推出各具特色的服务，在功能取舍上各有侧重。哪怕是依托 OpenAI 模型的服务，也有很多不同的开发者在提供差异化产品，这也是我所理解的 ‘多平台使用’ 的含义。当然，我并不认为 OpenAI 会垄断整个市场。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为维持算力与营收的同步增长，OpenAI需要持续投入数千亿美元用于基础设施建设与合作伙伴拓展，而单一的订阅制模式难以支撑如此庞大的资金需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;广告业务的引入，能够借助免费用户的流量规模，开辟新的收入来源，为算力投入提供稳定的资金补充，形成“算力支撑业务、业务反哺算力”的循环。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，广告业务的布局也与OpenAI的长期战略相契合。在ChatGPT月活用户突破8亿且仍有巨大增长空间的背景下，广告成为连接免费用户与商业价值的桥梁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据业内消息，OpenAI预计2026年通过广告获得数十亿美元级收入，未来将逐步放大这一收入来源，与订阅、API服务等形成互补，降低单一模式的经营风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI缺钱了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;既有巨额的算力成本支出，又有逐年翻倍的营收进账，那 OpenAI到底是不是真的缺钱了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，《纽约时报》的一位专栏作家却做出了一个明确的预测：OpenAI 将在 18 个月内因其在人工智能领域的投入而破产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该作家表示，根据去年的一份外部报告，OpenAI 预计在 2025 年将烧掉 80亿美元，到 2028 年将烧掉 400 亿美元。鉴于该公司据报道预计到 2030 年实现盈利，不难计算出其中的利害关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Altman的风险投资计划在数据中心领域投入1.4万亿&amp;nbsp;美元。正如外交关系委员会经济学家Mallaby所指出的，即便OpenAI重新考虑那些受盲目乐观影响的承诺，并“用其估值过高的股票为其他投资买单”，仍然存在巨大的资金缺口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mallaby并非唯一持此观点的人，贝恩公司去年发布的报告显示，即便在最乐观的预期下，该行业也至少存在8000亿美元的资金缺口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a05b0161bf40898d79a714388b440d0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位金融专家巧妙地分析了这种情况，他概括地指出，问题的关键不在于终端用户人工智能是否会在技术上得到普及，而在于开发人工智能在中长期内是否具有经济意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;分析师指出，理论上，投资者应该“弥合一项伟大技术出现与最终盈利之间的差距”，但实际上，许多人工智能公司烧钱的速度似乎远远超过了其盈利能力。Mallaby指出，鉴于微软或Meta等“传统”公司在人工智能出现之前就已经拥有盈利业务，并且（实际上）有能力等待必要的时期，直到人工智能最终带来收益，因此，这些新来者的处境比它们要糟糕得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据他所说，大多数人都在使用免费服务，一旦他们常用的AI模型添加了广告或使用限制，他们就会毫不犹豫地转向竞争对手。目前各种任务都有无数的选择，也证实了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，他认为这对人工智能提供商来说只是暂时的难题，随着智能人工智能越来越深入人们的日常生活，转换将会变得更加困难，因为AI模型最终应该能够掌握你所有的购物偏好、愿望和情感特征——甚至可能比你本人做得更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mallaby确实赞扬了 OpenAI 首席执行官 Sam Altman 的“吸金能力”，他成功筹集了400亿美元的投资，超过了历史上任何一轮私募融资的规模——甚至超过了沙特阿美300亿美元的融资额。不同之处在于，沙特阿美和其他一些上市企业拥有成熟的商业模式和盈利能力，而OpenAI目前这两点都不具备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能金融这条衔尾蛇看起来确实像是要吞噬自己的尾巴，但也有人认为它只会失去较新的部分。如果人工智能市场失去一个或多个开创者，那将颇具讽刺意味。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances&quot;&gt;https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46663341&quot;&gt;https://news.ycombinator.com/item?id=46663341&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xSxkCYr7zMyCVYQBYf1F</link><guid isPermaLink="false">https://www.infoq.cn/article/xSxkCYr7zMyCVYQBYf1F</guid><pubDate>Tue, 20 Jan 2026 10:03:57 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>聚焦算力市场痛点，嘉唐算力供应链平台重磅发布</title><description>&lt;p&gt;在近期举行的第五届AIGC开发者大会上，上海嘉唐科技发布了名为“算力供应链服务平台”的全栈式解决方案。该平台以“生态共建，供需协同”为理念，围绕算力交易、金融配套、资产管理及算电协同等维度展开设计，旨在应对当前算力行业存在的价格透明度低、流程不规范、服务缺乏标准化及供需匹配效率不高等问题，致力于为构建全国算力服务统一市场提供技术支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，算力作为数字经济发展的重要基础设施，已成为衡量新质生产力的关键指标之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据统计，近五年来我国算力产业规模年均增速超过30%，但与此同时，行业仍面临资源结构性失衡、整合程度不足等制约高质量发展的挑战。为此，市场上陆续出现多种服务模式探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉唐科技此次推出的平台整合了撮合与直营等模式，尝试在供需对接、资源保障、产业链协同及能耗优化等方面提供系统性支持，其中算电协同方案通过引入绿电直供等方式，尝试推动算力行业能耗成本优化与绿色化转型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从平台架构来看，其采用“1+3+N”的设计思路，即一个综合服务底座，涵盖算力交易、资产管理、金融服务三大核心模块，并计划拓展至多个行业应用场景。该架构试图在资源整合、智能调度与服务标准化等方面做出探索，与行业主管部门推动的算力互联互通方向具有一定的契合性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生态合作方面，多家来自能源、金融、科技等领域的企业参与了此次发布仪式，并表达了在资源共享与产业协同方面的合作意向。行业分析指出，此类跨领域协作有助于将企业单体优势扩展为产业链整体效能，对推动AI技术在不同行业的落地应用可能形成一定支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内观察显示，随着算力在经济社会各领域渗透不断加深，构建开放、高效、协同的算力供应链体系逐渐成为行业共同关注的议题。相关平台的出现，反映了市场主体在整合算力资源、提升服务能效方面的尝试，其长期成效仍有赖于技术可靠性、模式可持续性及行业协同机制的进一步完善。在算力市场竞争日趋全球化、绿色化的背景下，此类探索也为推动产业高质量发展提供了可供观察的案例。&lt;/p&gt;</description><link>https://www.infoq.cn/article/vbx7cX5Mvva7szH6Fl53</link><guid isPermaLink="false">https://www.infoq.cn/article/vbx7cX5Mvva7szH6Fl53</guid><pubDate>Tue, 20 Jan 2026 09:59:30 GMT</pubDate><author>李冬梅</author><category>芯片&amp;算力</category></item><item><title>智元首席科学家罗剑岚：2026年挖出真实场景的Scaling law，全系机器人将上线“新系统”</title><description>&lt;p&gt;“通用性不再是主要瓶颈，部署中的任务集熟练度和可靠性才是决定机器人能否真正落地的关键。”在近期的一场采访中，智元机器人合伙人、首席科学家罗剑岚称，2026 年是机器人从会做很多事但每个事做得不太好走向把事情做好并落地的关键节点，要求学习范式从静态离线训练升级为部署学习再部署的整套数据闭环系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示，正是基于这个判断，智元机器人具身研究中心提出了 SOP（Scalable Online Post-training），一套面向真实世界部署的在线后训练系统。SOP 的核心目标是，让机器人在真实世界中实现分布式、持续的在线学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据罗剑岚透露，智元今后会在所有机器人上应用SOP。今年，智元计划部署比现在大几个数量级的机器人，真正找到机器人真实场景部署和真实场景落地的 Scaling law。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要在真实世界中大规模运行，通用机器人必须同时满足两个看似矛盾的要求：在复杂多变的环境中保持稳定性与可靠性；在处理差异巨大的任务时，仍具备良好的泛化能力。现有 VLA 预训练模型已经提供了强大的通用性，但真实世界的部署受困于更高的任务专精度要求以及离线数据采集方式的边际效益递减，往往需要通过后训练获得更高的任务成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，当前主流的 VLA 后训练方法仍受离线、单机、串行采集等因素制约，难以支撑高效、持续的真实世界学习。这些限制并非源自具体算法，而是来自学习范式本身。智元方面介绍，SOP 改变的不仅是训练范式，更是机器人系统的生命周期。如果说 VLA 让机器人第一次具备了通用理解与行动能力，那么 SOP 所做的是让众多机器人的经验共同驱动智能的快速成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“SOP目前不是完全开源的，但不排除未来开放的合作形式。”罗剑岚表示，智元从成立之初就坚持走生态开放的路线，希望跟更多厂商一起共建SOP，把SOP的闭环真正接入到业务流程里。SOP不是封闭系统，而是一种新的持续学习、在线学习、协同进化的方式，任意的后训练算法和模型都可以接进来，智元会开放一些SOP的关键模块和接口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从长远来讲，智元的目标是构建一个开放的机器人在线学习生态，不同的机器人本体都可以接入，让数据共享上传到云端一个大脑，数据回传回来并不断进化，给大家使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;SOP：分布式在线后训练框架&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SOP采用Actor–Learner异步架构，本身是一套通用的框架，可以即插即用的使用任意后训练算法，让VLA从在线经验数据中获益。智元选取HG-DAgger（交互式模仿学习）与RECAP（离线强化学习）作为代表性算法，将其接入SOP框架以进化为分布式在线训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，他们将 VLA 后训练从“离线、单机、顺序”重构为“在线、集群、并行”，形成一个低延迟的闭环系统：多机器人并行执行 → 云端集中在线更新 → 模型参数即时回流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/165b264a617472a960966c8d46b445b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP 架构设计图&lt;/p&gt;&lt;p&gt;SOP的关键优势包括：&lt;/p&gt;&lt;p&gt;•&amp;nbsp;高效状态空间探索。分布式多机器人并行探索，显著提升状态–动作覆盖率，避免单机在线学习的局限。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;缓解分布偏移。所有机器人始终基于低延迟的最新策略进行推理采集，提升在线训练的稳定性与一致性。&lt;/p&gt;&lt;p&gt;•&amp;nbsp;在提升性能的同时保留泛化能力。传统的单机在线训练往往会使模型退化为只擅长单一任务的“专家”， SOP 通过空间上的并行而非时间上的串行，在提升任务性能的同时保留 VLA 的通用能力，避免退化为单任务专家。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;实验评估：性能、效率与 Scaling Law&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际效果方面，智元围绕三个方面对 SOP 进行了系统性评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是 SOP 能为预训练 VLA 带来的影响。实验结果说明，在各类测试场景下，结合SOP的后训练方法均得到了显著的性能提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比预训练模型，结合SOP的HG-Dagger方法在物品繁杂的商超场景中实现了33%&amp;nbsp;的综合性能提升。对于灵巧操作任务（叠衣服和纸盒装配），SOP 的引入不仅提升了任务的成功率，结合在线经验学习到的错误恢复能力还能明显提升策略操作的吞吐量。结合SOP的HG-Dagger方法让叠衣服的相比HG-Dagger吞吐量跃升114%。SOP让多任务通才的性能普遍提升至近乎完美，不同任务的成功率均提升至94%以上，纸盒装配更是达到98%的成功率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/33aa742594ed935cadfafc5c7304bf70.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了进一步测试真机SOP训练后VLA模型是否达到专家级性能，他们让SOP训练的VLA模型进行了长达36小时的连续操作，模型展现出了惊人的稳定性和鲁棒性，能够有效应对真实世界中出现的各种疑难杂症。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，智元使用了三种机器人队伍数量（单机、双机、四机配置），在同样的数据传送总量的基础上，进行了比较。实 验结果表明，在相同的总训练时间下，更多数量的机器人带来了更高的性能表现。在总训练时间为3小时的限制下，四机进行学习的最终成功率达到了92.5%，比单机高出12%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们认为，多机采集可以有效阻止模型过拟合到单机的特定特征上。同时，SOP 还将硬件的扩展转化为了学习时长的大幅缩短，四机器人集群相比单机能够将模型达到目标性能的训练速度增至2.4倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/7968e86516261f7dd57bb0a3763068e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP 学习效率提升&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，他们探究了 SOP 和预训练数据之间的关系，把总量为160小时的多任务预训练数据分为了三组：20小时，80小时和160小时，分别训练一组初始模型后再进行 SOP。接着发现，预训练的规模决定了基座模型和后训练提升的轨迹。SOP 能为所有初始模型带来稳定的提升，且最终性能与VLA预训练质量正相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，对比80小时和160小时实验效果，在解决特定失败情况时，在轨策略经验带来了非常显著的边际效果。SOP 在三小时的在轨经验下就获得了约30%的性能提升，而80小时额外人类专家数据只带来了4%的提升。这说明在预训练出现边际效应递减的情况下，SOP 能够高效突破VLA性能瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca3462b2b121f4f3c33ffa0e53ddaa2e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;SOP在不同预训练数据规模下的对比&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，智元将机器人队伍放到了预训练模型没有见到的真实新环境下执行任务，并使用SOP进行在线训练。当机器人被置于不同的环境时，即便是同样的任务，起初成功率和吞吐量如预期般下降，但在 SOP 介入仅仅几个小时后，机器人的性能便显著回升，能够鲁棒地执行相对复杂的实际任务。&lt;/p&gt;</description><link>https://www.infoq.cn/article/wJjaPXC8rLTxLTeOjLZ0</link><guid isPermaLink="false">https://www.infoq.cn/article/wJjaPXC8rLTxLTeOjLZ0</guid><pubDate>Tue, 20 Jan 2026 09:55:02 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>微软为MCP服务器发布了Azure函数支持</title><description>&lt;p&gt;微软已将其模型上下文协议（MCP）对 &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview&quot;&gt;Azure Functions&lt;/a&gt;&quot;的支持提升至一般可用性，标志着向标准化、身份安全的代理式工作流程的转变。通过集成原生OBO认证和流式HTTP传输，本次更新旨在解决历史上阻碍AI智能体访问敏感下游企业数据的“安全痛点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP扩展于&lt;a href=&quot;https://techcommunity.microsoft.com/blog/appsonazureblog/build-ai-agent-tools-using-remote-mcp-with-azure-functions/4401059&quot;&gt;2025&lt;/a&gt;&quot;年4月进入公开预览，现支持&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-dotnet&quot;&gt;.NET&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-java&quot;&gt;Java&lt;/a&gt;&quot;、JavaScript、&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-python&quot;&gt;Python&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/Azure-Samples/remote-mcp-functions-typescript&quot;&gt;TypeScript&lt;/a&gt;&quot;，而新的自托管选项允许开发者在不修改代码的情况下部署现有的基于MCP SDK的服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由Anthropic开发的模型上下文协议（&lt;a href=&quot;https://modelcontextprotocol.io/docs/getting-started/intro&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;）提供了一个标准化的接口，使AI智能体能够访问外部工具、数据源和系统。自2024年11月推出以来，包括OpenAI、谷歌DeepMind和微软在内的主要AI平台已采用该协议，到2025年4月，&lt;a href=&quot;https://www.mcpevals.io/blog/mcp-statistics&quot;&gt;服务器下载量从大约10万次增长到超过800万次。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，正如Mirantis的Randy Bias&lt;a href=&quot;https://www.mirantis.com/blog/securing-model-context-protocol-for-mass-enterprise-adoption/&quot;&gt;所指出的那样&lt;/a&gt;&quot;：“安全和合规团队不能允许运行在开发人员笔记本电脑上的未经审查的‘影子代理’访问电子医疗记录或客户个人身份信息等关键数据系统”——这推动了对具有内置治理的托管平台的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一般可用的MCP扩展引入了几个为生产部署设计的功能。对流式HTTP传输协议的支持取代了旧的服务器发送事件（SSE）方法，微软建议除非客户端特别需要SSE，否则使用新的传输。该扩展暴露了两个端点：/runtime/webhooks/mcp用于流式http和/runtime/webhooks/mcp/sse用于遗留的SSE连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Java开发人员，Maven构建插件（版本1.40.0）提供了构建时对MCP工具注释的解析和验证，自动生成正确的扩展配置。根据微软的说法，这种构建时分析可以防止运行时反射在Java应用程序中引入的冷启动时间增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;内置的认证和授权实现了&lt;a href=&quot;https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization&quot;&gt;MCP授权协议&lt;/a&gt;&quot;要求，包括发出401挑战和托管受保护资源元数据文档。开发者可以为服务器认证配置Microsoft Entra或其他OAuth提供商。该功能还支持代表用户（&lt;a href=&quot;https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-on-behalf-of-flow&quot;&gt;OBO&lt;/a&gt;&quot;）认证，使工具能够使用用户的身份而不是服务账户访问下游服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首席软件工程师Den Delimarsky在2025年4月分享了关于使用Azure Functions和API管理实现安全的MCP服务器的&lt;a href=&quot;https://den.dev/blog/remote-mcp-server/&quot;&gt;见解&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;开发者面临的一个主要痛点是实现与认证和授权相关的任何内容。如果你没有安全专业知识，这本质上是痛苦且有风险的。你可能会错误地配置一些东西，最终将所有数据暴露给不能看到它们的人。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sitecore的云架构师Victor Karabedyants&lt;a href=&quot;https://www.linkedin.com/pulse/mcp-servers-azure-functions-moving-your-ai-tools-from-karabedyants-uvxbc/&quot;&gt;详细说明&lt;/a&gt;&quot;了实践中的认证流程。当客户端连接到远程MCP服务器时，Azure Functions会以包含受保护资源元数据路径的401响应拒绝初始匿名请求。客户端读取此元数据，触发Microsoft Entra ID登录流程，获得OAuth令牌，并用令牌重试请求。“你的Python或Node脚本永远不会看到认证逻辑，”Karabedyants解释说。“平台负责处理繁重的工作。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Java开发者，Maven Build Plugin（版本1.40.0）在构建时提供MCP工具注释的解析和验证，自动生成正确的扩展配置。据微软称，这种构建时分析可以防止Java应用程序中运行时反射引入的冷启动时间增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/self-hosted-mcp-servers?pivots=programming-language-csharp&quot;&gt;新的自托管MCP服务器&lt;/a&gt;&quot;功能目前处于公开预览阶段，允许团队将使用官方&lt;a href=&quot;https://github.com/modelcontextprotocol/servers&quot;&gt;SDK&lt;/a&gt;&quot;构建的MCP服务器部署到Azure Functions作为自定义处理程序；轻量级Web服务器代理请求到开发者的现有进程。微软将此描述为“提升和转移”方法，只需要一个host.json配置文件来定义Functions应该如何运行服务器。该功能目前支持使用Python、TypeScript、C#或Java SDK实现的流式http传输的无状态服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5db15c5de3126e46d94a031df01b619b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（来源：&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/self-hosted-mcp-servers?pivots=programming-language-csharp&quot;&gt;Microsoft Learn&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软的高级云倡导者Yohan Lasorsa在开发者社区&lt;a href=&quot;https://developer.microsoft.com/blog/host-your-node-js-mcp-server-on-azure-functions-in-3-simple-steps&quot;&gt;博客文章中&lt;/a&gt;&quot;强调了自托管方法的简单性：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Azure Functions上托管MCP服务器，可以让你兼得两者的优点：无服务器基础设施的简单性和官方Anthropic SDK的强大功能。只需一个简单的配置步骤，你就可以将现有的Node.js MCP服务器部署到一个生产就绪、自动扩展的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gaurav Rawat在Medium上一篇关于生产部署模式的详细&lt;a href=&quot;https://blog.dataengineerthings.org/how-to-architect-deploy-and-operate-production-grade-model-context-protocol-mcp-servers-with-ce83d64bb015&quot;&gt;文章&lt;/a&gt;&quot;中，强调了在大规模运行MCP服务器时的几个运维考虑因素。他指出，对于P95延迟超过1秒、错误率超过2%以及SSE连接频繁掉线等监控指标，需要在生产环境中立即进行调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rawat还记录了实践者应该意识到的当前限制：在与Azure AI Foundry集成时，嵌套数组和复杂类型必须序列化为逗号分隔的字符串，并且由于UI基础的批准在自动化部署中不持久，因此需要使用require_approval=&quot;never&quot;进行程序化工具批准以用于生产工作流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Azure Functions 提供了多种托管计划，以满足不同的MCP服务器需求。&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/flex-consumption-plan&quot;&gt;Flex&lt;/a&gt;&quot;消费计划根据需求自动扩展，采用按执行付费的计费模式和零规模经济。当MCP工具闲置时，成本降至零，同时保持快速的唤醒时间。&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-premium-plan&quot;&gt;Premium&lt;/a&gt;&quot;计划支持“始终就绪”的实例，这些实例保持预初始化状态，消除了冷启动延迟，这对于初始化延迟可能导致SSE连接超时和代理响应时间差的关键时刻工具至关重要。Rawat建议为关键的24/7工具设置两到三个始终就绪的实例，以确保故障转移能力。开发人员还可以使用专用计划来满足需要可预测性能或与虚拟网络集成的工作负载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软已经发布了多种语言的快速入门模板，涵盖这两种托管方法。MCP扩展快速入门覆盖了&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-dotnet&quot;&gt;C# (.NET)&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-python&quot;&gt;Python&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Azure-Samples/mcp-sdk-functions-hosting-node&quot;&gt;TypeScript (Node.js)&lt;/a&gt;&quot;，Java快速入门即将推出。该平台直接与&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-mcp-tutorial?tabs=mcp-extension&amp;amp;pivots=programming-language-python#configure-azure-ai-foundry-agent-to-use-your-tools&quot;&gt;Azure AI Foundry&lt;/a&gt;&quot;集成，允许智能体在无需额外配置层的情况下发现和调用MCP工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/azure-functions-mcp-support/&quot;&gt;https://www.infoq.com/news/2026/01/azure-functions-mcp-support/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HY40dxuIKS2uTIJmUb96</link><guid isPermaLink="false">https://www.infoq.cn/article/HY40dxuIKS2uTIJmUb96</guid><pubDate>Tue, 20 Jan 2026 08:24:00 GMT</pubDate><author>Tim Anderson</author><category>微软</category><category>AI&amp;大模型</category></item><item><title>大模型低价趋势延续！智象未来姚霆：B端、C端界限模糊，API不够、逼出 “按结果付费”</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。我们采访了智象未来联合创始人姚霆，他指出在多模态领域，深度 Scaling up 模型能力提升收益放缓，而广度 Scaling up 会带来更多惊喜，多模态能力也在重塑大模型推理过程。另外，2025 年的模型价格战倒逼厂商三大加速：研发新模型抢占短暂的版本优势、提升推理速度、升级高性价比架构降本。他认为，低价趋势 2026 年将延续，核心原因是市场远未饱和。结合公司情况，姚霆表示模型商业模式从卖 API、积分制转向“按结果付费”。下面是详细对话内容，以飨读者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态大模型的 Scaling up&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：Scaling up 是否仍是最佳路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;对于多模态大模型而言，Scaling up 有深度和广度。深度 scaling up 就是类似于单一多模态任务的纯粹模型参数 scaling up 过程，我们会发现这种 scaling up 下模型能力提升收益放缓，并不是指数级的增长，与之搭配的还需要高质量数据和架构的“Scaling up”，而且盲目扩增模型参数也会对推理 cost 带来极大地负担，所以我们在深度 scaling up 过程中除了模型性能之外更多地会去考虑训练和推理的 cost，期望达到极致的性能 - 效率平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而广度 scaling up 指的是从垂域场景和商业化落地的视角下去看 scaling up，即不同多模态任务之间的 scaling up，我们发现这种广度上的 scaling up 会带来更大的惊喜，例如在联合架构中去实现多模态理解和生成任务的统一，以及视频生成和音频生成任务的统一，衍生出类似音画同步的特色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：MoE 架构为什么会成为 2025 年的主流架构？其在参数效率与推理成本间的平衡能力，是否彻底改变了大模型的开发与部署逻辑？非 MoE 路线的企业如何构建差异化竞争力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;稀疏 MoE 架构的一大优势是较高的推理效率，尽管其模型参数量很大，但在推理过程中只有部分参数被激活，这样既保持了高参数量带来的模型学习能力，也在部署推理过程中表现出较高的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而对于非 MoE 架构，也就是参数稠密型的模型，虽然推理的性价比会比 MoE 架构低，但是对于垂域任务，稠密型模型由于总参数量更小，部署更加灵活，也可以体现出较好的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态大模型的代表性发展&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年多模态能力取得了哪些飞跃性发展？Nano Banana Pro 代表的图片生成模型、OpenAI Sora、Google Veo 3 代表的视频生成模型，分别做了哪些优化得到了不错的效果？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2025 年多模态大模型能力有几个代表性的发展：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;音画同步生成，让视频从默片时代进入了有声时代；主体参考的一致性，实现了从片段化到连贯叙事的转变，AI 漫剧因此迎来了井喷的爆发；运镜表达、表情演绎，让视频生成更具备影视表达，从“形似”到“神似”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Veo 3 就在音画同步上做的很出彩，而 Nano Banana Pro 则将主体参考一致性发挥到新的高度，因为都是闭源模型，所以只能猜测在技术上不会局限于单一的 DiT 架构，例如借助多模态推理和生成的统一（VLLM+DiT）实现更精准的多模态内容编辑，而将更多不同模态的 token（文本、图像、视频、语音等）融入到统一的架构中则能端到端实现类似音画同步的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：多模态能力是否会重塑推理？跨模态推理是否也成为必答题？预计推理能力的突破方向在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2025 年&amp;nbsp;多模态能力已经在重塑大模型推理过程，从 DeepSeek OCR 中使用图片来进行长文本压缩，到 Nano Banana 中直接生成解题过程的图片，多模态能力已经成为大模型推理能力中不可或缺的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多模态数据往往能提供比纯文本数据更稠密、直观和具备逻辑关联的信息。目前多模态数据越来越多的引入，对于大模型结构、训练方法以及数据三方面都会带来新的挑战。其中，大模型结构要尽可能支持原生多模态的输入或者输出，对于模型的参数量上提出了更高的要求；训练方法上需要去平衡各种不同的任务，保证模型在不同任务上都达到一定的收敛程度；数据上则对数据的广度和精度上又有了进一步的要求，广度上需要尽可能涵盖需要的多模态推理任务，同时高质量精品数据可以在训练后期提升推理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：从语言模型到多模态模型，再到世界模型，这个演进的本质是什么？您认为世界模型未来发展趋势如何？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;从语言模型到多模态模型，再到世界模型，演进的本质是“大模型对真实世界的建模能力升级”：语言模型是“理解人类符号”，多模态理解模型是“感知世界表象”，多模态生成模型则是“模拟世界表象”，而世界模型是“掌握物理规律和因果关系并与之交互”，这也是通往 AGI 的必经之路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，世界模型未来必将会在理解物理世界空间结构的同时，提升对物理规律和因果关系的刻画能力，而且通过与物理真实世界的交互实现从感知到决策的闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“低价趋势肯定会延续”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年模型价格战最关键的影响是什么？价格战倒逼厂商做了哪些架构演进？低价趋势在 2026 年是否会继续延续？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;主要还是倒逼模型厂商去持续加速，一是加速研发新模型形成短暂的版本优势，二是加速模型的推理时间，时间就是金钱，三是加速模型架构的升级，引入性价比更高的架构设计来降低成本。低价趋势肯定会延续，因为市场还远没有饱和。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年在 B 端和 C 端，都有哪些创新的商业模式出来吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;创新的商业模式是很难的，所以我觉得更多是一些特色吧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;B 端和 C 端的界限越来越模糊，总体来说都是内容的生成者，真正的海量 C 端其实是内容的消费者，所以可以把两个端一起谈，商业模式的创新就是从售卖 API 提升到了售卖结果，以前 B、C 两端都是积分制，本质就是价值折算的积分，但是我们在不断探索按照结果来付费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在移动端，我们也在突破过去 web 端复杂的积分逻辑对应的不同的会员等级，pro、ultra 等等，我们只会把功能区分为会员功能和非会员功能，然后按需充值即可，不会再纠结额度来觉得是否续费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在您看来，2026 年大模型竞赛的核心是什么？您认为下一次“大模型代际飞跃”可能来自哪条技术路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：2026 年&amp;nbsp;大模型竞赛的核心，会从“技术能力”转向“价值落地能力”，类似于比拼“行业收入规模”和“客户留存率”。谁能更快将技术转化为行业实效，谁就能占据先机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一次“代际飞跃”很可能来自两个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一是新颖的用户交互体验，随着基础原子能力目前逐渐饱和，2025 年 Agent 相关的应用出现了爆发式的增长，而 Agent 爆发的背后实际上代表了用户在认可大模型能力的同时又对于 AI 应用的交互体验提出了更高的要求，让大模型从单一的原子能力向完整解决方案提供者演变，一旦在用户交互方式、交互体验上跨越式提升，就会带来新的机遇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;二是专业级能力的大众化，目前大模型能力对于专业从业者来说已经达到一个很惊艳的程度，但是对于大众来说还是存在一些使用上的“困难”，这种困难可能来自于高昂的推理成本，编写专业级 prompt 的入门难度，以及缺乏大模型使用经验以及思维，而下一次飞跃可能就来自于如何拉近大模型对于大众的隔阂，出现真正的全民级 AI 应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;&amp;nbsp;“模型和商业化一直会是两个最大挑战”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：根据您的观察，科技公司 2025 年面临的压力如何？对此采取了什么样的应对措施？员工们的状态如何？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;对我们这样的模型研发的公司来说，模型和商业化一直会是两个最大的挑战，这两个挑战汇集在一起就是对于底层模型架构的突破变成必选项，模型公司不能像过去那样不断的优化数据和推理来解决用户的问题，而是要在架构上做出突破，敢为人先。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非常开心的是我们的员工状态始终保持战斗状态，因为我们不要 80 -&amp;gt;85，而是要 120 分的创新和颠覆，同时模型团队也和业务团队有了更多的协同，这种协同对于模型团队的能力落地起到非常重要的作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：经过一年竞赛，国内前沿 AI 水平取得了怎样的成绩？是否赶上了硅谷科技公司？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;在多模态大模型这个赛道，我觉得国内外是百花齐放，例如我们在 2025 年 4 月的图像模型 HiDream-I1 开源打响了国内多模态生成式大模型登顶国际竞技场的第一枪，同时大家也开始重视了多模态生成式大模型的竞技场，这些过去只有硅谷科技公司的模型名单里开始快速出现国内的各家模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您认为，2026 年的技术赛点可能是什么？您会重点关注哪些行业和技术？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚霆：&amp;nbsp;技术赛点从多模态模型架构上来说我觉得还有比较长的路，但是在应用上我觉得技术的赛点是多模态 agent 的成熟落地。2025 年上半年的 Manus，下半年持续火热的 vibe &amp;nbsp;coding 都是大语言模型的应用落地的典型案例，多模态模型看似比大语言模型更解决用户，但是生图生视频场景还没有出现真正技术应用上完全解决用户痛点的 agent，所以我们也会更关注多模态 agent 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fijwq6waNUKJhmbUGMqc</link><guid isPermaLink="false">https://www.infoq.cn/article/fijwq6waNUKJhmbUGMqc</guid><pubDate>Tue, 20 Jan 2026 07:18:07 GMT</pubDate><author>蔡芳芳,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>产业级 Agent 如何破局？百度吴健民：通用模型难“通吃”，垂直场景才是出路</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文为《2025 年度盘点与趋势洞察》系列内容之一，由 InfoQ 技术编辑组策划。本系列覆盖大模型、Agent、具身智能、AI Native 开发范式、AI 工具链与开发、AI+ 传统行业等方向，通过长期跟踪、与业内专家深度访谈等方式，对重点领域进行关键技术进展、核心事件和产业趋势的洞察盘点。内容将在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。我们采访了百度智能云平台产品事业部算法架构师、千帆策略部负责人吴健民，他指出，Agentic 模型训练最大卡点不是模型，是真实环境复刻，外部接口、数据库、登录依赖等真实链路的稳定访问，技术实现门槛极高。在当前，通用全能的 Agentic 模型现阶段不可能实现，业务场景、工具、环境差异过大，通用模型泛化性有限，针对垂直场景的模型定制和持续学习或是破局关键。在多模态模型发展方面，吴健民指出，视觉生成主流为 模型框架从 Diffusion Model 发展到 Flow Matching，效果、稳定性碾压前代方案，视觉理解模型仍以 ViT Encoder 嫁接语言模型的主流方案，模型能力迭代的主要聚焦在垂直方向的数据合成。虽然工业和学术界有很多尝试，当前未真正实现多模态理解和生成的统一建模，目前分开独立优化效果依旧优于融合建模。下面是详细对话内容，以飨读者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“没有模型可以支持所有 Agent 场景”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：如何让大模型更好支持 Agent 应用？技术有哪些瓶颈？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：目前我们的研发目标，是让模型能够在各类 垂直 Agent 场景中更好地发挥作用。其中，最核心、发展也最快的场景是 Coding Agent，包括通用编程以及面向网页开发或特定垂直领域的 Agent 应用。现阶段，我们的工作重点之一就是更具体地聚焦在网页开发相关的 Agent 能力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，有一个重要的问题需要回答：SOTA 的通用模型是否能在各种垂直 Agent 场景下都能达到工业级的效果。就目前来看，具备这种能力的通用模型还没有出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因在于，不同 垂直 Agent 所处的场景设定、可使用的工具集合以及运行环境差异极大，而当前的通用模型尚不足以在如此多样的场景中实现稳定泛化。因此，围绕具体应用场景定制模型，反而更容易形成优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，不同场景对效果的评估标准也存在显著差异，即 Reward 的定义并不通用。如果一个场景能够清晰地定义 Reward，并且该 Reward 判断能够高效自动地完成，那么针对这一场景通过强化学习在通用基座模型上定制训练的 Agentic 模型，往往可以显著超过现有通用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个难点在于环境的复杂性。以代码场景为例，其运行环境不仅涉及代码本身，还包括外部接口调用、工具使用、数据库依赖，以及登录、扫码等一系列真实应用中的外部依赖。在训练过程中，这些依赖都必须能够被高并发、稳定地访问，这对技术实现提出了很高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个挑战在于强化学习系统本身。当前业内已形成共识，即要实现模型在特定场景中的持续迭代，必须依赖一套在该场景下运行顺畅、具备高效率和高吞吐能力的强化学习系统。﻿由于强化学习系统本身的架构复杂性，也出现了不少 RLaaS 的平台产品，把算法复杂性封装在平台内，业务仅需要聚焦在业务场景定义，Reward 评估方案制定和迭代。这也是百度千帆平台 26 年的重点业务方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：那现在有没有比较通用、效果较好的强化学习框架？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：目前开源社区中已有不少强化学习框架，例如 OpenRLHF、TRL 以及 VeRL 等，它们基本覆盖了强化学习流程中的主要环节。但在工业级应用中，这些框架仍然不够成熟，﻿特别是涉及多轮工具调用的 Agentic 场景，往往需要进行深度定制和打磨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;打磨方向主要在两个方面：首先是模型规模支持，严肃应用往往依赖参数量较大的 SOTA 模型，例如百度文心或 DeepSeek 开源的模型，强化框架能否高效支撑这类大模型至关重要；其次是 Agent 训练能力，早期的强化学习多集中于单步任务，例如数学推理，而代码类、客服、DeepReasearch 等 Agent 更依赖多轮工具调用的复杂交互，这就要求强化训练框架能够配合一整套稳定、高效的脚手架系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，工业级 Agentic 模型的 研发对整体技术栈的要求极高，包括沙盒环境以及高性能、高并发的调度运行能力；若涉及联网搜索，还需要稳定的高并发搜索 API 支持。因此，具备云计算或搜索基础能力的团队往往更具优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：要在基座模型上增强 Agentic 能力，需要哪些技术支持？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这一问题的核心仍然在于强化学习如何在基座模型之上更好地服务于具体场景。强化训练的本质并不是创造全新的能力，而是激发和稳定模型在特定场景中的既有能力。因此，首要前提是基座模型本身在目标场景上具备优势。这种优势通常来源于预训练阶段的数据分布。例如，搜索相关数据占比更高的模型，在代码类 Agent 场景中往往更具潜力，不同场景基座模型的选择，通常观察基座模型在对应场景的 Pass@k 指标，即推理多次能得到正确答案的比例。Pass@k 指标高的模型，有更大空间通过强化学习训练激发并稳定模型在对应场景的表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个关键依赖是训练效率。强化学习的过程本质上更接近一种搜索机制：模型通过大量尝试生成不同路径，Reward 对每次尝试进行优劣评估，并将表现较好的路径通过强化训练反馈到模型参数中。在这一过程中，生成尝试路径（Rollout）通常占据 80%—90% 的时间成本。因此，是否能够以高吞吐方式高效完成 Rollout，是强化训练成败的关键。这个过程的关键是“训推一体”的技术，实现训推计算资源的高效利用以及训练精度差异的对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：另外，现在强化学习的 scaling 在业内似乎未形成共识？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：的确不像预训练 scaling 一样普遍的共识。过去，强化训练通常只占总体训练很小的一部分，被视为对预训练模型的微调，给预训练模型的蛋糕上放一个樱桃。而现在，强化训练的样本规模已经可以扩展到百万级，系统性地提升了模型推理和复杂问题解决能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要实现大规模多场景的强化训练，前提是结果评估能够准确自动完成，且最好能有稠密的评估奖励反馈。在代码或数学等评估相对确定的场景中，这一点相对容易实现，模型在代码和数学解题方向能力也得到显著提升。但在通用问答或复杂垂直场景中，由于缺乏统一、自动化的评估方案，规模扩展变得困难。这也是模型尚未在更通用场景实现泛化的重要原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管如此，业内普遍认为强化训练依然具有显著的 scaling 效果，﻿问题的焦点转化到可泛化到评估奖励方案设计上。从依赖人工反馈的小规模 RHF，到基于规则甚至更通用奖励方案的 RLVR 强化训练，随着规模扩大，模型效果确实在持续提升，这一点在实际应用中也得到了验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：通用 Agent 与专用 Agent 之间的能力差距，该如何弥补？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：当前主要存在两种思路。一种是追求在所有方向上都表现出超过人类的全能模型或 Agent，这本质上指向 AGI。业内对实现 AGI 需要的时间判断差异很大，而我们认为这一目标仍然相当遥远。另一种更现实的路径，是在特定专业场景中不断提升模型和 Agent 能力，能够在局部任务上超过人类水平，这在相当长一段时间内仍将是主流方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们负责研发的全球领先的可商用自我演化超级智能体百度伐谋，为可以准确定义评估验证方案的 NP-hard 问题，提供高效的最优解演化方案，实现超过人类水平的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：长上下文能力对 Agent 的支持非常重要，应当如何建设？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：模型支持的上下文长度与 Agent 能力之间存在直接关系。上下文决定了模型能够记忆和理解的信息规模，而在复杂任务中，Agent 需要不断与环境交互，每一次反馈都会进入上下文，成为下一步决策的依据。因此，交互轮次越多，对模型长上下文理解能力的要求就越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，业界也在探索通过 Agent 脚手架本身“放大记忆”的方案。类似人类并不会记住所有信息，而是通过笔记、字典或工具进行辅助，Agent 也可以通过工具使用来弥补上下文长度的限制。例如，在审核数百页合同的场景中，即便无法一次性将全文放入上下文，Agent 仍可以借助工具调用逐页查看、回溯关联内容，从而完成整体审核任务。从这个角度看，通过工具增强记忆能力，也是实现长上下文处理的一种有效路径，体现了 Agent 开发中 Progressive Disclosure 的原则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在一些偏注意力机制的底层架构方面，业内是否做了调整？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这个涉及模型网络结构本身的问题了。无论通过何种工具把上下文扩展得更长，模型本身的上下文理解能力始终存在上限。比如目前常见的 128K 或 256K 甚至 1M 上下文，长上下文能力的关键是模型能否准确理解高效处理，这依赖高效的注意力机制设计和实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型利用上下文，在生成下一个 token 时，一个重要的观察是：并非全部上文 token 都对预估当前 token 同等重要，真正起作用的往往只是其中一小部分。基于这一特性，注意力机制可以采用稀疏化策略，不必对全部 128K 的 token 做同等精细的计算，可以采用比如 DeepSeek DSA 方案，先租略进行一次快速扫描，再对相关性高的部分 token 进行精细注意力计算。另一个思路是把上文 token 进行分块，先筛选相关的块，再对相关块内 token 进行精细注意力计算。结合两个方案的优势，也是一个实现的思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年 MoE 架构被广泛采用，是否意味着更强模型的整体方向已经基本确定？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：MoE 架构被广泛应用到搜索、推荐等不同预估场景。大模型提到的 MoE，实际上是稀疏 MoE。其实从去年年初开始，这项技术就在业内受到较多关注。它要解决的核心问题仍然是 Scaling Law：随着模型参数规模不断增大，训练和推理成本也在持续上升，是否能在保持参数规模扩展的同时，控制实际训推计算的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MoE 给出的答案是肯定的。通过这种方式，可以在继续增大模型总参数的同时，让训练和推理所实际使用的参数规模保持次线性增长。具体而言，在 Transformer 架构中，MoE 将原本的全连接层拆分为多个对等的小模块，即“专家”，在每次前向推理只激活其中一部分，从而显著降低计算成本。稀疏 MoE 已逐渐成为业内的主流选择，稀疏比耶做到了 5% 甚至更低的水平，成为推动模型规模继续扩展的一种现实可行方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;多模态模型架构层逐渐收敛&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：从单一模态发展到多模态并引入 Agent，在底层架构上发生了哪些变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：一个最显著的变化，是在原有语言模型基础上引入视觉能力，这也是从去年开始 VLM 大量出现的主要方向。实际工作中，核心仍然在语言模型本身：通常是在语言模型训练到一定阶段后，引入视觉编码器，并用图文对其数据与语言模型联合训练，对齐文本和视觉 token，使模型能够理解视觉信号。这种 “桥接”或“嫁接”的方案，逐步成为当前的主流方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在多模态领域，一个长期目标是希望视觉模型也能像语言模型一样有很好的﻿Scaling Law，但这一问题至今仍未解决。视觉信号本身的信息密度比较低，它更像是自然世界的直接映射，并不一定承载明确的知识结构。﻿相比而言，互联网上存在的海量文本数据，是人类产生的对世界知识的总结压缩，信息密度很高。这使得仅依赖视觉输入进行大规模训练，难以达到语言模型那样的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，现有方案高度依赖图文对齐数据，即为图片配备高质量、细粒度的文本描述，通过充分对齐文本与图片，来提升模型的理解能力。但这类数据难以规模化获取，不易全面覆盖实际的图片分布，目前行业可用的规模大致在 3–5T token，量级上存在明显差距，也限制了多模态模型的进一步 scale。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年文生图、图生图模型更新频繁，突破点主要在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：这属于视觉生成方向。从 Sora 开始，这一领域受到了广泛关注，也出现了不少高质量的开源项目，支持生成效果不断提升。但像 Sora 2 或 Nano Banan 等业内 SOTA 的生成模型，其具体实现细节并未完全公开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从算法角度看，视觉生成方案本身仍在快速演进，从早期的 Stable Diffusion 到当前的 Flow Matching，建模方法和训练效率都得到了显著优化。不过，从能力定位上看，视觉生成模型更偏向专精模型，主要解决“生成”的问题，也有观点认为，生成模型可能进一步发展为所谓的“世界模型”，即在理解物理规律的基础上生成符合现实约束的内容，进而通向 AGI 的实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2026 方向：生成与理解的统一建模&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在此基础上，未来一段时间，尤其是 2026 年，大家主要会沿着哪些方向继续演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：一个非常重要的方向，多模态生成与理解的统一建模。很多公司都在尝试通过统一的多模态建模方式，让生成能力和理解能力形成协同效应，而不再是彼此割裂。这意味着模型既不是单纯为生成而设计，也不是只服务于理解任务。外界对 GPT-5 等模型也曾寄予类似期待，尽管目前看相关路径尚未完全跑通，但可以确定的是，这一方向仍在持续探索之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：在专家视角下，生成与理解真正实现统一，应当达到什么样的效果？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：最终评价标准仍然是结果导向。如果通过统一训练得到的模型，在生成和理解两个维度上的表现，都优于分别独立训练的模型，那么这种统一才是有意义的。举例来说，如果一个生成 - 理解统一模型在生成质量上能够超过当前生成领域的 SOTA 模型，那么就可以认为内生的理解能力确实提升了生成效果。但就目前来看，分开针对生成和理解进行优化，独立效果仍然更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：也就是说，目前融合后的效果还不如单独优化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：是的，至少在现阶段仍是如此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：但很多团队似乎还是在把各种能力揉合进一个模型里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：确实存在这种趋势，但并非所有团队都选择同一条路径。不同团队对通用人工智能实现方式的理解并不一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一种思路是将多种能力融合到单一模型中，希望模型像人一样具备听、说、读、写等多种模态能力，这是一种全模态模型的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一种思路则是强调模型学会使用工具。人类智能的显著提升，本质上源于工具使用能力的不断演进，从最原始的简单工具到今天的计算机系统，工具极大放大了人的能力。Agent 的发展，本质上正是沿着“工具使用”这一路径展开的，不同理解会带来不同的技术路线和实现方式，当前没有看到哪条路一定能走通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：2025 年“世界模型”这个概念被频繁提及，从语言模型到动态模型再到世界模型，这条演进逻辑是怎样的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：“世界模型”这一说法本身就存在多种理解。最早在 Sora 第一代发布时，其自称为世界模型，核心目标是通过建模来理解物理世界的运行规律，尤其是借助视觉输入，让模型学习空间关系和物理约束，例如生成的视频必须符合基本物理常识。这一路线随后发展得很快，重点在于提升模型的空间感知推理和物理一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但也存在另一种理解路径。例如 Meta 前段时间发布的 CWM 模型，强调的是代码能力和工具调用能力，同样定义为世界模型。在这种视角下，只要模型能够高效使用现实世界中的各种工具，就可以被视为对“世界”的一种建模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Agentic 模型是今年必答题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：展望明年，大模型能力提升的核心突破点可能来自哪些技术路线？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：明年的变化大概率会延续 2025 年已经显现的趋势。2025 年一个非常明显的方向是 Agentic Model，即模型具备稳定、准确的工具调用能力。代码场景已经率先验证了这一点，明年这一能力很可能扩展到更多应用场景，模型将不再只调用编程相关工具，而是能够使用更广泛的现实世界 API，这是一个较为明确的发展趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：那面对复杂环境，大模型将如何应对？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴健民：通用场景的环境通常非常复杂，模型需要对接的 API 接口、数据库、人际交互界面等系统差异较大。针对后者，目前较为可行的方案，仍然是让模型在特定场景的 Agent 脚手架中学会熟练使用该场景所涉及的工具。尽管应用场景很多，但每个场景对应的工具集合通常是相对有限的。模型通过场景反馈不断优化工具使用方式，就可以逐步适应复杂环境。代码 Agent 场景正是一个典型例子，模型通常只需要掌握十几种工具调用方式，随着打磨程度提升，其在该场景下的表现也会持续改善。&lt;/p&gt;</description><link>https://www.infoq.cn/article/RFVtzWIOoQhubp6cz0yK</link><guid isPermaLink="false">https://www.infoq.cn/article/RFVtzWIOoQhubp6cz0yK</guid><pubDate>Tue, 20 Jan 2026 07:10:19 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Mistral发布OCR 3，提升了手写及结构化文档识别的准确率</title><description>&lt;p&gt;Mistral近日发布了其光学字符识别（optical character recognition，OCR）模型的最新版本，&lt;a href=&quot;https://mistral.ai/news/mistral-ocr-3&quot;&gt;Mistral OCR 3&lt;/a&gt;&quot;，该版本专注于在多种文档类型上实现更高的精度，包括手写笔记、表单、低质量扫描件以及结构复杂的表格。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据Mistral表示，OCR 3相较于前一代产品是一次重大的飞跃。在基于真实客户文档工作流的内部评估中，新模型在整体表现上以74%的胜率超越了Mistral OCR 2，尤其在表单、手写内容和含大量表格的文档上优势更为显著。这些基准测试采用模糊匹配（fuzzy-match）指标与人工标注的真实结果进行比对，旨在反映实际业务场景，而非理想化的合成数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba88c8ec331199980f9e1cbdf0583891.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：Mistral博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度看，Mistral OCR 3不仅能够提取文本，还能识别并保留嵌入的图像，同时完整保留原始文档的结构信息。它的输出格式为Markdown，其中表格通过HTML标签（如rowspan和colspan）重建，使下游系统不仅能获取纯文本，还能保留布局语义。这一特性使其非常适合需要结构化JSON、可搜索档案的管道，或集成到智能体（agentic）和检索增强系统（RAG）中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在以往通常依赖人工复核的场景中，该模型也展现出显著的进步。它能够高效处理手写内容，包括连笔的笔记和批注。在表单解析方面，对标签、复选框及混合输入项的识别更加准确。此外，OCR 3对扫描档案中常见的倾斜、压缩伪影、低分辨率以及背景噪点等问题具备更强的健壮性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期用户特别强调了其性能提升和多语言方面的支持能力。ICT安全负责人兼AI安全专家Patrick Jacobs&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407434928260390914?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407434928260390914%2C7407449619036524544%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287407449619036524544%2Curn%3Ali%3Aactivity%3A7407434928260390914%29&quot;&gt;评论&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在速度方面，真的令人印象深刻，而且它处理荷兰语毫无压力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;得益于准确率的大幅提升，Mistral OCR 3的生产部署正在快速扩展。Techseria创始人兼首席顾问Niraj Bhatt&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407434928260390914?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407434928260390914%2C7407454745524678656%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287407454745524678656%2Curn%3Ali%3Aactivity%3A7407434928260390914%29&quot;&gt;分享了&lt;/a&gt;&quot;其实际应用的变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们一直在生产环境中使用Mistral OCR处理销售和采购发票，实现ERP系统的零人工数据录入。现在v3在表单和手写内容上准确率提升了74%，终于让我们能够将覆盖范围扩展到送货单、水电账单以及过去只能靠人工处理的遗留档案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在定价方面，Mistral OCR 3的标准费率为每1000页2美元；若使用Batch API，成本可降至每1000页1美元，使其成为许多企业级OCR系统的高性价比替代方案。开发者可通过API直接集成模型（标识符为mistral-ocr-2512），非技术用户则可通过拖放式的Document AI Playground界面轻松使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于具有严格数据治理要求的组织，Mistral提供了私有化的部署选项，确保OCR工作负载完全运行在客户可控的基础设施内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，Mistral OCR 3已经可以使用了，并完全向后兼容OCR 2。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/mistral-ocr3/&quot;&gt;Mistral Releases OCR 3 With Improved Accuracy on Handwritten and Structured Documents&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/DX8LLPkyH06INOoAjxxa</link><guid isPermaLink="false">https://www.infoq.cn/article/DX8LLPkyH06INOoAjxxa</guid><pubDate>Tue, 20 Jan 2026 07:06:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item></channel></rss>