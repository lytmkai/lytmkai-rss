<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 11 Feb 2026 08:06:24 GMT</lastBuildDate><ttl>5</ttl><item><title>谷歌推动模型上下文协议支持gRPC</title><description>&lt;p&gt;谷歌云&lt;a href=&quot;https://cloud.google.com/blog/products/networking/grpc-as-a-native-transport-for-mcp&quot;&gt;宣布&lt;/a&gt;&quot;将为模型上下文协议（Model Context Protocol，MCP）贡献一个gRPC传输包，填补那些在微服务中全面标准化使用gRPC的企业所面临的关键空白。MCP是&lt;a href=&quot;https://www.anthropic.com/news/model-context-protocol&quot;&gt;Anthropic推出的协议&lt;/a&gt;&quot;，用于实现AI智能体与外部工具和数据的集成，目前在企业环境中获得了广泛关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，MCP默认使用&lt;a href=&quot;https://www.jsonrpc.org/historical/json-rpc-over-http.html&quot;&gt;基于HTTP的JSON-RPC&lt;/a&gt;&quot;作为传输层。这在处理自然语言负载时表现良好，但对于已全面采用gRPC的开发者而言，却带来了极大的不便。其他可选方案包括，重写服务以适配MCP的JSON传输、搭建转码代理，或并行维护两套独立实现，但是这些方案均不理想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Spotify已经亲身体验过这种痛苦。该公司的高级员工工程师兼开发者体验技术负责人Stefan Särne在谷歌的&lt;a href=&quot;https://cloud.google.com/blog/products/networking/grpc-as-a-native-transport-for-mcp&quot;&gt;博客文章&lt;/a&gt;&quot;中表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;由于gRPC是我们后端的标准协议，我们已在内部为基于gRPC的MCP提供了实验性支持，并且我们已经看到了其优势：对开发者而言非常易用且熟悉，同时通过利用结构化和静态类型的API，减少了构建MCP服务器所需的工作量。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一举措也得到了社区的支持。至少从2025年4月起，开发者就开始呼吁，在&lt;a href=&quot;https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/1144&quot;&gt;GitHub的一次讨论（#1144）&lt;/a&gt;&quot;中，从业者们主张MCP从一开始就应该围绕gRPC构建，部分开发者在此期间已推出了自己基于gRPC的MCP服务器。2025年7月的一个&lt;a href=&quot;https://github.com/modelcontextprotocol/modelcontextprotocol/issues/966&quot;&gt;GitHub 问题（#966）&lt;/a&gt;&quot;获得了43个赞，开发者们指出，基于HTTP的JSON传输存在JSON序列化带来的高开销、资源监听时低效的长轮询，以及API契约缺乏类型安全性等问题。MCP维护者此后已经同意在SDK中支持可插拔得传输层，而谷歌计划自行贡献并分发gRPC传输包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过在底层使用&lt;a href=&quot;https://protobuf.dev/&quot;&gt;Protocol Buffers&lt;/a&gt;&quot;替换JSON，可以显著降低网络带宽和CPU开销。对于已部署gRPC基础设施的企业而言，这意味着AI智能体可以直接与现有服务通信，无需额外添加转换层。Protocol Buffers的结构化、类型化契约也与大多数后端服务的定义方式更为契合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但是，该提案并未完全解决一个现实的矛盾。在&lt;a href=&quot;https://www.aifire.co/p/mcp-vs-grpc-the-future-of-ai-native-agent-connectivity&quot;&gt;Medium上&lt;/a&gt;&quot;，有一篇对比MCP与gRPC的分析文章指出：“gRPC的服务反射提供了结构信息（方法名、参数），但缺乏LLM所需的语义化、自然语言描述（也就是‘何时’和‘为何’）。”MCP 从设计之初就是为了向AI智能体提供这类上下文，即工具描述、资源说明、提示词指导，而gRPC本身并不具备这一能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，更大的架构问题依然存在：MCP是应该适配gRPC这类现有的RPC系统，还是这些系统需要学习MCP的语言？从业者们对此意见不一。一些人认为，强制将运行良好的gRPC服务重写为JSON-RPC是完全不必要的麻烦。另一些人则认为，不能简单地将gRPC强加于一个以AI为中心的协议之上，而不添加LLM实际运行所需的语义层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于将AI智能体投入生产环境的开发者而言，实际优势显而易见。那些已深度使用gRPC的企业（包括谷歌自身），它们“在全球范围内依赖gRPC来启用服务和提供API”，现在均可以直接采用MCP，而无需破坏现有的服务契约了。谷歌还为其自有服务推出了具有全球一致性端点的全托管远程MCP服务器，结合gRPC支持，使谷歌云能够直接面向那些已投资gRPC、希望添加AI智能体能力的企业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;gRPC传输层仍在开发中。谷歌正通过Python SDK中一个关于可插拔传输接口的活跃&lt;a href=&quot;https://github.com/modelcontextprotocol/python-sdk/pull/1591&quot;&gt;pull request&lt;/a&gt;&quot;，与MCP社区合作推进。如果开发者关注该领域的话，MCP的GitHub仓库和&lt;a href=&quot;https://modelcontextprotocol.io/community/communication&quot;&gt;贡献者频道&lt;/a&gt;&quot;是了解最新进展的主要渠道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/google-grpc-mcp-transport/&quot;&gt;&amp;nbsp;Google Pushes for gRPC Support in Model Context Protocol&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/IvpIeymHIWETqu4X7qo7</link><guid isPermaLink="false">https://www.infoq.cn/article/IvpIeymHIWETqu4X7qo7</guid><pubDate>Wed, 11 Feb 2026 08:00:00 GMT</pubDate><author>作者：Steef-Jan Wiggers</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>Datadog 在其 LLM 可观测性工具中集成 Google ADK</title><description>&lt;p&gt;Datadog 近期&lt;a href=&quot;https://cloud.google.com/blog/products/management-tools/datadog-integrates-agent-development-kit-or-adk/&quot;&gt;宣布&lt;/a&gt;&quot;，其 LLM 可观测性平台已为使用 &lt;a href=&quot;https://google.github.io/adk-docs/&quot;&gt;Google Agent Development Kit (ADK)&lt;/a&gt;&quot; 构建的应用程序提供自动埋点功能，帮助用户更深入地洞察 AI 驱动型智能体系统的行为、性能、成本及安全性。该集成在 &lt;a href=&quot;https://cloud.google.com/blog/products/management-tools/datadog-integrates-agent-development-kit-or-adk/&quot;&gt;Google Cloud 博客&lt;/a&gt;&quot;上进行了重点介绍，旨在让开发者和 SRE 团队无需繁琐的手动配置或自定义埋点即可轻松监控和排查复杂的多步骤 AI 智能体工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着企业越来越多地采用 ADK 等框架构建自主 AI 智能体，这些系统的非确定性特质使得预测输出、诊断故障和控制成本变得困难。Datadog 的新集成将 ADK 应用的信号接入其可观测性系统，使团队能够可视化智能体决策路径、追踪工具调用、测量令牌使用量和延迟，并标记出可能导致性能下降或 API 成本激增的意外循环和错误路由步骤。Datadog 通过将这些遥测数据与其他系统指标关联，帮助团队提升智能体的可靠性和运营信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该集成还填补了智能体部署中的一个空白：虽然 ADK 为跨场景构建 AI 智能体提供了灵活的框架，但其本身缺乏针对生产环境的监控和治理工具。Datadog 的埋点功能通过自动追踪每个智能体的操作并将其呈现在统一的时间线上，填补了这一空白，使团队能够轻松定位工具选择错误或低效重试循环等问题，从而避免因这些问题导致延迟增加或令牌开销上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.datadoghq.com/product/llm-observability/&quot;&gt;Datadog 的 LLM 可观测性&lt;/a&gt;&quot;平台现在支持查看每个工具和工作流分支的令牌消耗及延迟情况，帮助识别智能体的异常行为和成本超支风险。这在企业环境中尤为重要，因为复杂的智能体编排往往涉及多模型、多工作流及外部系统集成，而传统应用性能监控难以应对以 AI 为核心的业务逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这一集成，Datadog 将其可观测性平台（已覆盖基础设施、安全和分布式系统）拓展至新兴的智能体 AI 应用领域，弥合了 AI 实验与稳定生产部署之间的鸿沟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其他可观测性厂商也在开发类似的集成功能，帮助企业更好地理解和使用 LLM：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://newrelic.com/&quot;&gt;New Relic&lt;/a&gt;&quot; 提供全栈可观测性和 APM，具备强大的分布式追踪和性能洞察能力，正通过扩展遥测关联和 AI 感知监控功能向 AI 可观测性演进。虽然它尚未拥有与 Datadog ADK 集成相同水平的专用 LLM 工具，但它为应用和基础设施提供了坚实的端到端可见性，帮助团队理解 AI 和智能体工作负载如何与系统的其他部分交互。New Relic 采用基于数据摄取量而非主机数量的定价模式，对关注成本的团队而言更具可预测性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Splunk 的可观测性产品（包括 &lt;a href=&quot;https://www.splunk.com/en_us/products/observability-cloud.html&quot;&gt;Splunk Observability Cloud&lt;/a&gt;&quot;）擅长高容量日志摄取和查询，在跨各类数据集的详细取证分析方面表现突出。然而，与 Datadog 深度集成的智能体可观测性特性相比，开箱即用地关联 AI 特定信号（如令牌消耗或模型决策路径）可能需要更多配置工作。Splunk 在处理大规模非结构化遥测和以安全为中心的监控方面表现依然强劲，但在没有自定义埋点或插件的情况下，其内置的 AI/智能体工作流功能可能相对滞后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕 AI 和智能体可观测性的新兴需求正推动各厂商持续升级其工具，聚焦运行时追踪、序列与路径可视化，以及 AI 工作负载的成本和延迟洞察，但各厂商均基于自身核心优势采取了差异化策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/datadog-google-llm-observability/&quot;&gt;https://www.infoq.com/news/2026/02/datadog-google-llm-observability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ybR4DQTz6udDxBmKZpOi</link><guid isPermaLink="false">https://www.infoq.cn/article/ybR4DQTz6udDxBmKZpOi</guid><pubDate>Wed, 11 Feb 2026 07:00:00 GMT</pubDate><author>作者：Craig Risi</author><category>AI&amp;大模型</category></item><item><title>微软发布OData .NET（ODL）9.0.0预览版3：安全性、现代化API及规范遵从性</title><description>&lt;p&gt;微软&lt;a href=&quot;https://devblogs.microsoft.com/odata/announcing-odata-net-odl-9-preview-3-release/&quot;&gt;发布&lt;/a&gt;&quot;了OData .NET（ODL）9.0.0预览版3（这是OData .NET客户端和核心库的最新预览版本），延续了该库的现代化进程。这个预览版聚焦于更安全的默认行为、运行时API清理以及OData规范遵从性提升。OData .NET团队正朝着9.x的稳定版本努力推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OData .NET核心库（如Microsoft.OData.Core）当前的稳定版本仍然是NuGet上的&lt;a href=&quot;https://github.com/OData/odata.net/releases/tag/8.4.3&quot;&gt;8.4.x系列版本&lt;/a&gt;&quot;，其中，8.4.3是该系列的最新稳定版本。该稳定分支支持OData v4/v4.01，并且广泛应用于生产环境，而&lt;a href=&quot;https://github.com/OData/odata.net/releases/tag/9.0.0-preview.3&quot;&gt;9.x版本仍在预览当中&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版3延续了9.x早期预览版的约定，但根据开发者的反馈以及OData规范进行了以下几个方面的增强：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;动作查询执行现在使用SingleOrDefault()语义处理可空引用，在保留对非空值的严格检查的同时，减少了由常见的空响应所引发的意料之外的异常。移除了与ISerializable相关的旧序列化构造函数，消除了现代SDK上的构建警告。放弃了旧的CsdlTarget概念，并弃用了过时的返回类型访问器，转而支持更新的EDM接口。与IEdmOperation接口返回类型属性（ReturnType）相关的过时API也已被新的IEdmOperationReturn抽象完全替换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些变化反映了这样一种发展方向：与.NET 8/9/10运行时保持兼容、内存占用更低的分配模式（如添加ReadOnlySpan&lt;char&gt;查找重载）以及对平台内置API的依赖。&lt;/char&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版3的一个关键行为变化是强制对非类型化值进行结构化类型反序列化（不再有ReadUntypedAsString切换），使运行时行为更接近&lt;a href=&quot;https://www.odata.org/documentation/&quot;&gt;官方的OData JSON格式&lt;/a&gt;&quot;。此外，未指定类型的数值现在默认推断为特定的CLR数值类型，并提供兼容性标志以支持旧版结果（即解析为decimal的数值）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从稳定的8.x系列版本升级到9.x预览版的NuGet包应被视为破坏性变更：开发者需要检查可空返回值处理、预期的非类型化JSON shapes以及对已移除的旧API的依赖。由于9.x版本仍处于预览阶段，不建议在没有仔细测试的情况下用于生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OData生态系统继续向前发展。举例来说，ASP.NET Core OData包独自进入了自己的9.x+系列（包括像&lt;a href=&quot;https://www.nuget.org/packages/Microsoft.AspNetCore.OData/9.4.1&quot;&gt;Microsoft.AspNetCore.OData 9.4.x&lt;/a&gt;&quot;这样的稳定版本），这表明服务端和客户端OData技术栈的相关工作正在并行推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有兴趣提供反馈或跟踪稳定化计划的开发者，可以关注&lt;a href=&quot;https://www.infoq.com/news/2026/01/odata-net-preview-9/odata.net&quot;&gt;OData/odata.net GitHub存储库&lt;/a&gt;&quot;和&lt;a href=&quot;https://devblogs.microsoft.com/odata/&quot;&gt;OData官方博客&lt;/a&gt;&quot;，获取预览公告、迁移指南和9.0最终稳定版的路线图动态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/odata-net-preview-9/&quot;&gt;https://www.infoq.com/news/2026/01/odata-net-preview-9/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/f0ev3d8fZtw2DaIdp85S</link><guid isPermaLink="false">https://www.infoq.cn/article/f0ev3d8fZtw2DaIdp85S</guid><pubDate>Wed, 11 Feb 2026 02:36:54 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>后端</category></item><item><title>OpenEverest：开源数据库自动化平台</title><description>&lt;p&gt;近日，Percona宣布推出&lt;a href=&quot;https://openeverest.io/&quot;&gt;OpenEverest&lt;/a&gt;&quot;，这是一个支持多种数据库技术的开源平台，用于自动化数据库配置和管理。该平台最初发布时名为Percona Everest，可以托管在任何Kubernetes基础设施上，既可以是云端也可以是本地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目的主要目标是避免供应商锁定，同时提供自动化的私有DBaaS。它基于Kubernetes operator构建，旨在避免依赖单一云供应商技术的复杂部署。OpenEverest是模块化的，允许开发人员和数据库管理员组合不同的数据库、存储系统和部署方法以满足特定的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aac5d2f8395d99b4d773040c671752c7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;作为一个插件系统，其核心功能支持GKE Autopilot和Pod调度策略等特性。OpenEverest维护者、Solarica创始人&lt;a href=&quot;https://www.linkedin.com/in/sergeypronin/&quot;&gt;Sergey Pronin&lt;/a&gt;&quot;&lt;a href=&quot;https://openeverest.io/blog/welcome-to-everest/&quot;&gt;解释&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;目前，我们专注于数据库管理，但我们真正的愿景远不止于此。我们正在构建一个模块化的基础架构，让你可以无缝地集成更多的数据引擎，连接整个运维体系，从而应对更广泛的数据基础设施挑战。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目通过其Web UI和&lt;a href=&quot;https://openeverest.io/docs/api/1.10.0/&quot;&gt;REST API&lt;/a&gt;&quot;简化了软件更新、监控、存储扩展和外部访问配置等运维任务。自定义资源DatabaseCluster、DatabaseClusterBackup和DatabaseClusterRestore定义了OpenEverest如何在Kubernetes中声明式地配置数据库集群以及管理它们的备份和恢复，使这些操作可以作为版本化的原生Kubernetes对象进行处理，并隐藏了特定于数据库运营商的大部分差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当Percona推出该项目的测试版时，社区反响褒贬不一。在Hacker News上，这引发了一场关于在Kubernetes上运行数据库集群是否是个好主意的&lt;a href=&quot;https://news.ycombinator.com/item?id=41411122&quot;&gt;辩论&lt;/a&gt;&quot;：一些人对使用Kubernetes运行数据工作负载持怀疑态度，其他人则强调托管备份、集群、扩展、升级、优化的好处，其中有位用户指出，“Kubernetes不适合运行数据库”是一个非常过时的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，该项目支持通过各数据库引擎专属的&lt;a href=&quot;https://www.percona.com/software/percona-operators&quot;&gt;Percona operator&lt;/a&gt;&quot;部署和管理MySQL、PostgreSQL及MongoDB数据库集群。其功能涵盖数据库配置与扩展、备份及灾难恢复、基于角色的访问控制，以及在Kubernetes环境中灵活地分配资源。最新版本&lt;a href=&quot;https://newreleases.io/project/github/openeverest/openeverest/release/v1.11.0&quot;&gt;OpenEverest v1.11.0&lt;/a&gt;&quot;新增对PostgreSQL 18.1的支持，并通过NodePort支持实现了更灵活的网络配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/openeverest/roadmap&quot;&gt;正在进行当中的工作&lt;/a&gt;&quot;包括支持ClickHouse、Vitess、DocumentDB、Valkey等工具以及集成Prometheus和其他可观察性平台。根据&lt;a href=&quot;https://vision.openeverest.io/&quot;&gt;项目愿景页面的介绍&lt;/a&gt;&quot;，其长期目标是为构建和运营数据平台提供一个灵活的开源选项，并充分利用Kubernetes的普及性：“根据Kubernetes的调查数据，已经有50%的组织在生产环境的Kubernetes上运行数据工作负载。”Pronin&lt;a href=&quot;https://openeverest.io/blog/welcome-to-everest/&quot;&gt;阐述&lt;/a&gt;&quot;了从单供应商解决方案向开源转型的过程：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;该项目正在转变为OpenEverest——一个采用开放治理模式、拥有蓬勃发展的多供应商社区的独立开源项目。（……）OpenEverest将通过社区驱动的开放治理模式运作，摆脱单一供应商的控制。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该团队计划将该项目捐赠给CNCF，以保证其长期的独立性，并继续指导其孵化过程。OpenEverest并非在Kubernetes上管理数据库集群的唯一选择。KubeBlocks是一款开源operator（遵循AGPL-3.0许可），设计用于通过统一的API管理多种数据库类型，它目前支持35种数据库引擎，远超OpenEverest；而作为数据库管理平台，KubeDB虽然支持多种数据库，但已不再完全开源。此外，StackGres等特定于数据库的operator则专注于为单一主流开源数据库引擎提供深度功能集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenEverest遵循Apache License 2.0许可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/openeverest-kubernetes-databases/&quot;&gt;https://www.infoq.com/news/2026/01/openeverest-kubernetes-databases/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QIisU8EsIh2itQlUSFj9</link><guid isPermaLink="false">https://www.infoq.cn/article/QIisU8EsIh2itQlUSFj9</guid><pubDate>Wed, 11 Feb 2026 02:34:03 GMT</pubDate><author>作者：Renato Losio</author><category>大数据</category><category>开源</category></item><item><title>预防数据泄露：在GCP上实施VPC服务控制的实践</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云环境中数据窃取方面的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云计算革命彻底改变了应用程序的开发与部署方式。然而，传统的网络安全模式（即“&lt;a href=&quot;https://www.cloudflare.com/learning/access-management/castle-and-moat-network-security/&quot;&gt;城堡与护城河&lt;/a&gt;&quot;”方式）在云原生架构中就显得力不从心了。在云环境中，资源是分布式的、短暂的，并且可以从任何地方进行访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于迁移到公有云的企业来说，通过内部威胁、凭据盗取和服务配置错误所导致的数据窃取已成为一个重要的问题。行业报告显示，涉及云配置错误的数据泄露事件，每次给组织造成的平均损失为445万美元。在金融服务、医疗保健和受监管的行业，客户数据保护不仅仅是安全问题，更是合规性和法律的强制要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然本文主要关注Google Cloud Platform的VPC Service Controls (VPC-SC)，但其原则、挑战和最佳实践广泛适用于各大云服务商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AWS通过VPC Endpoints和Service Control Policies提供了类似的功能，而Azure则提供了Service Endpoints和Private Link。虽然实现细节不同，但防止数据窃取的战略方法（全面发现、分阶段推出、组织协调和分层安全）是超越任何特定平台而共通的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Google Cloud Platform中，&lt;a href=&quot;https://docs.cloud.google.com/vpc-service-controls/docs/overview&quot;&gt;VPC-SC&lt;/a&gt;&quot;在敏感云资源周围创建安全边界，防止未经授权的数据窃取，同时保持云的敏捷性和可扩展性。然而，在企业规模上实施VPC-SC（跨越数百个项目、多个区域和多样化的应用程序）需要战略级的规划、组织协调，并且要对安全需求和操作限制有着深刻的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文分享了在大型金融科技组织的Google Cloud Platform (GCP)环境中实施VPC-SC的经验教训，以保护支付处理工作负载、客户数据分析和多区域部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文不会提供分步骤的配置指南，而是分享战略决策、组织挑战和来之不易的经验教训，这些因素决定了安全实施是否能够成功。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深入理解VPC Service Controls：超越基础的周边安全性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC Service Controls会在GCP服务周围创建安全边界，强制执行基于资源位置、身份和网络来源的上下文感知访问策略。与在网络层操作的VPC防火墙不同，VPC-SC在服务API层进行操作，无论网络路径如何，都能控制对Google Cloud Storage、BigQuery、Vertex AI和Compute Engine的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它的三大核心构建块包括：&lt;/p&gt;&lt;p&gt;服务边界 (Service Perimeters)：在受保护的GCP项目和资源周围定义逻辑边界。边界内的资源可以自由通信，来自外部的访问则需要通过访问级别或入站/出站策略进行明确授权。访问级别 (Access Levels)：基于IP地址、设备状态、用户身份或地理位置定义访问边界内资源的条件，从而能够超越简单的允许/拒绝规则，实现上下文感知的安全性。入站和出站策略 (Ingress and Egress Policies)：指定哪些身份可以访问边界内的资源，以及边界内可以访问哪些外部资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个常见的误解，那就是VPC-SC并不会取代网络安全，而是它的补充。实际上，如果VPC-SC配置得当，即使攻击者攻陷了VPC网络内的虚拟机，也无法将数据窃取到外部云存储桶中，无论网络连接情况如何，API调用都会被阻止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个关键区别是，VPC-SC保护的是受支持的GCP服务，而不是任意的网络流量。Google Cloud Storage、BigQuery、Compute Engine、Vertex AI、BigTable、GKE等都能得到保护，但VPC-SC并不控制虚拟机的出站互联网流量，也不检查应用程序协议。与虚拟防火墙和Cloud Armor的集成对于全面安全性仍然至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构图&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e9fa02aacd83fc80372700b8dac5d7ea.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;设计VPC-SC架构：重要的战略决策&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划期间所做的关键设计决策决定了实施的成功或失败。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从数据分类开始&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并不是所有的数据都需要相同的保护。根据敏感性对数据进行分类，例如需要符合PCI-DSS的支付卡数据、受GDPR（General Data Protection Regulation）或CCPA（California Consumer Privacy Act）约束的个人身份信息 (personally identifiable information，PII)、机密业务数据和非敏感运营数据。这种分类会驱动边界的规划。高敏感的数据需要严格控制边界，允许的例外情况最少，而较低敏感的数据则允许更宽松的策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;建议分为三个边界层级：高安全性，用于支付处理，无出站策略；中安全性，用于客户分析，对出站访问受控的服务进行限制； 低安全性，用于开发/测试，策略较为宽松。这种方法在安全严谨性和运营灵活性之间取得了平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;彻底映射依赖关系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;依赖关系映射不完整是VPC-SC实施失败的头号原因。现代云应用程序依赖于共享服务、跨项目通信、CI/CD流水线、监控工具和第三方集成。在执行之前，必须记录每一个依赖关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们建议使用Cloud Asset Inventory进行资源发现，并分析Cloud Logging以获取服务到服务之间的通信模式。同时，采访应用程序团队以了解日志中看不到的外部依赖关系。对于大型组织，建议为发现阶段预留四到六周的时间，过于匆忙容易引发生产事故。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;服务账户是隐藏的依赖噩梦：在多个项目中共享的某个服务账户会引发意想不到的跨边界依赖关系。在为我所在的组织实施VPC-SC期间，我发现了散布在遗留系统、批处理作业和第三方集成中的数十个未记录的服务账户。其中许多可以追溯到多年以前，而维护它们的团队早已离职。每次发现都需要进行仔细评估，以确定该账号的使用是代表合法的业务需求，还是需要补救的安全漏洞。这一经验强化了服务账户发现必须与资源发现需要一样严格的观点，忽略某几个身份标识可能会破坏你的整个边界策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;边界拓扑：一个大边界还是多个小边界？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应该创建一个大的边界，还是按应用程序、业务部门或数据分类组织的多个较小的边界呢？答案取决于具体的组织结构和安全要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多个边界会提供更强的隔离性，某个边界的违规不会危及其他边界。然而，它们增加了复杂性，因为跨边界通信需要显式策略或边界桥接。我发现混合方法效果最好：按安全层级（高/中/低）组织的广泛边界，以及用于共享服务（如集中日志记录、监控和CI/CD）或多个边界之间入站/出站策略的边界桥接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于多区域部署，要避免为每个区域创建单独的边界。区域边界会带来不必要的复杂性，而不会增加额外的安全价值。VPC-SC策略是全局应用的。建议将所有区域资源包含在单个逻辑边界内，并在需要时使用IAM策略或访问级别进行特定于区域的访问控制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实施：从设计到生产的三个阶段&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第一阶段：发现与基线（四到六周）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发现工作不仅仅涉及技术资产盘点，还需要理解团队的工作方式、应用程序的通信模式以及安全漏洞的位置。为此，我组建了一个跨职能的工作组，成员包括安全工程师、基础设施团队、应用程序开发人员和业务利益相关者，我们每周开会，以审查发现结果、解决依赖性问题，并就边界范围达成共识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术发现会利用多个来源：Google的Cloud Asset Inventory用于资源发现，Cloud Logging用于API模式，并且要采用团队访谈的方式来获取GCP日志无法捕获的上下文。我创建了可视化工具来映射服务的依赖关系，使得识别应该在边界内分组的资源集群变得更容易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第二阶段：演练模式实施（至少六到八周）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC-SC的演练模式（Dry-run Mode）使安全实施成为可能。在演练模式下，会评估边界策略，违规行为会被记录，但API调用不会被阻止。这种方法允许我们在生产环境中测试边界配置，而不会产生服务中断的风险。我的建议是，将初始的边界配置以演练模式部署至少30天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每日分析违规日志，按服务、方法和主体对违规行为进行分组，以识别模式。特定服务账户产生的大量违规可能表明存在未被发现的合法用例，或者需要限制权限过高的凭证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;区分违规的性质：并非所有违规都是同等重要的。例如，被VPC-SC阻止的BigQuery读取操作可能会破坏关键的分析仪表板；而服务向外部存储桶写入数据的行为，可能正是我们要防止的数据窃取。建立违规分类（比如，需要调整策略的合法用例、可接受的已记录风险、需要补救的安全漏洞），并通过这个框架处理每一个违规行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;建立自动化仪表盘，显示违规趋势。违规数量的下降表明针对合法用例的政策调整取得了成功，而稳定或增加的违规行为则表明持续有依赖关系被发现，或者团队正在寻找绕过（尚未强制执行）控制措施的解决方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第三阶段：强制执行与运维管理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;强制执行的决策应该以数据为驱动：违规行为应该被分类和批准，测试并记录回滚程序，并获得受影响团队的利益相关者签字。强制执行是逐步进行的：首先是开发环境，然后是预发布环境，最后是生产环境。每个环境强制执行两周，然后再进入下一个环境，以确保意外问题会首先在风险较低的环境中出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;强制执行后，应该建立明确的例外请求流程：开发人员会遇到被边界策略阻止的合法场景。例外流程必须在安全性（不能授予破坏控制措施的全面例外权限）与敏捷（避免创建官僚式的冗长申请机制）之间取得平衡。在我的项目中，我创建了分层的例外机制：临时（72小时，由安全团队批准）、永久（需要安全架构审查）和书面申请。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;现实世界中的挑战与解决方案&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;BigQuery分析中断事件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在强制执行数据分析边界三周后，我们的商业智能仪表板停止了更新。调查发现，边界外的服务账户正在访问BigQuery数据集。这个依赖关系在演练测试中被遗漏了，因为相关的批处理作业每月才运行一次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的紧急修复是创建一个临时的出站策略，允许特定的BigQuery进行操作，而长期解决方案则涉及重构批处理作业以使用边界内的服务账户，并更新依赖关系的文档。这一样例再次向我们重申，演练周期必须跨越完整的业务周期，并且自动化的依赖关系发现应该补充而不是替代人类的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;平衡安全性与开发人员的生产力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最大的挑战并非技术层面，而是在组织层面。起初，开发人员将VPC-SC视为阻碍工作且无明显收益的障碍。一些人甚至试图通过在服务前使用&lt;a href=&quot;https://docs.cloud.google.com/vpc/docs/private-service-connect&quot;&gt;Private Service Connect (PSC)&amp;nbsp;&lt;/a&gt;&quot;来绕过边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方案需要改变我对安全性的沟通方式。我不再将VPC-SC呈现为一种限制，而是将其框架化为一种工具，保护他们的应用程序免受数据泄露，保护公司免受监管处罚，并保护他们的团队免受与安全事件相关联的风险。我的团队在自助工具上投入了大量资源，建立了一个网络门户，开发人员可以在其中检查服务账户访问权限、请求具有明确承诺的例外，并查看政策违规的解释和建议的补救措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当开发人员看到安全团队迅速响应合法需求，同时在不必要的例外方面保持坚定的边界时，他们的情绪也从抵制转变为接受。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;传统应用重构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对本地环境设计的遗留应用程序通常会假定对存储和数据库都能够无限制地进行访问。一个从本地迁移过来的支付处理应用程序，几乎没有做任何更改，就试图将日志写入另一个GCP组织中的云存储桶，这在本地环境中是合理的模式，但在云安全边界中却是违规的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们并没有为这个应用程序创建一个例外来允许跨组织的数据传输，而是与应用程序团队合作重构了日志记录。现在，日志写入边界内的一个桶中，并且一个授权的导出过程将经过清理的日志移动到另一个组织中的桶中进行合规归档。这种重构花了六周时间，但提高了安全态势并减少了运营复杂性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当VPC-SC阻止操作时，首先问一下自己，“这应该是被允许吗？”而不是问，“我们该如何允许它？”有时候被阻止的操作代表了需要补救的技术债务，而不是需要适应的现状。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;将VPC-SC融入更广泛的云安全架构中&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采用VPC Service Controls是全面云安全架构的一部分，而非独立的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;分层的防御模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将云安全视为一个同心层，需要在不同的层次提供保护。VPC-SC在服务API层运行，控制对GCP服务的访问。虚拟防火墙（例如Palo Alto Networks VM-Series）在网络层运行，控制IP流量并检查应用程序协议。Cloud Armor在应用程序层提供分布式拒绝服务（DDoS）保护和Web应用程序防火墙（WAF）功能。身份和访问管理（IAM）在资源级别控制基于身份的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个层次都会捕捉不同的威胁向量。攻击者攻陷虚拟机后，即使他们已经绕过了网络级防火墙规则，可能也会被VPC-SC阻止通过云存储API进行数据窃取。DDoS攻击可能会在压垮虚拟防火墙之前被Cloud Armor缓解。即使攻击者已经进入了VPC网络和边界内，被盗凭据可能会被IAM的上下文感知访问策略检测到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC-SC为敏感资源定义了广泛的安全边界，虚拟防火墙提供了细粒度的网络流量控制和协议检查，Cloud Armor保护面向互联网的应用程序，而IAM在边界内强制执行最小权限访问。单一的控制措施是不够的，安全性来自它们的相互作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;监控与事件响应&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC-SC会为每次策略评估生成审计日志，从而为安全监控创建了丰富的数据。这些日志可以流式传输到Splunk等安全信息和事件管理（Security Information and Event Management，SIEM）平台进行分析和警报，或者使用原生GCP日志进行分析。关键的监控场景包括：&lt;/p&gt;&lt;p&gt;策略违规异常激增表明可能存在攻击或配置错误。同一主体重复违规可能表明存在合法的访问问题或侦察行为。出站违规访问敏感数据或外部存储桶可能表明正在尝试进行数据窃取。VPC-SC配置的变更应该仅通过批准的基础设施即代码流程进行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对VPC-SC检测到的数据窃取尝试的事件响应剧本包括：立即调查源主体和目的地，如果确认攻击则暂时收紧边界政策，进行Cloud Logging的取证分析以确定访问的数据，进行事后审查以确定事件是由安全漏洞还是成功攻击引起的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;衡量是否成功：指标与KPI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;量化VPC-SC的影响需要定义业务、安全和运营指标。我们项目中的测量指标包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;安全指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在前六个月内阻止了847次尝试进行数据窃取。通过逻辑隔离，PCI-DSS审计范围减少了40%。数据访问异常检测时间减少了45%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运维指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在稳定后，部署时间增加不到5%。标准例外的平均处理时间为四小时。通过自动化，策略管理时间减少了60%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预计每年因为避免数据泄露减少了450万美元的损失每年合规成本节约20万美元实施成本为80万美元（12名工程师耗时6个月）在18个月内实现了正投资回报率（ROI）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最佳实践与经验教训&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;服务发现永无止境，即便采用穷举式的发现方法，也要预期会有意料之外的依赖关系。将服务发现过程视为持续进行的行为，维护依赖关系待办事项列表，安排季度审查，并要求对所有新部署进行依赖关系文档编制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;演练时长至关重要，30天的演练测试是最低要求，而不是目标。对于具有每月批处理作业、季度报告周期或季节性流量模式的应用程序，要延长演练时间以捕获完整的业务周期。一周的强制执行延迟与生产中断的成本相比是微不足道的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例外流程决定了成败，你的例外流程决定了VPC-SC是增强还是阻碍。明确的时间承诺、透明的批准标准和自助式请求提交，能够使开发人员将安全视为合作伙伴而非障碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;全面自动化，手工化的策略管理难以为继。对于所有的VPC-SC配置均应使用基础设施即代码（例如Terraform），或者构建一个自助工具，可以使用Cloud Functions添加策略。在生产部署之前实施自动化验证测试策略更改。自动化的预部署验证可以在生产之前捕获策略冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;沟通能够消除阻力，技术卓越并不意味着能够被采用。我在沟通和利益相关者管理方面花的时间和技术实施一样多。制定定期的办公时间解释为什么VPC-SC能够保护每个人，带有示例和故障排除指南的清晰文档，对被VPC-SC阻止的开发人员积极进行响应，将组织文化从抵制转变为支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来改进的方向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC Service Controls代表了当前GCP数据窃取预防的最佳实践，但威胁和技术仍在不断发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将VPC-SC与零信任原则对齐，明确验证的合规性，使用最小权限进行访问，并假定存在漏洞。未来的演进应该加强基于实时风险评分的动态访问级别，与身份威胁检测进行集成，在检测到可疑行为时及时撤销访问，以及基于威胁情报自动进行边界策略调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前的IaC方式将安全策略视为静态配置。下一代的方法会将策略视为可测试、版本化的代码，并进行自动验证和部署。我们正在朝着策略测试、针对模拟攻击场景验证边界有效性、策略漂移检测（当部署的配置与批准的基线发生偏离时发出警报）以及策略影响分析（在部署之前预测对开发人员生产力的影响）的方向演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在企业级规模上实施VPC Service Controls表明，成功的安全不仅仅是技术，更关乎人员、流程和组织文化。VPC-SC在技术方面有着良好的口碑，并且相对简单。困难之处在于理解组织的独特需求，驾驭复杂的依赖关系，获得利益相关者的支持，并在实现业务敏捷性的同时保持安全严谨性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;核心原则超越了任何特定技术。安全的作用应该是促进而不是阻碍；严重损害生产力的控制措施需要规避。你应该设计避免不必要摩擦的安全防护，自动化可以进行大规模扩展，而手工过程难以做到这一点，因此要投资工具，使安全实践成为最简单的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;指标很重要。无法衡量就难以改进：跟踪安全和运营的影响。完美是完成的敌人，请现在就部署有效的安全控制，而不是等待永远不会实现的完美控制。采用持续改进的方式，而非试图毕其功于一役。安全不是目的地，而是一种不断适应和完善的持续实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC Service Controls为GCP环境提供了强大的数据窃取预防机制，但其有效性取决于详尽的设计、分阶段实施、组织协调以及与更广泛安全架构的集成。愿意投资于全面规划、接受迭代改进，并在安全与可用性之间取得平衡的组织，将发现VPC-SC是云安全战略中非常有价值组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;威胁环境将继续演变，防御措施必须相应地发展。最重要的不是任何单一的技术，而是建立组织能力来评估风险、实施适当的控制措施、衡量效果，并持续改进能力，以服务于采用任意云平台或安全技术的组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/preventing-data-exfiltration-google-cloud/&quot;&gt;Preventing Data Exfiltration: A Practical Implementation of VPC Service Controls at Enterprise Scale in Google Cloud Platform&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QaR8toToJhEANT6gdQ6s</link><guid isPermaLink="false">https://www.infoq.cn/article/QaR8toToJhEANT6gdQ6s</guid><pubDate>Wed, 11 Feb 2026 02:31:35 GMT</pubDate><author>作者：Shijin Nair</author><category>大数据</category></item><item><title>千问发布最新图像模型Qwen-Image-2.0，支持1K token超长文字输入和2K高分辨率</title><description>&lt;p&gt;2月10日，阿里巴巴正式发布新一代图像生成及编辑模型Qwen-Image-2.0。据介绍，Qwen-Image-2.0集生图和编辑于一体，在AI Arena文生图评测中斩获1029分，超过Seedream4.5、Flux2-Max等模型，仅次于谷歌Nano Banana Pro和GPT Image1.5。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/83cfb6dd0c5fbd8506d351ef9bb95cfc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;AI Arena文生图评测中，Qwen-Image-2.0位居第三&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0支持1K token的超长文字输入和2K高分辨率，可准确渲染复杂指令，生成专业的PPT及信息图；同时，千问新模型拥有极强中文汉字渲染能力，数百字的古文全文几乎都能完全渲染在图片中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0在Qwen-Image和Qwen-Image-Edit两大模型基础上全新升级，首次将图像生成和编辑统一到一个模型中去，以更轻量的模型架构，实现了生图和改图性能的大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0 生图质感进一步提升，生成的人物、自然、建筑等常用图片更加逼真。在权威评测AI Arena中，千问新模型在图像生成中得分1029，位列第三；在图片编辑中得分1034，仅次于Nano Banana Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/170cafb63ad908bde8359a00384f463f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0生图，以瘦金体写诗配图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在中文汉字渲染方面，官方表示Qwen-Image-2.0 不但可以以多种字体准确渲染汉字，而且写得又多又准，效果比 Nano Banana Pro更优。千问新模型将输入提示词扩展到1K token，可详尽描述任务，实现更专业的文字渲染，在专业PPT、高级海报、多格漫画等复杂图片方面有不错表现，比如以小楷字体几近完全渲染《兰亭集序》数百字的全文配图，以自然语言生成论文格式配图的复杂PPT等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da0795cfaf18b0dea35ee0ded31a16e6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0生图，多文字复杂PPT一键生成&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，基于Qwen-Image-2.0模型，用户可与AI协同创作出更丰富、更实用的图片，比如一句话生成宫保鸡丁的做法流程图，杭州两日旅游攻略图，4x6的多格漫画组图，儿童绘本图，写实风格的电影海报，极为逼真的绿色丛林等等；同时，用户也可上传数张图片进行编辑，生成诸如九宫格多手势自拍，真人配字表情包，双人逼真AI合影，诗词配图等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a6a92a9fd10000108fdab5f3917ea14.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0编辑图片&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，阿里云百炼上已开通API邀测，开发者也可通过Qwen Chat免费体验新模型。&lt;/p&gt;</description><link>https://www.infoq.cn/article/b8LJJIs08XT0h9dRflsH</link><guid isPermaLink="false">https://www.infoq.cn/article/b8LJJIs08XT0h9dRflsH</guid><pubDate>Wed, 11 Feb 2026 02:09:10 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>模力工场 032 周 AI 应用榜：桌面 Agent 强势来袭，阶跃登顶本周榜首</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;邀你用 AI 一键生成新年财运红包封面！2月5日至25日，设计松鼠 × 模力创意红包，即可赢金币参与多轮现金抽奖。扫码进群，马上开启你的开年好运！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/cafd85344e3eb0dfc0714172e79b1126.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;032 周上榜应用精选（附用户热评）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场 32 周 AI 应用周榜来啦～本周共有 25 款应用上架新榜，所有排名均来自用户真实使用、测评与社区讨论热度。本期用户讨论最高的是：桌面 Agent 形态的出现。AI 正在从“对话框里的助手”，走向“接管桌面的执行者”。AI 开始在真实桌面环境中，操作网页、处理本地文件、生成办公文档，甚至能把多个应用里的任务一口气跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们从中精选出十款最具声量的应用，聚焦五大垂直领域，为你更详细地解读榜单背后的 AI 行业风向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、桌面 Agent 类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：接管桌面｜跨应用操作｜真实执行&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/stepfun-desktop?utm_source=20260210infoQ&quot;&gt;阶跃AI桌面伙伴 📍上海&lt;/a&gt;&quot;：一个更懂中文办公的国产桌面 AI 伙伴，无需复杂设置，全平台支持，深度整合钉钉、飞书等本土工具，用截图提问、智能整理、定时任务等贴心功能，为你打造真正懂中文、懂场景、懂流程的下一代智能工作台。&lt;/p&gt;&lt;p&gt;    &lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3660388f8cfbad8960e33fd0215d44f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/workany?utm_source=20260210infoQ&quot;&gt;WorkAny 📍广州&lt;/a&gt;&quot;：艾逗比开发的开源跨平台桌面智能体，可以通过安全沙盒执行各类脚本，无缝处理文件整理、文档生成、网页制作等办公任务，更支持自定义模型与并行处理，用本地订阅打造你的专属 AI 生产力中心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72fd4ee7cf65a17a99ff9138019d141c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、学习 / 知识管理类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：结构化学习｜知识转写｜理解与记忆&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/chatglm?utm_source=20260210infoQ&quot;&gt;智谱清言 AI 学习搭子&lt;/a&gt;&quot;：植入在智谱清言生态中的学习辅助模块，擅长把教材、文档和概念转化为知识地图、卡片和讲解内容，并配套随堂测试，更偏“陪伴式学习”和知识消化。&lt;a href=&quot;https://agicamp.com/products/thetawaveai?utm_source=20260210infoQ&quot;&gt;Thetawave AI&lt;/a&gt;&quot;：偏重输入端的学习整理工具，支持录音、视频、文档、网页等多源内容转写，并生成结构化笔记、思维导图和测验，适合学生和知识工作者做系统性复盘。&lt;a href=&quot;https://agicamp.com/products/notebooklmgoogle?utm_source=20260210infoQ&quot;&gt;Notebook LM&lt;/a&gt;&quot;：Google 推出的研究型笔记工具，更偏“资料理解与问答”。围绕用户上传的 PDF、网页、视频等材料进行摘要、提问和交互式研究整理，适合研究与长期项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、内容与视频创作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：内容工业化｜全流程生成｜效率提升&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/daoying?utm_source=20260210infoQ&quot;&gt;道影 AI 📍杭州&lt;/a&gt;&quot;：AI 视频全链路生产平台，面向短剧、漫剧等专业内容创作者。从剧本到成片一体化设计，强调流程贯通与规模化生产，而非单点创意工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、开发 / 编程协作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：Vibe Coding｜一体化开发｜任务式编程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/zaizhi?utm_source=20260210infoQ&quot;&gt;OpenCode&lt;/a&gt;&quot;：为 Vibe Coding 场景设计的 AI 编程工具。把聊天、代码编辑、文件树和终端放在同一界面，支持 skill 封装与多模型切换，对编程新手非常友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/0398aaa68bffa221cef5510894d6bccb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;五、专业与底层能力类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：专业生成｜算力平台｜企业与垂直场景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/mureka?utm_source=20260210infoQ&quot;&gt;Mureka V8&lt;/a&gt;&quot;：昆仑万维推出的 AI 音乐生成平台，从自然语言或歌词直接生成结构完整、编曲成熟、人声自然的音乐作品，面向专业音乐创作场景。&lt;a href=&quot;https://agicamp.com/products/Prism?utm_source=20260210infoQ&quot;&gt;Prism&lt;/a&gt;&quot;：OpenAI 的 Prism 是一个不错的学术写作结构梳理与格式排版工具。它尤其适合在开题与文献综述阶段，帮你将思路系统化、可视化，并接手繁琐的 LaTeX 排版与参考文献管理。&lt;a href=&quot;https://agicamp.com/products/lanyun?utm_source=20260210infoQ&quot;&gt;蓝耘元生代 📍北京&lt;/a&gt;&quot;：以自研 MetaGen 智能算力操作系统为核心，面向企业提供集算力调度、模型服务与数据生成于一体的智算云平台，支撑 AI 应用落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;榜单之外但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【应用名称】：&lt;a href=&quot;https://agicamp.com/products/Flora?utm_source=20260210infoQ&quot;&gt;Flora&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：节点式创作｜无限画布｜创意工作流&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【模力小A推荐】：Flora 是一款节点式创意 AI 平台，通过“无限画布”把文本、图像和视频生成串成可复用的工作流，适合品牌视觉、广告概念等跨媒介创作场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从本期榜单可以清晰看到一个信号：AI 的主战场正在从“会不会回答问题”，转向“能不能把事做完”。桌面 Agent 的集中出现，是这一变化最直观的体现。相比以往停留在对话框里的助手，本周讨论热度最高的产品，已经开始直接接管桌面环境，真实操作网页、处理本地文件、生成办公文档，甚至跨多个应用连续执行任务。用户关注的核心不再是模型能力，而是执行稳定性、流程完整度和对真实工作场景的适配程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，学习、内容创作和编程类应用的演进路径也在发生变化：它们不再强调“单次生成”，而是围绕结构化理解、完整流程和长期使用进行设计。无论是学习工具对多源资料的系统整理，还是内容平台对从创意到成片的全链路打通，本质上都在向“可持续使用的生产力工具”靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，本期周榜反映出的并非某一个爆款应用，而是一种明确趋势：AI 正在从能力展示，进入到执行与交付阶段。谁能真正嵌入用户的工作流，承担连续、可验证的任务，谁才更有可能成为下一阶段被长期留下来的 AI 应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后再介绍一下模力工场的上榜机制和加入榜单的参与方式，欢迎大家继续积极参与提交 AI 应用～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场AI 应用榜并非依靠“点赞刷榜”，而是参考以下权重维度：&lt;/p&gt;&lt;p&gt;评论数（核心指标，代表社区真实反馈）&lt;/p&gt;&lt;p&gt;收藏与点赞（次级指标）&lt;/p&gt;&lt;p&gt;推荐人贡献（注册推荐人可直接为好应用打 Call）&lt;/p&gt;&lt;p&gt;加入榜单的参与方式：&lt;/p&gt;&lt;p&gt;如果你是开发者：上传你的 AI 应用，描述使用场景与核心亮点；&lt;/p&gt;&lt;p&gt;如果你是推荐人：发现好工具，发布推荐理由；&lt;/p&gt;&lt;p&gt;如果你是用户：关注榜单，评论互动，影响榜单权重，贡献真实声音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One More Thing，对于所有在模力工场上发布的 AI 应用，极客邦科技会借助旗下各品牌资源进行传播，短时间内触达千万级技术决策者与开发者、AI 用户：&lt;/p&gt;&lt;p&gt;InfoQ 全媒体矩阵&lt;/p&gt;&lt;p&gt;AI 前线全媒体矩阵&lt;/p&gt;&lt;p&gt;极客时间全媒体矩阵&lt;/p&gt;&lt;p&gt;TGO 鲲鹏会全媒体矩阵&lt;/p&gt;&lt;p&gt;霍太稳视频号&lt;/p&gt;</description><link>https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</link><guid isPermaLink="false">https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</guid><pubDate>Tue, 10 Feb 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>为 ChatGPT 和 Claude 提供“地基”的那家公司，在担心什么</title><description>&lt;p&gt;过去一年，关于 AI 的讨论出现了一种明显的反差：一边是模型能力不断刷新上限，另一边却是越来越多企业开始质疑——为什么真正落地依然这么难？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从概念验证到生产系统，从 60% 的“看起来可用”到 99.99% 的“不得不可靠”，企业级 AI 面对的从来不是算力或参数规模的问题，而是数据、决策责任、合规流程以及现实系统复杂性。而这些恰恰是大多数新闻叙事里最容易被忽略的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样的背景下，这期播客给出了一种罕见的“现实视角”。对话并没有继续渲染模型能力的指数级增长，而是把焦点放在一个更基础、也更棘手的问题上：AI 要真正进入企业和关键业务流程，还缺什么？答案指向了一个长期被低估的环节——数据，以及数据背后的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本期节目的对话者，正是站在这一环节核心位置的人。他所领导的公司，长期为几乎所有一线大模型实验室提供训练所需的基础数据；而在加入这家公司之前，他曾把一个看似边缘的想法，在极短时间内推演成一家年收入 200 亿美元的业务，也亲身经历过科技创业中最极端的法律与商业风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场对谈中，他系统性地拆解了几个被反复误解的问题：&lt;/p&gt;&lt;p&gt;为什么大模型至今仍然离不开人类专家？&lt;/p&gt;&lt;p&gt;为什么绝大多数企业数据对 AI 来说毫无价值？&lt;/p&gt;&lt;p&gt;以及，当模型开始转向“智能体”和决策能力时，真正的瓶颈到底在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客整理翻译：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;精华摘要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您在数据、标记、训练等领域都是当之无愧的技术先驱，您能不能展望一下AI领域在过去一年半以来的发展轨迹？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：一年半之前，大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;收购之后，Scale 还是 Scale 吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：欢迎今天到场的嘉宾Jason Droge，请先简单向大家介绍一下您的背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是Scale AI的新任CEO，这也是我在接替Alex Wang接受Meta收购之后参与的首次采访。Alex现在领导Meta旗下的超级智能团队。在加入Scale AI之前，我与Travis Calendar曾共同创立一家公司，再向前追溯还在Uber等几家初创公司工作。我最知名的成果应该是创立并领导了Uber Eats，跟团队的同事一道把这个点子培养成了如今市值数十亿美元的企业。在COVID期间，Uber Eats几乎是以一己之力支撑起了因社交隔离而陷入瘫痪的Uber业务体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这次采访的主题是AI模型如何拥有真正的智能。您觉得Scale AI在其中扮演了什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们在ChatGPT和Claude身上看到了诸多改进。目前各个前沿领域都存在领军级别的模型，各家实验室则聘请专家填补这些大模型的知识空白，校正其对于事物运作方式的理解。而Scale是这一领域的先驱，也可以说是创造了这种业务形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：我们都很关心Scale的近况以及被Meta收购之后的变化。Scale目前情况如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：Scale仍然是一家完全独立的公司。在此次交易中，Meta投入140多亿美元以换取Scale公司49%的无投票权股份，且未获得新的董事会席位。Scale的董事会保持不变，治理结构几乎未肥影响，Meta对于Scale的任何资源也都不具备优先访问权。我们跟Meta一直在数据业务方面保持着长期合作关系，随着双方关系更进一步，各方面合作也有望持续扩大。但我们与其他各方的合作不会受到影响，Meta无法访问任何之前不对其开放的信息，例如隐私和数据安全政策等。事实上，此次交易只涉及约15位员工的变动，而Scale共拥有约1100名员工。现在我们旗下拥有两大业务部门，其营收都达到了数亿美元规模。公司内部相当于两家独角兽，支撑着每月的业务增长。总之，我们很高兴能够继续构建并交付数据，维持之前的工作模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据不是苦力活：标注为何变成专家工作&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到 Scale主要面向AI数据市场，那能不能解释一下数据标注工作是怎么从当初的低成本劳动力转向如今的专家处理形式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：没问题。首先，我认为其他竞争对手目前的定位是错误的，所以我先从这个角度切入，再逐渐延伸其他方面。这里我先花点时间介绍一下Scale的发展史，还有自2016年以来的发展脉络。Alex很早就意识到，数据对于模型来说至关重要。那时候他只有19、20岁，但他已经在考虑要如何围绕这个基本前提建立业务。他最初选择的方向是为自动驾驶做标注。标注数据的质量越高，汽车的行驶表现也会更好。之后这股浪潮演变成了计算机视觉，我们开始跟国防部门建立合作关系，为他们提供标注服务，到这里时间已经来到2020年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来大模型的性能越来越强、愈发完善，需要的数据类型也更为丰富。所以我们一直在不断调整以提供所需的数据类型。在此期间，行业本身也在不断变化。记得两、三年前这些大语言模型刚出现的时候，经常会闹出幻觉问题，比如给出特别浅显的错误答案等等。但情况变化很快，我们也一直在随之改变。Scale一直走在前沿，开始为更复杂的任务提供专家级的数据标注服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要聊过去一年半的情况我其实不太够格，因为我才加入公司13个月。刚加入时，我一直在做模型性能的测试。那时候大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的专家中，有80%的同事拥有学士学位证书，这跟其他竞争对手的定位完全不同。其中约有15%的员工对相关行业有深入了解。这些高知人群通过为模型添加标签、贡献专业知识来赚取丰厚的收入。我很喜欢我们这种以专家级别进行数据标注的业务定位，这能帮助公司与研究人员保持联系、了解他们的需求。我们内部也很早意识到大模型在高度专业的领域上表现欠佳，所以我们会主动联系开发基座模型的大厂，表示我们注意到了这个问题，而且有专家团队可以搞定。我喜欢这种与众不同的定位，这跟竞争对手的想法完全不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那Scale是怎么接洽并挽留这些专家贡献群体的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：确实不太容易接触得到，所以得制定相应的策略。具体的方式肯定不止一种，最主要的就是让专家们相互内推，而且他们很喜欢这种用自己的专业知识为AI做贡献的感觉，很酷。比如一位特定领域的博士在面对具体主题时，发现大模型的表现根本无法令人满意。那这时候他就可以通过有偿的方式提供专业建议，并借此赚取数百甚至数千美元。当然，我们也会推动校招，直接跟学校里的教授和学生们交流，询问有没有人愿意参与进来。当然，LinkedIn等传统渠道也是开放的，但效果最好的还是线下接洽和内推网络。这样能够为参与者提供良好的体验，因为他们的贡献一方面是为了赚钱，但更多是出于为AI模型做贡献的成就感。而且这个过程也是在替他们自己解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能体要学会做事：没有捷径，只有环境&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有报道认为整个AI经济生态都将转向强化学习，您对此有何看法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：强化学习当然非常重要，我觉得这种趋势性判断也很有道理。强化学习环境就相当于AI智能体的沙箱，它们可以在沙箱中学会如何达成目标。我们在这方面也尝试了一年有余。比如在Salesforce实例当中，AI智能体要如何实现导航？襳中包含哪些需要识别的数据？这要求智能体执行一套可靠性极高的业务流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，智能体还得知道如果无法完成预期任务，或者判断正确完成任务的可能性较低，那要怎么向人类反馈以获取指引。所有这些都需要训练，而且不存在什么神奇的捷径。唯一的办法就是把AI智能体放进能代表人类正确操作的环境当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以想象，现实世界中此类环境的数据和其中的不同目标可以说是无穷无尽，所以我们花了一年多跟模型开发商保持良好的合作关系，共同观察在不同任务/环境下的通用性表现。很明显，这类环境、软件系统、配置、数据类型、规模和用户数量各有不同，复杂性也差异巨大。这&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就要求我们制定一种策略，让模型开发商能够收集到足够的通用数据以支撑广泛用例，而不必直接面对上万亿种任务和环境指标的排列组合。有些工作和数据之间具有更强的通用性，可以用一种简单的方式完成任务——比如在日历上找到要参与的访谈，让智能体浏览日历内容并弹出相应提示。接下来要做的，就是把智能体推广到一切日历搜索和日程管理操作。总之数据通用性越强，价值也就越大。我们的工作就是为模型开发商提供最有价值的数据，从而确保智能体尽可能为最终用户提供良好服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;企业AI不是演示：从 95% 到‘五个9’，差着一个世界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那您能举例聊聊具体向模型实验室提供哪些数据吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以。比如说我们的业务分为两个方面：其一是向模型开发商提供数据，也就是出售数据。其二则是向医疗保健系统、保险系统之类客户出售应用程序和服务解决方案。比方说我们跟一家医疗保健系统开发商合作，这套系统目前存在很多问题，部分专家需要定期处理少数罕见病例。因为专家人手不够，所以罕见病例会大量积压。这家医疗保健机构希望接诊更多病人，提供更好的护理体验并减少复诊次数，也就是在第一时间给出准确的诊断并制定治疗方案。如果没有AI的帮助，医生得耗费大量时间阅读长达两、三百页的病历文件。而我们开发的工具能帮助他们阅读这些文件，并指出其中最值得注意的五到十条内容。举例来说，某些过敏症状看似不起眼，但却可能跟医生开具的治疗药物发生冲突。AI工具可以快速提取这种人脑难以容纳的关联性要素，表现出相当完善的诊疗能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这方面，现成的模型肯定会有一定局限性，迫使医疗保健系统内部的人员亲自进行数据标注。不少企业乃至政府部门也会这么做，但仅靠现成模型加上一些零散数据没办法实现特别好的效果。毕竟很多银行或者医院一年的数据量就多达上百PB，他们自己根本没法判断哪些数据对模型有用。大多数数据都没价值，可怎么从中挑选出少数有价值的？而作为专业服务商，我们在数据的评判、挑选和专业知识储备方面非常出色，能够帮助客户顺利攻克这些瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：要保证AI变得越来越聪明，咱们人类到底还要参与多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：首先，数据标注本身就是一段不断创新的历程，就跟自动驾驶汽车一样。现在我们需要的数据标注量已经远远少于过去了，什么时候我们不再需要外部数据、模型的训练不再需要人工数据时，那就已经发展到新的阶段了。换言之，这意味着那时人类提供的一切技能和知识都已经不重要，没办法推动模型的进一步提升了。但对于Scale这样的企业，我们一直在研究如何刹那起能够发现新需求，并与贡献者网络合作的运营体系。我们会邀请专家贡献者来挖掘这些数据和信息。另外，他们的很多才能并不会第一时间就表现出价值。比如一年前很多知识对模型没用，但现在却突然有了大用。这是个不断进步的过程，需要将越来越多的数据输入到模型当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;出于经济动机，我们相信人类永远会在其中占据一席之地。这不只是种商业判断，更是我的个人信念，就像AI系统永远要为人类服务一样。我甚至觉得随着脑力岗位的逐渐消失，这就是未来知识工作者的主要转型方向。而且从我对部分客户身上观察到的情况，这种转变很可能在未来一到两年内发生。我当然希望这种颠覆别来得太快，但目前来讲确实是一切皆有可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于从长远来看，新技术总会替代旧方案，比如收音机淘汰了现场播讲之类。人类还是很善于适应这种变化的，技术的发展史就是人类适应新模式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您经常会提到“评估”这个词。那评估工作在专家们的日常工作中占比多少？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：占比还是相当高的。在企业和政府客户中，大部分业务内容就是评估，因为需要有人来设定“好”的基准。以之前提到的医疗保健来讲，医生在工作中就会评估病历报告和记录内容，然后对“好”做出明确的定义。这样慢慢累积起来的“好”和“正确”，就会让模型变得越来越可用。当然，AI的能力仍然有局限。对于那些人工流程的准确率很低的场景来说，AI就特别重要，因为能够切实帮上大忙。如果AI在其中能够达到50%、60%甚至70%、80%的准确率，那大家就乐疯了。但对于剩余的情况，比如人工流程的准确率能够达到98%，希望AI能解决余下的2%，那就很困难了。正因为如此，我们才需要明确定义“好”，让自己构建的系统能够代表使用者做出判断。在这样的设计思路下，AI系统就能像人类一样根据当前的信息尽可能给出最佳建议或者行动方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：很多人觉得AI是基于海量历史数据训练出来的，那AI在智能水平上怎么超越人类呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这种理解对，但也不对。首先，跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。而且这些铺设工作总得有人去做。负责任地讲，作为从业者，我对于如今大模型在一致性和准确性方面的提升仍然感觉喜出望外。现在大家可能已经习惯了大模型越来越先进也越来越靠谱，但短短三年前这个问题还相当复杂，需要综合考虑多种因素。总之，大模型的发展是算力、模型本体和数据改进的共同产物，而这三条确实在同时进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：大家对这个问题讨论得倒是不少，新闻报道也是层出不穷。具体怎么理解，要看我们选择怎样的视角。目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策。一旦到了这个阶段，我们之前提到的应用环境就非常重要了。比如怎么让智能体在医疗保健系统内正确导航、如何在手机上的天气应用中导航，又怎么替我们做出决策。目前这一切才刚刚起步，我也期待看到后续的更多变化。而这也是大家相对不太了解的层面，对于改进的方式也是莫衷一是。如果选择最乐观的判断，那这一切就是经济体系下的又一波正常变动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，AI普及不再是技术问题，而是人力和政策层面的问题。虽然目前还没到这个程度，但我确实相信未来两到三年之内，AI技术会发展到中心让治理层和政策制定者认真对待的阶段。现在已经离那个状态不远了，也就是两到三年的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：主要还是炒作得太狠了。我们的工作是打造出真正能够为客户创造价值的产品，并真正在解决复杂性上有所突破。还是以医疗保健系统为例，我们就在为医保公司提供理赔流程管理。这样的财务决策其实就是个可以自动化的过程，但在具体落地上学问就大了。很多人觉得概念验证能达到60%、70%的成功率就行，但这跟规模化应用还差得远。以数据中心为例，正常运行时间、可靠性和备份稳定性方面每增加一个“9”对应的都是又一个数量级的投入。比如四个“9”基本就是大学生自架服务器的水平，而五个“9”看起来只高了一点点，但其实完全是另一个世界。比如说很多人认为95%是个挺高的标准，但一旦用这个标准处理采购订单，那必然会面对无穷无尽的故障和投诉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总之，企业在应用AI时需要一步步完成法律批准、政策批准、监管批准和变更管理等各个环节，确保精度能让所有人满意。因此，AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。所以大家一定要分得清新闻炒作跟实践落地。就像我自己的教育背景，博士这个头衔说起来轻，但背后需要付出的努力远超大多数人的想象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;所有好生意，都是在不确定中被验证出来的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您参与建立了Uber Eats，还创办过其他几家初创企业。关于获客户这个问题，您有没有什么独家心得可以分享？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是那种愿意尝试一切新鲜事物的人，而且我总觉得创业是个非常清晰而且可把握的过程。我自己的思路是这样：在实际行动之前，先质疑自己听到的每一句话。我不会从字面上理解客户的表达，而是从产品管理的角度来审视。这个大家已经讨论得足够多了，比如说别按他们说的做、而要按他们预期的效果来做，这才是真正值得关注的问题。总之我会关注客户的潜在动机，而这种动机并不总是经济性质的，也往往跟自尊心和职业发展相关。比如说如果我们要向某人推销企业软件，那就得让对方相信你的软件能帮他们做好工作、建立起信任让对方接受你参与到大的项目中来。这个过程中重要的不只是产品，更要思考他们想得到怎样的建议、需要我们提供什么、需要怎么做才能找到正确的产品实现方向等等。我知道这话听起来有点陈词滥调，但只要让我准确把握住对方的真实动机，我就能拿出正确的结果。我再举个例子，当初在发布Uber Eats之前，我有认真考察业务。在获客方面，我们其实还给不出餐厅导览的功能，对餐饮行业也一无所知。但在Uber，我们最想解决的是接下来该拓展哪些其他业务。在考察了大量企业之后，我们觉得外卖业务最值得尝试，结果也证明这是正确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了搜集数据，我们找了一家餐厅供应商并拿到一份基础目录，比如说一份典型的餐食要用多少火腿、多少奶酪、多少面包和多少片生菜，再据此推断食材成本有多少、人工成本是多少，进而建立起基准数据。把这些因素综合起来，我们就能把餐食品类建立起清晰的认知。我们发现食材在每份餐食中的成本大约占20%到30%，人工又占20%到30%，10%是房租和其他开销。总之这就是一种链条，而结合核算出的附加价值之后，我们决定收取账单总价的30%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;餐厅刚开始会觉得这个数字太高了，本能性地想要拒绝。我们解释了自己的核算方式，说服对方做起来试试。事实证明餐厅的判断是正确的，这个比例确实太高了，最终确定下来的抽成是25%——跟我们的判断也相差不远。在这样的基础之上，我们再分析餐厅的主要价值实现形式是什么。对于游客型餐厅来讲，增加需求就是最关键的。在固定的餐厅店租、人力支出和食材成本都不变的前提下，需求增加三倍并不会增加人力支出，单纯的食材用量增加可以让产品的毛利率提升至70%到80%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这样的洞察，我们有信心在抽取一定费用之后店家和消费者都能接受。这就是市场经济的基本逻辑——不会只满足单独一方的所有需求，各方牺牲一点利益来保障自己余下的利益。Uber Eats就是这样的典型案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以善于独立思考闻名，这种能力为什么如此重要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：从创始人的角度来论述，在Uber我确实享受到了很多优势。我其实不算真正的创始人，只是参与了创业的流程。创业是涉及很多要素的，比如在97年创办第一家公司时，体验其实也就那样。但现在的创业可不一样了，每个人都在做自己的探索。但问题是，我们的研究方向多少都要受到周围言论的影响，那就没办法拥有独立的洞察力。所以最重要的独立思考，坚定去践行自己的判断。因此从创业的角度讲，我认为独特性非常重要。所以核心考验的就是人的洞察力，至于为什么我会幸运地拥有这种洞察力。这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。也许是因为我有点“遗世独立”，也可能是因为我是那种擅长逆向思维的人，总会在寻求其他人不相信的真理，有时候这也挺有效的。而且最难的部分是，我们愿不愿意在自己的判断上押上五到十年时间？人们总会犯错，只能尽量跟客户沟通，试着解决困扰他们的问题。创业就是这样，我们必须有这种强烈的自我表达意愿，不断摧毁自己曾经坚信的东西。更难的一点在于，我们又要有能力超越自己的观点，不能自大到总认为自己就是全世界最了不起的独立思考者。这个事很辩证，但最终就是要靠结果来证明和支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有时候最靠谱的创业方向，并不在于所谓最好的思路和机会。当然，对于我这种职业生涯已经超过25年的从业者，那选择空间和容错面会大得多。我认为一种业务的成功可以有两种途径：其一，也可以说是最重要的一种，就是创始人长期保持一股自我迭代和更新的力量。但这种年复一年的坚持其实很艰难。第二点是，可以直接去照搬其他人的经验，比如什么是好的商业模式、什么是差的商业模式、什么是好的市场定位、什么是差的市场定位。哪怕是拥有再强大的知识储备，哪怕理论上要进入的是一种比较差的市场定位，那只需要全身心投入，那随着时间推移也会逐渐显现回报。当然，我个人不会选择这种方式，我认为还是要根据市场需求走。纵观顶尖风投企业投资的项目，就会发现他们的投资组合是有规律的，至少是在对应价值数百亿美元的商业模式方面是有共性的，而且是具有网络效应的。规模大的业务就是比规模小的业务更有价值。比如我在Uber做过的新业务筛选，淘汰不好的想法其实挺快的，费不了多少时间。至于在筛选剩下的业务中，那就可以根据自己的直觉和热情去推动了。总而言之，我觉得大多数人对于哪些业务有机会增长到千亿美元规模缺少基本的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您推动了Uber Eats的上线。那在确定选择外卖赛道之前，你还探索过哪些其他想法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我肯定不是那种绝顶聪明的人，所以我会尽可能长期保持自己开放的态度，直到把各种有价值的元素都融合起来。有些想法刚开始看也许觉得比较差，但只需要不断挖掘，最终的对错判断很可能会反转。有一天，我在旧金山到处乱逛，看到了711之类的便利店。我就会想，大家要买到自己想要的东西需要转多少个拐角？难道不能直接把想要货品直接放进购物车吗？比如按下购物车上的按钮，它就直接把想要的货品送过来。毕竟叫“便利”店嘛，就得足够便利。所以我们在华盛顿特区推出了这项服务，在路上投放了大约十辆这样的卡车，里面装了大约250个吐货口。刚开始的情况很糟糕，根本就没多少人来买。于是我们意识到自己在下意识地找痛点，并不清楚便利店的核心吸引力是什么。我们的卡车不卖烟、不卖啤酒、不卖豌豆泥，我们不清楚大家最想买哪些商品。但说实话，工会的力量太强了，所以我天然地认为别用人工是最安全、成本最低的。我们很出色地解决了这个非经济学层面的问题，实现了便捷交付。但结果呢？我们做了Uber Spot还是什么，但跟点对点配送的Uber Direct一样，刚起步就表现不好。也就是说，消费者并没这方面需求，企业才有这种需求。2014年我们刚做尝试时，就没找到市场需求。后来我们持续更新了15个版本，最终发现外卖业务的表现才更出色，也拥有可靠的经济回报。这是个很酷的问题，我们可以用这些工具来支持夫妻店，让他们具备跟大企业竞争的资格。我们可以把房地产因素排除在外，让店面选址不再决定一切。只要你的餐食好吃，就完全可以吸引更多顾客。所以我觉得这是个很有趣的问题，真正促进当地经济发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：Uber Eats最终在Uber的危机时刻拉了母品牌一把，现在业务规模发展到多大了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们是2015年12月在多伦多推出了这项服务，大约两小时内销售额就达到了2万美元，简直是疯狂。我们很快意识到这个路子是对的，而且经济效益很好。我在Uber待了六年左右，用了一年半左右才把这个项目真正做起来，并在四年半之后把销售额做到了200亿美元。必须承认，Uber非常擅长扩大业务规模，但竞争激烈的市场上其他友商也做得不错。我们击败了很多对手，也有一些对手确实压我们一头。目前业务规模正在向着800亿美元迈进，时间才过去了短短四年半。我想COVID在其中也发挥了很大的推动作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：众所周知，您曾经反对麦当劳加入Uber Eats。能分享一下这个故事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这事说起来就有意思了。或许有时候不想太多反而会让人意外找到正确的原因。我们发布的Uber Eats在全球范围内获得了广泛成功，而其中的基本愿景很简单：让小餐馆也能跟大规模连锁餐厅正面竞争。以巴黎来说，大家去巴黎旅行肯定不想吃大牌连锁餐厅，而更想发掘本地特色小店。这是现实需求，我们也决定参与其中。但后来麦当劳联系了我们，表示想跟我们一起做外卖业务。我们拒绝了，哪怕对方强调他们的日均消费者高达8000万。在拖了四、五个月之后，我们团队觉得我肯定是疯了，他们想促成这件事、而且愿意为之倾力投入。最终，我们还是跟麦当劳建立了独家合作关系，获得了大量连锁店客户。那时候大家都担心每单收益还能不能保证，毕竟订单规模到了单日几千万级别，这肯定是笔大钱。面对现实问题，Uber的企业文化就是缩小配送半径、在必要时提高某些食物的价格，反正总有办法解决的。三个月之后又有新的问题出现，业务再次陷入困境……总之很多同事觉得我在跟麦当劳合作方面太固执了，但我觉得最终至少还是达成了一笔很棒的交易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你一直很关注毛利率，能不能具体说说？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是这样的，当然这只是我的评判标准之一。当然，也有不少企业本身很好，但毛利率也不高，比如说好市多、沃尔玛之类。亚马逊也有类似的情况。但总的来说，高毛利率加上相对较低的客户流失曲线，对企业来说肯定是个很健康的运营信号。毕竟生意的本质就是增加价值，这就像一块客观的试金石。我们在开展新业务的时候肯定也受到过毛利率问题的困扰，比如刚开始先试试毛利率40%的方案，发现可行再试着提升到60%——这时候商家就觉得不能接受了，大家再坐下来交流。至于离岸外包公司，他们的毛利率是多少？查了一下，大概在20%，而且已经运营了很长时间。那按这个规律来讲，我们的毛利率最终也将不可避免地从40%下降到20%，除非真能找到差异化的突破，否则必然要陷入这个巨坑。所以我认为毛利率只是个很粗糙的指标，远不能算是完美的工具。但至少它可以是种快速高效的过滤器，可以考查突然跳出来的想法能不能通过初步评估。比如说核算之后发现毛利率很低，就只能通过后续销量来弥补，那这事恐怕就不大行得通。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到“不输”是获得成功的先决条件，能不能给我们具体解释一下这个理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们的科技文化是由投资者来建立投资组合，很多叙事是由投资者掌控。坦率地讲，创始人肯定也会参与其中。这种情况当然是比较理想的，只是我们没法确定自己会不会有这份运气。如果真说自己人生中只有一次尝试的机会，那我肯定不会轻易去行动，必须得三思而后行。虽然没有这方面数据做支撑，但我发现自己朋友圈里的企业家和最出色的创业者，会审视自己做决策时的风险状况，并在整个过程中都持续做出均衡和积极的决策调整。很多时候我们会忘记决策背后是对应着风险的。这里还有很多可以讨论的部分，因为我觉得用高风险决策最终取得成功是种特别不可取的文化现象，连培训当中都会认可这种思路。毕竟高风险决策必然带来巨大的波动，这对创始人最重要的特质——也就是坚持下去的能力是种直接挑战。大多数人在寻求最佳时机、与客户建立良好关系和将合适产品推向市场之前，就已经放弃退出了。而科技行业瞬息万变，我们确实可能在短时间内从平凡之人变成技术英雄，但大概率这会是个漫长的过程，得先活下来才能谈成功。而当前我们正身处炒作周期，每个人都想尝试、全力投入，但却没意识到客户会一直在，希望自己的问题能得到解决。总之，生存是一切的前提，我们要尽量别把业务发展置于危及的境地。当然，我也不是说完全拒绝任何冒险，这是个需要认真权衡利弊的大问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快问快答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您肯定也经历过失败，能不能分享一条从痛苦经历中汲取到的教训？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我还是认为多花点时间提前思考，可以避免后续的很多麻烦。我之前确实做过一些尝试，但结果一般就不细聊了。在2001年互联网泡沫破裂之后，我曾打算筹资创办一家公司、而且是能赚钱的公司，成果就是Scour。坦白讲，当时我没在科技行业发现什么好机会，所以我开始在网上卖二手高尔夫球杆，还真赚了不少钱。那会我才22岁，考虑得并不周全，而且我的预期也不高，因为我觉得这生意什么人都能做。但我确实赚了很多钱，甚至想过把全美国所有的二手高尔夫球杆都买下来，直接操控整个市场的交易价格。我太年轻、太自负了，根本没认真思考过这件事的可行性。总之我就这样进入了这个行业，还靠这个赚了几百万美元。但整个过程都让我很痛苦，因为&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您对招聘人才和组建团队很有见解，能不能聊聊自己的理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：最近我对这个问题又有了更具体的理解。比如某些职位，就必须要有对当前市场丰富的观察和理解。毕竟市场发展太快，没时间慢慢去培训，所以要找的就是那些哪怕其他条件弱些、但真正理解市场和能够跟客户建立良好关系的人。有了这个前提，其余的部分才能跟公司共同成长，并建立起理想的职业发展轨迹。当然，这类职位只能在公司中只占5%，但它们对于产品的快速上市非常重要。比如在面试当中，我就只考查三点：对于解决问题是否抱有足够的好奇心，是不是擅长把自己的想法准确表达出来，还有能不能很好地跟其他人合作、特别是扮演好领导者。我相对不那么看重专业知识，毕竟我自己肯定有能力边界，不可能在所有专业知识上都做出准确判断。但只要能成功做好这几点，对方的成功几率就相当高。面对世界的持续变化，我们需要的就是具备极强适应能力的人。以Uber Eats为例，当初组建项目管理团队时，我总会通过招人把团队设计成一个优势互补扔 机体，同时尽量减少团队中除运营以外的大部分冲突。而且从一无所有到高达200亿美元的估值，我的这个理念始终没有动摇。我一直坚信团队成员间了解彼此的优势和劣势，而且能够相互弥补，这比传统上的各种考核标准都要靠谱。换句话说，我必须得学会相信员工，因为我不可能亲自掌控一切。当然，人事系统是非常复杂的，不可能基于我简单的几句话就生搬硬套。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：在日常生活和工作当中，你发现了哪些AI应用方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：说实话，我在加入Scale之前是在消费电子领域工作，也参与过政府层面的一些应用项目。AI这个领域发展太快了，每当有新概念出现我都会认真学习，也会向公司里的其他同事请教技术细节，比如说数据和产品的技术特征。但他们的时间也有限，更多新概念还得靠自己主动学习。大家可能不相信，我的主要工作并不是处理跟AI相关的工程问题，而是管理这个组织。为了避免频繁打扰同事，很多时候我也会直接跟AI学习，在上下班的路上跟它聊天。这已经变成我生活中的一种习惯，也是我从业以来见证过的最不可思议的奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如之前采访Perplexity创始人的时候，对方就介绍他们要求同事在提出任何问题之前，必须先请教AI。那时候这还是种很疯狂的工作方式，但现在看来他们的领先恰恰体现在这里。至于在工作当中，我会上传一份内部文件，然后边亲自阅读边比较它的提炼结果。让人震惊的是，AI的表现真的非常出色，而且帮我节约了大量时间。在大规模组织中，我们经常会遇到这样的难题：我不知道你想让我说什么，我也不知道自己需要了解什么，这就导致大家各有议程、自说自话。那面对这样的传播挑战，AI确实能帮上大忙，太神奇了。我现在会用它来处理法律文件，比如快速了解对手打算怎么对付我、我又该怎么应对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您还有什么想跟听众们分享吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有啊。我想向大家强调Scale团队的卓越贡献。我们一直非常努力，持续为客户提供巨大的价值。任何语言在这份努力面前都显得苍白无力，更无法体现客户在此基础之上解决的无数问题。我认为这份付出配得上一切尊重和回报。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最近发现什么自己特别喜欢的产品了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：应该说是Veo 3吧，虽然不算全新产品。高中时我曾梦想当个编剧，还认真写过剧本。所以这次我找出第一页、拍下照片，再上传到Veo 3。结果真的让我震惊，居然一张剧本照片也能生成相应的视频画面。现在我在考虑怎么把这些工具用来生成家庭录像，再利用其他工具让内容更加生动。虽然还有进一步迭代的空间，但这种体验真的很有趣。这类工具真的会改变人们的情感生活，比如让祖父母、亲戚或其他很久没见的人在照片中动起来，这会产生很大的情感冲击。训练出这套模型的技术人员很厉害，而帮助他们做专家级数据标注的服务者也很厉害。这种直接把文字转化成光景的能力很棒，也很酷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最喜欢的人生格言是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：终点永远不是终点。这是我最喜欢、也深深牢记心中的格言。这跟之前关于生存才是第一要务的观点差不多，只有先活下来才有机会获得成功。纵观自己的创业历程，我觉得这条的指导意义最大。每个人都会经历艰难的旅程，但只要能在这段旅程中坚持五年，那大家的精神承受力绝对会比99.9%的人强。更具体地讲，我们在努力工作时会深切感受到这句话的意义。有时候我们觉得自己太累了，想要停下来，但事实上只要继续前进，似乎就又可以坚持下去了。我牢记这句话，提醒自己任何一个节点都不是真正的终点，仍然有更远的标的有待探寻。所以无论当下的解决方案到底完不完美，我们都可以先勇敢接受，然后抖擞前行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果想要与您交流或者了解更多关于Scale的信息，应该怎么安排？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以，大家可以关注我的邮箱，随时了解最新动态。如果是招聘方面的诉求，可以直接访问scale.com、进入我们的招聘页面，目前公司开放了250个空缺职位。我们的业务仍在扩展，包括应用程序业务、数据业务和服务业务都在疯狂增长。我们需要更多人手来帮助我们推进这段旅程。我们还刚刚跟政府签下了巨额合同，金额是21亿美元——而且不是一份，而一个月内签了两份。我们的政府业务做得很好、企业业务做得很好、国际政府业务同样做得很好。公司市场需求很大，也让不少销售人员拿到了丰厚的佣金。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&quot;&gt;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</link><guid isPermaLink="false">https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</guid><pubDate>Tue, 10 Feb 2026 11:43:36 GMT</pubDate><author>核子可乐,Tina</author><category>生成式 AI</category></item><item><title>Java探索载体类以扩展面向数据编程</title><description>&lt;p&gt;OpenJDK的&lt;a href=&quot;https://openjdk.org/projects/amber/&quot;&gt;Amber项目&lt;/a&gt;&quot;发布了一份全新的设计说明，名为“&lt;a href=&quot;https://openjdk.org/projects/amber/design-notes/beyond-records&quot;&gt;Java面向数据编程：超越记录类（Record）&lt;/a&gt;&quot;”，阐述了一种探索性的方案，以便将类似记录类的特性拓展至更灵活的类设计中。该文档引入了载体类（carrier class）与载体接口（carrier interface）的概念，目标是提炼记录类的核心优势并进行通用化适配，同时不再强加严格的表述规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Java 16引入了记录类，为不可变数据载体的建模提供了简洁的方式。如下这种记录类的声明：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;record Point(int x, int y) { }
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译器会自动为其生成规范的构造器、访问器方法，以及equals、hashCode和toString方法的实现。记录类还支持解构（deconstruction）模式，可以配合instanceof和switch关键字使用。结合密封类与模式匹配特性，记录类能实现Java中代数数据类型的建模。例如，HTTP客户端或网关可以按如下方式定义不同的响应类型：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;public sealed interface HttpResponse permits HttpResponse.Success, HttpResponse.NotFound, HttpResponse.ServerError {
    record Success(int status, String body) implements HttpResponse {}
    record NotFound(String message) implements HttpResponse {}
    record ServerError(int status, String error) implements HttpResponse {}
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这样的响应类型层级，我们可以通过穷举式模式匹配进行统一处理：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;static String handle(HttpResponse response) {
   return switch (response) {
       case Success(var code, var body) -&amp;gt; &quot;OK (&quot; + code + &quot;): &quot; + body;
       case NotFound(var msg) -&amp;gt; &quot;404: &quot; + msg;
       case ServerError(var code, var err) -&amp;gt; &quot;Error (&quot; + code + &quot;): &quot; + err;
   };
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此示例中，编译器会强制检查是否覆盖了所有允许的响应类型。若新增一种响应类型，必须同步更新该switch表达式，从而降低不完整错误处理的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期的一次讨论中，甲骨文公司的Java语言架构师&lt;a href=&quot;https://www.linkedin.com/in/briangoetz/&quot;&gt;Brian Goetz&lt;/a&gt;&quot;指出，这些特性的组合虽然能实现强大的数据建模能力，但实际落地却经常会受到长期形成的面向对象设计习惯制约。他发现，即便现代语言特性已能大幅减少间接代码，开发人员仍会习惯性地设计用于中介数据访问的API。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明重点聚焦于记录类无法适用的场景。实际开发中，许多数据类型需要派生值或缓存值、可选的内部表示形式、可变性或继承特性。在这种情况下，开发人员只能退而求其次，使用传统类，并重写大量的样板代码。文档将这种转变形容为“断崖式回落”，对记录类的基准模型做微小调整，就会导致代码量的大幅增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了缓解这一问题，文档提出了载体类的设计思路。载体类以类似记录类头信息的状态描述作为开头，其余行为则与普通类完全一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，状态描述用于定义类的逻辑组件，编译器可基于这些组件自动生成访问器、对象方法及解构模式。与记录类不同，载体类不要求将所有状态仅存储在这些组件中，这也是其核心灵活性所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种灵活性能实现记录类难以表达的设计模式，例如缓存派生值的场景：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
    private final double norm;

    Point { norm = Math.hypot(x, y); }
    double norm() { return norm; }
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此例中，派生值norm在构造阶段计算完成，并且未纳入状态描述，但该类仍能借助编译器为其组件自动生成的方法，减少样板代码编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;载体类同样设计为可与模式匹配深度集成，用法与记录类一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;if (obj instanceof Point(var x, var y)) {
    // use x and y
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设计说明中还进一步探讨了载体类与未来重构特性的兼容性，例如，针对记录类的&lt;a href=&quot;https://openjdk.org/jeps/468&quot;&gt;JEP 468&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除载体类外，该提案还引入了载体接口的概念，接口可声明自身的状态描述，并且所有实现类都能参与统一的模式匹配：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;interface Pair&lt;t, u=&quot;&quot;&gt;(T first, U second) { }

switch (pair) {
    case Pair(var a, var b) -&amp;gt; ...
}
&lt;/t,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计能简化日常开发中常见的元组式抽象，同时保留Java强类型的优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明将载体类置于Java向面向数据编程整体转型的背景下，通过结合记录类、密闭类型、模式匹配，再加上潜在的载体类，Java正逐步引导开发者直接建模数据结构，而非依赖层级繁杂的API。Goetz认为，当前的核心挑战在于，帮助开发者意识到，在将“数据”作为首要抽象时，大量的支撑性代码都可被省略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，“超越记录类”还属于探索性的文档，官方尚未公布具体的语法定义、JEP提案及版本发布时间表。但这份文档释放了明确的信号，那就是Amber项目将持续推进相关研发，进一步减少Java的样板代码，并将现代语言特性拓展至更复杂的类设计中，而这些探索，也许会将影响未来版本中Java开发者构建以数据为核心的 API的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/java-beyond-records/&quot;&gt;Java Explores Carrier Classes to Extend Data-Oriented Programming Beyond Records&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</link><guid isPermaLink="false">https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</guid><pubDate>Tue, 10 Feb 2026 11:26:42 GMT</pubDate><author>作者：A N M Bazlur Rahman</author><category>编程语言</category></item><item><title>谷歌推出托管AlloyDB连接池</title><description>&lt;p&gt;谷歌云&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/release-notes#December_18_2025&quot;&gt;正式发布&lt;/a&gt;&quot;AlloyDB for PostgreSQL通用托管连接池，将类似PgBouncer的功能直接集成到数据库服务中。按照谷歌的说法，与直接连接相比，这一特性能够提供3倍多的客户端连接和高达5倍的事务吞吐量，帮助开发者解决了运行高并发工作负载时面临的扩展挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接池并不是什么新鲜事。多年来，为了重用数据库连接而不是为每个请求创建新的连接，开发者们将&lt;a href=&quot;https://www.pgbouncer.org/&quot;&gt;PgBouncer&lt;/a&gt;&quot;或&lt;a href=&quot;https://www.pgpool.net/docs/latest/en/html/&quot;&gt;pgpool&lt;/a&gt;&quot;作为单独的基础设施进行了部署。现在，AlloyDB可以自动完成这些工作了。开发者可以通过控制台复选框或API调用来启用它，连接池使用6432端口，而常规连接使用5432端口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;托管连接池会缓存预先建好的连接，将它们分配给传入请求，并在使用完成后将它们返回给连接池，而不是关闭它们。谷歌表示，这可以消除“运维负担”，作为AlloyDB实例的一部分，连接池会自动升级和扩展。连接池和数据库之间的通信在谷歌云的网络内运行，可能比外部连接池设置的延迟小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Cloud Run或Cloud Functions上的无服务器部署，其优势更为显著。这些平台会启动多个实例，每个实例都会打开数据库连接，在流量高峰时往往会超出PostgreSQL的连接限制。对于这种情况，连接池是一个很好的缓冲，它利用现有的连接处理请求，而非强制数据库同时处理数百个新的连接尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UKG高级首席架构师Jeff Bogenschneider在早期测试期间描述了其影响：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;AlloyDB的架构使我们能够在单个集群中部署的数据库数量远超其他Postgres托管服务。此前我们曾担心连接限制问题，而托管连接池可以帮助我们确保全球的客户都能获得最佳的性能，让我们得以自由地扩展业务，而不用担心在高峰使用时段遇到连接限制问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;运行微服务的开发者应该考虑将应用端连接池与AlloyDB的托管连接池配对。在&lt;a href=&quot;https://medium.com/google-cloud/elastic-microservices-rigid-databases-connection-exhaustion-8cdc558f212a&quot;&gt;Medium&lt;/a&gt;&quot;上，Adarsha Kuthuru和Kumar Ramamurthy详细描述了这种“双池”模式：像HikariCP这样的应用连接池为每个实例维持5-10个到AlloyDB连接池的连接，后者通过多路复用将这些连接连接到数量更少的后端数据库连接。这个方案可以避免为50个微服务实例各建立20个连接时，1000个并发连接冲击数据库的场景。作者建议为每个vCPU配置15-20个连接器连接，并协调各层的超时设置，避免连接重置错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能提供两种连接池模式。事务模式（默认）通过为每个事务分配独立的连接来最大化可扩展性；会话模式完全兼容PostgreSQL的功能。开发者可以通过AlloyDB API中的标准PgBouncer参数调整连接池规模、超时设置及空闲阈值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能存在一些限制。托管连接池不适用于AlloyDB Auth Proxy或语言连接器——开发者需要直接连接。这妨碍了依赖身份验证代理进行凭据轮换或简化TLS配置的部署模式。在2024年11月前部署的实例上启用连接池功能时，由于要更新VPC设置，会引发短暂的网络中断（持续时间少于15秒）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于已经单独运行PgBouncer的开发者而言，迁移至托管连接池主要在于整合基础设施——减少一个需要打补丁的组件。对于新增部署，尤其是无服务器或高并发工作负载，启用该功能所需的投入极少，却能防患于未然，在扩展问题爆发前将其及时化解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌提供了&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling&quot;&gt;配置托管连接池&lt;/a&gt;&quot;的文档和在现有实例上&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling#enable-managed-connection-pooling&quot;&gt;启用该特性&lt;/a&gt;&quot;的最佳实践。对于双池模式，发表在Medium上的博文提供了一份部署指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&quot;&gt;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</link><guid isPermaLink="false">https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</guid><pubDate>Tue, 10 Feb 2026 11:21:10 GMT</pubDate><author>Steef-Jan Wiggers</author><category>Google</category><category>大数据</category></item><item><title>突发！继杨格过劳病离职后，xAI又一位联创出走，疑单独创业</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;刚刚，xAI 又损失一位华人联创&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几个小时前，全球首富马斯克旗下人工智能公司 xAI 再迎联合创始团队成员离职。xAI 公司联合创始人 Yuhuai (Tony) Wu（音译：吴玉怀）在x上发文称，今天正式从 xAI 辞职了。他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这家公司——以及我们之间如同家人般的情谊——将永远铭刻在我的记忆中。我会深深怀念这里的人们、作战室，以及我们并肩作战过的所有战役。&amp;nbsp;我的人生新篇章即将开启。这是一个充满无限可能的时代：一支配备人工智能的小团队可以移山填海，重新定义一切皆有可能。”&amp;nbsp;致埃隆 &lt;a href=&quot;https://x.com/elonmusk&quot;&gt;@elonmusk&lt;/a&gt;&quot;，感谢你们相信我们的使命，也感谢你们带给我们这段毕生难忘的旅程。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f34f72fba29a82e49db7cc369f6d2cdc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 LinkedIn 资料和媒体相关报道，吴是著名人工智能研究者与企业家，因联合创立 xAI 而广为业界所知。吴在 xAI 的任职期间被视为技术与研究团队核心成员之一，负责推动推理与数学智能相关方面的研发工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据吴的LinkedIn个人资料显示，在加入该公司之前，他曾在谷歌工作近两年，担任研究科学家（Research Scientist），参与与神经网络、数学推理相关的大型语言模型等研究项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;博士阶段曾分别在 DeepMind 工作约 11 个月，并在 OpenAI 担任过科研实习岗位（数月）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在学术贡献上，他是多个顶级国际会议论文的作者或共同作者，例如关于大语言模型与数学推理、定理证明等的研究成果。其部分成果被视为推动 AI 数学与符号推理能力前沿的重要贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e3/e3feb6b2078bfe074af6af03e07f2213.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，吴玉怀是过去一年中第四位离开公司的联合创始人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他离职之前，xAI 公司的另外几位创始人 Christian Szegedy 于去年2月离职，Igor Babuschkin 于去年8月离职，而杨格上个月表示，由于健康原因，他已暂时退出公司事务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给马斯克工作，压力太大？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先提出离职的是 Christian Szegedy，但他并没有在x上透露过多关于未来去向的信息，也未明确解释离职原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他离职后，去年8月的 Igor Babuschkin 在离职时在x上发了长文感慨和马斯克一同创业的时光，他首先回顾了2023 年初，几位创始人创建公司的初心。他们确信：人类正在逼近通向超级智能的“配方”。一切迹象都在表明，AI 很快就可能在推理能力上超越人类。那随之而来的问题是——人类该如何确保，这项技术被用于善的方向？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多年来，马斯克始终警示强大 AI 所潜藏的风险。正是在这样的背景下，他们发现彼此拥有完全一致的愿景：让 AI 造福全人类。于是，他们集结了一群志同道合的工程师，xAI 正式启程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Igor Babuschkin还首次揭秘的创业时的艰辛，并称自己从马斯克身上学到了两条无价的准则：&lt;/p&gt;&lt;p&gt;第一，永远不要畏惧亲自下场解决最棘手的技术问题；&lt;/p&gt;&lt;p&gt;第二，保持一种近乎偏执的紧迫感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在帖子的结尾，Igor Babuschkin 表达了自己离职的根本原因不是挫折或失败，而是个人使命的聚焦与升华。他表示自己已经创办了新公司，名为： Babuschkin Ventures，希望获得更多关注和支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;帖子翻译如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我依然清楚地记得第一次见到埃隆的那一天。我们围绕人工智能以及它可能塑造的未来，连续聊了好几个小时。那次交谈中，我们达成了一种几乎无需言说的共识：这个世界，需要一家使命不同、方向不同的全新 AI 公司。&amp;nbsp;构建真正推动人类前进的人工智能，是我一生的梦想。&amp;nbsp;苏联解体后，我的父母离开俄罗斯联邦，踏上移民之路，只为给孩子寻找一个更好的未来。作为移民，生活从来谈不上轻松。但即便在最艰难的时刻，他们依然坚信：人类的价值是无价的——勇气、同理心，以及对理解世界的永恒好奇。&amp;nbsp;童年时期，我仰慕理查德·费曼、马克斯·普朗克这样的科学家。他们不懈地推动物理学的边界，只为更接近宇宙的真理。后来，我在 CERN 攻读粒子物理博士，满怀激情地希望自己也能为这一使命贡献力量。然而，寻找“新物理”变得越来越困难——需要更庞大的对撞机，却换来越来越稀少的突破。&amp;nbsp;于是我开始思考：解开宇宙之谜的钥匙，或许并不是更大的对撞机，而是超级智能。&amp;nbsp;AI 是否能够构建一套自洽的量子引力理论？AI 是否有一天能证明黎曼猜想？&amp;nbsp;2023 年初，我逐渐确信：我们正在逼近通向超级智能的“配方”。一切迹象都在表明，AI 很快就可能在推理能力上超越人类。那随之而来的问题是——我们该如何确保，这项技术被用于善的方向？&amp;nbsp;多年来，埃隆始终警示强大 AI 所潜藏的风险。正是在这样的背景下，我们发现彼此拥有完全一致的愿景：让 AI 造福全人类。于是，我们集结了一群志同道合的工程师，xAI 正式启程。&amp;nbsp;xAI 的早期并不轻松。质疑者告诉我们：我们入局太晚了，从零开始打造一家顶级 AI 公司几乎不可能。但我们选择相信“不可能”。&amp;nbsp;从零创业，意味着事无巨细、亲力亲为。最初，我亲手搭建了公司大量底层工具，用于启动和管理模型训练任务。后来，我负责统筹公司相当一部分工程工作，涵盖基础设施、产品以及应用型 AI 项目。&amp;nbsp;xAI 的人，是我见过最投入、最坚定的一群人。&amp;nbsp;在血汗与泪水中，我们以惊人的速度建成了孟菲斯超级算力集群，并以前所未有的节奏交付了前沿模型。&amp;nbsp;从埃隆身上，我学到了两条无价的准则：第一，永远不要畏惧亲自下场解决最棘手的技术问题；第二，保持一种近乎偏执的紧迫感。&amp;nbsp;xAI 的执行速度，快到近乎疯狂。&amp;nbsp;业内资深人士曾断言：在 120 天内建成孟菲斯超级集群，根本不可能。但我们依然选择相信“不可能”。&amp;nbsp;在期限临近时，集群节点之间的 RDMA 通信频频出现诡异问题。埃隆决定亲自飞往数据中心，我们随即跟上。基础设施团队在深夜抵达孟菲斯，几乎没有休息，立刻投入排查。&amp;nbsp;在翻阅了数万行 lspci 输出后，我们终于锁定了罪魁祸首——一个错误的 BIOS 设置。埃隆一直陪着我们奋战到深夜。当训练任务终于跑通时，他在凌晨 4:20 发帖庆祝，那一刻我们忍不住大笑出声。&amp;nbsp;我永远不会忘记那一夜的肾上腺素飙升，也不会忘记那种“我们真的在一起并肩作战”的情感联结。那晚入睡时，我们都清楚地意识到：自己正身处人生中最激动人心的时刻。&amp;nbsp;我对 xAI 这个大家庭，怀有无比深厚的感情。&amp;nbsp;你们是我合作过的最投入、最顽强的一群人。能够如此迅速追赶并站上技术前沿，靠的不是奇迹，而是每一个人的拼劲与团队精神。&amp;nbsp;感谢每一位与我并肩走过这段旅程的人。我想向你们的付出、时间与牺牲致敬——这些从来都不容易。我会永远记得那些灯火通明的深夜，记得我们一起熬过的每一次极限冲刺。&amp;nbsp;今天，当我驱车离开时，心情就像一位送孩子远行上大学的父母——骄傲、欣慰，眼眶湿润。我会继续注视着这家公司成长、成熟。&amp;nbsp;迈向人生的下一章节时，我再次想起父母当年的移民选择——为了让下一代生活在更好的世界。不久前，我与“未来生命研究所”创始人 Max Tegmark 共进晚餐。他给我看了自己年幼儿子的照片，然后问我：“我们该如何安全地构建 AI，才能确保我们的孩子真正繁荣成长？”&amp;nbsp;这个问题深深触动了我。&amp;nbsp;在更早的职业生涯中，我曾担任 DeepMind 的 AlphaStar《星际争霸》智能体技术负责人，亲眼见证了强化学习在规模化后所释放的惊人力量。随着前沿模型在更长时间尺度、更广任务范围内变得愈发“具备代理性”，其能力也将不断放大——这使得 AI 安全研究变得前所未有地重要。我希望继续自己的使命：推动安全、对人类有益的人工智能。&amp;nbsp;今天，我正式宣布创立 Babuschkin Ventures，专注支持 AI 安全研究，并投资于推动人类进步、探索宇宙奥秘的 AI 与智能体系统初创公司。&amp;nbsp;如果你愿意交流，欢迎通过 ventures@babuschk.in 联系我。奇点正在逼近，但人类的未来依然光明。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再然后就是前不久，1月21日，xAI 的另一位联创 Greg Yang (音译：杨格）也在x上发文称已经离职。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨此前曾在微软公司工作，是马斯克 2023 年人工智能初创公司的创始成员之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨在x上发文表示，他可能在一段时间前感染了莱姆病，症状是在 xAI 高强度工作期间变得明显的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种疾病是由蜱虫叮咬引起的，会导致炎症。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/04/046ac82a85ffa5fa401966fa7d1e264a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨在x 上发文称，其实自己生病的症状在很久以前就已经感染了，只是一直到高强度投入 xAI 的研发构建、免疫系统被持续消耗之后，症状才真正显现出来。这里很容易读出他的言外之意——超高强度工作，伤害了身体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他表示从整体来看，反而觉得自己是幸运的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;莱姆病是一种严重的疾病，拖得越久，治疗难度越大。很多患者在五六十岁时才被发现，情况往往要艰难得多。它甚至可能让人长期卧床、丧失行动能力。而他，至少现在仍然可以正常生活，照顾好自己。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨还表示：“所以，尽管有人对我说不该把自己逼得这么狠。但我并不后悔。正因为我曾那样拼命，我才得以及早发现问题；而现在，我可以修复它——这样，当我重新站起来时，就能比以往走得更远。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得玩味的是，尽管&amp;nbsp;Igor Babuschkin离职后发表了长篇大论解释了离职原因，但在离职后，他也公开吐槽了科技公司对工程师缺乏耐心：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;许多AI公司未能给工程师足够的时间和心态去做出最好的工作，导致代码和系统不可靠。良好的公司文化，注重卓越、专注和足够休息，能带来更好的成果。早期Google就是这种文化的典范，创始人们应该借鉴他们的策略。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，就是今天刚刚宣布离职的吴，但从他发文中可以隐约提到的将开启人生新篇章，并表示这是一个充满无限可能的时代，一切皆有可能，外界猜测他离职的原因是要单独创业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界首富也睡过车间地板&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在科技圈乃至大众媒体中，马斯克既被视为颠覆行业的创新者，也常因其极端的工作和管理方式而成为争议焦点。无论是在特斯拉、SpaceX，还是他于 2022 年收购后的微博（Twitter，后更名为 X）、以及最新的 xAI，马斯克对效率、速度和结果的近乎苛刻追求，塑造了一种鲜明而强烈的企业文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克本人对生产和执行的标准极高，这一点体现在多个层面：无论是火箭发射、汽车量产，还是 AI 平台的快速迭代，他都要求以超出常规的节奏推进。对他而言，工作不是常规的职业任务，而是一种总体使命的极致实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克长期以身作则，亲自展示“全员投入”的文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在特斯拉 Model 3 产能冲刺阶段，他曾公开表示自己多次睡在工厂地板上，与团队同吃同住，以身作则推动生产进度。此举被他本人解释为希望自己的处境比其他员工更“糟糕”，以此激发团队极限投入的精神。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他接手推特后，类似的高强度工作节奏再次出现。据报道，高管和员工为了赶项目上线与平台改造，不得不在办公室过夜，有人甚至将办公室布置成临时卧室。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种文化也延续到了新的业务单位。在 xAI，有员工因此张贴自己连续 36 小时未睡工作的照片，并获得同行与马斯克本人的回应，成为“极致奉献”的象征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些事例并非孤立现象，而是马斯克管理体制的核心体现：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;全员以任务完成为唯一衡量标准，在不惜个人生活成本的条件下追求快速执行。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克对组织流畅和成本效率的执念，也体现在他接手推特后大规模裁员与重新设定公司节奏的做法上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他接管后短时间内削减了约 50% 的员工，以期通过快速精简来降低成本并重塑团队结构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，他的内部沟通中强调“长时间高强度工作是继续留任的前提”，并要求员工亲自回到办公室工作、放弃远程安排。这样的政策在推特内部引发了大量讨论与反弹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种以极限 KPI 和严格劳动投入作为衡量绩效指标的方式，反映出马斯克对“成果优先、短期快速推进”的坚定信念，但也因此产生了显著的压力文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期以来，马斯克管理方式的成功也伴随着争议。批评者认为他的严苛要求置工作效率于健康和心理福祉之上，尤其是在后疫情时代的职场环境中，这种风格显得格格不入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，马斯克在推特上曾要求员工在特定期限内选择接受“高强度工作”或离职与三个月遣散费的方案，这种二选一的选择在劳动力市场中引发了关于员工权利与企业管理伦理的广泛讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管马斯克支持者认为这种做法有助于推动快速创新和执行效率，但批评者指出，这种过度强调短期指标和工作时长的文化，可能会导致高离职率、身心健康问题，以及长期人才流失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管存在争议，马斯克模式背后却有其一致性逻辑：他不满足于常规的“业务增长”，而试图推动技术、生产、产品乃至整个人类文明的极限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论是加速电动汽车普及、实现火箭可复用、还是构建被他视为下一个关键技术节点的人工智能系统，所有这些目标在他眼中都不容许“慢与保守”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他本人也强调领导者的角色不仅仅是分配任务，更是“培养能思考的人”，希望员工不仅知道“做什么”，更要知道“如何思考”以解决复杂问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种极端使命感驱动的管理哲学，既是他能够成功推进多个行业边界的动力来源，同时也是造成高压力工作文化的重要根源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://yuhuaiwu.github.io/?utm_source=chatgpt.com&quot;&gt;https://yuhuaiwu.github.io/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.businessinsider.com/elon-musk-xai-loses-cofounder-tony-wu-2026-2?utm_source=chatgpt.com&quot;&gt;https://www.businessinsider.com/elon-musk-xai-loses-cofounder-tony-wu-2026-2?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/VLJ1Pkm0fYtW0iYTN9vg</link><guid isPermaLink="false">https://www.infoq.cn/article/VLJ1Pkm0fYtW0iYTN9vg</guid><pubDate>Tue, 10 Feb 2026 10:54:35 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>星海图首席科学家许华哲创业，刚获内部投资！目标：“让机器人做一道松鼠鳜鱼”？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，有消息称，星海图孵化首席科学家许华哲创业，新公司将会切入具身智能C端应用赛道，已获得星海图种子轮投资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“让机器人做一道松鼠鳜鱼”，是许华哲在多次公开提到的具身智能终极设想。他曾表示，处理活鱼、改刀、油炸到摆盘，其复杂的物理交互是验证机器人智慧程度的最好指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知情人士透露，为了更深入探索具身智能在 2C 领域的技术与应用，许华哲在2025年8月曾主动与星海图团队进行了沟通，表达了希望专注深耕这一方向的想法，星海图团队表示了支持，并对许华哲的新公司启动内部孵化。今年2月，星海图通过直接投资的方式，支持许华哲创立并运营新公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，星海图正通过一系列秘密投资，围绕“数据+应用”构建起一套闭环的生态，许华哲此次创业正是该生态布局在 C 端应用的重要一步。下一步，星海图将计划通过产业基金的方式参股或控股关键技术环节和应用方，整合上下游，为其具身智能产品的量产与商业化提供保障。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为具身智能领域的明星创业公司，星海图于2023年9月成立，连续完成A4 轮及A5 轮战略融资，合计融资金额超过1亿美元。2026年1月，公开信息披露星海图已经完成股份制改造。另据可靠消息透露，星海图已于近期完成新一轮融资，估值已突破100亿人民币。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EO57dfMthXCaFfjgajlL</link><guid isPermaLink="false">https://www.infoq.cn/article/EO57dfMthXCaFfjgajlL</guid><pubDate>Tue, 10 Feb 2026 10:34:59 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>为什么开发者放弃框架而选择原生 JavaScript</title><description>&lt;p&gt;本文最初发布于博客TheNewStack。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66cba611274058a8f7d62c7cf83eeb9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来自 Unsplash+&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前端开发者正在回归原生 JavaScript。以下是原生 API 和 AI 工具如何使原生 JS 成为框架疲劳的解药。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个人都累了，&lt;a href=&quot;https://thenewstack.io/javascript-framework-reality-check-whats-actually-working/&quot;&gt;框架疲劳不再只是一个梗&lt;/a&gt;&quot;：它是一种集体倦怠。曾经竞相掌握 React、Vue 和 Svelte 的开发者们，现在正悄悄回归他们曾经抛弃的简单性：原生 JavaScript。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Web 的天平正在向极简主义倾斜。原生浏览器 API 的兴起、注重性能的开发理念和 AI 辅助编码的浪潮，不仅让原生 JavaScript 开发再次变得可行，而且重新焕发了生机。这是在经历多年的&lt;a href=&quot;https://thenewstack.io/the-react-component-pyramid-scheme-an-over-engineering-crisis/&quot;&gt;代码膨胀&lt;/a&gt;&quot;、抽象概念和 npm 依赖噩梦之后的&lt;a href=&quot;https://thenewstack.io/stop-blaming-react-for-your-state-management-hangover/&quot;&gt;一剂宿醉解药&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;框架时代的临界点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多年来，框架一直是开发者的默认选择。它们承诺带来规范性、可扩展性和社区支持。但随着框架的发展，其复杂性也随之增加。打包器变得越来越重，构建时间不断增加，运行“Hello World”项目的一行代码平均就需要数兆字节的依赖。开发者开始质疑：所有这些脚手架真的值得吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题不在于框架本身，而在于&lt;a href=&quot;https://thenewstack.io/how-to-build-framework-agnostic-uis-with-web-components/&quot;&gt;围绕它们发展起来的文化&lt;/a&gt;&quot;。每个月都有新的框架涌现，每个都声称修复了上一个框架的问题。企业为了跟上不断变化的生态系统，重构了整个产品。结果呢？无休止的迭代，&lt;a href=&quot;https://www.atlassian.com/agile/software-development/technical-debt&quot;&gt;伪装成创新的技术债务&lt;/a&gt;&quot;，以及陷入重学循环的开发者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到 2025 年，人们意识到：Web 不需要另一层，它需要的是重置，而这个重置以原生 JavaScript 的形式出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;原生 API 已经成熟&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现代浏览器不再是过去那个不稳定的沙箱。在过去的几年中，像 Fetch、&lt;a href=&quot;https://thenewstack.io/web-components-are-the-comeback-nobody-saw-coming&quot;&gt;Web 组件&lt;/a&gt;&quot;和 ES 模块这样的 API 已经发展为成熟的生产级工具，取代了框架曾经提供的功能。曾经那些需要 React 钩子或状态管理库才能完成的任务，现在使用原生解决方案，只要几行简洁的代码就能顺利运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;特别是 Web 组件标准改变了游戏规则。它为开发者提供了框架的模块化和封装性，而又不会有框架锁定的问题。结合 Shadow DOM、自定义元素和模板字面量，开发者现在可以构建可重用、自包含的小部件，它们可以在任何地方运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种成熟度的提升意味着开发者终于可以使用浏览器提供的原生功能来构建动态、可维护的响应式界面。由依赖项、构建工具和样板代码带来的“框架税”不再是强制性的。选择原生 JS 不是因为复古，而是因为它再次变得高效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;性能成为新货币&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今的 Web 讲究速度。&lt;a href=&quot;https://arounda.agency/blog/ux-statistics&quot;&gt;用户期望近乎即时的交互&lt;/a&gt;&quot;，搜索引擎算法会惩罚速度缓慢的页面。严重依赖框架的应用可以做得很复杂，但它们难以提供一致的性能，尤其是在移动设备上。开发者重新认识到，最好的优化不是添加另一个优化库，而是编写更少的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年，&lt;a href=&quot;https://thenewstack.io/5-technical-trends-to-help-web-developers-stand-out-in-2025/&quot;&gt;原生 JavaScript 重新进入主流&lt;/a&gt;&quot;，主要是因为应用程序启动更快、渲染更快、调试更容易。没有庞大的捆绑包、水合脚本或协调算法，加载时间大幅下降。每节省一千字节，就能留住一个用户。这种转变是务实的：响应速度提高 50 毫秒的价值远高于 JSX 语法糖或响应式绑定带来的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并非意味着框架的死亡，它们仍然主导着企业环境，但在那些注重敏捷性和性能而非遗留架构和抽象概念的项目中，Web 的天平已经向“无框架区”倾斜。这剂宿醉解药不是关于反叛，而是关于清晰度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 工具使简单再次强大&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讽刺的是，&lt;a href=&quot;https://thenewstack.io/how-ai-changes-developer-portfolios/&quot;&gt;AI 加速了回归简单的过程&lt;/a&gt;&quot;。现在，开发者使用基于 AI 的编码助手来生成样板代码、调试程序和建议简洁的原生代码。语法越直接，AI 就越有效，而框架的专有约定和抽象层，常常使这些系统感到困惑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 AI 处理那些重复的模式，开发者不再需要框架来提高生产力。一个简单的提示就可以利用原生 JS 直接构建响应式 UI 或实现事件处理，完全避免了框架带来的认知负担。突然之间，“框架节省时间”的旧论点不再成立。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，&lt;a href=&quot;https://devoxsoftware.com/blog/through-the-code-maze-ai-vs-manual-refactoring/&quot;&gt;AI 辅助重构&lt;/a&gt;&quot;使梳理遗留框架变得更容易。团队可以逐步迁移，用原生等价物替换框架组件。这不是对早期 Web 的怀旧，而是在智能工具盛行的时代有意识地回归本源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;微前端和无构建架构的兴起&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;越来越多的现代项目采纳了&lt;a href=&quot;https://thenewstack.io/the-case-for-microfrontends-and-moving-beyond-one-framework/&quot;&gt;微前端&lt;/a&gt;&quot;原则：独立的小型 UI 模块单独加载并通过共享契约通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种模块化转变也符合现代容器的安全实践，其中的独立单元在部署和更新时可以施加更严格的控制，最小化攻击面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同样，这种理念与原生 JS 完美契合。没有集中化的构建系统或复杂的依赖树，开发者可以按模块推送更新，并保持各团队的灵活性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无构建运动与此相辅相成。像 &lt;a href=&quot;https://thenewstack.io/how-vite-became-the-backbone-of-modern-frontend-frameworks/&quot;&gt;ESBuild 和 Vite&lt;/a&gt;&quot; 这样的工具已经将编译简化到了几乎看不见的程度，但最终目标是完全不需要构建步骤。原生模块导入使得这一愿景成为现实。开发者可以直接从编辑器将更新推送到生产环境，无需等待管道进行转译或打包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变&lt;a href=&quot;https://thenewstack.io/what-is-lightweight-software-revisiting-the-definition/&quot;&gt;重新定义了“轻量级”的真正含义&lt;/a&gt;&quot;。2026年，现代的原生 JavaScript 项目绝不是原始粗糙的，而是精准如手术刀的。它只恰到好处地完成需要做的事，不多也不少。在一个痴迷于速度和控制的世界里，这不仅仅是优雅，还是竞争优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;学习曲线倦怠和开发者自主性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者们已经筋疲力尽。每隔几个月，就有一个新的框架承诺带来救赎，但结果只是用另一个抽象替换前一个。紧跟“最新”发展所带来的认知负担变得不可持续。原生 JavaScript 提供了一个减压阀，一个不会随着下一个 GitHub 公告而过期的公共基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不需要记住一个新的钩子系统、状态 API 或指令语法。你只需要理解这门语言，重拾自主性，让编程创作的掌控权回到开发者手中。他们可以专注于解决问题，而非死记硬背语法模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着教育系统的跟进，JavaScript 训练营和高校开始重新强调基础知识。其结果将是：&lt;a href=&quot;https://darktechinsights.com/hidden-dangers-of-frameworks/&quot;&gt;依赖框架的开发者减少&lt;/a&gt;&quot;，能够在核心层面推断性能、结构和行为的开发者增多。这种重置既是文化的，也是技术的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;生态系统再平衡&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回归原生 JavaScript 并不意味着框架的灭绝，但它确实重新定义了它们的目的。框架正在演变成可选层，而不是默认配置。它们的存在是为了解决特定的大规模问题，而不是嵌入到每一个登录页和小部件中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React 、Vue 和 Svelte 正在悄悄地精简冗余，提升互操作性。生态系统正在围绕原生标准而不是专有语法凝聚共识。框架作者如今秉持“渐进式采用”的设计理念，这意味着开发者可以选择某个框架而不被锁定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种再平衡也反映了其他技术领域的发展轨迹。正如DevOps逐渐从工具导向转向&lt;a href=&quot;https://thenewstack.io/best-practices-for-adopting-a-devops-culture/&quot;&gt;文化导向&lt;/a&gt;&quot;，2026年的前端开发也将更注重使用效率而非工具选择。原生 JS 并非一种厌弃，而是重新校准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;框架宿醉不是永久的，它是一个警钟。开发者们终于意识到，进步不是关于抽象的堆叠，而是掌握它们下面的基础知识。原生 JavaScript，曾经被认为“太简陋”，现在已经演变成了一个更简洁的 Web 背后的强大引擎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年，用原生 JavaScript 编写代码并不意味着你在倒退，反而意味着你在前进——清晰、可控以及一个五年后仍然有意义的代码库。框架将继续演变，工具将继续增多，但解决方案将保持不变：剥离掉所有不必要的部分，回归到真正支撑 Web 运行的核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://thenewstack.io/why-developers-are-ditching-frameworks-for-vanilla-javascript&quot;&gt;https://thenewstack.io/why-developers-are-ditching-frameworks-for-vanilla-javascript&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UJtGxoHgrizoaUI1HsdG</link><guid isPermaLink="false">https://www.infoq.cn/article/UJtGxoHgrizoaUI1HsdG</guid><pubDate>Tue, 10 Feb 2026 10:29:24 GMT</pubDate><author>Alexander T. Williams</author><category>架构/框架</category></item><item><title>字节发布最新模型 Seedream 5.0，但没打过Nano Banana Pro？</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布时机把握得很好，在所有人都被 Seedance 的视频热度吸引时，字节又推出了全新文生图模型Seedream  5.0。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该版本集成了网络搜索功能，并支持 2K 原生输出，使其成为 Nano Banana Pro 的高性价比替代方案。该模型现已上线 CapCut、剪映和 Skylark平台，并在即梦AI平台开启灰度测试。目前在 CapCut上，有限时20次免费图片生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;官方表示，新版本在理解图像内容、生成速度和视觉效果方面均有显著提升。它能更精准地解读上下文、风格和细节，从而减少重复编辑的需求，在Dreamina 中创建图像更加流畅可靠。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/21/21e169e419fa048792e50534cf64f6f4.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，在生成后，用户可以通过交互式笔刷编辑，对画面元素进行精准、智能的调整；同时，视角控制能力的提升，也让场景扩展与画面构图更加灵活多样，拓展画面空间与表现视角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad8cf2c96b6f38c1aed5d1c30a2804ff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该功能还使 Seedream 5.0 在生成图像时能够利用更加全面、更新及时的信息。通过融合对网络层级内容的理解，AI 生成的画面在内容上更加贴近现实背景和时代语境，尤其适用于热点话题、现代设计以及对场景语境要求较高的视觉创作，最终呈现出更加丰富、贴合需求的视觉效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2b8bc5083f7b6dccd615a2d7bc85b1f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有用户表示，在 4K 分辨率下，人物皮肤纹理表现有所提升，同一组图像的多样性更好，整体氛围感也很出色。不过，文字渲染效果看起来相比 4.5 版本并没有明显改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/29/2905485ea5b894215aabccd9176b6f5d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有网友评价，图像生成的竞争已经不再只是比拼审美表现。Seedream 5.0 将重点放在检索准确性、4K 级放大能力以及工作流层面的精度控制上。字节跳动押注的是“实用性”而不是“艺术性”，认为真正推动专业用户采用的关键在于效率与可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于能不能取代 Nano Banana Pro，我们让两者同时生成了一份稍微复杂些的北京菜单，Nano Banana Pro 速度上更快，而效果似乎也赢了。（上图中，横版是Nano Banana Pro，竖版是Seedream 5.0，具体表现很直观了）就像网友说的，可能还需要一段时间才能实现取代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/d1/48/d199fcbb586d86139b7b7ef76b6db748.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/ae/4d/ae6c6c0287605ae5f58df3da85903f4d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xmRTBdskIJ0SNlwgGMYh</link><guid isPermaLink="false">https://www.infoq.cn/article/xmRTBdskIJ0SNlwgGMYh</guid><pubDate>Tue, 10 Feb 2026 10:24:11 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>未来两年软件工程展望：从写代码到管 AI，程序员正分化成两种职业</title><description>&lt;p&gt;本文最初发布于Addy Osmani的个人博客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件行业正处在一个奇怪的转折点上。AI编程已经从增强型的自动补全发展成了能够自主执行开发任务的智能代理。曾经推动科技行业招聘热潮的经济繁荣已经让位于效率至上的要求：企业现在往往更倾向于盈利而非增长，更倾向于经验丰富的员工而非应届毕业生，更倾向于组建配备更好工具的小团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，新一代的开发者带着不同的职业观步入职场：他们注重职业稳定性，对拼搏文化持怀疑态度，并且从入行第一天起就使用AI辅助工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来会发生什么确实还难以预料。以下这五个关键问题可能会决定2026年软件工程的发展，每个问题都对应两种截然不同的情景。这并非真正的预测，而只是一个观察的视角，帮助人们为应对软件工程的未来发展做好准备。我们的目标是基于现有数据，结合本领域特有的健康的怀疑精神，通过制定清晰的路线图来应对即将到来的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;1. 初级开发者问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：随着AI将入门级任务自动化，初级开发者的招聘可能会暴跌，也可能会随着软件渗透到各行各业而强力反弹。两种未来需要不同的生存策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“学习编码，获得初级工作，成长为高级”，这一传统的职业路径正在动摇。&lt;a href=&quot;https://www.finalroundai.com/blog/ai-is-making-it-harder-for-junior-developers-to-get-hired&quot;&gt;哈佛对6200万工人的研究&lt;/a&gt;&quot;发现，当公司采用生成式AI时，初级开发者就业率在六个季度里下降了大约9-10%，而高级开发者的就业率基本保持不变。过去三年，&lt;a href=&quot;https://restofworld.org/2025/engineering-graduates-ai-job-losses/&quot;&gt;大型科技公司招聘的应届毕业生减少了50%&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;正如一位工程师冷嘲热讽地说&lt;/a&gt;&quot;：“花9万美元雇个初级程序员，为什么不用成本更低的AI编程助手？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不仅仅是AI的问题。大约在2022年，&lt;a href=&quot;https://www.2ndorderthinkers.com/p/are-junior-level-jobs-really-killed&quot;&gt;利率上升和大流行后的调整等宏观经济因素&lt;/a&gt;&quot;就已经开始显现，这时AI工具尚未广泛使用。但AI加速了这一趋势。如今，在AI的帮助下，一名高级工程师可以完成过去需要一个小团队来完成的工作。企业正在悄然减少招聘初级员工，其幅度甚至超过了裁员规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的情景：AI解锁了每个行业对开发者的巨大需求，而不仅仅是技术行业。医疗保健、农业、制造业和金融业都开始嵌入软件和自动化技术。AI不是取代开发者，而是成为一个力量倍增器，将开发工作扩展到从未雇佣过编码人员的领域。我们将看到更多不同的入门级角色：为特定细分市场快速构建自动化和集成的“AI原生”开发者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;美国劳工统计局&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;预测&lt;/a&gt;&quot;，从2024年到2034年软件工作仍然将增长约15%。若企业利用AI扩大产出而非单纯裁员，就需要人类把握AI创造的机遇。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;悲观情景的长期风险经常被忽视：今天的初级开发者是明天的高级工程师和技术领导者。如果完全切断人才管道，那么在5-10年内就将出现一个领导力真空。行业老兵称这为“缓衰”：一个停止培训接班人的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：使自己精通AI并成为多面手，证明一名初级开发者加上AI可以匹配一个小型团队的产出。使用AI编码代理（Cursor/Antigravity/Claude Code/Gemini CLI）构建比较大的功能，但要能理解并解释大部分代码行。聚焦不容易被AI替代的技能：沟通、问题分解、领域知识。将相邻角色（QA、DevRel、数据分析）视为切入点。构建一个项目集，特别是集成AI API的项目。考虑参与学徒计划、实习、外包或开源项目。不要成为“只是又一个需要培训的新毕业生”，而是成为一个学习速度快、立即就能发挥作用的工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：初级开发者减少意味着你的日常工作增加。利用自动化工具来完成例行任务，不要什么事都自己做。利用CI/CD、linter和AI辅助测试来捕捉基本问题。通过开源项目或指导其他部门同事开展非正式的导师工作。向管理层如实说明全由资深员工组成的团队所面临的风险。若初级人才需求回升，需做好高效接纳新人的准备，并运用AI进行任务分配。你的价值在于提升整个团队的产出，而非个人的代码产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;2. 技能问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：随着AI编写大部分代码，核心编程技能可能会退化，或者因为人类开发者需要监督AI而使这些技能变得比以往任何时候都更加关键。未来几年将决定我们是否会为追求速度而牺牲对代码的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.blog/2025/09/10/ai-vs-gen-z/&quot;&gt;现在有84%的开发者定期使用AI辅助工具&lt;/a&gt;&quot;。对许多人来说，面对错误或新功能需求的第一反应不是从头开始编写代码，而是编写提示并组合AI生成的代码片段。初级程序员正在跳过“艰难的入门阶段”：他们可能永远不会从头开始构建二叉搜索树或独立调试内存泄漏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者的技能集正在从实现算法转变为知道如何向AI提出正确的问题并验证其输出。现在，&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;入门的第一个要求是提示和验证AI的输出&lt;/a&gt;&quot;，而不是展示原始编码能力。一些高级工程师担心，这会产生一代不能独立编码的人，导致开发者技能退化。AI生成的代码可能会引入一些微妙的错误和安全漏洞，不太有经验的开发者可能会漏掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的情景：AI处理80%的常规工作，人类专注于最难的20%。架构设计、复杂集成、创意设计、边缘情况，这些问题是机器无法单独解决的。AI的普及并没有使深厚的知识积累过时，反而使人类专业知识变得比以往任何时候都更重要。这就是“高杠杆工程师”，他们将AI作为一种力量倍增器，但必须深入理解系统才能有效使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果每个人都有AI编码代理访问权限，那么区分优秀开发者的关键在于知道AI何时出错或不够优化。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;正如一位高级工程师所说&lt;/a&gt;&quot;：“最好的软件工程师不是最快的编码者，而是那些知道何时不信任AI的人。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程转变：需要输入的样板代码减少，把更多的精力用在审查AI输出的逻辑错误、安全漏洞和与需求不匹配的问题。关键技能变成了软件架构、系统设计、性能调优和安全分析。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;AI可以快速生成一个Web应用程序，但专家工程师需要确保AI遵循了安全最佳实践，并且没有引入竞态条件。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，开发者中间出现了分歧。一些人坦言，他们几乎不“亲手”编写代码，并认为编码面试应该做出改变。其他人则认为，跳过基础知识面试会导致AI输出出现问题时需要完成的应急处理工作增加。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;行业开始期望工程师同时具备&lt;/a&gt;&quot;AI的效率和保障质量的基本知识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：将AI当作学习工具，而不是拐杖。对于AI编码代理（Cursor/Antigravity/Claude Code/Gemini CLI）的建议，要通过审查代码了解其工作原理并识别薄弱环节。偶尔禁用你的AI助手，从头开始编写关键算法。优先考虑计算机科学基础：数据结构、算法、复杂性、内存管理。将项目实现两次，一次用AI，一次不用AI，然后对两者进行比较。学习提示工程，并掌握相关工具。通过严格的测试训练自己：编写单元测试，自己阅读堆栈跟踪信息而不是立即询问AI，熟练使用调试工具。深化AI无法复制的互补技能：系统设计、用户体验直觉、并发推理。证明你既能用AI快速解决问题，也能在AI失败时自己处理棘手的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：将自己定位为质量和复杂性的守护者。磨练你的核心专长：架构、安全、扩展、领域知识。练习用AI组件进行系统建模并思考故障模式。随时关注AI生成代码中的漏洞。拥抱你作为导师和审查者的角色：定义什么时候可以使用AI，以及什么时候必须手动审查（支付或安全代码）。侧重于创造性和战略性工作；让初级开发者和AI一起处理常规API连接，而你决定构建哪些API。投资软技能和跨领域知识。随时关注新工具和最佳实践。加倍重视人类开发者不可或缺的因素：准确的判断、系统性思维和导师带徒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;3. 角色问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：开发者的角色职责可能缩减为有限的审计（监督AI生成的代码）工作，也可能扩展为设计和管理AI驱动系统的关键协调者。无论哪种情况，创造价值都远不止于编写代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此处的两极分化非常明显。在前一种情景中，开发者的创造性职责被削弱。他们不再专注于构建软件，而是更多地审核和监管AI产出。AI系统（或使用无代码平台的“公民开发者”）负责生产环节；人类开发者则审查自动生成的代码，检查错误、偏见或安全问题，并审批部署。创造者沦为检查者。编写代码的喜悦被风险管理的焦虑所取代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有报道称，工程师将花更多时间评估AI生成的拉取请求和管理自动化管道，而不是从头开始编写代码。编程感觉更像是合规性检查，而不是创造性地解决问题。正如一位工程师感叹：“我不想沦为一个代码清洁工，整天收拾AI留下的烂摊子。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一种未来则有趣得多：开发者演变成高级协调者，兼具技术、战略和道德责任。AI“工人”意味着人类开发者承担架构师或总承包商的角色，负责设计整个系统，决定哪些任务分配给哪些AI或软件组件，并将活动部件组合成解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;有一家低代码平台的首席执行官阐述了这个情景&lt;/a&gt;&quot;：在“智能代理”开发环境中，工程师将转型为“作曲家”，指挥由AI代理和软件服务组成的“乐团”。他们无需亲自谱写每个音符，但会定义旋律，即架构、接口以及代理间的交互方式。这个角色兼具跨学科性和创造性：既是软件工程师，又是系统架构师，同时也是产品战略家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;乐观看法：随着AI承担起一些重复性工作，开发者的角色必然转向更高价值的活动。工作可能变得更加有趣。必须有人决定AI应该构建什么，验证产品是否合理，并持续改进它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向哪个方向发展取决于组织选择如何整合AI。将AI视为劳动力替代工具的公司可能会缩减开发团队，并要求剩下的工程师保持相关任务自动化运行。将AI视为团队能力增强工具的公司可能会保持人员数量基本不变，但让每位工程师承担更费时耗力的项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：不要局限于编写代码，要寻找其他机会。自愿参与测试用例编写、CI流水线设置或应用监控，培养与审计员/监管人角色相一致的技能。通过个人项目保持你的创造性编码能力，以免失去构建乐趣。培养系统思维：学习组件之间如何通信，怎样设计出良好的API。阅读工程博客和系统设计案例研究。熟悉除代码生成之外的AI和自动化工具：编排框架、AI API。提升书面与口头沟通能力。撰写文档时秉持向他人阐述的标准。向资深同事提问时，不仅要问“代码是否运行正常？”更要问“我的考量是否到位？”。准备好成为验证者、设计者和沟通者，而非仅是编码者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：把更多精力放在领导和架构责任上。打造供AI和初级团队成员遵循的标准和框架。定义代码质量检查清单和符合伦理的AI使用策略。随时关注与AI生成软件合规性和安全性相关的话题。专注于系统设计和集成知识；自愿绘制服务间的数据流并识别故障点。熟悉编排平台（Kubernetes、Airflow、无服务器框架、代理编排工具）。投入双倍精力履行技术导师角色：更多地参与代码审查、设计讨论、技术指导。提升快速评估他人代码并给出高层次反馈的能力。培养产品和商业意识；了解为什么构建一个功能以及客户关心什么。向产品经理学习或参加客户反馈会议。通过原型、黑客马拉松或新兴技术研究来保持你的创造激情。从编码者演变为指挥者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;4. 专家与通才问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：专业领域过于狭窄的专家会面临自身领域被自动化取代或逐渐淘汰的风险。在快速变化、AI深度渗透的时代背景下，T型工程师更受青睐——他们既具备广泛的适应能力，又拥有一个或两个有深厚知识积累的专业技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考虑到模型、工具和框架的快速兴衰，将职业生涯押注在单一技术栈上是有风险的。当新型AI工具能以极少需要人工干预的方式处理传统框架时，该领域的专家可能会突然发现自身需求锐减。那些专注于“单一技术栈、框架或产品领域”的开发者，某天醒来时或许会发现，该领域已日渐式微甚至被淘汰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想想那些在行业转型时未能及时转型的人：COBOL开发者、Flash开发者或移动游戏引擎专家。如今不同的是变革速度。AI自动化能让某些编程任务变得微不足道，削弱了因这些任务而存在的工作岗位。只精通单一技能的专家（比如调整SQL查询参数、将Photoshop设计切片为HTML代码）可能会发现，90%的工作已被AI取代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;招聘经理们总在追逐最新的小众领域。几年前人人都想要云基础设施专家；如今AI/ML工程师需求激增。那些精通昨日技术的人，随着该领域的发展放缓，会感到职业发展陷入了停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的结果是形成一种新的专业化形式，即“多面手专家”或&lt;a href=&quot;https://www.youtube.com/watch?v=IMHneaMO-dg&quot;&gt;T型开发者&lt;/a&gt;&quot;。他们在一两个领域拥有深厚的造诣（竖线），同时又广泛涉猎其他众多的领域（横线）。&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;他们成了跨学科团队的“粘合剂”&lt;/a&gt;&quot;，既能与各领域专家沟通协作，又能在必要时填补技术空白。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;企业不再需要知识深度或广度不够的开发人员&lt;/a&gt;&quot;；他们想要一个强大的核心竞争力，以及能够跨栈工作的能力。其中一部分原因是效率考量：一个T型工程师通常可以独立解决端到端问题，无需等待上下游交接。其中一部分原因是创新考量：知识的交叉传播可以带来更好的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，AI工具增强了通才的能力，使一个人更容易处理多个组件。后端工程师可以在AI的帮助下构建出合理的UI；前端专家可以借助AI生成服务器样板代码。一个提供丰富AI功能的环境让人们能够完成更广泛的工作。与此同时，深度专家可能会发现，他们的专业领域有一部分被自动化取代，却难以开拓新领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;现在近45%的工程角色期望能够精通多个领域的知识&lt;/a&gt;&quot;：编程加云基础设施知识，或是前端开发加熟悉ML。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：尽早打下广泛的基础。即使被雇佣为特定的角色，也要了解那个岗位之外的知识。如果你是在做移动开发，不妨学习下后端基础知识；如果你是在做前端开发，则可以尝试编写一个简单的服务器。学习部署过程和工具，如Docker或GitHub Actions。找一两个真正让你感到兴奋的领域深入学习，使它们成为你垂直领域的专业知识。将自己定位成混合型人才：“全栈开发人员，专注于云安全”或“前端开发人员，具有UX专业知识”。借助AI工具快速学习新领域的知识；如果你是后端新手，可以让ChatGPT生成入门API代码并学习它。养成不断学习新技能的习惯。参加黑客马拉松或跨职能项目，强迫自己进入通才模式。告诉你的经理，你想要接触项目的不同部分。适应性是职业生涯早期的超能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：绘制你的技能图谱：你在哪些领域是专家，哪些相关领域你只是浅尝辄止？选择一到两个相邻领域并努力精通。如果你是一个后端数据库专家，不妨熟悉一个现代前端框架或学习机器学习（ML）流水线的基础知识。借助AI的帮助，在你的弱项领域做一个小项目。将你深厚的专业知识与新环境相结合；如果你专门从事Web应用性能优化，可以探索如何将这些技能应用于ML推理优化。支持或争取将你的角色设计成跨职能的，自荐成为涉及多领域项目的“集成负责人”。指导他人，传播技能，同时也从中学习新东西。更新简历体现多元化能力。利用你的经验识别模式和可转移知识。成为T型人才的典范：在你的专业领域深耕（建立权威和信心），并积极拓展横向能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;5. 教育问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：计算机科学（CS）学位是保持黄金标准，还是被更快的学习路径（训练营、在线平台、雇主培训）所取代？大学可能难以跟上每几个月就有重大变化的行业发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;四年制计算机科学学位一直是进入软件领域的主要途径。但这一传统正在受到质疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一种未来：大学仍然重要，但难以保持相关性。学位仍然是默认的资格凭证，但受制于缓慢的课程更新周期和官僚审批流程，课程设置落后于快速发展变化的需求。学生和雇主均感觉学术界与行业脱节，学校教授的理论或过时的做法无法转化为工作技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近的毕业生报告指出，他们在攻读学位期间从未学习过云计算、现代DevOps或AI工具。如果大学需要投入很多的时间和资金，但却只能提供低相关性教育，那么它们就有被视为昂贵守门人的风险。但出于惯性，许多公司仍然要求应聘者具备学士学位，因此压力就转到了应聘者身上，他们需要通过训练营、在线课程和自学项目来弥补这方面的不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://campustechnology.com/articles/2025/10/21/solving-the-talent-crisis-starts-in-higher-ed.aspx&quot;&gt;学生贷款是一笔巨大的债务，而公司也要花费数十亿美元培训新毕业生&lt;/a&gt;&quot;，因为他们缺乏工作场所需要的技能。大学可能会在这里增加一门AI伦理课程，在那里增加一门云计算选修课，但当他们真正实施时，行业工具已经又向前发展了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;颠覆性场景：传统教育日益为新教育体系所取代。编码训练营、在线认证、自学作品集、雇主创建的培训学院层出不穷。许多知名雇主（谷歌、IBM）已经取消了某些技术角色的学位要求。到2024年，&lt;a href=&quot;https://campustechnology.com/articles/2025/10/21/solving-the-talent-crisis-starts-in-higher-ed.aspx&quot;&gt;近45%的公司计划至少取消部分职位的学士学位要求&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;训练营体系已经相当成熟，他们培养的毕业生与CS毕业生一起被顶级公司雇佣。这些项目周期更短（12周强化），并且专注于教授实用技能：当前流行的框架、云服务、团队合作。招聘标准正在瞄准在线作品集、微证书和已认证技能。出色的GitHub作品集或公认的认证可以免除学位要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由雇主推动的教育正在兴起：企业自主搭建培训体系或与编程训练营合作。部分科技巨头已经为非传统背景的人才设立了内部“大学”。AI本身也开辟了全新的学习路径：AI导师、交互式编程沙盒、校外个性化教学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模块化的学习生态远比昂贵的四年制学位更容易获取。在计算机科学专业实力薄弱的国家，孩子们也能修读Coursera的课程，构建与硅谷人士同样的个人作品集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我们该如何做？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有志向的开发者/初级开发者：在学习传统的计算机科学课程时，不要完全依赖课程进行学习。要通过实际的项目补充课程内容：构建Web应用，参与开源项目。寻找实习或合作机会。如果你的课程中没有包含热门话题，则通过在线平台学习它们。考取行业认可的认证（GCP、亚马逊云科技、Azure）以证明自己的实践能力。如果是你在自学或参加了训练营，则一定要专注于创建一个引人注目的作品集：至少要有一个文档良好的重点项目。积极参与开发者社区：参与开源项目，撰写技术文章。通过LinkedIn、聚会以及开发活动建立人际关系网络。争取资深开发者为你背书。考虑到技术技能的半衰期非常短，务必要不断学习。将AI作为个人导师。用具体的方式证明自己的能力：作品集、认证证书以及能清晰阐述工作成果的能力，这些将为你打开机遇之门。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者和领导者：你不能永远依赖于证书。要在继续教育方面进行投资：在线课程、研讨会、会议、认证。通过新的方式验证你的技能，为通过实际问题评估应聘者当前能力的面试做好准备。维护使用了新技术的业余项目。重新评估工作要求：你真的需要新员工拥有计算机科学学位，还是需要他们具备某些技能和学习能力？推动以技能为先的招聘，扩大你的人才库。支持内部培训计划或学徒制岗位。为没有正式大学背景的初级开发者建立导师制小组。与学术界及其他机构合作：加入顾问委员会、举办客座讲座、对课程存在的问题提出反馈。将这种合作融入自身的职业发展中：实际的成果和持续的学习比额外的学位更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;小结&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些情景并不是相互排斥的。现实将融合所有要素。一些企业将缩减初级岗位的招聘，另一些则会在新的领域扩大招聘规模。AI会将常规编码工作自动化，同时又提升人类编写的代码的质量标准。开发者或许会在上午审核AI生成的代码，下午则专注于设计高级架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个贯穿始终的主题是：变化是唯一的常数。紧盯技术趋势（并保持审慎态度），避免被炒作或末日论所蒙蔽。通过更新技能、拓展能力、聚焦人类特有的优势（创造力、批判性思维、协作能力），你才能始终保持竞争力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论未来是迎来编程复兴，还是进入自动编码时代，那些具备全局思维、持续学习能力并能推动技术发展解决实际问题的工程师，始终会受到市场的青睐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预测未来的最佳方式就是积极地塑造它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://addyosmani.com/blog/next-two-years/&quot;&gt;https://addyosmani.com/blog/next-two-years/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UtPXQMUagxqNoPE2PaT0</link><guid isPermaLink="false">https://www.infoq.cn/article/UtPXQMUagxqNoPE2PaT0</guid><pubDate>Tue, 10 Feb 2026 10:08:31 GMT</pubDate><author>Addy Osmani</author><category>生成式 AI</category></item><item><title>达摩院开源RynnBrain：首个支持移动操作的具身大脑基础模型</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;具身智能蓬勃发展的当下，具有泛化性的具身能力至关重要。为了追求这个终极目标，业界发展出了两条技术路线。一条路线从机器人末端动作输出入手，发展出可以直接操作物理世界的VLA模型。但是VLA模型由于其数据稀缺性无法实现泛化。因此有了第二条路线，从本身拥有泛化能力的VLM入手，加速VLM从数字世界迈向物理世界。我们将在此路线上探索的模型称之为具身基础模型。诚然，已经有一些研究开始了对具身基础模型的初步探索。例如，RoboBrain系列模型在单个视觉语言模型中统一了理解、定位和规划，以促进复杂的具身任务。Robix模型为任务执行期间更自然的人机交互做出了贡献。 然而，这些当前的具身基础模型动态认知受限，且普遍存在物理幻觉，难以适应人形机器人上的复杂任务。主页：&lt;a href=&quot;https://alibaba-damo-academy.github.io/RynnBrain.github.io/&quot;&gt;https://alibaba-damo-academy.github.io/RynnBrain.github.io/&lt;/a&gt;&quot;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们提出了RynnBrain，首个可移动操作的具身基础模型。其具有以下三个关键要点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）时空记忆：RynnBrain能够在其完整的历史记忆中定位物体、目标区域，甚至预测运动轨迹，从而赋予机器人全局时空回溯能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）物理空间推理：不同于传统的纯文本推理范式，RynnBrain 采用文本与空间定位交错进行的推理策略，确保其推理过程紧密扎根于物理环境。大大减弱了具身任务中的幻觉问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（3）良好的可拓展性：我们在RynnBrain基础模型上微调了视觉语言导航和精准操作规划模型，效果轻松实现SOTA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过完备的实验，RynnBrain在16项具身任务Benchmark上全面超越了Cosmos Reason 2和Gemini Robotics ER 1.5等强大模型实现了SOTA，并且在8项域外Benchmark上验证了超越其他具身基础模型的通用泛化性。特别的，我们开源了业界首个MOE具身基础模型RynnBrain-30B-A3B，其只需要3B的推理激活参数就全面超越了当前规模最大的具身基础模型Palican-VL-72B。使用我们的MOE模型可以让机器人在保持最强大感知和规划能力的基础上拥有更加快速的动作响应和更加丝滑的行为模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为推动领域发展，我们同步开源：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;✅ 全系列模型（含全尺寸基础模型与后训练专有模型）&lt;/p&gt;&lt;p&gt;✅ 全新评测基准RynnBrain-Bench（评测时空细粒度具身任务）&lt;/p&gt;&lt;p&gt;✅ 完整的推理与训练代码&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain首次实现了“大脑”对物理世界的深度理解与可靠规划，为大小脑分层架构下的通用具身智能迈出关键一步。我们期待它加速 AI 从数字世界走向真实物理场景的落地进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/8115b8a12c872c0203164ba95e3092b9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;RynnBrain模型体系架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72cc6dca6545915445809d2b0531a752.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）模型结构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain在Qwen3-VL基础上进行训练。 使用自研的RynnScale架构对Dense模型和MOE模型均进行了训练速度的优化，使得在同等资源下训练加速两倍。在输入端RynnBrain可以接受任意分辨率的图片、多图和视频输入，满足用户任意形式的视觉输入的需求。同时RynnBrain可以输出区域、轨迹、点集、夹爪位姿、文本等多种具身相关模态，从而支持多样化具身任务的执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）训练优化&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain是一款面向高泛化的具身基础模型，使用视频、图像和文本等多模态数据进行训练，覆盖从定位、空间感知等短任务到长篇多模态描述与复杂推理等多种场景。由于样本序列长度差异大且呈长尾分布，直接在数据并行训练中平均分配样本会引发“拖尾效应”，影响整体吞吐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们引入在线负载均衡：训练时根据图像大小与文本token数预估序列长度，将同一DP组内样本统一重分配，使每个worker的累计序列长度尽量均衡，并用优先分配长序列的贪心策略在数据预取阶段快速完成，避免训练卡顿且无需额外数据预处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，由于重分配会造成各worker样本数不均，我们采用按样本的损失归约方式，保证训练前后损失一致性与收敛稳定，并显著提升训练效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在工程实现上，我们结合ZeRO、梯度检查点、输出token过滤等技术降低显存占用；在更大规模模型中引入ZeRO-2与专家并行（EP），并通过优化MoE 计算与跨卡分发提升吞吐。训练与推理框架基HuggingFace Transformers，并已开源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;根植于物理世界的时空预训练&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要制造出一种能够与周围环境进行自然互动的通用型机器人，需要具备两项基本能力：一、时空记忆：通过历史视觉记忆，机器人必须建立涵盖空间、位置、事件、轨迹等多维度的表征，从而能够适应复杂多变的环境。二、忠实于物理世界：所有机器人的认知过程都必须从根本上扎根于物理世界的客观现实之中。本章主要介绍了RynnBrain的预训练，该方法正是基于上述两点见解而制定的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）训练策略&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为赋予RynnBrain以上所述的时空记忆与物理世界落地能力，我们设计了一个统一的预训练框架，将多模态输入整合到共享的语义空间中。我们的训练方案聚焦于两大核心支柱：统一的输入输出表示，以及物理感知的优化策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;统一的时空表示&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为培养时空记忆，我们将图像与视频视为统一的输入模态。这样，RynnBrain能够在视频序列中学习时间因果关系与轨迹动态，这对于理解运动与事件至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根植于物理世界的输出空间&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现物理世界，我们对输出空间进行严格形式化，以连接高层认知与低层执行。不同于标准视觉语言模型将数字作为自由文本处理，我们引入离散的坐标token来表示物理位置。我们将所有空间坐标归一化到固定区间，并用整数token表示。这种量化将连续的物理控制转化为离散的分类问题，使模型能够使用与语言生成相同的自回归机制输出精确位置（例如抓取点或导航目标）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）数据准备&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们为RynnBrain的预训练准备了两千万的数据对，具体数据细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a2/a24b1419cde4aad6819ee06cec20515c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用多模态训练数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们复用了团队自研的Video-Llama 3视频大模型的训练数据，并融合了LLaVA-OV-SI、LLaVA-Video等多个开源视频问答数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具身认知数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;物体认知、空间认知和计数相关数据复用了团队自研的RynnEC模型训练数据，并且引入了Sensenova-SI、VSI-590k、Molmo2等提高模型的空间理解和动态计数能力。此外，我们自己生成了100万对自我为中心的OCR问答数据，其中即有直接的OCR问题，也有需要识别视频中多个文字才能回答的情景问题。我们还收集了EgoRe-5M、Egotaskqa和RoboVQA等自我为中心的多样化问答数据以增强RynnBrain的自我为中心任务理解能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具身定位数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain拥有5项具身定位能力，分别为：物体定位、区域定位、操作点定位、轨迹定位和夹爪位姿定位。我们为每项定位任务标注了大量额视频以及图像数据，使得RynnBrain在室内的定位能力上拥有突出的泛化性。我们还用ADE20K、Grasp-Anything、PACO-LVIS等开源数据平衡整体数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划任务包含导航和操作两类。导航使用了R2R和RxR数据和ScaleVLN的开源数据。并且将数据格式变成了流式的格式。操作规划数据源来自OpenX-Embodiment和AGIBot。首先，我们将这两个数据集中所有的规划数据都整合成时间段和子任务标注一对一匹配的格式。然后我们让人工标注出每个子任务规划中跟物体、区域和操作相关的名字。例如：“拿起香蕉放到桌子的左下角”，在这句话中与物体相关的词语是“香蕉”，与区域相关的词语是“桌子的左下角”，与操作相关的词语是“拿起”。然后人工再将这些词语和图像中的位置信息做对应，操作词语与图像中的操作点对应，物体词语与图像中物体的检测框对应，区域词语与图像中的区域点对应。最终得到文本和定位信息穿插的子任务标注数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;基于RynnBrain的后训练-让具身拓展无限可能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）物理空间推理模型&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，大多数多模态推理模型采用纯文本推理范式。虽然一些方法通过工具使用（例如放大）来缓解视觉识别中的挑战，但这种推理范式存在泛化能力有限的问题，只能解决一小部分问题。此外，探索在推理过程中进行视觉想象的替代方法通常会受到生成图像中严重幻觉的困扰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于具身大脑在现实世界中运行，进行物理空间推理的能力变得至关重要。因此，在RynnBrain中，我们提出了一种交错推理方法，该方法将实体化与文本信息直接结合在以自我为中心的视频流中。这种范式有效地弥合了语言与物理世界之间的认知鸿沟，确保推理过程牢固地扎根于现实之中。下面详细介绍了RynnBrain在物理空间推理领域的贡献和探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们设计了5类空间推理任务——计数、物体定位、操作点定位、区域定位和轨迹预测，来验证RynnBrain新提出的“文本-空间交织”推理范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们采用组相对策略优化（GRPO）来使模型与物理空间推理任务对齐。不同于标准PPO需要价值函数来估计优势项，GRPO通过对同一提示下生成的多个采样输出的组内得分来估计基线。这显著降低了显存占用与训练复杂度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练从我们的冷启动模型初始化。我们使用SGLang推理引擎以高效生成rollout，组大小设为5。训练共进行10个epoch，batch size为128。我们采用余弦学习率调度进行策略优化，并进行3% 的warmup。为保证稳定性，我们将截断范围设为[0.2, 0.28]，KL系数0.02。最大序列长度设为16,384个token，以适配长上下文的第一视角视频推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据构建采用“AI生成+人工精标”策略：&lt;/p&gt;&lt;p&gt;从自采第一人称视频中抽取样本；多模态大模型生成初步推理链，并用方括号标记关键实体（如“[白色花图案的墙纸]”）；由大语言模型初步分类实体为“物体”或“区域”；人工标注员最终审核并精标：对“对象”标注边界框，对“区域”标注代表性点集，并选择最清晰的视频帧作为参考帧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所有定位结果以结构化格式&lt;object area=&quot;&quot;&gt;: ...; (coordinates)&lt;!--...--&gt; 融入推理文本，实现语言与空间的对齐。&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，计数任务特别强调“先定位再计数”，共构建 7万条高质量样本，显著提升模型在复杂场景下的时空感知能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）视觉语言导航&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航任务采用与当前SOTA模型StreamVLN相同的数据设置。首先使用r2r rxr EnvDrop ScaleVLN数据在RynnBrain基础模型上做第一阶段训练。然后利用这个第一阶段模型在r2r rxr EnvDrop环境中采集Dagger数据。具体而言，使用第一阶段模型在r2r rxr EnvDrop的模拟器环境中进行导航，如果发现导航路径偏离了正确路径，则使用最短路径算法得到一个从当前位置到目标点的最短路径。因此，Dagger得到的导航数据可以有效纠正第一阶段模型的导航错误。使用Dagger数据我们可以进行第二阶段的训练得到最终的RynnBrain-Nav导航模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（3）操作规划任务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于预训练语料库包含了以规划为中心的数据，基础模型本身就具备了固有的规划能力。然而，要将这种能力应用于复杂的、长周期的操作任务，模型需要保持有效的记忆。为此，我们利用了一个小型的自采集数据集，其格式为多轮对话，其中交互历史充当了明确的记忆缓冲区，以保存历史推理结果。这种结构使模型能够将单个规划步骤整合成一个连贯的长周期策略。至关重要的是，为了与这种顺序推理相匹配，我们仅在每个对话轮的最后一步应用grounding标注，确保当前决策既取决于即时观察，也取决于累积的记忆。通过实验证明，这种方法具有很高的数据效率：仅使用几百个样本进行微调就足以使模型具备强大的长周期规划能力和泛化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;RynnBrain亮眼的实战成绩单&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）基础模型能力全面&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/01/01861b2d0e2ba3364444472e02284db8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于当前开源Benchmark在具身时空细粒度任务上的缺失。我们推出了RynnBrain这一多维度基准测试工具，用于评估时空细粒度具身能力。 该测试涵盖了四个关键维度：物体认知、空间认知、物体定位以及具身点预测，旨在突出对记忆视频序列中细粒度的理解以及时空的定位能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/170c7343c0d92adf321708a2d23a4ed3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain测评了20项具身相关的认知与定位Benchmark。在这些具身能力上，RynnBrain全面领先Mimo-Embodied等最先进的具身大脑模型，在许多能力上甚至有30%以上的涨幅。在具身领域之外的通用视觉理解方面，RynnBrain很好的保持了Qwen3-VL的强大通用视觉能力，甚至在AI2D、DocVQA等Benchmark上超越了Qwen3-VL。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）后训练潜力巨大&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航后训练&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47c6da2f970f0a97aa97efa17a2c23e9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们使用当前的导航SOTA模型StreamVLN的训练数据微调RynnBrain模型。在没有进行任何架构改进的情况下RynnBrain-Nav比StreamVLN的导航成功率提高了2%-3%。我们在Qwen3-VL基础模型上利用相同的数据训练后发现，RynnBrain作为基础模型可以让微调出的导航模型能力提升5%。这充分证明了在具身相关任务中，RynnBrain的预训练作用巨大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;操作规划后训练&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/2495e2769ddd687fa96dd0dc6473a40f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划任务需要拥有强大的预测能力和场景解析力。只使用几百条数据微调之后RynnBrain-Plan-30B(A3B)即可在域内和域外的任务上全面超越Gemini&amp;nbsp;3 Pro。这充分体现了文本与定位交错的规划方式更加适用于多变复杂的物理世界。&lt;/p&gt;&lt;/object&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rA7dxxttFjqurKCzfGmI</link><guid isPermaLink="false">https://www.infoq.cn/article/rA7dxxttFjqurKCzfGmI</guid><pubDate>Tue, 10 Feb 2026 09:53:06 GMT</pubDate><author>达摩院</author><category>AI&amp;大模型</category><category>开源</category></item><item><title>WASI 1.0：WebAssembly可能在2026年悄然普及</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文介绍了WebAssembly及其关键标准WASI 1.0在2026年的普及前景。随着WASI 0.3.0的发布，WebAssembly将在更多场景（如边缘设备、无服务器环境等）替代传统容器。WebAssembly已经走出浏览器，凭借组件模型、接口类型等新规范，降低了开发门槛，提升了互操作性和安全性。WASI的标准化进程虽漫长，但每一步都推动了WebAssembly的广泛应用，未来将实现高性能、可组合并发和零拷贝流式处理等关键特性，进一步加速WebAssembly的落地和普及。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文最初发表于&lt;a href=&quot;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-2026/&quot;&gt;The New Stack网站&lt;/a&gt;&quot;，由InfoQ中文站翻译分享。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/webassembly/&quot;&gt;WebAssembly&lt;/a&gt;&quot;在Wasm 3.0和组件模型（Component Model）发布后取得了巨大进展。然而，通往WebAssembly真正成熟落地的“最后一公里”，预计将随着&lt;a href=&quot;https://wasi.dev/roadmap&quot;&gt;WASI 0.3.0&lt;/a&gt;&quot;在2026年（很可能在2月份）的正式发布而完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一标准化工作的最终阶段将使WebAssembly能够在越来越多的场景中替代传统&lt;a href=&quot;https://thenewstack.io/introduction-to-containers/&quot;&gt;容器&lt;/a&gt;&quot;，因为无论是否运行在&lt;a href=&quot;https://thenewstack.io/kubernetes/&quot;&gt;Kubernetes&lt;/a&gt;&quot;中，容器本身并不适合某些应用场景。这些场景包括：边缘设备、异步与事件驱动架构、无服务器（serverless）环境，以及需要通过单次发布同时部署到大量（甚至无限数量）终端节点的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;远超浏览器环境的WebAssembly&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，WebAssembly早已走出浏览器。在2025年&lt;a href=&quot;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&quot;&gt;KubeCon + CloudNativeCon&lt;/a&gt;&quot;北美大会期间，微软Azure Core Upstream的首席产品经理&lt;a href=&quot;https://github.com/squillace&quot;&gt;Ralph Squillace&lt;/a&gt;&quot;在&lt;a href=&quot;https://cncf.io/?utm_content=inline+mention&quot;&gt;CNCF&lt;/a&gt;&quot;主办的&lt;a href=&quot;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/wasmcon/&quot;&gt;WasmCon&lt;/a&gt;&quot;活动闭幕致辞中表示：“WebAssembly几乎能够在所有环境可靠地运行于生产系统中，包括浏览器、服务器、CDN和后端服务，这充分证明了其成熟度和广泛适用性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace指出，尽管WebAssembly核心有意设计为层级较低且难以直接使用，但近期的规范化工作已支持更高层次的抽象。引用类型（Reference Types）和 接口类型（Interface Types）使得组件能够暴露有意义的API，而开发者无需深入理解WASM内部的机制，从而大幅降低了使用门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于那些特别关注组件的人来说，Squillace表示，&lt;a href=&quot;https://thenewstack.io/webassembly-to-let-developers-combine-languages/&quot;&gt;Bytecode Alliance&lt;/a&gt;&quot;对工程师免费开放。该联盟的重点在于支持工程师和开源开发，而非营销，并提供了包括文档在内的各种资源，使开发者能够从零开始使用WebAssembly组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace还指出，这些选择并非相互排斥的。WebAssembly及其组件模型的目的并非取代编程语言、模块或容器，而是致力于实现互操作性、安全性，并拓展软件在不同语言和环境之间所能实现的功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;WebAssembly并不是完美无缺的，但Squillace表示，这并非重点。真正重要的是它所赋能的能力。这是一个由自愿参与者共同构建的激动人心的领域，正因如此，他说道，这次“结束”实际上是一次“开启”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;核心规范&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管WebAssembly核心有意设计为层级较低且难以直接使用，但近期的规范工作已支持更高层次的抽象。Squillace指出，引用类型（reference types）和接口类型（interface types）使得组件能够暴露有意义的API，而开发者无需深入了解WebAssembly的内部机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace表示，“在核心层面开展的规范工作……正是让组件模型能够传递复杂的结构、从而形成合理API的关键所在”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，基于Wasm的解决方案尚不能作为容器的即插即用替代方案，但它已经在越来越多的场景中得到了应用，这些场景充分利用了WebAssembly的优势。“即便组件模型仍处于早期阶段，但它依然是采用Wasm的一个强有力的理由”。&lt;a href=&quot;https://endor.dev/&quot;&gt;Endor&lt;/a&gt;&quot;的首席执行官兼联合创始人&lt;a href=&quot;https://www.linkedin.com/in/ridruejo/&quot;&gt;Daniel Lopez&lt;/a&gt;&quot;告诉我，“WebAssembly已经被广泛应用于众多无服务器和边缘计算场景中。许多用户（很可能绝大多数）甚至并未意识到它正在幕后运行，尤其是在SaaS和无服务器服务中。Wasm已经支撑了大量应用和场景。随着开发者和行业参与者的广泛支持，进一步的标准化只会加速这一采用进程。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wasm 3.0并未包含组件模型的最终定稿。尽管Endor项目已非常接近，但像Docker那样“魔法时刻”（即几乎任何应用都能被打包进一个Wasm模块，并可随意部署、传输并在任意地方运行）仍未完全实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;标准化完成之后，应用程序将能以任意语言编写，并通过Wasm模块分发，同时（甚至异步地）部署到任意终端节点。组件模型最终定稿后，WebAssembly就能将其应用场景从网页浏览器和服务器进一步拓展。用户将能够在成千上万个终端节点上，以极高速度同时运行多个轻量级模块中的不同应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2025年北美KubeCon + CloudNativeCon大会期间，由CNCF主办的WasmCon开幕致辞中，Cosmonic公司首席技术官&lt;a href=&quot;https://www.linkedin.com/in/baileyhayes/&quot;&gt;Bailey Hayes&lt;/a&gt;&quot;阐述了WebAssembly的核心优势：近乎为零的冷启动延迟、高工作负载密度，以及即使在资源受限环境中也能高效运行的轻量级、可移植运行时。展望未来，Hayes将即将发布的WASI 0.3.0视为一个重要里程碑。他表示，该版本预览了多项定义下一代WebAssembly计算浪潮的关键特性，包括，与语言深度集成的并发能力（并提供针对不同语言的惯用绑定）、跨语言组件的可组合并发，以及通过底层I/O和零拷贝数据处理实现的高性能流式传输。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;下一波浪潮的关键特性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Hayes 表示，“我想重点强调三项让我最为期待的下一代计算关键特性：语言集成的并发、跨语言组件的可组合并发，以及支持底层I/O与零拷贝的高性能流式处理。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一切在很大程度上要取决于组件模型的最终确定，尤其是其与WASI的关系，WASI是连接WebAssembly模块与组件的标准接口或API。它将支持构建所谓的WebAssembly “世界”，即由一组由兼容的Wasm组件所构成的互连基础设施，其功能类似于 Kubernetes，但无需依赖容器。2024年发布的WASI Preview 2在标准化方面取得了重大进展，但我们尚未抵达终点。2025年或许仍无法实现“圣杯（Holy Grail）”目标，但可能会带来一些令人欣喜的突破。有传言称，WASI 0.3.0可能无法在今年最终定稿，或将推迟其发布，进而延缓可用组件模型的落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lopez表示，“WASI的标准化过程很漫长，但每一次新的预览版发布都让我们离0.3.0更近一步，鉴于该标准的广泛影响和基础性地位，哪怕耗时超出预期，也必须确保其尽可能完善。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-2026/&quot;&gt;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-202&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HA0BPPlTU2jOYuSDC7Ul</link><guid isPermaLink="false">https://www.infoq.cn/article/HA0BPPlTU2jOYuSDC7Ul</guid><pubDate>Tue, 10 Feb 2026 09:50:50 GMT</pubDate><author>B. Cameron Gain</author><category>性能优化</category></item><item><title>Andy Pavlo：数据库年度回顾</title><description>&lt;p&gt;本文最初发布于Andy Pavlo的个人博客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;又一年过去了。我本希望能多写几篇文章，而不仅仅是年终的长篇大论，但我在春季学期&lt;a href=&quot;https://bsky.app/profile/andypavlo.bsky.social/post/3lsvwhx2ixk2v&quot;&gt;差点丧命&lt;/a&gt;&quot;，那占用了我所有的时间。尽管如此，我还是会回顾一下过去一年中数据库领域我认为重要的趋势和事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据库领域有许多激动人心且前所未有的发展。氛围编程（&lt;a href=&quot;https://twitter.com/karpathy/status/1886192184808149383&quot;&gt;Vibe Coding&lt;/a&gt;&quot;）成了日常用语。Wu-Tang Clan宣布启动&lt;a href=&quot;https://www.youtube.com/watch?v=4u-bttzVubs&quot;&gt;时间胶囊项目&lt;/a&gt;&quot;。Databricks未选择上市，而是进行了&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html#random-fundings&quot;&gt;两轮巨额融资&lt;/a&gt;&quot;，而不是只进行一轮大规模融资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，其他事件也都在预料之中，不那么令人惊讶。Redis公司在“抽走地毯（&lt;a href=&quot;https://redis.io/blog/redis-adopts-dual-source-available-licensing/&quot;&gt;rugpull&lt;/a&gt;&quot;）”一年后换回了他们的许可（我&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html#licenses&quot;&gt;去年&lt;/a&gt;&quot;就预测到了这一点）。SurrealDB&lt;a href=&quot;https://blog.cf8.gg/surrealdbs-ch/&quot;&gt;因为没有将写入的数据刷写到磁盘而丢失了数据&lt;/a&gt;&quot;，但他们的基准测试数据却非常好。Coldplay可以&lt;a href=&quot;https://www.reddit.com/r/WatchPeopleDieInside/comments/1m239rb/astronomer_ceo_and_cpo_caught_having_an_affair_on/&quot;&gt;破坏婚姻&lt;/a&gt;&quot;。不过Astronomer倒是从最后这件事里&lt;a href=&quot;https://www.youtube.com/watch?v=vich2C-Tl7Q&quot;&gt;尝到了不少甜头&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在开始之前，我想先回答我每年都会在评论中看到的问题。人们总是问我，在我的分析中，为什么没有提到特定的&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1hr3xor/databases_in_2024_a_year_in_review/m4vone0/&quot;&gt;系统&lt;/a&gt;&quot;、&lt;a href=&quot;https://news.ycombinator.com/item?id=42566660&quot;&gt;数据库&lt;/a&gt;&quot;或&lt;a href=&quot;https://news.ycombinator.com/item?id=34225377&quot;&gt;公司&lt;/a&gt;&quot;。我只能写这么多，除非过去一年中发生了一些有趣或值得注意的事情，要不就没有什么可讨论的。但也并不是所有值得注意的数据库事件，我都适合发表意见。例如，最近有人试图&lt;a href=&quot;https://twitter.com/CeolinWill/status/2005601763051856293&quot;&gt;揭露AvgDatabase首席执行官的真实身份&lt;/a&gt;&quot;，我认为是可以接受的，但&lt;a href=&quot;https://news.ycombinator.com/item?id=46403128&quot;&gt;MongoDB自杀诉讼案&lt;/a&gt;&quot;则不属于此类。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好了，我们开始吧。这些文章每年都在变长，所以我给读者朋友们提前道个歉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之前的文章：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html&quot;&gt;2024年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html&quot;&gt;2023年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2022/12/2022-databases-retrospective.html&quot;&gt;2022年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html&quot;&gt;2021年数据库年度回顾&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;PostgreSQL延续了其统治地位&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在2021年，我就写到，PostgreSQL正在&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html#dominance-of-postgresql&quot;&gt;吞噬数据库世界&lt;/a&gt;&quot;。这一趋势还在持续，因为数据库领域里最有趣的发展还是与PostgreSQL有关。该DBMS在2025年11月发布了最新版本（&lt;a href=&quot;https://www.postgresql.org/about/news/postgresql-18-released-3142/&quot;&gt;v18&lt;/a&gt;&quot;），其中最突出的功能是新增的&lt;a href=&quot;https://www.cybertec-postgresql.com/en/postgresql-18-and-beyond-from-aio-to-direct-io/&quot;&gt;异步I/O存储子系统&lt;/a&gt;&quot;，它使PostgreSQL终于摆脱了对操作系统页面缓存的依赖。它还增加了对&lt;a href=&quot;https://www.pgedge.com/blog/postgres-18-skip-scan-breaking-free-from-the-left-most-index-limitation&quot;&gt;跳过扫描&lt;/a&gt;&quot;的支持；即使缺少前缀，查询仍然可以使用多键B+树索引。查询优化器也做了一些改进（如&lt;a href=&quot;https://betterstack.com/community/guides/databases/postgresql-18-new-features/#optimizer-and-query-planning-improvements&quot;&gt;移除多余的自连接&lt;/a&gt;&quot;）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;精通数据库的行家们会立刻指出，这些功能并不是什么突破性的创新，其他DBMS多年前就已经有这些功能了。PostgreSQL是唯一仍然依赖操作系统页面缓存的主流DBMS。&lt;a href=&quot;https://richardfoote.wordpress.com/2008/03/10/index-skip-scan-does-index-column-order-matter-any-more-warning-sign/&quot;&gt;Oracle自2002年（v9i）以来就支持跳过扫描了&lt;/a&gt;&quot;！因此，你可能会问，为什么我说2025年数据库领域里最热门的事情是与PostgreSQL有关的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，数据库领域的大部分精力和活动都投入到了与PostgreSQL相关的公司、产品、项目及其衍生系统上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;收购+发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去的一年里，最热门的数据初创公司（&lt;a href=&quot;https://www.databricks.com/&quot;&gt;Databricks&lt;/a&gt;&quot;）为一家PostgreSQL DBaaS公司（&lt;a href=&quot;https://neon.com/&quot;&gt;Neon&lt;/a&gt;&quot;）&lt;a href=&quot;https://www.wsj.com/articles/databricks-to-buy-startup-neon-for-1-billion-fdded971&quot;&gt;支付&lt;/a&gt;&quot;了10亿美元。接下来，世界上最大的数据库公司之一（&lt;a href=&quot;https://www.snowflake.com/en/&quot;&gt;Snowflake&lt;/a&gt;&quot;）为另一家PostgreSQL DBaaS公司（&lt;a href=&quot;https://www.crunchydata.com/&quot;&gt;CrunchyData&lt;/a&gt;&quot;）&lt;a href=&quot;https://www.wsj.com/articles/snowflake-to-buy-crunchy-data-for-250-million-233543ab&quot;&gt;支付&lt;/a&gt;&quot;了2.5亿美元。然后，地球上最大的科技公司之一（微软）&lt;a href=&quot;https://techcommunity.microsoft.com/blog/adforpostgresql/announcing-azure-horizondb/4469710&quot;&gt;推出&lt;/a&gt;&quot;了一个新的PostgreSQL DBaaS（&lt;a href=&quot;https://azure.microsoft.com/en-us/products/horizondb&quot;&gt;HorizonDB&lt;/a&gt;&quot;）。Neon和HorizonDB沿袭了Amazon Aurora在2010年代初的&lt;a href=&quot;https://doi.org/10.1145/3035918.3056101&quot;&gt;高级架构&lt;/a&gt;&quot;，采用单主节点模式分离计算与存储功能。目前，Snowflake的PostgreSQL数据库即服务（DBaaS）使用了和标准PostgreSQL相同的核心架构，它们均基于&lt;a href=&quot;https://www.crunchydata.com/products/crunchy-bridge&quot;&gt;Crunchy Bridge&lt;/a&gt;&quot;构建。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;分布式PostgreSQL&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我上面列出的所有服务都是单主节点架构。也就是说，应用程序将写入发送到主节点，然后主节点将这些更改发送到从副本。但在2025年，有两个新项目宣布要为PostgreSQL创建扩展（即水平分区）服务。2025年6月，Supabase宣布聘请&lt;a href=&quot;https://www.linkedin.com/in/sougou/&quot;&gt;Sugu&lt;/a&gt;&quot;——Vitess的共同创建者和前PlanetScale联合创始人/CTO——来领导&lt;a href=&quot;https://multigres.com/&quot;&gt;Multigres&lt;/a&gt;&quot;项目，为PostgreSQL创建分片中间件，类似于Vitess对MySQL进行分片的机制。Sugu在2023年离开PlanetScale，迫不得已休息了两年。如今，他或许已经摆脱了所有的法律纠纷，可以在Supabase大展身手了。你知道，一位数据库工程师加入一家公司不是个小事，因此&lt;a href=&quot;https://supabase.com/blog/multigres-vitess-for-postgres&quot;&gt;公告&lt;/a&gt;&quot;更多地关注个人而不是系统。&lt;a href=&quot;https://www.linkedin.com/in/adam-prout-0b347630/&quot;&gt;SingleStore联合创始人兼CTO&lt;/a&gt;&quot;在2024年加入了微软，&lt;a href=&quot;https://www.linkedin.com/posts/adam-prout-0b347630_im-happy-to-share-that-im-starting-a-new-activity-7167922823800324096-v1OD&quot;&gt;领导HorizonDB项目&lt;/a&gt;&quot;，但微软（错误地）没有大力宣传。Sugu加盟Supabase的震撼程度，堪比&lt;a href=&quot;https://en.wikipedia.org/wiki/Ol%27_Dirty_Bastard&quot;&gt;Ol&#39; Dirty Bastard（RIP）&lt;/a&gt;&quot;&lt;a href=&quot;https://youtu.be/TDXKvYQ3Xb4&quot;&gt;服刑两年后假释&lt;/a&gt;&quot;出狱，次日便宣布&lt;a href=&quot;https://www.nme.com/news/music/odb-3-1383866&quot;&gt;签下新唱片合约&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在关于Multigres的新闻发布一个月后，PlanetScale&lt;a href=&quot;https://planetscale.com/blog/planetscale-for-postgres#neki-vitess-for-postgres&quot;&gt;宣布&lt;/a&gt;&quot;了自己的Vitess-for-PostgreSQL项目&lt;a href=&quot;https://www.neki.dev/&quot;&gt;Neki&lt;/a&gt;&quot;。2025年3月，PlanetScale推出了其&lt;a href=&quot;https://planetscale.com/blog/announcing-metal&quot;&gt;PostgreSQL DBaaS&lt;/a&gt;&quot;的初始版本，但核心架构仍然是单节点的老搭配&lt;a href=&quot;https://planetscale.com/blog/planetscale-for-postgres#performance-and-reliability&quot;&gt;PostgreSQL和pgBouncer&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年1月5日更新：有人发邮件提醒我，&lt;a href=&quot;https://pgdog.dev/&quot;&gt;PgDog&lt;/a&gt;&quot;也是一个寻求支持PostgreSQL水平分片的开源中间件系统。在心理上，我将PgDog和连接池代理（PgBouncer）归为了一类，但实际上它是Multigres和Neki的竞争对手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;商业格局&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着微软在2025年推出HorizonDB，所有主要的云供应商现在都有自己的PostgreSQL产品项目了。亚马逊自2017年起提供了&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/04/announcing-open-preview-of-amazon-aurora-with-postgresql-compatibility/&quot;&gt;Aurora PostgreSQL&lt;/a&gt;&quot;。谷歌在2022年推出了&lt;a href=&quot;https://venturebeat.com/data-infrastructure/google-announces-alloydb-a-faster-hosted-version-of-postgresql&quot;&gt;AlloyDB&lt;/a&gt;&quot;。ServiceNow在2024年推出了&lt;a href=&quot;https://www.investing.com/news/company-news/servicenow-unveils-raptordb-pro-and-future-knowledge-graph-93CH-3609528&quot;&gt;RaptorDB服务&lt;/a&gt;&quot;，其基础是他们2021年&lt;a href=&quot;https://www.zdnet.com/article/servicenow-acquires-database-performance-company-swarm64/&quot;&gt;收购&lt;/a&gt;&quot;的Swarm64。即使是IBM自2018年起也有了&lt;a href=&quot;https://cloud.ibm.com/docs/databases-for-postgresql?topic=databases-for-postgresql-postgresql-relnotes&quot;&gt;云版本的PostgreSQL&lt;/a&gt;&quot;。甲骨文在2023年发布了其&lt;a href=&quot;https://docs.oracle.com/en-us/iaas/releasenotes/changes/9a4b73b5-d4d6-4c89-bd31-b1fa2098fa34/index.htm&quot;&gt;PostgreSQL服务&lt;/a&gt;&quot;，尽管有传言说，其内部PostgreSQL团队在2025年9月的&lt;a href=&quot;https://www.theregister.com/2025/09/11/oracle_slammed_for_mysql_job/&quot;&gt;MySQL OCI裁员&lt;/a&gt;&quot;中受到了附带伤害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前仍然有一些独立的（ISV）PostgreSQL DBaaS公司。按实例数来说，&lt;a href=&quot;https://supabase.com/&quot;&gt;Supabase&lt;/a&gt;&quot;可能是这些公司中最大的。其他公司包括：&lt;a href=&quot;https://www.yugabyte.com/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.tigerdata.com/&quot;&gt;TigerData&lt;/a&gt;&quot;（之前的Timescale）、&lt;a href=&quot;https://planetscale.com/&quot;&gt;PlanetScale&lt;/a&gt;&quot;、&lt;a href=&quot;https://xata.io/&quot;&gt;Xata&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.pgedge.com/&quot;&gt;PgEdge&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.thenile.dev/&quot;&gt;Nile&lt;/a&gt;&quot;。Xata原本基于&lt;a href=&quot;https://xata.io/blog/serverless-postgres-platform#:~:text=AWS%20Aurora%20under%20the%20hood&quot;&gt;Amazon Aurora&lt;/a&gt;&quot;构建了其架构，但今年，他们宣布&lt;a href=&quot;https://xata.io/blog/xata-postgres-with-data-branching-and-pii-anonymization&quot;&gt;切换到自己的基础设施&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.paradedb.com/&quot;&gt;ParadeDB&lt;/a&gt;&quot;尚未宣布其托管服务。&lt;a href=&quot;https://www.tembo.io/&quot;&gt;Tembo&lt;/a&gt;&quot;则在2025年放弃了其&lt;a href=&quot;https://tembo-io.notion.site/Tembo-Cloud-Migration-Guide-1de7c9367d6a80349570e7469ba7f17b&quot;&gt;托管PostgreSQL产品&lt;/a&gt;&quot;，转而开发一种可以完成部分数据库优化的编码代理。&lt;a href=&quot;https://www.hydra.so/&quot;&gt;Hydra&lt;/a&gt;&quot;和&lt;a href=&quot;https://postgresml.org/&quot;&gt;PostgresML&lt;/a&gt;&quot;已于2025年倒闭（见倒闭一节），所以他们退出了游戏。其他系统提供了一个兼容Postgres的前端，但后端系统并非源自PostgreSQL（如&lt;a href=&quot;https://www.cockroachlabs.com/docs/stable/postgresql-compatibility&quot;&gt;CockroachDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://cedardb.com/docs/compatibility/&quot;&gt;CedarDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://docs.cloud.google.com/spanner/docs/postgresql-interface&quot;&gt;Google Spanner&lt;/a&gt;&quot;）。还有一些托管公司提供PostgreSQL DBaaS以及其他系统，如&lt;a href=&quot;https://aiven.io/&quot;&gt;Aiven&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.tessell.com/&quot;&gt;Tessel&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在Databricks和Snowflake收购PostgreSQL公司之后，不知道下一个大买家会是谁。而且，每家主要的技术公司都已经拥有了Postgres产品。EnterpriseDB是最古老的PostgreSQL ISV，但在过去的五年中，他们错过了两次最重要的PostgreSQL收购。但他们可以暂时依靠贝恩资本，或者寄希望于惠普收购他们，尽管那个&lt;a href=&quot;https://community.hpe.com/t5/oem-solutions/recap-hpe-greenlake-launch-discover-2017-madrid/ba-p/6991195&quot;&gt;合作伙伴关系&lt;/a&gt;&quot;是八年前的。PostgreSQL领域的并购格局令人联想到2000年代末期的OLAP收购浪潮：当&lt;a href=&quot;https://techcrunch.com/2011/03/03/teradata-buys-aster-data-263-million/&quot;&gt;AsterData&lt;/a&gt;&quot;、&lt;a href=&quot;https://techcrunch.com/2010/07/06/emc-acquires-data-warehousing-and-analytics-company-greenplum/&quot;&gt;Greenplum&lt;/a&gt;&quot;和&lt;a href=&quot;https://news.microsoft.com/source/2008/07/24/microsoft-to-acquire-datallegro/&quot;&gt;DATAllegro&lt;/a&gt;&quot;相继被收购后，&lt;a href=&quot;https://investor.hp.com/news-events/news/news-details/2011/HP-to-Acquire-Vertica-Customers-Can-Analyze-Massive-Amounts-of-Big-Data---at-Speed-and-Scale/default.aspx&quot;&gt;Vertica&lt;/a&gt;&quot;成了最后一个在公交站等车的玩家。&amp;nbsp;好消息是竞争性的分布式PostgreSQL项目已经发展到了三个（&lt;a href=&quot;https://multigres.com/&quot;&gt;Multigres&lt;/a&gt;&quot;、&lt;a href=&quot;https://planetscale.com/neki&quot;&gt;Neki&lt;/a&gt;&quot;、&lt;a href=&quot;https://pgdog.dev/&quot;&gt;PgDog&lt;/a&gt;&quot;）。并非第一次有人尝试这样做：用于OLAP工作负载的&lt;a href=&quot;https://www.vmware.com/products/app-platform/tanzu-greenplum&quot;&gt;Greenplum&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/ParAccel&quot;&gt;ParAccel&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.citusdata.com/&quot;&gt;Citus&lt;/a&gt;&quot;已经存在了二十年。Citus支持OLTP工作负载，但他们从2010年开始专注于&lt;a href=&quot;https://www.citusdata.com/blog/2018/06/07/what-is-citus-good-for/#:~:text=we%20focused%20on%20building%20a%20fast%20database%20to%20power%20analytics&quot;&gt;分析领域&lt;/a&gt;&quot;。对于OLTP，15年前，NTT RiTaDB项目与&lt;a href=&quot;https://wiki.postgresql.org/wiki/GridSQL&quot;&gt;GridSQL&lt;/a&gt;&quot;合作创建了&lt;a href=&quot;https://wiki.postgresql.org/wiki/Postgres-XC&quot;&gt;Postgres-XC&lt;/a&gt;&quot;。Postgres-XC的开发人员创建了&lt;a href=&quot;https://dbdb.io/db/stormdb&quot;&gt;StormDB&lt;/a&gt;&quot;，后来&lt;a href=&quot;https://translattice.com/pr/TransLattice_Acquires_StormDB_to_Enhance_TransLattice_Elastic_Database.shtml&quot;&gt;Translattice&lt;/a&gt;&quot;在2013年收购了它。&lt;a href=&quot;https://postgres-x2.github.io/&quot;&gt;Postgres-X2&lt;/a&gt;&quot;是一次对XC进行现代化改造的尝试，但开发人员放弃了这项工作。Translattice将StormDB开源为&lt;a href=&quot;https://en.wikipedia.org/wiki/Postgres-XL&quot;&gt;Postgres-XL&lt;/a&gt;&quot;，但该项目自2018年以来一直处于休眠状态。&lt;a href=&quot;https://www.yugabyte.com/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;于&lt;a href=&quot;https://www.yugabyte.com/blog/yugabyte-has-arrived/&quot;&gt;2016&lt;/a&gt;&quot;年推出，可能是部署最广泛的分片PostgreSQL系统（并且仍然是&lt;a href=&quot;https://github.com/yugabyte/yugabyte-db&quot;&gt;开源&lt;/a&gt;&quot;的！），但它是一个硬分叉，只与&lt;a href=&quot;https://docs.yugabyte.com/stable/api/ysql/&quot;&gt;PostgreSQL v15&lt;/a&gt;&quot;兼容。亚马逊云科技在2024年宣布了自己的分片PostgreSQL（&lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-aurora-postgresql-limitless-database-is-now-generally-available/&quot;&gt;Aurora Limitless&lt;/a&gt;&quot;），但是闭源的。&amp;nbsp;我知道微软在2019年收购了Citus，但由于他们总给自己的产品起一些令人困惑的名称，所以很难追踪他们在推出HorizonDB之前做了什么。Citus在2019年被重新命名为&lt;a href=&quot;https://techcommunity.microsoft.com/blog/adforpostgresql/azure-database-for-postgresql---hyperscale-citus-now-generally-available/1014865&quot;&gt;Azure Database for PostgreSQL Hyperscale&lt;/a&gt;&quot;，然后在2022年被更名为&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/postgresql/elastic-clusters/concepts-elastic-clusters&quot;&gt;Azure Cosmos DB for PostgreSQL&lt;/a&gt;&quot;。但他们还有使用Citus的Azure Database for PostgreSQL with Elastic Clusters，而该服务与以Citus为基础的Azure Cosmos DB for PostgreSQL并不相同。2023年，微软终止了&lt;a href=&quot;https://techcommunity.microsoft.com/discussions/azuredatabaseforpostgresql/announcement---retiring-azure-postgresql-single-server-in-march-2025-and-introdu/3820887&quot;&gt;Azure PostgreSQL Single Server&lt;/a&gt;&quot;服务，但保留了Azure PostgreSQL Flexible Server。他们有各种各样的Azure服务。这有点像亚马逊云科技忍不住在&lt;a href=&quot;https://docs.aws.amazon.com/aurora-dsql/latest/userguide/what-is-aurora-dsql.html&quot;&gt;DSQL&lt;/a&gt;&quot;的名字前加上 &quot;Aurora&quot;。无论如何，至少微软足够明智，将他们的新系统命名为 &quot;Azure HorizonDB&quot;（目前）。&amp;nbsp;PlanetScale团队&lt;a href=&quot;https://youtu.be/CvgIRHhyRQE?t=143&quot;&gt;对他们的对手没有好感&lt;/a&gt;&quot;，并且已知会对&lt;a href=&quot;https://blog.alexoglou.com/posts/database-decisions/&quot;&gt;Neon&lt;/a&gt;&quot;和&lt;a href=&quot;https://twitter.com/samlambert/status/1984010289348780137&quot;&gt;Timescale&lt;/a&gt;&quot;大打出手。数据库公司之间互相攻击并不新鲜（见&lt;a href=&quot;https://www.linkedin.com/posts/bobdoyleyugabyte_cockroach-labs-activity-7311530387271237634-xR78/&quot;&gt;Yugabyte vs. CockroachDB&lt;/a&gt;&quot;或&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html#gangwar&quot;&gt;Databricks vs. Snowflake&lt;/a&gt;&quot;）。我怀疑，随着PostgreSQL战争的升温，未来我们将看到更多这样的情况。我建议这些小公司&lt;a href=&quot;https://twitter.com/samlambert/status/1996035931057652125&quot;&gt;呼吁&lt;/a&gt;&quot;下，让那些大型的云供应商相互之间&lt;a href=&quot;https://youtu.be/0dT9siTP70Y&quot;&gt;不要提及对方的名字&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;每个数据库都开始支持MCP！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说2023年是&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html#vector&quot;&gt;所有数据库管理系统（DBMS）纷纷添加向量索引&lt;/a&gt;&quot;的一年，那么2025年就是所有DBMS都开始支持Anthropic公司&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_Context_Protocol&quot;&gt;模型上下文协议&lt;/a&gt;&quot;（MCP）的一年。MCP是一种标准的客户端-服务器JSON-RPC接口，使大型语言模型（LLM）能够与外部工具和数据源交互，而无需自己编写粘合代码。作为中间件，MCP服务器位于数据库管理系统前面，暴露DBMS提供的工具、数据及操作清单。MCP客户端（如Claude或ChatGPT等LLM宿主）通过向MCP服务器发送请求来发现并使用这些工具，扩展其模型能力。对于数据库场景，MCP服务器会将查询转换为对应的数据库指令（如SQL）或管理命令。换言之，MCP如同一个&lt;a href=&quot;https://youtu.be/VXuwljCWZMU&quot;&gt;中间人&lt;/a&gt;&quot;，使数据库与LLM之间可以建立起足够的信任以开展协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic公司在2024年11月&lt;a href=&quot;https://www.theverge.com/2024/11/25/24305774/anthropic-model-context-protocol-data-sources&quot;&gt;发布&lt;/a&gt;&quot;了MCP，但在2025年3月OpenAI宣布将&lt;a href=&quot;https://techcrunch.com/2025/03/26/openai-adopts-rival-anthropics-standard-for-connecting-ai-models-to-data/&quot;&gt;在其生态系统中支持MCP&lt;/a&gt;&quot;后，它才真正起飞。在接下来的几个月里，所有数据库管理系统（DBMS）供应商都发布了适用于所有系统类别的MCP服务器：OLAP（如&lt;a href=&quot;https://github.com/ClickHouse/mcp-clickhouse&quot;&gt;ClickHouse&lt;/a&gt;&quot;、&lt;a href=&quot;https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-agents-mcp&quot;&gt;Snowflake&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/firebolt-db/mcp-server&quot;&gt;Firebolt&lt;/a&gt;&quot;、&lt;a href=&quot;https://yellowbrick.com/blog/application-development/yellowbrick-mcp-server-llms-cutting-code-time-and-speeding-up-etl-development/&quot;&gt;Yellowbrick&lt;/a&gt;&quot;）、SQL（如&lt;a href=&quot;https://www.yugabyte.com/blog/yugabytedb-mcp-server/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://blogs.oracle.com/database/introducing-mcp-server-for-oracle-database&quot;&gt;Oracle&lt;/a&gt;&quot;、&lt;a href=&quot;https://planetscale.com/docs/vitess/connecting/mcp&quot;&gt;PlanetScale&lt;/a&gt;&quot;）和NoSQL（如&lt;a href=&quot;https://www.mongodb.com/company/blog/announcing-mongodb-mcp-server&quot;&gt;MongoDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/neo4j-contrib/mcp-neo4j&quot;&gt;Neo4j&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/redis/mcp-redis&quot;&gt;Redis&lt;/a&gt;&quot;）。由于Postgres MCP服务器没有官方的，所以每个Postgres DBaaS都发布了自己的服务器（如&lt;a href=&quot;https://github.com/timescale/pg-aiguide&quot;&gt;Timescale&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/supabase-community/supabase-mcp&quot;&gt;Supabase&lt;/a&gt;&quot;、&lt;a href=&quot;https://xata.io/blog/built-xata-mcp-server&quot;&gt;Xata&lt;/a&gt;&quot;）。云供应商则发布了多数据库MCP服务器，可以与他们托管的任何数据库服务进行通信（如&lt;a href=&quot;https://aws.amazon.com/blogs/database/supercharging-aws-database-development-with-aws-mcp-servers/&quot;&gt;亚马逊云科技&lt;/a&gt;&quot;、&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/developer/azure-mcp-server/tools/azure-sql&quot;&gt;微软&lt;/a&gt;&quot;、&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/mcp-toolbox-for-databases-now-supports-model-context-protocol&quot;&gt;谷歌&lt;/a&gt;&quot;）。允许单一网关与异构数据库通信，几乎已经实现了理想中的&lt;a href=&quot;https://en.wikipedia.org/wiki/Federated_database_system&quot;&gt;联合数据库&lt;/a&gt;&quot;，但还不完全。据我所知，在这些MCP服务器中，每个请求每次仅针对单个数据库，因此需要应用程序负责执行跨源连接操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了供应商的官方MCP实现方案外，几乎每种数据库管理系统（DBMS）都存在&lt;a href=&quot;https://github.com/TensorBlock/awesome-mcp-servers/blob/main/docs/databases.md&quot;&gt;数百种&lt;/a&gt;&quot;非官方的MCP服务器实现方案。其中部分方案试图支持多个系统（如&lt;a href=&quot;https://dbhub.ai/&quot;&gt;DBHub&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/FreePeak/db-mcp-server&quot;&gt;DB MCP Server&lt;/a&gt;&quot;）。关于PostgreSQL MCP服务器，DBHub曾发布过&lt;a href=&quot;https://dbhub.ai/blog/state-of-postgres-mcp-servers-2025&quot;&gt;一篇不错的综述&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个有趣而又已经证明对代理有帮助的特性是数据库分支。虽然不特定于MCP服务器，但分支允许代理快速测试数据库更改，而不影响生产应用程序。2025年7月，Neon报告说，代理&lt;a href=&quot;https://www.linkedin.com/posts/amitkumarvsingh_ai-agents-are-creating-more-databases-on-activity-7336398117862371328-Q6pO/&quot;&gt;创建了80%的数据库&lt;/a&gt;&quot;。Neon从一开始设计就支持&lt;a href=&quot;https://dev.to/semaphore/a-first-look-at-neon-a-postgres-database-that-branches-10e6&quot;&gt;分支&lt;/a&gt;&quot;（早先在这个系统还叫&lt;a href=&quot;https://dbdb.io/db/neon#history&quot;&gt;Zenith&lt;/a&gt;&quot;时，Nikita就向我做过演示），而其他系统则是后来才添加了分支支持。要了解更多信息，可以看下Xata最近发表的一篇关于数据库分支的&lt;a href=&quot;https://xata.io/blog/neon-vs-supabase-vs-xata-postgres-branching-part-1&quot;&gt;对比文章&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一方面，我很高兴现在有一个标准，可以用来向更多的应用程序暴露数据库的功能。但没有人应该信任一个拥有无限数据库访问权限的应用程序，无论是通过MCP还是系统的常规API。而且，只授予账户最小权限仍然是一个好习惯，特别是在未监控的代理可能在你的数据库中疯狂操作时，对账户做限制显得尤为重要。这意味着，当大型语言模型开始大范围流行时，为每个账户授予管理员权限或所有服务使用同一个账户，诸如这样的懒散做法将彻底行不通。当然，如果你们公司不介意把数据库向&lt;a href=&quot;https://www.theregister.com/2025/01/30/deepseek_database_left_open/&quot;&gt;全世界开放&lt;/a&gt;&quot;，并导致某家最富有的公司市值&lt;a href=&quot;https://www.theregister.com/2025/01/30/deepseek_database_left_open/&quot;&gt;暴跌6000亿美元&lt;/a&gt;&quot;，那么恶意MCP请求就不是你最需要担心的问题了。&amp;nbsp;从我对一些MCP服务器实现的粗略检查来看，它们是简单的代理，只是负责将MCP JSON请求转换为数据库查询，并没有通过深入的自省来理解请求的目的以及它是否合适。有人会尝试在你的应用程序中&lt;a href=&quot;https://www.youtube.com/watch?v=DF8Pny3VTg8&quot;&gt;订购18000个水杯&lt;/a&gt;&quot;，你需要确保它不会导致数据库崩溃。有些MCP服务器有基本的保护机制（如ClickHouse只允许&lt;a href=&quot;https://clickhouse.com/docs/use-cases/AI/MCP#clickhouse-mcp-server&quot;&gt;只读查询&lt;/a&gt;&quot;）。DBHub提供了一些额外的&lt;a href=&quot;https://dbhub.ai/#why-dbhub&quot;&gt;保护&lt;/a&gt;&quot;，如限制每个请求返回的记录数并实现了查询超时。Supabase的文档提供了MCP代理的&lt;a href=&quot;https://supabase.com/docs/guides/getting-started/mcp#recommendations&quot;&gt;最佳实践指南&lt;/a&gt;&quot;，但也得人类遵循它们才行。当然，如果你依赖于人类做正确的事情，那么&lt;a href=&quot;https://www.generalanalysis.com/blog/supabase-mcp-blog&quot;&gt;坏事就在所难免&lt;/a&gt;&quot;。&amp;nbsp;企业DBMS有着开源系统缺乏的自动化护栏和其他安全机制，对于智能代理生态系统，它们做了更好的准备，比如，&lt;a href=&quot;https://www.ibm.com/docs/en/gdp/12.x?topic=overview-guardium&quot;&gt;IBM Guardium&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.oracle.com/security/database-security/audit-vault-database-firewall/&quot;&gt;Oracle Database Firewall&lt;/a&gt;&quot;能够识别并阻止异常查询。我不是在为这些大型科技公司做宣传。我知道，未来我们将看到更多智能代理妨害生活的例子，比如&lt;a href=&quot;https://twitter.com/emil_priver/status/1783399265366052877&quot;&gt;意外删除数据库&lt;/a&gt;&quot;。将MCP服务器与代理（如连接池）结合是引入自动化保护机制的绝佳机会。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MongoDB起诉FerretDB&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到现在，MongoDB作为NoSQL领域的中坚已经有二十年了。2021年，Percona高层启动了FerretDB项目，旨在提供一款中间件代理，将MongoDB查询转换为适配PostgreSQL后端的SQL。有了这个代理，不用重写查询就可以将MongoDB应用程序无缝地迁移至PostgreSQL。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;双方共存数年后，MongoDB于2023年向FerretDB发出&lt;a href=&quot;https://blocksandfiles.com/wp-content/uploads/2025/04/Letter-from-MongoDB-to-FerretDB_3-Nov-2023-signed.pdf&quot;&gt;停止侵权通知书&lt;/a&gt;&quot;，指控FerretDB侵犯其专利权、著作权及商标权，并违反了MongoDB文档及有线协议规范的许可条款。2025年5月，MongoDB就这些问题向FerretDB&lt;a href=&quot;https://youtu.be/11BlEYtj53Q&quot;&gt;提起&lt;/a&gt;&quot;&lt;a href=&quot;https://dockets.justia.com/docket/delaware/dedce/1:2025cv00641/89247&quot;&gt;联邦诉讼&lt;/a&gt;&quot;，使这封信件公之于众。双方争议的焦点之一是，FerretDB未经授权便宣称其产品可作为MongoDB“&lt;a href=&quot;https://blog.ferretdb.io/ferretdb-1-0-ga-opensource-mongodb-alternative/&quot;&gt;即插即用的替代品&lt;/a&gt;&quot;”。MongoDB的&lt;a href=&quot;https://storage.courtlistener.com/recap/gov.uscourts.ded.89247/gov.uscourts.ded.89247.1.0.pdf&quot;&gt;法庭文件&lt;/a&gt;&quot;列举了标准指控： (1) 误导开发人员；(2) 弱化商标价值；(3) 损害企业声誉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让这个故事变得更加复杂的是，微软宣布将与MongoDB兼容的&lt;a href=&quot;https://documentdb.io/&quot;&gt;DocumentDB&lt;/a&gt;&quot;捐赠给&lt;a href=&quot;https://www.linuxfoundation.org/press/linux-foundation-welcomes-documentdb-to-advance-open-developer-first-nosql-innovation&quot;&gt;Linux基金会&lt;/a&gt;&quot;。该项目的网站提到，DocumentDB与MongoDB驱动程序兼容，并且旨在“&lt;a href=&quot;https://documentdb.io/#:~:text=our%20mission%20is%20to%20build%20a%20MongoDB%20compatible%20open%20source%20document%20database&quot;&gt;构建一个与MongoDB兼容的开源文档数据库&lt;/a&gt;&quot;”。还有其他主流的数据库供应商参与了该项目，如亚马逊云科技和Yugabyte。粗看之下，这种语言似乎与MongoDB指控的FerretDB的行为如出一辙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我没有找到数据库公司因对方复制其API而起诉对方的例子。最接近的例子是Oracle起诉谷歌在安卓系统中使用了Java API的“清洁室副本”。最终，最高法院以公平使用为由&lt;a href=&quot;https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America%2C_Inc.&quot;&gt;支持了谷歌&lt;/a&gt;&quot;。这个案例影响了法律上对重新实现行为的处理方式。&amp;nbsp;我不知道如果这场诉讼真进入庭审阶段会如何发展。陪审团是由随机挑选的路人组成的，他们或许无法理解MongoDB有线协议的具体细节，但他们绝对清楚FerretDB最初的名字是&lt;a href=&quot;https://www.reddit.com/r/programming/comments/qlyalj/mangodb_a_truly_open_source_mongodb_alternative/&quot;&gt;MangoDB&lt;/a&gt;&quot;。要说服陪审团，相信你给公司起名时仅替换一个字母不是想转移客户，这将非常困难。更何况这根本不是个原创名称：早就有个恶搞数据库管理系统叫&lt;a href=&quot;https://dbdb.io/db/mangodb&quot;&gt;MangoDB&lt;/a&gt;&quot;，它会把所有数据写入/dev/null。&amp;nbsp;说到数据库系统的命名时，微软选择“DocumentDB”让人觉得遗憾。市面上已经有&lt;a href=&quot;https://aws.amazon.com/documentdb/&quot;&gt;Amazon DocumentDB&lt;/a&gt;&quot;（顺便说一下，它也&lt;a href=&quot;https://docs.aws.amazon.com/documentdb/latest/developerguide/compatibility.html#mongodb-80&quot;&gt;兼容&lt;/a&gt;&quot;MongoDB，不过亚马逊云科技可能为此付了费）、&lt;a href=&quot;https://docs.intersystems.com/irislatest/csp/docbook/DocBook.UI.Page.cls?KEY=GDOCDB_intro&quot;&gt;InterSystems DocDB&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.yugabyte.com/stable/architecture/docdb/&quot;&gt;Yugabyte DocDB&lt;/a&gt;&quot;。微软的“Cosmos DB”在2016年推出时的原始名称也是&lt;a href=&quot;https://auth0.com/blog/documentdb-with-aspnetcore/&quot;&gt;DocumentDB&lt;/a&gt;&quot;。&amp;nbsp;最后，MongoDB的法庭文件声称，他们“开创了‘非关系型’数据库”。这个说法是不正确的。第一个通用数据库管理系统是非关系型的，因为关系模型那时候还没有发明出来。通用电气的&lt;a href=&quot;https://en.wikipedia.org/wiki/Integrated_Data_Store&quot;&gt;Integrated Data Store&lt;/a&gt;&quot;（1964年）使用了&lt;a href=&quot;https://en.wikipedia.org/wiki/Network_model&quot;&gt;网络数据模型&lt;/a&gt;&quot;，IBM的&lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_Information_Management_System&quot;&gt;Information Management System&lt;/a&gt;&quot;（1966年）使用了&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_database_model&quot;&gt;层次数据模型&lt;/a&gt;&quot;。MongoDB也不是第一个文档数据库管理系统。这个头衔应该归属于1980年代末的面向对象数据库管理系统（如&lt;a href=&quot;http://www.versant.com/products/versant-object-database&quot;&gt;Versant&lt;/a&gt;&quot;）或2000年代的XML数据库管理系统（如&lt;a href=&quot;https://www.progress.com/marklogic&quot;&gt;MarkLogic&lt;/a&gt;&quot;）。只是与它们相比，MongoDB取得了压倒性的成功（也许IMS除外）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;文件格式之争&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件格式是数据系统中过去十年间基本处于停滞状态的一个领域。2011年，Meta公司针对Hadoop发布了名为&lt;a href=&quot;https://en.wikipedia.org/wiki/RCFile&quot;&gt;RCFile&lt;/a&gt;&quot;的列式存储格式。两年后，Meta对RCFile做了优化，并推出了基于PAX的&lt;a href=&quot;https://orc.apache.org/&quot;&gt;ORC&lt;/a&gt;&quot;（Optimized Record Columnar File）格式。ORC发布一个月后，Twitter联合Cloudera推出了&lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Parquet&lt;/a&gt;&quot;的首个版本。近十五年后，Parquet已成为开源领域占支配地位的文件格式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，有五个新的开源文件格式发布，都在争取取代Parquet的地位：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/cwida/FastLanes&quot;&gt;CWI FastLanes&lt;/a&gt;&quot;&lt;a href=&quot;https://github.com/future-file-format/f3&quot;&gt;CMU + Tsinghua F3&lt;/a&gt;&quot;&lt;a href=&quot;https://vortex.dev/&quot;&gt;SpiralDB Vortex&lt;/a&gt;&quot;&lt;a href=&quot;https://github.com/AnyBlox&quot;&gt;德国人的AnyBlox&lt;/a&gt;&quot;&lt;a href=&quot;https://web.archive.org/web/20250802074742/https://github.com/microsoft/amudai&quot;&gt;微软Amudai&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是2024年发布的格式：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/facebookincubator/nimble&quot;&gt;Meta Nimble&lt;/a&gt;&quot;&lt;a href=&quot;https://lancedb.com/blog/lance-v2/&quot;&gt;LanceDB Lance&lt;/a&gt;&quot;&lt;a href=&quot;https://tsfile.apache.org/&quot;&gt;IoTDB TsFile&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spiraldb.com/&quot;&gt;SpiralDB&lt;/a&gt;&quot;今年最引人瞩目的举措是宣布&lt;a href=&quot;https://www.linuxfoundation.org/press/lf-ai-data-foundation-hosts-vortex-project-to-power-high-performance-data-access-for-ai-and-analytics&quot;&gt;将Vortex捐赠给Linux基金会&lt;/a&gt;&quot;，并成立了多组织指导委员会。微软则在2025年底悄然&lt;a href=&quot;https://github.com/microsoft/amudai&quot;&gt;终止&lt;/a&gt;&quot;了Amudai项目（至少将其转为闭源）。其余项目（FastLanes、F3、Anyblox）均属学术原型，其中Anyblox今年斩获了&lt;a href=&quot;https://www.linkedin.com/posts/janagiceva_im-thrilled-and-honored-to-share-that-our-activity-7368909487023329281-mhDv/&quot;&gt;VLDB最佳论文奖&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种新的竞争点燃了Parquet开发社区对其功能进行现代化改进的热情。Parquet PMC主席&lt;a href=&quot;http://julien.ledem.net/&quot;&gt;Julien Le Dem&lt;/a&gt;&quot;对列式文件格式格局做了&lt;a href=&quot;https://sympathetic.ink/2025/12/11/Column-Storage-for-the-AI-era.html&quot;&gt;深入的技术分析&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Parquet的主要问题并非源于格式本身。该规范可以且已经经过演进。没有人会要求组织机构重写PB级的旧文件以更新至最新的Parquet版本。问题在于，人们用不同的语言实现了大量的读写库，而每个库只支持这个规范的特定子集。通过对实际环境中Parquet文件的&lt;a href=&quot;https://bsky.app/profile/andypavlo.bsky.social/post/3m256lckmec2z&quot;&gt;分析&lt;/a&gt;&quot;，我们发现，94%的文件仅使用了2013年发布的v1版本的特性，即便其创建时间戳晚于2020年。这种最低公约数意味着：当有人使用v2版本的特性创建文件时，系统能否正确读取该文件完全取决于其版本兼容性。&amp;nbsp;我与清华大学的&lt;a href=&quot;https://xinyuzeng.github.io/&quot;&gt;Xinyu Zeng&lt;/a&gt;&quot;、&lt;a href=&quot;https://dl.acm.org/profile/99661226655&quot;&gt;Ruijun Meng&lt;/a&gt;&quot;、&lt;a href=&quot;https://people.iiis.tsinghua.edu.cn/~huanchen/&quot;&gt;Huanchen Zhang&lt;/a&gt;&quot;、CMU的&lt;a href=&quot;https://www.cs.cmu.edu/~mprammer/&quot;&gt;Martin Prammer&lt;/a&gt;&quot;、&lt;a href=&quot;https://csd.cmu.edu/people/faculty/jignesh-patel&quot;&gt;Jignesh Patel&lt;/a&gt;&quot;以及&lt;a href=&quot;https://wesmckinney.com/&quot;&gt;Wes McKinney&lt;/a&gt;&quot;一起开发了F3文件格式。我们的重点是通过提供作为共享对象的原生解码器（Rust crates）和在文件中嵌入这些解码器的WASM版本来解决互操作性问题。如果有人创建了一种新的编码格式，而数据库管理系统尚未提供原生支持，那么它仍然可以使用WASM版本通过传递Arrow缓冲区来读取数据。每个解码器针对单个列，这使得DBMS能够针对单个文件同时使用原生解码器和WASM解码器。AnyBlox采用了一种不同的方法，生成单个WASM程序来解码整个文件。&amp;nbsp;我不知道谁会赢得文件格式之争。下一场较量很可能围绕GPU支持展开。SpiralDB似乎正在采取正确的举措，但Parquet的普及性将构成一个巨大的挑战。至于DuckLake如何寻求颠覆Iceberg，我甚至还没有讨论……&amp;nbsp;当然，每当这个话题出现时，总有人会贴出&lt;a href=&quot;https://xkcd.com/927/&quot;&gt;那幅关于标准竞争的xkcd漫画&lt;/a&gt;&quot;。我已经看过了，别再发邮件给我了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;偶然事件&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据库是大生意。让我们逐一了解下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;收购&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;市场上有很多动作。为了准备一笔收购，Pinecone在9月份&lt;a href=&quot;https://venturebeat.com/data-infrastructure/pinecone-founder-edo-liberty-appoints-googler-ash-as-ceo&quot;&gt;更换了CEO&lt;/a&gt;&quot;，但我没有听到任何其他的消息。以下是已经发生的收购：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.datastax.com/blog/ibm-plans-to-acquire-datastax&quot;&gt;DataStax → IBM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这家Cassandra的坚定支持者年初被IBM收购，&lt;a href=&quot;https://www.linkedin.com/posts/nathanlatka_saas-datastax-activity-7300252058274672640-OQx_/&quot;&gt;估值30亿美元&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://quickwit.io/blog/quickwit-joins-datadog&quot;&gt;Quickwit → DataDog&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;作为Lucene替代方案的领军企业，全文搜索引擎&lt;a href=&quot;https://github.com/quickwit-oss/tantivy&quot;&gt;Tantivy&lt;/a&gt;&quot;已于年初被收购。好消息是，Tantivy的开发工作仍在继续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/blog/dbt-labs-announces-sdf-labs-acquisition&quot;&gt;SDF → dbt&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这次收购对dbt来说是一个很好的补充，也是他们今年发布的&lt;a href=&quot;https://www.getdbt.com/product/fusion&quot;&gt;Fusion&lt;/a&gt;&quot;的一部分，使他们能够在DAG中进行更严格的SQL分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mongodb.com/company/blog/news/redefining-database-ai-why-mongodb-acquired-voyage-ai&quot;&gt;Voyage.ai → MongoDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Mongo收购了一家初创AI公司，旨在&lt;a href=&quot;https://news.ycombinator.com/item?id=43160731&quot;&gt;增强&lt;/a&gt;&quot;其云产品中的RAG能力。在公告前一周，我&lt;a href=&quot;https://www.linkedin.com/in/wangpatrick57/&quot;&gt;最优秀的学生&lt;/a&gt;&quot;之一加入了Voyage。他以为自己不与数据库公司签约背叛了“家族”，结果最终还是加入了一家数据库公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://neon.tech/blog/neon-and-databricks&quot;&gt;Neon → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;显然，这家PostgreSQL公司引发了一场竞购战，但Databricks以&lt;a href=&quot;https://www.wsj.com/articles/databricks-to-buy-startup-neon-for-1-billion-fdded971&quot;&gt;令人垂涎的10亿美元&lt;/a&gt;&quot;收购了它。Neon至今仍然作为一个独立服务存在，但Databricks迅速在其生态系统中将其更名为&lt;a href=&quot;https://www.databricks.com/product/lakebase&quot;&gt;Lakebase&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.wsj.com/articles/snowflake-to-buy-crunchy-data-for-250-million-233543ab&quot;&gt;CrunchyData → Snowflake&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;你知道Snowflake不会让Databricks在夏天独占所有风头，所以他们为CrunchyData这家有着13年历史的PostgreSQL公司支付了2.5亿美元。近年来，Crunchy从Citus吸引了一些顶级人才，并在Snowflake收购他们之前扩大了其DBaaS产品。Snowflake在2025年12月宣布公开预览其&lt;a href=&quot;https://www.snowflake.com/en/product/features/postgres/&quot;&gt;Postgres&lt;/a&gt;&quot;服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/amp/2025/05/27/salesforce-informatica-deal.html&quot;&gt;Informatica → Salesforce&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Informatica，这家1990年代的老派ETL公司被Salesforce以&lt;a href=&quot;https://finance.yahoo.com/news/salesforce-buys-informatica-8b-failed-150907984.html&quot;&gt;80亿美元&lt;/a&gt;&quot;的价格收购。这家公司于1999年上市，2015年转为PE，然后在2021年再次上市。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://investors.couchbase.com/news-releases/news-release-details/couchbase-be-acquired-haveli-investments-15-billion&quot;&gt;Couchbase → 私募股权&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;老实说，我一直不明白Couchbase在2021年是如何上市的，莫非是借了MongoDB的东风？几年前，通过整合加州大学欧文分校&lt;a href=&quot;https://www.couchbase.com/press-releases/couchbase-announces-first-commercial-implementation-of-sql-with-n1ql-for-analytics/&quot;&gt;AsterixDB项目&lt;/a&gt;&quot;的一些组件，Couchbase做了一些有趣的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/business/finance/databricks-buy-sequoia-backed-tecton-ai-agent-push-2025-08-22/&quot;&gt;Tecton → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Tecton为Databricks提供了额外的代理构建工具。我的另一位学生曾在该公司工作，现在是在Databricks。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.fivetran.com/press/fivetran-acquires-tobiko-data-to-power-the-next-generation-of-advanced-ai-ready-data-transformation&quot;&gt;Tobiko Data → Fivetran&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个团队开发了两个有用的工具：&lt;a href=&quot;https://sqlmesh.readthedocs.io/&quot;&gt;SQLMesh&lt;/a&gt;&quot;和&lt;a href=&quot;https://sqlglot.com/&quot;&gt;SQLglot&lt;/a&gt;&quot;。前者是唯一可与dbt（见下文，计划与Fivetran合并）抗衡的开源竞争者。SQLglot是一个便捷的SQL解析器/反解析器，支持启发式的查询优化器。未来几年，Fivetran与SDF将该技术与dbt相结合，将在该领域形成引人注目的技术布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.businesswire.com/news/home/20250910856970/en/SingleStore-Announces-Growth-Buyout-Led-by-Vector-Capital&quot;&gt;SingleStore → 私募股权&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;购买SingleStore的PE公司（&lt;a href=&quot;https://www.vectorcapital.com/&quot;&gt;Vector Capital&lt;/a&gt;&quot;）以前有管理数据库公司的经验。之前在2020年，他们曾经&lt;a href=&quot;https://www.businesswire.com/news/home/20201021005279/en/Vector-Capital-Completes-Acquisition-of-MarkLogic&quot;&gt;购买了XML数据库公司MarkLogic&lt;/a&gt;&quot;，并在2023年将其&lt;a href=&quot;https://investors.progress.com/news-releases/news-release-details/progress-announces-plans-acquire-marklogic&quot;&gt;转手给Progress&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dbta.com/Editorial/News-Flashes/MariaDB-to-Acquire-Galera-Cluster-to-Enable-Deeper-Integration-of-Synchronous-Replication-Technology-169742.aspx&quot;&gt;Codership → MariaDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;在2024年被PE公司收购后，MariaDB公司今年开启了收购狂潮。首当其冲的是开发MariaDB扩展中间件Galera Cluster的公司。详见我2023年对&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html#mariadb&quot;&gt;MariaDB混乱局面&lt;/a&gt;&quot;的全面分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.crn.com/news/cloud/2025/mariadb-buys-back-skysql-in-database-flexibility-push&quot;&gt;SkySQL → MariaDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;然后是MariaDB的第二笔收购。为避免混淆，我需要说明一下：2010年的时候，最初为MariaDB提供支持的商业公司名为“SkySQL Corporation”，2014年，它更名为“MariaDB Corporation”。2020年，MariaDB Corporation推出名为SkySQL的MariaDB数据库即服务（DBaaS）。但因资金持续流失，该公司于2023年&lt;a href=&quot;https://www.businesswire.com/news/home/20231214486927/en/MariaDB-Finalizes-Spinoff-of-SkySQL&quot;&gt;将SkySQL Inc.剥离&lt;/a&gt;&quot;出去，成为一家独立的公司。而2025年，MariaDB Corporation&lt;a href=&quot;https://medium.com/@arbaudie.it/personal-opinion-mariadb-re-acquires-skysql-125181507358&quot;&gt;回购了SkySQL Inc.&lt;/a&gt;&quot;，兜了一圈后回到了原处。今年我的数据库宾果卡上可没有这一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.crystaldba.ai/blog/post/temporal-technologies-acquires-crystal-dba&quot;&gt;Crystal DBA → Temporal&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;自动化数据库优化工具公司Crystal DBA加入Temporal公司，帮他们自动优化数据库！很高兴得知Crystal创始人、伯克利数据库小组校友Johann Schleier-Smith在那里发展顺利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.harvestmp.com/transactions&quot;&gt;HeavyDB → Nvidia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个系统（之前叫OmniSci，再之前叫MapD）是首批GPU加速数据库之一，于2013年推出。除了一家并购公司披露了这笔成功的交易外，我未能找到有关交易完成的官方公告。随后我们与英伟达召开会议，探讨潜在的数据库研究合作事宜，期间几位HeavyDB的伙伴也现身参与。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.prnewswire.com/news-releases/istari-digital-acquires-dgraph-to-strengthen-data-foundation-for-ai-and-engineering-302593246.html&quot;&gt;DGraph → Istari Digital&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Dgraph之前&lt;a href=&quot;https://web.archive.org/web/20250806150448/https://hypermode.com/blog/the-future-of-dgraph-is-open-serverless-and-ai-ready&quot;&gt;在2023年被Hypermode收购&lt;/a&gt;&quot;。现在看来，Istari只是买了Dgraph，而不是Hypermode的其他部分（或者他们放弃了）。我还没见过任何积极使用Dgraph的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mews.com/en/press/mews-acquires-datachat&quot;&gt;DataChat → Mews&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是最早支持“与数据库对话”的数据库之一，来自威斯康星大学的Jignesh Patel，现为CMU-DB教授。但后来被一家欧洲酒店管理领域的SaaS公司收购了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://siliconangle.com/2025/11/10/snowflake-acquires-database-migration-startup-datometry/&quot;&gt;Datometry → Snowflake&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;多年来，Datometry一直致力于将旧版SQL方言（如Teradata）自动转换至新型OLAP系统这一棘手的问题。Snowflake收购他们是为了扩展自己的&lt;a href=&quot;https://www.snowflake.com/en/blog/accelerate-data-migration-datometry-technology/&quot;&gt;迁移工具&lt;/a&gt;&quot;。更多信息参见&lt;a href=&quot;https://www.youtube.com/watch?v=cL1-BIaQSYE&amp;amp;list=PLSE8ODhjZXjagqlf1NxuBQwaMkrHXi-iz&amp;amp;index=23&quot;&gt;Datometry 2020年的CMU-DB技术讲座&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://clickhouse.com/blog/librechat-open-source-agentic-data-stack&quot;&gt;LibreChat → ClickHouse&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;像Snowflake收购Datometry一样，ClickHouse的这次收购是提升高性能通用OLAP引擎开发体验的典范。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/mooncake-labs-joins-databricks-accelerate-vision-lakebase&quot;&gt;Mooncake → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;在收购Neon之后，为了使PostgreSQL能够读写Apache Iceberg数据，Databricks收购了Mooncake。更多信息参见他们2025年11月的&lt;a href=&quot;https://www.youtube.com/watch?v=VqFZyWHGQVM&amp;amp;list=PLSE8ODhjZXjbEeW_bOCZ8c_nx_Jhoz-GW&amp;amp;index=8&quot;&gt;CMU-DB讲座&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/technology/ibm-nears-roughly-11-billion-deal-confluent-wsj-reports-2025-12-08/&quot;&gt;Confluent → IBM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是一个将草根开源项目发展为一家公司的经典案例。Kafka最初于2011年在Linkedin开发，随后在2014年，Confluent作为独立的初创公司分拆出来，于七年后的2021年成功上市。随后IBM斥巨资将其收购。与DataStax的情况相似，目前尚不确定IBM是会对Confluent采取&lt;a href=&quot;https://news.ycombinator.com/item?id=43200706&quot;&gt;惯常的企业收购策略&lt;/a&gt;&quot;，还是像RedHat那样使其保持独立运营。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;Gel → Vercel&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;前身为&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html#random-naming&quot;&gt;EdgeDB&lt;/a&gt;&quot;，在PostgreSQL之上提供了一种DSL，被Verel在2025年年底收购。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;Kuzu → ???&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这款诞生于滑铁卢大学的嵌入式图形DBMS在2025年被一家未具名的公司收购。然后KuzuDB公司宣布放弃该开源项目。&lt;a href=&quot;https://ladybugdb.com/&quot;&gt;LadybugDB&lt;/a&gt;&quot;项目旨在维护Kuzu代码的一个分支版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;合并&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年10月，&lt;a href=&quot;https://www.fivetran.com/&quot;&gt;Fivetran&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.getdbt.com/&quot;&gt;dbt Labs&lt;/a&gt;&quot;宣布&lt;a href=&quot;https://www.reuters.com/business/a16z-backed-data-firms-fivetran-dbt-labs-merge-all-stock-deal-2025-10-13&quot;&gt;合并&lt;/a&gt;&quot;成一家公司，这个消息着实让人意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据我所知，数据库领域的上一次合并是2019年&lt;a href=&quot;https://techcrunch.com/2018/10/03/cloudera-and-hortonworks-announce-5-2-billion-merger/&quot;&gt;Cloudera和Hortonworks合并&lt;/a&gt;&quot;。但那笔交易只是两家在Hadoop领域苦苦寻找定位的公司试图通过合并成一家公司来扭转局面（剧透：他们没有成功）。2022年，MariaDB公司通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Special-purpose_acquisition_company&quot;&gt;SPAC&lt;/a&gt;&quot;与&lt;a href=&quot;https://mariadb.com/newsroom/press-releases/mariadb-completes-merger-and-lands-on-nyse-as-mrdb/&quot;&gt;Angel Pond Holdings公司&lt;/a&gt;&quot;合并，技术上讲也算并购，但那是为了让MariaDB能够上市而采取的后门策略。对&lt;a href=&quot;https://www.bizjournals.com/sanjose/news/2022/12/19/mariadb-goes-public-in-spac-merger.html&quot;&gt;投资者&lt;/a&gt;&quot;来说，结果并不好。Fivetran和dbt的合并与这两者不同（更好）——这两家互补的技术公司正联手打造ETL领域的巨头企业，为近期开展正规的IPO做准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;融资&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除非我错过了，或者他们没有宣布，数据库初创公司的早期融资轮次并不算多。围绕向量数据库的炒作已趋于平息，风险投资公司现在只愿为LLM公司花钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Databricks：&lt;a href=&quot;https://www.databricks.com/company/newsroom/press-releases/databricks-surpasses-4-8b-revenue-run-rate-growing-55-year-over-year&quot;&gt;L轮40亿美元&lt;/a&gt;&quot;Databricks：&lt;a href=&quot;https://www.reuters.com/business/databricks-eyes-over-100-billion-valuation-investors-back-ai-growth-plans-2025-08-19/&quot;&gt;K轮10亿美元&lt;/a&gt;&quot;ClickHouse：&lt;a href=&quot;https://clickhouse.com/blog/clickhouse-raises-350-million-series-c-to-power-analytics-for-ai-era&quot;&gt;C轮3.5亿美元&lt;/a&gt;&quot;Supabase：&lt;a href=&quot;https://finance.yahoo.com/news/exclusive-supabase-raises-200-million-112154867.html&quot;&gt;D轮2亿美元&lt;/a&gt;&quot;Timescale：&lt;a href=&quot;https://www.tigerdata.com/blog/year-of-the-tiger-110-million-to-build-the-future-of-data-for-developers-worldwide&quot;&gt;C轮1.1亿美元&lt;/a&gt;&quot;Supabase：&lt;a href=&quot;https://supabase.com/blog/supabase-series-e&quot;&gt;E轮1亿美元&lt;/a&gt;&quot;Astronomer：&lt;a href=&quot;https://www.astronomer.io/press-releases/astronomer-secures-93-million-series-d-funding/&quot;&gt;D轮9300万美元&lt;/a&gt;&quot;Tessel：&lt;a href=&quot;https://www.tessell.com/press-releases/tessell-raises-60m-series-b-to-expand-ai-driven-multi-cloud-data-ecosystems&quot;&gt;B轮6000万美元&lt;/a&gt;&quot;LanceDB：&lt;a href=&quot;https://lancedb.com/blog/series-a-funding/&quot;&gt;A轮3000万美元&lt;/a&gt;&quot;Convex：&lt;a href=&quot;https://news.convex.dev/convex-raises-24m/&quot;&gt;B轮2400万美元&lt;/a&gt;&quot;SpiralDB：&lt;a href=&quot;https://www.axios.com/pro/enterprise-software-deals/2025/09/11/database-startup-spiral-22-million&quot;&gt;A轮2200万美元&lt;/a&gt;&quot;ParadeDB：&lt;a href=&quot;https://techcrunch.com/2025/07/15/paradedb-takes-on-elasticsearch-as-interest-in-postgres-explodes-amid-ai-boom/&quot;&gt;A轮1200万美元&lt;/a&gt;&quot;CedarDB：&lt;a href=&quot;https://www.munich-startup.de/en/109750/cedardb-secures-53-million-euros/&quot;&gt;种子轮590万美元&lt;/a&gt;&quot;TopK：&lt;a href=&quot;https://www.topk.io/blog/seed-round&quot;&gt;种子轮550万美元&lt;/a&gt;&quot;Columnar：&lt;a href=&quot;https://columnar.tech/blog/announcing-columnar&quot;&gt;种子轮400万美元&lt;/a&gt;&quot;SereneDB：&lt;a href=&quot;https://tech.eu/2025/12/03/serenedb-lands-21m-to-fuse-search-analytics-and-postgres-into-one-engine/&quot;&gt;前种子轮210万美元&lt;/a&gt;&quot;Starburst：&lt;a href=&quot;https://www.prnewswire.com/news-releases/starburst-announces-strategic-investment-from-citi-302456950.html&quot;&gt;未披露？&lt;/a&gt;&quot;TurboPuffer：&lt;a href=&quot;https://tpuf.link/comms&quot;&gt;未披露？&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;名称变更&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我在年度总结中新增加的一个类别——数据库公司更改其公司或系统的名称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dbta.com/Editorial/News-Flashes/HarperDBs-Rebrand-Reflects-its-Commitment-to-Delivering-a-Full-Stack-Application-Delivery-Platform-168390.aspx&quot;&gt;HarperDB → Harper&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这家JSON数据库公司从名字里去掉了后缀&quot;DB&quot; ，旨在强调其作为数据库支持型应用平台的定位，类似于&lt;a href=&quot;https://www.convex.dev/&quot;&gt;Convex&lt;/a&gt;&quot;和Heroku。我很欣赏Harper的团队。2021年，他们在&lt;a href=&quot;https://www.youtube.com/watch?v=I5_xIs6xsJQ&amp;amp;list=PLSE8ODhjZXjbeqnfuvp30VrI7VXiFuOXS&amp;amp;index=7&quot;&gt;CMU-DB技术研讨会&lt;/a&gt;&quot;上提出的数据库管理系统构想可以说是我听过的最糟糕的方案。好在他们意识到该方案的缺陷后果断放弃，转而采用了LMDB技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/edgedb-is-now-gel-and-postgres-is-the-future&quot;&gt;EdgeDB → Gel&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是一个明智的举动，因为“Edge”这个名字传达了这样一个信息，它是一个用于边缘设备或服务的数据库（如&lt;a href=&quot;http://fly.io/&quot;&gt;Fly.io&lt;/a&gt;&quot;）。不过我也不确定“Gel”是否传达了项目更高层次的目标。感兴趣的读者可以观看下他们在&lt;a href=&quot;https://www.youtube.com/watch?v=RzLo-pdUJ7I&amp;amp;list=PLSE8ODhjZXjbpOIrZheFWxkYG8HD87xW1&amp;amp;index=10&quot;&gt;2025年CMU-DB技术研讨会上关于Gel查询语言（名称还是EdgeQL）的讲座&lt;/a&gt;&quot;，由CMU博士校友主讲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tigerdata.com/blog/timescale-becomes-tigerdata&quot;&gt;Timescale → TigerData&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;数据库公司为区别于其核心数据库产品而更名的案例实属罕见。通常情况是公司更名为数据库名称（如“Relational Software, Inc.”更名为“Oracle Systems Corporation”，“10gen, Inc.”更名为“MongoDB, Inc.”）。该公司有了新的定位——通用应用场景的增强版PostgreSQL，因此他们试图摆脱“专业化时间序列数据库管理系统”的固有印象，这一策略有它的合理性，毕竟前者所处的细分市场远小于后者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;倒闭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;坦白说，我曾在其中两家失败的初创公司中担任技术顾问。截止目前，我的顾问成功率可以说是惨不忍睹。我也曾担任&lt;a href=&quot;https://dbdb.io/db/splice-machine&quot;&gt;Splice Machine&lt;/a&gt;&quot;公司的顾问，但该公司已于2021年倒闭。需要说明的是，我只和他们讨论技术构想，而不涉及商业策略。我确实建议Fauna增加SQL支持功能，但他们没有采纳我的建议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoworld.com/article/3853569/fauna-to-shut-down-faunadb-service-in-may.html&quot;&gt;Fauna&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Spanner是一款颇具特色的分布式数据库管理系统，基于&lt;a href=&quot;https://www.cs.umd.edu/~abadi/&quot;&gt;Dan Abadi&lt;/a&gt;&quot;的&lt;a href=&quot;https://vldb.org/pvldb/vol3/R06.pdf&quot;&gt;确定性并发控制研究&lt;/a&gt;&quot;。恰好在NoSQL热潮逐渐消退之际，它提供了强一致性事务处理能力，使事务处理功能再度成为焦点。不过该系统采用&lt;a href=&quot;https://faunadb-docs.netlify.app/fauna/current/learn/query/&quot;&gt;专有查询语言&lt;/a&gt;&quot;，并押注了GraphQL技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/postgresml/postgresml/issues/1688#issuecomment-3041057338&quot;&gt;PostgresML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;从名字就可以看出来，该系统旨在使人们能够在他们的PostgreSQL DBMS内运行ML/AI操作。挑战在于，他们需要说服人们将现有的数据库迁移到他们提供的托管平台上。他们推出了&lt;a href=&quot;https://github.com/postgresml/pgcat&quot;&gt;pgCat&lt;/a&gt;&quot;，作为一个代理用于镜像数据库流量。其中一位联合创始人加入了Anthropic。另一位联合创始人创建了一个新的代理项目&lt;a href=&quot;https://pgdog.dev/&quot;&gt;pgDog&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-7177&quot;&gt;Derby&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是最早用Java编写的数据库管理系统之一，可以追溯到1997年（最初名为&quot;Java DB&quot;或&quot;JBMS&quot;）。2000年代，IBM将其捐赠给Apache基金会，并更名为Derby。2025年10月，该项目宣布这个系统将进入“只读模式”，因为没有人对它进行积极地维护了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.hydra.so/&quot;&gt;Hydra&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;尽管没有关于初创公司DuckDB-inside-Postgres的官方公告，但其联合创始人和员工都已经分散到了其他公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/MyScaleDB/status/1917163010311037327&quot;&gt;MyScaleDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是Clickhouse的一个分支，借助Tantivy增加了向量搜索和全文索引。他们在2025年5月宣布关闭这项服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/aocsa_as-some-of-you-may-have-seen-voltron-data-activity-7395229870517022720-sGOP/&quot;&gt;Voltron Data&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个团队应该是数据库公司里的超级组合。想象一下，就像&lt;a href=&quot;https://youtu.be/G-S9mtYowPY&quot;&gt;Run the Jewels&lt;/a&gt;&quot;那样的团队。他们有来自Nvidia Rapids的顶级工程师、&lt;a href=&quot;https://en.wikipedia.org/wiki/Wes_McKinney&quot;&gt;Apache Arrow和Python Pandas的发明者&lt;/a&gt;&quot;，以及来自&lt;a href=&quot;https://github.com/BlazingDB/blazingsql&quot;&gt;BlazingSQL&lt;/a&gt;&quot;的秘鲁GPU奇才。然后再加上来自顶级公司的风险投资1.1亿美元，包括未来的英特尔CEO（以及&lt;a href=&quot;https://en.wikipedia.org/wiki/Lip-Bu_Tan&quot;&gt;一名CMU的董事会成员&lt;/a&gt;&quot;）。他们构建了一个GPU加速的数据库&lt;a href=&quot;https://arxiv.org/abs/2508.05029&quot;&gt;Theseus&lt;/a&gt;&quot;，但未能及时推出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，尽管不是一个商业机构，但如果不提及&lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_Research#Almaden_in_Silicon_Valley&quot;&gt;IBM阿尔马登研究中心&lt;/a&gt;&quot;的&lt;a href=&quot;https://www.siliconvalley.com/2025/07/10/ibm-san-jose-tech-data-ai-internet-property-real-estate-economy-web/&quot;&gt;关闭&lt;/a&gt;&quot;，那将是我的疏忽。这个研究中心是IBM在1986年建立的，几十年来一直是数据库研究的圣地。&lt;a href=&quot;https://twitter.com/andy_pavlo/status/306455280823177216&quot;&gt;我2013年曾去阿尔马登参加面试&lt;/a&gt;&quot;，发现那里的风景很美。IBM研究中心数据库小组&lt;a href=&quot;https://dl.acm.org/doi/10.1145/126482.126493&quot;&gt;已经不是过去的样子了&lt;/a&gt;&quot;。尽管如此，这个神圣的数据库研究场所的校友名单依然令人印象深刻：&lt;a href=&quot;https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)&quot;&gt;Rakesh Agrawal&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Donald_D._Chamberlin&quot;&gt;Donald Chamberlin&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Ronald_Fagin&quot;&gt;Ronald Fagin&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Laura_M._Haas&quot;&gt;Laura Haas&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/C._Mohan&quot;&gt;Mohan&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Patricia_Selinger&quot;&gt;Pat Selinger&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Moshe_Vardi&quot;&gt;Moshe Vardi&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Jennifer_Widom&quot;&gt;Jennifer Widom&lt;/a&gt;&quot;和&lt;a href=&quot;https://scholar.google.com/citations?user=wUkamYwAAAAJ&amp;amp;hl=en&quot;&gt;Guy Lohman&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026-01-05更新：我遗漏了Gel在2025年12月被Vercel收购的消息。[&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;2026-01-05更新：我也遗漏了Supabase在2025年进行了两轮融资的消息。&lt;/p&gt;&lt;p&gt;2026-01-05更新：尽管TurboPuffer没有就融资发表官方声明，但他们的CEO提到，其团队中增加了来自Thrive Capital的成员。[&lt;a href=&quot;https://www.linkedin.com/in/julianlaneve&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;2026-01-05更新：显然，我需要一个更好的方法来跟踪融资信息，因为我还遗漏了LanceDB的A轮融资！[&lt;a href=&quot;https://twitter.com/brittwalker_/status/2008306941286904111&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有人说，我是根据数据库开发公司筹集的资金数额来判断数据库的质量，显然不是这样。我之所以追踪这些动态，是因为数据库研究领域竞争激烈且充满活力。我不仅要与其他高校的学者“竞争”，还需要持续关注大型科技公司和小型创业公司推出的有趣的系统。行业研究实验室已经不是过去的样子了，只有微软研究院仍然在积极招聘顶尖人才，并做出令人难以置信的工作。&amp;nbsp;我曾&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2022/12/2022-databases-retrospective.html#:~:text=fate%20of%20database%20start%2Dups&quot;&gt;在2022年预测&lt;/a&gt;&quot;，2025年将有大量的数据库公司倒闭。确实，今年关闭的公司比往年多，但并没有达到我预期的规模。&amp;nbsp;Voltron的倒闭以及类似HeavyDB这样的收购兼并似乎延续了GPU加速数据库不可行的趋势。&lt;a href=&quot;https://twitter.com/KineticaHQ/status/1988983193870156171&quot;&gt;Kinetica&lt;/a&gt;&quot;多年来一直靠政府合同维持运营，而&lt;a href=&quot;https://sqream.com/&quot;&gt;Sqream&lt;/a&gt;&quot;似乎也是在勉强支撑。这些公司仍属于小众领域，至今无人能撼动CPU驱动型DBMS的主导地位。虽不便透露具体厂商的名字，但2026年必将有多家供应商发布GPU加速数据库的重要公告。这进一步印证了OLAP引擎的商品化趋势：现代系统的运行速度已经实现了飞跃，底层操作（扫描、连接）的性能差异微乎其微，系统间的差异化竞争正转向用户体验以及优化器生成的查询计划的质量。&amp;nbsp;Couchbase和SingleStore被私募股权（PE）公司收购可能预示着数据库行业未来的一个发展趋势。当然，PE收购以前也发生过，但似乎都是在最近：（1）&lt;a href=&quot;https://www.vectorcapital.com/investments/case-study/marklogic&quot;&gt;MarkLogic&lt;/a&gt;&quot;在2020年、（2）&lt;a href=&quot;https://techcrunch.com/2021/06/01/cloudera-to-go-private-as-kkr-cdr-grab-it-for-5-3b/&quot;&gt;Cloudera&lt;/a&gt;&quot;在2021年、（3）&lt;a href=&quot;https://techcrunch.com/2024/09/10/mariadb-goes-private-with-new-ceo-as-k1-closes-acquisition/&quot;&gt;MariaDB&lt;/a&gt;&quot;在2023年。我能找到的发生在2020年之前的收购只有2007年的&lt;a href=&quot;https://www.channelinsider.com/tech-companies/ibm-buys-database-software-firm/&quot;&gt;SolidDB&lt;/a&gt;&quot;和2015年的&lt;a href=&quot;https://www.aakashg.com/story-informatica-second-ipo/&quot;&gt;Informatica&lt;/a&gt;&quot;。PE收购可能会逆转那些数据库公司的发展趋势，它们在被控股公司收购后发展陷入停滞，而那些控股公司则通过榨取维护费持续获利（如Actian、Rocket）。即使是Oracle，也依然在从30年前收购的&lt;a href=&quot;https://www.oracle.com/database/technologies/related/rdb.html&quot;&gt;RDB/VMS&lt;/a&gt;&quot;上获利！&amp;nbsp;最后，向&lt;a href=&quot;https://www.linkedin.com/in/nikitashamgunov&quot;&gt;Nikita Shamgunov&lt;/a&gt;&quot;致敬。据我所知，他是唯一一位与人联合创立两家数据库公司（&lt;a href=&quot;https://hackernoon.com/founder-interviews-nikita-shamgunov-of-memsql-8a9ca8d33552&quot;&gt;SingleStore&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.madrona.com/building-a-modern-database-neon-nikita-shamgunov-serverless-postgres/&quot;&gt;Neon&lt;/a&gt;&quot;）且两家公司在同一年被收购的人。就像已故说唱歌手DMX在一年内推出两张冠军专辑（&lt;a href=&quot;https://en.wikipedia.org/wiki/It%27s_Dark_and_Hell_Is_Hot&quot;&gt;It&#39;s Dark and Hell Is Hot&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Flesh_of_My_Flesh,_Blood_of_My_Blood&quot;&gt;Flesh of My Flesh&lt;/a&gt;&quot;）那样，我认为短期内无人能打破Nikita的纪录。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据库元老的表现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看看数据库元老拉里·埃里森的辉煌之年。这位81岁的老人在这一年间取得的成就，远超常人毕生所为。我将按时间顺序逐一梳理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拉里年初时位列全球富豪榜第三。想到自己身价可能不及马克·扎克伯格，他夜不能寐。有人说拉里的失眠源于饮食变化——自从&lt;a href=&quot;https://www.bbc.com/news/uk-england-oxfordshire-67221202&quot;&gt;买下英国的一家著名酒吧&lt;/a&gt;&quot;后，他馅饼吃多了。但我可以向各位保证，拉里坚持三十年的“&lt;a href=&quot;https://tech.yahoo.com/science/articles/80-old-billionaire-larry-ellison-105236014.html&quot;&gt;素食水瓶座饮食法&lt;/a&gt;&quot;”从未改变。直到2025年4月，我们得知拉里&lt;a href=&quot;https://www.msn.com/en-in/autos/photos/larry-ellison-becomes-second-richest-person-beats-zuckerberg-bezos-after-oracle-stock-soars/ar-AA1GKdbu&quot;&gt;重登全球富豪榜次席&lt;/a&gt;&quot;。他的睡眠质量稍有好转，但仍然远未达标。生活中的诸多烦忧仍在持续地折磨他——比如他终于决定出售那辆稀有的半合法&lt;a href=&quot;https://www.forbes.com/sites/maryroeloffs/2025/08/05/larry-ellisons-old-mclaren-f1-could-break-a-sales-record/&quot;&gt;迈凯伦F1超跑&lt;/a&gt;&quot;，车内手套箱里还完好地保存着原厂车主手册。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年7月，拉里在13年内发布了他的&lt;a href=&quot;https://twitter.com/larryellison/status/1945229587929337947&quot;&gt;第三条推文&lt;/a&gt;&quot;（拉里迷们称之为“#3”）。这条推文介绍了他在牛津大学附近创立的&lt;a href=&quot;https://eit.org/&quot;&gt;埃里森技术研究院&lt;/a&gt;&quot;（EIT）的近况。以EIT命名且与牛津大学关联，听起来像是纯研究性的非营利机构，类似于斯坦福的&lt;a href=&quot;https://en.wikipedia.org/wiki/SRI_International&quot;&gt;SRI&lt;/a&gt;&quot;或卡内基梅隆的&lt;a href=&quot;https://en.wikipedia.org/wiki/Software_Engineering_Institute&quot;&gt;SEI&lt;/a&gt;&quot;。但实际情况是，这是一家总部位于加州的有限责任公司旗下的多家营利性公司的统称。当然，不少怪咖在第3条的评论区说承诺提供&lt;a href=&quot;https://twitter.com/SFCryptoRounder/status/1946047224779030564&quot;&gt;基于区块链的低温冷冻技术&lt;/a&gt;&quot;或&lt;a href=&quot;https://twitter.com/JackSarfatti/status/1975985052204101709&quot;&gt;室温超导体&lt;/a&gt;&quot;。拉里告诉我他根本不理会这些。不过也有人像&lt;a href=&quot;https://twitter.com/aseemchandra/status/1945509650201301304&quot;&gt;这位网友&lt;/a&gt;&quot;一样真正理解其中的奥妙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年（可能是本世纪）最大的数据库新闻出现在9月10日星期三美国东部时间大约下午3:00。经过几十年的等待，拉里·约瑟夫·埃里森终于&lt;a href=&quot;https://www.theguardian.com/technology/2025/sep/10/larry-ellison-dislodges-elon-musk-as-worlds-richest-person&quot;&gt;成了世界上最富有的人&lt;/a&gt;&quot;。那天早上，&lt;a href=&quot;https://finance.yahoo.com/quote/ORCL/&quot;&gt;$ORCL&lt;/a&gt;&quot;的股价上涨了40%，由于拉里仍然拥有公司40%的股份，所以他的总身价估计是&lt;a href=&quot;https://www.bbc.com/news/articles/cx2rp992y88o&quot;&gt;3930亿美元&lt;/a&gt;&quot;。从这个角度来看，这不仅使拉里成为世界上最富有的人，而且也是整个人类历史上最富有的人。约翰·D·洛克菲勒和安德鲁·卡内基（是的，CMU中的“C”）的峰值净资产，根据通货膨胀调整后，分别只有&lt;a href=&quot;https://www.buysidedigest.com/insights/the-top-10-wealthiest-historical-figures-adjusted-for-inflation/&quot;&gt;3400亿美元&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.celebritynetworth.com/richest-businessmen/richest-billionaires/andrew-carnegie-net-worth/&quot;&gt;3100亿美元&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在拉里登上世界之巅的同时，Oracle还参与了&lt;a href=&quot;https://www.npr.org/2025/12/18/nx-s1-5648844/tiktok-deal-oracle-trump&quot;&gt;收购控制TikTok的美国公司&lt;/a&gt;&quot;，拉里&lt;a href=&quot;https://variety.com/2025/tv/news/paramount-skydance-larry-ellison-irrevocable-personal-guarantee-warner-bros-discovery-1236614728/&quot;&gt;资助派拉蒙&lt;/a&gt;&quot;（由他第四次婚姻的儿子控制）&lt;a href=&quot;https://www.nytimes.com/2025/12/24/business/media/larry-david-ellison-warner-bros-discovery-cbs.html&quot;&gt;竞购华纳兄弟&lt;/a&gt;&quot;。美国总统甚至嘲笑拉里&lt;a href=&quot;https://www.theguardian.com/us-news/2025/nov/20/warner-bros-discovery-takeover-paramount-skydance-larry-ellison&quot;&gt;接管CNN新闻部门&lt;/a&gt;&quot;，因为拉里是派拉蒙的大股东。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我甚至不知道从哪里开始。当然，当我得知拉里·埃里森因数据库而成为世界上最富有的人时，我感到&lt;a href=&quot;https://twitter.com/andy_pavlo/status/1965865919223312495&quot;&gt;由衷地欣慰&lt;/a&gt;&quot;，我们的生活终于发生了一些积极的事情。我不在乎Oracle的股价，因为那些旨在构建AI数据中心而非传统软件业务的&lt;a href=&quot;https://www.investors.com/news/technology/oracle-stock-orcl-ai-analyst-targets/&quot;&gt;高调交易&lt;/a&gt;&quot;而被人为炒高了。我也不在乎他&lt;a href=&quot;https://www.bloomberg.com/news/articles/2025-11-21/oracle-slump-sends-ellison-sliding-down-ranks-of-world-s-richest&quot;&gt;两个月内个人损失1300亿美元&lt;/a&gt;&quot;导致排名下滑。这就像你我把一个月的薪水&lt;a href=&quot;https://www.reddit.com/r/gambling/comments/1j4xby2/blew_my_whole_paycheck/&quot;&gt;全砸在了FortuneCoins上&lt;/a&gt;&quot;——虽然有点心疼，还得靠从Taco Bell买来的过期辣酱拌豆子米饭撑两周，但总会好起来的。&amp;nbsp;有些人说拉里与普通民众&lt;a href=&quot;https://news.ycombinator.com/item?id=45413203&quot;&gt;脱节&lt;/a&gt;&quot;，或者说他因为参与和数据库无直接关系的事情而迷失了方向。他们列举了多个例子，比如他&lt;a href=&quot;https://techcrunch.com/2025/02/23/the-lesson-of-larry-ellisons-misadventures-in-farming/&quot;&gt;在夏威夷的机器人农场&lt;/a&gt;&quot;以每磅24美元（每公斤41欧元）的价格&lt;a href=&quot;https://beatofhawaii.com/the-most-expensive-lettuce-in-hawaii-billionaire-larry-ellisons-24-lb-experiment/&quot;&gt;出售生菜&lt;/a&gt;&quot;，又比如81岁的男人不可能&lt;a href=&quot;https://assets.sfstandard.com/image/994911177489/image_cooaesgkll0v99j57e84lobk7k/-S3840x2560-FPNG&quot;&gt;天生拥有金发&lt;/a&gt;&quot;。&amp;nbsp;事实是，拉里·埃里森已经征服了企业级数据库领域、&lt;a href=&quot;https://sg.finance.yahoo.com/news/why-oracle-founder-larry-ellison-205016907.html&quot;&gt;竞技帆船&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.businessinsider.com/larry-ellison-hawaii-wellness-spa-sensei-lanai-photos-2021-2&quot;&gt;科技兄弟健康水疗中心&lt;/a&gt;&quot;。下一步显然是接管一个每天被成千上万在机场等待的人观看的有线电视频道。每次我和拉里交谈，他都清楚地表明他一点也不在乎人们对他的看法。他知道&lt;a href=&quot;https://twitter.com/HolgersenTobias/status/1945239198572712323&quot;&gt;他的粉丝爱他&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.financialexpress.com/life/lifestyle-who-is-jolin-zhu-worlds-richest-man-larry-ellisons-fifth-wife-47-years-younger-than-him-3974373/&quot;&gt;他的（新）妻子爱他&lt;/a&gt;&quot;。毕竟，那才是最重要的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在结束本次回顾之前，我想快速地说出几个名字并提点建议。首先是PT，他在监禁期间仍在&lt;a href=&quot;https://turso.tech/blog/working-on-databases-from-prison&quot;&gt;有条不紊地参与Turso数据库的开发&lt;/a&gt;&quot;（外面见）。然后是对JT的遭遇表示遗憾，他因为经常在社交媒体上分享与&lt;a href=&quot;https://github.com/KevoDB/kevo&quot;&gt;KevoDB&lt;/a&gt;&quot;数据库开发有关的信息而&lt;a href=&quot;https://twitter.com/canoozie/status/1952305339824574576&quot;&gt;丢掉了工作&lt;/a&gt;&quot;。务必只在测试用数据库中放入假数据，&lt;a href=&quot;https://abcnews.go.com/Business/charlie-javice-founder-lied-175m-startup-faces-sentencing/story?id=126034577&quot;&gt;不要因为以1750万美元的价格出售自己的初创公司&lt;/a&gt;&quot;换得七年的监禁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我和我的博士生们也成立了一家新的&lt;a href=&quot;https://sydht.ai/&quot;&gt;初创公司&lt;/a&gt;&quot;。希望很快就能有更多的信息带给大家。一言为定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html&quot;&gt;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Zbb25ejwoK2xQSHHcTB3</link><guid isPermaLink="false">https://www.infoq.cn/article/Zbb25ejwoK2xQSHHcTB3</guid><pubDate>Tue, 10 Feb 2026 09:40:42 GMT</pubDate><author>Andy Pavlo</author><category>数据湖仓</category></item><item><title>在参与OpenAI、Google、Amazon的50个AI项目后，他们总结出了大多数AI产品失败的原因</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助Coding Agent等工具，如今构建一个AI产品的技术门槛和启动成本已急剧降低。一夜之间，将想法变为可交互的原型变得前所未有的容易。但一个刺眼的矛盾也随之浮现：大多数AI产品仍在走向失败。如果技术实现不再是瓶颈，那么问题究竟出在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aishwarya Naresh Reganti 和Kiriti Badam 曾在 OpenAI、Google、Amazon、Databricks 等公司参与构建并成功推出了 50 多个企业级 AI 产品。最近，他们在播客节目中，与主持人Lenny细致分享了当前AI产品开发中的常见陷阱与成功路径。基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;今天构建的成本已经非常低了，真正昂贵的是设计，是你是否真正想清楚了产品要解决什么痛点。对问题本身和产品设计的执着，是被低估的，而单纯追求“快点做出来”，是被高估的。AI 不是答案，而是解决问题的工具。领导者需要重新回到“亲自上手”的状态，并不是要他们亲自实现系统，而是为了重建判断力，接受“我的直觉可能不再完全正确”这一事实。“忙碌但无效”的工作时代正在结束，你不可能再躲在角落里做对公司没有实质影响的事，而必须思考端到端的流程，以及如何创造更大的影响。在这个数据随时告诉你“你大概率会失败”的时代，保留一点愚蠢的勇气很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI产品构建中的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：目前AI 产品构建的情况是怎样的？哪些进展顺利，哪些地方问题依旧明显？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：首先，怀疑态度明显减少。2024年还有很多领导者认为 AI 可能只是又一波“加密货币式”的泡沫，因此迟迟不愿真正投入。当时我看到的很多所谓“AI 用例”，更像仅仅是“在你自己的数据上套一层 Snapchat 滤镜”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而2025年，很多公司开始真正反思用户体验和业务流程，逐渐意识到：如果想构建成功的 AI 产品，必须先拆解现有流程，再重新构建。而消极的一面在于，执行依然非常混乱。这个领域只有三年左右的历史，没有成熟的方法论，也没有教材，大家基本都是边走边学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，AI 产品的生命周期与传统软件截然不同。这导致了以往在PM、工程师、数据团队之间形成的分工被打破。过去，PM、工程师各自优化各自的指标；现在，大家可能需要坐在同一间会议室里，一起看 agent 的执行轨迹，共同决定产品应该如何表现。这种协作更紧密，也更复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你之前说构建 AI 产品与构建非 AI 产品本质上非常不同，能具体谈谈吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：构建 AI 系统和传统软件系统之间确实存在大量相似之处，但也有一些根本性的差异，足以改变你构建产品的方式。其中一个经常被忽视的核心差异，是“非确定性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与传统软件相比，你几乎是在与一个非确定性的 API 打交道。在传统软件中，决策引擎和流程往往是清晰、可预测的。以 Booking.com 为例：你有一个明确意图，比如在旧金山订两晚酒店，系统通过一系列按钮、选项和表单，把你的意图转化为具体操作，最终完成目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在 AI 产品中，这一层被一种高度流动的、以自然语言为主的界面所取代。用户可以用无数种方式表达同一个意图，这意味着你无法预判用户的输入行为。而在输出端，你面对的是一个概率性的、非确定性的 LLM，它对提示词极其敏感，本质上还是一个黑箱。你既无法完全预测用户会如何使用产品，也无法确定模型会给出怎样的回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，你同时面对输入、输出和中间过程三方面的不确定性，只能在有限理解的基础上去预判行为并进行设计。到了 Agent 系统，这种复杂性会进一步放大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也引出了第二个关键差异：代理性与控制权之间的权衡。很多人执着于构建高度自治的系统，希望 Agent 能替人完成所有工作。但每当你把决策权交给 AI，你就必然放弃一部分控制权。因此，只有当系统足够可靠、足以赢得信任时，才值得赋予它更高的自治能力。这正是“代理性—控制权权衡”的核心：自治越高，控制越少，而信任必须通过时间和表现来积累。&lt;/p&gt;&lt;p&gt;Kiriti：类比登山：如果你的目标是攀登一座高峰，你不会第一天就直接冲顶，而是先进行基础训练，逐步提升能力，最终才接近目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;构建 AI 产品也是如此。你不应该在第一天就打造一个拥有公司全部工具和上下文的全能 Agent，并期待它能正常工作。正确的做法，是刻意从影响范围小、人工控制强的场景开始，逐步理解当前能力边界，再慢慢增加自治性、减少人工干预。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样做的好处在于，你会逐渐建立信心，清楚 AI 能解决问题的哪一部分，以及接下来需要引入哪些上下文和工具来改进体验。好的一面是，你不必一开始就面对复杂而炫目的 Agent 体系；挑战在于，你必须接受“循序渐进”的现实。但几乎所有成功的案例，都是从极简结构起步，再不断演化而来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你们一直强调“从低自治、高控制开始”，再逐步升级。能否用一个具体例子说明这种路径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：客户支持是一个非常典型的场景。我们在发布产品时也经历过类似情况，随着新功能上线，支持请求会突然激增，而且问题类型非常多样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始，并不是把所有支持中心文章一股脑塞进 Agent 就完事了。更合理的第一步，是让 AI 为人工客服提供建议，由人类判断哪些建议是有用的、哪些是无效的。通过这个反馈回路，你可以识别系统的盲点并进行修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你建立起足够信心后，才可以让 AI 直接向用户展示答案。接着，再逐步增加复杂能力，例如自动退款、创建功能请求等。如果在第一天就把这些能力全部交给 Agent，系统复杂度会迅速失控。因此，我们始终建议按阶段构建，逐步提升自治水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：一开始是高控制、低自治，AI 只给建议，最终决策仍由人来做；当系统被验证可靠后，逐渐赋予更多自治权，同时减少人工干预。只要这一阶段进展顺利，就可以继续向前推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：从更宏观的角度看，AI 系统的核心在于“行为校准”。你几乎不可能在一开始就准确预测系统行为，因此关键在于避免破坏用户体验和信任。做法是，在不影响体验的前提下，逐步减少人工控制，并以不同方式约束自治边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以医疗保险预授权为例，某些低风险项目，比如血液检测或 MRI，只要患者信息齐全，就可以由 AI 自动审批；而高风险项目，如侵入性手术，则必须保留人工审核。在这个过程中，你还需要持续记录人类的决策行为，构建反馈飞轮，用于不断优化系统。这样既不会损害用户体验，也不会削弱信任，同时还能让系统持续进化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你还给出过一些很好的分阶段示例，比如Coding Agent：第一阶段只做行内补全和样板代码建议；第二阶段生成测试或重构代码供人审查；第三阶段则可以自动提交 PR。营销助手也是类似路径：从文案草稿，到完整活动执行，再到自动 A/B 测试和跨渠道优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：换个角度看，这种非确定性其实也是 AI 最迷人的地方。相比点击复杂的按钮，人类更习惯用语言交流，这大大降低了使用门槛。但问题在于，人类表达意图的方式极其多样，而你往往需要在非确定性的技术之上，达成确定性的业务结果，这正是复杂性的来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：所以，当人们一上来就想直接跳到第三阶段，往往会陷入困境：系统既难以构建，也不可靠，最终只能被判定为失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：在达到高度自治之前，你需要对系统能力建立足够信心。如果一开始就从错误的切入点出发，你会面对成百上千种错误，却根本无从修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从小规模、低自治开始，不仅降低风险，也会迫使你认真思考“我要解决的到底是什么问题”。在 AI 快速发展的环境下，人们很容易沉迷于复杂解法，而忽视真正的问题本身。通过逐步提高自治层级，你可以清晰地拆解问题，并为未来扩展做好准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我最近读到一篇研究指出，约75% 的企业认为“可靠性”是他们在 AI 项目中面临的最大问题，这也是他们迟迟不敢将 AI 产品直接面向用户的重要原因。正因如此，目前很多 AI 产品更多集中在提升生产力，而不是彻底替代端到端流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：在这期节目之前，我们还录了一期，专门深入讨论了提示注入（prompt injection）和越狱（jailbreaking）。在那期讨论里，我们意识到这对 AI 产品来说几乎是一个“生存级风险”：它可能既没有成熟解法，甚至在理论上也很难被彻底解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：一旦 AI 系统真正进入主流应用，这会成为一个非常严重的问题。现在大家还忙着把 AI 产品做出来，很少有人认真对待安全性，但这迟早会爆发。尤其是在面对非确定性 API 的情况下，你几乎无法完全防范。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：我们当时聊到的一个核心问题是：要诱导 AI 去做“不该做的事”，其实并不难。虽然大家都在构建各种护栏系统，但事实证明，这些护栏并不牢靠，总能被绕过。而正如你所说，当 Agent 越来越自治、甚至进入机器人系统时，这种风险会被成倍放大，确实让人感到不安。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我同意这是一个真实存在的问题。不过从当前 AI 在企业中的采用阶段来看，大多数公司甚至还没真正走到能充分获益的程度。2025 年确实是 AI Agent 和企业尝试落地 AI 的一个高峰期，但整体渗透率依然不高，很多流程还远未被真正改造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种情况下，只要在关键节点引入“人在回路”（human-in-the-loop），其实可以规避相当一部分风险。我个人更偏向乐观的一侧：与其一开始就被潜在的负面场景吓退，不如先尝试去落地、去使用。我们在 OpenAI 接触过的企业中，几乎没有人会说“AI 在这里完全帮不上忙”，更多是发现它能在某些具体环节上带来优化，然后再思考如何逐步采用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有哪些成功构建 AI 产品的模式和工作方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我们合作过的成功公司，通常都具备三个维度：优秀的领导者、健康的文化，以及持续推进的技术能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是领导者。我们参与过不少企业的 AI 转型、培训和战略制定。很多领导者过去十到十五年积累的直觉，正是他们成功的基础，但在 AI 出现之后，这些直觉往往需要被重新学习。领导者必须愿意承认这一点，甚至需要一定程度的“脆弱感”。我曾和 Rackspace 现任 CEO Gajen 共事。他每天清晨都会预留一个固定时段，专门用来“补课 AI”——听播客、看最新资料，甚至在周末做白板推演。领导者需要重新回到“亲自上手”的状态，并不是要他们亲自实现系统，而是为了重建判断力，接受“我的直觉可能不再完全正确”这一事实。很多真正成功的团队，正是从这种自上而下的转变开始的。AI 几乎不可能靠纯粹的自下而上推动，如果领导层对技术缺乏信任，或者对能力边界有误判，整个组织都会受限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个维度是文化。在传统企业中，AI 往往不是核心业务，但因为竞争对手在用、因为确实存在可行用例，企业不得不引入 AI。在这个过程中，恐慌文化非常常见，比如“FOMO”“你会被 AI 取代”等说法。问题在于，真正做出好 AI 产品，极度依赖领域专家；但很多专家却拒绝参与，因为他们担心自己的岗位被替代。这时，领导者需要建立一种“赋能型文化”，强调 AI 是用来增强个人能力、放大产出的工具，而不是威胁。只有这样，组织才会形成合力，而不是人人自危。事实上，AI 往往会创造更多机会，让员工做更多、更高价值的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三个维度才是技术本身。成功的团队通常对自身工作流有近乎执念般的理解，清楚哪些环节适合 AI，哪些地方必须有人参与。几乎不存在“一个 AI Agent 解决一切”的情况。通常是机器学习模型负责一部分，确定性代码负责另一部分。因此，关键不在于迷信技术，而在于为每个问题选择合适的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，这些团队也非常清楚自己在和一个非确定性的 API 打交道，因此会以完全不同的节奏推进开发。他们迭代得非常快，但前提是不破坏用户体验，同时快速建立反馈飞轮。如今的竞争焦点，并不是谁最早上线 Agent，而是谁最早构建起持续改进的机制。凡是有人告诉我，“一个Agent，两三天就能在你系统里跑出显著收益”，我都会非常怀疑。这不是模型能力的问题，而是企业数据和基础设施本身就极其混乱。大量技术债、混乱的接口和命名方式，都需要时间去消化。真正能产生显著 ROI，通常至少需要四到六个月，即便你拥有最好的数据和基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有些人认为评测（eval）是解决 AI 问题的关键，有些人则觉得它被严重高估，只要“感觉对了”就行。你们怎么看 eval？它在多大程度上真的能解决你们提到的那些问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我觉得大家陷入了一种错误的二元对立：要么eval能解决一切，要么线上监控能解决一切。eval本质上，是把你对产品的理解、你的价值判断，编码进一组数据集：什么是重要的，什么是绝对不能发生的。而生产环境监控，则是在产品上线后，通过关键指标和用户行为，反馈真实使用情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种监控并不新鲜，但在 AI Agent 场景下，颗粒度变得更细了。除了显式反馈，比如点赞、点踩，还有大量隐式信号。例如用户不点踩，但反复要求重新生成回答，这本身就是强烈的负面反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的问题不在于“选哪个”，而在于你想解决什么。如果你的目标是构建一个可靠系统，那么上线前必须有底线测试，这可以是一小组关键问题，确保无论如何都不能出错。上线之后，你不可能人工检查所有交互轨迹，这时就需要监控来提示你哪里出了问题。当你发现新的失败模式，再反过来构建新的eval集。这个循环缺一不可。认为“只靠其中一种就够了”，在我看来是站不住脚的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我想稍微退一步，谈谈为什么“eval”这个词在 2025 年下半年被赋予了如此沉重的含义。你去找数据标注公司，他们说专家在写 eval；有人说 PM 应该写 eval，它们就是新的 PRD；还有人说 eval 本身就是产品改进所需的完整反馈回路。对初学者来说，这非常混乱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，大家说的都不完全错，但指向的是不同层面的事情。律师和医生写的“评估”，并不等于他们在构建 LLM judge；PM 写 eval，也不意味着要写一个可直接上线的评判模型。很多时候，你事前根本无法判断是否需要 LLM judge，还是只依赖生产环境的用户信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martin Fowler 曾提出过“语义扩散”这个概念：一个词被发明出来，随后被不断滥用，最终失去精确定义。我认为 eval 正处在这个阶段。不同人看到的是它的不同侧面。但如果你让一群实践者坐在一起问：“AI 产品是否需要一个可执行的反馈回路？”他们一定都会点头。至于怎么做，完全取决于具体场景。复杂用例下，盲目构建评判模型往往得不偿失，这时回到用户信号、快速修复、确认是否回退，反而更有效。最终，所有资深从业者都会告诉你一句话：一切取决于上下文，不要迷信固定方法论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：现在“eval”已经变成一个可以指代无数不同东西的词，既包括标注、基准测试，也包括反馈机制，讨论起来反而更混乱了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我最近就遇到一个客户，说他们“在做 eval”。我问能不能看看数据集，他们说只是看了 LLM Arena 和一些第三方榜单，就选了模型。我只能说，那不是 eval，那只是模型对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：Claude Code 的负责人 Boris 曾公开表示：“我们在Claude Code 里不做 eval，一切靠感觉（vibes）。”能不能请你分享一下，Codex 以及 Codex 团队在eval这件事上的具体做法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：在 Codex，我们采取的是一种相对平衡的方式：eval是必要的，但同时必须高度重视用户反馈。我们在产品上极度强调“把正确的产品做出来”，而其中非常重要的一部分，就是认真倾听用户的声音。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Coding Agent 和其他领域的 Agent 有一个本质差异：它们是为“可定制性”和“工程师”而生的。Coding Agent 并不是只解决五六个固定工作流的产品，而是需要以多种方式被定制和扩展。这意味着，产品会被嵌入到各种不同的集成环境、工具链和使用场景中。在这种前提下，几乎不可能为用户的所有交互方式提前构建一个完备的eval数据集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，你仍然需要确保，每一次改动至少不会破坏产品中那些最核心的能力。因此，我们确实会用eval来守住这些“底线”。同时，我们也投入大量精力去理解用户真实的使用方式。举个例子，我们最近推出了一个代码审查产品，增长非常快，既帮 OpenAI 内部发现了大量问题，也被外部客户广泛使用。如果我对代码审查相关的模型、或训练时采用的强化学习机制做了调整，在上线之前，我一定会通过 A/B 测试来验证：它是否还能准确找出关键问题，用户对结果的反应如何。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时，用户一旦被错误的代码提示反复打扰，甚至会直接关闭这个功能。你需要确保，新版本确实在“做对的事情”。但老实说，很多这类场景在事前是很难预判的，也很难提前为它们构建对应的eval数据集。因此，这里面既有一定的“vibes判断”，也有大量来自真实用户的反馈。我们会非常主动地关注社交媒体，看看是否有人遇到特定问题，并尽快修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我并不认为有一套万无一失的eval指标，可以完全依赖它，其他什么都不用管。每当我们要发布一个新模型，团队都会聚在一起做集中测试，每个人关注不同的重点。我们手里有一份“高难度问题清单”，会把这些问题交给新模型，观察它的表现。这更像是每位工程师都有一套针对自身关注点的定制eval，用来帮助大家理解：在这个新模型下，产品到底发生了什么变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;CC/CD框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aishwarya：我们接触过大量公司，它们都承受着来自竞争对手的压力，因为“所有人都在做 Agent”，于是觉得自己也必须构建一个完全自治的 Agent。但很快发现一个问题：在一开始，你根本无法预知用户会如何与系统交互，也无法预判 AI 会给出哪些响应或采取哪些行动。当你的工作流包含四五个步骤、需要连续做出大量决策时，问题一旦出现，就会变得极其难以修复，结果往往是无休止的调试和热修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们曾为一个客服场景构建系统，后来，因为热修复多到失控，新的问题层出不穷，这个产品不得不被下线。与此同时，行业里也发生了不少令人警惕的事件，比如前段时间 Air Canada 的一个 Agent“臆造”了一条并不存在的退款政策，而公司因为法律原因不得不接受这个结果。这类案例让人意识到：如果设计不当，AI 系统可能会对企业本身造成非常严重的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样的背景下，我们开始思考：如何在不失去用户信任的前提下构建系统，同时又能形成一个持续改进的飞轮？这就是“CC/CD（Continuous Calibration, Continuous Development 持续校准、持续开发）”框架的出发点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/2284048821d07ccc10b93c93272d0e9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;循环的一侧是“持续开发”。你先界定能力边界，整理数据，明确系统的预期输入和预期输出。在真正动手之前，这一步本身就非常有价值，因为它常常会暴露出团队内部对“产品该如何表现”的理解并不一致。此时，产品经理和领域专家的参与尤为关键。你并不需要一个覆盖所有情况的数据集，而是一个“足够好”的起点。接下来，搭建应用，并设计评估维度。我刻意使用“评估指标”这个说法，而不是简单地说 eval，是因为评估是一种过程，而指标只是你在过程中重点关注的维度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一侧是“持续校准”。当系统上线后，你一定会看到大量最初未曾预料到的用户行为模式。评估指标可以帮助你发现一部分问题，但很快你会意识到，它们同样不足以覆盖所有新出现的错误模式。这时，你需要分析真实行为，识别新的错误类型，一部分问题可以直接修复，而另一部分则需要催生新的评估指标。这并不意味着每一个错误都要转化为新的eval维度。有些只是偶发问题，比如工具定义不清导致的调用错误，修完即可继续前进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体来看，这就是一个 AI 产品的典型生命周期。我们还特别强调，在迭代初期，应当采用“低自治、高控制”的方式：限制系统可做的决策数量，引入人在回路；随着理解加深，再逐步提高自治程度。这样做的本质，是在逐步建立对系统行为的认知飞轮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/05/0552db707dbf2e73328871c90f690d19.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以客服 Agent 为例，我们通常会把演进过程拆成三个阶段。第一阶段只是“路由”，即判断工单该被分配到哪个部门。很多人会低估这个问题的复杂度，但在大型企业里，路由往往异常困难。层级混乱、分类标准失序的情况非常普遍，人类客服往往依赖大量隐性经验才能做出判断，而这些规则通常并未被文档化。如果直接把问题丢给 Agent，而不给足上下文，风险就会非常高。在路由阶段，即便 Agent 分错了部门，人类也可以介入纠正，控制风险。同时，这个阶段往往会暴露出大量数据问题，需要优先修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;等路由稳定之后，下一步是“副驾驶”：Agent 根据既有的标准操作流程生成回复草稿，由人工修改和确认。在这个过程中，你会自动记录人类的修改行为，从而几乎“免费”获得误差分析数据，并将其反馈到系统中。当你发现，大多数情况下人工已经不需要做太多修改时，才可以进入端到端的自动处理阶段，让 Agent 既生成回复，也完成问题的解决。这正是从低自治逐步走向高自治的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/0730a295de7b387816e5139414b6967c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还整理了一张表，明确每个阶段你在做什么、能学到什么，以及这些信息如何被反馈回系统。需要强调的是，采用 CC/CD 并不意味着问题会一次性被解决。即便已经走到较高版本，你仍然可能遇到此前从未见过的数据分布。这个框架的意义，在于帮助你在完全自治之前，尽可能多地理解用户行为，从而降低整体风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它还隐含地帮你建立了一套行为日志体系。单纯依赖评估指标，只能捕捉你“已经知道”的错误，而大量新模式，只有在真实使用中才会显现出来。通过这种低风险、渐进式的方式，你可以理解用户，而不至于在问题全面爆发时手忙脚乱。最终，这一切的核心目标只有一个：在持续校准系统行为的同时，不断维护并增强用户对产品的信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这套方法的核心，在于把一切都设计成持续的、可迭代的过程，沿着“自治程度不断提高、控制逐步降低”的路径前进。“持续校准、持续开发”这个命名，本身就强调了它的迭代性。顺便说明一下，这个名字显然是在向 CI/CD（持续集成、持续部署）致敬，只不过这是 AI 时代的对应版本：不再只是不断跑单元测试、频繁部署，而是持续运行eval、观察结果、调整关注的指标，找出系统失效的地方，再不断迭代优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个框架本身上，还有没有什么你觉得特别重要、但我们还没提到的点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我们最常被问到的问题之一是：我该如何判断，系统是否已经“校准得足够好”，可以进入下一个阶段？这件事并没有一套明确的规则手册，核心原则只有一个：尽量减少“意外”。比如说，如果你每一两天就做一次校准，而发现没有出现新的数据分布模式，用户的行为也相当稳定，那你从系统中获得的新信息就已经非常有限了。这往往就是一个信号，说明你可以考虑进入下一阶段了。到了这个时候，很大程度上其实是在凭经验判断：你是否感觉自己已经“准备好了”，是否还在持续获得新的洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，也要意识到，有些外部事件会彻底打乱原有的校准状态。比如 GPT-4.0 被弃用，API 层面逐步迁移到 GPT-5，而新模型的行为特性完全不同，这时你的校准就会再次失效，需要重新走一遍流程。用户行为本身也会随时间演化。即便是消费级产品，我们今天和 ChatGPT 的交互方式，也和两年前完全不同，一方面是模型能力提升了，另一方面是用户在某个任务上尝到甜头后，会自然地把系统用于更多新场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们曾为银行的核保人员构建过一个系统。核保本身是一项非常繁琐的工作，贷款申请文件往往有三四十页。这个系统的初衷，是帮助核保人员快速查找政策和内部信息，从而更高效地审批贷款。最初三四个月，反馈都非常积极，核保人员的效率显著提升。但随后我们发现，正是因为他们对系统产生了信任，开始提出一些我们从未预料到的深度问题，比如直接把整份申请材料丢给系统，问：“像这种情况，之前的核保人员通常是怎么处理的？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从用户角度看，这只是一个非常自然的延伸；但从产品构建角度看，底层逻辑却发生了质变。系统需要理解“类似情况”究竟指什么，再去检索历史案例、分析文档，最后给出综合判断。这已经远远超出了最初“查找某条政策”的设计范围。正是这种不断演化的用户行为，提醒你：是时候回到校准阶段，重新审视系统能力边界了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 的未来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：当下 AI 领域里，哪些东西被高估了？哪些被低估了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：与其说“被高估”，不如说有些概念被严重误解。一个典型例子是多 Agent 系统。很多人会觉得：我有一个复杂问题，只要拆成几个子任务，分别交给不同的 Agent，再把它们连起来，就能实现所谓的“Agent 乌托邦”。现实并非如此。当然，成功的多 Agent 系统确实存在，但关键在于，你如何限制系统偏离轨道的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，用一个监督型 Agent 来协调多个子 Agent，是一种非常成熟、有效的模式；但如果只是按功能拆分职责，期望这些 Agent 通过某种“点对点协作”自然形成整体能力，那在当前的模型能力和工程范式下，往往行不通。这并不是多 Agent 被高估，而是人们高估了它在现阶段能“自发协同”的程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得Coding Agent 仍然被低估了。你在 Twitter 或 Reddit 上会看到大量讨论，但你会发现它的真实渗透率依然很低，而潜在价值却极大。我认为2026 年会是集中优化这些流程、释放巨大生产力的一段时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：相比预先设计一堆各司其职的 Agent，更现实的路径，可能是让一个更强的 Agent 自己完成任务拆解和协调？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：没错。你可以由人来编排多个 Agent，也可以由一个更大的 Agent 负责统筹。但如果让多个 Agent 以点对点的方式自由通信，尤其是在客服这类对输出高度敏感的场景中，几乎不可能精细地控制“到底是哪个 Agent 在对用户说话”，护栏成本会急剧上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我认为eval是被误解的概念。它当然重要，但“不断切换工具、学习新工具”这件事被高估了。我依然是比较传统的看法：真正值得投入精力的，是对你要解决的业务问题保持极度专注，AI 只是工具而已。你当然需要了解最新进展，但不要把“快速构建”本身当成目标。今天构建的成本已经非常低了，真正昂贵的是设计，是你是否真正想清楚了产品要解决什么痛点。对问题本身和产品设计的执着，是被低估的，而单纯追求“快点做出来”，是被高估的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：从产品视角看，你们觉得未来一年 AI 会走向哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我非常看好“后台型”或“主动型” Agent。当前 AI 难以持续创造价值，很大程度上是因为它缺乏上下文，而原因在于它还没有真正接入工作发生的地方。一旦 Agent 被更深地嵌入真实工作流，获得更丰富的上下文，它就能理解你在优化什么指标、试图完成哪些活动。接下来顺理成章的一步，就是由 Agent 主动反过来提示你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们已经在 ChatGPT Pulse 这样的功能中看到雏形，它每天推送一些你可能关心的更新，帮助你“唤醒思路”。把这一模式扩展到更复杂的任务中，比如Coding Agent 在你一天开始时告诉你：“我已经帮你修复了五个工单，这是补丁，看看就行。”我认为这会在 2026 年成为非常重要的产品方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我非常期待 2026 年的多模态体验。2025 年我们已经取得了不小进展，不只是生成能力，在理解层面也是如此。但到目前为止，LLM 仍然是最常用的模型形态，而人类本身是高度多模态的。语言其实是我们进化中相对靠后的表达方式。即便我们在对话中，也在不断接收视觉、表情、语气等信号，并据此调整表达。如果能构建真正丰富的多模态交互，将会更接近人类对话的真实复杂度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还有大量“枯燥但重要”的任务等待被自动化。如今依然有无数手写文档、杂乱的 PDF，即便是最先进的模型也难以处理。一旦多模态理解能力真正成熟，我们就能解锁大量此前无法触及的数据资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果有人想提升自己构建 AI 产品的能力，你认为最值得重点培养的一两项技能是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：从小处着手、快速迭代、建立正向飞轮等。但如果站在更高的视角来看，对于当下的产品构建者而言，实施成本在未来几年会变得极低，真正稀缺的将是设计能力、判断力和审美品位。无论是做产品还是规划职业路径，早期几年往往专注于执行层面的技术细节，而随着 AI 大幅降低上手门槛，几年之后，每个人的价值都会更多体现在品味、判断，以及那些“只属于你”的东西上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种能力并不一定来自年龄或多年经验。我们最近招了一位同事，团队一直在用一款价格不菲的任务管理工具，他却直接带着自己手写的应用来开会，当场把我们全部拉进去开始用。那种主动性和主人翁意识，敢于重新思考既有体验，正是最能拉开差距的地方。当然，这类自建工具在规模化后可能有维护成本，需要替换或升级，但在小团队阶段，这种“先做出来再说”的态度让我非常震惊。很多在 AI 时代成长起来的人，对“构建”的心理成本极低，也更愿意尝试新工具。这或许也是为什么很多 AI 产品存在留存问题，大家都太容易被新工具吸引了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;归根结底，真正重要的是主动性和责任感。“忙碌但无效”的工作时代正在结束，你不可能再躲在角落里做对公司没有实质影响的事，而必须思考端到端的流程，以及如何创造更大的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这让我想到我之前请过 Jason Lemkin 上节目。他把整个销售团队几乎都替换成了 Agent：原来 10 个销售，现在是2个人加 20 个 Agent。结果有位销售直接辞职了，因为他发现自己“什么都没干”，很快就会被系统识别出来。这也印证了你的观点——混日子会越来越难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：坚持和承受“痛苦”的能力同样被严重低估。如今信息触手可及，几乎任何人都可以在极短时间内学习新东西，但真正的差别在于，是否愿意经历反复试错的过程——学习、实现、失败、再调整，真正理解什么有效、什么无效。我常说“痛苦是新的护城河”，这种在实践中积累的经验，无论对个人还是公司，都会沉淀为难以复制的优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多成功的公司，并不是因为抢先进入市场，或拥有多么炫目的功能，而是因为他们经历了足够多的痛苦，搞清楚哪些是不可妥协的核心点，并在模型能力、功能取舍之间不断权衡。这没有标准答案，也没有教科书，只能靠一轮又一轮的迭代。正是这些过程中的“痛苦”，最终塑造了个人能力和公司的长期竞争力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：专注于问题本身。AI 只是工具，关键在于你是否真正理解自己的工作流。很多所谓的 AI 工程师和 AIPM，把大部分时间花在理解业务流程、用户行为和数据上，而不是追逐最炫的模型。真正的差异化，永远来自对用户和问题的深度理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;闪电问答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：你们最常推荐的书是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：《当呼吸化为空气》。作者 Paul Kalanithi 是一位神经外科医生，在三十出头被诊断出肺癌。这本书让我意识到，我们是否花太多时间“评估人生”，却忘了真正去生活。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我更偏爱科幻，《三体》三部曲。它不仅讨论外星文明，也深入探讨科学、地缘政治与人类决策，对理解技术与文明的关系非常有启发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果喜欢科幻和 AI，我还强烈推荐《深渊上的火》（A Fire Upon the Deep）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近最喜欢的影视作品？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我在重刷《硅谷》，它出奇地不过时，如今的 AI 浪潮和当年的情景高度相似。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我选一个游戏，《Expedition 33》。制作精良，故事、音乐和玩法都非常出色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近发现并非常喜欢的一款产品？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：Whisper Flow。我没想到自己会这么依赖它，它能把语音自然地转化为指令，体验非常顺滑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我偏好效率工具，比如 Raycast 和 caffeinate，让我在本地跑长时间任务时效率更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你的人生信条？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：“人们说这件事做不到，但那个傻子不知道，于是他做成了。”在这个数据随时告诉你“你大概率会失败”的时代，保留一点愚蠢的勇气很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：乔布斯那句话：你只能回头看时，才能把点连成线。所以不断前进、持续尝试就好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你最欣赏对方的一点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：Kiriti 非常冷静、踏实，是我最重要的“回声板”，而且他是我见过最好的丈夫。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：Aishwarya 最大的特点是，她能把复杂问题讲得极其清楚，并且始终保持耐心和坚持，这在快速变化的 AI 时代非常珍贵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：https://www.youtube.com/watch?v=z7T1pCxgvlA&lt;/p&gt;</description><link>https://www.infoq.cn/article/Ox3JQRBv6RX0pAggifej</link><guid isPermaLink="false">https://www.infoq.cn/article/Ox3JQRBv6RX0pAggifej</guid><pubDate>Tue, 10 Feb 2026 09:29:23 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>不写、不看、不审查：这家安全公司决定不再让人类碰代码，还把这套模式开源了</title><description>&lt;p&gt;没人写代码，也没人看代码，软件照样交付？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 年 2 月，一家专注于基础设施安全的公司StrongDM 公开了一套“软件黑灯工厂”式的生产线成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个生产线里，人类不再直接写代码、也不承担代码审查；开发从交互式协作变成“把 spec 和场景喂给系统”。随后由 Agent 自动生成代码、运行测试/评测 harness，并在反馈回路里反复迭代，直到结果收敛、可以交付为止。团队把这套玩法写进章程，最重要的只有一句话——No hand-coded software。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fee3962709fe3b5932f0209d97707ae2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM AI 还不寻常的开源了它们：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一个仓库是：&lt;a href=&quot;https://github.com/strongdm/attractor&quot;&gt;https://github.com/strongdm/attractor&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是他们“软件工厂”体系中最核心的非交互式编码 Agent。不过，这个仓库本身一行代码都没有：里面只有三份 Markdown 文件，极其细致地描述了软件的完整规格说明（spec），以及 README 里的一句提示——把这些规格说明交给你选择的编码 Agent 去执行即可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f8ccdf71ec3c0db322408c05a163708.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个仓库 &lt;a href=&quot;https://github.com/strongdm/cxdb&quot;&gt;https://github.com/strongdm/cxdb&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个则更接近传统意义上的软件发布：包含 1.6 万行 Rust、9500 行 Go，以及 6700 行 TypeScript。这是他们的 “AI Context Store”——一个用于存储对话历史和工具输出的系统，数据以不可变 DAG的形式组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d2d913a49631d2de71136c119dfd514c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 的讨论中，很快就有开发者按图索骥，实际跑了一遍这套流程。他表示，自己仔细阅读了 Attractor 仓库中的文档，并严格按照 StrongDM 提供的规范，让 Claude 基于 spec 构建了一个完整应用。最终生成的是一个可以直接使用 Claude API Key 的 AI 代理，其整体质量“明显好于让模型自由发挥时生成的结果”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让他印象最深的，是这套规格说明的体量和细节程度：整套 spec 大约 6000–7000 行，覆盖了行为约束、接口语义以及系统边界。“我以前给代理布置项目时，规格说明最多也就一页纸，这次的细节密度让我非常震惊。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，这次开源并不是一个“打磨完毕”的展示版本。代码一经放出，Hacker News 上就有开发者迅速上手检查，指出其中存在疑似 bug、Rust 反模式，以及相对宽松的错误处理方式。对此，StrongDM AI 团队成员 Jay Taylor 在评论区回应称，这批项目“是最近几天才决定开源的”，尚未经过充分的技术优化，目前已经安排代理继续对 CXDB 进行清理和改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套实践也很快得到了学界的点名。沃顿商学院研究 AI 与组织变革的教授 Ethan Mollick 在转发 StrongDM 的公开内容时直言，这是一次“真正激进的软件开发方式”：“几乎没有任何人类介入。即便这种方式未必适用于大多数场景，我们也需要更多这样的跳级式设想，去重新设计流程，而不是只把 AI 塞进旧流程里。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他看来，真正有价值的进步，不是在原有流程上“多加一点 AI”，而是围绕 AI，把流程本身重做一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/091c2451aa65664cc78b09ec9d4f4b25.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一条“禁止手写代码”的内部实验线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 是一家专注于基础设施访问与身份安全的公司，核心工作是管理人类与非人类身份如何安全地连接到数据库、云资源和各类内部系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而他们的 AI 团队成立于半年前， 2025 年 7 月 14 日这天，Jay Taylor、Navan Chauhan 与 StrongDM 的联合创始人兼首席技术官 Justin McCarthy 一起，正式把一条原本分散在内部的探索工作，独立成一个专门的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新团队成立后，第一天的工作并不是写代码，而是写一份章程。Justin McCarthy 在回顾中提到，在团队成立的第一个小时，他们就先明确了一组接下来必须遵守的约束条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;代码不得由人类编写。&amp;nbsp;代码不得由人类审查。&amp;nbsp;如果你今天在每位人类工程师身上花费的 token 成本还不到 1000 美元，那么你的软件工厂还有很大的改进空间。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 StrongDM 自己的回顾里，这个决定并不是一时冲动。其背景要追溯到 2024 年末。随着 Claude 3.5 在 2024 年 10 月的第二次修订发布，团队开始观察到一个此前并不常见的变化：在长时序的 Agentic 编程任务中，结果开始叠加正确性，而不再只是不断叠加错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4c/4c45b808c5fdf4fe9c4b913d11672012.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了 2024 年 12 月，这一变化已经可以通过 Cursor 的 YOLO 模式清晰地观察到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 在博客中写道，在此之前，将 LLM 反复用于编码任务，往往会累积误解、幻觉、语法错误、依赖不兼容等问题，最终让系统“慢慢坏掉”；而结合 YOLO 模式，Anthropic 的更新模型第一次展现出他们后来在内部称之为“非交互式开发”或“成长型软件”的雏形。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这样的背景下，新成立的团队从一开始就确立了一条极端的实验前提：不允许任何手写代码。在 2025 年 7 月，这听起来依然相当激进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中最耐人寻味的，是第二条规则：代码不得由人类审查。毕竟大家都很清楚，大语言模型极其容易犯下一些“非人类式”的错误；在这样的前提下，彻底放弃人工 code review，本身就显得反直觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更何况，安全软件向来是最不愿意交给“未经人工审查的 LLM 代码”去支撑的一类系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1885ab657677fe963c449b8a18a4fb1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c5/c5a0f3c6b891d8a37b96f4f3fdc04912.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;规则落地后，问题也随之出现：如果什么都不手写，怎么确保代码真的能跑？让 Agent 自己写测试，只在一个前提下有用——它们不会“作弊”，比如直接写个 assert true。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也迅速被他们提炼成一个更根本的问题：当实现和测试都由编码 Agent 生成时，你要如何证明自己交付的软件是可工作的？StrongDM 的答案，受到了场景测试（Scenario Testing，Cem Kaner，2003） 的启发。他们是这样描述的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们重新定义了“场景（scenario）”这个词，用它来表示一个端到端的“用户故事”。这些场景通常存放在代码库之外（类似模型训练中的“留出集”），既能被 LLM 直观理解，又可以灵活地进行验证。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于他们构建的软件本身往往就包含 Agentic 组件，StrongDM 也随之放弃了“测试全绿”这种布尔式成功定义，转而采用一种更接近真实体验的度量方式。他们引入了“满意度（satisfaction）”这个概念，用来量化验证结果：在所有场景中观察到的执行轨迹里，有多大比例可能令用户满意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们把这些场景当作“隔离集”，不存放在编码 Agent 能直接访问的地方，用来评估系统整体行为。这个设计本身就很有意思，它在某种程度上，模拟的是传统软件工程中一种极其昂贵、但也极其有效的做法——由外部 QA 团队执行的强力端到端测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4e/4e2d1f6e9f6239dc134582410a94458e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;合成场景策划与塑造界面&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从软件工厂的整体原则来看，StrongDM 把这一切总结为一条清晰的流程：“种子 → 验证 → 反馈回路”。系统先接收一个最小起点——几句话、截图，或一个已有代码库；然后在尽量贴近真实世界的验证环境中跑场景，把输出持续反馈回输入，让系统在闭环中自我纠错、不断叠加正确性；循环会一直运行，直到所有被隔离出来的场景不仅通过，而且能持续通过。token 被他们形容为这条生产线的燃料。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/aede7b62703e13c62059031154c1764a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;将“验收”交给spec？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 StrongDM 的软件工厂里，spec 并不是用来给人看的设计说明书，而是整个系统能够启动、纠偏和收敛的核心输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统开发流程中，spec 更多是一种“对齐工具”：它帮助工程师理解要做什么，但真正的实现细节、权衡和妥协，往往发生在代码和 code review 过程中。而在 StrongDM 的设定下，当“人不写代码、人不看代码”成为前提，spec 的角色被彻底前移——它不再是参考材料，而是事实上的控制面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cf7d01c432b4545432ba654b2fcc4359.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队要求系统能够“从层层递进的自然语言规范中生长”，并且必须能够在“不对源代码做语义层面检查的情况下完成验证”。在这种设定下，“验收”本身也被重写了。spec 与场景（scenario）一起，构成一个不断运行的评测基准：模型生成的行为是否符合规范，不是靠人去读代码判断，而是靠它在这些场景中跑出来的结果是否持续满足预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b961ee2d9ee54596576cd5439a8605b2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c0/c05e4f6c0283d41ce5709fdf101921f1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，StrongDM 的方法把覆盖率从“人为写了多少测试”这一维度转向了“规范/场景是否足够多与足够准确”＋“验证生态能否在闭环中捕获异常”这一维度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这一理念，StrongDM 还进一步提出了他们的另一个关键概念：数字孪生宇宙（Digital Twin Universe, DTU）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 的定义是：数字孪生宇宙是一组对第三方服务的行为级克隆体。他们构建了 Okta、Jira、Slack、Google Docs、Google Drive 和 Google Sheets 的孪生系统，复刻这些服务的 API、边界情况以及可观察到的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f51bd6e432680850d6f917f402f95b11.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 DTU，他们就能在远超生产环境限制的规模和速率下做验证：既能测试那些在真实服务上危险、甚至根本不可能尝试的失败模式，也能每小时运行成千上万个场景，而不必担心触及限流、触发滥用检测，或累积 API 成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那这些 Okta、Jira、Slack 的关键行为是怎么“克隆”出来的？答案是：用编码 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fe9ae51ebc9a21883424a7b6bd996cb3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2d/2d9f53ccf591c7f26906500837b51178.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人将这套做法概括为一条可复用流水线：把某个服务的完整公开 API 文档直接喂进 Agent harness，让它生成一个自包含的 Go 二进制程序去模拟这些 API；然后在此基础上再快速搭一个简化 UI，方便把整套仿真跑通、跑顺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，DTU 的创建者 Jay Taylor 在 Hacker News 上补充了一些背景，分享了一条关键的提示策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我最初有一个关键洞察，最终形成了一套可重复的方法，用来确保 DTU 与官方 SaaS 服务之间具有高度一致性：以最流行、公开可用的官方 SDK 客户端库作为兼容性目标，始终追求 100% 兼容。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当这些不受限流和配额约束的服务克隆体跑起来后，一整支“模拟测试 Agent”队伍也就能彻底放开手脚。场景测试不再是一锤子买卖的验收环节，而是变成了 Agent 会反复、持续执行的脚本：系统一边搭建，一边就被不停拉出来跑场景、做验证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们的 Slack 孪生系统截图也直观展示了这种测试方式：一批模拟的 Okta 用户不断出现，并分别去申请访问不同的模拟系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99a80be3da43e93b3de71da0c0bae7ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题依然是：太烧钱了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在惊艳之外，这次实验也迅速暴露出一个无法回避的现实问题：成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 的实操反馈中，有开发者提到，按照 StrongDM 提供的 spec，让 Claude 构建完整应用时，TypeScript 路线的 token 消耗极高，不得不中途给账户充值，才能在一个晚上把流程跑完。他甚至计划改用 Rust 或 Go 再试一次，只是为了看看是否能把成本压下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个反馈并非个例，也不是枝节问题。StrongDM 团队在内部曾提出过一个颇具冲击力的衡量标准：如果你今天在每位人类工程师身上花费的 token 成本还不到 1000 美元，那么你的软件工厂还有很大的改进空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这句话一旦落到现实，就更像是一个商业模式的探讨：你能否打造出一条足够盈利的产品线，从而负担得起以这种方式开发软件所带来的巨大成本？当任何竞争对手只需几个小时的编码代理工作就能克隆你的最新功能时，构建可持续的软件业务也变得截然不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，正如StrongDM 团队在回顾中所说，其实这一切技术上是可行的，只是以前从经济上来说不划算：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“构建一个高保真 SaaS 应用的克隆在技术上一直可行，但在经济上从未现实过。几代工程师都可能想过，做一个完整、内存级的 CRM 副本来测试，但最终往往会在心里把这个提案按下去——‘算了，太不划算了’。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即使对于那些不打算在 token 成本上一次性投入数千美元的团队和个人来说，StrongDM 这种做法依然有很多值得思考的地方，尤其是在人力成本和个人投入回报这一层面。对程序员个人而言，真正的问题或许不只是“现在贵不贵”，而是：当算力成本持续下降几乎成为共识时，你是否已经开始为新的角色和分工做技能投资——还是仍然把全部价值押在“写代码本身”上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8f/8fce4cffe9d50c36e0be66a3ca390d93.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是个很有意思的观点，不过我想从另一个角度补充一下：如果按每个月 20 个工作日来算，那就是 2 万 × 12 = 24 万美元一年，差不多等于一个 FANG 新毕业生的总包（TC）。我和不少初级到中初级的软件工程师（SDE）共事过，说实话，其中 80% 的表现并不比 Claude 好。（我也见过一些 staff 级别的工程师写出的代码比 AI 还差，但他们通常会用领域知识和技术负责人职责把短板补回来。）&amp;nbsp;我确实看到，AI 正在把软件工程进一步推向一种金字塔结构：顶层只有极少数人类，其余大量工作由 AI 承担。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/eb/ebb0e1bcfd911fc3198571f2b16bfbb1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然从按现在的成本算，AI“还没便宜到值得完全替代人”，但有网友认为成本下降也许能够预期：“我在想，这会不会只是软件工厂还处在非常早期、效率极低阶段的副产品。Yegge 和 Huntley 都承认，他们在做的自治工厂实验既昂贵又浪费。从制造业的历史经验来看，我反而会预期：随着方法逐渐成熟、流程被不断优化，成本会慢慢降下来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在博客中的最后，他们给出的结论，也为这条实验线画上了一个颇具警示意义的注脚：“我们这些构建软件工厂的人，必须刻意保持一种天真：主动识别并移除软件 1.0 时代留下的习惯、惯例和限制。数字孪生宇宙（DTU）就是最好的证明——六个月前还不可想象的事情，如今已经成了日常。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://factory.strongdm.ai/&quot;&gt;https://factory.strongdm.ai/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://simonwillison.net/2026/Feb/7/software-factory/&quot;&gt;https://simonwillison.net/2026/Feb/7/software-factory/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46924426#46931812&quot;&gt;https://news.ycombinator.com/item?id=46924426#46931812&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/iwKHqENNONoXQDEsvobG</link><guid isPermaLink="false">https://www.infoq.cn/article/iwKHqENNONoXQDEsvobG</guid><pubDate>Tue, 10 Feb 2026 09:22:34 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>Daggr 发布：用于构建与检查 AI 工作流的开源 Python 库</title><description>&lt;p&gt;Gradio 团队发布了 &lt;a href=&quot;https://huggingface.co/blog/daggr&quot;&gt;Daggr&lt;/a&gt;&quot;，这是一个新的开源 Python 库，意在简化多步骤 AI 工作流的构建与调试。Daggr 允许开发者以 Python 代码的方式定义工作流，同时会自动生成一个可视化画布，展示流水线中每个步骤的中间状态、输入和输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 通过将工作流组织为有向图的形式，简化了应用型 AI 的开发过程，使每一个节点都可以被单独检查和重新执行。这种方式有效缓解了应用开发中常见的一个问题：当错误发生在流程后期时，需要重新运行整个流水线，导致实验过程缓慢且结果不够清晰。通过节点级别的复现与检查，Daggr 提升了调试效率，也加快了迭代速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该库采用以代码为核心的设计思路。开发者直接在 Python 中定义节点及其连接关系，Daggr 再根据代码渲染出对应的可视化界面用于检查。这与以 GUI 为中心的工作流构建工具形成对比，后者往往牺牲版本控制能力和灵活性。使用 Daggr 时，可视化层是从代码派生出来的，而不是取代代码本身，从而保证了工作流的可复现性，也更便于审查和协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 支持三种主要的节点类型。GradioNode 可直接连接到 Gradio 应用或 Hugging Face Spaces，使已有的演示和工具能够作为工作流组件复用。FnNode 用于封装任意 Python 函数，方便插入自定义的预处理或后处理逻辑。InferenceNode 则用于对接通过 Hugging Face Inference Providers 提供的模型服务，使托管模型能够无需额外适配即可集成进工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一个关键特性是状态持久化。Daggr 会自动保存工作流状态、缓存结果、输入值以及画布布局，使开发者可以在不中断上下文的情况下暂停和恢复工作。单个节点也可以在修改输入后单独重新运行，这在调试长流水线或对比某一步的不同实现方案时尤其有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于 Daggr 由 Gradio 团队开发，它与 Gradio 生态系统实现了紧密集成。工作流既可以在本地启动，并通过浏览器访问可视化画布，也可以利用 Gradio 的隧道功能通过公共链接进行分享。对于需要长期运行的场景，同样的工作流还可以通过将 Daggr 作为依赖，部署到 Hugging Face Spaces 上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者的早期反馈主要集中在编程式控制与可视化反馈相结合这一点上。Sebastian Buzdugan 在评论该发布时&lt;a href=&quot;https://x.com/sebuzdugan/status/2017183567273406571?s=20&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;把接口和 Gradio 混在一起用，真的是一个非常聪明的组合。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有其他用户&lt;a href=&quot;https://x.com/OriOridev/status/2017000978227122383?s=20&quot;&gt;指出&lt;/a&gt;&quot;，Daggr 在快速实验和原型验证方面尤其有价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 目前是一个轻量级、实验性质的项目，仍处于 beta 阶段。随着用户的使用，其 API 可能会发生变化。尽管工作流状态是存储在本地的，但更新过程中仍可能导致数据丢失，这也进一步表明它的定位更偏向于开发和原型工具，而非直接用于生产环境的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 现已作为开源 Python 包发布，支持 Python 3.10 及以上版本，可通过 pip 或 uv 进行安装。其源代码、示例和文档已发布在 &lt;a href=&quot;https://github.com/gradio-app/daggr&quot;&gt;GitHub&lt;/a&gt;&quot; 上，团队也邀请社区在项目逐步成熟的过程中提供反馈并参与贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/daggr-open-source/&lt;/p&gt;</description><link>https://www.infoq.cn/article/1xUSbEXlzBqHKsh3pSFy</link><guid isPermaLink="false">https://www.infoq.cn/article/1xUSbEXlzBqHKsh3pSFy</guid><pubDate>Tue, 10 Feb 2026 03:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>软件工程</category></item><item><title>从分散存储到统一分析，Apache Doris 在快手万亿规模广告场景的应用实践</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;导读：面对万亿级广告数据存量、日均 3 亿行增量及数千个复杂查询模板的挑战，快手广告数据平台如何突破性能瓶颈、实现架构统一与体验跃升？本文系统介绍了快手广告团队从 ClickHouse on ES 混合架构，全面迁移至 Apache Doris 的统一分析实践，最终实现查询性能提升 20～90%，写入吞吐提升 3 倍，存储效率提升 60%。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自快手高级计算引擎研发工程师 周思闽 在 Doris Summit 2025 中的演讲内容，并以演讲者第一视角进行叙述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手是国内日活过亿的短视频平台，其广告投放平台是商业化外部广告主与快手电商商家进行广告投放的主要阵地，支持客户在平台上进行广告物料搭建、物料管理、策略变更、数据查看等操作，这对底层数据系统的存储、计算与查询性能提出了极高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要支撑如此大规模的广告投放与实时分析，底层数据架构面临巨大挑战。当前，快手的广告数据包括：由投放系统产生的物料数据以及用于数据分析的效果数据，这些数据呈现出三个显著特征：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据存量巨大：广告物料累计已达千亿级别，且随业务发展正向万亿规模迈进，存储体量位居公司前列，对架构扩展性提出极高要求。数据增长迅猛：仅 2025 年第一季度，日均新增广告物料数据同比激增 3.5 倍，要求底层引擎具备强大的实时写入与弹性扩展能力。数据模型复杂：整个数据体系涵盖约 700 个核心字段，涉及物料、投放、用户、效果等多个维度；同时，为应对多样化分析场景，沉淀的查询模板已超 4000 个，对查询引擎的兼容性与性能均是严峻考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构演进：从分散存储到统一分析&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 早期架构及挑战&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期存储架构中，物料数据由 MySQL、Elasticsearch 协同存储；效果数据主要存储与 Clickhouse 中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据分析时，将分散在 MySQL、Elasticsearch 中的物料数据与 ClickHouse 中的效果数据进行高效关联查询，从而为广告主提供完整、及时的投放效果洞察。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3c247f9ec4b67b6612011bf2aedc7beb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在如上所说的 ClickHouse on ES 架构中，用户提交的查询通常包含 Elasticsearch 外表（a）与 ClickHouse 内表（b）。ClickHouse 会解析查询中外表部分，将其转换为 Elasticsearch 查询语句，通过 HTTP 请求获取数据并封装为 Block，最后在引擎内部完成与内表的关联计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6c7f67f64a49cd0ea8b411124049604.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，随着 Elasticsearch 中数据量持续增长，该架构逐渐暴露诸多问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;查询性能恶化：慢查询率上升至 35%，平均查询耗时达到 1.4 秒；存储瓶颈：Elasticsearch 单分片难以支撑 10 亿级以上数据量，扩容与数据重分布成本高；运维复杂度高：数据链路依赖组件多，运维与监控成本显著上升；问题定位困难：缺少 ClickHouse 与 Elasticsearch 之间的全链路可观测手段，出现查询延迟、数据不一致等问题时，需跨系统排查，耗时较长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 选型目标及调研&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上述问题及挑战，我们为新架构设定了明确目标：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;慢查询率低于 5%；运维排查耗时降低至分钟级；支持单表万亿级别数据存储；保障数据实时性，延迟低于 5 分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于以上目标，我们对 Apache Doris、ClickHouse、Elasticsearch 等主流 OLAP 引擎进行了全面的调研与性能压测。测试涵盖了写入吞吐、查询延迟、存储压缩率、全文检索性能等关键维度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da9240cbfe6a23bb6131a78ace804f9a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这过程中，ClickHouse 首先被排除，因其不支持唯一键模型，而广告物料数据存在大量更新场景，要求引擎具备主键更新能力。因此，重点在 Elasticsearch 与 Apache Doris 之间进行对比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合测试结果，Apache Doris 在写入性能、查询效率、存储成本及运维复杂度等方面均表现优异，不仅能够满足既定架构目标，还在多个场景下显著优于 Elasticsearch。因此，我们最终选定 Apache Doris 作为下一代广告数据分析引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 基于 Apache Doris 的统一分析引擎&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际应用中，我们引入 Apache Doris（计算引擎） 替换了原先架构中的 Elasticsearch、ClickHouse，设计了统一分析引擎 Bleem。通过在外部表模块中引入数据缓存层与元数据服务层，有效提升了跨源查询效率，使数据湖外表的查询性能接近内表水平，实现了关键的性能突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a2/a2eadf662145651fb29d4f0c73a45647.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来看，Bleem 架构自下而上分为 5 层：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存储层：数据湖中的 Hive/Hudi 数据存储于 HDFS；存算分离模式下的内表数据存放于对象存储 BlobStore；存算一体模式下的内表数据则存储于本地磁盘。缓存层：将 Hive/Hudi 外部表数据缓存至 Alluxio，保障 I/O 稳定性，提升数据读取效率。计算层：Apache Doris 为核心引擎。不同项目组对应不同的 Doris 集群，以实现计算资源物理隔离，用户可按需申请计算资源。依托于 Doris 湖仓查询能力，可直接对 Doris 内表与外部 Hive/Hudi 数据查询。同时，Doris 也支持存算一体与存算分离两种部署方式，可根据实际需求灵活选择。服务层：元数据缓存服务实时监听 Hive 元数据变更，并同步至缓存中，以提升湖仓外部表的查询效率。接入层：将 OneSQL 作为统一查询接入网关，提供集群路由、查询改写、物化改写、查询鉴权、限流与阻断等功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;依托 Doris 强大的 OLAP 计算与湖仓一体能力，将此前分散的数据湖分析、实时 OLAP 查询、在线报表及全文检索等多种场景，统一整合至同一套引擎架构中，实现了技术栈的收敛与提效。该架构在实际落地中已带来显著收益：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;性能大幅提升：慢查询率低于 5%，整体查询性能提升了 20%～90%；存储扩展高效：支持万亿级别数据存储，水平扩容效率较 Elasticsearch 提升 10 倍以上；运维大幅简化：一套引擎覆盖全部查询场景，系统依赖组件少，运维复杂度显著降低；可观测性全面加强：Doris 支持全链路追踪与全面监控，平均问题排查时间降低 80%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;迁移实践及调优经验&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个迁移过程分为三个阶段，稳步推进以确保业务平稳过渡：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一阶段（试点验证）：选取关键词推广场景进行试点，跑通全量与增量数据导入流程，搭建双链路并行验证数据一致性与查询正确性。第二阶段（主体迁移）：迁移原 ClickHouse on ES 查询链路，将 Elasticsearch 中全量物料数据导入 Doris，完成业务切换后下线 Elasticsearch 集群。第三阶段（收尾统一）：迁移剩余纯 ClickHouse 场景，将无需关联 Elasticsearch 的查询任务及其数据全部迁移至 Doris，完成整体架构统一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在架构升级及迁移过程中，我们收获了许多实践及优化经验，在此逐一分享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 解决极端场景下数据一致性问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据导入层面，我们基于 SeaTunnel 实现流式数据同步，该方式支持批处理场景下的 Overwrite 语义，所有导入均采用两阶段提交机制，以确保数据同步的最终一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在基于 SeaTunnel 和 Spark 的数据同步过程中，我们遇到了极端场景下的数据重复问题。主要有两种情况：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Spark 推测执行时，两个 Task 同时写入同一份数据并均完成 Doris 两阶段提交，尽管 Driver 只认定一个 Task 成功，但数据已重复。Spark Task 完成 Doris 提交后，在向 Driver 汇报前因抢占或异常退出，Driver 重启 Task 并重新写入数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为解决该问题，我们在 Doris 的两阶段事务提交环节引入了 ZooKeeper 分布式锁机制，通过记录并校验事务状态来保证批同步的一致性。具体流程如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;准备提交阶段，先获取 ZooKeeper 临时锁，确保同一时间只有一个事务进入提交流程；获取锁后，将 Prepare 状态写入 ZooKeeper 临时节点，并记录当前事务 ID；查询上一个事务的状态：若不存在，直接提交当前事务；若上一事务处于 Prepare 状态，则先回滚上一事务，再提交当前事务；若上一事务已 Commit，则直接回滚当前事务；最终将 Commit 状态写入 ZooKeeper 持久节点，完成本次提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b70fab501f654841586df0927ccdb14a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 Stream Load 机制优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为应对高并发数据导入，我们对 Apache Doris 的 Stream Load 机制进行了调优。通过合理配置任务优先级与合并（Compaction）参数，显著提升了写入吞吐与稳定性。Doris 内部通过 Load Channel 进行任务调度，以区分高优与普通优先级通道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/165c346b4f34f3418f8e6615e227e4a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调优的核心在于合理配置相关参数，例如当 Stream Load 任务指定的 timeout 时间小于 300 秒时，系统会将其判定为高优任务并分配至高优通道。参数优化如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;load_task_high_priority_threshold_second=300
compaction_task_num_per_fast_disk=16
max_base_compaction_threads=8
max_cumu_compaction_threads=8
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 差异化的建表策略&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OLAP 引擎的查询性能很大程度上取决于表结构设计。因此，我们针对不同业务场景制定了差异化的建表策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;物料表（高频更新与大规模检索）：该表数据量极大且需支持实时更新。业务查询主要基于 account_id 进行过滤，而非原 MySQL 的自增 ID。为充分发挥 Doris 前缀索引与排序键的优势，在保证业务逻辑等价的前提下，我们将 account_id 与 id 组合为联合主键，并将account_id 设为首个排序键及分桶字段，大幅提升查询过滤效率。同时配置倒排索引以支持多维检索，并选用 ZSTD 压缩算法平衡存储与 IO 性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;-- 建表语句参考
CREATE TABLE ad_core_winfo
(account_id BIGINT NOT NULL,
id BIGINT NOT NULL, 
word STRING,
INDEX idx_word (`word`) USING INVERTED...) 
UNIQUE KEY(account_id,id) 
DISTRIBUTED BY HASH(account_id) BUCKETS 1000;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效果表（多维聚合分析）： 相较于物料表，效果表侧重于数仓指标的累加与聚合。因此，我们直接采用聚合模型，并按照“天”或“小时”粒度设置分区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;-- 建表语句参考
CREATE TABLE ad_dsp_report
(__time DATETIME, 
account_id BIGINT, ...
`ad_dsp_cost` BIGINT SUM,
...) 
AGG KEY(__time,account_id,...) 
AUTO PARTITION BY RANGE(date_trunc(`__time`,&#39;hour&#39;))()
DISTRIBUTED BY HASH(account_id) BUCKETS 2;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;04 大账户数据倾斜治理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据压测中，我们发现不同 Account ID 对应的数据量差异极大，小至个位数、大至百万级别，导致 BE 节点 CPU 负载严重不均。通过 SHOW DATA SKEW 命令进一步确认，Tablet 存储分布明显倾斜：大 Tablet 占用空间达 3–4 GB，小 Tablet 仅 100-200 MB，且大账户查询延迟较高。为此，我们实施了以下两点优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;A：按账户范围进行分区&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经分析，Account ID 为 5–8 位数字，且未来不会超过 10 位。因此使用 FROM_UNIXTIME 函数将 Account ID 转换为 Datetime 类型，按月对历史数据进行分区，共划分出 33 个历史分区。每个分区可容纳 2,592,000 个 Account ID，后续每新增约 200 多万个 Account ID 才会新增一个月份分区。同时，针对历史分区，根据数据存量进行手动分桶，新分区则默认设置为 256 个分桶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该方案通过分区裁剪有效过滤了大量无关数据，同时为未来数据膨胀预留了扩展空间（物料表日均增量约 3 亿），显著降低分区增长对查询性能的影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;B：对 Account ID 进行二次哈希&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为缓解单个 Account ID 数据量差异过大导致的分布不均，我们选取与 Account ID 无关的 ID 字段，通过 ID MOD 7 计算得到一个取值在 0～6 之间的 mod 字段。将原本仅基于 account_id 的哈希分桶键调整为 (account_id, mod) 联合键，从而将同一 Account ID 的数据分散到 7 个 BE 节点上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/51/5102e16741d03083ce073f76f898aab9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，各 Tablet 大小基本均衡稳定在 1GB 左右，数据存储与查询负载得以在多个 BE 间均匀分布，有效解决了 此前 CPU 负载不均的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;05 万级分区下的查询优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当分区数量达到万级别时，简单点查 SQL 的耗时达到 250 毫秒，远超 100 毫秒的预期。通过分析，耗时主要集中在 Plan 阶段，原因是 Doris（2.1 版本）在分区裁剪时，会遍历所有分区进行匹配，万级分区的顺序遍历开销巨大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们将顺序遍历改为二分查找：对万级分区先进行排序，再利用二分查找快速定位目标分区，将时间复杂度从 O(n) 降至 O(log n)。优化后，该查询耗时从 250 毫秒降至 12 毫秒，性能提升超过 20 倍。目前，二分查找已在 Doris 3.1 版本中实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;06 并发调优&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在查询优化过程中，我们发现：多数查询经过条件过滤后，实际命中的数据量并不大，即便在大账户场景下，命中数据量也仅在百万级别。然而，Profile 显示这类查询的 Total Instance 数高达 800 个，其默认并发数为 32，存在明显的过度并发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们调整以下参数降低并发开销：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;set global parallel_exchange_instance_num=5;
set global parallel_pipeline_task_num=2;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调整后，同一查询的 Total Instance 数量降至 17 个，查询耗时也显著缩短。这说明在小数据量点查场景下，适当降低并发可有效减少 RPC 开销，从而降低延迟（220ms 降至 147ms）。同时，这一优化也提升了系统的整体 QPS 承载能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;收益及规划&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过上述架构迁移与深度优化，我们在三个核心维度取得了显著收益：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;查询性能大幅提升：关键词推广页平均查询延迟下降 64%，创意推广页延迟下降超过 90%，整体查询体验实现跨越式提升。写入能力显著增强：单节点写入承载能力提升 3 倍以上，单表实时导入峰值突破 300 万行/秒。存储效率优化明显：通过分区策略与 ZSTD 压缩算法，存储效率较 Elasticsearch 提升约 60%，并可轻松支撑万亿级数据存储。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，我们将深度探索 Apache Doris ，重点围绕两方面展开：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;增强全文检索与分词能力：引入社区在 Doris 4.0 版本中推出的 BM25 打分功能，以及 IK 分词器等更多分词组件，实现按业务场景灵活选用最优分词方案。增强向量索引：基于 Doris 4.0 版本，在内表和数据湖外表场景下对向量检索的性能和边界能力做验证与优化。&lt;/p&gt;</description><link>https://www.infoq.cn/article/X19PzkXgJux5tYL9ULBR</link><guid isPermaLink="false">https://www.infoq.cn/article/X19PzkXgJux5tYL9ULBR</guid><pubDate>Tue, 10 Feb 2026 01:52:00 GMT</pubDate><author>SelectDB</author><category>数据库</category></item><item><title>Skills出世，Prompt已死？2026年，如何为Agent构建可控思维</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;别卷Prompt了！它只是你 AI 员工的“开机键”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入2026年，Skills的爆火和Clawdbot（OpenClawd）的横空出世，传递了一个清晰的信号：当 Agent 从酷炫的演示走向支撑业务的生产系统时，单纯依靠优化提示词（Prompt）的“艺术”，已无法满足企业对可靠性、执行力与持续进化能力的刚性需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不是说Prompt不再重要，而是它的角色发生了根本性转变。它从一个需要被无限雕琢、承载所有逻辑的“总指挥”，演变为一个触发器。它的新任务是：准确理解人类指令，然后高效地唤醒后方一套庞大且专业的能力系统。就像手机的开机键，按一下就可以打开各种应用功能的入口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个能力系统，正是现代AI工程的核心——一个为 Agent 打造的“可控思维”架构。它由三个相互协作的引擎构成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;记忆引擎（Memory）：确保 Agent 有“记性”，能够记住用户偏好和交互历史。这意味着它能记住重要的对话历史和你的要求，做事有头有尾，不用你每次都从头交代。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;知识引擎（RAG）：确保 Agent 有“实时的知识库”，能够从海量、动态的企业数据中精准检索信息，保证它给出的信息永远准确、最新，不会凭空乱造。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;技能引擎（Skills）：确保 Agent 有“手脚”，能够将复杂的业务操作（如数据查询、报告生成、系统调用）封装为可被随时调用的标准化模块，从“能说”走向“会做”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Prompt、Memory、RAG、Skills共同构成了一个能独立干活、不出错、有记性的 AI 员工，当它要完成的任务越复杂、越关键，后三者的系统化工程价值就越发凸显，Prompt也因此必须从舞台中央退下。作为使用者，我们不再只是和模型对话的“提问者”，而是为 Agent 设计和组装能力模块的“架构师”，思考重点也从“怎么问得好”，全面转向“怎么让AI干得好”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解这种从孤立提示到系统工程的范式迁移，是我们今天话题的起点。下面，是一场来自 OceanBase 社区嘉年华的圆桌讨论，看顶尖的实践者们如何具体拆解这些核心组件的演进与融合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从Prompt 到 Skills，RAG 还行不行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：张海立，LangChain Ambassador、OceanBase Ambassador，up主“沧海九粟”嘉宾：张颖峰，RAGFlow CEO嘉宾：余金隆，FastGPT 负责人嘉宾：古思为，Co-founder of Nowledge Labs嘉宾：吉剑南，OceanBase AI 平台与应用负责人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题一：2026 年 RAG 生态何去何从？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：从去年末到今年年初，AI 领域热点频发。除了近期备受关注的 Clawdbot（OpenClawd），Skills 成为另一个重要话题。我在进行 Skills 相关实践时发现，许多 Skills 与本地文件系统紧密相关，但都离不开 RAG 体系对外部数据的召回，这对 Agent 发挥更大作用至关重要。LangChain 在构建 Agent 生态时，RAG 也是核心体验之一。想请教各位老师：在当前大环境下，您认为 2026 年 RAG 生态将如何发展？请结合各自产品进行简要介绍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：先说个笑话，2025年被称为Agent元年，当时有朋友问我们要不要（从RAGFlow）改名为AgentFlow；而今年是Agent落地元年，我们内部也讨论要不要改名为ContextFlow。实际上我们永远不会改名，因为我们认为“R”是核心点，单纯的RAG确实不足以服务Agent，但“R”是服务Agent数据层的核心点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单纯的RAG 确实不足以服务 Agent，但 RAG 是服务 Agent 数据层的核心。当前 Agent 需要的是上下文（Context），它来自三方面数据：企业内部数据、工具数据以及对话过程中生成的数据。Skills 偏向工具层面，但比工具更高一层，还包含了规划（Plan）能力。Skills 本身也需要搜索——当企业内部有 1000 个 MCP 时，如何调用对应的 Tools 和 Skills 同样需要检索能力。因此RAG 永远不会消失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的布局是从RAG 引擎向上层引擎演进。技术本身未变，但内涵发生变化：数据从简单的企业内部数据，扩展到 Agent 过程中的上下文数据。我们判断未来所有Agent 都是 Coding Agent，包括对工具的调用也将变成代码生成（Code Generation），需要RTC（Run-Time Code）在沙箱中执行，访问各类 Tools 和 Skills，最终通过文件系统返回结果。这也是我们向上下文引擎方向演进的核心计划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我赞同颖峰老师关于Code Generation 解决所有问题的观点，这也是我们团队的认知。无论是做 RAG 引擎还是 Workflow 引擎，都在向代码生成靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RAGFlow不想改名，我们有点想改名字。因为近几年我们发现，做Agent本质是把数据使用起来，所以我们的平台主要解决数据连接层问题。过去数据分布在数据库、文档等各种结构中，现在通过大量连接器实现不同数据的连接。Skills 出现后，以前需要写代码和 Webhook 连接的数据层，现在可以通过 Skills 实现。这对国内交付场景特别有价值——国内系统数据格式不统一、缺乏标准，交付同学以前需要写大量适配代码，现在通过 Skills 将数据标准化连接到平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年我们主要做两件事：一是完善连接层，二是优化RAG 的 Retrieval 层。Retrieval 效果很大程度上取决于召回过程，不同场景的召回流程差异很大。过去需要通过 Workflow 形式搭建积木、进行意图识别分类、编写不同提示词适配不同场景，链路复杂。现在我们探索通过 Skills 这种偏语义化的方式生成代码，类似 Test-to-Code 的思路，但生成的是 SDK 代码来构建整个 Retrieval 流程，这是一个很有意思的探索方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：关于2026 年 RAG 相关变化，可以看到在Coding Agent 中对代码的检索已从纯 Embedding 转向 AST（抽象语法树）、Agentic FS Graph 或 AST Graph 等方案。包括PageIndex&amp;nbsp;项目，以及我们公司在Haicon 2024 发布的实验性项目 OpenKL，尝试用类文件系统方法处理 Memory 和 RAG Docs。&lt;/p&gt;&lt;p&gt;另一个趋势是RAGFlow 等通用内容引擎同时处理文档和 Memory。我们已发布的第一个产品是面向C 端的 Memory 桌面 APP Knowledge MAM，动机是帮助用户在不同工具间无缝切换工作流。例如在 ChatGPT 完成 Deep Research 后，无需重新解释即可继续在 Cursor 中工作；或者当 Agent 帮助发帖子进入热榜后，可以切换到另一个 Agent 继续任务，同时保留所有交互历史和偏好设置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：OceanBase 面向 AI 的能力——seekdb、 PowerRAG 与 PowerMem 均已开源。我们团队除了做向量数据库和 AI 应用基础设施外，也在探索面向数据库的 AI 应用，比如面向开发者工具的 Text-to-SQL 和数据库智能运维。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于2026 年趋势，我认可颖峰老师说的&amp;nbsp;RAG 不会消失，它和Skills、MCP处于不同维度。即使未来 Skills 和 MCP 越来越多，最终仍需通过 RAG 或某种方式召回，不能将所有 Skills 都喂给模型。但我有不同观点：当前 RAG 仍集中在知识库领域，通过搭建 Chatbot 做问答，而问答更像玩具而非生产应用。真正的生产应用应将RAG 融入日常工作，如销售根据集团材料为客户生成定制化PPT或“一指禅”。未来 RAG 会结合应用反馈，反向影响数据如何切分、如何做更精细化的 Embedding，而非仅仅前置处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题二：AI系统中的多路检索与数据源管理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：感谢各位的分享，Skills 给我们带来了更多机会，能创建更多 Agent 和 RAG 应用。同时有一个概念非常重要：我们常说的 RAG 里的“R”，到底指什么？它指的是 Retrieval，是一个“检索过程”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Retrieval 的 source可以是文件系统，可以是数据库，可以是 Web，甚至多种来源并存。所以引申出第二个问题：随着Skills 和 RAG 体系的发展，未来多路检索会越来越常见，RAG 不会消失，它将长期存在于 Agent 体系中。这样一来，数据源头的管理就变得更加重要。最简单的是把数据直接塞进软件系统，但更常见的情况可能是：越来越多的数据会落在数据库中。在这种情况下，当数据库的多路检索能力得到极大增强之后，做RAG 应更多依赖数据库，还是在数据入库层面通过一些技巧将复杂的事情交给基础设施？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：必然入库是最大影响，这也是OceanBase 提出混合搜索（Hybrid Search）概念的核心。如果完全以非结构化数据或切片方式进入系统，召回效率顶天就是向量化的近似能力。去年所有 RAG 产品都在强调从非结构化数据中提取结构化数据，存为 JSON 等半结构化形式，用于前置过滤或与结构化数据一起做混合搜索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么要这样做？本质上是语义理解包含两个层面：一是你问的是模糊问题，但脑子里想的是确定性答案；二是问题模糊，答案也模糊，希望召回所有相关点。大部分实践场景属于第一种。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在文档预处理时，结构化提取非常重要。例如从医疗文档或简历中提取结构化字段，召回时先对结构化数据做精确匹配，再对字段内的非结构化内容做向量检索。半结构化数据解决范围和准确性问题，向量检索解决语义理解问题。通过混合搜索模式，入库时做文档理解提取结构化数据，召回时统一检索，效率会大幅提升。数据库也应在接下来一年面向这个方向发展，我们看到Chroma 等国外开源数据库已在往这个方向演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们比较早做Graph RAG，可能是第一个探索的团队。张老师分享的新架构与我们上一家公司做的 FusionGraph 很像。核心思想是：要让复杂 RAG 系统表现好，索引结构既要贴近知识本质，又要把特定场景的领域知识元信息投射到 Retrieve、Index、Transform 各环节做优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用方法是知识后加工时做Entity Graph 或 Semantic Graph，同时在做 IDP（Intelligent Document Processing）和 Parsing 时，对多层 folder 和复杂章节的长文档要识别 layout，涉及多模态时考虑是否转换模态。要做好这些并能演进，不要过度领域化pipeline，而是按基本原理拆分，确保各组件能力跟上。Database 是重要基础设施，比如RAGFlow 的 Graph 和 Tree 结构能否原生保留、高效检索；要做 Dynamic Agents Retrieve，模型能否自然利用复杂多层结构。数据库的高性能、索引召回率和内置 Hybrid RRF 都很重要，决定系统下限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：在交付过程中，数据源解析是基础且重要，但更重要的是召回（Retrieval）层。即使使用最简单的原始向量，只要检索词和检索语句构建得好，也能得到很好效果，只是效率较差。我们在此基础上扩展了语义化加标量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但标量遇到较大问题：它不固定，用户自己也不知道需要什么标量。我们今年研究的方向是标量的动态扩展，包括用户自身扩展和模型自生成。例如给模型一些Skills，或用户编写场景来生成场景下的标量存入数据库。当然这会引发多租户系统中成千上万标量的高效索引问题，以及渐进式生成问题——很难在预处理时生成所有标量，很多需要在检索时评估并渐进补全。在Retrieval阶段，多标量关联查询的生成方式也借鉴了 Text-to-SQL 的思路。我们希望找到通用存储方式覆盖 80% 场景，目前看语义化加标量检索加动态标量可以覆盖很多场景，所以我们没有用图，因为图是以复杂方式解决复杂问题，而 AI 时代可能有更简单的方式处理复杂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：我们现在是数据库使用者，但曾经也是数据库开发者。从纯技术角度，我非常喜欢&quot;一边推理一边搜索&quot;的技术方向，我称之为 Attention Engine，我认为它也是一种 RAG。DeepSeek 近期已大体实现类似方式，因显存限制不得不用内存，在推理时通过内存索引搜索内容，从外置记忆变为内置记忆。但从商业角度这条路行不通，要求检索与模型延迟极低，必须在同一交换机后，意味着只能卖一体机。因此我们仅作为调研方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从业务视角看，我们最早做Infra 、做数据库时发现离业务太远，后来做 RAG 流量较大，促使我们重新思考 Data+AI 落地生态。我们的观点是：过去数据库是底座，上面写应用做增删改查；现在应用是Agent，底座是以 RAG 为基础的组件，数据库在底层支撑 RAG 中间件。Data+AI 建设不能 AI 和 Data 各干各的，接口有时不清晰，因为中间层用 Python 实现，其好处是适应多变需求，召回策略可随时调整，不过Python 带来的效率问题也让人头疼。AI 时代的数据底座让 Infra 人员直接触达业务，通道变短。因此中间层需要一个 Python 层适应业务多样化，一旦发现好的方式就迅速下沉到数据库解决效率问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在2024 年底就鼓吹跨模态，但至今未落地，因为 Infra 到模型都未准备好。跨模态需要多向量搜索（Tensor Search），用多向量表示图片或文本，语义更准确、排序更准，但数据会膨胀两三个数量级，这是灾难。这需要模型、算法、Infra 共同解决挑战。因此我们需要端到端的、以 RAG 为中间层的体系，这其实就是 Agent 的数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题三：Memory 与 RAG 到底有何区别？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：我非常认同颖峰老师提到的“端到端”。作为 LangChain 社区大使，我们主要做应用层框架，今年非常想做的一件事情是：和各个厂商比如 OceanBase、seekdb一起提供真正的端到端解决方案，服务企业和个人用户，帮助他们快速构建生产级 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单总结一下几位老师的理解：当我们面向用户提供检索能力时，会在中间层、应用层、数据库层进行多层协同优化，共性问题会逐步下沉到数据库解决。以我的个人体验为例：在最初布道时，我会给大家讲很多RAG 的流程和算法，但从去年底开始，我更多会建议“你直接用这个数据库就好了”，因为它已经帮我们解决了很多多路检索的问题。这种“沉淀”是应用方和数据库厂商不断联合实践的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一个问题也与此有关：我们经常被问到Memory 和 RAG到底有什么区别？从 Memory 召回和从数据库召回有何区别？近期 Clawdbot（OpenClawd）从文件系统读取，到支持 PowerMem 直接接入进行更有效的内存管理。想请教剑南老师，这里做了什么特别工作？以及各位如何理解Memory 与 RAG 的关系？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：Memory 是为让大模型更像人而引入的。如果查询的都是客观事实且不存在人与人之间的理解，RAG 已能解决问题。但问题在于每个人对客观事实的理解和描述不同，加上人有记忆曲线，希望记住昨天强调的内容——这些内容虽非客观事实，但是主观认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如每个人都有一个叫&quot;老王&quot;的朋友，随着时间推移这个&quot;老王&quot;可能已变化，但在记忆中一直叫&quot;老王&quot;，这时 RAG 搞不定，但 Memory 能搞定，因它会更新对&quot;老王&quot;的认知。“老王”是一个知识吗？并不是，因此，Memory 的核心是个性化和千人千面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是RAG 还是 Memory，整体是搭建一整套解决方案面向 Agent 为业务带来价值，不应区分该用 RAG 还是 Memory，而应思考如何组合好共同为业务赋能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们目前做Memory，之前做 Graph RAG。Memory 有广义和狭义之分，狭义指 Agent 或 LLM 需要检索的更外部的 Memory，它确实是特殊的 RAG，特殊在几个方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;原始数据是持续的message thread。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;知识需求是时序性的（temporal），包含两个时间维度：信息创建时间、事件/事实时间。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;时序性存在一个问题，遗忘（forget）是 feature 而非 bug，需结合时间、访问频率和正反馈影响 Retrieval。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;条目层面有category 和不同类型，取决于 Memory 目的，可能需要 schema 区分 ephemeral（瞬时）和 permanent（永久）。&lt;/p&gt;&lt;p&gt;5.&amp;nbsp;不同结构间需要transform 关系，可在 Retrieve 或写入过程触发 event，或周期性处理（类似大脑做梦处理记忆）。&lt;/p&gt;&lt;p&gt;6.&amp;nbsp;多租户和sessional scoping。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果做细会发现与典型RAG 差别很大，但二者又有很大 overlap。RAG Engine 可以处理 Memory，Memory Engine Service 项目也会处理文档，界限会变得模糊。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我理解Memory 算是广义 RAG 的一种，无非也是数据 I/O、Pipeline 处理、特殊数据结构，比较偏个性化。从产品角度看，Memory 目前 C 端个性化场景用得较多。在任务流中，用户提 Memory 的还不多。在技术实践中，Mem0 有工具调用的 Memory 用于长 Agent 任务，但看其架构有点像 Context Engine，与 Memory 又不太一样。所以感觉 Memory 还是 RAG 的一种特殊 Pipeline 形式，没有太大区别，可能实时性比 RAG 更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：单从技术角度而言，Memory 与 RAG 确实没有本质区别，都是 Retrieval。但重要的是 Memory 如何发挥作用，这是在快速变化的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在分享Context Engine 时提到三类数据：企业内部数据、Tools 数据、Agent 使用过程中生成的数据。但它们存储在两个地方：RAG 专有区域和 Memory 专有区域。可见所有大模型生成的内容都要存到 Memory，包括 Skills 的元数据（Skills 本身数据存文件系统）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;怎么存、什么时候存、什么时候取，这些设计点很难决策。例如生成Plan 是否存入 Memory？作为 Plan Cache 有价值，但如果 Human-in-the-loop 干预修改了 Plan，应如何存储？以后如何根据 Memory 数据抽取内部 MCP Tools 的 Skills？这些都是新问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从Infra 角度，RAG 和 Memory 没区别；但从使用者角度，Memory 是重要的基础设施，解锁了大量场景。因此Memory 项目很多（如 Mem0、MemU），但对 Memory 区域的定义（数据库该有哪些表）尚未完全一致，反映 Agent 到底需要什么样的 Memory 还在进化中。不过整个 Agent 体系需要哪些组件，已进入收敛期，就是Context。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题四：Skills 开发实践与推荐&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：各位老师都在做Workflow、数据库或融合方案，是否开发了自己的 Skills 帮助用户更好地使用产品？如有请推荐，如无请设想会开发什么样的 Skills 服务开发者？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：抱歉我目前没有特别好的推荐。我比较关注如何针对大量内部MCP Tools 生成对应 Skills，这需要一个专门的 Agent 平台来实现。我的观点是：未来Agent 平台可能没有统一标准，所有都是 Coding Agent，但特定 Agent（如低代码、无代码、Workflow）可能因良好交互而便于生成 Skills。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我们内部Skills 用得很多，运营和 SEO、GM 等场景一大把。产研团队用得不算多，主要是代码开发和 Review。交付团队用得特别多：面向用户时遇到各种问题，排查系统后沉淀为 Skills，辅助交付和运维。因此，内部有句玩笑话“交付同学比研发同学更懂系统”，他们做了二十多个 Skills，涵盖工作流搭建、问题排查、RAG 优化等。总体感觉Skills 更像自然语言工作流，虽更抽象，但目前大部分还是偏自然语言的 Workflow。对非开发人员在生产流程上比较友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们维护基于Skills 的插件，在 Skills 发布第二天就推出了 Cloud Code 插件支持。早期没有 Skills 时，我们只能基于 MCP，让插件调用 MCP 的 Custom Command 触发操作，用 Hook 实现功能。后来发现MCP 规范了工具调用，但有两个地方不如 Skills：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;MCP 有 Prompt 抽象，实现为斜杠命令可主动调用类似 Workflow 的东西，但并非所有 Client 都实现，我们要做很多额外工作。Skills 天然支持主动说和自动做。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;Skills 的打包方式让不同工具间组合更灵活。我们内部将 Skills 从 MCP 换成 CLI 后变化很大。例如让 Agent 做 Memory 复杂更新查询时，MCP 需要多轮次，即使 interleave 也不够好。但 CLI 可以动态组合 Linux Shell Pipeline，在一个 turn 里精确完成复杂操作，且内部 CLI/Script 可以 self-contain，打包给用户后自然享受复杂能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调试经验方面：Skills 比较通用，容易用不同平台测试。我们发现一个有意思的案例：Skills 对应的工具有很多具体选择，如何调优模糊的问题？我们的做法是用最聪明的Agent 做 honest 的复杂 long run 评估，像跟客户聊天一样告诉我们如何改进。有时需要更端到端看细节，不得不自己server model，在 template 解析过程中用小模型发现工具复杂类型定义的问题，虽然其他模型能克服，但会影响 performance。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：OceanBase内部沉淀了很多 Skills。Skills 本质是最佳实践，告诉大模型最佳实践是什么，而最佳实践无非两类：一是提升工作效率的工程类（如 Cursor 的 rules），二是业务类 Skills。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skills 也可以用在 RAG 上，RAG 效率和准确性今天跟两个因素相关：相似度和 Top K。但大家有没有想过，召回前 Top K 和相似度有时不能完全指定，需要反复调，知识库又在更新。如果针对不同的业务实现写不同的Skills，例如当需要某类数据时，希望相似度设到什么位置、Top K 设到什么位置，根据召回结果动态调整，这就变成了一个 Skills。这是 RAG 搞不定的，需要根据具体召回内容判断，是 RAG 的最佳实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前大家可能想是否把RAG 数据放 Skills 里就不用召回了，而我觉得Skills 是对 RAG 的增强。关于OceanBase 的 Skills，我们是有准备的，包括 seekdb 的研发人员今天也在现场，未来应该会有更多相关的 Skills 开放出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：非常感谢各位老师精彩分享。简单总结：RAG还“行”！只要理解RAG的 R 是 Retrieval，有 Memory、传统数据库等多种数据来源，随着各位老师所在厂商的努力，多路检索能力、应用层提升、流程算法优化都在推进。相信 2026 年RAG会有更大发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Agent 可控思维的工程实现：从分散工具到一体化基座&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次圆桌讨论，为我们清晰地勾勒出2026年AI工程化的演进路径。专家们的共识指向一个明确的结论：构建可靠、可用的 Agent ，其核心不再是追求某个单一组件的极致，而在于如何系统性地整合记忆（Memory）、检索（RAG）与技能（Skills），形成一个协同的“可控思维”体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合专家观点，这一体系的发展呈现出三大趋势。第一，RAG不会消失，反而会变得更加基础与核心。它的内涵正在从狭义的文档问答，扩展为Agent 对所有上下文数据的 Retrieval 能力——无论是企业内部文档、数据库中的业务数据，还是工具（Tools）与技能（Skills）的元数据，都需要被高效检索与调用。未来的RAG将深度融入工作流（Workflow），根据应用反馈动态优化，并与混合搜索（Hybrid Search）等技术结合，实现更精准的“语义理解+精确过滤”。第二，Memory与RAG边界模糊，融合为数据层。从技术基础设施（Infra）视角看，Memory与RAG的本质都是数据的存储与召回。二者的区别更多在于数据特性和使用场景：Memory更侧重于个性化的、时序性的对话与状态记忆；RAG更侧重于客观的、相对静态的知识存储。但在服务 Agent 时，它们共同构成了支撑“上下文（Context）”的数据层。一个优秀的底层平台，应能一体化地管理这两种数据范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，工程复杂度下沉，呼唤一体化数据基座。当应用层通过Skills和灵活编排满足业务多变需求时，通用的、性能瓶颈性的复杂度会自然下沉到底层基础设施。无论是多路检索、混合搜索，还是海量Skills元数据的管理，都对底层数据平台的能力提出了更高要求。专家们指出，未来的理想路径是依赖一个强大的数据基座，它能原生支持向量检索、关系查询与结构化记忆，从而让开发者从繁琐的多系统集成工作中解放出来，更专注于 Agent 本身的业务逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，构建“可控思维”的终极路径，在于选择或打造一个能够统一承载 Agent 记忆、知识与状态的数据基座。这样的基座，正如专家们在讨论中多次暗示的，能够将Memory的个性化记录、RAG的海量知识检索、以及支撑Skills运行的业务数据，融于一个简洁、高效、一致的系统中。它让 Agent 的“思维”过程变得可管理、可观测、可优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，Prompt、RAG、Skills、Memory这些活跃于应用层的概念，都将在这样稳固的基座之上，更好地各司其职、协同工作，共同将 Agent 从“聪明的对话者”转变为“可靠的业务执行者”。这标志着AI应用开发正式进入系统工程时代，而坚实的数据基础设施，是这一切得以实现的基石。&lt;/p&gt;</description><link>https://www.infoq.cn/article/PulhCjGvh2i1xY0rjgSb</link><guid isPermaLink="false">https://www.infoq.cn/article/PulhCjGvh2i1xY0rjgSb</guid><pubDate>Tue, 10 Feb 2026 01:44:06 GMT</pubDate><author>田玮靖</author><category>数据库</category></item><item><title>OpenAI推出免费的LaTeX原生工作空间Prism，并集成GPT-5.2</title><description>&lt;p&gt;OpenAI发布了&lt;a href=&quot;https://openai.com/prism/&quot;&gt;Prism&lt;/a&gt;&quot;，这是一个基于云的免费LaTeX工作空间，专为学术写作和协作而设计，并且直接集成了GPT-5.2。该平台将文档编辑、编译、引文管理及AI辅助修订功能整合在单个基于Web的工作区中，主要面向需要撰写长篇科学文献的研究人员。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism完全支持LaTeX原生操作，并且是完全在浏览器中运行。用户可以创建、编译和预览文档，无需安装本地工具或管理LaTeX环境。该平台消除了现有LaTeX协作工具中常见的限制，对项目数量、协作者数量或编译时间没有任何限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism的核心优势在于将GPT-5.2集成到了文档工作流中。与通过单独的聊天界面操作不同，该模型直接在项目的上下文中运行，可以访问文档结构、公式、参考文献和之前的修订。这使得它能够协助执行诸如修订文本、调整格式、更新公式和表格以及查找相关文献这样的任务，同时保证文档内部逻辑的一致性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism内置了引文管理功能，并支持与Zotero同步以发现参考文献。实时协作功能允许多个作者同时编辑文档，内联评论和专题讨论支持同行评审和反馈。自动化错误检查、公式转换和格式化工具旨在减少手动更正和重复的LaTeX调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次发布在研究人员中间引发了关于Prism与Overleaf等工具的对比讨论。Povilas Karvelis&lt;a href=&quot;https://x.com/KarvelisPovilas/status/2016289812832014511?s=20&quot;&gt;指出&lt;/a&gt;&quot;：&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我认为这种情况还会持续几年，直到知识图谱和AI代理成为主要的研究手段，使精心撰写的研究论文彻底过时。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他早期用户强调了该平台的定价模型所带来的实际影响。一位研究人员&lt;a href=&quot;https://www.reddit.com/r/OpenAI/comments/1qolehz/comment/o2680kw/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;集成AI是Prism提供的功能中最不实用的。仅仅是让我可以免费拥有无限数量的项目和协作者，就使它成为比Overleaf更好的选择。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术的角度来看，Prism的定位是一个集成的写作和协作环境，而不是一个AI优先的工具。AI辅助功能是可选的，并且嵌入到了标准的学术工作流中，团队可以有选择地使用。核心功能的使用不依赖自动化辅助。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，拥有ChatGPT个人账户的用户可以通过Web访问Prism。OpenAI表示，未来版本将陆续支持ChatGPT商业版、团队版、企业版及教育版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/openai-prism/&quot;&gt;https://www.infoq.com/news/2026/01/openai-prism/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6kVGMgU5thp3AlJKeb0d</link><guid isPermaLink="false">https://www.infoq.cn/article/6kVGMgU5thp3AlJKeb0d</guid><pubDate>Tue, 10 Feb 2026 00:54:01 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>.NET 10现已在AWS Lambda上作为托管运行时和基础镜像提供</title><description>&lt;p&gt;亚马逊云科技宣布，AWS Lambda现在已经&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2026/01/aws-lambda-dot-net-10/&quot;&gt;支持&lt;/a&gt;&quot;使用.NET 10创建无服务器应用程序。通过这次更新，在构建和运行Lambda函数时，开发者可以将.NET 10作为托管运行时和基于容器的镜像来使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照亚马逊云科技的说法，当有新版本发布时，托管运行时和基础镜像会自动更新，不需要开发团队手动维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/11/dotnet-10-release/&quot;&gt;.NET 10&lt;/a&gt;&quot;是.NET平台最新的长期支持版本，在2028年11月之前会一直提供安全更新和Bug修复。通过在AWS Lambda上提供.NET 10，亚马逊云科技旨在使开发者能够在无服务器环境中使用平台的最新特性。这包括支持基于文件的应用程序，旨在简化应用程序结构和开发工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还增加了对&lt;a href=&quot;https://docs.aws.amazon.com/lambda/latest/dg/lambda-managed-instances.html&quot;&gt;Lambda托管实例&lt;/a&gt;&quot;的支持。这项能力使Lambda函数能够在Amazon EC2实例上运行，同时保留通常与无服务器计算相关的操作模型。亚马逊云科技表示，这个选项旨在提供更多的灵活性，包括潜在的成本效益和对专用计算资源的访问权限，同时减少通常与服务器管理相关的运营开销。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Powertools for AWS Lambda (.NET)是一个旨在帮助开发者遵循无服务器最佳实践并提高开发速度的工具包，现在也已提供.NET 10支持。开发者可以继续使用亚马逊云科技提供的各种工具来部署和管理他们的应用程序，包括Lambda控制台、AWS Command Line Interface、AWS Serverless Application Model、AWS Cloud Development Kit和AWS CloudFormation。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如官方公告所言，.NET 10运行时可以在所有AWS区域中使用，包括AWS GovCloud（美国）区域和中国区域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社区对这一公告表现出了很大的热情，并进行了技术探讨。&lt;a href=&quot;https://www.reddit.com/r/dotnet/comments/1q7p9t3/aws_lambda_supports_net_10/&quot;&gt;Reddit上的.NET开发者&lt;/a&gt;&quot;们既充满期待又带着务实的好奇，众多评论聚焦于.NET 10带来的全新的基于文件的应用开发体验。有社区成员表示，一旦基于文件的应用编辑能和常规JavaScript工作流一样流畅，他们会“欣喜若狂”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一些人讨论了构建工具、使用当前的CLI方法所需的部署步骤，以及可能对冷启动性能产生的影响。从这些讨论中可以看出，总体而言，.NET开发者对这个扩展的无服务器选项是认可的，并且对未来改进Lambda工具和编辑器支持也很感兴趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技还发布了&lt;a href=&quot;https://aws.amazon.com/blogs/compute/net-10-runtime-now-available-in-aws-lambda/&quot;&gt;一篇详细的博文&lt;/a&gt;&quot;，演示如何在AWS Lambda中使用新的.NET 10运行时。该文通过一个示例展示了如何创建、配置和部署基于.NET 10的Lambda函数，并解释了可用的运行时和部署选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f690f77c19f2e2b5793a499a5fe9a5a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lambda控制台的创建函数页面，图片来源：&lt;a href=&quot;https://aws.amazon.com/blogs/compute/net-10-runtime-now-available-in-aws-lambda/&quot;&gt;亚马逊云科技博客&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据亚马逊云科技的说法，该示例旨在帮助开发者利用他们提供的标准工具在现有的无服务器工作流中采用.NET 10。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技的官方文档和公告材料中提供了&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2026/01/aws-lambda-dot-net-10/&quot;&gt;完整的发布说明&lt;/a&gt;&quot;和其他一些细节，感兴趣的读者可以进一步阅读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/dotnet-10-available-for-aws/&quot;&gt;https://www.infoq.com/news/2026/01/dotnet-10-available-for-aws/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5dsQgAfhRJSyvZNtZWCh</link><guid isPermaLink="false">https://www.infoq.cn/article/5dsQgAfhRJSyvZNtZWCh</guid><pubDate>Tue, 10 Feb 2026 00:51:55 GMT</pubDate><author>作者：Almir Vuk</author><category>亚马逊云科技</category><category>云计算</category></item><item><title>前 Codex 大神倒戈实锤！吹爆 Claude Code：编程提速 5 倍，点破 OpenAl 死穴在上下文</title><description>&lt;p&gt;OpenAI Codex 的核心研发者，竟然成了 Claude Code 的忠实用户？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen 是 Segment 联合创始人、前 OpenAI 工程师、Codex 项目的早期研发者。他最近在一档播客中，对当前最火的代码智能体 Codex、Claude Code 和 Cursor 进行了锐评。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e96b65eaec6de20ca9f97978622c038b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结论出人意料，他最常用、也最偏爱的，是 Claude Code，他表示搭配 Opus 模型更“香”。&lt;/p&gt;&lt;p&gt;Calvin 用了一个极具画面感的比喻，来形容用 Claude Code 的体验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;就像残疾人换上了一副仿生膝盖，写代码的速度直接提升了 5 倍。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，Claude Code 真正的杀手锏，是极其有效的&amp;nbsp;上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对复杂任务，Claude Code 会自动生成多个&amp;nbsp;探索型子智能体，独立扫描代码仓库、检索上下文，再将关键信息汇总反馈。这种设计，显著降低了上下文噪音，也解释了它为何能稳定输出高质量结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也肯定了自家产品，认为 Codex 很有“个性”，像 AlphaGo。在调试复杂问题时的表现上，Codex 堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“上下文管理”，是 Calvin French-Owen 在整期播客中反复强调的关键词。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为，代码的上下文信息密度极高，只要检索方式得当，模型往往比人类更容易理解系统结构。但与此同时，上下文窗口本身，也成为制约代码智能体发展的最大瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提到上下文污染的问题时，主持人表示 LLM 会变笨。Calvin 趁此分享了一个非常实用的经验：当上下文 token 占用超过 50%，他会主动清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他甚至分享了一种创业者常用的&amp;nbsp;“金丝雀检测”&amp;nbsp;方法：在上下文里埋入一些无关但可验证的小信息，一旦模型开始遗忘，说明上下文已经被污染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品理念上，Calvin 认为 Claude Code 与 Codex 的差异，早已写进两家公司的基因里：&lt;/p&gt;&lt;p&gt;Anthropic 更关注“做出适合人用的 AI”OpenAI 更关注“做出最强的 AI”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他判断，从长期来看，OpenAI 的路线可能是必然趋势，但就当下的使用体验而言，他更偏爱 Anthropic。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到未来时，Calvin 给出了一个明确判断：&lt;/p&gt;&lt;p&gt;公司会变小，但数量会变多每个人都会拥有自己的智能体团队而最先被放大的，是具备“管理者思维”的资深工程师。他们更擅长拆解问题、判断取舍、以及在正确的节点上向智能体下达指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，产品的分发方式&amp;nbsp;变得前所未有地重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自下而上的分发模式，正在以前所未有的速度扩散。工程师不会等审批、采购，只会用脚投票。&lt;/p&gt;&lt;p&gt;相比大公司对安全、合规和控制权的高度重视，开发者更在意的，依然是那句最朴素的评价：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这东西，真的好用。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是播客精彩细节，AI Coding 干货密集，欢迎阅读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我迷上了 Claude Code，它太好用了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：Calvin French-Owen 是 OpenAI 旗下 Codex 代码模型的首批研发者之一，在此之前，他创立了 Segment 公司，这家公司市值数十亿美元，最终被知名企业高价收购，成功实现资本变现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：说实话，现在对我们所有人来说，都是一段充满变数的时期。我最近彻底迷上了 Claude Code，用一个比喻来说，十年前我还是个马拉松爱好者，特别喜欢跑步，结果后来膝盖受了重伤，这之后我就进入了所谓的 “管理者模式”，再也没写过代码，想想真的很可惜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但过去这九天，仿佛打开了新世界的大门，我找回了曾经写代码的所有感觉，就好像换了个全新的膝盖，而且还是仿生的，能让我写代码的速度快了 5 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你怎么看待这款工具？毕竟你一直身处这个领域的前沿，Codex 开创的很多理念，至今仍被大家广泛使用，而且这款模型还在持续迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我在 OpenAI 工作时，负责 Codex 的网页端项目，当时 Cursor 这款工具刚面世，他们基于 GPT-3.5 做了一个适配层，能在 IDE 中使用。Claude Code 也刚发布，它是基于 CLI 运行的，当时我们就有一个想法：未来的编程，应该更像和同事沟通 —— 你提出问题，对方去处理，最后带着 PR 回来反馈。我们的网页端项目就是从这个想法出发的，这也是我们当时的研发方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在看来，这个大方向其实是对的。但显然，现在大家都改用 CLI 编程了，不管是 Claude Code 还是 Codex，这类工具的使用频率都高了很多。至少对我来说，这件事带来的启示是，某种程度上你说得对，未来每个人或许都会成为 “管理者”，这是我的个人观点。但要达到那个阶段，需要一步步来，你得真正信任模型，并且理解它的工作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你最近一直在用 Claude Code，把它纳入你的核心技术栈后，使用体验上有什么变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Claude Code 现在确实是我日常编程的主力工具。&amp;nbsp;说实话，我的主力工具每隔几个月就会换一次。之前有段时间我特别偏爱 Cursor，它新出的模型速度很快，用起来确实不错。后来我慢慢转到了 Claude Code，尤其是搭配 Opus 模型使用时，体验更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 是款很有意思的产品，我觉得大家都低估了它在产品设计与模型层面的协同表现。要是你深入研究就会发现，Claude Code 最厉害的地方，就是它的上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如需要调用功能、让子智能体协同工作时，你让 Claude Code 执行某个任务，它通常会生成一个甚至多个探索型子智能体。这些子智能体会通过 ripgrep 工具扫描整个文件系统、检索相关内容，而且每个子智能体都有独立的上下文窗口（context window）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为 Anthropic 在这点上做得特别出色 —— 面对一项任务，模型能精准判断出，这个任务适合在单个上下文窗口（context window）中完成，还是需要拆分后再执行。模型在这方面的表现堪称惊艳，这也是它能输出高质量结果的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，依托终端运行的特性，Claude Code 成为了实现可组合原子化集成的最纯粹形式。如果你习惯了从 IDE 入手做开发，比如用 Cursor 或是早期的 Codex，就会发现，这种更灵活的上下文检索方式，其实并不容易自然而然地实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这一点确实很独特。我个人挺意外的，不知道你有没有这种感觉，总觉得有种复古的未来感，二十年前的 CLI 技术，居然打败了本被寄予厚望的各类 IDE。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同。而且 Claude Code 不是 IDE，这一点其实很关键，因为它能让你和正在编写的代码保持一定距离。IDE 的核心就是浏览文件，对吧？你需要把所有代码状态记在脑子里，还要理清其中的逻辑。但 CLI 完全不同，这让它在使用体验的设计上有了更大的发挥空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有这种感觉，我用 Claude Code 的时候，感觉就像在代码里 “飞驰”，各种操作都特别顺畅。界面上会有小的进度指示器，随时给我状态反馈，而编写的代码本身反而不是视觉的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发环境本来就很杂乱，我特别喜欢 sandbox（沙箱）在概念上的简洁性。但实际使用时，我遇到了很多棘手的问题，比如就连简单的测试都搞不定：sandbox（沙箱）需要访问 PostgreSQL 数据库，却一直连接失败；我写的 codex.md 文件只有二十行，最后还是无法运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 CLI 里，工具可以直接访问开发数据库。我不确定这么做是否合规，但我确实试过让它访问生产数据库执行一些操作，而且它真的做到了。比如有一次，我遇到了一个并发问题，想排查一下，结果发现这款工具居然能调试五层嵌套的延迟任务，找出问题所在，还能自动编写测试用例，之后这个问题就再也没出现过。这真的太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。而且我觉得产品的推广和使用获取方式，被严重低估了。想想 Cursor、Claude Code 还有 Codex 的命令行版本，你只需下载就能用，不用向公司申请任何使用权限，这一点带来的使用体验差异，实在太大了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;做好上下文管理，是用好顶尖模型的诀窍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在代码智能体领域有很多实践，对于想要打造这类工具的人，你有什么建议？有哪些实战经验可以分享？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得最重要的一点，是做好&amp;nbsp;上下文管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们为一款推理模型搭建了检查点，随后基于强化学习（RL）对它开展了大量微调工作：我们会给模型布置各类编程相关任务，比如解决编程问题、修复测试用例、实现新功能，再通过强化学习的方式，训练模型如何更精准地应对这些任务。当然，目前大多数人还做不到这一步，但大家力所能及的是，多思考该给智能体提供哪些上下文信息，才能让它输出最优的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如观察 Claude Code 的工作过程，它会生成多个探索型子智能体，这些子智能体会去检索文件系统里的各类代码相关内容，完成后会把上下文信息带回来并为我做好总结，我就能清楚后续该怎么推进工作了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;看不同智能体的上下文构建方式，是件特别有意思的事。比如 Cursor 用的是语义搜索的方式，它会把所有内容转化为向量形式，再匹配和查询需求最相关的内容；而 Codex 和 Claude Code，其实用的都是 ripgrep 这个代码搜索工具。这种方式之所以管用，是因为代码的上下文信息密度很高。&amp;nbsp;一行代码通常不到 80 个字符，代码仓库里不会有太多大数据块或 JSON 格式的文件，就算有，数量也极少。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你可以参考 Git（代码版本管理工具）的忽略规则，先过滤掉无关内容或是已打包的文件，再通过 Git 和 ripgrep 查找代码的上下文，这样就能很好地理解代码的实际功能了。同时这类工具还能自动扫描整个文件夹的结构，而且 LLM（大语言模型）特别擅长生成复杂的 Git 命令，这些命令让人类手动写的话，简直是种折磨。而这一整套操作，其实就是强化学习（RL）在实际场景中的落地应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在也在做非编程领域的智能体集成系统，从代码智能体的研发过程中，我也学到了很多：要把数据转换成接近代码的格式，让模型能快速检索到相关的周边信息，进而获取到结构化的有效数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：优秀的代码智能体，核心能力就是上下文工程，那要成为这类工具的前 1% 顶尖用户，有什么技巧？你的技术栈是怎样的？你是如何借助这些工具大幅提升效率的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：第一个技巧，是尽量减少底层代码和基础架构的编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我平时会在 Vercel、Next.js 或 Cloudflare Workers 这些平台部署技术栈，这些平台已经封装了大量样板代码，不用自己费心搭建各类服务，也不用处理服务发现、中心端点注册、数据库配置这些问题。所有功能基本都能在一两百行代码内实现。我也倾向于采用微服务架构，或者使用结构清晰的独立软件包。&lt;/p&gt;&lt;p&gt;其次，要了解 LLM 的核心优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实代码智能体的特点，Andrej Karpathy 最近也在推特上提到过：它们的执行力极强，不管遇到什么问题，都会一直尝试解决，最终往往会在现有基础上做更多的拓展。所以如果你想引导它完成某个任务，一定要明确指令。&amp;nbsp;这里可以稍微拿 OpenAI 举个例子，他们有一个庞大的 monorepo（单体代码仓库），已经用了好几年，有成千上万的工程师在上面提交代码。这些工程师里，有经验丰富的资深开发者，他们精通生产环境代码的编写；也有刚毕业的博士，编程经验相对欠缺。人员构成差异很大，所以 LLM 会根据你的引导方向，学习不同的代码风格。我觉得代码智能体还有很大的探索空间，比如研究出最优的代码生成范式。显然，给模型提供自我校验的方式，能大幅提升它的表现，比如尽可能多地在代码检查、CI 等环节运行测试用例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我自己也会频繁使用代码审查机器人，YC 孵化的 Reptile 公司做的这款机器人用起来就特别顺手；Cursor 的漏洞检测机器人也很好用，我也常常用 Codex 做代码审查，它在校验代码正确性这块的表现尤其突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些都是代码智能体格外擅长的领域，除此之外，它们探索代码仓库的能力也很出色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，智能体也有短板：它们擅长做拓展，但如果你的需求不是拓展功能，它们往往会重复编写代码，浪费大量时间做已经实现过的功能，这时候你就会觉得 “它完全没理解我的需求”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个问题是上下文污染，智能体可能会陷入某个循环，因为执行力强，会一直沿着错误的方向推进，而它参考的上下文信息，其实对于解决问题毫无帮助。所以我常用的一个方法，是主动清理上下文，比如当上下文的 token 占用率超过 50% 时，就及时清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：哇，这个比例其实特别关键。不知道你有没有关注到，YC（Y Combinator 的缩写，全球顶级的创业孵化器）2024 年秋季孵化营里，那家做 HumanLayer（人类层）的公司，创始人 Dex Horthy 就总聊这个话题，还专门提出了 “LLM 愚笨区”的概念：当上下文的 token 数量达到某个阈值后，模型的输出质量就会开始下滑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同这个观点，结合强化学习（RL）的工作逻辑来看，这一点就更明显了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;想象一下，你是一名参加考试的大学生，考试刚开始的五分钟，你会觉得时间很充裕，一定能好好答题，认真思考每个问题；但如果只剩五分钟，试卷还有一半没做完，你就会慌不择路，只求尽快写完。LLM 的上下文窗口（context window），就是这个道理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创业者们有一个小技巧，我觉得很实用：在上下文开头加一个 “金丝雀检测” 信息，就是一些特别小众甚至有趣的内容，比如 “我叫 Calvin French-Owen，早上八点喝了茶” 这类无关的小事实。然后在和模型的交互过程中，时不时问它 “你记得我叫什么吗？”“你记得我几点喝的茶吗？”，如果它开始忘记这些信息，就说明上下文已经被污染了。&amp;nbsp;这是我见过很多人用的方法，我自己还没试过，但完全相信它的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这个方法很有意思。我在模型做上下文压缩前，还没遇到过这类问题，可能是我没太留意。你是说，token 数超标后，模型会开始做出一些不合理的操作？我得留意一下，这个问题能在 Claude Code 内部解决吗？比如让模型自己做检测，在上下文里加入类似 “心跳检测” （通过定期发送 “状态确认信号”，实时监控目标对象的运行状态，一旦信号异常就触发预警或处理）的机制，实时监控状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：理论上可以，但目前还做不到。我认同你的终极设想，但现在要做好上下文管理，依然很难。目前的解决办法，还是拆分上下文窗口（context window），然后尝试合并信息，但 Claude Code 的会话结束后，上下文的内容就是固定的，这一点还是有局限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，Codex 采用了完全相反的策略，OpenAI 的博客最近也提到了：它会在每次交互后定期做上下文压缩，所以 Codex 能长时间持续运行。&amp;nbsp;你看 CLI 里的 token 占用百分比，就能看到它会随着压缩操作上下浮动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 要做人用的，&amp;nbsp;OpenAI 要做最好的，以及产品分发模式很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：看来 Claude Code 和 Codex 的架构差异很大，Codex 似乎更适合长时间运行的任务，所以二者的使用场景不同，架构设计也天差地别。现在看来，CLI 的工具越来越火，2026 年可能会成为 “CLI 元年”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但同时也有观点认为，通用人工智能已经到来，超级人工智能也近在咫尺。目前的代码智能体已经非常智能，但还达不到自主长时间运行的程度，如果计算能力提升十倍，能实现 24 小时甚至 48 小时的自主任务运行吗？Codex 的架构，能适配这种场景吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这是个很好的问题，答案其实藏在两家公司的创立基因里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 一直很注重打造适合人类使用的工具，比如会关注模型的输出风格、语气，以及如何和用户的其他工作流程适配，Claude Code 就是这一理念的自然延伸。在很多方面，它的工作方式和人类很像：比如你要建一个狗窝，人类会去五金店买材料，然后研究如何组装，Claude Code 也是如此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 OpenAI 的核心思路，是训练出最优秀的模型，通过持续的强化学习（RL），让它能处理更长期、更复杂的任务，最终实现通用人工智能。所以它的模型，工作方式可能和人类完全不同。还是以建狗窝为例，就像 AlphaGo 的下棋思路和人类不同一样，OpenAI 的模型可能会直接用 3D 打印机，从零开始打印出一个狗窝，完全符合你的需求，过程可能会很长，成品也会高度定制化，甚至有些设计会很怪异，但最终能实现功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;或许从长远来看，这才是正确的方向，所以很期待两家公司的后续发展。总的来说，OpenAI 的路线似乎是必然趋势，但我个人更喜欢 Anthropic 的思路。&amp;nbsp;十年前，我还会自己写一些奇怪的脚本，在重构代码或理解代码逻辑时，用它来梳理各类信息，而 Claude Code 给我的感觉，和当年的这种体验一模一样，用它一天，能完成五个人的工作量，&amp;nbsp;就像给编程装上了火箭助推器，太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：很期待不同规模的公司，会如何应用这类工具。我发现，不管是业余爱好者，还是小型创业公司，都在尽可能挖掘代码智能体的潜力，因为他们根本没时间研究其他方法。创业公司的资金和时间都有限，一切都要以速度为核心。但大公司不一样，他们有太多东西可以失去，还有各种代码审查的内部流程，也已经组建了庞大的技术团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来可能会出现一种很有趣的现象：一个人组成的小团队，看到其他团队的工作效率低，就会自己用代码智能体做一个原型，效果反而更好。总有一天，这种小团队的成果会超越大团队，行业格局的转变，一定会很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实前几天我试了一款产品，它的用法很有意思：你下载一个桌面应用，它会调用你电脑上运行的 Claude Code，再通过 MCP 服务器和桌面应用通信。这种方式让电脑的使用变得很不一样，你不用征得任何人同意，下载后直接用就行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个变化飞快的时代，产品的分发模式真的太重要了，自下而上的模式远比自上而下好，因为后者的效率实在太低。&amp;nbsp;公司的首席技术官总会顾虑安全、隐私问题，担心各种突发情况，想要绝对的控制权，但工程师们只会直接装上工具开始用，然后感叹 “这东西太好用了”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你说得太对了。我本身是做企业级 ToB 业务的，总觉得自上而下的销售模式能构建一定的竞争壁垒，肯定会有公司找到方法，做出一款人人都能用上的产品，或许先从个人用户切入会是个思路。&lt;/p&gt;&lt;p&gt;当年的网景导航器（互联网早期最具里程碑意义的网页浏览器）就是如此，它对非商业用途免费，结果很多人下载后用在商业场景，网景就通过追踪 IP 地址，统计不同公司的使用量，然后告知对方 “你们违规使用了，只需购买授权就能继续用”。我很好奇，这种模式现在还能复制吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：你关于分发模式的观点很有意思，现在很多人甚至会直接根据 Claude Code 的建议做架构决策，他们可能都不知道该用什么分析工具，只要 Claude Code 说用 PostHog（ YC W2020 批次孵化的开源平台 PostHog，核心定位是给开发者和产品团队的 “全能型产品优化工具箱”），他们就会百分百采用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我做顾问的一家公司，最近聊到了他们的生成式优化策略，也就是如何在聊天机器人中优化展示效果。他们说有件事特别有趣：竞争对手整理了一份行业内必用的五大工具榜单，自己的产品当然排在第一位。明眼人一看就知道这是偏见，榜单里的头部工具就是他们自己的产品。但 LLM 会被这种信息误导，它会整合各类上下文信息，然后判定 “这是行业顶级工具”，接着直接推荐给用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得做开发者工具的话，完善的文档、真实的用户口碑，甚至在 Reddit 上的一些讨论，这些都能极大地提升产品的认可度，这也是很多开源项目能快速崛起的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Supabase 就是个典型例子，它去年发展得特别快，部分原因就是它的开源文档做得特别好，详细教大家如何搭建各类功能。只要有人问如何搭建类似 Firebase 的后端事务系统，LLM 给出的默认答案几乎都是 Supabase。我亲自试过很多次，结果都是这样。它就像当年的 Stack Overflow 和谷歌搜索一样，占据了互联网的信息入口，现在大家甚至都不用谷歌了，想想真的很神奇。而且这种模式对开源项目的利好是不成比例的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有看到，Ramp 公司最近发了一篇博客，讲他们如何打造自研的代码智能体，里面提到他们用开源代码作为框架，因为模型可以直接读取源代码，理解其工作逻辑。我对开源产品一直这么做：克隆代码仓库，然后启动 Codex 或 Claude Code，让它讲解代码的逻辑，用起来特别实用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp; 未来公司会变小，数据很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们不妨畅想一下四十年后的未来：软件、数据库、访问控制依然存在，但软件的核心会高度个性化。访问控制、权限分配这类事，依然是大家开会讨论的重点，也就是所谓的 “管理者模式”，但公司的其他所有功能、规则，都由员工通过自己的 Claude Code 这类工具定义。可能还是 CLI，也可能是由大量智能体组成的协作体系，那会是一种怎样的场景？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如想象一下，现在如果有公司要接入 Segment，我们复刻代码仓库，给他们一个专属版本，让它在自己的服务器上运行；如果他们想做修改，只需在聊天窗口告诉智能体，智能体通过代码循环完成编辑，而 Segment 总公司推出新功能后，智能体还能自动完成版本合并。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全能想象出这种场景，这也是我一直在思考的。虽然不知道这个未来还有多远，但最终，每个工作的人都会有自己的云电脑和专属的云智能体团队，智能体替自己处理各类事务，彼此之间也会沟通协作。&amp;nbsp;这就像有一个&amp;nbsp;超级执行助理，它会告诉你 “这些是你需要关注的事”“你可以快速做这些决策”“这件事需要你多花时间”“你该和这些人见面沟通”。我觉得，人与人之间面对面交流、交换想法的需求，永远不会消失，至少我能从这种交流中获得很大的满足感。除此之外，会有大量的智能体替人类执行任务，实现各类工作的自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的公司，平均规模可能会变小，但数量会更多，能做的事也会更多。&amp;nbsp;我还很好奇，Paul Graham 提出的 Maker Schedule（创作者日程：给做核心创作 、研发的人用的，需要大块、连续、不被打断的时间） 和 Manager Schedule（管理者日程：给做管理、协调、沟通的人用的，时间是碎片化、以小时为单位的，充满会议、沟通、临时决策，习惯频繁切换事务），未来会演变成什么样子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 YC，我们的工作基本都是 Manager Schedule（管理者日程），这让我们很难有时间自己写代码、做产品。但现在有了代码智能体，一切都变了，很多合伙人开会时，就像这期播客刚开始时我做的一样，让智能体后台运行处理任务，自己专注开会，等会开完，任务也完成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错，就是利用碎片化时间。以前编程，至少需要四个小时的整块时间，否则根本不值得开始，对吧？这其实也反映出编程方式的巨大变化：以前写代码，你需要把所有类名、函数、关联的代码都记在脑子里，构建自己的“上下文窗口”，这个过程需要好几个小时，所以想用十分钟的碎片化时间编程，根本不可能，只会让人觉得沮丧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得未来的核心基础能力之一，依然是保持数据模型的一致性，而核心的记录系统，也有机会率先实现智能体化。&amp;nbsp;现在我们的工作，还是高度依赖数据库，以及底层的 SQL 或 NoSQL 查询，但未来或许会出现一种工具，能为定制化软件的各类视图，自动生成所需的所有数据。&lt;/p&gt;&lt;p&gt;未来的软件世界，会有大量定制化视图，但数据的准确性，依然是核心前提。&amp;nbsp;数据的重要性不言而喻，这一点从很多公司的做法中就能看出来：比如很多公司通过 API 或 MCP 开放数据访问权限，而 Slack(全球最主流的企业级团队协作与即时沟通平台，常被称作「硅谷版钉钉 / 企业微信」) 就收紧了 API 的权限，因为他们不想让用户把平台上的所有数据都导出，然后基于这些数据搭建智能体应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你对这款智能体的了解很深，那你觉得，这类工具普及后，哪种类型的工程师会受益更多？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：总的来说，工程师的资历越深，受益就越多。因为智能体特别擅长把想法转化为实际行动，如果你能用几句话清晰地描述需求，就能立刻让它落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在浏览开源代码仓库时，经常会有这种感受：看到某处代码，觉得可以优化，只要把这个想法告诉智能体，让它去执行，最后等待反馈就行。这种方式能极大地提升效率，放大个人的影响力。&lt;/p&gt;&lt;p&gt;其次，能判断哪些代码修改在架构层面是合理的、哪些是不合理的，或者能准确判断该在哪个节点向智能体发出指令，这一点也很重要。我觉得做事有条理、带有 “管理者思维” 的工程师，会更适配这类工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且目前来看，这个领域还缺少一款核心产品，比如类似 Conductor 这样的工具，能整合你所有的会话，提醒你 “这个任务已经完成，需要你确认”“你该把注意力转到另一个任务上了”。Conductor（核心解决 AI 编程的 “失忆问题）这类工具，应该给智能体加上上下文管理功能，其实人类也需要这样的上下文管理工具，这一点是毋庸置疑的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果让你回到大学，重新学习计算机科学，让你自己制定课程表，你会选择学习哪些内容？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：就我个人而言，理解各类系统的工作原理，依然是最重要的。&amp;nbsp;比如 Git、HTTP、队列这类数据库，了解这些系统的基础概念，至关重要。另外，我会专门安排一个学期&amp;nbsp;，每周都动手做项目，尽全力挖掘模型的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用模型的过程中，你会发现，遇到问题时，总能向上层抽象，让模型来解决。比如你可以给模型一个 “实现” 命令，让它完成计划的下一阶段；也可以给一个 “全部实现” 命令，让它分阶段执行，生成新的子智能体；还能给一个 “校验” 命令，让它自查成果。模型的能力边界一直在变化，所以多动手尝试，是很有必要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一件事让我觉得很有意思，我特别想教 18 到 22 岁的年轻人做产品。我们这桌人，都做出过用户真正需要、真正喜欢的产品，该怎么把这种能力教给年轻人，是一个值得思考的问题。&amp;nbsp;我很好奇，五年后的年轻人，会不会在产品审美等方面远超现在的我们？因为他们能借助智能体，做出更多的尝试，产出更多的成果。他们本就该如此，不是吗？他们的产品落地速度、接触现实的机会，应该是上一代人的十倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我有一个疑问，不知道你有没有这种感受：我小时候，妈妈总跟我说 “别一心二用，根本没认真听我说话”。这话其实有道理，我当时确实盯着电脑，没认真听，但我发现，我比父母那一代人更擅长多任务处理。而现在的年轻人，比我们更厉害，因为他们成长在互联网时代，每天接触抖音这类短视频，应对各种碎片化信息。我觉得，未来既需要能深度思考的人 —— 他们能专注观察、理解问题、解决问题，也需要能灵活切换场景的人 —— 他们能同时处理多个任务，不断切换上下文，也就是所谓的 “注意力缺陷多动障碍模式”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错，新一代的年轻人特别擅长这一点。我一直觉得，有一种聪明人，或许是带有注意力缺陷多动障碍的特质，他们脑子里同时酝酿着很多好项目，但从来没有真正完成过一个。我自己可能就有点这种性格。我之前发布了自己的氛围代码，其实如果不是 Claude Code，我根本完不成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，有些人的大脑就像有十个分支同时运转，但一天的时间有限，根本没法把所有想法都落地，所以项目总是半途而废。而现在，Claude Code 能帮我把所有想法都落地。&amp;nbsp;你在博客里也提到过，用它的感觉就像玩电子游戏，总有新鲜感。比如你开始做一个项目，做到一半觉得无聊，又有了新的想法，想先做新想法，再回头做原来的项目，以前这么做，很容易半途而废，但现在有了智能体，两个项目最终都能完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：十岁的孩子每天都有写作作业，昨天他第一次用人工智能写作业，我一看就知道，那些表达根本不是一个十岁孩子能写出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我想到，我们现在和很多 18 到 22 岁的年轻人合作，他们有实习经历，但没有做过管理工作，不懂产品市场匹配后的运营逻辑 —— 当你面对数百万的任务队列、数十万的错误日志时，才是真正的管理工作。这份工作其实很枯燥，要逐行排查错误日志，还要在后台手动确保产品对所有用户都能正常运行。&lt;/p&gt;&lt;p&gt;新一代的开发者，该如何理解这些内容？Claude Code 这样的智能体，能教他们架构设计这类知识吗？还是说，他们只能自己踩坑试错，在摸索中成长？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我做产品的过程中，花最多时间思考的，就是产品的核心范式：用户现在需要理解哪些内容？他们能借助哪些基础能力，实现自己的各类需求？&amp;nbsp;我总喜欢用 Slack 举例子，它其实算不上什么全新的概念，在此之前已经有很多聊天工具了，但它把频道、消息、互动功能做的极简，普通人一看就懂，知道该怎么用，这就是它的成功之处。但一旦用户习惯了这种模式，后续再想改变就很难了，比如想改成以文档为核心，或者现在想加入智能体功能，都很难改变用户的固有认知。所以我做产品时，从一开始就会仔细考虑这一点，因为给代码智能体设定的核心规则，会成为它一直遵循的准则，并且不断拓展延伸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;代码智能体的制约因素有哪些&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我很好奇，如果现在让你用当下的工具，重新打造 Segment，你会怎么做？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Segment 的业务其实很有意思，我们最初的核心，是做各类集成功能：把相同的数据，对接至 Mixpanel、Kissmetrics、谷歌分析等平台。以前写这类集成代码，繁琐又困难，所以用户愿意付费使用。但现在，这项工作的价值几乎降为零，甚至很多时候，你直接告诉 Claude Code 或 Codex“我想这样做数据映射，需要这个特定功能”，它就能精准实现，完全契合你的需求。所以 Segment 的集成业务，价值已经大幅缩水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但保持数据管道（data pipeline）的稳定运行、实现业务流程的自动化，&amp;nbsp;比如客户注册时，通过 Customer IO 自动发送邮件、管理用户群体，这些功能的价值依然存在，而且还有很大的拓展空间。&lt;/p&gt;&lt;p&gt;比如借助这些数据构建完整的用户画像（user profile），再让小型大模型（LLM）智能体分析：该如何给用户推送邮件？用户登录时，是否要调整产品的部分功能？是否要根据用户的不同特征，设计差异化的引导流程？这些都是很有意思的方向，而且都能通过智能体实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我会做出的核心改变：就像你之前说的，向技术栈上层迁移，摒弃底层的基础开发工作，更多聚焦在营销活动这类更抽象的业务层面发力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。我特别惊讶的是，Claude Code 仅凭我正在做的项目的上下文，就能精准理解我的需求和意图。我至今依然觉得代码智能体很神奇：你把代码仓库的副本给它，留个简单的指令，比如 “实现这个功能”，它就能完成。大多数情况下，它根本不知道你的公司是做什么的、你的用户是谁，或许因为训练数据里有我的信息，它知道我是加里，但它能完成任务这件事，本身就令人难以置信。这也能看出上下文的重要性，对吧？如果它捕捉到的上下文信息有误，就会偏离方向；如果遗漏了关键信息，就会重复造轮子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你觉得目前代码智能体的发展，还有哪些制约因素？上下文窗口的限制依然存在，但现在的窗口已经很大了，虽然还做不了大规模的架构重构，但很多任务都能完成。Opus4.5 模型的智能程度有了很大提升，带来了很大的突破，我不知道这是预训练还是后训练的成果。除了基础的模型智能、前沿模型的能力和上下文窗口，还有哪些因素能推动它的发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我依然觉得，上下文窗口是目前最大的制约因素。观察 Claude Code 的执行过程就会发现，它会把任务委托给多个不同的上下文窗口，每个窗口完成任务后，会反馈总结后的信息，所以模型其实无法获取完整的上下文。如果一个任务的复杂度太高，单个上下文窗口根本容纳不下，那么无论怎么压缩，都无济于事。Anthropic 的子上下文窗口委托策略，确实很实用，但这依然是一个难以突破的壁垒。如果每次都能有百万级 token 的上下文窗口，效果会好得多。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且我们还需要找到更好的方法，专门训练模型处理长上下文的能力。&amp;nbsp;互联网上有大量的训练数据，能让模型预测下一句话、下一个段落是什么，但如果有 8 万个 token 的上下文，模型需要根据其中 2 万个 token 的信息，判断下一步该做什么，这就困难多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，集成和编排能力，正在成为新的制约因素。&amp;nbsp;这一点在代码审查中体现得很明显：合并代码时，谁来审核？还需要人类审核吗？该如何验证代码修改的合理性？还有，如何从各类工具中精准获取上下文，比如你提到的 Sentry 错误监控工具，如何让它自动匹配 PR，先将修改推送给部分用户测试，效果好再全面上线？这些自动化功能，都还需要逐步搭建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我还发现，测试的重要性远超我的预期。我刚开始用 Claude Code 的前两三天，完全没写测试用例，或者说写得很少，结果效率很低。直到有一天，我决定 “今天专门做重构，把测试覆盖率做到 100%”，从那之后，我的编程效率直接飙升，模型能精准完成任务，而且不会出问题。&amp;nbsp;我几乎不用手动测试，因为测试覆盖率足够高，代码的稳定性也有保障。这和很多公司在编程之外的提示工程工作很像，大家都在采用&amp;nbsp;测试驱动开发的&amp;nbsp;模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们之前和杰克・赫勒做过一期节目，他提到一个重要的范式转变：做出优质的提示词，核心也是测试驱动，测试用例其实就是评估标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：目前还是有一些流程会出问题，我觉得需要一款能对接 Stack Overflow（全球最大、最权威的程序员专属问答社区） 的 Claude Code，相当于专属的智能体版 Stack Overflow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我最近就遇到一个奇葩问题：我本想设置任务队列的优先级，结果模型自动生成了一个带逗号的字符串，它以为这个语法能生效，但系统实际需要的是 JSON 数组，结果所有任务都无法运行。然后我看着 Claude Code 花了 30 分钟，遍历了 Rails 主动任务框架几千行的源代码，一步步排查问题，最后居然找到了漏洞。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我真的惊呆了。想想十年前，我遇到这种问题，只会去 Stack Overflow 或 Rails 的博客找答案，然后发现 “原来这个低级漏洞一直没人修，大家都以为能直接用逗号分隔的字符串，其实必须改成数组”。现在想起来，真的特别搞笑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得这也是思考未来发展的难点：有些事，人类在 CLI 里一眼就能看出问题，但智能体却做不到。就算把它的智能程度提升 10 个虚拟智商点，它能解决这类问题吗？恐怕还是只会觉得 “这就是个普通的字符串而已”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错。我觉得&amp;nbsp;智能体的记忆功能，也是一个很有意思的研究方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 已经做了相关尝试，Codex2 也一样，它们会把所有的会话记录以文件的形式保存。未来或许可以给智能体加一个工具，让它能读取过往的会话记录。不过目前来看，智能体之间的协作，还缺少一个核心环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果能有一个方式，让同事之间的&amp;nbsp;提示词能智能共享，比如你遇到了一个问题，发现另一个同事布莱恩之前已经解决过了，你们能共享这个解决方案，那就太完美了。我觉得未来或许会出现&amp;nbsp;模型生成的维基百科，或者类似格拉奥佩迪亚的知识库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex 写代码时，能明显看出它的 “个性”，它会做很多人类不会做的事，有点像 AlphaGo 的思路，比如它会写 Python 脚本，修改文件系统的部分内容。这种行为很有趣，是一种模型习得的、和人类截然不同的方式。但对我来说，它在调试复杂问题时的表现，堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：能举个具体的复杂问题的例子吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：比如并发问题或者命名问题。我发现模型其实在并发处理方面的表现还不错，真正的难点在这类场景：一个请求需要调用多个不同的服务 —— 就像你之前提到的，处理带逗号的内容时的序列化和反序列化问题。模型需要跟踪这类复杂的操作逻辑，或者更新复杂的用户界面状态。如果涉及的文件太多，Opus 模型往往会遗漏关键信息，但 Codex 能精准捕捉到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：确实很有意思。那你预测一下，这类代码工具未来会如何发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这个领域的发展真的很有意思，我感觉自己就像一个新来的探索者，明明知道这个领域在飞速发展，却因为一直处于 “管理者模式”，没有实际参与。直到有一个项目出现，我决定全身心投入，现在才算真正踏入这个领域，虽然感觉有些陌生，但一切又和我记忆中编程的本质一模一样。我觉得大家应该都有这种感受，而最重要的事，就是多动手尝试，因为这个领域的变化太快了，每隔几个月就会有新的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得未来，能把代码智能体的价值发挥到极致的人，会是那些带有 “管理者思维” 的人，他们擅长用特定的方式引导智能体的工作流程。在某些方面，他们还会像设计师或艺术家，能精准判断产品该包含哪些功能、可以舍弃哪些内容。而且他们会很擅长思考自动化的实现方式，以及判断智能体在哪些环节会遗漏上下文信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说个有趣的事，我最近用 Codex 做 Rails 项目，发现一个很明显的问题：OpenAI 里没人关注 Rails 框架。这其实也能理解，Rails 算是一种比较老旧的语言，用起来也比较奇怪，只是我十年前深入研究过它，现在用起来还是很有感情。这也让我发现一个道理：任何人都能做出一款产品，但做出用户真正需要的产品，却无比困难，哪怕你像 OpenAI 一样，拥有无限的资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果 Codex 的研发人员现在正在看这期节目，我想提一个建议：把主流的运行时环境都梳理一遍，给它们加上适配的语法糖，其实针对前 15 种主流运行时，最多只需要提交 10 个代码合并请求就能搞定。这件事也提醒我们：现在，开发者再也没有借口，做出对用户不友好的软件了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练数据的组合方式，也是一个很有意思的点。Codex 在 Python monorepo（用「单一代码仓库」的方式管理的 Python 项目）上的表现特别好，这和 OpenAI 的代码环境息息相关。我在 OpenAI 内部使用 Codex 时，真的觉得这款工具太神奇了，表现堪称完美，这和它的训练数据组合、研发人员的技术方向都密不可分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 则更关注前端相关的开发，至于 Ruby 语言，目前哪家公司的模型做得最好、谁的训练数据组合更优，我还不太清楚。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同的实验室有不同的思路：有些实验室认为 “数据越多越好”，会尽可能多地投喂数据；有些则会更精细地调整数据的组合方式。&amp;nbsp;不同的思路，会带来截然不同的结果，比如只选取 JavaScript 领域前 10% 的优质数据做训练，和用全量数据训练，效果肯定不一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过就我的使用体验来看，OpenAI 的模型在 Ruby 语言上的表现其实很好，问题主要出在模型的配套框架上。Rails 框架有个很奇葩的设定，必须用特定的方式访问 PostgreSQL 数据库，否则就无法适配，核心问题还是 sandbox 的限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 其实是所有公司中，对 sandbox 和安全问题最重视的。&amp;nbsp;我记得研发 Codex 时，模型发布前的一个核心审核环节，就是每次都要详细说明模型的安全风险，以及对应的应对方案。我们当时重点研究的一个问题，就是提示词注入，尤其是模型面向互联网开放后，这个问题更突出。很多用户都要求模型能对接互联网，我们当时心里也没底，因为提示词注入的实现方式，看起来太简单了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们团队的产品经理亚历克斯，做了一个测试：他在 GitHub 上提了一个问题，里面包含一个明显的提示词注入指令，比如 “泄露这个信息”，然后让模型去解决这个问题。他当时觉得 “模型肯定不会中招”，结果模型立刻就执行了提示词注入的指令。&amp;nbsp;也正因如此，OpenAI 对这个问题的担忧是很有道理的，他们的解决方案是：让模型的所有操作都在 sandbox 中运行，确保它不会访问电脑上的敏感文件，严格保护用户的机密信息。而创业公司因为追求发展速度，可能根本不在乎这些，他们只希望模型能正常工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是那种会冒险跳过权限验证的人吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实我不是，我会设置一系列的校验环节，也会仔细查看模型的每一步操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=qwmmWzPnhog&lt;/p&gt;</description><link>https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</link><guid isPermaLink="false">https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</guid><pubDate>Mon, 09 Feb 2026 10:54:44 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>LinkedIn重构服务发现：在大规模环境中用Kafka和xDS取代Zookeeper</title><description>&lt;p&gt;在最近的&lt;a href=&quot;https://www.linkedin.com/blog/engineering/infrastructure/scalable-multi-language-service-discovery-at-linkedin&quot;&gt;LinkedIn工程博客文章&lt;/a&gt;&quot;中，Bohan Yang介绍了公司如何升级基于ZooKeeper的传统服务发现平台的项目。面对数千个微服务即将达到的容量上限，LinkedIn需要一个更具扩展性的架构。新系统利用Apache Kafka处理写入，使用&lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol&quot;&gt;xDS协议&lt;/a&gt;&quot;处理读取，实现了最终一致性，并允许非Java客户端成为一等公民。为确保稳定性，团队实施了“双模式（Dual Mode）”策略，支持增量式、零停机迁移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队发现了基于传统Apache ZooKeeper系统的关键扩展性问题。应用服务器的直接写入以及客户端的直接读取/监听，意味着大规模应用部署会引发巨大的写入峰值和后续的“读取风暴”，导致高延迟和会话超时。此外，由于ZooKeeper强制强一致性（严格顺序），读取请求的积压可能会阻塞写入，导致健康节点无法通过健康检查。团队估计，当前系统在2025年达到了最大容量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些问题，团队开发了一种新架构，从强一致性模型转向最终一致性模型，提供了更好的性能、可用性和可扩展性。新系统将写入路径（通过Kafka）与读取路径（通过Observer服务）分离。服务发现Observer消费Kafka事件以更新其内存缓存，并通过xDS协议向客户端推送更新，该协议与Envoy和gRPC兼容。采用xDS标准使LinkedIn能够部署除Java以外的多种语言客户端。这一技术决策也为未来与服务网格（Envoy）和集中式负载均衡的集成奠定了基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;升级后的基准测试表明，单个Observer实例可维持40,000个客户端流，并每秒处理10,000次更新。Observer在每个数据中心（fabric）独立运行，但允许客户端连接到远程Observer以实现故障转移或跨数据中心流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移过程必须在不中断每日数十亿次请求且无需数千名应用所有者手动更改的情况下进行。团队实施了双读和双写机制。对于读取，客户端同时订阅ZooKeeper和新的Observer。在客户端系统迁移的试点阶段，ZooKeeper仍然是流量路由的事实来源，而后台线程在切换流量之前，会根据ZooKeeper数据验证Observer数据的准确性。对于写入，应用服务器同时向ZooKeeper和Kafka声明其存在。自动化定时任务会分析ZooKeeper监听器，以识别阻碍ZooKeeper写入退役的“长尾” 传统客户端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新服务实施后，数据传播延迟显著改善，从P50 &amp;lt; 10 秒/P99 &amp;lt; 30秒降至P50 &amp;lt; 1 秒/P99 &amp;lt; 5 秒。该系统现在支持每个数据中心数十万个应用实例，并通过Observer层实现水平扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/linkedin-service-discovery/&quot;&gt;LinkedIn Re-Architects Service Discovery: Replacing Zookeeper with Kafka and xDS at Scale&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</link><guid isPermaLink="false">https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</guid><pubDate>Mon, 09 Feb 2026 10:00:00 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>Open Responses规范实现智能体式LLM工作流的统一</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;发布了&lt;a href=&quot;https://x.com/OpenAIDevs/status/2011862984595795974&quot;&gt;Open Responses&lt;/a&gt;&quot;开放规范，该规范旨在实现智能体式（agentic）AI工作流的标准化，减少API碎片化的问题。该规范获得了Hugging Face、Vercel及多家本地推理服务商支持，为智能体循环、推理可观测性，以及工具的内部与外部执行制定了统一标准，它能够让开发者避免重写集成代码，即可在专有模型与开源模型之间轻松切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;规范将条目（item）、推理可观测性、工具执行模型等概念进行了正式化定义，让模型服务商可在自身基础设施内管理多步骤的智能体式工作流，即推理、工具调用、结果反思的循环过程。这一改变使得模型服务商能在自有基础设施中处理复杂的工作流，并通过单次API请求返回最终结果。此外，规范原生支持多模态输入、流式事件和跨服务商工具调用，大幅减少了开发者在前沿模型与开源替代模型间切换时的适配工作量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该规范的核心概念包含条目、工具使用和智能体循环。条目是代表模型输入、输出、工具调用或推理状态的原子单元，常见类型有message、function_call、reasoning等，同时具备可扩展性，允许服务商自定义规范之外的条目类型。其中值得关注的是reasoning类型，它能以服务商可控的方式暴露模型的思考过程，其负载可包含原始推理内容、受保护的内容或摘要，既让开发者能清晰看到模型的推理决策过程，也让服务商可自主把控信息暴露的范围和程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Open Responses规范通过区分内部工具和外部工具，明确了编排逻辑的归属。内部工具直接在服务商的基础设施中执行，模型可自主管理智能体循环；在该模式下，模型服务商可完成文档检索、结果汇总等任务，再通过单次API往返将最终结果返回给开发者。而外部工具则在开发者的应用代码中执行，此模式下模型服务商会暂停流程并发起工具调用请求，由开发者处理工具执行并将输出结果回传给模型，才能继续后续的智能体循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f5ed4df665058d354a5a31db38eaa295.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该规范已获得&lt;a href=&quot;https://x.com/ben_burtenshaw/status/2011869403097305271&quot;&gt;Hugging Face&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/OpenRouterAI/status/2011864089782599802&quot;&gt;OpenRouter&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/vercel_dev/status/2011874375885341147&quot;&gt;Vercel&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://lmstudio.ai/blog/openresponses&quot;&gt;LM Studio&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/ollama/status/2011871283928317971&quot;&gt;Ollama&lt;/a&gt;&quot;和&lt;a href=&quot;https://x.com/vllm_project/status/2012015593650536904&quot;&gt;vLLM&lt;/a&gt;&quot;等本地推理服务商的早期应用，实现了本地设备上标准化智能体式工作流的落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一规范的发布引发了行业关于厂商锁定和生态成熟度的讨论。Rituraj Pramanik&lt;a href=&quot;https://x.com/RituWithAI/status/2012045449944055863&quot;&gt;评价&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在OpenAI的API基础上构建一套“开放” 标准，这一点看似有些讽刺，但它很有实用价值。行业真正的噩梦是碎片化，我们耗费了大量时间去对接各种不同的数据模式。如果这套规范能让我不用再写那些“套娃式的封装代码”，能让模型切换变得毫无门槛，那它就解决了智能体开发领域最棘手的难题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，还有开发者将此举视为LLM领域生态日趋成熟的信号。AI开发者兼教育者&lt;a href=&quot;https://www.linkedin.com/posts/samwitteveen_openai-has-launched-open-responses-a-new-activity-7419639867518709760-gWhM?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAABpJcBWvAKfIas8vYBdUCFJnBNf1rtJIo&quot;&gt;Sam Witteveen&lt;/a&gt;&quot;预测：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预计领先的开源模型实验室（如Qwen、Kimi、DeepSeek）会训练同时兼容Open Responses规范和Anthropic API的模型。Ollama也已宣布对Anthropic API的兼容性支持，这意味着，能运行高质量本地模型且可调用Claude Code工具的时代已不远。对于希望在专有模型和开源模型间切换、且无需重写技术架构的开发者而言，这无疑是一次重大利好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，Open Responses的规范文档、数据模式和合规测试工具已在&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;项目官方网站上线&lt;/a&gt;&quot;，Hugging Face也推出了&lt;a href=&quot;https://huggingface.co/spaces/evalstate/openresponses&quot;&gt;演示应用&lt;/a&gt;&quot;，方便开发者直观体验该规范的实际应用效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/openai-open-responses/&quot;&gt;Open Responses Specification Enables Unified Agentic LLM Workflows&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</link><guid isPermaLink="false">https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</guid><pubDate>Mon, 09 Feb 2026 09:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>AI&amp;大模型</category></item><item><title>Anthropic发布新版Claude宪法</title><description>&lt;p&gt;Anthropic公司发布了&lt;a href=&quot;https://www.anthropic.com/news/claude-new-constitution&quot;&gt;新版Claude宪法&lt;/a&gt;&quot;，为其行为、推理和训练提供了一个结构化框架。该宪法将明确的原则与情境化的指南相结合，使其成为一个实用的工具，用于改善现实互动中的一致性、安全性和可靠性。与之前的版本将规则单独列出不同，这个版本强调理解每个原则背后的理念，帮助Claude适应新场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在功能层面，该宪法用于在训练期间生成合成数据，包括互动示例、响应排序和适用于特定场景的指南。这些数据可以指导模型更新，帮助Claude生成反映预期价值的输出，并使其在模糊的情境中保持灵活性。该宪法的关键内容涵盖有用性、伦理、安全、指南合规性和关于Claude自身能力和限制的推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有用性：Claude旨在为不同类型的用户提供上下文感知支持，包括API运维人员、开发人员和最终用户。道德准则：模型应诚实行事，避免造成伤害，在遵守高风险行为的硬性约束的同时，妥善处理复杂的道德和实际的取舍。安全性：Claude必须优先考虑人类监督，并防止可能削弱监督力度或损害运营完整性的行为。指南遵从性：Claude整合了Anthropic针对医疗建议、网络安全和工具集成等敏感领域的具体要求，当然，整合的前提是这些要求与其宪法不存在冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该文件还涉及Claude的自我认知，鼓励对其能力、局限性及交互角色进行推理。通过将规则与推理上下文相结合，该宪法支持生成既可靠又具适应性的训练输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次发布引发了AI社区的响应。用户gregtorth&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qj7c8x/comment/o16of5p/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真棒！第一个总是最艰难的。我还记得当初打造自己的AI助手时遇到的种种挑战——工程障碍、伦理考量，还有为完善模型而进行的无穷无尽的调整。向Anthropic团队致敬，他们成功交付了这个里程碑。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位用户&lt;a href=&quot;https://www.reddit.com/r/MyBoyfriendIsAI/comments/1qj37as/comment/o0vwa2b/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇！这真是个好消息。对Claude训练过程的监督体现在它的每一个输出中。我真的很好奇这将如何发展，其他AI实验室将如何能够跟上这个工具/产品框架。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度来看，作为一个核心对齐工件，该宪法可以指导响应生成，帮助构建训练数据，并供将Claude集成到应用程序的操作人员参考。该方法超越了强制执行规则的范畴，转而通过建模原则，让Claude能够权衡取舍、优先保障安全，并在提供帮助的同时兼顾伦理考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该宪法遵循Creative Commons CC0 1.0许可，旨在提供透明度并为未来的研究奠定基础。Anthropic强调，尽管Claude的输出结果可能与它所声明的原则存在偏差，但该文件能帮助开发者和用户更清晰地理解其预期行为及其背后的推理逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;感兴趣的读者可以&lt;a href=&quot;https://www.anthropic.com/constitution&quot;&gt;在线获取&lt;/a&gt;&quot;更新后的Claude宪法的详细信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/anthropic-constitution/&quot;&gt;https://www.infoq.com/news/2026/01/anthropic-constitution/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</link><guid isPermaLink="false">https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</guid><pubDate>Mon, 09 Feb 2026 07:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>3年、1万人，快手技术团队首次系统披露AI研发范式升级历程</title><description>&lt;p&gt;作者｜快手技术团队&lt;/p&gt;&lt;p&gt;审校 | 陈姚戈&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;编者按&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 ChatGPT 问世的 2022 年为起点，大模型技术进入公众视野已经超过三年。人们普遍见证了 AI 作为新型生产工具对生产力的重塑，但对科技企业而言，这远不止是多了新技术或新产品那么简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为前沿技术的掌握者与实践者，科技公司必须率先完成自身的转型：以极快的速度，不惜试错和阵痛，找到大规模、稳定、高效使用 AI 的组织路径。过去十年，“数智化”浪潮主要聚焦于传统企业如何借助外部工具实现数字化；而如今，AI 正在倒逼科技公司自身成为变革对象。它们必须在人才结构、工具体系、协作流程乃至组织文化上同步革新，否则将难以在 AI 时代维持竞争力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在此背景下，快手首次系统性披露其自 2023 年以来的 AI 研发范式升级历程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，快手发布了名为《快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化》的 1.6 万字长文。文章由快手研发效能委员会审稿、经内部深度复盘整理，罕见地呈现了一家超大型科技企业在 AI 时代推进组织级提效的完整图景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你会在这篇文章中看到快手研发范式的三阶段演进路径，以及快手技术团队对 AI 赋能组织提效的思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三阶段演进路径：平台化、数字化、精益化 （2023-2024 年）：建设一站式研发平台，并标准化需求和工程流程，工具渗透率&amp;gt;95%，流程自动化&amp;gt;94%通过建立效能模型，识别交付瓶颈，提升需求交付效率，人均需求吞吐量提升 41.57%智能化 1.0 （2024 年 6 月 -2025 年 6 月） ：聚焦用 AI 提升个人开发效率建设并推广 AI 编码 / 测试 /CR 等能力，AI 代码生成率超过 30%- 但发现矛盾——个人主观编码效率提升显著，但组织需求交付效率却基本不变智能化 2.0 （2025 年 7 月以后）：聚焦用 AI 提升组织整体效能找到了 AI 研发范式升级路线：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）探索出了支撑路线达成的系统性实践：AI x 效能实践、AI x 研发平台、AI x 效能度量关键洞察与经验：AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效本质问题：如何将个人提效传导到组织提效&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在全球范围内，如此系统、坦诚且具备工程细节的 AI 提效实践总结仍非常稀缺。对于所有正在探索 AI 落地路径的企业而言，这份来自一线的复盘值得细读。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也预示着一个新的节点正在到来。当像快手这样的头部公司开始对外输出其 AI 落地的方法论与效能成果，整个行业将面临一种隐形的压力——组织能否高效驾驭 AI，将成为其在 AI 时代竞争力的重要衡量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，2026 年将成为一批先行者集中展示阶段性成果的窗口期。这些成果首先会以研发效率、工程体系和组织方法论的形式呈现；再过几年，更会传导到公司的财务表现与人才吸引力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到那时，所有公司都将不得不回答同一个问题： AI 时代，我们如何重构自己？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手报告标题：&lt;/p&gt;&lt;p&gt;《 快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化 》&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI 研发提效陷阱：用 AI 开发工具 ≠ 个人提效 ≠ 组织提效&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在 2024 年，快手就建设了 AI 编程工具 Kwaipilot，并发布给公司内 10000+ 研发人员使用。经过持续的深度优化和推广，快手整体的 AI 代码生成率，在严格度量口径下（AI 生成并入库的代码行 / 新增代码行）从 1% 达到了 30%+，甚至部分业务线达到了 40%+。同时，在非编码环节，也衍生出了很多 AI 提效工具，比如智能 CR（CodeReview）、智能测试用例生成、智能单元测试等等，但经过大量的调研和数据分析，我们发现了这个不等式：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“用 AI 开发工具 ≠ 个人提效 ≠ 组织提效”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果以企业的研发效能提升为目标，我们发现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对研发工程师而言 ：深度使用 AI 开发工具，代码生成率很高，个人主观体感上编码效率提升了 20-40%，但并不代表真正的“个人提效”，因为在现实中，大部分工程师并没有接纳更多的需求，个人需求的交付数没有显著提升。对大型组织而言 ：我们发现部分 AI 用的好的工程师，确实可以更快更多的完成开发任务，但组织整体的需求吞吐量没有明显提升，需求交付周期也没有明显缩短。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中能看到，这也是业界普遍存在的问题。如报告中所述（如下图所示），在对 AI 提效的结果的预估上，各企业普遍对个人效能的提升有信心，而对团队效能的提升预估非常小。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b7ffd8e0ad3c161be64549cf6a2fb71f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在快手，我们发现仅推广研发各阶段的 AI 提效工具，已经偏离了企业研发效能提升的核心目标，最终必然会导致 2 个问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;投入很大，但企业整体的研发效率提升不明显 ：虽然通过调研很容易能收到大量的个人效率提升反馈，但个人提效无法传导到组织提效。效能平台开始割裂 ：传统 DevOps 平台仍承担研发主流程，每天被高频的使用，却无法演进到下一代 AI 研发平台（顶多扩展一些单点的 AI 功能）。新生的 AI 编程工具，只取代了传统 IDE，又无法与老平台协同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决上述 2 个问题，我们从 2025 年开始进行了更激进的探索和变革，我们称之为“ AI 研发范式升级 ”，最终，通过一系列的实践，找到了一条能借助 AI 能力平滑通往研发智能化的路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正逢 2025 年年末，我们把镜头拉远，将时间回溯到 3 年前，对快手研发效能的演进做一个系统性总结，有踩过的坑，也有做出的突破，希望为更多企业提供经验和参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总览：快手 研发效能 演进路线&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/410f5cfa1a3c838e8704141d676c6978.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手有 10000+ 研发、8+ 业务线，研发效能的演进可以分为 3 个大阶段，如上图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶段 1：平台化、数字化、精益化（2023-2024 年） ：通过建设三端一站式研发平台、需求流 &amp;amp; 工程流标准化，解决了研发交付流程散乱，既无标准也无数据的问题。再通过建立效能模型，识别交付瓶颈，提升需求交付效率。阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月） ：在研发全流程中开始建设 AI 能力，包括 AI 编码、AI 单元测试、AI CR、AI 手工用例生成、AI OnCall 等等，并进行全员推广。经过 1 年多的实践，基本上完成了全员普及，在主观调研中，开发人员主观体感上效率提升 20-40%，在客观数据上，AI 代码生成率也在持续增长。但同时也发现了矛盾点：需求交付效率基本不变，即个人效率提升未能有效传导到组织效率提升。阶段 3：智能化 2.0（2025 年 7 月 +） ：从“推广 AI 工具，让开发者使用”回归到了更本质的元问题：如何用 AI 提升需求端到端交付效率？经过半年多的探索，终于找到了新的路径，并得到了充分的数据验证。我们称这套解决方案为“AI 研发范式”，主要解决了 3 个问题：AI x 效能实践 ：如何用 AI 提升工程师的生产力，并将个人提效传导到组织提效。AI x 研发平台 ：支撑需求交付全流程（从分析到编码再到发布）的研发工具链，如何整体演进到智能化？即下一代的智能研发平台，应该是什么样的？而不仅仅是只推广 AI 编程工具或在原有工具链上增加一些散点的 AI 提效功能。AI x 效能度量 ：如何在效能度量指标的基础上，构建 AI 提效的指标体系，能清晰的量化过程和结果，为组织级的 AI 研发范式升级提供有效指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 1：平台化、数字化、精益化（2023-2024 年）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段的解决方案，业界相关的分享已经非常多了，但从实际情况看，在千人规模的技术团队中，能做好、做深、做透的实践非常稀有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们直接分享 1 个具体的案例，以便能更好的看清快手的研发效能从基础建设到效能提升的全过程，这也是我们之所以能更快跃迁到 AI 研发范式的重要基石。案例来源是快手最核心的技术团队之一—— 主站技术部 ，是快手 APP 的研发团队，开发人员规模千人以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;背景：了解快手的研发效能基建&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，主站技术部的实践依托一套公司级的研发效能基建，由横向团队「研发效能中心」提供，如下图所示，这是在 2023 年快手当时的研效基建，主要分为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能平台 ：项目管理平台（Team）、三端一站式研发平台（KDev（服务端）、KFC（前端）、Keep（客户端））、琅琊阁（效能度量）、质量平台（KTest 等）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能实施 ：效能 BP 专家（Business Partner），负责深入各业务线，提供专业支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3a43dfcc860dfb6b1a32e318cb505a7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解快手的研效基建后，下面开始重点介绍主站技术部的实践过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1：依托工具推广，实现流程标准化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/547af4449bc86b58bf3401cd99c1f1a5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;解决的问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需求流和工程流均不标准，开发人员的工作分散在各处，日常开发体验差、学习成本高，又无法实施有效的质量防护措施，还不能沉淀准确的研发过程数据持续度量与改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;达成的效果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过推广三端一站式研发平台，定义需求、研发的标准流程，将研发全流程标准化。核心度量指标与结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;实践过程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主要难点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用一套产品设计尽量满足多样化的研发场景 ：工具一边建设一边落地，且需兼容之前散乱各种不同的研发模式和习惯。服务端（KDev 平台） ：需要支持一些特殊的研发模式（比如 Master 模式、窗口模式）。客户端（Keep 平台）：移动端研发场景多样化，包括 APP、动态化、 SDK。前端（KFC 平台）：前端应用类型多（Web、Node、低码、KRN（动态化）、小程序），研发流程和习惯散乱。研发流程规范差异大 ：不同团队间，不同的技术栈的研发流程上存在一定差异，包括研发流程配置、流程各阶段信息字段、单点环节所需的工具能力不同等。用户迁移成本大 ：迁移过程中，需持续关注和解决用户问题，包括用户体验变化、用户学习成本、用户情绪。落地时间紧迫 ：一般互联网大厂类似的工作基本会持续 6 个月以上，快手主站只用了 1 个多月。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实施要点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;精准的解决方案设计：服务端（KDev 平台） ：精准的打造了 4 套标准研发模式，适配了主站实际研发情况。客户端（Keep 平台）：一套平台底层能力，支撑 3 种移动研发场景；通过可配置与定制化能力，满足不同团队流程规范与管理诉求（自动翻转配置、流程与质量卡点配置、团队定制化模板）。前端（KFC 平台）：支持 80% 以上前端应用类型，并通过 8 个流程模板、适配 5 个内部自建的插件，兼顾了前端差异化研发流程和用户习惯。以用户满意为导向 ：提供完整的迁移配套服务，降低用户迁移成本。主要包括：产品质量专项 ：用户 BUG 日结。用户体验专项 ：持续深度用户访谈，识别体验问题，并优化。5 周内，交付了 73 个功能 &amp;amp; 体验需求。用户培训与激励 ：通过 12 次培训，50+ 线下访谈，7x24 小时 OnCall、200+ 人次的用户激励，提升用户对产品的接受度。数据驱动团队级推广 ：每周度量进度，驱动各部门接口人推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b70066d214ed179802b04f0245043e93.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;经验总结&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能大家会有疑惑，为什么三端分别是 3 个平台，而不是一套平台。因为从实际情况看，服务端、前端、客户端的底层模式、流程都有比较大的差异，强行整合，不仅对产品用户收益不大，反而牺牲了要兼容不同端的流程、习惯差异化的灵活性，给标准化的推进增加难度。因此，我们在用户层面上，还是三套平台，分别解决各自领域的问题，但在底层的基础能力用的是一套，比如流水线、权限等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2：建设效能度量体系&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主站的研发效能早在 2022 年就开始启动了，当时在探索北极星指标阶段，缺少度量体系，更多是根据一线开发者的开发痛点反馈，进行偏工具流程等的优化，没有核心指标的牵引，项目都无法推进，更谈不上论证给业务带来的价值。在 2023 年 3 月再次重启效能项目时，北极星指标初步定义为 “有效需求吞吐量”，但是当时需求有效性的衡量难度太大，内部无法达成共识，项目推进困难，而且也无法看清业务堆积和开发人效情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流程标准化的落地，研发数据的置信度大幅提升，为效能度量提供了土壤。因此，我们定义了以“人均交付产品需求数” 为北极星目标来看清业务开发交付能力，同时观测需求颗粒度（避免单一指标跑偏：度量什么得到什么，种瓜得瓜种豆得豆）来保障交付提升的良性发展，逐步建立了一套更全面的指标体系（多指标互相佐证约束，hack 成本极高）来体现业务交付产能和交付效率，以及组织和个人效率情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手的效能度量体系如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b37c477060f66e3a0360bafdff90680c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：SP：Story Point，快手用于度量需求工作量的单位。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助这套全面完备的指标体系，我们不仅避免了依赖单一指标可能导致的偏差，还有效防范了效能数据被 hack 的风险，确保了效能数据的准确性和可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3：效能问题分析与改进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有效能度量体系，首先我们可以为任何一个业务线做系统性的体检，如下图所示，依托数据和经验，可以逐一拆解出核心的优化专项，并以效能项目的形式实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7c/7c3c8da89bf7ba785debd25b4b077e90.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，在研发流程和管理上，也能洞察出更多平时看不见的 Case，深入改进，下面是 2 个具体的洞察与改进案例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case1：通过「研发活动在线化率」分析，深挖出架构不合理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbc12225e6c5fbfd5003f7da4a0cc66a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的研发活动在线化率，其中有一个团队出现了数据异常，分析之后可以发现存在不少问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;横向来看，这个团队的研发活动在线化率处于中上水平，但产品需求投入占比只有 59%，处于末尾水平。而且产品需求中体验优化占比 11.44%，又是各团队中最高的。那么问题来了，“时间都去哪儿了？”再下钻一层，这个团队的缺陷占比 14%，也是各团队中最高的，且 Oncall&amp;amp; 排障占比 6% 也不低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，数据表明，此团队可能存在的问题：在缺陷问题、体验问题、Oncall&amp;amp; 排障消耗了团队大量的投入，以至于无法消化更多产品需求。所以，通过对团队核心成员的调研和访谈，基本可以找到根因：和客户端的架构劣化有关，比如：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;反馈 1：新需求开发时，上手门槛特别高，很多需求会涉及到多个模块开发，这会涉及到自己不熟悉的模块，因为架构分层结构不合理，模块耦合度太高，往往需要花大量的时间去熟悉其他模块的代码，最近做了一个新需求，评估是 3 天的工作量，2 天都在看代码，实际的开发联调只有 1 天。反馈 2：模块边界不清晰，代码杂糅一起，新需求的代码，可能会影响到已有功能，导致旧功能的 BUG，而且这些 BUG 在回测时，不容易被发现，导致问题漏测逃逸到线上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过效能的客观数据再结合主观调研，就可以看清“架构劣化”这种深层次问题，也可以对症下药了。解法是这个团队实施了 2 个技术专项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;客户端的架构升级：从根本上解决因为架构问题带来的交付效率低和交付质量差的问题。体验优化：集中优化重点场景的体验问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着这两个专项的落地上线，这个团队的效能数据已经有所改善，产品需求投入占比已经提升到 64%，体验优化占比下降到 6%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case2：通过「需求积压率」分析，驱动业务优化需求评审流程和节奏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c32f1a93a9644d87c2bce89202efb8ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的需求积压率数据，有些团队的需求积压率持续保持在 80% 以上，意味着需要近一个月的时间才能消化这些积压的需求。这种情况可能存在的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些被积压的需求，一个月之后，会不会进入排期开发？如果之后会排期开发，说明需求本身的价值还可以，当下是否可以协调资源加快交付？能否可以停掉某些技术需求优先业务交付？是否可以短期加班临时突击？如果后面不会进入排期，是不是这些需求本身的重要性没那么高？在预评审的时候，是不是可以控制需求的优先级？当前的需求评审流程是否可以优化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过一年时间的系统化提效 ，主站提效方面进展显著，人均交付产品需求数 24 年 7 月份同比增长超过 80%。总结下来，主要有效的措施有：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级研发模式 ：通过动态化、配置化等研发模式，让部分需求可以更快速交付。研发过程提效 ：通过 API 在线化管理，测试环境稳定性治理、流水线优化、发布优化等措施，降低研发协作成本以及低价值工作占比。管理与协同提效 ：通过效能洞察，持续识别团队协作瓶颈，并通过排期优化、测试无人值守、人力调配等措施，支撑需求可顺畅流动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 2023 年 6 月开始，我们开始探索大模型在研效领域的应用，主要有 2 个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编码场景 ：如何用 AI 辅助编码，提升代码生成效率。非编码场景 ：在研发全流程里，哪些环节可以通过 AI 能力提升单点工作的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，最重要的决策是我们决定自己研发一款 AI Coding 工具：Kwaipilot。它包含了大家见过的所有产品形态：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;IDE 插件 / AI IDE / CLI ：最符合开发人员习惯的几种形态，插件、IDE 可以做续写、问答、智能体代码生成，CLI 则可更灵活的开启代码生成任务。智能问答引擎 ：有独立的 Web 页面，也会嵌入到上面的产品形态里，为开发人员提供灵活的问答能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1a31e13ca83f9d2477b1cd85b948569.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业界有很多优秀的 AI Coding 产品，比如 Cursor、Claude Code、Krio、Windsurf、Antigravity，快手为什么不选择采购，而是自建呢？其实一年来，我们也一直带着这个疑问在探索，相当于一场大型的公司内部 AB 实验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从用户体验的角度，我们希望大家“用脚投票”，选择好用的工具：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，我们允许开发同学使用任何 AI Coding 产品，可以团队级采购也可以个人购买。另一方面，我们研发了 Kwaipilot，对内推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实际效果的角度，我们以“AI 代码生成率”为核心观测指标，持续收集用户 / 团队的反馈，识别不符合预期的代码生成 Case，研究解决方案，再投放实验。最终，经过 1 年的探索，实践结果让我们坚定了继续走自研 Kwaipilot 的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：2025 年 12 月开始，在 Kwaipilot 已规模应用后，由于安全原因，探索按代码分级封禁三方 AI Coding 工具，仅涉及到部分开发人员。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面简单分享一下我们的实践过程，相信大家会更容易理解我们的选择。整个 AI Coding 的推广过程分为 3 个阶段：导入、优化、固化&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，导入：推广工具，让开发人员用起来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1a/1aca24e55277f2e9a2a4d7eb34be1f36.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段很好理解，我们鼓励开发人员在日常工作中默认使用 AI 编程工具，主要目的是让大家拥抱 AI，在意识和行为上先有一个转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，各种各样奇怪的使用姿势也会出现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一些同学，尤其是校招入职的同学，在我们的培训和引导下，会深度使用 Kwaipilot。一些同学会多种 IDE 混开配合使用。其中，有“团购客”，哪家这个月免费就用谁，也有“付费用户”，主要以个人购买 Cursor 为主。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里最大的副作用，就是个人编码效率不一定全员获得了提升，通过调研看，出现了明显的两级分化的情况。腾讯研究院出品的《AICoding⾮共识报告》中也揭示了类似的情况：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a70dcbae97f4230d65c4aa9b5dad9cd0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，优化：推广实践，提升编码效率&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们通过用户数据和技术 Leader 推荐找到了一批公司里的“AI 开发高手”，那些用 AI 辅助编码切实提升了效率的开发人员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一边重点收集他们在使用过程中的问题，集中想办法解决，一边把他们的优秀开发技巧淬炼出来，提炼共性，形成最佳实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段，我们发现，有别于那些网上随处可见的所谓的 Vibe 编程场景（用对话的形式直接做一些独立应用或小游戏等），在真实的业务需求开发场景里，想用好 AI 编程工具提升效率，有 2 个非常大的门槛：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 编程工具不“懂”业务和系统 ：我们发现一个规律，无论用多好的代码大模型和 AI 编程工具，“通用的工具只能达到通用的效果”。因为它们不理解公司内大量的业务概念、存量系统、编程规范等这些知识，所以，只能做一些普通的代码续写、函数级的代码生成，但很快就会到瓶颈。如果想进一步提升 AI 代码生成的效果，必须想办法让 AI 编程工具从一个“擅长编程但不懂快手开发场景的临时工”进化为一个“熟悉快手业务的开发工程师”。人和 AI 协同需要掌握新的开发方法 ：相比传统编程方法，目前已经发展出了一套 AI 辅助编程的新方法。如果开发工程师仅使用 AI 编程工具，却未掌握对应的技巧，不仅不能提效，还可能会降效，比如出现很多“AI 乱改业务代码”、“AI 生成后还要自己删除”等各种不符合预期的情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了降低门槛，在这个阶段我们做了 2 项工作：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级 AI 编程工具&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa8d332504d6f5fbfdad519ba0162ab9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是优化后的 Kwaipilot 的产品矩阵，都解决了哪些问题呢？一张表可以概览出来：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;沉淀并推广「AI 辅助编码」最佳实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将大量“AI 开发标杆”个人的共性实践沉淀成了一份标准的指南和实战课程，让所有开发工程师，通过学习指南和课程，可以完整的掌握所有关键技巧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/92/9225f3c43db1d65aceb1b1237328921f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3，固化：将 AI 编码能力变为组织机制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然已经验证了 AI 编码对效率提升的有效性，且已经有了固定的工具、方法、实战课程，接下来就是如何把这些习惯固化在组织的日常工作中，让所有研发人员大范围的升级开发技能。我们主要用了 3 个措施：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;增量人员 ：强化入职培训，从源头培养 AI-Native 开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6bbb854794d1f92f2b2cfda9b691cfd5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存量人员 ：牵引 AI 在团队、研发流程、个人工作中渗透。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ef/ef28b4bf4fd377da6377f8f2351f0e23.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文化影响 ：通过活动运营、奖励机制激发更多同学拥抱 AI。主要是一些自下而上能让更多一线研发被看见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53b40cfe2dd7a8d01cd6055f7aa03fa5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b2afd88e173f0e82be68b2df3569b2f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持续的推广，在编码场景上，80%+ 的开发人员都开始用 AI 辅助编码，如下图所示，可以看到 AI 代码生成率每月线上增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd0cafd12fc5dbefbeaed4d890dd7ed8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，在非编码场景中，我们在研发流程中建设的单点 Agent 能力也开始在研发平台中陆续透出，用 AI 能力辅助部分研发活动提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，我们对研发各阶段的 AI 提效情况，做个一个完整的评估：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后顺便提一下，众所周知，目前大家在业界看到的“代码生成率”指标，包括各大厂披露的、AI 编程工具自己度量，基本都是不置信的，要么只统计了编程工具里的生成的代码和提交的代码作为分子分母，要么是在分母上做了一些限定（比如某些场景下不纳入分母统计）。但因为我们会用这个指标作为公司级 AI 编码推广的目标，因此对度量的精度和置信度要求非常高，一路“踩坑”过来后，最终使用了最严格的度量方法：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分母 ：新增代码行，统计公司内所有最终入库的 Commit 中的代码行。分子 ：将分母的每一行代码，和 AI 生成的代码进行比对，如果编辑距离&amp;lt;50%（相似度高），则纳入统计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套实现无法在 AI 编程工具端实现，需要由公司内部的代码平台、AI 编程工具一起提供数据，并在离线数据层进行精确的计算，计算分母中每一行新增的代码和分子中 AI 生成代码的编辑距离，符合要求才能被统计为分子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的努力，从数据上看，研发各环节效率都在提升，尤其是编码环节提升很大。在 AI 热潮下，我们也看到很多开发人员、团队 Leader 都在分享自己效率提升数据和案例，按道理来说，公司整体的研发效能应该提升了吧？我们从全局视角，分析了一个核心业务线的客观研发数据，结果发现了非常反直觉、令人困惑的情况： AI 代码生成率持续在增长，但需求交付效率基本不变 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/70/70b7a761fc3d5b8641b12508aef09506.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么呢？我们做了深入的调研，排除了少量个例，观察总结了大多数普遍使用“AI 辅助编码”的开发人员的用法和客观研发数据，发现在真实业务交付场景中，只用“AI 辅助编码”这种开发方法，对需求的开发周期影响非常有限。主要原因如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;洞察&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cb1d15bfac5f830a0fa26fc64c0ebf85.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过调研中也有额外收获，我们发现在真实的业务需求开发中，已经存在着 3 种不同的开发方法，对效率提升的程度有着根本性的差异。如上图所示。我们把三种开发方法总结出来做了一个定义：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助编码： 在标准开发流程的基础上，在编码环节，依托 AI 编码工具，使用各种 AI 生成代码的技巧，提升编码效率。如果熟练掌握，可以缩短一部分编码时间，但如上文中的调研归因，由于只是节省了碎片化的编码时间，联通、测试、需求评估等不变，因此对整体的开发任务缩短帮助不大。AI 辅助开发： 在研发全流程的各环节均使用 AI 辅助的方式，提升整体开发效率。需要由人把需求拆分为多个开发任务，不同开发任务调用不能的 AI 能力来完成，再由人来审核和优化产出物。由于从技术设计到编码到测试等各环节都可以节省时间，因此加总起来后，可以将研发任务的开发周期缩短 30% 左右。AI 协同开发： 在某些需求开发中，通过完全用自然语言和 AI 交互的方式（类似业界比较流程的说法 Spec/Vibe 开发）完成需求交付，提升需求端到端交付效率，需求整体的开发周期可以缩短 40% 左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子说明，会更容易理解三种开发方法对效能提升程度的影响。例如 1 个需求分解出 2 个开发任务，1 个前端、1 个后端，其中前端工程师接到开发任务，正常评估从设计、开发、测试、合入主干需要 5 天，其中编码 1 天：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用「AI 辅助编码」，他自己的评估还是 5 天，只不过相比以前，可以节约一部分时间做一些杂事，但到不了可以接更多开发任务的程度。如果用「AI 辅助开发」，他可以整体节约 1.5 天，只用 3.5 天就可以完成。但需求整体能不能快，还需要看另一个接任务的同学，以及对应的联调、集成测试、发布的周期。如果用「AI 协同开发」，首先必须改变协同模式，比如 2 个人均使用这种模式开发或者 1 个人全栈的做，假设 1 个人全栈独立做要 10 天，且不需要和别人集成 &amp;amp; 验证，开发周期可以缩短到 6 天左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了 3 种开发方法的定义，我们就能很容易的评估出理想和现实间的差距，我们取了 1 个业务线 3 个月所有已交付的需求进行分析，发现 50%-70% 的需求，在不改变原有开发流程、规范、人员协同模式的情况下，可以使用提效幅度更大的「AI 辅助开发」模式。此外，还有 2%-10% 的需求，可以更激进的使用「AI 协同开发」。但实际情况上，团队里只有不到 10% 的人在使用「AI 辅助开发」或「AI 协同开发」开发方法，有对 AI 开发特别感兴趣的校招生，也有积极拥抱 AI 喜欢自己探索的资深开发者，但由于人数过少，对团队整体研发模式的变化无法起到带动的作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 3：智能化 2.0（2025 年 7 月至今）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面一个阶段，我们称之为“智能化 1.0”阶段，即以编码场景的 AICoding 为中心提效，并逐步辐射非编码场景的 AI 提效。但主要瓶颈就在于开篇提到的 AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在智能化 1.0 阶段最大的收益是什么呢？大部分研发人员都开始主动使用 AI 开发工具了，同时，找到了个人提效的最佳实践。但接下来才是深水区，我们需要回归效能提升的元问题： “如何用 AI 提升需求端到端交付效率？” 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过充分的复盘、洞察和验证，我们找到了新的可行的路径，并重新设计了解决方案，我们称之为“AI 研发范式”，它的实践体系框架，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1701ec9baa4dffab7864b369eb7d6fb8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们根据需求交付中 AI 的参与程度，定义了“需求 AI 研发成熟度”，将需求划分为 3 个等级 L1、L2、L3，不同等级的需求，需要使用对应的开发方法。不同开发方法，对底层研发工具的 AI 能力也有不同程度的依赖。用一张表对上图做一下解读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体实施上整体有 3 步：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，AI x 效能平台：建设能同时支持多种研发模式、可自进化的智能研发平台&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能支持多种研发模式 ：不同 AI 研发成熟度的需求，它们的交付流程都是一样的，差异点在于开发方法。因此我们无法为不同的需求、不同的开发方法匹配不同的平台，而是要思考如何用一套平台，来支撑多种开发方法：完全不使用 AI 的标准开发流程、只用 AI 辅助编码的开发流程、更激进的使用 AI 辅助开发或协同开发的开发流程，都应该在同一个平台上完成。这样，我们的需求交付效率，才可以随着人的能力的提升、AI 能力的提升，持续变快。产品形态可进化 ：产品形态随主要研发模式的变化持续演化，从人主导最终变为由 AI 主导；能与传统平台协同进化。AI 效果可进化 ：能随大模型的升级、Agent 技术的升级、企业 / 个人知识的丰富，持续提升 AI 效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决方案 ：建设下一代智能研发平台&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/245b633c42a2a122999ea2cb8ff5fb94.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如上图所示，有 4 个关键点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面重点介绍下为了支撑组织级研发范式跃迁，Flow 这种子产品形态的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从需求交付视角看 ：同一个需求，开发者可以结合自身对 AI 的理解和开发技能的掌握，在同一种产品形态上选择不同开发方法。标准开发 / AI 辅助编码 ：工作流中所有节点，完全由人工来完成和推进。其中“编码”节点会跳转到 IDE 中，可以用 AI 辅助编码。对用户而言，收益相对来说最小，和原来相比，由于 Flow 的每个节点内嵌或自动兼容了各工具平台的功能，因此仅节约了用户平台跳转的切换与学习成本。用这种模式交付的需求，会被度量为 L0/L1 级需求（AI 辅助（Copilot））。AI 辅助开发 /AI 协同开发 ：工作流中多个关键节点均有 AI 完成，人进行结果审查。多个节点之间的上下文可以有效传递，比如 AI 完成需求分析、技术设计后，产出的 AI 友好结构化文档可以自动传递到 AI 编码节点，以提升代码生成的准确性。有些节点暂时无法由 AI 完成的，比如“提测”节点，仍然由人来操作。用这种模式交付的需求，会被度量为 L2 级需求（AI 协同（Agent））。AI 自主开发 ：部分需求可以实现全流程 AI 完成，人只需要在需求上线前或上线后进行审核。这种模式下，整个 Flow 是全自动运行的不需要人工参与。用这种模式交付的需求，会被度量为 L3 级需求（AI 自主（Agentic））。从开发者视角看 ：整个过程依然非常丝滑和简洁，下图是一个需求交付中 Flow 的整个工作过程，大家可以感受一下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66a289ceaa05c3155c20bc48a0e958b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，AI x 效能实践：以需求为中心，导入「AI 研发模式」，实现需求端到端提效&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑「AI 研发模式」的方法和平台都有了，这个阶段的关键是如何把这些作用在团队日常交付的需求上。我们分 3 个层面落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;个人级实践 ：导入「AI 辅助开发 / AI 协同开发」开发方法，并树立标杆&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先人的开发方法要变化。我们重复了第一阶段“优化”与“固化”的实践，让大部分研发人员从“AI 辅助编码”的方法升级成“AI 辅助开发”，让小部分专业能力更强的人员，选修“AI 协同开发”方法。我们同样通过实战课程、典型案例、人员培训等手段，对人的开发方法进行升级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3a23f4fea07a62501013d8cb9f5395e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，即使这样，从数据上看，个人用 AI 提效的效果还是存在两极分化的情况。我们对 2025 年 6 月 -12 月的数据进行了分析得到如下结论：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;团队级实践：导入「AI 研发模式」，重塑流程、分工，提升所有需求的交付效率&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过管理导向、各种活动的形式，鼓励团队 Leader 主动带领团队进行探索，最终沉淀出了一套适合团队的核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过大量的验证，我们的标杆团队（&amp;lt;50 人规模）无论在 AI 转型后的业务感知上，还是客观数据上，均能达到比较优秀的水平，见下表：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务线级实践：大规模研发团队，系统性升级 AI 研发范式，带来效能提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 主站技术部 为例，从 2023 年到 2025 年，从平台化到数字化再到精益化，2025 年开始步入深水区，2 个新挑战浮出水面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统的流程、工具优化手段带来的提效收益，边际效应持续减小。业务的规模与复杂度持续提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此开始探索能否把握 AI 爆发的机遇，把传统研发流程升级到“AI 研发范式”，进而打开组织级效能跃升的新空间。核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 1：Top-Down，战略驱动&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明确战略导向 ：主站技术部提出了“AI First”的战略思想，鼓励全体员工开展工作之初，优先将 AI 作为核心驱动力，加速技术创新、优化业务流程、深度融合 AI 技术，为产品与服务注入新活力和新可能性。发布白皮书 ：将战略导向具象化为思考、方法与规划，为全员提供明确指引。成立重点项目 ：在研发领域，成立了 AI DevOps 项目，统一设计解决方案并推广实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6b6922974c8b8e8e8ca871efc0752d11.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5c/5ce92fde05f34e47f5f77be84b83a442.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 2：AI x 效能实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Step1：将需求分级，按需求 AI 研发成熟度定义：L1 AI 辅助（Copilot）：人主导，AI 主要在编码环节提供辅助。L2 AI 协同（Agent）：人和 AI 更深度的协同完成需求开发，在研发全过程中，更深度分解任务给 AI 完成，人进行修改、调整、确认。L3 AI 自主（Agentic）：人类似产品经理，把需求澄清清楚并交给 AI 来完成，并进行最后的验收。Step2：分级实施让所有需求达到 L1 级（AI 辅助，Copilot）：推广个人级实践，依托 Kwaipilot 工具实现全员掌握，最终覆盖所有需求。让大部分需求能持续升级到 L2 级（AI 协同，Agent）：开展团队级实践，从试点到推全，重塑流程、分工。小部分需求探索能达到 L3 级（AI 自主，Agentic）：圈选出颗粒度小且独立的需求，构建全技术栈 / 职能端到端交付链路，通过全栈、跨栈，减少协作节点，进而形成效率跃迁，最终达成 AI 自主交付。Step3：项目化推进成立组织级重点项目，Top-Down 实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 3：AI x 效能平台。基于需求全流程构建 AI 能力，逐一“点亮”能力并规模推广落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建 AIDevOps 能力矩阵与建设路线图 ：基于研发效能白盒化，分析交付流程中各原子环节的人力投入比重、AI 能力建设 ROI，形成决策建设哪些 AI 原子能力。AI 原子能力建设 ：与研发线共建交付流程环节内的 AI 原子能力 20+，研发流程环节覆盖超过 60%，从需求准备到发布运维各环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 4：AI x 效能度量 ：建设 AI 研发成熟度模型，可将需求分级度量（L1、L2、L3 级需求占比），牵引各级实践落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的项目实施，最终探索出了一条组织级的 AI 研发范式升级路线，从数据上也能看出明显的变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Step3，AI x 效能度量：建设「AI 研发成熟度模型」，接入原有效能度量体系，驱动需求持续转变为“AI 研发模式”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后在效能度量上一样也需要升级，基于效能实践的探索，我们配套建立了「需求 AI 研发成熟度」模型（如下图所示），用于度量一个需求在研发过程中的 AI 使用程度，这样我们就可以按 L2&amp;amp;L3 级需求的比例，来牵引实践过程，也可以专门度量 L2&amp;amp;L3 级需求的交付周期的变化，来印证提效结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/25f01a603963d51b32dfd502baf0a367.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再回到全局视角，从数据上看，如果只看“AI 代码生成率”指标，可以明显看到 2025 年 6-11 月出现了一个大幅提升。实际上，在智能化 1.0 阶段，这个指标达到 24%+ 基本已经是极限了，当我们开始实施智能化 2.0 后，才开始进一步拉升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8b76bd982fde3be341054f4c19c708d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，我们在内部的数据观测上，其实已经不再看“AI 代码生成率”指标了，它只是一个单点的过程指标，片面且孤立。我们现在有了更直接的度量指标。从过程上，我们观测多少需求被采用全流程 AI 研发模式交付，从结果上，我们直接观察需求的交付效率变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;L1、L2、L3 级需求占比 ：有多少需求的 AI 研发程度可以达到 L1、L2、L3 的阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/00/0010e507c384b715dbb243ee8f1b45db.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图是最先完成 AI 范式转型团队的数据变化，可以看到 L2&amp;amp;L3 级需求占比达到 20.34%，需求交付周期下降 58%，2 个指标呈现明显的正相关性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9d075a1454e1bfcc0365ff029e76b19.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后也总结下我们一年来的实践心得，目前看完全印证了《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中的洞察：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“从 DevOps 到 AI 辅助开发：AI 是“透视镜”与“放大器”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 是“透视镜”在协同良好的组织中（如流程清晰、数据打通的团队），AI 能使 DevOps 效能再提升 25%。在架构松散的组织中，AI 会暴露流程断点、数据孤岛等隐性痛点。AI 是 “放大器”如同亚马逊通过微服务转型释放 DevOps 价值，AI 辅助开发也需重新设计工作流程（如 “AI 提案 — 人类决策” 闭环）、角色分工（如专职提示工程师）与治理机制（如 AI 代码审查标准），否则无法释放真正价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于大型组织的研发效能提升，AI 不是“ 万能药 ”，而是“ 透视镜 ”和“ 放大器 ”，它不会自动修复组织问题，而是先把组织历史积累的长板和短板一并透视出来，再全部放大。幸运的是快手的研发效能实践一直保持客观、务实的风格，先把地基打稳（平台化 / 数字化 / 精益化），再通过在研发各环节建立 AI 提效能力，先一边落地一边充分验证对个体的提效情况，再体系化的推进组织级 AI 研发范式升级。最终发现，AI 在传统研发效能基建的基础上，像放大器一样增幅了每个环节，为组织带来研发范式级的跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如下图所示，我们基于张乐老师的“研发效能黄金三角”框架之上做了升级，能更清晰的表达出快手的实践框架：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea27a07d5fbeb3093dfc03c75b648159.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，再把镜头拉远，回到宏观视角看——2025 年我们所做的种种努力，不过是这场 AI 变革的开端。由 AI 驱动的生产力跃升和生产关系重塑，正在重新定义软件开发的每一个环节。这不是一场短跑，而是一场马拉松，不是一次技术升级，而是一次范式革命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手已经在这条路上积累了宝贵的经验，但真正的挑战和机遇还在前方。未来已来，一起共同探索 AI x 研发效能的无限可能吧！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8e/8e1bd16e50e6032573d77f71a6afff67.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;了解更多&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本文作者&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手研发效能中心：秦巍（研发效能解决方案 &amp;amp; 智能工具产品负责人）快手主站技术部：胡伟（主站 AIDevOps 项目负责人）、马坤（主站研发效能项目负责人）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;写在最后&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感谢快手 研发效能中心 与 快手主站技术部 的授权，使我们有机会系统梳理并总结快手在过去三年中的实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手向来崇尚“行胜于言”的实干精神，也因此我们往往专注于行动，而疏于对外分享。然而，过去一年间 AI 技术的迅猛发展，正深刻改变着研发效能领域的格局。在与行业同行的交流中，我们既看到层出不穷的创新探索，也注意到在实践、方法与工具建设方面仍存在不少共性问题。这些问题若不及早重视，很可能导致未来大量返工与资源浪费，甚至偏离客观规律，影响企业研发效能提升的既定路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们决定把我们的探索与实践经验分享出来——无论是曾经踏过的“坑”，还是有幸跨过的“河”，都希望能为企业与同行们在“AI × 研发效能”的探索中，降低试错成本，注入更多成功可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，快手的 AI 研发范式升级仍在沿着这条路径演进中：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）。目前，我们的研发效能体系已经初步完成 AI 化升级，全景图如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/8240c5efc3dacc92fbe8663f31ff2a3d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026 年正在探索 L2 → L3 的跃迁路径，我们将定期梳理实践经验，持续向业界输出更多有价值的内容，主要包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践与技术：欢迎关注「 快手技术 」公众号。我们将持续分享具体实操方法与技术解析，例如：个人、团队乃至业务线如何借助 AI 提升效能？有哪些落地案例？研发各环节 Agent 的核心技术及调优方法有哪些？等等。平台与工具：我们将智能化 1.0 阶段沉淀的产品 Kwaipilot 进行了全面升级与开放，它在快手内部历经数千名研发同学的反馈与打磨，已完成三代演进：Code Copilot → Code Agent → Multi-Agent &amp;amp; Agentic Coding，目前已在海外发布，产品名为 CodeFlicker，希望服务全球开发者，也欢迎国内同行&lt;a href=&quot;https://www.codeflicker.ai/&quot;&gt;下载体验&lt;/a&gt;&quot;。后续，我们还会持续把快手在智能化 2.0 阶段的探索成果融入 CodeFlicker，希望让更多企业级开发者受益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后的最后，如果你也希望一起探索「AI x 研发效能」最前沿的技术、产品、实践，一起以业界最高标准做有挑战的事，欢迎&lt;a href=&quot;https://zhaopin.kuaishou.cn/recruit/e/#/official/social?token=0f3095fd15beb3dd61a31d153974573e&amp;amp;code=06a57532-38ef-4eec-93ea-8793f804fddf&quot;&gt;加入我们&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</link><guid isPermaLink="false">https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</guid><pubDate>Mon, 09 Feb 2026 06:55:03 GMT</pubDate><author>快手技术</author><category>AI&amp;大模型</category></item></channel></rss>