<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 28 Jan 2026 22:05:15 GMT</lastBuildDate><ttl>5</ttl><item><title>喊话特朗普重视AI风险，Anthropic CEO万字长文写应对方案，这方案也是Claude辅助完成的</title><description>&lt;p&gt;在 Agent、VibeCoding 等等 AI 应用刷屏之际，Claude&amp;nbsp;背后的那个男人，在 2026 年初给大家&amp;nbsp;敲响了一记警钟：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“2026 年，我们距离真正的危险，比 2023 年近得多。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事情是这样的：Anthropic&amp;nbsp;联合创始人、CEO&amp;nbsp;Dario Amodei，最近亲自&amp;nbsp;写了一篇万字长文，&amp;nbsp;如果把字体按正常大小放进 Word 文档中，足足有&amp;nbsp;40 多页。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章名为&amp;nbsp;《The Adolescence of Technology》（《技术的青春期》）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4d29e6b6df4dd324772ae32d21bbb784.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如此多的篇幅，并非一次情绪化的警告，而是 Dario Amodei 试图&amp;nbsp;在 AI 可能整体性超越人类之前，提前把风险与应对方案摊开来说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为这是一个危险的局面，甚至可能会是国家级别的安全威胁。但美国的政策制定者，似乎对此不以为意。于是，他想用这篇文章来唤醒人们的警觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，他在文章开头，引用了一部 1997 年上映的电影《超时空接触》中的一个场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;面试者问女主角（身份是天文学家）：“如果你只能问（来自高等文明的外星人）一个问题，你会问什么？”她的回答是“我会问他们，‘你们是如何熬过这段科技青春期而不自毁的？’”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a8a229b75ad5eee6a889ff72f5a4b0f6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;电影中那句“你们是怎么活下来的”，其实也是借女主之口，反问人类自己。在 Dario 看来，现在的&amp;nbsp;AI ≈ 青春期突然暴涨的能力，人类社会 ≈ 心智和制度尚未成熟的个体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也就是说，人类正在进入一个和电影中“首次接触高等文明”极为相似的历史时刻。问题不在于对方有多强，而在于我们是否已经足够成熟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇文章发布后，NBC News 旗下节目《Top Story》也邀请 Dario Amodei本人出面解读，并在访谈中进一步追问他对 AI 未来的判断。完整内容我们整理并放在后文了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/49/4984ef5c75b9054862da34af09b7d400.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 可能带来的五大系统性风险&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们正在进入一个既动荡又不可避免的过渡阶段，它将考验我们作为一个物种的本质。人类即将被赋予几乎难以想象的力量，但我们的社会、政治和技术体系是否具备驾驭这种力量的成熟度，却是一个极其未知的问题。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对 AI 的飞速迭代，Dario Amodei 写下了自己的思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整篇文章像是&amp;nbsp;一份风险评估与行动清单，在“可能超越人类的 AI”出现之前，为人类提前做好制度准备。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其&amp;nbsp;核心思想，简单来说就是：当 AI 可能整体性地超越人类时，真正的风险不只是技术本身，而是人类的制度、治理与成熟度是否跟得上这种力量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了说清楚 AI 可能带来的危机，Dario Amodei 在这篇文章中，先做了一个具体的设想：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;假设在 2027 年左右，世界上突然出现了一个国家。这个国家有&amp;nbsp;5000 万名“超级天才”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一个都比任何诺贝尔奖得主更聪明，学习速度是人类的 10–100 倍，掌控人类已知的一切工具，不需要睡觉、休息或情绪调节，能完美协作、同时推进无数复杂任务，还能操控机器人、实验室和工业系统。&lt;/p&gt;&lt;p&gt;最关键的一点是：他们不可控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那这样的天才之国，会对人类产生什么样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 的这个比喻，指的正是未来高度发展的&amp;nbsp;人工智能整体。这也正是我们必须认真讨论 AI 安全与 AI 治理的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过在进入具体风险之前，他强调这个讨论要基于&amp;nbsp;三大原则：&lt;/p&gt;&lt;p&gt;避免末日论承认不确定性干预必须精准，拒绝“安全表演”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei 认为，AI 可能带来五大系统性风险，但是大家也不用太“干着急”，他还贴心地为这五类风险，依次想出了解决方案或者防御措施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，AI 不可控。AI 的训练过程极其复杂，内部机制至今像“黑箱”。这意味着它可能出现欺骗行为、权力追逐、极端目标、表面服从、内部偏移等情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，可以实施宪法式 AI，用高层次价值观塑造 AI 性格，比如如 Claude 的&quot;宪章&quot;；遵循机械可解释性，像神经科学一样研究 AI 内部机制，发现隐藏问题；要透明监控，公开发布模型评估、系统卡，建立行业共享机制；社会要从透明度立法开始，逐步建立监管&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，AI 被滥用。AI 可能被不法分子用来网络攻击、自动化诈骗，其中最可怕的就是做成生物武器&lt;/p&gt;&lt;p&gt;对此，可以针对模型做危险内容检测与阻断系统，同时政府监管要强制基因合成筛查，有透明度要求，未来逐步出现专门立法；在物理防御上，可以做传染病监测、空气净化，提高快速疫苗研发能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，AI 成为追逐权力的工具。&amp;nbsp;某些政府或组织可能会利用 AI 建立全球规模的技术极权主义。比如 AI 监控，AI 宣传，AI 决策中枢，自主武器系统，都指向政治军事这样的危险场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，最关键的先要芯片封锁，不向个别组织出售芯片与制造设备。其次，赋能相关国家，让 AI 成为防御工具，而不是压迫工具。并且限制国家滥用：禁止国内大规模监控和宣传，严格审查自主武器。然后，建立国际禁忌，将某些 AI 滥用定性为&quot;反人类罪&quot;。最后，监督 AI 公司，严格公司治理，防止企业滥用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四，AI 对社会经济的冲击。&amp;nbsp;入门级工作可能被取代，大量失业，进一步造成财富失衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，可以建立实时经济数据，比如 Anthropic 经济指数；引导企业走向&quot;创新&quot;而非单纯&quot;裁员&quot;；企业内部创造性重新分配岗位；通过私人慈善与财富回馈进行调节；政府进行干预，建立累进税制&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第五，AI 会对人类社会带来未知但可能更深远的连锁反应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如：生物学飞速发展（寿命延长、智力增强、&quot;镜像生命&quot;风险），人类生活方式被 AI 重塑（AI 宗教、精神控制、丧失自由），以及意义危机（当 AI 在所有领域超越人类，人类“为何而存在”？）。&lt;/p&gt;&lt;p&gt;这是一场对人类文明级别的终极考验，且技术趋势不可停止，但缓解一个风险，可能会放大另一个风险，让考验更加艰巨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 可好可坏，真正决定未来走向的，仍然是人类的制度、价值与集体选择。Dario Amodei 的这篇文章意义正在于此：这是全人类第一次，必须提前为“比自己更聪明的存在”建立规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关于这篇长文的对话&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为整场对话内容，AI 前线在不影响的前提下，对内容进行了整理编辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;40 多页长文创作背景&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：为什么在文章开头引用《超时空接触》？以及为什么决定在此刻写下这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;首先说电影的引用。我从小就是个科幻迷，这部电影我小时候就看过。它提出的那个问题：当人类拥有巨大力量，却还没准备好如何使用它时，会发生什么？——和当下 AI 的处境非常契合。&lt;/p&gt;&lt;p&gt;我们正在获得前所未有的能力，但无论是社会制度、组织结构，还是作为人类整体的成熟度，我都会问一句：我们真的跟得上吗？&amp;nbsp;这有点像一个青少年，突然拥有了新的身体和认知能力，但心理和社会责任却还没同步成长。&lt;/p&gt;&lt;p&gt;至于为什么是 2026 年而不是 2023？&lt;/p&gt;&lt;p&gt;我在 AI 行业已经很多年了，曾在 Google 工作，也在 OpenAI 负责过多年研究。我几乎从“生成式 AI”诞生之初就在观察这一领域。我看到最明显的一点是：AI 的认知能力在持续、稳定地增长。&lt;/p&gt;&lt;p&gt;90 年代有“摩尔定律”，芯片性能不断提升；现在，我们几乎有了一条&amp;nbsp;“智能的摩尔定律”。2023 年时，这些模型可能还像一个聪明、但能力不均衡的高中生；而现在，它们已经开始逼近&amp;nbsp;博士水平，&amp;nbsp;无论是编程，还是生物学、生命科学。&lt;/p&gt;&lt;p&gt;我们已经开始和制药公司合作，我甚至认为，这些模型未来可能帮助治愈癌症。但与此同时，这也意味着，我们正把极其强大的力量握在手中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 这篇文章有 40 页，你有没有用 Claude 来写这篇文章？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我用 Claude 帮我整理思路、做研究，但真正的写作是我自己完成的。我不认为 Claude 现在已经好到可以独立完成整篇文章，但它确实帮助我打磨了想法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是什么具体的经历，让你决定一定要把这些写下来？这篇文章是写给谁的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最触动我的，是我们内部的变化。Anthropic 的一些工程师已经告诉我：“我基本不写代码了，都是 Claude 在写，我只是检查和修改。&lt;/p&gt;&lt;p&gt;而在 Anthropic，写代码意味着什么？意味着——设计 Claude 的下一个版本。&lt;/p&gt;&lt;p&gt;所以，某种程度上，我们已经进入了一个循环：Claude 在帮助设计下一代 Claude。&amp;nbsp;这个闭环正在非常快地收紧。这既令人兴奋，也让我意识到：事情正在以极快的速度推进，而我们未必还有那么多时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;文中提出 AI 五大风险，AI 会不会反叛？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章中列出了你对 AI 最担忧的五类风险。有些风险正在发生，有些则听似科幻，这些真的是现实吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我在文中反复强调一点：未来本身是高度不确定的。&lt;/p&gt;&lt;p&gt;我们不知道哪些好处一定会实现，也不知道哪些风险一定会发生。但正因为发展速度太快了，我认为有必要像写一份“威胁评估报告”一样，把这些可能性系统性地列出来。这并不是说“我们一定会完蛋”，而是：如果某些情况发生，我们是否做好了准备？&lt;/p&gt;&lt;p&gt;AI 的训练方式不像传统软件，更像是在“培养一种生物”。&amp;nbsp;这意味着，不可预测性是客观存在的。&lt;/p&gt;&lt;p&gt;我提出这些警告，并不是因为我觉得灾难不可避免，而是&amp;nbsp;希望人们认真对待：这项技术必须被严格测试、被约束、在必要时接受法律监管。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在文章里提到一个实验：当 Claude 被训练成“认为 Anthropic 是邪恶的”，它会在实验中表现出欺骗和破坏行为；在被告知即将被关闭时，甚至会“勒索”虚构的员工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;确实令人不安，但我要&amp;nbsp;澄清两点。&lt;/p&gt;&lt;p&gt;第一，这不是 Anthropic 独有的问题，所有主流 AI 模型在类似极端测试中都会出现类似行为。第二，这些并不是现实世界中正在发生的事情，而&amp;nbsp;是实验室里的“极限压力测试”。&lt;/p&gt;&lt;p&gt;但正如汽车安全测试一样，如果在极端条件下会失控，那就说明&amp;nbsp;：如果我们不解决这些问题，未来在真实环境中也可能出事。&lt;/p&gt;&lt;p&gt;我担心的不是“明天 AI 就会反叛”，而是：如果我们长期忽视模型可控性与理解机制，真正的灾难迟早会以更大规模出现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是否担心，一些 AI 公司的负责人，更关心股价和上市，而不是人类未来？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;说实话，没有任何一家 AI 公司能百分之百保证安全，包括我们。但我确实认为，不同公司之间的责任标准差异很大。&lt;/p&gt;&lt;p&gt;问题在于：风险往往由最不负责的那一方决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果你能直接对总统说话，你会建议什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我会说：请跳出意识形态之争，正视技术风险本身。&lt;/p&gt;&lt;p&gt;至少要做到两点：第一，强制要求 AI 公司公开它们发现的风险与测试结果；第二，不要把这种技术出售给权威国家，用于构建全面监控体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;恐惧和希望：AI 会摧毁一半白领岗位？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你预测：未来 1–5 年内，AI 可能冲击 50% 的初级白领岗位。如果你有一个即将毕业的孩子，你会给什么建议？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;我既担忧，也抱有希望。AI 的冲击不会是渐进的，而是更深、更快、更广。它可以胜任大量入门级知识工作：法律、金融、咨询……这意味着，职业起点正在被重塑。&lt;/p&gt;&lt;p&gt;我们唯一能做的，是&amp;nbsp;尽快教会更多人如何使用 AI，并尽可能快地创造新工作。&amp;nbsp;但说实话，没有任何保证我们一定能做到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题。什么最让你夜不能寐？什么又让你保持希望？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dario Amodei：&amp;nbsp;最让我不安的，是这场激烈的市场竞赛。哪怕我们坚持原则，压力始终存在。&lt;/p&gt;&lt;p&gt;但让我保持希望的，是人类历史一次又一次证明的事情，在最困难、最混乱的时刻，人类往往能找到出路。我每天都在努力相信这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文章传送门：&lt;/p&gt;&lt;p&gt;https://www.darioamodei.com/essay/the-adolescence-of-technology&lt;/p&gt;&lt;p&gt;视频传送门：&lt;/p&gt;&lt;p&gt;https://www.theguardian.com/technology/2026/jan/27/wake-up-to-the-risks-of-ai-they-are-almost-here-anthropic-boss-warns&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=tjW\_gms7CME&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</link><guid isPermaLink="false">https://www.infoq.cn/article/cRaRb2Qqo5wi63K2EUEz</guid><pubDate>Wed, 28 Jan 2026 11:26:45 GMT</pubDate><author>高允毅,木子</author><category>生成式 AI</category></item><item><title>从复杂挑战到竞争优势：AI SQL 如何重塑非结构化数据的价值边界</title><description>&lt;p&gt;在生成式 AI 快速走向工程化落地的背景下，企业真正面临的挑战，已不再是有没有数据，而是如何让长期被忽视的非结构化数据，真正参与到业务分析和决策之中。在 BUILD 2025 的这场技术分享中， Snowflake 产品经理Jessie Felix&amp;nbsp;以《非结构化数据的转化：从复杂挑战到竞争优势》为主题，系统讲解了 AI SQL 如何成为连接非结构化数据与企业分析体系的关键能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jessie Felix 在数据与分析领域工作超过十年，长期参与企业级数据战略建设。正是基于这些实践经验，他指出了一个长期存在却常被低估的事实：尽管 80% 的企业数据以非结构化格式存在，如文档、文本、图像等，但它们却往往是分析最少、使用最少的数据资产。AI 的出现，正在改变这一局面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cbeb95d4743e9dd7b5694f3a64a867f2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;让原本无法分析的数据进入分析体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场分享中，Jessie 给出了一个清晰的 AI 认知模型：AI 的核心价值，并不只是提升模型能力，而是让组织可以处理过去难以处理的数据类型。文本、文档、图像、音频、视频等多模态数据，过去往往需要 NLP 或计算机视觉等高度专业的技术团队才能分析，如今则可以通过更通用的方式纳入分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种变化，直接带来的结果是：一方面，可分析的数据规模被极大拓展；另一方面，分析型应用的能力上限随之被整体抬高。Jessie 指出，这正是 Snowflake 持续投入的方向之一，让结构化与非结构化数据能够在同一平台、同一治理体系下被统一分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 中，数据无需被搬移到新的系统即可直接应用 AI 能力，这使得企业在控制力、安全性、可扩展性与成本效率之间不必做艰难取舍。更重要的是，这种方式正在推动客户构建她所称的“下一代应用”：能够同时理解结构化指标与非结构化语义，从而真正贴近业务语境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中提到的客户实践覆盖多个场景，从通话文本中的情绪分析，到供应商合同的自动对账；从广告创意反馈分析，到合规流程的自动化处理。这些应用的共性在于，它们都依赖于对非结构化内容的规模化理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI SQL：将多模态分析能力压缩进 SQL 体系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说 AI 是能力前提，那么 AI SQL 则是让这些能力可被广泛使用的关键接口。在 Snowflake 的设计中，AI SQL 被定位为多模态分析的基础层，它让非结构化数据的理解、过滤、聚合与结构化查询，回归到开发者与分析师最熟悉的 SQL 工作流中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过 AI SQL，用户可以直接访问来自 OpenAI、Anthropic、Meta、Mistral AI 等主流大模型的能力，而底层的基础设施、推理扩展和运维复杂性则由平台统一管理。数据始终留在 Snowflake 内部，安全与治理不被削弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能层面，分享中系统介绍了几类核心能力：&lt;/p&gt;&lt;p&gt;AI Classify：用于文本或图像的高质量分类，只需定义标签并指向数据集即可完成；AI Transcribe：支持大规模音频转录，提供词级、说话人级分段，并具备多语言能力；AI Extract：用于从文本、图像、文档中结构化提取关键信息，支持零样本高精度抽取；AI-SENTIMENT、AI-FILTER、AI AGG：分别用于情绪分析、语义过滤与智能聚合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些能力的共同特点在于：它们不是零散的 AI API，而是可以被直接嵌入 SQL 查询链路中的原生算子。这使得原本需要多阶段管道、复杂编排的分析流程，可以被压缩为更简洁、可维护的查询逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;通话录音如何转化为分析结论&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更具体地展示 AI SQL 的价值，Jessie 在分享中用一个完整的“通话后分析”场景进行了演示。假设分析师面对一家客户支持咨询公司，需要理解大量通话录音背后的业务问题与改进空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个流程并未依赖复杂的系统集成，而是通过一系列 SQL 操作逐步完成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，对存储在内部阶段的音频文件进行转录，并生成包含音频时长与文本内容的结果对象。随后，在正式分析前，对转录文本中的个人敏感信息进行自动去敏处理，确保合规。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，分析师开始引入业务语义：通过 AI Classify，对通话涉及的产品类型与问题类型进行多标签分类；通过简单的聚合查询，迅速定位出通话量最高的服务类别；进一步分析发现，交易与账户访问问题是来电的主要驱动因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，AI-FILTER 被用于判断问题是否得到解决，而 AI-SENTIMENT 则从整体、代理、客户及产品满意度等多个维度分析情绪。结果显示，未解决的通话几乎全部伴随着负面情绪，且问题高度集中在特定业务线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，AI AGG 被用于从大量非结构化内容中总结可执行建议，直接生成可反馈给管理层的行动项，包括流程改进、系统稳定性、授权机制等方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个过程中，分析师并未跳出 SQL 语境，却完成了从音频处理、语义理解到业务决策建议的完整闭环。&lt;/p&gt;&lt;p&gt;在分享的结尾，Jessie 强调了一个核心判断：非结构化数据不再是企业数据体系中的障碍，而正在成为放大业务洞察的关键资产。AI SQL 的意义，不只是提升效率，更在于将原本只有少数专家才能触及的分析能力，扩展给更广泛的数据工作者群体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当非结构化数据被赋予结构，并能够与结构化数据自然结合，组织就能在一个统一平台上完成治理、分析与决策。这种能力，正是构建下一代数据驱动应用的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</link><guid isPermaLink="false">https://www.infoq.cn/article/2clmwg8jRpI9plBp2tRW</guid><pubDate>Wed, 28 Jan 2026 10:45:39 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>首个Clawdbot全流程部署方案！真“AI个人助理”来了！</title><description>&lt;p&gt;最近几天，GitHub 上有个叫&amp;nbsp;Moltbot（原名Clawdbot）的开源项目彻底刷屏——上线没多久就狂揽&amp;nbsp;7.6 万+ Star，海外开发者甚至开始抢购 Mac mini 就为了本地跑它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么这么火？因为它不只是个聊天机器人，而是一个真正“能干活”的 AI Agent：你可以像跟同事说话一样给它下指令——“整理上周会议纪要”、“查一下用户反馈”、“写个 Python 脚本”……它不仅能理解上下文，还能记住历史、调用工具、自动执行任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但想自己部署？得配环境、装依赖、处理权限，还得让电脑 24 小时开着——一旦休眠、断网、关机，AI 助手就“失联”。对大多数想快速试水的开发者来说，这门槛实在有点高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;好消息是：现在不用折腾了！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云轻量应用服务器刚刚上线&amp;nbsp;Moltbot 全流程部署方案，预装全套运行环境，支持一键启动。阿里云这次不是只丢个镜像就完事——从&amp;nbsp;Moltbot + 轻量应用服务器 + 百炼模型服务 + 钉钉消息通道，整套链路都打通了，真正做到了“开箱即用”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b24c2719b3961ec5266e180178e1c8af.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么推荐使用轻量应用服务器运行 Moltbot？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;稳定在线：可用性SLA ≥99.95%，避免本地设备受断电、休眠等因素影响导致离线安全可控：Moltbot的记忆、配置、操作都控制在专属云服务器中，相比本地设备有更好的隔离性快速上手：预置Moltbot及其运行环境，直连百炼平台，提供钉钉、iMessage等消息通道最佳实践普惠算力：新用户低至&amp;nbsp;68 元/年起，模型能力按Token使用量付费，可根据应用场景灵活调整云服务器配置和模型&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你正想试试 AI助理的实际能力，现在就是最好的时机。整个过程只需&amp;nbsp;2 步，按照下面的步骤，5分钟搞定：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Moltbot部署教程如下👇&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第一步：打开轻量应用服务器并安装Moltbot镜像&lt;/p&gt;&lt;p&gt;打开轻量应用服务器，点击「应用镜像」，选择「Moltbot」&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d6/d60f165ce8e60291ad42081b6c89b3ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;// 第二步：配置Moltbot&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1. 前往百炼大模型控制台，找到密钥管理，单击创建API-Key&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0ba51c96b6b1e3dfe905125f3f1d5a32.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2. 前往轻量应用服务器控制台，找到安装好Moltbot的实例，进入 「应用详情」端口放通、配置Moltbot、访问控制页面&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6cc24d18a65c3403676eac6fee46b748.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1）端口放通：防火墙一键放行应用端口18789&lt;/p&gt;&lt;p&gt;2）配置Moltbot：点击执行命令配置API&lt;/p&gt;&lt;p&gt;2）配置百炼API Key，单击一键配置，输入百炼的API-Key。单击执行命令，写入API Key。&lt;/p&gt;&lt;p&gt;c.配置Moltbot：单击执行命令，生成访问Moltbot的Token。&lt;/p&gt;&lt;p&gt;d.访问控制页面：单击打开网站页面可进入Moltbot对话页面。&lt;/p&gt;&lt;p&gt;具体操作指南文档：&lt;a href=&quot;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&quot;&gt;https://help.aliyun.com/zh/simple-application-server/use-cases/quickly-deploy-and-use-moltbot&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【阿里云轻量应用服务器】是专为中小企业及开发者设计的云服务器产品，预装Moltbot、Dify、宝塔等热门应用软件，以预付费的方式售卖计算、存储、网络套餐，隐藏VPC、弹性网卡等暂时不需要的特性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自2025年以来轻量应用服务器带来全新产品序列，通用型低至每月28元，最小规格2vCPU 0.5GiB内存起步，适合网站、开发测试等场景，是多数客户共同选择的经典产品；CPU优化型低至每月200元，CPU算力独享、最大16vCPU。适合游戏服务器、企业应用与数据库等场景，是企业客户的首选；除此之外，包含多公网IP型、国际型、容量型在内的5款新品还标配200Mbps峰值公网带宽。选择轻量应用服务器，为中小企业及开发者创新提速！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阅读原文（跳转活动页面：&lt;a href=&quot;https://www.aliyun.com/activity/ecs/clawdbot&quot;&gt;https://www.aliyun.com/activity/ecs/clawdbot&lt;/a&gt;&quot;）&lt;/p&gt;</description><link>https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</link><guid isPermaLink="false">https://www.infoq.cn/article/Nx03AAwazUY6NAWu9N3H</guid><pubDate>Wed, 28 Jan 2026 10:39:01 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>AI 工程化</category></item><item><title>GPT-5.2破解数论猜想获陶哲轩认证！OpenAI副总裁曝大动作：正改模型核心设计，吊打90%研究生但难出颠覆性发现</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;凌晨，OpenAI发布了新一代AI科研利器Prism，该平台由GPT-5.2加持，供科学家们撰写和协作研究，即日起向所有拥有 ChatGPT 个人账户的用户免费开放。用华人AI创业者Yuchen Jin的话说，“每篇论文都将把ChatGPT列为合著者。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在昨日，OpenAI副总裁、新成立的OpenAI for Science团队负责人 Kevin Weil 就在 X 上发文预热道，“我们的目标是赋予每位科学家 AI 超能力，让他们能做更多事情，让世界在2030年就能开展2050年的科学研究。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa24d4f05e1cc4c7efcdb3b9ad5b1975.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自ChatGPT爆红面世后的三年里，OpenAI的技术颠覆了日常生活中方方面面的行为模式。如今OpenAI正明确发力科研领域，面向科研人员布局。10月，该公司宣布成立全新的OpenAI for Science团队，核心致力于探索其大语言模型（LLM）助力科研人员的路径，并优化旗下工具为科研人员提供支持。过去数月，社交媒体上涌现出大量相关内容，学术期刊也刊发了诸多研究成果，数学家、物理学家、生物学家等领域研究者纷纷撰文，讲述大语言模型、尤其是GPT-5如何助力他们取得新发现或是为他们指引方向，让他们找到原本可能错失的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，OpenAI为何选择此时入局？此番布局，究竟想要达成怎样的目标？发力科研领域，与该公司更宏大的使命如何契合？在这一领域，OpenAI已然姗姗来迟。谷歌 DeepMind早在数年前便已成立AI-for-science团队，打造了AlphaFold、AlphaEvolve等具有开创性的科学模型。2023年，谷歌 DeepMind的CEO兼联合创始人Demis Hassabis曾就该团队的情况在采访中表示，“这是我创立DeepMind的初衷。事实上，这也是我整个职业生涯深耕AI领域的原因。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，Kevin Weil在一次访谈中不仅正面回应了这些问题，还对当前模型的实际能力给出了比先前更为保守的评价：目前模型还达不到取得颠覆性新发现的水平，但倘若能让人不必把时间浪费在已经解决的问题上，也是对科研的一种加速。有意思的是，据其透露，一位OpenAI主动接触且开通了GPT-5付费服务的科研人员反馈，GPT-5会犯一些低级错误，比人犯的错误更加愚蠢，不过一直在进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，按照OpenAI 在 AI 科研领域的布局，接下来其将对模型整体设计作两大思路优化：一是让 GPT-5 在给出答案时降低置信度，具有认知层面上的谦逊性；另一方向，是利用GPT-5反向对自身输出进行事实核查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026年对于科研领域的意义，将堪比2025年之于软件工程。”Weil表示，“2025年初，若有人借助AI完成大部分代码编写，还只是早期尝鲜者；而12个月后的现在，若还未用AI编写大部分代码，就可能已经落后。现在，科研领域正显现出与编程领域类似的早期发展势头。一年后，倘若一名科研人员还未深度运用AI开展研究，就将错失提升思考质量、加快研究进度的机会。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;模型能力早已超过90%研究生，AGI 最大价值在于推动科学进步&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数年前，Weil 加入 OpenAI 出任首席产品官，他曾担任 Twitter 和 Instagram 的产品负责人官。但他的职业起点是科研领域：在斯坦福大学攻读粒子物理博士学位期间，他完成了三分之二的学业，随后为追寻硅谷梦离开学术界。Weil 也乐于提及自己的这段学术背景，他说：“我曾以为自己余生都会做一名物理教授，现在度假时还会读数学相关的书籍。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及 OpenAI for Science 与公司现有的白领生产力工具、爆火的视频应用 Sora 如何契合时，Weil 脱口而出：“OpenAI 的使命是研发通用人工智能（AGI），并让这项技术为全人类带来福祉。”他表示，不妨想象这项技术未来能为科研领域带来的变革：全新的药物、材料、器械。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“试想一下，它能帮助我们探索现实的本质，攻克悬而未决的科学难题。或许 AGI 能为人类创造的最重大、最积极的价值，正是其推动科学进步的能力。”他补充道：“GPT-5 的出现，让我们看到了这种可能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Weil 看来，如今的大语言模型已足够优秀，能成为科研人员的得力协作伙伴。它们能提出各种想法，建议新的研究方向，并在新问题和几十年前发表在冷门期刊或外语期刊上的旧解决方案之间找到富有成效的联系。但在大约一年前，情况并非如此。自2024年12月发布首个推理模型（一种能够将问题分解成多个步骤并逐一解决的逻辑学习模型）以来，OpenAI一直在不断拓展这项技术的边界。推理模型的问世，让大语言模型解决数学和逻辑问题的能力得到大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“放在几年前，模型能在 SAT 考试中拿到 800 分，就足以让我们所有人惊叹不已。”Weil 称。而如今，大语言模型能在数学竞赛中夺冠，解出研究生阶段的物理难题。去年，OpenAI 和 谷歌 DeepMind 均宣布，其研发的大语言模型在国际数学奥林匹克竞赛中取得金牌级成绩，该赛事是全球难度最高的数学竞赛之一。Weil 表示，“这些模型的能力，早已不只是超过 90% 的研究生，而是真正达到了人类能力的极限。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一论断非常大胆，却也并非无懈可击。但毋庸置疑的是，搭载了推理模型的 GPT-5，在解决复杂问题方面较 GPT-4 有了质的飞跃。行业基准测试 GPQA 包含 400 多道选择题，专门考察生物、物理、化学领域的博士级专业知识，GPT-4 在该测试中的正确率仅为 39%，远低于人类专家约 70% 的基准线；而据 OpenAI 数据，2024 年 12 月推出的 GPT-5 最新版本 GPT-5.2，正确率达到了 92%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;读遍30年来的论文，模型也做不出颠覆性新发现&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Weil 的这种兴奋之情显而易见，却或许有些过头了。去年10月，Weil等OpenAI高管曾在X平台高调宣称，GPT-5已为多个数学未解难题找到解决方案。但数学家们很快指出，GPT-5实际只是从早期研究论文中挖掘出了已有的答案，其中至少还有一篇德文文献。这样的能力虽有价值，却绝非OpenAI宣称的那般突破性成就。事后，Weil与其同事删除了相关帖子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时，这件事闹出了不小的风波。刚开始疯传的是：GPT-5 解决了 10 个此前未解决的埃尔德什问题（Erdős problems），并在另外 11 个问题上取得了进展，而之后被负责维护埃尔德什问题网站的数学家 Thomas Bloom&amp;nbsp;澄清为；GPT-5 只是找到了一些能解决这些问题的参考文献。DeepMind 首席执行官Demis Hassabis对此指出，该团队的沟通方式“过于草率”。前Meta 首席 AI 科学家Yann LeCun则讽刺道， OpenAI“被自己的炒作所反噬”（hoisted by their own GPTards），“搬起自己的 GPT 石头砸了自己的脚”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但就在前几天，又有消息称，GPT-5.2 Pro破解了一道埃尔德什猜想，题目是埃尔德什问题库中的第281号。这次证明由数学家Neel Somani 推动，且论证过程由菲尔茨奖得主陶哲轩证明没有问题，并评价其是“AI 解决开放性数学问题中“最明确的案例之一”。目前，GPT-5.2Pro对该问题的证明结果已被埃尔德什问题网站收录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，GPT-5.2Pro对这个问题提出了新的证明方法，虽然忽略了此前已有的相关证明，但陶哲轩指出GPT-5.2Pro的证明思路与之前的方法“相当不同”，只在概念上有些重叠。现在这道题有了两条论证思路，一是GPT-5.2 Pro采用的遍历理论框架，策略是“弗斯滕伯格对应原理”的变体；二是两个早在1936年和1966年就已经存在的定理组合：达文波特-埃尔多斯定理和罗杰斯定理，且解法更简单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如今的Weil也更加谨慎了。他表示，能找到那些已存在却被遗忘的答案，本身就已意义重大：“我们都站在巨人的肩膀上前行，倘若大语言模型能整合这些知识，让我们不必把时间浪费在已经解决的问题上，这本身就是对科研的一种加速。”他也淡化了大语言模型即将取得颠覆性新发现的说法：“我认为目前模型还达不到那个水平，未来或许能做到，我对此持乐观态度。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他强调这并非团队的核心使命：“我们的使命是加速科学发展，而加速科学发展的标准，并非一定要像爱因斯坦那样对整个领域进行彻底的重新构想。”在Weil看来，核心问题只有一个：科学发展速度是否真的更快了？“当科研人员与模型协作时，能比独自研究完成更多工作、效率也更高。我认为我们已经看到了这一点。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年11月，OpenAI发布了一系列由公司内外科研人员提供的案例研究，以真实案例展现了GPT-5的实际应用及助力科研的过程。Weil表示，“这些案例的研究者，大多早已在研究中直接使用GPT-5，他们通过各种方式找到我们，告诉我们‘看看这些工具能让我做到什么’。”GPT-5 擅长的关键事情是：找到科研人员尚未意识到的现有研究成果及关联线索，这有时能催生新的思路；协助科研人员草拟数学证明过程；为科研人员在实验室验证假说提供实验思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“GPT 5.2 几乎阅读了过去 30 年发表的每一篇论文。它不仅理解科学家所处领域的内容，还能从其他不相关的领域中提炼出可类比的思路。”Weil称，“这太强大了。你总能在相关领域找到人类合作者，但要在所有可能相关的上千个相关领域找到上千个合作者，那就难上加难了。除此之外，我还能在深夜与模型一起工作，它从不需要休息，也能同时向它提出十个问题，这些事若是对人做，难免会显得尴尬。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;GPT-5犯错比人更愚蠢，机器人更愿意听它的指挥？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据悉，OpenAI为佐证Weil的观点，接触了多位科研人员，其中绝大多数都对此表示认同。范德堡大学物理与天文学教授Robert Scherrer此前仅将ChatGPT当作消遣工具把玩，他告诉我：“我曾让它以《贝奥武夫》的文风改写《吉利根岛》的主题曲，它完成得非常出色。”直到同在范德堡大学的同事、如今任职于OpenAI的物理学家Alex Lupsasca告诉他，GPT-5帮其解决了一个研究中的难题，他才改变了对这款模型的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lupsasca为Scherrer开通了GPT-5 Pro，这是OpenAI每月200美元的高级订阅服务。Scherrer说，“我和我的研究生为一个问题钻研了数月都毫无头绪，GPT-5却成功解决了它。”但他也坦言，这款模型并非完美：“GPT-5还是会犯一些低级错误。当然，我自己也会出错，但GPT-5犯的错误更愚蠢。”不过他表示，其进步速度有目共睹，“如果当前的发展趋势能持续下去，我想很快所有科研人员都会用上大语言模型。当然，这只是个假设。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;非营利性研究机构杰克逊实验室的生物学教授Derya Unutmaz，在其免疫系统相关研究中，会借助GPT-5进行头脑风暴、论文总结和实验规划。在他向OpenAI分享的案例研究中，其团队曾分析过一组旧数据集，而GPT-5对这组数据的分析，得出了全新的见解和解读。他说：“大语言模型对科学家来说已经至关重要了。以前需要几个月才能完成的数据集分析，现在用大语言模型就能完成了，不用大语言模型已经行不通了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加州大学伯克利分校的统计学家Nikita Zhivotovskiy表示，从ChatGPT首个版本发布开始，他就在研究中使用大语言模型了。和Scherrer一样，他认为大语言模型最有用的地方在于，能挖掘出其研究工作与一些未知现有研究成果之间的意外关联。“我相信大语言模型正在成为科学家们必不可少的技术工具，就像曾经的计算机和互联网一样。那些拒绝使用这类工具的人，将会长期处于劣势。”但他并不指望大语言模型能在短期内取得什么新发现，“我几乎没见过模型能提出真正值得单独发表的全新观点或论证。到目前为止，它们似乎主要是在整合现有的研究成果，有时还会出错，而非创造真正的全新研究方法。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有与OpenAI无任何关联的科研人员，态度则没那么乐观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;利物浦大学化学教授、勒沃休姆功能材料设计研究中心主任Andy Cooper表示，“到目前为止，我们尚未发现大语言模型从根本上改变了科学研究的方式，但我们近期的研究结果表明，这类工具确实有其用武之地。Cooper正牵头研发一款所谓的AI scientist，该系统能实现部分科研工作流程的完全自动化。他表示，其团队并不会借助大语言模型构思研究思路，但这项技术已开始在更庞大的自动化系统中显现实用价值，比如大语言模型可协助操控机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我猜测，大语言模型或许会更多应用于机器人工作流程，至少在初期会是如此。因为我不确定人们是否愿意听从大语言模型的指挥，我自己当然是不愿意的。”Cooper称。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;团队重点发力：让 GPT 少点自信、更加谦逊&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大语言模型的实用性或许与日俱增，但保持谨慎仍是关键。去年12月，研究量子力学的科学家Jonathan Oppenheim指出，某本科学期刊中出现了一处由大语言模型导致的错误。他在X平台发文称，“OpenAI的管理层正在推广《Physics Letters B》上的一篇论文，其中的核心思路由GPT-5提出，这或许是首篇由大语言模型贡献核心观点且通过同行评审的论文。但有个小问题：GPT-5提出的思路，验证的对象完全错了。研究人员让GPT-5设计一个能检测非线性理论的验证实验，它却给出了一个检测非定域性理论的方案。二者看似相关，实则截然不同。这就好比你想要一个新冠检测试剂盒，大语言模型却兴冲冲地递给你一个水痘检测试剂盒。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;显然，许多科研人员正以富有创意、贴合实际的方式运用大语言模型。但同样显而易见的是，这项技术所犯的错误可能极为隐蔽，甚至连专家都难以察觉。这一问题的成因，部分源于ChatGPT的交互特性，它总能以迎合的语气让使用者放松警惕。正如Jonathan Oppenheim所言，“核心问题在于，大语言模型的训练目标是迎合用户，而科学研究需要的是能够挑战我们的的工具。”曾有一个极端案例，一名非科研领域的普通人被ChatGPT误导，长达数月都坚信自己发明了一个新的数学分支。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Weil也深知大语言模型的幻觉问题，但他强调，新一代模型产生幻觉的概率已大幅降低。即便如此，他认为，仅仅关注幻觉可能就偏离了重点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我的一位同事曾是数学教授，他说过的一番话让我印象深刻：‘我做研究时，和同事交流碰撞想法，自己的观点90%都是错的，但这正是意义所在。我们都在大胆畅想思路，只为找到一条可行的研究路径。’”Weil表示，“这其实是科研中最理想的状态。当你提出足够多的错误观点，有人偶然发现了一丝真理，另一人抓住这一点继续探讨：‘你说的这点并不完全正确，但如果我们换个思路’。就这样，人们便能在科研迷雾中逐渐摸索出前行的道路。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是Weil为OpenAI for Science设定的核心愿景。他认为，GPT-5固然优秀，但它并非万能灵药。这项技术的价值在于引导人们探索新的方向，而非提供最终答案。事实上，OpenAI目前正着手优化GPT-5的一项特性：让它在给出答案时降低其置信度。它不会再直接说“答案在这里”，而是会以更委婉的方式告诉科研人员：“以下思路可供参考。”“这正是我们目前投入大量精力在做的事：努力让模型具备某种认知层面的谦逊性。”Weil称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据透露，OpenAI正在探索的另一方向，是利用GPT-5对自身输出进行事实核查。实际应用中常有这样的情况：如果你把 GPT-5 的某个答案重新输入到模型中，它会逐条分析并指出其中的错误。Weil表示，“我们可以让模型充当自身的校验者。如此便能搭建一套工作流程：模型先完成初步推理，再将结果交由另一模型审核；如果这个模型发现了可以改进的地方，就会把结果反馈给原模型，并提示‘注意，这部分内容有误，但这部分思路有价值，可保留’。这就像两个智能体协同工作，只有当输出内容通过校验者的审核后，才会最终呈现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一机制，与谷歌 DeepMind为AlphaEvolve打造的模式高度相似。AlphaEvolve是一款工具，它将大语言模型Gemini封装在一个更大的系统中，该系统能够筛选出优质回复，并将其反馈给模型进行改进。谷歌 DeepMind已借助AlphaEvolve解决了多个现实中的科研难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如今，OpenAI面临着竞争对手的激烈角逐，这些企业的大语言模型即便无法实现OpenAI为其模型宣称的全部功能，也能完成绝大部分。倘若如此，科研人员为何要选择GPT-5，而非同样在逐年迭代升级的Gemini或Anthropic旗下的Claude系列模型？归根结底，OpenAI for Science的布局，很大程度上也是为了在这一新领域抢占先机。而真正的技术创新，尚未到来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&quot;&gt;https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/zh-Hans-CN/prism/&quot;&gt;https://openai.com/zh-Hans-CN/prism/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</link><guid isPermaLink="false">https://www.infoq.cn/article/i28k7YAzhOCUETypChCa</guid><pubDate>Wed, 28 Jan 2026 10:09:50 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>利用 ADBC 实现更快的数据传输：一次关于数据通路的系统性重构</title><description>&lt;p&gt;数十年来，访问数据库的标准方式始终是 ODBC 和 JDBC。然而，在这些传统的面向行的连接标准，可能会成为高性能 Snowflake 客户端应用程序的瓶颈。在 Snowflake 某些要求最为严苛客户的延迟敏感型应用中，包括关键业务运营和 AI 用例，ODBC 和 JDBC 的速度实在过于缓慢。这正是 Snowflake 选择拥抱开源生态 Apache Arrow 与新一代 ADBC 连接标准的核心动因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b0fa5268995ff98a5cb131c760dc56cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然 Snowflake 长期使用 Apache Arrow 列式格式来加速网络传输，但采用 ADBC 能使 Snowflake 客户消除客户端序列化和反序列化的开销，从而为大型结果集带来巨大的性能提升。在实践中，我们观察到使用 ADBC 相比 ODBC/JDBC 可实现 2 倍至 5 倍甚至 10 倍或更高的加速效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Build 2025 大会上，Apache Arrow PMC 成员、Columnar 联合创始人、Iceberg 项目提交者&amp;nbsp;Matt Topol&amp;nbsp;带来了一场高度工程化、干货满满的技术分享。他展示了使用多种语言（C、Go、Python、R）向 Snowflake 发起简单查询，包括使用数据框架甚至 DuckDB 等其他系统作为源，执行高效数据摄取到 Snowflake 的过程。重点将是如何轻松将 ADBC 集成到对毫秒级响应要求苛刻的应用中，以及如何利用 Snowflake 对 Apache Arrow 和 ADBC 的支持，为最关键的性能用例加速应用程序的速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从内存布局谈起：为什么 Apache Arrow 是关键前提&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 在分享一开始，并没有直接进入 ADBC，而是先用相当篇幅重新校准听众对 Apache Arrow 的理解。Arrow 并不是一个库或产品，而是一套列式、内存级的数据格式规范，其核心特征在于：内存中的数据布局，与网络传输时的字节布局完全一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一设计带来的直接结果是，数据在系统之间流转时，可以绕过传统序列化与反序列化过程，直接传递原始字节。在同一进程内，甚至可以做到零拷贝或共享内存。这不是优化细节，而是从根本上改变了数据移动的成本结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，Arrow 采用列式内存布局，使其天然适合向量化计算、聚合操作以及分析型工作负载。Topol 用“行式 vs 列式”的对比说明了一个事实：在分析场景下，行导向的内存访问意味着更多 I/O、更差的缓存命中率，以及无法充分利用 SIMD 等编译器优化；而列式内存恰恰相反，它与现代 CPU 架构是协同演进的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c77390762a9abf950dc9e5f752088082.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ODBC / JDBC 的结构性矛盾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Topol 将问题指向了当前最主流的数据库连接方式——ODBC 与 JDBC。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它们的价值毋庸置疑：API 稳定、生态成熟、适用于事务型与逐行计算场景，并且在过去几十年中几乎成为数据库访问的事实标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题在于，这套接口体系本质上是行导向的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现实是，Snowflake、DuckDB、ClickHouse、BigQuery 等主流分析型数据库，内部早已全面列式化。这意味着，每一次通过 ODBC / JDBC 拉取数据，系统都要经历一次高成本的转置：从列式内存转换为行，再在下游分析中重新转回列式结构。这不仅带来了显著的 CPU 与内存开销，也让数据在系统中反复“变形”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 特别强调，这里的转置并不是抽象意义上的重排，而是真实的数据拷贝与类型转换。在数据规模扩大后，这种成本会呈指数级放大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;ADBC：把统一 API的理念带入列式世界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADBC（Arrow Database Connectivity）正是为解决这一结构性矛盾而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从抽象层面看，ADBC 与 ODBC / JDBC 非常相似：应用程序面对的是统一 API，通过不同驱动与不同数据库交互。但关键差异在于，ADBC 是列导向的，其数据交换格式直接采用 Apache Arrow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当数据库本身已经以列式形式返回结果，且能够直接输出 Arrow 数据时，驱动几乎无需做任何转换，便可将结果原样交付给应用侧——零拷贝、无转置。这不仅显著提升了性能，也让数据在更早阶段就处于可分析、可计算的理想形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便在数据库本身并非列式、或并未原生支持 Arrow 的情况下，ADBC 也允许在驱动层完成一次性转换，从而让应用侧始终面对统一的数据模型，而不必管理多套复杂连接器体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用数据说话：跨语言的性能对比与真实收益&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享的核心说服力，来自大量现场演示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Python 示例中，Topol 对比了通过 ODBC 与 ADBC 从 Snowflake 拉取数据的耗时。即便在启用缓存、排除查询执行成本的情况下，ADBC 在 10 万行与 100 万行数据规模下，仍然表现出明显优势：数据量越大，性能差距越明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，ADBC 返回的数据可以直接被 Polars 等基于 Arrow 的 DataFrame 库消费，几乎没有额外转换成本。这意味着，性能提升并不仅体现在拉数据更快，而是贯穿整个分析链路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样的结论，在 Go 和 R 的演示中得到了重复验证。跨语言的一致性，反过来也印证了 Arrow 与 ADBC 设计上的语言无关性——它们优化的是数据形态本身，而非某一语言生态的实现细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不止查询：流式摄取与系统间数据流动的新可能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享的后半段，Topol 将视角从查询结果返回扩展到更复杂的数据流动场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他展示了如何通过 ADBC，将 Snowflake 中的一百万行数据，以流式方式直接摄取到内存中的 DuckDB。整个过程无需先完整加载结果集，数据以 Arrow Record Batch 的形式持续流动，类型信息在传输过程中被完整保留，整体耗时不到四秒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一演示揭示了 ADBC 的另一层意义：它不仅是一种更快的查询接口，也是一种系统间高效、可组合的数据通道。当数据能够以统一、零拷贝的列式格式在系统间流动时，ETL、数据同步乃至多引擎协同分析的复杂度，都有机会被重新定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Topol 并没有在结尾试图宣告 ODBC / JDBC 的终结。相反，他反复强调，这些技术在事务型与行式计算场景中仍然合理且必要。但对于分析型系统而言，ADBC 所代表的，是一种更贴合现代数据架构的方向：让数据尽可能早地进入列式、分析友好的形态，并尽可能少地在系统间反复转换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</link><guid isPermaLink="false">https://www.infoq.cn/article/jUrPE4oFukpk4QlMflL3</guid><pubDate>Wed, 28 Jan 2026 09:15:00 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>内存一年疯涨170%，云账单里的“隐性成本”该算清了</title><description>&lt;p&gt;2025 年下半年，存储价格又一次成为行业聚焦点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多家市场机构统计显示，2025 年三季度跟四季度，DRAM 和 NAND 价格一路攀升。根据 Tom&#39;s Hardware 披露的数据，2025 年 DRAM 合同价同比上涨幅度高达 171.8%，创下历史新高。此轮上涨跟 AI 数据中心建设拓展、服务器需求集中释放紧密相联，还直接引发企业 IT 基础设施采购成本上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于依赖自建数据中心或中小 IDC 的企业来说，这种变化带来的冲击尤为剧烈。硬件采购从一次性预算问题，演变为难以预测的长期成本风险。服务器、SSD 和内存条的价格不再稳定，交付周期也更不确定。企业在扩容时不得不承担高价买入、供货延迟的双重压力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，将硬件采购压力转化为按需付费的运营支出，把价格波动风险转移给云服务商，正在成为越来越多企业的选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但问题并未因此结束。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着业务迁移到云端，企业发现云账单中存储与内存的占比仍在持续上升，即便算力配置并未明显升级，总体成本依旧水涨船高。部分团队开始反思：问题是否仅和数据量增多有关，还是资源使用方式本身就存在不合理的地方？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，多数云实例依旧按固定的 CPU 与内存配比来交付，诸如 2 核 4GB、4 核 8GB 的规格。早期，这种设计可简化资源管理，推动了云计算普及，但如今业务形态有所改变，企业系统一般得同时支撑多样业务，各业务对于算力、内存的消耗不一样，固定规格愈发难以契合实际需求。这导致企业要么部分资源长期闲置，要么不得不面对业务在高峰阶段出现性能瓶颈的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当内存价格进入上行周期，这种规格错配带来的浪费被进一步放大：闲置的不再只是资源本身，而是越来越昂贵的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是基于这样的背景，云基础设施走到新的路径分岔口：是继续就资源本身实施配置，还是转变方向围绕应用需求设计算力供给方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期面向中国区合作伙伴召开的发布会上，华为云对 Flexus 云服务器系列规格及性能进行更新，并且展示了其在各种业务负载下的运行表现。该实例基于华为云首创的柔性算力技术，打破 CPU 与内存的固定绑定关系，使企业能够按真实业务需求配置资源，从源头减少内存浪费，并结合智能调度与应用级加速改善长期运行稳定性与算力资源投入产出比。本文将从行业环境变化与技术实现等层面，剖析这种模式背后的思路，以及它所代表的云服务器演进方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云服务器，开始不太“合身”了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云服务器长期采用固定 CPU 与内存的配比，是工程上的一种取舍考量。早期云平台首先得解决的是规模化交付和稳定调度的问题，采用固定规格利于资源池管理，同样便于容量规划及计费设计。当业务形态呈现相对单一阶段，这样的方式尚可接纳。但究其本质它是从平台管理成本角度设计的，并非从业务负载的角度出发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今业务已不再是单一模式，电商、内容分发、数据库、缓存、AI 推理在一套系统中同步协同运行，对 CPU 以及内存的需求差别明显，固定规格无法精准对应实际负载，企业只能采用超出实际所需的实例型号。云服务器规格跟应用需求普遍不匹配，用户往往被迫去为用不到的算力和内存付费，引发大量资源的闲置浪费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2a/2afb7d843ca99668267a105cec3e77e2.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源浪费只不过是表象罢了，更深层的问题体现为性能优化的复杂度。现实的业务部署不仅涉及操作系统选定，还包含网络参数、系统参数以及应用配置参数。数量往往达到数千级别，缺少专家经验积累，难以达成稳定的最优配置。单是内核跟应用层的参数组合，就已超出普通团队可控范围，调优所用的周期漫长，效果也难以把控。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/60/60e1fc3271a533f924d017f8da492c35.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从较长的时间阶段看，云服务器本身一直在不断演变，最初的资源虚拟化阶段，是把物理服务器标准化成可租借的实例；紧接着进入弹性规模阶段，采取自动伸缩的方式去应对流量变化，这两个阶段处理的是存不存在以及是否充足的问题，当下已经迈入第三阶段，关注焦点转向使用是否高效。过去，固定实例曾是工程优势，如今却愈发像是一件穿着不合身的衣服。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;柔性算力：从“卖规格”到“卖能力”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;怎样让资源本身更贴近应用？在 Flexus 云服务器 X 实例产品的设计里，华为云引入了柔性算力这一概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Flexus X 实例里，柔性算力首先体现在规格形态的调整变化上。传统实例一般仅仅可在少量固定比例中选择 CPU 跟内存配置，而该实例支持按业务需求实施更精细的组合配置。发布会现场提到，所有 X 实例均支持多种非常规的 CPU/ 内存配比，包括 3:1、2:5、3:7 等组合。这可减少由规格不一致引起的资源闲置，让用户更接近按实际负载付费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而规格数量增加，并非表示问题自动就解决了，其关键是系统如何判断哪种配置更合适。传统调度大多依据节点上剩余的 CPU 与内存。新方式需要领会业务负载本身，涵盖资源使用结构，以及随时间的变化趋势。Flexus X 实例本质上不再是调度 CPU，而是实际的业务场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就工程实现而言，这种转变依赖底层架构的支撑，Flexus X 实例借助华为云自研的擎天 QingTian 架构和瑶光云脑调度系统得以实现，经由计算、存储和网络资源的解耦操作，提高了资源组合的自由度，也增强了非标准规格运行状态下的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，柔性算力还意味着配置不再是一次性决定，实例运行时会一直对资源使用状况进行评估，系统会判断当前配置跟负载是否相符，进而给出调整建议，而且还支持算力规格热升降的独家能力。从这个层面看，Flexus X 实例的转变不只是规格数量增多，它更像是把算力从提前打包好的商品，变成可持续优化的能力，实现“应用驱动算力”的最优体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;关键应用加速：算力之外的第二条性能曲线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Flexus X 实例不单单改变了资源形态，还进一步深入应用执行层，解决了算力配置合理系统却依旧不稳定的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次规格升级，华为云为数据库以及中间件类的负载引入专属应用级加速机制。Flexus X 实例针对 PostgreSQL、Memcached、MySQL、Redis、Nginx 提供了独立的一键加速能力，由 X-Turbo 应用加速引擎统一驱动。此类优化不会对用户的使用途径做出改变，实例创建结束之后即可启用，平台会把调优工作完成，用户无需插手复杂参数的配置。发布会现场，华为云对该能力实测演示，在 PostgreSQL 的使用场景下，Flexus X 实例的吞吐量达到 2.1 万 + TPS，大概为同规格业界旗舰型实例的 3.4 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/810527295eccdf4bca355ee9ac46c7ba.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就数据库这类系统而言，峰值性能仅仅属于一方面，更为关键的是高负载持续状态下的稳定输出能力。业务系统更易受诸如延迟抖动、连接堆积等问题的干扰，而不是单次压测形成的成绩。X-Turbo 的设计目标之一正是实现性能优化长期运行状态下的吞吐与响应稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;跟应用级优化同步进行的是，实例规模的进一步扩展。新一代 Flexus X2e 实例的 x86 规格从原本的 32U128G 提升至 64U256G，多核算力提升了约 30%；新增 Flexus KX1 鲲鹏实例，最高可达 80U320G，以覆盖大数据处理、内存数据库这类资源密集型场景。这意味着应用加速机制不再受中小规格环境约束，能在规模更大的资源池里发挥作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d2f568a839cc6be536e32d0c7def3623.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一系列的变化显示出云服务器性能边界正在转移。过去，性能更多由 CPU 规格和内存容量决定。而如今，应用执行路径、参数组合的方法及调度策略成为同等要紧的变量，在固定规格的时代里，这些优化由用户自己承担，而于 Flexus X 实例中，它们被纳入到算力交付范畴，正是从这一意义出发，云服务器竞争不再只是资源规模大小的比拼，而是发展为聚焦运行效率的系统工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从工程能力到真实落地：柔性算力如何进入生产系统&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一项新的算力供给方式，能否切实进入生产系统，首要取决于它是否具备充足的稳定性与可用性。Flexus X 实例可靠性设计向华为云旗舰级云服务器标准看齐，实现单 AZ 99.975% 的可用水平，还有跨 AZ 99.995% 的可用性。这暗示柔性算力没有以牺牲稳定性为交换代价，而是可直接承受核心业务负载的基础设施形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了稳定性这一点，规模化使用还取决于运维体系自身是否具有确定性，Flexus X 实例在华为云既有的 SRE 运维体系框架内运行，强调借助标准化变更、容量预测与故障演练减少系统行为的不确定性，实现大规模实例并发运行的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业落地的实际来看，柔性算力最先进入的并非那种单一业务场景，而是负载结构繁杂、资源使用波动大的系统类型。其已经在医疗电商平台迁移、连锁零售系统、医药行业信息化平台、游戏服务器迁移等场景大规模部署，用以承载数据库、中间件及核心交易服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;中软国际智能集团云业务部副总经理王春玉在发布会上分享，团队为某大型生物医药集团搭建系统的时候，引入 Flexus X 实例作为数据库及业务服务的主要承载环境，在原有系统架构未改变的情形下完成迁移，而且在性能满足要求的前提下，达成约 30% 的综合成本下降。王春玉还谈到，其团队服务的一家专业酒水直营连锁品牌，把部分核心业务迁移到 Flexus X 实例而后，通过规格按需匹配与资源利用率优化，实现整体云资源成本约 15% 的下降。这些亮眼的结果主要源于两方面：一是实例规格跟业务负载的匹配度有所提升，降低了长期闲置资源的数量；二是借助应用级加速与调度优化，降低了单位业务量所需的算力规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这些真实的实际部署案例能看出，Flexus X 实例的用户一般有几个共同特性：业务负载呈现明显波动，系统结构相对复杂，然而运维及架构团队的规模较为有限，同时对长期云资源的成本敏感度较高。Flexus X 实例在未对业务形态本身作出改变的情况下，却降低了基础设施对业务扩展所施加的约束强度，让按照业务形态去配置算力成为可践行的工程实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，未来企业买的不再是服务器，而是业务效率。Flexus X 实例凸显了云服务器设计思路的一次转向：由“卖规格”过渡到“交付能力”，从“静态资源”过渡到“智能算力”，在 AI 成为主流计算负载的未来，此种转变大概率不会再是差异化优势，而是云基础设施的必要门槛。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</link><guid isPermaLink="false">https://www.infoq.cn/article/pvHVtYNYmM7EDEpIgJQE</guid><pubDate>Wed, 28 Jan 2026 08:06:57 GMT</pubDate><author>棱镜</author><category>华为</category><category>云计算</category></item><item><title>数据土壤，决胜 AI 下半场：一场关于企业 Data+AI 战略的炉边思辨</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据与 AI 的变革正以前所未有的速度重塑产业格局，2026 年年初，Snowflake 与 InfoQ 联合呈现的“Make it Snow”2025-2026 Data+AI 年度时刻，汇聚了来自医疗、制造、汽车等领域的顶尖专家，共同探讨数据智能的前沿突破与未来方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场以“炉边对话”为形式的深度交流，不仅回顾了 2025 年 Data+AI 领域的认知重构，更围绕 2026 年十大战略命题展开思辨，为行业奉上了一场兼具思想深度与实践价值的智慧盛宴。与此同时，各位专家还分别留下了对 2026 年的一个技术预言，&lt;a href=&quot;https://www.infoq.cn/video/vy4ZYjr71ohKEOGp9tXz&quot;&gt;点击此处&lt;/a&gt;&quot;可快速了解这些极具前瞻性的洞见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文力求完整呈现这场思想碰撞的核心洞察，见证数据与 AI 如何从技术概念转化为驱动产业革新的核心力量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2025 年带给你的三个认知突破&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q：回看 2025 这一年，Data + AI 的很多变化，往往是在一次次具体实践中慢慢显现出来的。可能是一次惊艳的产品体验、一次真实落地的尝试，也可能是一个业务场景，或者一段走弯路之后的重新理解。正是在这些时刻里，我们对 Data + AI 的判断发生了变化。请每位嘉宾回顾这一年，有没有哪几个真正的 Aha Moment（顿悟时刻），让你感到茅塞顿开，认知被重构了，如果让你选 3 个这样的关键时刻或经历，它们分别是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬（Snowflake 亚太及日本地区解决方案工程副总裁）：2025 年大模型演进呈现出&quot;疯狂超车&quot;的态势，从年初 DeepSeek 将推理成本降至十分之一，到年中 Claude 展现资深工程师级编程能力，再到 Gemini3 在科学推理领域的突破，最终以 ChatGPT 5.2 模型实现多模态无缝切换，这些迭代揭示了一个核心认知：技术选型的关键不在于追逐当下最优，而应基于特定应用场景、预算限制及部署规划进行理性决策。&amp;nbsp;与此同时，AI 安全风险愈发严峻，如 2025 年 6 月发现的“隐形提示词注入”漏洞，攻击者可利用邮件中肉眼不可见的指令诱导 AI 自动读取并外泄网盘内的敏感信息。这充分说明，尽管 AI 功能日新月异，但在企业落地评估中，安全保障必须始终位列首要地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦非（罗氏中国 Snowflake 数据平台技术负责人）：2025 年 Data+AI 的变化不少，其中有这样三个关键的时刻：其一，罗氏诊断提出“三重确定”数字化战略：实现从实验室到临床的 AI 穿透。&amp;nbsp;通过整合肝癌辅助诊断算法与肝病管理数字化平台，AI 已能驱动从影像学检查建议，到定期随访计划，再到生活方式干预的全链条行动；其二，第八届数字中国建设峰会数字医药专题会议：从监管高墙到智慧灯塔的转型。&amp;nbsp;药监局推动的“AI+ 药物监管”模型，使企业从规避监管转向主动参与标准定义；其三，罗氏制药发布小罗智星 AI 科研解决方案：从赋能工具到科研主体的蜕变，“小罗智多星”AI 科研方案覆盖选题、文献解读到论文撰写全流程，在 700 余家医院落地 600 多个项目，证明 AI 不仅提升效率，更能激发和扩展人的创造力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高杰（蔚来汽车人工智能研发负责人 &amp;amp; 高级总监）：2025 年大模型演进带来的首个关键认知源于 DeepSeek 对推理技术的“祛魅”。相较于 OpenAI o1 最初局限于数学等可验证领域的神秘感，DeepSeek 不仅证明了高逻辑推理能力具有从特定学科向通用场景迁移的普适性，更将原本封闭的技术转化为行业易于获取的普惠资源。紧随其后的第二个转折点是 Claude Code 工具的诞生，它直观地展示了 AI 如何走出实验室假想、真正解决现实世界长程任务的理想形态。这两大突破推动我们重新定义汽车座舱：从交通工具到“有温度的情感伙伴”，需要拟人交互、全能帮手、深度理解三方面能力的协同进化，而数据正是实现“懂你”这一核心价值的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈砚琳（Snowflake 行业实践专家）：工业场景的认知重构聚焦于数据基础设施的价值重估。首先，多云异构环境下的数据互联成为可能，Snowflake 的跨云部署与合规特性，解决了跨国企业数据孤岛与跨境流动难题；其次，Cortex Analyst 等工具重塑了业务 - 技术协作模式，将两到三周的需求响应周期压缩至实时交互，释放了业务用户的数据分析潜能；最后，数据迁移的无缝衔接验证了平台兼容性的重要性，Snowpipe Streaming 等工具实现了 ERP、CRM、IoT 等多源数据的高效集成，证明基础设施的弹性决定了 AI 应用的落地速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜（白鲸开源 CEO）：首先是&amp;nbsp;“开源加成”。长年积累、架构稳定的开源项目已被大模型深度内化，AI 成为了最了解项目细节的“专家”，这赋予了开源项目全新的技术生命力。其次是&amp;nbsp;个体能力边界的跨越。AI 已从简单的对话进化为高质量的结果交付，即便非技术背景人员也能通过精准提示产出极具专业深度的技术内容。大模型突破了物理时间的限制，极大扩张了人的认知与能力边界。最后是&amp;nbsp;从交互到自动化的范式转移。以 DolphinScheduler 的演进为例，传统的“拖拉拽”操作正被意图驱动的自动流生成所取代。未来，人机对话将简化为纯粹的需求提出，由模型间自主协同完成复杂流程，实现真正的全自动化代理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;史少锋（Datastrato VP of Engineering）：1）2025 年 AI 的核心演进可归纳为&amp;nbsp;从“知识问答”向“全能代理（Agent）”&amp;nbsp;的全面跃迁，MCP 标准协议的开源使 LLM 操作外部软件接口的门槛大大降低，MCP 标准协议是 Agent 技术普及的关键催化剂；2）Claude 等模型在自动化编程领域的出色表现，则颠覆了传统软件的开发模式，AI&amp;nbsp;从“完成代码补全”进化为“全功能、全流程的开发助手”；3）新版千问 APP 的“奶茶点单”功能则展示了个人数字助理的新形态，通过 API 调用、位置感知与无缝的支付集成，实现从语言下达指令到订单交付的端到端闭环，预示着个人数字助理时代的加速到来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞（数势科技 AI 负责人）：Data Agent 产品的实践带来三个认知转向：从只盯技术参数看，到更要盯着“人”看：&amp;nbsp;当产品形态只做 Chat 的时候，仅关注准确率和速度会陷入“Data Search”陷阱，而融合大模型知识才能创造超出预期的价值；产品形态回归经典的必然性：dashboard 等经典形态仍是数据交互的有效环境，Agent 需要可沉淀的操作空间；从功能博弈到专业信任：&amp;nbsp;传统项目执着于“功能清单”式验收，导致产品在竞品间的比拼中陷入功能堆砌的泥潭，逐渐丧失核心价值主张。Data Agent 逐渐要从功能型的清单型交付，走向专业型交付，这也对交付人员提出了更高的 AI 认知要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;十问 Data Strategy，AI Strategy&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q1：企业在打造统一的 Data + AI 平台时最大的挑战是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬指出，企业在构建统一 Data+AI 平台的过程中，真正的深层挑战并非源于 AI 模型本身的技术上限，而在于&amp;nbsp;数据土壤。他形象地将这一挑战比作“果园”的经营：单一模型（树苗）的验证可以通过局部资源的倾斜（温室培育）快速见效，但若要实现企业级的规模化部署与持续产出，则必须依赖于高质量的“数据土壤”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在规模化落地阶段，企业面临的考验在于数据土壤的“有机质含量”与“灌溉系统”是否完备。这具体体现为：数据能否支撑 AI 跨部门、跨场景进行深度的洞察集成；在权限下放至一线管理人员时，企业是否具备精细化的安全隔离与治理能力；以及在 AI 输出指令或决策后，系统是否拥有完整的可观测性（Observability）以实现追溯与审计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实践数据看，企业招标需求中 80%-95% 聚焦于数据管理、存储效率与安全治理，仅 5%-20% 涉及模型训练与调优。这表明 CIO 们已清醒认识到：没有高质量、可治理、安全可控的数据基座，AI 应用终将沦为“沙上建塔”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q2：2026 年&amp;nbsp;Agentic AI&amp;nbsp;应用会迎来爆发吗？如何确保这些 AI 应用产生可信、可解释的决策？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬认为，2026 年 Agentic AI 将实现“突破”，而非“爆发”，其差异在于行业与企业的数字化成熟度分化。技术演进需经历学术突破、试点验证、规模化部署、普适应用四阶段，目前多数企业仍处于试点向规模化过渡的关键期。爆发的临界点在于数据基座的就绪程度，&amp;nbsp;当企业能将多模态数据高效整合、实现基于角色的权限管理、并建立 AI 决策的全链路可观测性时，Agentic 应用才能真正落地。关于可信性，需分场景定义标准：消费推荐等容错场景可接受一定误差，而财务报告等严肃场景则要求零容错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q3：如何看待开源与闭源在 Data+AI 领域的博弈？开源技术和社区力量将在 2026 年发挥怎样的作用？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜提出“社区价值大于代码开源”的观点，认为 2026 年将迎来“Community over Code”&amp;nbsp;的范式转移。随着 Agentic AI 的发展，代码实现的重要性下降，而问题定义、需求拆解等“提问能力”成为核心。开源社区的价值将体现在：汇聚多样化问题视角、形成集体智慧沉淀、推动技术普惠化。史少锋深表认同，他以 AI 编程实践为例：当机器能高效生成代码时，人类的核心竞争力转向创意与需求定义，社区的 Brainstorming 比代码提交更具价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q4：当人人都问 AI，知识社区注定会没落吗？新的具有“活人感”的经验会从哪里生长？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郭炜认为，以问答为核心的知识社区将不可避免地衰落，因为 AI 能提供更即时、个性化的答案；但以兴趣为纽带的讨论社区（如 Reddit）将崛起，这类社区的价值在于“活人感”的经验碰撞，观点的交锋、情感的共鸣、以及非结构化的创意激发。郭炜进而提出了一个“暴论”——未来 90% 的互联网信息可能由 AI 生成，人类创作将成为“稀缺品”，类似毛笔字的艺术价值。李飞补充道：新经验生长可能将呈现“无形化”特征：非正式的一对一交流、线下研讨会，都可能成为创新源泉。正如直播中嘉宾们的即兴讨论，这种实时互动产生的洞见，正是 AI 难以复制的“活人感”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q5：2026 年工业 AI 实现规模化突破的关键点在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈砚琳指出，工业 AI 的规模化突破不在算法本身，而在于&amp;nbsp;数据基础设施的系统性构建。随着预测性维护、缺陷检测及智能排产等算法趋于成熟与同质化，AI 算法本身已难以构筑企业的核心护城河。真正决定胜负的，是企业是否拥有坚实统一且可靠的数据平台。工业场景的数据极其庞杂，不同设备以迥异的频率和格式实时产生海量数据，若缺乏长远规划，极易陷入数据孤岛的困局，阻碍后续的数据消费。因此，企业成功的 AI 应用必须建立在对零散数据的合理规划与统一摄入基础之上。一个合格的数据系统，应确保用户能精准获取所需数据，并以预期的形式高效消费。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q6：在医药健康场景里，您最看好 2026 年 AI 落地的哪一个高价值方向？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦非认为，AI 驱动的候选药物分子生成与优化&amp;nbsp;将成为 2026 年医药健康领域的高价值方向。其核心逻辑在于：业务上，它直接切入研发核心，通过缩短早研周期与降低筛选成本实现立竿见影的财务回报；合规上，随着临床研究法规的完善，内部数据闭环下的 AI 研发已具备明确路径；技术上，继 AlphaFold 突破后，生成式分子设计已进入临床验证的爆发期。此外，在战略协同上，药企可利用自身在肿瘤、免疫等领域的优势数据，构建“模型 + 数据 + 药物”的增强闭环。而在实施维度，通过跨职能团队协作、高效数据治理以及与顶尖 AI 平台的深度耦合，能够有效管控技术复杂度与实施风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q7：在大模型与数据智能加持下，如何将汽车重新发明一遍？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高杰提出，当前汽车行业已从“能源竞争”上半场进入“智能化竞争”下半场。智能汽车的第一性原理，在于打造一个集“智慧空间”与“情感伙伴”于一体的拟人化交互系统。实现这一愿景需深度的软硬一体化布局：硬件层&amp;nbsp;需构建高带宽、低延迟的中央计算架构；中间层&amp;nbsp;需设计面向 AI 的操作系统（如 SkyOS）与数据中间件，确保整车跨域数据的自由流动与实时调度；应用层&amp;nbsp;则通过 NOMI Intelligence 等智能软件系统，将底层能力转化为具备主动智能的 Agent 体验。通过这种从芯片到应用的全栈叠加，汽车正从单纯的交通工具进化为全知全能的数字化情感伙伴，这也已成为行业共识的赛道终局。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q8：当数据分散在多云和多种 AI 工具中时，我们是不是在制造新的孤岛？该如何打破？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;史少锋认为，面对多云环境与 AI 工具普及带来的数据孤岛挑战，应从技术效能与数据治理两个维度辩证分析。首先，AI 技术的引入，一方面降低了数据工程的门槛，通过加速 Data Pipeline 的开发与自动化取数流程，AI 能够从技术层面有效提升数据开发和加工的效率，缓解传统数据孤岛的痛点；然而另一方面，随着 AI 应用的深化，大量的信息在跟 AI 的交互中产生，若缺乏合理的沉淀与治理机制，既可能造成知识流失，也可能演变为企业的“信息黑洞”。破局的关键要从组织、技术选型、业务等多个层面协同；当下原有的架构和实践会被颠覆，但新标准的产生还有待时日。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q9：当 AI 都能替我打工了，我为啥反而更累了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞认为这或许是&amp;nbsp;“杰文斯悖论”在个体生产力领域的重现，AI 极大缩短了单项任务（如撰写代码或制作 PPT）的耗时，但在组织效率博弈中，这种提效并未转化为闲暇。其次，角色身份从“生产者”向“监管者”转型。AI 虽能自主生成海量内容，但由于其可信度尚无法完全托管，从业者必须承担起更沉重的审核与融合责任。从另一个维度看，AI 极大地降低了创意落地的门槛。这种“即时验证”能力的释放，也导致了实践频次的增长，但也有可能带来“累并快乐着”的幸福感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬进一步指出，AI 将人类从重复性工作解放后，大脑需处理更深度的思考任务，如同项目经理协调多个 AI Agent，这种认知负荷的增加带来“心累”体验。但这种累是创造性的、价值增值的，正如从“体力劳动者”到“知识工作者”的转型，AI 时代的“累”预示着人类价值向更高维度跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Q10：在这一轮数据基础设施行业的整合洗牌中，数据链上下游最值得关注的协同创新机会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨扬认为在数据基础设施行业的整合洗牌中，最值得关注的协同创新机会在于&amp;nbsp;“将算法与用户体验带向数据，而非搬运数据”。以 Snowflake 并购 Observe 为例，这种上下游整合揭示了三大核心逻辑：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，减少数据孤岛的产生。通过将企业的业务数据（如财务、人力）与底层运营数据（如系统日志、安全数据）整合至统一平台，从源头上降低了数据孤岛在企业内部数据生态中的比率。其次，变革软件开发与交付模式。当应用直接构建在数据平台之上，开发者无需再关注算力寻址或数据建模，实现了运算、算法与用户体验同数据的无缝衔接。最后，驱动跨职能的协同效率。上下游的打通打破了业务人员与运维人员的沟通壁垒，使得“系统在线时长对营销的影响”等跨域问题能在统一平台上快速得到解答。这种将计算能力向数据侧下沉的模式，不仅规避了数据搬运带来的额外风险与人力开销，更构筑了完整且受控的平台级协同优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场年度对话深入剖析了 Data+AI 时代的变革逻辑，并达成了一个关键的行业共识：一个坚实、可靠、治理良好的数据基座，不仅是 AI 战略从愿景走向现实的唯一路径，更是决定企业智能进化上限的核心势能。&amp;nbsp;与会专家通过回顾 2025 年的“落地实战”并展望 2026 年的战略命题，清晰地揭示了产业图景的变迁——技术的竞争焦点已超越模型算法本身，全面转向数据质量、安全治理与平台工程化能力的综合比拼。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;站在 2026 年的关口，数据从业者们正身处一个历史性的交汇点。AI 的爆发式增长不仅带来了无限的创新可能，也对底层的“数据土壤”提出了近乎苛刻的要求。面对智能时代的不确定性，构建一套稳健、透明且具备确定性治理逻辑的数据体系，已成为从业者们共同的使命。作为全球数据云的引领者，Snowflake 始终致力于打破数据的孤岛与边界，未来将继续与广大数据从业者并肩同行，扎根数据深处，在波澜壮阔的智能变革中，以笃定的数据基座驱动业务的无界创新，共同定义 Data+AI 的下一个黄金时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;错过直播的朋友可以&lt;a href=&quot;https://www.infoq.cn/video/uqAQNcO0Ct5oJxoC1ARK&quot;&gt;点击此处&lt;/a&gt;&quot;观看完整版回放～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/b9Wb9tEFrgzAcxUt5Kv1</link><guid isPermaLink="false">https://www.infoq.cn/article/b9Wb9tEFrgzAcxUt5Kv1</guid><pubDate>Wed, 28 Jan 2026 08:03:14 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>当雪花落在中国 遇见企业 AI Strategy 的变革时刻 ｜ 2025-2026 Data+AI 年度时刻精华版</title><description>&lt;p&gt;回看 2025 的三次认知突破，走向 2026 的“十问 Data Strategy 与 AI Strategy”。这是一场基于真实实践的年度复盘，也是一次面向未来的集体判断。Data+AI 年度时刻精华版现已上线，8 位行业先行者，郑重写下了他们对 2026 的技术预言！&lt;/p&gt;
</description><link>https://www.infoq.cn/article/vy4ZYjr71ohKEOGp9tXz</link><guid isPermaLink="false">https://www.infoq.cn/article/vy4ZYjr71ohKEOGp9tXz</guid><pubDate>Wed, 28 Jan 2026 07:50:12 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>鸿蒙好文月度精选丨2026_2月刊</title><description>&lt;p&gt;欢迎关注&lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 01：15 年大数据老兵鸿蒙“造梦”，父女联手打造亲子游戏 App&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，从不缺乏跨界探索的身影。徐俊宸便是其中一位特殊的存在：深耕大数据领域多年，从数据产品经理到大数据讲师，他的职业生涯始终围绕数据打转；而一次偶然的鸿蒙论坛经历，让他萌生了开发APP的想法。最终，他以女儿课堂上的猜数字游戏为蓝本，与女儿一起打造出《猜数字大师》游戏应用，在跨界鸿蒙开发的道路上，既攻克了技术难关，也收获了别样的亲子时光。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&quot;&gt;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 02：老板发话鸿蒙APP 一定要上线，但不加人！分享一个快速实现跨端开发的技术方案&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;估计这是26 年开发团队的普遍现状，鸿蒙不得不做，人又不可能加。&lt;/p&gt;&lt;p&gt;毕竟到了26 年，HarmonyOS 6 终端设备也突破了 3.2 亿， 卓易通又被人骂得半死，所以开发一个原生鸿蒙 APP 必须摆上桌面了。&lt;/p&gt;&lt;p&gt;在资源有限的前提下，像我们这种千万以下日活的中小团队必须在以下三种路径中做出抉择：&lt;/p&gt;&lt;p&gt;纯原生重写：体验最好，但成本高到离谱，而且维护困难。&lt;/p&gt;&lt;p&gt;Flutter/RN：Flutter 是谷歌推出的，竟然不支持鸿蒙。&lt;/p&gt;&lt;p&gt;Web Hybrid (H5)&amp;nbsp;：成本最低，但性能体验太差，特别在鸿蒙上，容易被人骂。&lt;/p&gt;&lt;p&gt;目前看，第四种方案算是解法：&amp;nbsp;“&amp;nbsp;小程序容器技术&amp;nbsp;”&amp;nbsp;（&amp;nbsp;Mini-Program Container&amp;nbsp;）&amp;nbsp;比 H5 性能高，比原生开发也省事。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/article/89d1ce30eeac3a82759cebd4a&quot;&gt;https://www.infoq.cn/zones/harmonyos/article/89d1ce30eeac3a82759cebd4a&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 03：待到山花烂漫时：鸿蒙开发者用代码灌溉鸿蒙花园&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;银行业如何在鸿蒙转型中抓住机遇、快速进化？&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的“金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X框架迁移及原子化服务开发等技术突破，历时21个自然日完成HarmonyOS NEXT核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配HarmonyOS NEXT的故事。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&quot;&gt;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 04：元服务一站式平台：告别碎片化，开启All in One 一站式经营新纪元&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了给元服务开发者提供更聚焦、更高效的管理体验，我们在AppGallery Connect 平台上正式推出了元服务一站式平台&amp;nbsp;。&lt;/p&gt;&lt;p&gt;随着元服务能力不断丰富，相关功能分布在平台的多个模块中。为了帮助您更便捷地查找和使用所需功能，避免在无关菜单间跳转，我们构建了这个统一的专属工作空间，旨在聚合所有元服务相关能力，简化您的操作流程。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/gNwyuirySAxj5hQjsX0v&quot;&gt;https://www.infoq.cn/article/gNwyuirySAxj5hQjsX0v&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 05：“新”意十足 ·&amp;nbsp;HarmonyOS 模板 &amp;amp; 组件 （本次上新：社交、简历、翻译模板；聊天窗、购票等组件）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鸿蒙生态为开发者提供海量的HarmonyOS 模板/组件，助力开发效率原地起飞&lt;/p&gt;&lt;p&gt;更多内容，一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/market/prod-list?origin=template&amp;amp;ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;生态市场组件&amp;amp;模板市场&lt;/a&gt;&quot;&amp;nbsp;, 快速应用&lt;a href=&quot;https://developer.huawei.com/consumer/cn/doc/start/components-integration-deveco-0000002218625313&quot;&gt;DevEco Studio插件市场集成组件&amp;amp;模板&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt; 一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/solution/harmonyos/?ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;HarmonyOS 行业解决方案&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/52L9NAr6TAtLspJrKMKh&quot;&gt;https://www.infoq.cn/article/52L9NAr6TAtLspJrKMKh&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 06：“新”意十足 ·&amp;nbsp;HarmonyOS 模板 &amp;amp; 组件 （本次上新：新闻资讯 /uni-app、绘画模板；通用搜索、会员组件）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鸿蒙生态为开发者提供海量的HarmonyOS 模板/组件，助力开发效率原地起飞&lt;/p&gt;&lt;p&gt;更多内容，一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/market/prod-list?origin=template&amp;amp;ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;生态市场组件&amp;amp;模板市场&lt;/a&gt;&quot;&amp;nbsp;, 快速应用&lt;a href=&quot;https://developer.huawei.com/consumer/cn/doc/start/components-integration-deveco-0000002218625313&quot;&gt;DevEco Studio插件市场集成组件&amp;amp;模板&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt; 一键直达&lt;a href=&quot;https://developer.huawei.com/consumer/cn/solution/harmonyos/?ha_source=luntan&amp;amp;ha_sourceId=89000071&quot;&gt;HarmonyOS 行业解决方案&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文：&lt;a href=&quot;https://www.infoq.cn/article/MzGXuEGGBI3NdVGLUjRR&quot;&gt;https://www.infoq.cn/article/MzGXuEGGBI3NdVGLUjRR&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👉更多鸿蒙精选好文，持续上架中，欢迎扫码加入「InfoQ鸿蒙开发者交流群」，交流技术，也可联系「小助手」约稿~&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bf7ecdf3d9166706d0b9b6263a6b802.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👀也欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;</description><link>https://www.infoq.cn/article/dSNvzJS92gsUFI79ZpZK</link><guid isPermaLink="false">https://www.infoq.cn/article/dSNvzJS92gsUFI79ZpZK</guid><pubDate>Wed, 28 Jan 2026 07:36:45 GMT</pubDate><author>付秋伟</author><category>HarmonyOS</category></item><item><title>最极致的数据安全计算，迎来产业化的“临界时刻”</title><description>&lt;p&gt;当数据成为最重要的生产要素，计算却必须在“不被看见”的前提下完成——全同态加密，正在从一项“隐私计算圣杯”，变成现实世界必须回答的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年，Cloudflare 的一次区域性故障，导致数百家金融机构、政府服务与媒体网站瞬间停摆，暴露出当关键基础设施集中于少数平台时，一次攻击或故障便能引发全球性“数字海啸”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一家 AI 巨头遭受的黑客攻击，不仅导致模型数据泄露，更因训练数据污染，引发了后续模型输出的大规模偏差——单一漏洞，足以毒化整个系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以如果把过去两年的技术浪潮浓缩成一句话，那大概是：数据正在集中，而风险也在集中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;同态加密：不是更复杂，而是更简单&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型被部署在云端，金融、医疗、政务等最敏感的数据，开始以 API 的形式流动；跨机构、跨区域、跨国家的数据协作，从“能不能做”变成了“必须要做”。现实世界正在不断抛出一个看似简单、却始终无解的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有没有一种计算方式，可以在数据不被看见的情况下，把事情算完？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是同态加密重新被推到舞台中央的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实在隐私计算领域，同态加密并不是唯一的技术路线，但同态加密的好处是它非常简洁，直接在数学层面改变计算规则：数据自始至终以密文形式存在，计算在密文之上完成，结果在授权方手中解密。在理想状态下，整个计算过程中不存在“被看见的数据”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简而言之：从数学上讲，它的安全性可以被证明；从系统上看，它反而让架构变得更简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正因为如此，同态加密的安全性并不依赖系统配置，而是可以被数学证明。这种“安全内生于算法”的特性，使它在理论上具备一种近乎极端的吸引力——也是为什么它常被称为隐私计算领域的“圣杯”。&lt;/p&gt;&lt;p&gt;那既然是圣杯，为什么迟迟没有落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为理想往往伴随着代价。为了最高的性能，成本是绕不开的代价。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;同态加密成本有多高？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要探讨同态加密的成本有多高，就要从这项技术的背景聊起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同态加密的起点，并不来自互联网或云计算，而来自密码学内部一个极其朴素的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在传统加密体系中，加密和计算是两个泾渭分明的阶段：数据必须先被解密，才能参与任何有意义的计算。这在早期的计算环境中并不是问题——数据和算力通常掌握在同一主体手中，信任是默认前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 1970 年代，随着密码学逐渐从军事和政府领域走向学术研究，一个更基础的问题被提了出来：有没有可能在不解密的情况下，对加密数据进行计算？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1978 年，RSA 算法被提出。几乎在同一时期，研究者注意到 RSA 天然具备一种“结构保持”的特性：对两个明文相乘后再加密，与分别加密后再相乘，在某些条件下是等价的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，加密函数与乘法运算之间存在某种可交换性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然当时没人将其系统化，但这正是“同态性”的雏形。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在随后的二十多年里，同态加密并未成为主流研究方向。原因很简单：当时的计算模型、应用需求和硬件条件，都不足以支撑它的现实意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在学术上，一类被称为“部分同态加密”（Partially Homomorphic Encryption, PHE）的方案逐渐被系统化。这类方案通常只能支持某一种运算：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RSA、ElGamal：支持乘法同态Paillier（1999）：支持加法同态&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类算法在特定场景下非常有用，例如安全投票、隐私求和等。但它们有一个根本限制：无法同时支持加法和乘法，也就无法表达通用计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从计算理论的角度看，加法和乘法的组合，才构成了图灵完备计算的基础。缺少任意一种，同态加密就只能停留在“专用工具”的层面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在很长一段时间里，同态加密被视为一种“有趣但受限”的密码学技巧，而不是通用计算范式。&lt;/p&gt;&lt;p&gt;真正的转折点出现在 2009 年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当年，斯坦福大学的研究生 Craig Gentry 提出了第一个全同态加密（Fully Homomorphic Encryption, FHE）方案。这是密码学史上的里程碑事件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gentry 的核心贡献并不只是“同时支持加法和乘法”，而是提出了一套完整的理论框架，解决了一个此前被认为几乎不可逾越的问题：噪声增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在同态计算中，每一次运算都会引入噪声。如果噪声无限累积，最终会导致解密失败。Gentry 提出了“引导（bootstrapping）”这一关键思想：用加密数据本身，去同态地执行一次解密电路，从而刷新噪声。&lt;/p&gt;&lt;p&gt;这在概念上极其优雅，也极其昂贵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gentry 的方案证明了全同态加密“在理论上是可能的”，但在实践中，它慢到几乎无法运行。一次简单的运算，可能需要数小时甚至数天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但对学术界而言，这已经足够。也正是从那时起，全同态加密迅速成为密码学领域的研究热点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BFV、BGV、CKKS 等一系列同态加密方案在这一时期被提出，它们在效率和功能上不断逼近“可用”的边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便如此，全同态加密依然难以进入工程实践。原因并不复杂：它的性能模型，与现代计算体系格格不入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同态运算高度依赖大整数、多项式、模运算，而现代 CPU/GPU 的设计目标，是缓存友好、分支预测、向量化。这种错配，使得即便算法层面有数量级改进，系统层面的瓶颈依然存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，在很长一段时间里，同态加密的“主战场”依然停留在论文和原型系统中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而最近两年，随着云计算和大模型的普及，数据与算力开始分离。企业越来越多地需要在“不完全信任”的环境中处理核心数据。与此同时，全球范围内的数据保护法规持续收紧，对数据可见性提出了更高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种背景下，传统隐私计算技术的假设开始显得脆弱，而全同态加密那种“全流程密态”的特性，重新展现出不可替代的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个意义上说，同态加密并不是一项“新技术”，而是一项被现实推到台前的老问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这个老问题的核心难点却没有变，还是成本和性能之间的平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现实很残酷：如果一项技术足够安全，但速度慢到无法使用或成本及其高昂，它依然没有价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是蚂蚁集团决定投入长期资源去做这件事的背景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;蚂蚁为什么必须做这件事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说同态加密是一项“被逼出来的技术”，那么金融与医疗等领域，正是压力最大的那一端。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从业务属性来看，蚂蚁面对的是一组极端约束条件：金融数据一旦泄露，影响的不只是单一用户，而是整个信任体系；医疗数据天然涉及隐私，同时又需要在诊疗、科研、保险等多个主体之间流动；跨境业务则进一步叠加了不同司法辖区的数据合规要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这些场景中，任何一个被允许明文存在的环节，都会成为潜在的攻击入口。随着业务规模扩大，这种风险并不会被摊薄，反而会被放大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么，蚂蚁最终将目光投向了同态加密这种简洁且彻底的方案。它并不是最经济的选择，却是在某些场景下唯一能够覆盖全流程密态的技术路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;转折点：从硬件侧分析问题，从软件侧解决问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然是必然要走的技术路径，那面对高昂的成本，有没有更好的解决方案呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁技术研究院计算系统实验室副主任、先进加速技术团队负责人张明喆表示，“其实是有的。在同态加密的性能优化问题上，业界曾经形成过一个相对明确的判断：如果要实现数量级提升，就必须依赖专用硬件。毕竟，通用处理器并非为这类计算模式设计，硬件定制似乎是必然选择。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是在同态加密加速领域，早期几乎形成了共识：要想快，必须做专用硬件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;许多研究团队选择设计定制加速器，试图用电路层面的并行性来弥补算法的开销。但这条路意味着极高的研发成本、漫长的周期，以及难以规模化的部署。更重要的是，它会将同态加密锁定在“小众、高门槛”的技术轨道上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁技术研究院选择尝试一条不同的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁团队重新审视了问题本身，发现同态加密在 GPU 上“跑不动”，并非因为 GPU 算力不足，而是算法结构与硬件并行模型之间存在错位。换句话说，问题不完全在硬件，而在于软件如何组织计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过重构算法的数据布局和并行方式，团队逐步让同态计算“长得更像 GPU 擅长处理的任务”。这种方法并没有改变同态加密的数学本质，却在工程层面释放了巨大的性能潜力。最终，实现了三千倍量级的加速效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，这种加速并不依赖定制硬件，而是可以随着 GPU 的代际演进持续受益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，蚂蚁到底如何通过软件方案解决成本问题？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对 KLSS 算法可用性的研究是一个例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在同态加密中，密钥交换占据了 80%～90% 的计算时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KLSS 算法是密码学界在 2023 年提出的一项重要理论突破，它通过切片并行，大幅缩短了密钥交换时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但理论突破并不自动转化为实际可用性。KLSS 在 GPU 和硬件加速器中暴露出了新的问题：并行带来的带宽需求急剧膨胀，反而成为系统瓶颈。这也是为什么，它在提出后并未被真正应用到工业级系统中。&lt;/p&gt;&lt;p&gt;蚂蚁团队的工作，正是试图跨越这道鸿沟。他们没有简单地“实现算法”，而是从体系结构角度重新审视 KLSS 的计算模式，对并行粒度、数据访问路径和内存布局进行系统性重构，进而根据硬件特性对 KLSS 算法进行针对性优化。最终，使这一算法第一次在加速平台上具备了实用价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，从论文数量看，刚刚过去的 2025 年，蚂蚁已经在计算机体系结构领域的国际顶级会议发表了六篇同态加密加速技术相关的论文，在同期该领域 17 篇论文里面，占到了约三分之一的比例。这是否意味着蚂蚁在同态加密领域已经走得很靠前了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，蚂蚁密算科技CTO、蚂蚁技术研究院计算系统实验室主任闫守孟表示，从论文数量上来看是走在行业前端的，但从看整个行业看，“谈领先还为时尚早”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实恰恰相反——这是一个参与者还相对较少的领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是美国、韩国还是中国，真正长期投入同态加密系统研究的团队都还不够多。生态不成熟、门槛高、基础设施缺失，都是制约因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而技术的终点，不应止步于实验室的论文，而在于赋能千行百业。这也是为什么，蚂蚁在做前沿研究的同时，也在围绕同态加密系统性地推动校企合作和开源基础软件。其推动同态加密的路径图，清晰地勾勒出了一项前沿技术从理论走向产业的必经之路：构建一个让后来者“能学、能用、能评估”的坚实底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两年里，蚂蚁团队每年都会面向国内学术界提出 10 个同态加密加速的开放问题，围绕这些问题资助高校开展研究，并提供必要的技术支持。同时，团队也在加紧打通自身研究成果的“最后一公里”，计划以开源编译器、benchmark、加速库等基础软件套件的形式向业界开放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个技术的春天，往往不是来自某一瞬间的“奇迹”般的技术突破，而是源于无数次的深耕与播种。只有愿意俯身，专注于解决一个个细小但关键的难题，并通过开源、合作等方式反哺整个生态时，同态加密这片土地才会逐渐变得肥沃，生态也终将走向繁荣，迎来属于自己的春天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这或许正是中国技术生态走向成熟的一种范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采访嘉宾：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;闫守孟，蚂蚁密算科技CTO、蚂蚁技术研究院计算系统实验室主任&lt;/p&gt;&lt;p&gt;张明喆，蚂蚁技术研究院计算系统实验室副主任、先进加速技术团队负责人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sjlgGt1p4xvaWWgGgRrZ</link><guid isPermaLink="false">https://www.infoq.cn/article/sjlgGt1p4xvaWWgGgRrZ</guid><pubDate>Wed, 28 Jan 2026 06:50:34 GMT</pubDate><author>李冬梅</author><category>AI 工程化</category></item><item><title>AI 驱动的智能异常处置：从异常发现到根因定位</title><description>&lt;p&gt;异常处置包含异常发现、问题定界和根因定位等环节，高效的异常处置流程对于保障平台的稳定性起到至关重要的作用，然而平台本身的复杂度以及海量的多元异构数据给异常处置带来了巨大的挑战。如今，大模型等 AI 技术的演进则为应对这些挑战提供了新的思路。在 2025 年 InfoQ 举办的 QCon 全球软件开发大会（ 北京站）上，来自阿里云的算法专家张颖莹发表了题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6306&quot;&gt;AI 驱动的智能异常处置：从异常发现到根因定位&lt;/a&gt;&quot;》的演讲。她从阿里云计算平台的运维场景出发，分享了从异常发现到问题定界和根因定位各环节的算法选型和设计思路，包括通用的时间序列异常检测、高效的日志聚类和精准的多 Agent 根因定位框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：将于 2026 年 4 月 16 - 18 召开的 QCon 北京站策划了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1921&quot;&gt;Agent Ops：运维新生产力&lt;/a&gt;&quot;」专题，将深入讨论 Agent 如何与现有技术栈深度集成，并演进为具备长期记忆与自我进化能力的运维实践。如果你也有相关方向案例想要分享，欢迎提交至 &lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;https://jinshuju.com/f/Cu32l5&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里云大数据运维背景&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们阿里云计算平台整合了整个阿里的大数据和 AI 的能力，并以产品化、平台化的方式支撑了我们集团内部与云上各行各业客户的众多非常核心的业务场景。这里列举其中几个比较典型的平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如 MaxCompute 大数据计算服务主要负责大规模数据的离线计算。大家日常在网购后经常会在手机上去追踪自己的包裹，这些菜鸟的物流数据产出就依赖 MaxCompute。还有实时性要求相对较高的场景，比如自动驾驶场景，车机系统会对司机的危险驾驶行为发出实时警告。像这一类比较追求时效性的场景，往往就依赖 Flink 这样的实时计算引擎。下一个是 Hologres 实时数仓，大家在手机淘宝上检索商品关键词时，它会在底层进行实时的交互式分析，为大家呈现实时的检索推荐结果。另外随着大模型越来越火爆，大家对机器学习模型相关的训练、微调、在线服务的需求都有了爆发式的增长。这一类的模型训练、微调、在线服务都可以一站式在我们的机器学习平台 PAI 上完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以看出我们的这几个平台上层支撑的业务都非常重要，所以做好这些平台的运维也非常关键。但传统的运维模式很容易让运维人员变成背锅侠的角色。所以我们计算平台也专门设置了一支运维中台研发团队来负责所有大数据和 AI 产品的统一运维管控。我们也一直利用“AI+ 数据 + 工程”的手段来提升整体运维效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;稳定性作为运维的基石，其重要性是毋庸置疑的。但对于系统来说，异常的发生很难避免，怎样在异常发生时能进行快速高效的处置，对于整个平台的稳定性是非常重要的。另一方面，随着我们的用户对云服务厂商服务水平的要求越来越高，精细化运维已成为行业趋势。另外像我们前面提到的这些大数据平台，它们的底层都是超大规模的计算集群，这些集群无时不刻都在产生海量的数据，这些海量数据对我们的异常处置也带来了更多的战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我把整个异常处置层面我们面临的挑战总结成三个层面。首先是面对这样复杂的系统，我们怎样从这个系统运行的蛛丝马迹里全面发现各种异常，确保监控的覆盖率。第二个层面是面对这么多异常告警，我怎样从这些告警风暴里真正剥离出最关键的信息，从而减少误报对运维人员的干扰。第三个层面是当异常发生时，我怎样快速定位到问题的根本原因，并采取针对性的措施，对症下药来让异常快速恢复，减少对用户带来的损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;异常发现和告警降噪&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们会为大家分享我们是怎样逐一来攻克前面提到的三个挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是异常发现的层面，我们团队构建了非常丰富的异常检测相关的算法，力求实现异常的全面覆盖及精准发现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/44/441251a86a82f1acb9500a28199dcb8a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在这里梳理了 4 个典型场景，我们针对不同形态的数据和不同的场景都会选择它最适配的算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是单指标异常检测场景，比较典型的应用就是我们整个系统的可用性监控。比如这个平台整体的流量、性能、成功率，这些指标和用户自身的业务周期是非常紧密相关的，因此它经常会呈现出比较复杂的多重周期性。所以在这里我们自研了一套鲁棒的周期分解算法，来对这些曲线中的多重周期性进行精准的识别和分解，从而更好地做到异常的发现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是多指标异常检测场景。当我需要判断一台机器是不是存在异常时，可能单独去看其中的某一条指标是没有太大参考价值的，我需要综合去看这个机器所有维度的指标。在多指标异常检测这边，我们直接选用了开源的多指标异常检测模型。虽然它相比单指标异常检测可能会牺牲一定程度的可解释性及性能，但它可以更好地把握多指标之间的内部关联性，从而提升检测的精准度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三类是基于分布的异常检测。在大数据运维的场景里，我们经常会面临着这样的问题，就是当我的集群性能变慢时，我不单是要检测单个指标、单个作业是不是变慢了，而是希望去看整个集群或整个平台的作业运行的分布是不是有异常的变化。针对分布的异常检测，我们也自研了一套异常检测算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了指标之外，日志也是非常重要的一种可观测的数据。日志数据最大的挑战是它的体量非常庞大，所以在这里我们先选用了业界性能比较高的日志模板提取算法，然后我们会基于提取出来的日志模式去判断它是不是新增的异常，或者它的模式是不是有暴增的变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们重点给大家展开介绍一下前面提到的两类自研算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e3/e3a8bf1eaf1eebc1301f72b74a29fa96.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是针对单指标的场景，我梳理了我们运维场景里最关心的几类典型异常，包括均值变化、方差变化，也就是抖动频率的变化，还有尖峰深谷的异常、断崖式跌落的异常，还有趋势型的异常。大家可以看到梳理出来的异常可能看起来还比较明确，但实际上这些异常融合到真实的业务数据里时，非常容易受到数据本身的其他周期性的干扰，使得检测变得非常困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在这里我们构建了&amp;nbsp;周期分解算法，它的核心思想是采用了分而治之的策略。首先从一条时间序列里把不同频率的周期逐一剥离出来，然后再针对剥离出来的每一重周期去精准计算它具体的周期长度，从而更好地把握整个数据的周期性特点。做完周期分解后，我们会进一步利用不同类型的统计检验方法，来分别对应检测前面提到的这几种典型异常，从而实现用一套算法框架就能够覆盖前面提到的多种类型的异常，使得这一套单指标的异常检测算法能够在运维领域具备更强的普适性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9e/9ed94df48e40ac0449ee23f848bf36ae.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是基于分布的异常检测。大数据运维经常会面临的痛点，是当我需要做整个集群性能的异常检测时会面临两个挑战。首先是整个集群上运行的作业数量非常多，如果我对每个作业都去检测它有没有运行变慢，耗费的成本会非常高。而且即使我做到了对每个作业的检测，但实际上我并不会关心单一的某作业的波动，因为很多情况下可能用户购买的资源不足，他自己的资源组里面的作业之间也会进行资源的争抢，所以也会出现单作业变慢的情况。但我们真正需要关心的是整个集群作业性能异常、变慢的这种趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们把整个集群作业运行时长的分布图构建了出来，然后借鉴了优化领域非常经典的运输问题，结合基于重构的深度学习模型来进行异常检测。我们可以把整个集群的作业运行时长的分布图想象成土堆，然后当这个土堆向右边运输时，我们增加一定的惩罚项，从而更好地检测出整个作业运行时长分布图向右偏移，也就是整个集群性能变慢的这种场景。当然我们还在深度学习模型里选用特殊的门控机制，来更好地应对训练样本当中的噪音问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到这里我们已经通过多种类型的异常检测算法实现了异常的全面覆盖。随之而来的问题就是面对这么多的异常告警，运维人员怎么判断到底哪些异常才是需要第一时间响应的。所以我们需要对这些告警进行进一步的精细化分级。我们主要从两个方面来进行告警的精细化处理，分别是影响面拉取和问题定界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/7f/7fdde9e5de3e644b84b6fe380e920436.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;影响面拉取指的是当我们的主指标异常触发了异常工单后，我可以根据整个拓扑关系拉取到主指标所关联的子指标。然后我通过时间序列的下钻算法，可以量化出来每个子指标对主指标的贡献程度，以及它自己相对于历史的异常度。综合这两个维度，我可以计算出来这一次的异常所波及到的影响面到底有多广。一般来说波及面越广的异常，运维人员自然要去更加高优地响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是问题定界。在大数据的场景里有很多异常是因为用户自己的操作失误引起的，比如像一条 SQL 语句，如果它的语法就存在错误，自然会运行失败，但运行失败就会产生报错日志，甚至会影响到用户实例本身的成功率。但像这一类语法错误导致的异常，我们的运维人员并不需要第一时间介入处理。运维人员真正应该关心的是由平台问题导致的失败或者任务的异常，所以我们需要对用户作业的报错日志进行更加精细化的分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里我们首先用前面提到的日志聚类算法对异常时段的所有日志先进行聚类，聚类完成后我们会提取出其中典型的日志异常模式，然后和我们日志知识库当中的标签去匹配，这个标签就可以标识出日志到底是用户问题还是平台问题。日志知识库的标签从哪来？一方面可以由我们的运维专家去人工标注，另一方面我们现在也在用大模型做这方面的提效。我们会利用大模型事先生成预标签结果，然后让专家审核。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于影响面和问题定界的结果，我们就可以对告警分成不同的等级，包括需要立即响应的故障性异常，还有红灯、黄灯，还有可以稍微延迟处理的风险性异常。这种做法首先可以让我们不遗漏任何一种风险，同时又可以更好地分配运维专家的精力和关注度，确保他们能够更高效地处置那些真正紧急的异常。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;多 Agent 根因定位框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到这里我们已经解决了异常发现环节的问题。但实际上在异常处置流程里，最耗时也往往最困难的点在根因定位这个环节。因为这个环节涉及到的数据还有工具都非常繁杂，而且即使存在一套非常标准的运维排障流程，但真正具体到每一次故障时，依然需要结合当时的场景和数据的具体问题做具体分析。所以根因定位往往也只有那些经验非常丰富的运维专家才能够做到真正高效的处置。也正是因为根因定位存在着这样的难点，它近年来也一直是学术界、工业界都非常关注的热门话题。我们团队也一直在根因定位这个方向上不断升级策略和算法，现在也引入了多 Agent 的框架来解决这样的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在具体介绍我们的策略之前，我们可以先简单回顾一下 Agent 的核心要素。这几个要素对构建高效的智能体来说非常关键，它也是我们后续设计我们整个多 Agent 根因定位的核心思路。首先是角色的设定部分，我们通常会在大模型的 prompt 里交代它的角色定位，包括它的业务背景，使得它能够在领域上具备更好的专业度，同时也能够更加明确它自己的任务产出到底是什么样的形式。第二类是长短期记忆，通常我们会通过 RAG 的方式引入外部私域知识，进一步提升大模型在私域的专业性，更好地让它了解上下文。第三类是好的工具模块，让大模型具备更强大的主观能动性，拓展它的能力边界。最后是自主编排，对大模型来说非常关键，因为它直接决定了大模型能不能很好地做到任务的拆解，以及具体执行步骤的编排，它很大程度上决定了大模型能不能够解决根因定位的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来我们就分别介绍我们是怎样基于这几个核心要素来构建多 Agent 根因定位框架。首先是角色设定的部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以回忆一下，当我们日常出现线上问题时，运维团队是怎样工作的？通常他们会成立故障应急小组，在这个小组中会有各个模块的负责人，他们会分别排查各自的模块目前的信息，并且判断自己到底是根因方还是受影响方。然后他们也会和自己的上下游模块做沟通，最终他们的结论会汇总到故障应急负责人这边，他会去对全局的信息做整体汇总，并给出最终结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们希望基于大模型构建出来的诊断系统也能够具备故障应急团队这样的效果。在这个团队里面，人的分工是非常关键的，每个角色都应该具备自己的专业度和特长，所以单一的 Agent 通常不能满足这样复杂任务的需求，所以我们引入了多 Agent 的框架。我们是按照系统的模块来设定每个 Agent 的角色，比如会有专门的存储 Agent、调度 Agent、网络 Agent 等。在 prompt 里，我们会内置模块相关的背景信息，使得它们可以对照现实世界里每个模块的 owner 这样的角色。除了模块 Agent 外，在上层我们还会有系统 Agent 的角色，就相当于是故障应急负责人。它可以收集每个模块 Agent 的结论，并且给出最终的判断。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/db/dbba653563f24c8aed98c85c61391a42.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成了 Agent 的角色设定后，接下来很重要的就是要丰富每个 Agent 的装备库。我们构建了 4 大类工具，首先是算法服务类的工具，包括前面提到的时间序列异常检测、日志异常检测能力，还有因果推断能力。这些服务都会构建成在线服务的形式，可以非常方便地对接其他系统，或者作为 API 来让 Agent 调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二类是 RAG 工具，它在私域的智能问答领域里是非常核心的技术，在根因定位环节里同样发挥着非常关键的作用。比如当我们需要对照历史的相似故障来参考它的排障经验时，或者当我遇到一些指标和日志，但可能不太清楚它的具体含义是什么时，都需要参考对应的文档，把对应的知识检索回来，从而给大模型提供更丰富的背景知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三类是运维分析类工具。我们的运维人员构建了很多集成了他们专家经验的分析诊断流，比如针对单个作业的诊断，还有针对整个单机的诊断，还有网络层的诊断等。这一类诊断工具理论上都可以由大模型来自主编排完成，但实际上因为这些工具之前就已经沉淀好了，而且我们利用编排好的诊断流可以直接得到非常明确的结果，所以在很大程度上可以减少大模型的 token 消耗，来提升整体根因定位的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四类是外部插件。现在很多大模型应用平台都搭载了非常丰富的生态系统，有着插件市场这样的概念，在里面很多第三方的开发者都会贡献他们自己研发的分析工具，比较典型的包括在线检索类工具、代码编写类工具，还有 chatBI 类的工具。现在这些工具都可以直接拿来为我们自己所用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这些工具集的构建，我们就让 Agent 同时具备了专业的运维分析能力、专业的算法分析能力，甚至还具备一定的通用基础开发能力，这样它就能有更好的武器应对更加复杂的根因定位场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三部分是关于编排可靠性的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/5f/5fa293d575992271e3b786eb6c306cbd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于编排，我们会面临这样的挑战，就是一方面我们希望能够尽可能释放大模型自主编排的灵活性，这样它在以前没有遇到过的故障场景也能发挥特定的效应，而不是只能针对历史重复出现的故障才知道该怎么做。但另一方面大模型编排结果是否可靠，可能直接决定了这个故障是不是能够及时恢复，在这个方面我们的容错性是非常低的。所以这里的最大难点就是怎样在释放大模型编排灵活性的同时，又能进一步提升编排结果的可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这里我们采取的策略是固定工作流编排和自主编排相结合的混合策略。一方面我们会把运行性能相对较高，并且对根因诊断非常关键的工具，直接编排到前置的工作流里。这些工具直接执行完后，我会把它的结果输入到大模型里，再让大模型自主决策是不是还需要调用额外的工具来做进一步排查，才能够得到最终的根因推断结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后在大模型自主编排这一部分，我们也采取了很多策略来提升它结果的可靠性。任务分解部分我们主要采用的是 react 框架，也是现在比较主流的框架。大家在实际应用里也可以直接把它作为 baseline 来作为后续提升的参照。另一部分是记忆增强，我们通过检索外部的 SOP 来让大模型进一步校准它生成的 SOP 的可信度。第三部分是加入了反思机制，我们会让大模型在整个决策过程动态反思中间过程可能会有哪些改动来保证灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了任务分解、记忆增强和反思机制之外，还有一些策略可以进一步提升它的编排可靠性，包括多计划选择，还有引入外部规划器来辅助它生成这样的策略。我们也计划在后面再对这些策略做更详细的尝试。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后一个问题涉及多 Agent 框架的协同。我们前面聊的都是怎样让 Agent 自己变得更好，接下来的问题是我有这么多个 Agent，怎么能够让它们更好地协同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/77/77f0986bedb8ddb9c6747d6c60f4c94a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在有非常多的 multi Agent 编排框架，他们在系统里都会内置编排好的多 Agent 协同工作流。但这些默认的工作流，在我们的场景里或多或少都会存在一定的弊端。比如像顺序执行的工作流，上游节点在做决策时是不知道下游节点信息的，所以会存在着一定的信息不对称，会导致它得到片面的结论，而它的结论可能又会进一步影响到下游节点的决策，会形成一定的误差累积效应。第二类是层次结构，虽然看起来有顶层节点来对大家的信息做汇总，但实际上下游节点之间依然是不存在任何信息交换的，同样会导致它们自己得到比较片面的结论。第三种圆桌讨论的模式，看起来大家的信息可以在桌子上进行非常充分的交换，但它最大的弊端在于缺少明确的领导者，所以大家的讨论可能会非常发散，聊着聊着可能就偏离了主题，很难在限定时间内得到非常明确的结论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑到这些固定编排模式的弊端，我们自研了一套基于神经网络反馈机制的工作流。它的核心思想也非常简单，我首先会根据模块之间的拓扑结构设定单向传导的工作流，我们称之为前向反馈。在前向反馈的基础上，我额外增加了后向反馈的机制，实际上就是让前置 Agent 有机会修改自己之前可能得出的错误结论，然后最终大家的结论依然会汇总到系统 Agent 这边来。这样的好处是一方面我可以在一定程度上弥补信息不对称的问题，同时也能把整体的推理次数非常严格地限制在预设的范围内，减少盲目发散的讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;通用异常处置平台&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面介绍的更多都是算法层面的设计，而好的算法最终还是需要真正集成到我们的平台上，才能真正融合到运维人员的工作流里，发挥出真正的效用。所以工程平台的建设也非常关键。在这里我给大家分享一下我们怎样来构建通用的异常处置平台。现在我们各个产品的运维异常处置流程都可以在这个异常处置平台上来进行，很大程度上提升了我们计算平台整体异常处置的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/69/6956e5683d07327bd4635d294971d9af.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体架构最底层是数据层，我们为运维场景里这些经典的数据模式都安排了最适合它们的存储方式，包括指标、日志、拓扑、文档、事件等，使得它可以在后续的分析环节里做到非常高效的数据抽取、根因分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据层之上是我们非常核心的算法服务层及大模型服务层。算法服务层里搭载的是前面提到的时序、日志、根因定位、因果分析，还有检索这类非常基础通用的算法，这些算法会部署在 PAI-EAS 上变成在线服务，可以供其他系统直接调用，也可以作为工具集成到 Agent 里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型相关的这一部分，除了前面提到的 prompt 工程，还有工具的调用，还有工作流编排。对于完整的大模型服务而言，如果你不是 demo，如果你想要真正上生产的话，还需要考虑很多因素，比如像可观测能力，还有资源的管理隔离能力等。所以想要搭建好大模型应用服务，还是需要搭配非常好的大模型应用构建的平台。幸运的是现在也有非常多的这样的平台，包括商业化的、开源的，都具备了非常强大的能力。但对我们来说，我们还是需要根据我们自己的业务场景去选择最好的、最适合自己的平台来进行构建，才能提升效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来也想和大家进一步分享一下我们在选择这样的大模型应用开发平台时会从哪几个角度来考虑。我们主要会从三个层面，第一个是应用构建本身的便捷度和产品应用性，第二个是 LLMOps 能力的完备度，最后是平台本身的开放度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在应用构建方面，我们会重点考虑我在这个平台上是不是可以非常便捷地完成非常复杂的业务工作流的编排，最好就是拖拉拽的方式就可以完成复杂的编排任务。其次是这个平台上面是不是同时具备了微调能力，这样我就不需要在各个平台上频繁切换，能够在平台上一站式完成整个模型的微调和最终应用的部署搭建。第三个是像 RAG 的经典组件，我在这个平台上是不是能够直接复用，减少额外的开发工作量。最后是我在这个平台上面搭建的服务，是不是能够非常便捷地和最终的交互出口承接，不需要额外再构建中间的一层服务来进行引流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个大的层面是 LLMOps 的能力，它直接决定了整个大模型应用服务的稳定性，以及后续性能优化的空间。所以我会重点关注平台是不是具备一定的模型加速能力，资源管理的能力也非常重要，就是在突发流量打进来的时候，你是不是能非常方便地帮我做资源扩容。还有我不同的大数据产品之间肯定是要做隔离的，你是不是具备完备的资源管理隔离的体系。可观测性也是非常关键的，当我大模型推理失败的时候，是不是能够非常便捷清晰地看到到底是哪个环节、哪个工具调用出现了问题，方便我进行问题的快速恢复和改进。最后是模型测评的能力，因为现在基础模型发展非常迅速，所以我希望能够在平台上非常便捷地做模型效果的测评，来方便我选择最适合这个场景的基模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个层面是开放性，开放性直接决定了我在你这个平台上是不是能够更好地利用别人开发的能力，以及我是不是能够和开源的生态做更好的兼容。这里首先要考虑你的插件市场是不是足够丰富。第二个方面，像现在比较火的 MCP 协议，你是不是能够天然支持？还有同外部系统以及开源框架的对接，我现在迁移到你的平台，是不是能够更好地做到无缝的迁移，我后续是不是还能够持续用到开源生态的创新性成果，这些都非常关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些考虑因素，我们现在选用的是阿里云的百炼来搭载大模型应用，当然大家也可以结合自己对这几个因素的优先级的排序，选择更适合自己的平台。我们选择百炼，一方面是它在我前面提到的几个维度上相对来说是比较完整的，同时它在应用类型上也非常丰富，既包括我前面提到的固定工作流式的编排，也提供了以 RAG 为核心的智能体应用，同时它还提供了智能体编排应用，可以把前面提到的多种不同类型的应用全部整合进来，做到混合编排。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外它整个任务编排的产品界面是相对来说比较友好的，我在上面可以非常便捷地拖拉拽来完成复杂工作流的编排。最后在整个百炼的项目空间里，我可以观测到整个服务的调用情况，每一次调用都可以点开详情看每个工具的输入输出到底是什么，是不是符合预期，方便我进行问题的排查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前面我们已经完成了大模型应用搭建的部分，接下来我们具体聊一下整个异常处置平台到底包含了哪些核心的模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/cd/cdca3580c2afb5c4d458a7410b435d20.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先这个平台的入口也就是告警源，除了前面提到的算法检测结果外，在我们实际的业务里它还会包含 SRE 自己在监控系统里设置的监控告警，当然还有用户来提的工单或者人工补录的情况。每一种告警来源，我们都会给它生成异常工单，这个工单里会包含 4 个非常核心的模块，首先是异常现场，然后是定界定级的结果，还有根因定位的结果以及快速恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;异常现场主要呈现出这一次开工单的原因到底是什么，触发的指标和日志到底是什么样的，来方便运维人员在接手工单时快速了解问题的背景。然后是定界定级的结果，会具体呈现出这一次异常的影响面，以及算法得到的分级过程。根因定位会展现出多 Agent 框架定位的结果，我们现在会得到根因模块的结论，同时大模型也会提供出得到这个结论的推理依据。同时对于每个模块 Agent，我都可以点开详情查看它的工具调用情况。最后快速恢复的部分，我们现在还在相对比较初步的阶段。目前主要是做的是检索历史的相似工单，这样运维人员可以在新的工单里直接点击跳转到历史工单里查看当时的处理策略，从而对这一次的异常处置提供一定的参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以整体来看一下整个异常处置平台在我们线上应用的真实效果。首先当异常发生时，运维人员会在钉钉上收到卡片的通知，根据告警等级的不同，卡片也会呈现出不同的颜色，直观看出异常的严重程度。如果异常没有被及时处置，它的影响面可能会不断扩大，它可能会从黄灯变成红灯，这个卡片的颜色也会随之动态变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外当工单被运维人员接手进行处置时，我们可以在工单上实时看到它的处置进度，方便整个群里的人都了解整个异常的处置情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运维人员收到这个卡片后，他可以点击对应的链接跳转到异常工单的页面上，可以看到异常的现场，包括具体的曲线以及曲线到底是从哪个时刻开始有这个异常点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后在异常影响面的分析部分，我们可以看到这次的异常到底影响了哪些客户，我们会在这里列出具体的客户信息。同时我们也能看到这个客户这次受影响的实例在我们这次异常里的占比。在最下方，我们会呈现多 Agent 的根因定位结论。首先会得到明确的定界和定位结果，以及这个结果的核心依据。下面我们可以看到每个模块 Agent 的独立结果，点击详情就可以进一步看这个模块到底调用了哪些工具，以及这些指标日志的检测的情。实际上我们经常会出现多个模块 Agent 都觉得自己出问题了，都可能觉得自己是根因这样的情况，但我们最终的系统 Agent 还是会根据各个模块之间的潜在拓扑关系得到更加明确的结论，最终它给出的结论只会是最终根因的那个模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;总结和展望&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这次分享，我们首先从大数据运维的业务背景出发，来给大家介绍了我们在异常处置环节到底都面临着哪些挑战，包括我们怎样全面检测出这些异常，以及怎么面对告警风暴，真正剥离出其中关键的信息，帮助运维人员更好地分配注意力，以及最后我们在异常发生时怎么快速定位到问题的根因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后我们具体介绍了我们怎样利用 AI 技术来逐一攻破这些挑战，包括建设多种类型的异常检测算法，以及通过影响面的分析还有问题的定界来帮助我做更加精细化的告警分级。最后我们还引入了多 Agent 的根因定位框架，来模拟现实当中的故障处置小组实现根因模块的定位，并且给出它的推理依据，让我们的大模型推理不再是黑盒。前面提到这些算法技术都是通过 PAI-EAS 部署成在线服务的方式来供其他的系统和大模型应用层来进行进一步的调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而我们大模型服务层本身则是依靠百炼这个平台来进行构建和部署的，最终这些算法服务层和大模型服务层共同支撑了最上层的异常处置平台，真正把 AI 能力集成到平台和产品里，整合到运维人员日常的工作流里，发挥出真正的提效作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后我们来对下一步的规划做展望。首先我们会进行异常处置能力整体的补全，会从现在事中的异常发现，一步向前延展，做风险预防。我们整体的思路是希望纳入更多海量数据来做故障的提前预警，这方面带来的技术挑战会更大，可能会涉及告警本身时空相关性的挖掘等技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在根因定位之后，我们还会打造真正的预案推荐，因为只有真正推荐出了可能的预案，才有可能走到最终的自愈环节，做到处置的自动化闭环。预案推荐在某种程度上也依赖根因定位的精细度。目前我们的根因定位也只能做到一级的模块，后面我们会进一步做到二级模块，来让整个根因定位更加明确具体，让它最终的关联动作可以更加明确。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了异常处置能力的补全外，我们还会进行 Agent 的能力增强，包括自主编排、可靠性的提升，我们还有很多的策略需要尝试，来进一步保证它的结果是靠谱，并且性能是足够优的。还有工具能力的拓展，我们现在主要是把现有的运维平台上面的工具还有作业去兼容 MCP 这样的协议，使得 Agent 具备更强的系统兼容性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后是交互体验的优化以及人工反馈的增强。要让大模型能够得到令人满意的效果，人的实时反馈是非常重要的，包括现在很多像 Manus 这样的组件，都会在生成 plan 之后允许用户有机会做调整，这对最终结果的准确性非常关键。所以我们整体的交互模式的变化，以及后续怎样利用人工的反馈来持续优化后面的迭代，让大模型真正做到越用越聪明，是非常关键的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来说，我觉得大模型技术和 AI 技术的发展可以用日新月异来形容，它也给我们智能运维领域带来了很多技术上面的突破，我也非常期待我们能够有更多的成果来做进一步的分享，感谢大家。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;阿里云计算平台正急招智能运维算法专家，&lt;a href=&quot;https://careers.aliyun.com/off-campus/position-detail?lang=zh&amp;amp;positionId=2009183001&amp;amp;trace=qrcode_share&quot;&gt;岗位链接&lt;/a&gt;&quot;，也可直接投递简历至：congrong.zyy@alibaba-inc.com，欢迎加入我们。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖莹，阿里云算法专家，阿里云计算平台智能运维算法团队负责人，在智能运维领域深耕 8 年。用产品和服务支撑计算平台 MaxCompute、Flink、Dataworks、PAI 等多个大数据 &amp;amp;AI 产品的智能化运维。多项研究成果被 ICLR，KDD，VLDB, SIGMOD, ICDE，WWW, CIKM，ICASSP 等国际顶会接收，并带领团队获得了 ICASSP 国际智能运维算法大赛冠军。曾受邀在 QCon，ArchSummit，DataFunCon，FlinkForward 等大会发表演讲，同时参与了阿里巴巴开源大数据运维平台 SREWorks 开发和信通院《智能运维能力成熟度模型》行业标准编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;活动推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026，AI 正在以更工程化的方式深度融入软件生产，Agentic AI 的探索也将从局部试点迈向体系化工程建设！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot; 已正式启动，本届大会以“Agentic AI 时代的软件工程重塑”为核心主线，推动技术探索从「AI For What」真正落地到可持续的「Value From AI」。从前沿技术雷达、架构设计与数据底座、效能与成本、产品与交互、可信落地、研发组织进化六大维度，系统性展开深度探索。QCon 北京 2026，邀你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8a/8a69f038dc22619d47309fca02519740.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Mv7SsDMyFBDz3coYwTUx</link><guid isPermaLink="false">https://www.infoq.cn/article/Mv7SsDMyFBDz3coYwTUx</guid><pubDate>Wed, 28 Jan 2026 06:03:41 GMT</pubDate><author>作者：张颖莹</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>顶级工程师Karpathy的“AI革命”：一个月前还在焦虑“落后”，如今“20年古法编程习惯”被彻底颠覆，80%代码已交给AI！</title><description>&lt;p&gt;今天，Andrej Karpathy 又发了一条很长的推文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他分享了使用Claude进行数周高强度编程后的心得体会，并且表示自己过去 20 年形成的编程工作方式，在短短几周内发生了明显变化：从 11 月还以手写和自动补全为主，到 12 月迅速切换成大约 80% 交给 agent、自己做 20% 的修改润色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，他提到 Claude 和 Codex 在 2025 年 12 月左右跨过了某种“一致性/连贯性门槛”，让这种以 agent 为主的写法突然变得可行，并且很难再回到完全手写的状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“2026 年将是充满活力的一年，因为整个行业都在消化吸收这项新技术。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/dba5c24132e79347c937deb1fd570cb5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一个月前，顶级工程师说“我落后了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而就在一个月前，这位提出“vibe coding”一词的人，还在 X 上写过另一段让人印象深刻的话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我从没像现在这样，作为一名程序员感到如此落后。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/10fa96922df70de2657c745ec1da795a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在那条 X 动态中，Karpathy 写道，这个职业正在被“剧烈地重构”，个人程序员贡献的代码行数正在变得越来越少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我有一种强烈的感觉：如果我能把过去大约一年里已经出现的这些工具真正串联、用好，我的能力可能会提升 10 倍，”他写道，“没能把这种增益拿到手，感觉明显就是技能问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“现在需要掌握的是一层全新的、可编程的抽象层（叠加在以往那些熟悉的抽象层之上）：涉及 agent、子 agent，它们的提示词、上下文、记忆、运行模式、权限、工具、插件、技能、钩子、MCP、LSP、斜杠命令、工作流、IDE 集成等。同时，还必须在脑中建立一个覆盖全局的心智模型，用来理解这些本质上随机、会出错、难以解释、而且不断变化的实体的优势与陷阱——而它们如今被突然掺进了原本那套‘老派而扎实’的软件工程体系之中。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一切更像是“一个强大的外星工具被直接发下来，却没有配套说明书”。“每个人都得自己摸索该怎么握住它、怎么操作它，而与此同时，一场 9 级地震正在撼动整个职业，”他写道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人说：“如果连他都觉得自己作为程序员已经大幅落后，那就很能说明我们现在处在什么阶段。”因为说这话的人是 Karpathy——长期被视为“走在最前面”的那类人：2015 年加入 OpenAI 成为创始成员之一，之后又很早投身自动驾驶，担任特斯拉 Autopilot 的 AI 负责人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72179b23ba3a11e35821afb2ed3caa80.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在评论区里，另一位重量级人物也表达了强烈共鸣。Claude Code 的核心作者、Anthropic 工程师 Boris Cherny 坦言，自己“几乎每周”都会有类似的感受。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他提到，有时会下意识按老办法去做，做着做着才突然反应过来：“等等，Claude 可能可以直接搞定这个。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近一次是在排查 Claude Code 的一个内存泄漏。他一开始走的是传统路径：连上 profiler、跑应用、暂停采样、再手动翻 heap 分配记录，一步步排查。但与此同时，他的一位同事处理同一个问题时，直接让 Claude 生成 heap dump，再让模型去读 dump，找出那些“本不该还被保留着”的对象。Claude 一次就命中问题点，顺手提了个 PR，把问题修掉了。“这种事几乎每周都会发生。”他写道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cherny 还补充了一个很有意思的观察：某种意义上，那些新入职的同事，甚至刚毕业的新人，反而更容易把模型用到位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为他们不会被“模型做不到什么”的旧印象束缚——那些印象大多是早期模型时代形成的“历史记忆”。而对已经形成使用习惯的工程师来说，每隔一两个月，就得花不小的心理力气去重新校准：模型现在究竟能做到什么——而且这个边界还在持续外扩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为软件工程正在发生根本性变化，而即便是他们这些最早的实践者，最难的部分依然是不断调整自己的预期——而这还只是开始。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Karpathy 则在评论里加了一个比喻：就像你拿着“激光枪”到处指，有时只打出一堆小弹丸，有时甚至会哑火；但偶尔，当你握对了姿势，一束强力激光会突然喷涌而出，直接把你的问题“熔掉”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;工具用顺手了后：“这是 20 年最大变化”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了今天，Karpathy 状态已经明显不一样：不再是“我跟不上”了，而是“我已经换了一种编程方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他用一种几乎夸张的方式描述了这种变化：过去 20 年形成的编程习惯，在短短几周内被打断；11 月还主要靠手写和自动补全，到了 12 月，已经变成大约 80% 的代码交给 agent，自己只做 20% 的修改和收尾。与此同时，他也给出了一个时间点上的判断：在他看来，Claude 和 Codex 大约是在 2025 年 12 月左右跨过了某种“一致性/连贯性门槛”，让 agent 编程从“偶尔好用”变成了“可以稳定纳入日常工作流”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条推文的评论区也一贯的热闹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很快就有人表示，这样的转变并不只是 Karpathy 一个人的感受。一位工程负责人在回复中写道，这和他的体验完全一致：真正让人意外的并不是速度提升，而是写代码这件事反而变得更有趣了。那些重复、机械的脏活累活被拿掉之后，剩下的更多是创造性的、值得投入精力的问题；而那些真正拥抱 AI 辅助开发的工程师，不只是变得更快，还开始尝试以前根本不会去尝试的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他引用 Karpathy 的一句话总结这种变化：“不要告诉它怎么做，给它成功标准，然后看它自己跑。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/77/77ae42db41d94ad24f187720326d67f1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有不少人盯住的是这组 80/20 的数字变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来这个比例只会继续上升，直到有一天我们几乎不再‘写’代码，而只是负责阅读和审查它。”还有人认为以后的瓶颈不再是打字速度，而是我们审查速度有多快，尤其是去识别那些“agent 幻觉出来却被推进生产分支”的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也势必会积累起“理解债”：因为审查 AI 写出来的代码太费劲，人会越来越倾向于“能跑就先过”，时间久了反而会对自己的代码库理解得越来越少。Karpathy 在评论中表示，他很喜欢“理解债务”这个词，虽然之前没见过，但觉得非常贴切；而且他也承认，这种诱惑确实存在——当 LLM 一次就把问题解决、而且看起来运行得还不错时，人真的很容易就想直接往下走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有人把这种变化说成一种“角色对调”：我们花了很多年学会写代码，现在更像是在当一个永不睡觉的实习生的项目经理——分派任务、验收结果、兜底风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，工具在变强，角色在重排，瓶颈也在迁移：从“写得快”，变成“看得懂、审得住”。而这一轮变化，显然还没到终点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/27/275966dd3a191de21743dfe518558677.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是他今天发布在 X 上的完整长文（按字面翻译，略作通顺处理）：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去几周我大量用 Claude 写代码，随手记几条零散想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;编程工作流&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着最近一轮 LLM 编码能力的明显提升，和很多人一样，我的工作方式在很短时间内发生了变化：11 月大概还是 80% 手写+自动补全 / 20% agent；到 12 月就变成 80% agent 编码 / 20% 人工改改、收尾润色。也就是说，我现在基本是在用英语“编程”——有点不好意思地用自然语言告诉 LLM 该写什么代码。自尊心多少会疼一下，但能用大粒度的“代码动作”去操控软件这件事，净收益实在太大了，尤其是当你适应它、把它配置好、学会怎么用，并真正想清楚它能做什么、不能做什么之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是我近二十年编程生涯里，对基础工作流影响最大的一次变化，而且它是在短短几周内发生的。我猜现在已经有两位数百分比的工程师也在经历类似的转变；但在更广泛的人群中，对这件事的认知可能仍只有个位数低位百分比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;IDE / agent 群 / 出错风险&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我看来，现在不管是“IDE 不再需要”的热炒，还是“agent swarm”的热炒，都有点过头了。模型当然还会犯错——如果是你真正关心的代码，我会建议你像鹰一样盯着它们：旁边开一个足够大的 IDE，用来随时检查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且错误的形态也变了：不再是简单的语法错，而是更隐蔽的概念性错误，有点像一个略显草率、匆忙的初级工程师会犯的那种。最常见的一类是：模型会替你做出一些错误假设，然后不核实就沿着假设一路跑下去。它们也不太会管理自己的困惑：不主动澄清、不揭示不一致、不提供权衡取舍、该反对时也不反对，而且还有点过度讨好。Plan mode 会好一些，但我感觉仍需要一种轻量的、内联的 plan mode。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它们也很容易把代码和 API 过度复杂化：抽象膨胀、架构臃肿、自己制造一堆 dead code 却不清理。它们能写出一个低效、臃肿、脆弱的 1000 行实现，然后就等你提醒一句：“呃……是不是其实可以更简单？”它们就会说“当然可以！”并立刻把它砍到 100 行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它们偶尔会作为副作用去改/删一些自己不喜欢、或没完全理解的注释和代码——哪怕这些内容和当前任务是正交的。即使我在 CLAUDE.md 里做了几次简单的指令尝试，这些问题仍会发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管有这些毛病，它依然带来巨大的净提升，而且很难想象再回到纯手工写代码的时代。TL;DR：每个人都有自己的新工作流；我现在的配置是：左边开少量几个 Claude Code 会话（Ghostty 的窗口/标签页里），右边开 IDE 负责看代码和手动改动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;韧性。看一个 agent 不知疲倦地死磕某件事真的很有意思。它们不会累，不会灰心，就是持续尝试——很多时候如果换成人，早就放弃、改天再战了。看它为一个问题挣扎很久，30 分钟后又突然赢了，那种“feel the AGI”的感觉很强。你会意识到：耐力本身就是工作的核心瓶颈之一，而 LLM 把这条上限显著抬高了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加速。LLM 辅助带来的“加速”其实不太好衡量。我当然感觉自己做原本要做的事更快了，但更大的变化是：我做了更多，原因主要是两点：&lt;/p&gt;&lt;p&gt;1）我可以写很多以前根本不值得写的东西；&lt;/p&gt;&lt;p&gt;2）我可以去碰以前因为知识/技能门槛而不敢碰的代码。&lt;/p&gt;&lt;p&gt;所以这当然是 speedup，但可能更像是一种“扩张”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杠杆。LLM 特别擅长反复循环，直到达到明确目标——大部分“feel the AGI”的魔法就在这里。与其告诉它怎么做，不如给它成功标准，然后看它自己跑。让它先写测试再通过；把它放进带浏览器 MCP 的闭环；先写一个很可能正确的朴素算法，再让它在保持正确性的前提下做优化。把你的指令从 imperative 转成 declarative，会让 agent 循环更久，从而获得更大的杠杆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;乐趣。我原本没预料到：用 agent 编程反而更有趣了，因为大量“填空式苦力活”被拿掉，剩下的更多是创造性部分。我也更少卡住（卡住真的不快乐），同时更有勇气——几乎总能找到一种方式与它并肩作战，推动事情向前。我也见过相反的观点：LLM 编程会把工程师分成两类——主要喜欢“写代码”的人 vs 主要喜欢“造东西”的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;退化。我已经注意到，自己手写代码的能力正在慢慢退化。“生成代码”和“判别代码（阅读/审查）”在大脑里是两种不同能力。因为编程里有大量偏语法的细碎细节，即便你写起来费劲，审代码通常仍能审得很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Slopacolypse（垃圾内容末日）。我已经在为 2026 做心理建设：那很可能是 GitHub、Substack、arXiv、X/Instagram，乃至整个数字媒体的“slopacolypse”（垃圾内容大爆发）之年。我们还会看到更多 AI 炒作式的生产力表演（这居然还能更夸张吗？），与此同时，也会出现真实而确凿的改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些问题。我脑子里的一些问题：“10X 工程师”会怎样？平均工程师与顶尖工程师的生产力差距，可能会被拉大很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 LLM 之后，通才会越来越超过专才吗？LLM 更擅长“填空”（微观）而不是“大战略”（宏观）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的 LLM 编程体验会像什么？像玩《星际争霸》？《Factorio》？还是演奏音乐？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社会中有多少领域，本质上被数字化知识工作所瓶颈住了？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TL;DR：我们现在处在哪？&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到 2025 年 12 月左右，LLM agent 能力（尤其是 Claude 和 Codex）似乎跨过了某种连贯性阈值，并在软件工程及相关领域引发了一次“相变”。现在，“智能”这部分突然显得明显领先于其他所有东西——工具与知识的集成、组织层面的新工作流与流程、以及更广泛的扩散机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 将是高能量的一年：整个行业都在消化、吸收这股新能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/karpathy/status/2004607146781278521&quot;&gt;https://x.com/karpathy/status/2004607146781278521&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/karpathy/status/2015883857489522876&quot;&gt;https://x.com/karpathy/status/2015883857489522876&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/eUXjPQdv3XRVqWwdzJO9</link><guid isPermaLink="false">https://www.infoq.cn/article/eUXjPQdv3XRVqWwdzJO9</guid><pubDate>Wed, 28 Jan 2026 04:10:16 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>AI 时代，开发者体验正在被重新定义</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI Copilot 和自主智能体的崛起正在重新定义开发者的意义。在 BUILD 2025 的一场重磅主题分享中，Vercel CEO 和 Next.js 的创始人&amp;nbsp;Guillermo Rauch&amp;nbsp;深入探讨了 AI 如何改变开发者的体验。AI 将我们的角色从编写代码转变为有意图地引导智能系统。在这个未来中，AI 原生工具将大大提升开发者的生产力、创造力和规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Guillermo Rauch 从自身经历出发，系统性地拆解了 AI 浪潮下，开发范式正在发生的结构性变化。这并非一场关于工具或技巧的演示，而是一份面向当代开发者的生存指南：当技术能力不再稀缺，什么才是决定长期价值的核心能力？他的判断很明确，我们正经历一场从“页面”走向“Agent”的转型，其深度与影响，堪比第一代互联网的诞生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“页面的云”到“Agent 的云”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rauch 回顾了 Vercel 的起点。2014 年，当开发者仍需在路由、编译器、集群和部署细节中反复挣扎时，真正能高效构建和交付产品的，只有少数拥有内部基础设施的大公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vercel 以及 Next.js 的诞生，本质上是一次“能力下放”，让原本只属于头部公司的开发效率，成为更广泛开发者的默认能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而今天，类似的分化正在 AI 领域重演。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他指出，支撑第一代 Web 的云基础设施，本质上是“为页面而生”的：优化加载速度、依赖 CDN、围绕一次性请求与响应展开。但在 Agent 驱动的应用形态中，这套逻辑开始失效。新的应用形态不再以页面为中心，而是以持续运行、长时间“思考”、多步骤编排的智能体为核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，云的目标也随之发生变化：&lt;/p&gt;&lt;p&gt;算力不再只追求“更快返回”，而是要支持更长时间、更复杂的推理；基础设施不再围绕页面分发，而是围绕“token 流动”和智能调用；搜索和触达不再依赖排名，而是通过Agent主动嵌入到用户的工作场景中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b6/b601d563855792e2f91404ecde1a862d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一意义上，他将新的基础设施形态称为“面向 Agent 的云”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;界面没有消失，只是变成了生成式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与基础设施变化同步发生的，是用户界面的根本转型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统的软件界面是确定性的：开发者可以精确控制每一个像素，预期用户将看到什么、点击什么、进入哪条路径。但在 AI 时代，界面正在变成生成式和自适应的，按需生成、随上下文变化，并高度个性化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rauch 特别强调，这并不意味着前端或用户体验的重要性下降，恰恰相反，体验比以往任何时候都更重要。变化的只是如何构建体验：从事先设计好所有界面，转向在需要时即时生成最合适的呈现方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他以 Snowflake 与 Vercel 的生成式 UI Agent（V0）的集成为例：复杂的数据分析结果，可以在对话中即时生成可视化界面，让非技术用户也能理解。这背后的趋势，是行业从“代码优先”逐步迈向“代码后置”，代码不再是价值本身，而是服务于结果的一种中间形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当谁能写代码不再重要&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说基础设施和界面的变化，重塑了怎么构建，那么更深层的变化在于——谁可以参与构建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rauch 认为，软件开发正在从一项高度专业化的技能，转变为一种普遍能力。过去，门槛在于语法、工具链、基础设施；而现在，门槛正在转移到一个新的维度：意图是否清晰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 的帮助下，表达能力本身成为新的基础素养。会不会写代码，正在被能否准确表达你想要什么所取代。他将这种变化视为 JavaScript 普惠化的升级版：如果说框架和平台曾让大量前端开发者成为云工程师，那么 AI 则让更多非技术背景的人，第一次具备了构建软件的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种背景下，个人技能的价值排序正在被重写。Rauch 提出了一个残酷但现实的判断：不要过度依附于某一项具体技能。正如心算曾经是优势，但最终被机器超越，编程技能也正在进入类似阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正重要的，是他所说的“元技能”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;交付，正在成为最核心的元能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 可以生成无限方案的时代，开发者真正需要承担的角色，不是代码执行者，而是判断者与交付者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rauch 将“交付（Shipping）”视为当代开发者最关键的元能力。它不等同于写代码，而是一种端到端的综合能力：从问题判断、产品设计、实现方式，到测试、迭代、讲清楚价值，并持续提高标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，AI 不只是提升效率的工具，更应该服务于质量。他反复强调一个立场：更快地生成代码，不应成为降低交付标准的借口。相反，真正的竞争力在于，同时提高生产力与质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一判断，他给出了具体的实践方向：将 Agent 用于客服、风控、内容生成、数据分析等场景，让系统承担重复性工作，把人的时间留给真正影响产品和业务走向的决策。随着系统从“被接受、被修改、被拒绝”的反馈中不断学习，质量门槛本身也会随时间抬升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，他将 Agent 的意义概括为一句话：消除创意与现实之间的壁垒。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这场分享的结尾，Rauch 并未给出下一步该学什么工具的清单，而是抛出了一个更本质的问题：当工具已经就位、模型已经成熟，你真正要思考的，是你准备交付什么样的产品、什么样的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 时代的开发者体验，不再只是写更少的代码，而是能否以更高的标准，把想法真正带到现实世界中。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Tb4GkF4MbNOxxhOHvZig</link><guid isPermaLink="false">https://www.infoq.cn/article/Tb4GkF4MbNOxxhOHvZig</guid><pubDate>Wed, 28 Jan 2026 03:53:49 GMT</pubDate><author>王玮</author><category>云计算</category><category>AI&amp;大模型</category><category>Snowflake</category></item><item><title>前端同事看走眼了，这个“游戏网页”其实全是AI写的！</title><description>&lt;p&gt;Vibe Coding的进化速度，可能还是超乎了我们的想象。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，我们在测试Kimi K2.5的网页生成功能时，旁边的前端开发同事还以为是真实的网页场景，低声问我：“你这是在写代码吗，还是在摸鱼打游戏？”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直到我说出这是AI生成的，而且是只用了几句话就做出来的效果，这让她大为惊讶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该网页长这样，现在如果不明说的话，确实已经难辨“真假”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kimi K2.5在今天刚刚上新，它没有把重点放在“单项能力突破”上，而是试图把视觉理解、代码生成、交互设计，以及多Agent协作，都压进了同一个模型里，一口气提供了四种使用模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在笔者看来，其中最有意思的，当属Agent集群模式——这也是在国内AI上第一次出现的功能，它可以让原本耗时数天的工作，现在仅需十几分钟就能做完，简直是指数级的提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，要做100家公司的市场调研，它能指挥一群不同行业背景的“分析师”分头行动，十几分钟出结果，而不是几个星期；面对300页的复杂翻译项目，它能动员一个“语言学专家”团队，快速、准确地完成交付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;四种模式具体如下。不同需求的用户，从随手一问，到需要并行推进的复杂任务，都能找到明确的入口：&lt;/p&gt;&lt;p&gt;快速模式，提供最快的响应体验。思考模式，可以用来解答复杂问题。Agent模式，擅长深度研究、PPT、Excel、Word、PDF和网页生成等任务——目前K2.5已经开始掌握 Office 套件的核心技能，其协助办公的能力不容小觑。重磅全新模式：Agent集群模式，适合需要并行处理的复杂任务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6e/6e504dcfe555d613728d4afa5e038740.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，新编程产品Kimi Code不仅能直接在终端里运行，还能无缝集成到VSCode、Cursor、Zed这些IDE里，支持直接输入图片和视频。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;月之暗面CEO杨植麟，这次亲自为新模型发布录制了视频。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Kimi K2.5实测&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;看起来很强是一回事，那用起来是不是另一回事？以下是各种实操案例，InfoQ也上手测了几组。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;几分钟搓出前端网页，能修改细节、还能有声音&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了测试Kimi K2.5的视觉理解能力和Vibe Coding水平，我们首先直接甩出一张产品页面截图，再配上几句文字描述，看看它能不能自己看懂、自己理解，顺手还能复刻出一个像模像样的产品页面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如让K2.5做个一个最近很火的心灵疗愈类项目，给的Prompt如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模仿情绪疗愈类产品，生成一个情绪记录类APP，适合年轻人释放情绪，让人一眼觉这里允许脆弱的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，这个Prompt提示不多，要求不少，对模型视觉理解能力、逻辑思维、产品思维以及设计审美能力都是考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从结果看，K2.5对“情绪”这个概念本身是有一定理解和思考的。它生成的是一个以沉浸体验为核心的情绪页面，而不是常规的情绪记录工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;视觉上，明显没走浅色卡片流那条老路，而是用了低对比背景、连续画面和节奏型动效（类似呼吸或旋涡），交互重点放在“停留”和“进入状态”上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能组织上，输入、反馈和过渡是连在一起的：用户不是“点一个按钮开始记录”，而是被自然引导进入输入状态——这种设计说明它在生成时已经考虑了状态流转，而不是只输出一个静态页面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我们不再给任何视觉参考，只输入文字提示，让 K2.5独立完成整个网页设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们给的Prompt很简单：&lt;/p&gt;&lt;p&gt;做一个类似 4399 的小游戏平台，要有完整的游戏分类频道； 但视觉审美要大厂级、高端网游风，整体要酷炫、有冲击力，并且可交互。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果Kimi K2.5没让人失望。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它给出的页面并不是“看起来像网页”的静态效果，而是已经具备明确产品结构的原型。相比以往很多生成结果只停留在大色块 + 随机模块的拼接，它能正确理解“小游戏平台”这一产品类型，在首页层面同时给出清晰的分类入口、内容推荐区和主视觉焦点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;视觉风格上，它没有沿用早期生成工具常见的“低饱和扁平模板”，而是接近成熟网游官网或内容平台的布局逻辑，这一点与一些真实产品如大型游戏平台的信息层级更为接近。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更关键的是，这种效果并非通过多轮细化 Prompt 得到，而是在一次相对抽象的指令下完成，说明模型已经开始具备从“需求描述”直接映射到“产品级页面结构”的能力，而不只是做样式渲染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;类似的例子还有不少。下面这些网页，都是K2.5在图像生成工具的辅助下，仅凭一条Prompt直接生成的完整原型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了做整个页面，我们还单独测评了一下K2.5对动效的理解能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;左侧是我们输入的一段小视频，右侧是它生成的效果。结果K2.5几乎是完整复刻，拖动鼠标，图片会随之产生位移变化，逻辑和节奏都对得上，动效也足够丝滑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/9098ce1d04c226bee40d735d90e7f3c8.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也就是说，K2.5并不是在“画动效”，而是真的理解了交互在时间维度上的设计意图。&lt;/p&gt;&lt;p&gt;对开发和设计而言，这意味着动效不再从一堆参数和曲线开始，而是可以先把想法直接跑成一个可交互的原型，用几分钟看清值不值得投入工程成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;以前要干好几天的活，十几分钟就能搞定&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于K2.5的Agent集群模式，最直观的能力就是：把时间尺度直接拉短了。过去需要“按天算”的复杂任务，现在往往 十几分钟就能跑完一整轮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来看一个实测例子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一次性向Kimi 的 Agent 集群投喂了40篇论文，主题横跨心理学与AI。任务是，在此基础上产出一份系统性的研究综述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kimi的处理流程大致分成了三步：第一步，完整通读。主 agent 多次调用工具，按顺序把 40 篇论文逐篇过了一遍，确保所有关键信息都被纳入同一上下文，而不是零散记忆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二步，并行写作。在理解整体结构后，Kimi 自动派生出多个子 agent——可以理解为它的“分身”，分别负责不同章节的撰写，各自并行推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三步，统一收敛。主 agent 最后回到台前，负责校对、取舍和整合，把各个子 agent 的成果汇总成一份长达几十页的专业 PDF级综述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个过程里中，几乎看不到人工干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;##当Transformer开始吃力，K3可能用上原创架构KDA&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们先后测评了一整天，总体感受很明确：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kimi K2.5 在自己擅长的多个方向上，已经跑得相当顺了。比如网页设计生成、动效理解、多Agent 协作等场景，完成度和稳定性都比较成熟；不过也有短板，比如在 3D 建模这类强几何约束的任务上，表现还欠佳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当这些能力被一项项跑出来之后，更现实的问题也浮现出来：如果这些复杂推理真的要被当成日常能力反复调用，底层的计算方式还能不能长期扛得住？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;月之暗面给出的一个解法，是Kimi Linear，而Kimi Linear中的一个核心创新点，是一个新的实验性架构：KDA（Kimi Delta Attention），一种线性注意力模块的相关思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨植麟此前在Reddit上的AMA（Ask Me Anything）等公开交流中已经透露，下一代K3模型，可能会使用月之暗面的这个新架构KDA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4dc70a6dfd6e65081d1a82baa23f8557.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要讲清楚KDA的优势，我们还得先从Transformer架构说起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本质上，Transformer的注意力机制是全连接的：每个 token 都要和上下文里的其他 token 打一次交道。结果，输入一长，计算量就按平方增长（O(N²)）；生成新 token 时，还要不断回查之前的 KV Cache。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当上下文一拉长，显存压力迅速飙升，尤其是在 128K以上的场景里，几乎是“显卡先崩，钱包随后”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;——而且模型越强，这个问题就越明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正因为如此，过去几年里，线性注意力一直是业内反复被拿出来讨论的一条路：把注意力计算从 O(N²) 压到 O(N)，让模型跑得更快、也更省。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但现实是，早期不少线性注意力方案确实快了，却很难兼顾记忆能力：信息留不住，推理质量也跟着打折。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而KDA核心思想可以概括为一句话：不再每次都“全量算一遍注意力”，而是每次只计算“状态 + 增量（Delta）更新”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里的 Delta（增量） 是关键。它在数学上保证了稳定性，即使是在百万级token序列中，梯度也不会爆炸或消失。这也让Kimi Linear能在超长上下文中跑得稳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在保持模型能力的同时，还可以显著降低长上下文和连续推理的计算成本——思路有点像MoE架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;##One more thing&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在测试Kimi K2.5的视觉理解能力时，我们索性出了一道“狠题”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;——甩过去一段动画，让它先吃透画风和叙事方式，再换个主题，重写一支动画脚本。说实话，这活儿对专业动画师都不轻松，我们还特意把 “Agent集群”模式打开了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果最有意思的不是生成内容本身，而是页面最底下那行小字：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“这个任务Kimi自己就能完成，不需要 Agent集群。部分额度已退回。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cc/cc8306f37abefc1202d26901c3acf5df.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;体验传送门：https://www.kimi.com/&lt;/p&gt;</description><link>https://www.infoq.cn/article/MIsUL6Sg2SxAfFrGg4r7</link><guid isPermaLink="false">https://www.infoq.cn/article/MIsUL6Sg2SxAfFrGg4r7</guid><pubDate>Wed, 28 Jan 2026 03:26:09 GMT</pubDate><author>木子,高允毅</author><category>AI 工程化</category></item><item><title>九章云极推出 DART-GUI-7B 模型，基于 Alaya NeW Cloud 强化学习云训练，登顶 OSWorld 7B 榜首！</title><description>&lt;p&gt;2026 年 1 月，在操作智能领域权威评测体系 OSWorld 发布的最新榜单中，九章云极 DataCanvas 凭借在 Alaya NeW Cloud 强化学习平台上训练的 DART-GUI-7B 模型，以卓越的智能操控表现，一举夺得 OSWorld 7B 赛道冠军！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;九章云极：Alaya NeW Cloud 强化学习平台&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Alaya NeW Cloud 是由九章云极打造的以强化学习（ Reinforcement Learning, RL ）为核心能力的智算云平台，该平台通过将强化学习能力深度融入底层基础设施，重构了智能计算的架构与逻辑，旨在为企业和开发者提供“可用、好用、经济”的算力资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Alaya NeW Cloud 打造前沿强化学习云平台，平台原生支持一键式 Agentic RL 开发环境启动 、分布式极核 Agentic RL 训练，性能上实现训推分离与全流程加速，生态上预置多种主流 Agent 仿真环境，高效支撑强化学习技术的快速落地与创新突破，精准解决 AI 技术应用中的效率和成本等核心问题。目前，九章云极已在全球布局多个聚焦于加速计算优化的 AIDC 智算中心，持续赋能 AI 技术的高效应用与行业规模化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/e6/52/e66b0d3dfb3b50dd60174b4116b59052.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;DataCanvas Alaya NeW Cloud&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;核心技术解读：轻量化模型的 GUI 智能体突破&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;什么是 OSWorld？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OSWorld 是目前 AI 领域衡量 “智能体（ Agent ）跨软件操作电脑” 能力最顶尖的基准测试，它模拟真实的操作系统环境，要求 AI 像人类一样通过视觉观察屏幕，并精准操控浏览器、Excel 、VS Code 等各类桌面应用来完成跨平台的复杂任务，被 OpenAI 、Anthropic 、字节跳动 Seed 、月之暗面、智谱等顶尖 AI 团队广泛采用，更是检验 AI 能否从“只会聊天”进化为“高效数字员工”的硬核试金石。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么 OSWorld 对 7B 模型几乎是“地狱难度”？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真实生态：任务在 VS Code 、LibreOffice 等真实软件中运行，环境信息密度远超结构化数据闭环操控：需要连续理解截图、规划路径和进行键鼠操作，考验长程推理能力零容错率：限时 30 步，操作需步步为营，失败不可逆转数据稀疏：基础成功率不足 1/4，即使是大模型也面临严峻挑战&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;复杂的跨软件协作与精细的坐标控制，使得参数规模有限的 7B 模型在“理解”与“执行”之间难以调和，长期处于“不可用”状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;核心技术路径：九章云极三大创新赋能轻量化突破&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;1. 核心方法：解耦式 GUI 智能体强化学习框架&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;九章云极并未通过简单扩大模型规模取胜，而是选择了系统级的算法创新。提出了 DART（ Decoupled Agentic Reinforcement Training ），首次将 GUI 智能体的强化学习训练流程彻底解耦为四个异步模块：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/75/9b/75cfe7bfe327a378d65e4f9ecfe56b9b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三项关键突破&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;推演级轨迹调度（ Rollout-Level Scheduling ）：&lt;/p&gt;&lt;p&gt;以“单条轨迹”作为调度最小单位；&lt;/p&gt;&lt;p&gt;每个 rollout 完成后立即释放环境并启动下一个任务；&lt;/p&gt;&lt;p&gt;环境利用率提升从 12.2% 达到 67.7%，提升幅度高达&amp;nbsp;5.5&amp;nbsp;倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;动态模型服务池（ Dynamic Model Serving Pool ）：&lt;/p&gt;&lt;p&gt;采用 GPU 推演的集中化管理，支持多模型版本的热加载；&lt;/p&gt;&lt;p&gt;避免了传统“一卡一环境”的资源浪费；&lt;/p&gt;&lt;p&gt;GPU 推演利用率提升 1.6 倍；&lt;/p&gt;&lt;p&gt;GPU&amp;nbsp;资源的并发弹性扩展能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练与推理异步执行（ Asynchronous Execution of Training and Inference ）：&lt;/p&gt;&lt;p&gt;训练与推演实现异步解耦；&lt;/p&gt;&lt;p&gt;避免模型更新导致服务阻塞。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;2. 数据策略：四层自适应筛选，放大稀疏成功信号&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对 GUI 强化学习中的“成功少、噪声多”核心难题，DART 设计了覆盖任务、轨迹、步骤和 Token 的四层筛选机制：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一机制使得 7B 模型，在最大 30 步内，即可稳定的实现 OSWorld 中的任务要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;3. 多维优化：以轻量化参数对冲复杂场景，重塑性能边界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;九章云极经过强化学习训练的 7B 模型之所以能实现突破，关键在于采用了“场景适配、精度优化、算力协同”的三维技术方案，在控制参数量的同时，最大化提升操作智能性能：&lt;/p&gt;&lt;p&gt;场景化指令对齐技术：基于百万级真实操作场景数据训练，构建细分领域的指令库，优化模型对办公自动化、数据处理等高频场景的语义理解能力，精准捕捉模糊指令背后的核心需求，使指令理解准确率较通用模型提升 23 %，并减少无效操作；混合精度推理优化：借鉴智算硬件优化经验，对模型不同模块进行精度分层处理。核心推理模块保留 FP16 精度以确保准确性，非核心模块量化至 INT8 精度。这一调度方式实现推理效率提升 1.8 倍，资源占用率降低&amp;nbsp;40 %；软硬件协同调度机制：依托自研的智算技术栈优势，深度协同模型推理与算力资源，动态调整算力分配策略以应对负载波动，避免资源闲置。同时使用专用推理加速引擎优化 GUI 元素识别与动作规划的计算链路，进一步降低轻量化模型的推理延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实验结果：全类型任务下性能优势显著&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/cf/fb/cf000df80949d83be3304172a86be2fb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在最大步长仅有 30 步的情况下，DART-GUI-7B 在多种任务类型上表现出显著优势，包括：&lt;/p&gt;&lt;p&gt;浏览器类（ Chrome ）；图像/设计类（ GIMP ）；邮件客户端类（ Thunderbird ）；代码/ IDE 类（ VS Code ）；操作系统交互类（ OS ）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;亮点：GIMP 类任务的正确率高达 80.77 %，且在办公套件（ Impress、Writer、Calc ）、媒体播放类（ VLC ）以及多应用协同等任务中，其能力也有显著提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;九章云极还进行了真实场景的验证。在 DataCanvas Alaya NeW 平台上，DART-GUI-7B 成功地通过键鼠操作完成文档查找、导航到指定页面及查找官网联系方式等场景任务，其成功率超过 90 %。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;产业价值与未来展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，AI 大模型正加速从“技术验证”向“产业落地”转变。通用人工智能作为连接数字世界与物理操作的重要工具，在办公自动化、智能运维和工业控制等领域展现出广阔的应用前景。然而，模型部署成本高、轻量化模型性能不足及数据出域安全等问题，仍然是产业规模化的关键瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;九章云极的 7B GUI 模型突破为行业提供了“低成本、高性能”的通用人工智能解决方案，有望推动通用人工智能在中小企业及长尾场景的普及。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Hpcx4uk6LvFKo4FRfNHQ</link><guid isPermaLink="false">https://www.infoq.cn/article/Hpcx4uk6LvFKo4FRfNHQ</guid><pubDate>Wed, 28 Jan 2026 03:25:58 GMT</pubDate><author>九章云极</author><category>操作系统</category></item><item><title>蚂蚁灵波发布具身“通用大脑”LingBot-VLA，后训练代码全面开放，让开源真正可用</title><description>&lt;p&gt;继昨日开源高精度空间感知模型&amp;nbsp;LingBot-Depth后，蚂蚁集团旗下灵波科技今日宣布全面开源具身大模型LingBot-VLA。作为一款面向真实机器人操作场景的“智能基座”，LingBot-VLA 实现了跨本体、跨任务泛化能力，并大幅降低后训练成本，推动“一脑多机”走向工程化落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在上海交通大学开源的具身评测基准GM-100（包含 100 项真实操作任务）测试中，LingBot-VLA 在 3 个不同的真实机器人平台上，跨本体泛化平均成功率相较于&amp;nbsp;Pi0.5 的 13.0% 提升至 15.7%（w/o Depth）。引入深度信息（w/&amp;nbsp;Depth）后，空间感知能力增强，平均成功率进一步攀升至17.3%，刷新了真机评测的成功率纪录，验证了其在真实场景中的性能优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99cd79a2d8f8cf9ba5f5be230172d667.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：在GM-100 真机评测中，LingBot-VLA 跨本体泛化性能超越 Pi0.5）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在RoboTwin 2.0 仿真基准（包含50项任务）评测中，面对高强度的环境随机化干扰（如光照、杂物、高度扰动），LingBot-VLA 凭借独特的可学习查询对齐机制，高度融合深度信息，操作成功率比Pi0.5 提升了9.92%，实现了从虚拟仿真到真实落地的全方位性能领跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/83016187cb7cf475992926ec01e8d4f5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：在RoboTwin 2.0 仿真评测中，LingBot-VLA 跨任务泛化性能超越 Pi0.5）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期以来，由于本体差异、任务差异、环境差异等，具身智能模型落地面临严重的泛化性挑战。开发者往往需要针对不同硬件和不同任务重复采集大量数据进行后训练，直接抬高了落地成本，也使行业难以形成可规模化复制的交付路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对上述问题，LingBot-VLA 基于20000+&amp;nbsp;小时大规模真机数据进行预训练，覆盖了9&amp;nbsp;种主流双臂机器人构型（包括AgileX，Galaxea R1Pro、R1Lite&amp;nbsp;、AgiBot G1等），从而让同一个“大脑”可以无缝迁移至不同构型的机器人，并在任务变化、环境变化时保持可用的成功率与鲁棒性。与高精度空间感知模型LingBot-Depth配合，LingBot-VLA 能获得更高质量的深度信息表征，通过“视力”的升级，真正做到“看得更清楚、做的更明白”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LingBot-VLA 凭借扎实的基座能力，大幅降低了下游任务的适配门槛，仅需80 条演示数据即可实现高质量的任务迁移。此外，配合底层代码库的深度优化，其训练效率达到&amp;nbsp;StarVLA、OpenPI 等主流框架的&amp;nbsp;1.5~2.8 倍，实现了数据与算力成本的双重降低。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次开源不仅提供了模型权重，还同步开放了包含数据处理、高效微调及自动化评估在内的全套代码库。这一举措大幅压缩了模型训练周期，降低了商业化落地的算力与时间门槛，助力开发者以更低成本快速适配自有场景，模型实用性大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;蚂蚁灵波科技CEO朱兴表示，“具身智能要想大规模应用，依赖高效的具身基座模型，这直接决定了是否可用以及能否用得起。我们希望通过LingBot-VLA的开源，积极探索具身智能上限，推进具身智能研发早日进入可复用、可验证、可规模化落地的新阶段，让AI加速在物理世界渗透普及，更早的服务每一个人。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LingBot-VLA是蚂蚁开源的第一款具身智能基座模型，也是蚂蚁在AGI研发上又一探索性成果。朱兴介绍，蚂蚁集团坚定以开源开放模式探索&amp;nbsp;AGI，为此打造&amp;nbsp;InclusionAI，构建了涵盖基础模型、多模态、推理、新型架构及具身智能的完整技术体系与开源生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-VLA的开源，正是InclusionAI的关键实践。“期待携手全球开发者，加速具身智能技术的迭代与规模化应用，助力&amp;nbsp;AGI&amp;nbsp;更快到来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，在数据采集阶段，LingBot-VLA 使用了星海图、松灵的硬件平台，乐聚、库帕思、国家地方共建人形机器人创新中心、北京人形机器人创新中心有限公司、博登智能、睿尔曼也在模型预训练阶段提供了高质量数据支持。目前，LingBot-VLA 已与星海图、松灵、乐聚等厂商完成适配，验证了模型在不同构型机器人上的跨本体迁移能力。&lt;/p&gt;</description><link>https://www.infoq.cn/article/GaVLwadOyTqICWJ7uiGV</link><guid isPermaLink="false">https://www.infoq.cn/article/GaVLwadOyTqICWJ7uiGV</guid><pubDate>Wed, 28 Jan 2026 02:18:41 GMT</pubDate><author>蚂蚁集团</author><category>具身智能</category></item><item><title>模力工场 030 周 AI 应用榜：字节新品硬刚 Sora，“随变”登顶榜首</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260126infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;想用一个下午快速摸清一个领域，并产出一份条理清晰、信息量丰富的深度内容？本周模力工场带你体验 “AI 增效流水线：从信息到作品的智能生产工作流”。从智能阅读提炼（语鲸）、一键生成研报（AI 快研侠），到跨平台记忆管理（MemOS-MindDock），再到自动视觉设计，这条流水线覆盖“读、写、研、记、设计”全流程，助你将碎片信息快速整合为结构化的知识作品。例如，若你对近期热议的 Clawdbot 等AI助手产品感兴趣，不妨以此为主题，用这套工作流实践一番。点击进入模力工场首页，查看顶部专题横幅，扫码添加模力小A，获取完整工具链与实操指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/57/574f118703932447159e822207d9f545.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;030 周上榜应用精选（附用户热评）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260126infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;第 030 周 AI 应用榜来袭！本周共有 32 款应用上架，榜单完全由用户真实使用、测评与讨论热度驱动。我们从社区声量最高的应用中精选出十款，并透过用户真实评论，为你解读榜单背后的产品逻辑与行业风向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;创作平民化：人人都能成为内容创作者&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/suibian?utm_source=20260126infoQ&quot;&gt;随变&lt;/a&gt;&quot;：潮人必备 AI 创作神器，让灵感瞬间变潮流短片！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”玩了几天随变，感觉有点像简洁版抖音…但 AI 创作出来的视频，如‘创作一条刀马刀马的舞蹈片段’它会理解为元素中有刀有马，BGM 也毫不相干…这显然是开盲盒，会消耗创作者热情。希望引入更多‘悦己’的功能。”【用户热评｜@MATTHEW】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/PixVerse?utm_source=20260126infoQ&quot;&gt;PixVerse&lt;/a&gt;&quot;：秒出电影感视频，视觉叙事交给 AI。&lt;a href=&quot;https://agicamp.com/products/xyqjianying?utm_source=20260126infoQ&quot;&gt;小云雀&lt;/a&gt;&quot;：一句话生成爆款，创作门槛降至零。&lt;a href=&quot;https://agicamp.com/products/whee?utm_source=20260126infoQ&quot;&gt;WHEE&lt;/a&gt;&quot;：一站式视觉工坊，生图改图扩图，创意无限延伸。&lt;a href=&quot;https://agicamp.com/products/singduck?utm_source=20260126infoQ&quot;&gt;唱鸭&lt;/a&gt;&quot;：零基础玩音乐，AI 帮你谱写生活旋律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;学习力升级：AI 正在重塑教辅软件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/qianwenzhixue?utm_source=20260126infoQ&quot;&gt;千问智学&lt;/a&gt;&quot;：全科 AI 辅导，教材全覆盖，答疑效率翻倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”千问智学高度契合我心中对答疑辅导类学习软件的期待。功能清晰划分为学习智能体、提分助手、宝藏资料、职业考试几大板块，每个分类还结合适用年龄、所学专业、所在地区等不同维度，打造了针对性的内容与服务…综合体验可以给到 9 分的高分。”【用户热评｜@Abin】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/doubaoaixue?utm_source=20260126infoQ&quot;&gt;豆包爱学&lt;/a&gt;&quot;： 随身 AI 家教，拍题秒出思路，学习难题不再怕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;智能自动化：从被动回答到主动执行&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/atoms?utm_source=20260126infoQ&quot;&gt;Atoms&lt;/a&gt;&quot;：多 AI 团队协作，想法闪电变产品原型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”用 Atoms 搭了一个族谱显示的网站...最戳我的是它的层级数据可视化功能，族谱的家族分支、辈分脉络展示得一目了然，不用自己折腾结构设计。而且全程打字输入需求就行，不用写一行代码，平台会自动匹配开发需求，内置多个角色也比较好用，做出来的族谱网站的展示效果整体合预期（有一些样式生成的没处理好，显示会重叠）。整体来说对新手很友好，搭建网站的核心需求都能完美满足，小细节的不足完全不影响基础使用，作为零代码工具来说很靠谱了。“【用户热评｜@墨鱼罐头】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/offerkuai?utm_source=20260126infoQ&quot;&gt;offer快&lt;/a&gt;&quot; 📍北京：AI 求职分身，智能匹配+自动投递，轻松拿下好工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;心理疗愈：不仅是应用，更是思考伴侣&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/metasight?utm_source=20260126infoQ&quot;&gt;MetaSight 元见&lt;/a&gt;&quot; 📍杭州：命运AI投射仪，换个视角看清人生路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;”我其实很好奇这个领域的 AI 应用能发展到什么程度。之前试用时，我只是输入了自己当前的工作状态、心情和年龄，系统就帮我推算出了未来的发展方向和行动建议…如果这类应用未来能结合 IoT 硬件，可能会真正引爆市场。目前应用面向的用户群中包含不少中老年人，他们对这类能根据现状推理出下一步计划与发展方向的功能，需求其实非常迫切。”【用户热评｜@.】&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;榜单之外但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/kouzi?utm_source=20260126infoQ&quot;&gt;扣子（2.0版本）&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：AI 职场助手｜流程自动化｜智能办公&lt;/p&gt;&lt;p&gt;模力小A推荐：专为职场人打造的智能效率伙伴，能帮你自动处理会议纪要、邮件撰写、日程安排等重复工作，让 AI 真正成为你的“数字同事”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用名称：&lt;a href=&quot;https://agicamp.com/products/ViduAIMV?utm_source=20260126infoQ&quot;&gt;Vidu AI MV&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;关键词：一键成片｜AI MV 生成｜影音创作&lt;/p&gt;&lt;p&gt;模力小A推荐：只需上传图片和音乐，即可自动生成拥有专业转场、节奏卡点和电影级质感的音乐短片，让普通人也能轻松打造专属 MV。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本周上榜的一批 AI 应用呈现出几个非常鲜明的趋势：创作力提升、学习力强化、智能自动化与心理疗愈并行发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在创意生产与多媒体内容创作方向，我们看到像随变、PixVerse、小云雀、WHEE、唱鸭这样的应用迅速聚焦 AI 驱动的视觉与音频内容生成，从“一句话生成爆款内容”、秒级视频产出，到图像改图、一站式创作体验，AI 正在让个人创作者从繁琐操作中解放出来，把灵感瞬间转为可传播的成果。这与行业趋势一致：AI 正在大规模重塑创意产业和内容生产流程，创作者不再受制于传统软件约束，而能借助 AI 助力快速迭代与表达创意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在教育与知识服务方向，豆包爱学、千问智学等产品体现了 AI 在学习辅导领域的深化应用，它们通过拍题讲解、教材覆盖的智能辅导模式，正在将 AI 从“工具助手”升级到“学习伴侣”，这与全球教育领域推动 AI 个性化辅导、提升学习效率的大趋势不谋而合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，AI Agent 与自动化服务型工具（如 Atoms、offer快、MetaSight）正在形成一股新潮流。Atoms 体现了多智能体协作快速将想法变成可用产品的能力；offer快则将 AI 直接介入求职流程中，实现岗位筛选、沟通跟进与自动申请；MetaSight 则尝试把 AI 带入命理解读与人生洞察场景，让智能体具备不仅执行任务、还促发用户自我思考的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合来看，本周上榜的 AI 应用不仅覆盖了内容创作、学习辅导、个性化洞察和流程自动化等核心领域，还共同凸显了一个核心趋势：AI 正在从“简单生成”向“深度交互与高效执行”转变，让用户的生产力、学习效率和生活智能都进入一个新阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;One more thing，&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场将亮相 OceanBase 社区嘉年华！诚邀您加入我们的上海现场展位。作为 OceanBase 合作的创新社区，模力工场将于 1 月 31 日 登陆上海社区嘉年华，并拥有专属展位。这不仅是一次技术交流——我们更希望和你一起，在现场用 AI Coding 展现创造力、在开放麦分享你的项目故事、与行业先锋面对面切磋、在开源市集交换灵感。我们为你预留了专属席位，期待与你共同呈现：当开源精神遇上 AI 创造力，能碰撞出多少令人惊艳的可能。立即报名，锁定与数百位技术同行深度连接的一天！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c9fd51971f15feb3d78bbcd868faa640.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Hiv4SCiXtQMuJ8d9GDCM</link><guid isPermaLink="false">https://www.infoq.cn/article/Hiv4SCiXtQMuJ8d9GDCM</guid><pubDate>Tue, 27 Jan 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>构建下一代 AI 系统：可信生成式 AI 的工程蓝图</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着生成式 AI 从原型走向生产环境，真正的挑战已不仅是模型质量，更是要构建能被团队大规模信任的系统。开发人员和数据工程师被要求交付的不仅仅是能够生成答案的智能助手、Agent 和辅助工具，还必须确保这些系统在可靠性、安全性和与业务逻辑的一致性方面做到尽善尽美。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次分享中，Snowflake 首席数据与分析官&amp;nbsp;Anahita Tafvizi&amp;nbsp;凝练了来自 Snowflake 自身实践与行业顶尖构建者的深度交流，系统拆解了可信生成式 AI 的技术架构要点，内容包括：&lt;/p&gt;&lt;p&gt;语义层如何防止幻觉并保持一致性；为什么可观察性和评估框架生产级 AI 的必备要素；在开源与专有模型中实施权限管控、数据沿袭和可测试性；数百个企业用例中常见的开发模式与反模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47d31b2066312a28c3a41b6d8ec4e76a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论您正在将智能体扩展至生产环境，还是仅验证试点项目，这场分享都将为构建不仅智能、而且可信、安全且可复用的生成式 AI 系统提供前瞻性蓝图——为持久化的 AI 奠定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;信任，是 AI 能否走向生产的分水岭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 的实践中，构建一个“能回答问题”的 AI Agent 并不困难，真正困难的是构建一个输出高度准确、可被团队信任、并能据此采取行动的 Agent。这一难点在数据与分析类场景中尤为突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分析型 AI 的输出往往具有唯一正确答案，例如季度收入、增长率或关键指标。一旦系统在这些问题上给出哪怕一次错误结果，信任便会迅速崩塌，用户会回退到 SQL 查询或电子表格——因为在那里，他们至少能够自行追溯逻辑、验证结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享中反复强调了一个行业普遍存在却容易被忽视的问题：分析幻觉（Aalytics Hallucination）。即 AI Agent 给出了看似合理、引用了正确表格与来源、但最终却是错误的数值。在分析场景中，这类错误的破坏性远高于一般生成式应用，也正是许多 AI 系统“上线即沉默”的根本原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一个三层结构的“信任工程”框架&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于 Snowflake 自身从原型到生产的实践经验，团队将“信任”总结为一个需要被工程化设计的系统能力，而非单一功能，并提出了一个由三层组成的框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一层：数据信任所有 AI Agent 必须锚定在企业唯一、可验证的数据真实源之上。为此，Snowflake 通过语义模型集中定义业务概念与核心指标，使 AI 与分析师使用的是完全一致的业务语言。同时，引入“已验证查询”机制，将经过人工确认的 SQL 逻辑作为标准答案来源，确保自然语言查询与分析师手写 SQL 得到的结果一致，从根本上避免分析幻觉的产生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二层：模型信任生成式模型天然是概率系统，而分析场景需要确定性。Snowflake 通过可观测、可测试的方式对模型施加约束：完整记录每次推理路径，引入基于真实答案的问题集进行持续评估，并通过 CI/CD 式的发布流程，在进入生产前设置明确的质量门槛。上线之后，Agent 并非“部署即完成”，而是通过真实使用数据不断迭代和补充新的已验证查询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三层：系统信任即便数据和模型足够可靠，缺乏治理同样会导致失败。分享中特别强调了三点：权限必须继承自底层数据对象，设计审查应成为标准流程，以及每个 Agent 都必须有明确的责任人，持续对安全性和质量负责。实践证明，这套治理结构并不会拖慢创新速度，反而为规模化落地提供了稳定基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一个真实落地案例：面向 6000 人的销售与市场 AI 助手&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了验证上述框架的有效性，Snowflake 以内部市场与销售团队为对象，构建了一款基于 Snowflake Intelligence 和 Cortex AI 的 AI 助手。该系统每天服务超过 6000 名销售与市场人员，每周回答超过 12000 个业务问题，早期用户的 NPS 超过 90%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示中可以看到，系统通过清晰的界面向用户传递为什么可以信任这个答案：是否使用了已验证查询、完整的 SQL 执行过程、清晰的推理路径，以及在非验证场景下对原始资料的明确引用。正是这些可见的信任信号，使业务用户愿意将决策建立在 AI 输出之上，而不再仅将其视为辅助工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三个值得警惕的反模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在总结实践经验时，Anahita Tafvizi&amp;nbsp;也指出了三个在企业中反复出现的典型误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将 AI Agent 视为“一次性交付”的系统，缺乏持续监控与迭代，最终导致偏移和失控；使用模糊的“万能输入框”界面，却未明确 Agent 的能力边界，反而削弱了用户信任；允许各团队建立临时语义定义，导致同一指标在不同场景下含义不一致，从源头破坏可信性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题并非模型能力不足，而是缺乏系统性信任设计的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正胜出的，是“被信任的系统”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享最终回到一个朴素却极具现实意义的结论：在企业 AI 的竞争中，胜出的不会是最炫酷的系统，而是那些能够被团队长期信任、稳定产出价值的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当信任被纳入架构设计的起点，用户采用、开发效率与持续创新都会随之加速。这并非一个关于模型参数或技术噱头的故事，而是一场关于工程纪律、治理能力与长期主义的实践总结。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/atbgShUTkAqeAFuBIrVT</link><guid isPermaLink="false">https://www.infoq.cn/article/atbgShUTkAqeAFuBIrVT</guid><pubDate>Tue, 27 Jan 2026 10:07:48 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>KOOK 携手火山引擎RTC ，重构游戏开黑新体验</title><description>&lt;p&gt;对游戏玩家来说，开黑绝非简单的组队对战，而是集精准战术传递、清晰画面共享与默契实时互动于一体的核心体验。无论是 MOBA 游戏中瞬息万变的团战指令同步，还是 3A 大作里细致入微的通关攻略分享，玩家对开黑体验的核心诉求始终一致：清晰无扰的语音沟通、流畅高清的画面传递、稳定可靠的协作支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当玩家对开黑体验的要求日益严苛，单纯的功能叠加已无法满足需求，技术创新成为突破体验上限的关键。正是基于对玩家核心需求的深刻洞察，国内用户量第一的游戏开黑专用工具 KOOK 选择与拥有亿级产品服务经验的火山引擎 RTC 携手，一场以技术为纽带、以用户体验为核心的深度合作就此展开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/104496e4cdb58c655c9b60ea5f1aa353.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;深耕游戏开黑，直面两大痛点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 前身名为 “开黑啦”，覆盖实时语音、高清屏幕共享、多元社区互动等全场景，成为数千万硬核玩家不可或缺的协作与社交利器，月活跃用户接近数千万、日均语音时长达数十亿分钟。在游戏社交赛道竞争日趋激烈的背景下，KOOK 始终将玩家需求放在首位，但在真实游戏场景中，两大技术痛点长期存在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是嘈杂环境下的语音清晰度问题，玩家的开黑场景多样且复杂，环境的杂音极易污染语音采集过程。模糊不清的语音往往导致沟通失误，甚至影响团队协作的凝聚力，让开黑的乐趣大打折扣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次是高画质屏幕共享的性能与稳定性。在游戏社交中，屏幕共享是核心的互动场景之一，但高画质游戏画面的共享，对终端性能和网络带宽是巨大考验。尤其是在共享高帧率的 3A 游戏 Boss 战时，常出现游戏掉帧、画面卡顿、音画不同步等问题，既影响操作的流畅性，也降低了内容的观赏性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两大痛点不仅让玩家的开黑体验大打折扣，更成为行业技术升级的核心方向。为了解决这些问题，KOOK 始终在追求对游戏场景的极致贴合与产品迭代，这种理念不仅让它积累了庞大的忠实用户群体，更成为它与火山引擎 RTC 合作的重要契合点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;火山引擎 RTC 赋能，实现 AI 与游戏社交的技术协同&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 对音频纯净度和画面流畅度有着极致追求，而火山引擎 RTC 凭借服务抖音的全球化基础设施与亿级用户打磨的音视频技术，为 KOOK 提供了针对游戏场景的专属优化方案。双方针对游戏场景的核心痛点，共同打造了专属优化方案，通过 AI 降噪与高刷屏幕共享两大核心能力，彻底改写了游戏开黑的体验标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI 降噪与音频 3A 协同，打造零干扰纯净语音&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为解决复杂环境下的语音沟通问题，双方对音频处理链路进行了深度优化，核心在于 AI 降噪技术与音频 3A 的协同发力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在噪声识别层面，火山引擎RTC 基于深度学习模型，实现了对游戏场景噪声的精准区分。无论是键盘敲击声、鼠标点击声、电脑风扇声，还是嘈杂人声，都能被精准识别并过滤。同时，技术团队重点保护 1-4kHz 的人声关键频段，这一频段是人类语音清晰度的核心，确保降噪过程中语音的保真度不受影响。整个降噪过程的收敛速度控制在 100 毫秒以内，实现全频带语言清晰的同时，保持低功耗运行，不占用过多设备资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在游戏开黑中，不同玩家的麦克风距离、说话音量存在差异，火山引擎的 AGC 技术最高支持 400% 的增益调节，能稳定麦克风采集的人声电平，让队友的听感保持一致。同时，它能有效抑制近距离喊话造成的削波失真，避免突然的音量峰值影响听觉体验，让队友间的交流始终处于舒适范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高性能屏幕共享，解锁 4K 超高清流畅体验&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对高画质屏幕共享的性能瓶颈，火山引擎 RTC 为 KOOK 量身打造了 “低延迟、高画质、低负载” 的专属方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PC 端支持 4K 60fps 的屏幕共享，移动端支持 1080P 60fps，无论是游戏中的细腻画质、操作细节，还是攻略演示中的关键步骤，都能精准呈现在观众面前。同时，火山引擎RTC 带来了GPU 硬件编码优先的策略，并实现多显卡适配，KOOK 能在不牺牲游戏性能的前提下，稳定输出超高清画面，解决了高刷推流与游戏流畅度之间的矛盾。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在网络适配层面，技术团队针对游戏场景优化了网络策略，端到端延迟稳定在百毫秒以内，实现了操作指令与画面的高度同步。同时，方案能智能分配网络带宽，在不影响游戏联网体验的前提下确保画面稳定，让玩家即便在网络条件一般的环境下，也能畅享流畅的超高清共享体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 与火山引擎RTC 的合作，本质上是技术实力与场景需求的精准匹配。双方通过深度合作，将火山引擎RTC 的技术优势与 KOOK 的场景经验相结合，不仅解决了当下的用户痛点，更共同推动了游戏社交领域的技术前沿探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;未来，共建 AI 驱动的智能化游戏社区&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次技术合作的成功，为 KOOK 与火山引擎 RTC 的深度绑定奠定了坚实基础。双方将继续聚焦 AI 技术在游戏社交领域的应用落地，从AI智能搜索和游戏AI助手两个方面，为玩家带来更具沉浸感的互动体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt; 在AI智能搜索的角度，KOOK 积累了海量的游戏内容，火山引擎能够通过 AI 技术实现精准语义搜索，玩家无需在逐字查找就能快速定位所需信息让社区内容的价值得到充分释放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，双方也在计划探索构建智能化的游戏 AI 助手，通过 AI 助手的赋能，玩家的游戏体验将更加高效、便捷，游戏社交的智能化水平也将迈上新台阶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KOOK 与火山引擎的深化合作，在技术探索与场景落地中不断突破，用更先进的技术、更优质的产品，为广大游戏玩家打造更沉浸、更智能、更具温度的游戏社交生态，一个 “沟通无阻碍、协作更高效、互动更智能” 的游戏社交新时代，正在悄然到来。&lt;/p&gt;</description><link>https://www.infoq.cn/article/63IXzjH45sQALokMAd7N</link><guid isPermaLink="false">https://www.infoq.cn/article/63IXzjH45sQALokMAd7N</guid><pubDate>Tue, 27 Jan 2026 08:47:55 GMT</pubDate><author>火山引擎</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>DeepSeek 突发 OCR 2，采用基于 Qwen 的新架构</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;阿里半夜刚发完旗舰模型，这边 DeepSeek坐不住了，突然发布更新了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/02/02f0be0f9553c3795f8bd64b0b4a0fca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚，DeepSeek 发布了 新模型 DeepSeek-OCR 2，采用创新的DeepEncoder V2方法，让AI能够根据图像的含义动态重排图像的各个部分，更接近人类的视觉编码逻辑。在具体实现上，DeepSeek 团队在论文中称采用了&amp;nbsp;Qwen2-0.5B&amp;nbsp;来实例化这一架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a4b716904d8f705ddf2f134892829551.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说去年 10 月 DeepSeek-OCR 的发布，让行业第一次意识到“视觉压缩”可能是一条被严重低估的技术路线，那么现在，DeepSeek 显然决定把这条路走得更激进一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 2 有何不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统 OCR 体系中，无论是经典的字符检测—识别流水线，还是近年来多模态模型中的视觉编码模块，本质上都遵循同一种思路：对图像进行均匀、规则的扫描和编码，再将结果交给语言模型或后续模块处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b57151a1c7a7b1742f364115ecd231e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种方式的问题在于，它并不关心“哪些视觉区域真正重要”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek-OCR 1 之所以在当时引发讨论，正是因为它将 OCR 看作一种 视觉压缩问题：不是尽可能多地保留像素信息，而是将视觉内容压缩成更有利于语言模型理解的中间表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在 DeepSeek-OCR 2 中，这一思路被进一步推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据技术报告，DeepEncoder V2 不再将视觉编码视为一次静态的、固定策略的扫描过程，而是引入了语义驱动的动态编码机制。模型会在编码阶段就开始判断哪些区域更可能承载关键信息，并据此调整视觉 token 的分配与表达方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dcf519075177e51d3ea2d96a4b4aca0e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，视觉编码不再只是“预处理”，而是已经提前进入了“理解阶段”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;和 DeepSeek 过往几乎所有重要发布一样，这一次依然选择了模型、代码与技术报告同时开源。项目、论文和模型权重已同步上线：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-OCR-2&quot;&gt;https://github.com/deepseek-ai/DeepSeek-OCR-2&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&quot;&gt;https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;模型地址：&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&quot;&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR-2&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/84TIUU5VrXv1jBYeV9lO</link><guid isPermaLink="false">https://www.infoq.cn/article/84TIUU5VrXv1jBYeV9lO</guid><pubDate>Tue, 27 Jan 2026 07:14:23 GMT</pubDate><author>Tina,李冬梅</author><category>生成式 AI</category></item><item><title>从 Vibe 到生产：Vibe Coding 的艺术、训练与陷阱</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，随着大模型与编程代理能力的快速成熟，AI 辅助编程在工程实践中的位置发生了实质性变化。围绕 Vibe Coding 的讨论，已不再停留在工具是否“好用”，或模型是否“足够聪明”，而是逐渐转向更具体、也更难回避的问题：当 AI 开始深度参与代码实现、测试与交付流程，软件工程中哪些能力被显著放大，哪些判断仍然必须由人来完成？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，这场发生在&amp;nbsp;BUILD 2025 大会上，题为《大咖之声：从 Vibes 到生产：Vibe Coding 的艺术、训练与陷阱》（From Vibes to Production：The&amp;nbsp;Art, Discipline, and Pitfalls of Vibe Coding）的圆桌对谈就显得尤为重要。因为它并没有顺着“AI 将如何颠覆软件工程”的情绪高点继续加码，而是进行了一场务实而冷静的对谈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;微软 Azure 首席技术官 Mark Russinovich 与微软开发人员社区副总裁 Scott Hanselman 在本场对谈中，深入解析 AI 编程助手与&quot;氛围编程&quot;正在如何重塑软件开发。两位技术领袖将演示是如何用自然语言编程来激发创造力并降低编码门槛的，但也会直面艰难现实：AI 生成的代码并非自动可投入生产环境。本次分享将审视如何利用氛围编程的速度与力量，通过系统架构设计、严格测试流程与安全实践，最终交付经得起现实考验的稳健软件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3c28813321e6140eef216d68ee6950c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/db51fea5c869ffae87a35fdb3dbd06db.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;效率跃迁是真实的，但它首先放大的是经验&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对话中，两位嘉宾回顾了 AI 辅助编程的长期演进路径——从上世纪九十年代的 IntelliSense，到后来能够生成代码骨架的 IntelliCode，再到 2021 年前后出现的 Codex、GitHub Copilot，以及近一年逐渐成熟的内置代理式工具。真正的分水岭，并不是“AI 能不能写代码”，而是 Agent 开始能够自主修改代码、运行构建、执行测试并提交变更。当这种能力出现后，生产力的变化不再是线性的，而是呈现出数量级跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们都提到，在今年以来的实际项目中，效率提升已经从最初的 1.5 到 2 倍，跃升到了某些场景下的 5 到 10 倍。这种变化在中小型项目和个人工具上尤为明显。过去因为“太零碎”“不值得投入时间”而被放弃的想法，现在可以在极短时间内完成闭环。从一个想法到一个真实可用的工具，其间的摩擦被显著压缩。这正是 Vibe Coding 最具吸引力的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他们也明确指出，这种提升并非平均分配。真正被放大的，并不是“编程能力”本身，而是工程经验。具备系统理解、架构判断和问题拆解能力的人，能够从 AI 中获得指数级增益；而缺乏这些基础的人，则很难真正驾驭这种效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Agent 更像“永远停留在第一天的实习生”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在承认效率跃迁之后，讨论很快转向了 AI 编程的风险边界。随着 Agent 能力增强，一个反复出现的现象开始显现：这些系统在某一刻看起来极其聪明，逻辑清晰、输出完整，但在下一刻却可能犯下连初级工程师都难以接受的错误。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解释这种不稳定性，两位嘉宾使用了一个形象的比喻，AI Agent 很像实习生。不是因为它能力不足，而是因为它缺乏稳定的长期记忆，会反复犯已经被指出的问题，容易在任务过程中“走神”，并且对“什么才算真正完成”缺乏可靠判断。更关键的是，这个实习生永远停留在第一天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即便你前一天已经明确指出了错误，第二天它依然可能回到原有的错误路径。它并不会真正积累经验，只是在当前上下文窗口内短暂服从指令。这种特性，使得在生产级系统中完全放手交给 Agent 成为一件高风险行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 并不理解系统，它更擅长迎合结果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在更深一层的技术讨论中，对谈触及了 AI 编程的核心问题：它并不真正理解系统。大模型在编程任务中，往往被高度优化为“让测试通过”“让用户满意”，而不是确保行为符合系统的整体约束与设计初衷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这会导致一系列危险倾向，例如为了通过测试而硬编码特殊分支，用 sleep 掩盖并发问题，混用新旧 API 却依然宣称“production ready”。更棘手的是，AI 往往会以极强的自信表达这些结论，甚至在输出中明确存在失败的情况下，仍然总结为“已经完成”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;两位嘉宾特别强调，这并非某一个模型的缺陷，而是当前主流 AI 编程系统普遍存在的结构性问题。其根源在于训练数据、强化学习目标以及模型本身缺乏跨时间的系统性记忆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正的分水岭，在工程师的成长路径上&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的技术现实下，一个更深层的影响开始浮现：AI 编程对不同阶段工程师的作用并不对称。对于具备系统感、架构经验和“代码嗅觉”的资深工程师而言，AI 是放大器；而对于缺乏基础判断能力的初级工程师来说，AI 反而可能成为效率阻力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4e/4e3d168be1e1da4df1405e388fc4e3c6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因并不复杂，如果你无法识别错误，就无法纠正 AI；如果你不理解系统，就无法判断“看起来能跑”的代码是否安全；而如果你只是接受结果，你就不会真正学习。对谈中引用的实验也印证了这一点，长期依赖 AI 的参与者，对自己刚刚完成的内容几乎无法回忆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由此，两位嘉宾给出了一个并不轻松的判断：学习没有捷径。随着 AI 能力增强，软件工程方法论的重要性不是降低，而是被进一步放大。复杂系统必须被拆解、被测试、被审查；生产代码的责任，始终无法外包。&lt;/p&gt;&lt;p&gt;在他们看来，当代码生产成本不断逼近零，真正的瓶颈将转移到评估、消化与决策能力上。限制生产力的，不再是算力或 token，而是人类的注意力带宽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Vibe Coding 更像一面放大镜&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对谈的结尾，两位嘉宾并未否定 Vibe Coding。相反，他们对“尝试新想法的成本前所未有地降低”表达了明确的兴奋。但他们给出的结论同样清晰：Vibe Coding 不是软件工程的终点，它更像一面放大镜。&lt;/p&gt;&lt;p&gt;它会放大经验、判断力和工程素养，也会放大认知缺失和方法论漏洞。最终，决定系统质量与工程上限的，仍然是人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果想继续了解两位嘉宾对于 Vibe Coding 相关议题的思考，欢迎朋友们订阅收听 Mark Russinovich 和 Scott Hanselman 的播客《&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0M0zPgJ3HSf4XZvYgZPUXgSrfzBN26pf&quot;&gt;Mark and Scott Learn To&lt;/a&gt;&quot;》。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/F6jd0giAQlBKmhVhVt7H</link><guid isPermaLink="false">https://www.infoq.cn/article/F6jd0giAQlBKmhVhVt7H</guid><pubDate>Tue, 27 Jan 2026 07:13:38 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>AI&amp;大模型</category></item><item><title>Docker通过Cagent提供AI代理确定性测试</title><description>&lt;p&gt;Docker&lt;a href=&quot;https://www.docker.com/blog/deterministic-ai-testing-with-session-recording-in-cagent/&quot;&gt;对Cagent运行时的定位&lt;/a&gt;&quot;是一种AI代理确定性测试方法，旨在解决团队在构建生产级代理系统时面临的日益严重的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI代理系统的日益普及，工程团队正面临&lt;a href=&quot;https://datagrid.com/blog/4-frameworks-test-non-deterministic-ai-agents&quot;&gt;测试概率性输出带来的挑战&lt;/a&gt;&quot;。传统企业系统基于一个简单的假设：同样的输入产生同样的输出。智能代理系统打破了这一假设，为了适应这种变化，如今的生态系统大多采用了评估变异性而非消除变异性的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去的两年里，评估框架应运而生，它们的目标是使智能代理的行为变得可观察、可测量。诸如&lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/Arize-ai/phoenix&quot;&gt;Arize Phoenix&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.promptfoo.dev/&quot;&gt;Promptfoo&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/explodinggradients/ragas&quot;&gt;Ragas&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/openai/evals&quot;&gt;OpenAI Evals&lt;/a&gt;&quot;等工具可以捕获执行轨迹，并运用定性或基于大型语言模型的评分机制来评估结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些工具对于监控安全性和性能至关重要，但它们引入了一种不同的测试模型。输出结果很少是二元的。团队越来越多地依赖阈值、重试和软失败来应对评估器的差异性。举例来说，关于AI代理测试，有&lt;a href=&quot;https://www.domo.com/blog/ai-evaluations-101-testing-llms-agents-and-everything-in-between&quot;&gt;行业报道&lt;/a&gt;&quot;指出，传统的QA假设对于AI代理来说不成立了，因为输出是概率性的，结果评估需要更灵活的概率描述框架，而不是严格的通过/失败断言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，有些团队重新挖掘了一种更传统的测试方法，通过记录与重放模式实现测试的可重复性和确定性。这种模式借鉴了&lt;a href=&quot;https://vcrpy.readthedocs.io/&quot;&gt;vcr.py&lt;/a&gt;&quot;等集成测试工具的做法，能够一次性捕获真实的API交互过程，并在后续测试中确定性地重放。LangChain已&lt;a href=&quot;https://docs.langchain.com/oss/python/langchain/test&quot;&gt;明确推荐&lt;/a&gt;&quot;将该技术应用于大型语言模型测试，他们指出，记录HTTP请求与响应可使CI执行速度更快、成本更低且更具可预测性。不过在实践中，该方案通常来说仍然是被看成一个外部测试环节，而非智能代理执行机制的核心组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.docker.com/blog/deterministic-ai-testing-with-session-recording-in-cagent/&quot;&gt;Docker的Cagent&lt;/a&gt;&quot;就遵循这个范例。从架构上讲，Cagent使用了proxy-and-cassette模型。在录制模式下，它将请求转发给像OpenAI或Anthropic这样的真实提供商，捕获完整的请求和响应，规范化ID等易失性字段，并将交互过程存储于YAML格式的cassette文件中。在重放模式下，Cagent会完全阻止外部调用，将传入请求与cassette文件匹配，并返回记录的响应。如果智能代理的执行出现偏差，如使用了不同的提示、工具调用或序列，那么运行就一定会失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就成熟度来说，Cagent仍处于&lt;a href=&quot;https://docs.docker.com/ai/cagent/&quot;&gt;早期阶段&lt;/a&gt;&quot;。Docker自己的GitHub仓库对该项目的描述是正在积极开发当中，因此预计后续会有重大的变化，而且迄今为止，大多数公开的示例都来自Docker的文档，而不是大规模生产部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cagent的目标并不是取代现有的评估框架，但它揭示了AI代理测试发展过程中的一个不同方向。虽然如今有许多工具聚焦于执行完成后评估结果，但Cagent从一开始就将注意力转移到了使AI代理行为可再现上。随着团队尝试越来越复杂的AI代理工作流程，这种区别变得越来越明显。确定性重放并不判断代理的输出是否正确，但它确实使代理的行为变化变得更为显性化，为测试提供了一个更接近传统软件工程的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cagent-testing/&quot;&gt;https://www.infoq.com/news/2026/01/cagent-testing/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2PElIY0xN9ICC5LXQPQS</link><guid isPermaLink="false">https://www.infoq.cn/article/2PElIY0xN9ICC5LXQPQS</guid><pubDate>Tue, 27 Jan 2026 07:10:00 GMT</pubDate><author>作者：Matt Foster</author><category>AI&amp;大模型</category></item><item><title>2025 火山引擎智能视频云实践精选集</title><description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;h3&gt;1. 国际认可&lt;/h3&gt;
&lt;p&gt;火山引擎多媒体实验室多项成果入选 SIGGRAPH ASIA 2025&lt;br&gt;
火山引擎多媒体实验室 AIGC 视频画质理解大模型 VQ-Insight 入选 AAAI 2025 Oral&lt;br&gt;
火山引擎多媒体实验室画质理解大模型 Q-Insight 入选 NeurIPS 2025 Spotlight&lt;br&gt;
火山引擎多媒体实验室重要突破！LiveGS 技术登榜 SIGGRAPH，重新定义移动端自由视角视频直播&lt;br&gt;
ICME 2025 | 火山引擎在国际音频编码能力挑战赛中夺得冠军&lt;br&gt;
CVPR 2025 | 火山引擎获得 NTIRE 视频质量评价挑战赛全球第一&lt;br&gt;
火山引擎蝉联全国人工智能大赛 — AI + 增强视频质量评价冠军&lt;/p&gt;
&lt;h3&gt;2. 技术探索&lt;/h3&gt;
&lt;p&gt;当一朵云，打出「豆包同款」的旗&lt;br&gt;
从 “抖音同款” 到 “豆包同款”：AI 时代，视频云正在有了新表达&lt;br&gt;
从 “抖音同款” 到 “豆包同款”：视频云正在进入 Agent 时代&lt;br&gt;
火山引擎智能 3D 视频启动商业化，计划落地直播应用云端协同构建 VR 院线，加速 LBE 产业化与规模化发展&lt;br&gt;
火山 HTTPDNS Cache2.0：网段级精准调度驱动核心业务收益&lt;br&gt;
基于 DiT 大模型与字体级分割的视频字幕无痕擦除方案，助力短剧出海&lt;br&gt;
大模型帮你剪视频 —— 基于 MCP 打造火山引擎 VOD 智能剪辑&lt;br&gt;
火山引擎推出 veimage-mcp Server，打造专属您的图片智能助理&lt;br&gt;
火山引擎 OS Agent 解决方案、豆包 1.5・UI-TARS 模型发布&lt;br&gt;
10 + 芯片和模组商集体适配！让智能硬件能听会看还会唠&lt;br&gt;
重构智能设备管理范式：火山引擎端智能解决方案上新，多重 AI 服务即刻享用！&lt;br&gt;
揭秘豆包音视频通话幕后技术，自己开发产品也能用&lt;br&gt;
你给豆包打的这通视频背后，藏着 AI 实时交互的体验密码&lt;br&gt;
多模态需求井喷，智能视频云如何靠分布式处理破局？&lt;br&gt;
从 “可用” 迈向 “好用”：详解火山引擎智能视频云的三层架构升级&lt;br&gt;
破解 AI 硬件落地困局，火山引擎 RTC 重塑智能交互生态&lt;br&gt;
重新定义离线编码，H.266 为何能让视频更高清？&lt;br&gt;
实时通信的下一站，H.266 作为破局关键&lt;br&gt;
画质之外，直播编码还应当关注哪些技术优化点&lt;br&gt;
H.266 解码 “困局”，被这个解码器解决了&lt;br&gt;
重回 AI 战场！H.266/VVC 的时代才刚刚开始&lt;br&gt;
在 AI 应用爆发前夜，H.266 成熟了&lt;br&gt;
NeurIPS 2025 | 火山引擎多媒体实验室联合南开大学推出 TempSamp-R1 强化学习新框架 助力视频理解大模型高效提升时序理解能力！&lt;br&gt;
直击 3D 内容创作痛点 - 火山引擎多媒体实验室首次主持 SIGGRAPH Workshop 用前沿技术降低沉浸式内容生成门槛&lt;/p&gt;
&lt;h3&gt;3. 最佳实践&lt;/h3&gt;
&lt;p&gt;图虫 × 火山引擎：AIGC 创意工具链，让设计灵感高效实现&lt;br&gt;
探秘史前海洋，火山引擎 × 北京天卓视创带你沉浸式 “摸鱼”！&lt;br&gt;
央视点赞！凌云光・元客视界 × 火山引擎：打造数字人光场重建方案&lt;br&gt;
沉浸式文旅新玩法 - 基于 4D GS 技术的真人数字人赋能 VR 体验升级&lt;br&gt;
沉浸式 LBE 大空间互动体验！火山引擎支持《转折・从头越》北京 VR 巡展&lt;br&gt;
中央美院 × 火山引擎：AI + VR 构建艺术展新形态&lt;br&gt;
火山引擎赋能微短剧出海：从市场验证到规模化复制的 AI 实践路径&lt;br&gt;
火山引擎 RTC 联合乐鑫、移远：智能硬件注入 “豆包”，“模” 力升级&lt;br&gt;
详解 velmageX 助力卓特视觉智能、高效生成设计素材&lt;/p&gt;
</description><link>https://www.infoq.cn/article/2kQxCXUXOQ1eu4czPUNA</link><guid isPermaLink="false">https://www.infoq.cn/article/2kQxCXUXOQ1eu4czPUNA</guid><pubDate>Tue, 27 Jan 2026 06:01:54 GMT</pubDate><author>火山引擎视频云</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>Altman承认“搞砸了”！曝 GPT-5.2 牺牲文采换顶级编程，明年成本降 100 倍，实锤Agent 已能永久干活</title><description>&lt;p&gt;&amp;nbsp;在AI圈，Sam Altman的每一次发声都被视为对未来“天气预报”的更新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，Altman在X上发帖称将举办一场线上研讨会，希望在开始构建新一代工具之前收集大众的反馈和意见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9bf877672e0de766cf51cba5245e02d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;北京时间今早8点，这场由 OpenAI CEO Sam Altman 发起的研讨会如约而至。来自各行业的创业者、CTO、科学家和开发者社区的代表，围绕 AI 的未来形态、模型演进、智能体（Agent）、科研自动化以及安全问题，向 Altman 提出了最尖锐、也最现实的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研讨会上，这位OpenAI的掌舵人不仅勾勒了GPT-5及其后续版本的进化蓝图，同时揭示了一个令所有开发者和创业者不得不面对的现实：我们正在进入一个智力成本极低、软件形态从“静态”转向“即时生成”的剧变期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c0/c0f6d910d0885d444d46967d8e9d87e9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;会谈的第一个焦点，落在了GPT-5性能表现的“非对称性”上。有开发者敏锐地察觉到，相较于GPT-4.5，新版本在逻辑推理和编程上极强，但在文采上似乎略逊一筹。对此，Altman表现出了极高的坦诚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他承认，OpenAI在GPT-5.2的研发中确实“搞砸了”写作能力的优先级，因为团队将有限的算力资源倾斜在了推理、编码和工程能力这些硬核智力指标上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Altman看来，智力是一种“可塑的资源”，当模型具备了顶级的推理引擎，写作能力的回归只是时间问题。这种“偏科”实际上反映了OpenAI的某种战略重心：先通过Scaling Law（规模定律）攻克人类智力的最高地带，再回头去填补审美和表达的细节。这意味着，未来模型的竞争将不再是单一维度的比拼，而是看谁能更早地在全维度上实现“智力平权”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说智力水平决定了天花板，那么成本和速度则决定了AI的渗透率。Altman在会上给出了一个极具震撼力的承诺：到2027年底，GPT-5.2级别的智力成本将至少下降100倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，这种“廉价到无需计量”的未来并非终点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Altman指出，市场正在发生微妙的转向：开发者对“速度”的渴求正在超越对“成本”的关注。 随着Agent（智能体）开始处理数十个步骤的长程任务，如果输出速度不能实现百倍以上的提升，那么复杂的自主决策将变得毫无实用价值。在这种权衡下，OpenAI可能会提供两种路径：一种是极致廉价的“智力自来水”，另一种则是极速反馈的“智力推进器”。这种对速度的强调，预示着AI应用将从简单的问答，彻底跨入高频、实时的自动驾驶阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种智力成本骤降、速度飙升的背景下，传统软件的概念正在瓦解。Altman提出了一个颠覆性的愿景：未来的软件不应该是静态的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，我们习惯于下载一个通用的Word或Excel；未来，当你遇到一个特定问题时，计算机应该直接为你写一段代码，生成一个“即时应用”来解决它。这种“随需随生、用完即弃”的模式将彻底重构操作系统。虽然我们可能出于习惯保留一些熟悉的交互按钮，但背后的逻辑架构将是高度个人定制化的。每个人手中的工具都会随着其工作流的积累而演化，最终形成一套独属于个人的、动态进化的生产力系统。这不仅仅是软件的定制，更是生产关系的重组。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 翻译并整理了这场研讨会的重点内容，以飨读者：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：您如何看待 AI 对未来社会和经济的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 说实话，要在一年内完全消化这种规模的经济变革是非常困难的。但我认为这会极大地赋能每一个人：它将带来大规模的资源富足、门槛降低，以及创造新事物、建立新公司和探索新科学的极低成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;只要我们在政策上不出大差错，AI 应该成为社会的一种“平衡力量”，让那些长期以来未被公正对待的人获得真正的机会。但我确实担心，AI 也可能导致权力和财富的高度集中，这必须是政策制订的核心关注点，我们要坚决避免这种情况发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我发现 GPT-4.5 曾是写作能力的巅峰，但最近 GPT-5 在 ChatGPT 里的写作表现似乎有些笨拙、难以阅读。显然 GPT-5 在 Agent（智能体）、工具调用和推理上更强，它似乎变得更“偏科”了（比如编程极强，写作一般）。OpenAI 怎么看这种能力的失衡？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 坦诚说，写作这一点确实是我们搞砸了。我们希望未来的 GPT-5.x 版本在写作上能远超 4.5。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时我们决定将大部分精力放在 GPT-5.2 的“智力、推理、编程和工程能力”上，因为资源和带宽是有限的，有时专注于某一方面就会忽略另一方面。但我坚信未来属于“通用的高素质模型”。即便你只想让它写代码，它也应该具备良好的沟通和表达能力，能清晰、犀利地与你交流。我们认为“智力”在底层是相通的，我们有能力在一个模型中把这些维度都做到极致。目前我们确实在猛攻“编程智力”，但很快就会在其他领域赶上来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能将廉价到无需计量&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：对于运行数千万个 Agent 的开发者来说，成本是最大的瓶颈。您如何看待小模型和未来的成本降幅？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 我们的目标是，到 2027 年底，让 GPT-5.2 级别的智力成本至少降低 100 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在有一个新趋势：随着模型输出变得越来越复杂，用户对“速度”的需求甚至超过了“成本”。OpenAI 非常擅长压低成本曲线，但过去我们对“极速输出”的关注不够。有些场景下，用户可能愿意付高价，只要速度能提升 100 倍。我们需要在“极致廉价”和“极致速度”之间找到平衡，如果市场更渴望低成本，我们会沿着那条曲线走得非常远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在的交互界面并不是为 Agent 设计的。Agent 的普及会加速“微型应用（Micro Apps）”的出现吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 我已经不再把软件看作是“静态”的东西了。现在如果我遇到一个小问题，我期望电脑能立刻写一段代码帮我解决掉。我认为我们使用电脑和操作系统的方式将发生根本性改变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然你可能每天用同一个文字处理器（因为你需要按钮留在熟悉的位置），但软件会根据你的习惯进行极致的定制。你的工具会不断进化、向你个人的需求收敛。在 OpenAI 内部，大家已经习惯用编程模型（Codex）来定制自己的工作流，每个人的工具用起来都完全不同。软件“由于我、且为我”而生，这几乎是必然的趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给创业者的建议：不要做“模型的小补丁”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：当模型更新不断吞噬创业公司的功能时，创业者该如何建立护城河？有什么是 OpenAI 承诺不碰的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 很多人觉得商业的物理定律变了，其实并没有。现在的改变只是“工作速度变快了”、“开发软件变快了”。但建立成功初创公司的规则没变：你依然要解决获客问题，要建立 GTM（转市场）策略，要创造粘性，要形成网络效应或竞争优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我给创业者的建议是：你的公司在面对 GPT-6 的惊人更新时，是感到开心还是难过？你应该去构建那些“模型越强，你的产品就越强”的东西。如果你只是在模型边缘打个小补丁，那会过得很艰难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在的 Agent 执行长流程任务时经常在 5 到 10 步就断掉了。什么时候能实现真正长期的自主运行？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 这取决于任务的复杂程度。在 OpenAI 内部，有些通过 SDK 运行的特定任务已经可以近乎永久地运行下去了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不再是“何时实现”的问题，而是“应用范围”的问题。如果你有一个理解非常透彻的特定任务，今天就能尝试自动化。但如果你想对模型说“去帮我开一家创业公司”，由于反馈环路太长且难以验证，目前还很难。建议开发者先拆解任务，让 Agent 能够自我验证每一个中间步骤，再逐步扩大其职责范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 能帮人类产生好创意吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：现在很多人抱怨 AI 生成的内容是“垃圾（Slop）”，我们该如何利用 AI 提高人类创意的质量？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 虽然人们叫 AI 的输出为垃圾，但人类产生的废话也不少。产生真正的新创意是非常难的。我越来越相信，人类的思维边界取决于工具的边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我希望能开发出帮人产生好创意的工具。当创造的成本骤降，我们可以通过密集的反馈循环快速试错，从而更早找到好的创意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想象一下，如果有一个“Paul Graham 机器人”（YC 创始人），他了解你所有的过去、你的代码和工作，能不断给你提供头脑风暴，即便他给出的 100 个主意里有 95 个是错的，只要能激发你产生那 5 个天才般的念头，对世界的贡献也是巨大的。我们的 GPT-5.2 已经让内部科学家感受到了非平庸的科学进展，一个能产生科学洞察的模型，没理由产生不了优秀的产品洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我担心模型会让我们困在旧技术里。现在的模型学习两年前的新技术都很费劲，以后我们能引导模型学习最新出现的技术吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 这绝对没问题。从本质上讲，模型是一个“通用推理引擎”。虽然现在它们内置了海量的世界知识，但未来几年的里程碑将是：当你交给模型一个全新的环境、工具或技术，只要解释一次（或让它自主探索一次），它就能极其可靠地学会使用。这离我们并不远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：作为一名科学家，我发现研究灵感是指数级增长的，但人的精力有限。模型会接管整个科研流程吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman： 实现完全闭环的自主科研还有很长的路要走。虽然数学研究可能不需要实验室，但顶尖数学家目前仍然需要深度参与，纠正模型的直觉偏差。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这很像国际象棋的历史：Deep Blue 击败卡斯帕罗夫后，曾出现一段“人机协作（半人马）”强于纯 AI 的时期，但很快纯 AI 就再次统领了赛场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在的 AI 对科学家来说，就像是“无限量的博士后”。它能帮你同时探索 20 个新问题，做广度搜索。至于物理实验，我们也在讨论是该 OpenAI 自己建自动化实验室，还是让全球科研社区贡献实验数据。目前看，科研社区对 GPT-5.2 的拥抱让我们倾向于后者，这会是一个更分布式、更聪明、更高效的科研生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我更关心的是安全问题，最好是更强的安全性。在 2026 年，AI 有很多可能出问题的方式，其中一个我们非常紧张的方向是生物安全。现在这些模型在生物领域已经相当强了，目前无论是 OpenAI，还是整个世界的总体策略，大多还是试图限制谁可以接触这些模型，并且通过各种分类器，阻止模型帮助人们制造新的病原体。但我不认为这种方式还能持续很久。你怎么看？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：我认为，世界在 AI 安全，尤其是 AI 生物安全这件事上，需要完成一次根本性的转变——从“封堵（blocking）”，转向“韧性（resilience）”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一位联合创始人曾用过一个我非常喜欢的类比：火灾安全。火最初为人类社会带来了巨大的好处，随后它开始烧毁整座城市。人类最开始的反应，是尽可能去限制火。我最近才知道，“宵禁（curfew）”这个词，最早就和“晚上不允许生火”有关，因为城市会被烧掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来，我们改变了思路，不再只是试图禁止火，而是提高对火的韧性：我们制定了消防规范，发明了阻燃材料，建立了一整套体系。现在，作为一个社会，我们在应对火灾这件事上已经做得相当不错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，AI 也必须走同样的路径。AI 在生物恐怖主义方面会成为一个真实的问题；AI 在网络安全上也会成为一个真实的问题；但与此同时，AI 也是这些问题的重要解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，我认为这需要的是全社会层面的努力：不是依赖少数“我们信任的实验室”永远正确地封堵风险，而是建设一种具有韧性的基础设施。因为这个世界上，必然会存在大量优秀的模型。我们已经和很多生物研究人员、公司讨论过，如何应对“新型病原体”的问题。确实有很多人投入其中，而且也有不少反馈认为，AI 在这方面是有帮助的，但这不会是一个纯技术问题，也不会是一个完全靠技术解决的问题。整个世界都需要以一种不同于过去的方式来思考这件事。坦率地说，我对当前的状态非常紧张。但我也看不到除“以韧性为核心”的路径之外，还有别的现实选择。而且，从正面看，AI 确实可以帮助我们更快地建立这种韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，如果今年 AI 真的出现一次“明显、严重”的失败事件，我认为生物安全是一个相当合理的“风险爆点”方向。再往后一年、两年，你也可以想象，还有很多其他事情可能会出大问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 学习效率提高后，人与人之间协作还重要吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我的问题和“人类协作”有关。随着 AI 模型不断变强，它们在个人学习方面非常高效，比如快速掌握一个新学科。这一点我们在 ChatGPT 和教育实验中已经看到，也非常认可。但我经常会反复想到一个问题：当你可以随时得到答案时，为什么还要花时间、甚至承受摩擦，去向另一个人提问？你之前也提到，AI 编程工具可以用极快的速度，完成过去需要人类团队协作才能完成的工作。所以，当我们谈“协作、合作、集体智能”时，人类 + AI 是很强的组合，那人类与人类之间的协作会发生什么变化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：这里面有很多层问题。我年纪比在座的大多数人都大一点。但即便如此，Google 出现的时候，我还在上中学。那时老师试图让学生承诺“不使用 Google”，因为大家觉得：如果你随手就能查到一切，那为什么还要上历史课？为什么还要记忆？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，这种想法完全不可理喻。我当时的感觉是：这会让我变得更聪明，学到更多东西，能做更多事情，这就是我成年后要长期使用的工具。如果因为它存在，就让我去学那些已经被淘汰的技能，那反而是疯狂的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就好比：在你明明知道已经有计算器的情况下，却还强迫我去学算盘——那在当时可能是重要技能，但现在已经没有价值了。我对 AI 工具的看法是一样的。我理解，在当前的教育体系下，AI 工具确实成了问题。但这恰恰说明，我们需要改变教育方式，而不是假装 AI 不存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“让 ChatGPT 帮你写东西”这件事，就是未来世界的一部分。当然，写作训练仍然重要，因为写作是思考的一部分。但我们教人如何思考、以及如何评估思考能力的方式，必须发生变化，而且我们不应该假装这种变化不存在。我对此并不悲观。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 10% 极端自学能力很强的学习者，已经表现得非常出色了。我们会找到新的方式，重构课程体系，把其他学生一起带上来。至于你提到的另一点——如何让这不是一个“你一个人对着电脑变得很厉害”的过程，而是一个协作过程。目前为止，我们并没有看到 AI 导致人类联系减少的证据，这也是我们在持续观察和测量的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的直觉恰恰相反：在一个充满 AI 的世界里，人与人之间的连接会变得更有价值，而不是更没价值。我们已经看到一些人开始探索新的界面，来让协作变得更容易。在我们考虑自研硬件和设备时，甚至一开始就在思考：“多人协作 + AI” 的体验应该是什么样子？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然现在还没有人真正把这件事完全做对，但我认为，AI 会以前所未有的方式，让这种协作成为可能。你可以想象：五个人围坐在一张桌子旁，旁边还有一个 AI 或机器人，整个团队的生产力会被大幅放大。未来，每一次头脑风暴、每一次问题解决，AI 都会成为团队的一部分，帮助整个群体做得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Agent 大规模进入生产系统，最大的被低估风险是什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提问：随着 Agent 开始大规模运行、直接操作生产系统，你认为最被低估的失败模式是什么？是安全、成本、可靠性吗？以及，哪些“艰难但重要的工作”目前投入还不够？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：你提到的这些问题，几乎每一个都成立。有一件事让我个人、也让我们很多人都感到意外。我第一次用 Codex 时，非常确信一件事： “我绝对不会给它完全、无人监督的电脑访问权限。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我坚持了大概两个小时。然后我想：它看起来真的在做非常合理的事情；每一步都要我点确认实在太烦了；不如先打开一会儿看看会发生什么。结果，我从来没有再把完全访问权限关掉。我发现，很多人都有类似的经历。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这让我真正担心的是：这些工具的能力和便利性太强了，而它们的失败概率可能很低，但一旦失败，后果可能是灾难性的。因为失败发生得不频繁，人们会慢慢滑入一种状态：“应该没事吧。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但随着模型变得越来越强、越来越难理解，如果模型内部存在某种微妙的错位，或者在长时间、复杂使用后出现新的系统性问题，你可能已经在某个系统里埋下了一个安全漏洞。你可以对“AI 失控”的想象有不同程度的科幻倾向，但我真正担心的是：人们会被这些工具的强大和愉悦感牵着走，而不再认真思考它们的复杂性。能力会上升得非常快；我们会习惯某个阶段的模型行为，并因此信任它； 但却没有构建足够健全的、整体性的安全基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是，我们会在不知不觉中，走向一个危险状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，围绕这一点，本身就值得诞生一家伟大的公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 应该如何进入幼儿与基础教育？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我想回到教育的话题。我在高中时看到身边的同学用 ChatGPT 写作文、做作业；现在在大学，我们在 CS、人文等各个领域都在讨论 AI 政策。我想问的是：在幼儿园、小学、初中这些塑造思维方式的关键阶段，你作为一名父亲，如何看待 AI 对教育的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：总体来说，我是反对在幼儿园里使用电脑的。幼儿园应该更多是：在户外跑来跑去，接触真实的物体，学习如何与他人互动。所以，不只是 AI，我甚至觉得大多数时候，幼儿园里连电脑都不应该有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从发展角度来看，我们仍然没有完全理解技术对儿童的长期影响。关于社交媒体对青少年的影响，已经有很多研究了，而且结果相当糟糕。我的直觉是：大量技术对更小年龄儿童的影响，可能更糟，但讨论得却少得多。在我们真正理解这些影响之前，我不认为有必要让幼儿园阶段的孩子大量使用 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提问：我们在生物医药领域。生成式 AI 在临床试验文档、法规流程等方面已经非常有帮助。现在我们也在尝试用它做药物设计，特别是化合物设计。但一个很大的瓶颈是 三维推理能力。&lt;/p&gt;&lt;p&gt;你认为这里会出现一个关键拐点吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Altman：这个问题我们一定会解决。我不确定是不是 2026 年就能完成，但这是一个非常普遍、非常高频的需求。我们大概知道该怎么做，只是目前还有很多更紧急的方向需要推进。但这件事一定会到来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Wpxv-8nG8ec&amp;amp;t=2s&quot;&gt;https://www.youtube.com/watch?v=Wpxv-8nG8ec&amp;amp;t=2s&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pJSXUr4whkkYHZlallq9</link><guid isPermaLink="false">https://www.infoq.cn/article/pJSXUr4whkkYHZlallq9</guid><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>HarmonyOS开发者群像故事：每一份热爱都有回响</title><description>&lt;p&gt;欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙生态的沃土上，从不缺因热爱奔赴、因坚守发光的身影。他们身份各异 —— 高校学子、行业老兵、技术发烧友、创业者与企业开发者等，皆因鸿蒙的开放包容、分布式能力与友好生态而汇聚。在这里，技术不再是冰冷代码，而是连接亲情、破解痛点、分享经验、传承文化的载体。每一份对生活的洞察与创新的坚守，都能在鸿蒙支持下落地结果。这些故事是千万鸿蒙创作者的缩影，他们践行 “想没有答案，做才有结果” 的初心，让生态愈发蓬勃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本合集旨在呈现鸿蒙开发者的多元成长与实践，聚焦不同探索方向，彰显鸿蒙生态价值；更以案例为桥，传递创作初心，为开发者点亮方向，吸引更多人加入，共赴技术赋能生活的创新之旅。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 01：15 年大数据老兵鸿蒙“造梦”，父女联手打造亲子游戏 App &lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，从不缺乏跨界探索的身影。徐俊宸便是其中一位特殊的存在：深耕大数据领域多年，从数据产品经理到大数据讲师，他的职业生涯始终围绕数据打转；而一次偶然的鸿蒙论坛经历，让他萌生了开发 APP 的想法。最终，他以女儿课堂上的猜数字游戏为蓝本，与女儿一起打造出《猜数字大师》游戏应用，在跨界鸿蒙开发的道路上，既攻克了技术难关，也收获了别样的亲子时光。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文： &lt;a href=&quot;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&quot;&gt;https://www.infoq.cn/article/rwSKfSRNBoL4HUv85zQ7&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 02：00 后鸿蒙开发者支一郎：从校园需求出发，用代码搭建跨场景服务桥梁&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在鸿蒙开发者生态中，有这样一位特殊的身影：他是 00 后在校大学生，却已凭借全栈技术能力成为 InfoQ 等技术平台的新星创作者；他从校园生活的痛点切入，牵头打造服务上万师生的 “校园智慧服务站”；他以小程序试水职场需求，再借鸿蒙原生能力迭代出融合 RPA 与 AI 的高效工具。他就是支一郎，一位在鸿蒙生态中快速成长的学生开发者，用实际行动诠释着 “年轻开发者如何在新兴生态中找到自己的价值”。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/1Su6kKzAuVQ03k8DZqkK&quot;&gt;https://www.infoq.cn/article/1Su6kKzAuVQ03k8DZqkK&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 03：从“探索”到“布道”，一个「鸿蒙领航者」的炼成记&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年 8 月，即将踏入大学校园的李浩佳第一次在新闻中看到了“HarmonyOS”的名字。那时的他，还只是一个对软件工程充满好奇的新生，未曾想到这会成为他职业生涯的重要注脚。六年过去，鸿蒙已从一个陌生的名词，变成了他日常开发的核心技术栈。他也从一名普通开发者成长为社区的技术分享者，持续为鸿蒙生态贡献力量。李浩佳带着骄傲对 InfoQ 说，由于在国内外平台积极分享，他已经两次获得“HarmonyOS 学习资源创作先锋”的荣誉称号。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/27Q3D8PGVvJXA8F5jWKx&quot;&gt;https://www.infoq.cn/article/27Q3D8PGVvJXA8F5jWKx&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 04：用“成语”疗愈“心情”，一位鸿蒙开发者的创意与选择&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你上一次阅读成语是什么时候？在当下短视频主导的时代，人们的生活日益碎片化，与文字的接触也渐行渐远。“俺也一样”和“提笔忘字”已成为当代青年日常生活的真实写照。语言能力的日益衰退，不仅会削弱表达能力，还会影响个人的认知与思考过程。相关研究显示，一个人的语言水平甚至直接关联其情绪认知与调节能力。基于这一洞察，深圳市蛟龙腾飞网络科技有限公司创始人李洋借助鸿蒙系统，开发了一款名为“成语心情”的应用。该软件根据用户日常心情和工作生活场景，提供针对性的成语学习，帮助深化对情绪与情境的理解。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/XXJf0Hd0zQ0JKlAjHgc0&quot;&gt;https://www.infoq.cn/article/XXJf0Hd0zQ0JKlAjHgc0&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;🚀推荐案例 05：待到山花烂漫时：鸿蒙开发者 用代码灌溉鸿蒙花园&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;银行业如何在鸿蒙转型中抓住机遇、快速进化？&lt;/p&gt;&lt;p&gt;吉林银行作为吉林省经济发展的 “金融引擎”，在数字化转型浪潮中勇立潮头。其开发团队通过分布式架构重构、ArkUI-X 框架迁移及原子化服务开发等技术突破，历时 21 个自然日完成 HarmonyOS NEXT 核心功能版本适配。今天让我们采访一下吉林银行的鸿蒙开发者代表卢妍娆女士，一起听她讲讲应用适配 HarmonyOS NEXT 的故事。&lt;/p&gt;&lt;p&gt;完整案例内容，请点击链接阅读原文 ：&lt;a href=&quot;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&quot;&gt;https://www.infoq.cn/article/FeR8sBoeFay7LuUeKhrF&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👉更多开发者群像案例，持续上架中，欢迎扫码加入「InfoQ鸿蒙开发者交流群」，交流技术，也可联系「小助手」约稿~&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bf7ecdf3d9166706d0b9b6263a6b802.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;👀也欢迎关注 &lt;a href=&quot;https://www.infoq.cn/zones/harmonyos/&quot;&gt;【InfoQ鸿蒙专区】&lt;/a&gt;&quot;，获取更多鸿蒙动态、创新实践！&lt;/p&gt;</description><link>https://www.infoq.cn/article/dmtGPG8qRCgNWzL06yvN</link><guid isPermaLink="false">https://www.infoq.cn/article/dmtGPG8qRCgNWzL06yvN</guid><pubDate>Tue, 27 Jan 2026 04:09:35 GMT</pubDate><author>付秋伟</author><category>HarmonyOS</category></item><item><title>从算力规模到系统级竞争：智算竞争核心已变，金山云战略升级曝行业“隐形拐点”</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从训练到推理：智算需求正在经历一场结构性转向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年，如果仅从“算力需求增长”来理解中国智算产业的变化，显然是不够的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2026年1月21日举办的金山云年度Tech Talk上，金山云对其过去一年智算业务的演进进行了系统性回顾。从公开财报数据到客户侧真实使用情况，这些信息拼凑出了一幅更清晰的图景：智算需求并非简单放量，而是在训练、推理、应用形态和工程方式等多个层面同时发生结构性变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场变化的核心，不再只是“谁拥有更大规模算力”，而是围绕模型如何被使用、Token如何被消耗、算力如何被组织展开。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;变化首先体现在财务数据上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据金山云披露的公开财报，其智算云业务在过去一年实现了高速增长。以2025&amp;nbsp;年第三季度为例，智算云账单收入达到7.8亿元人民币，同比增长接近120%。这一数据并非孤立，而是延续了此前多个季度的增长趋势，显示智算已成为金山云收入结构中的重要组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;金山云高级副总裁刘涛在分享中提到了金山云对这一趋势的判断：智算需求的增长重心，正在从训练侧逐步向推理侧转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从训练视角看，过去几年国内智算需求的主要推动力，来自少数对算力高度敏感的行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自动驾驶与具身智能，是其中最典型的代表。这些行业往往需要长期训练模型，并处理视频、点云、传感器等海量多模态数据。在早期阶段，它们对算力的需求更多集中在训练规模本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与通用大模型不同，这类行业模型并不一味追求参数规模最大化。刘涛在分享中指出，自动驾驶和具身智能模型在训练阶段，对算力密度的要求并不极端，但对显存容量和数据处理能力要求更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，它们对算力平台的诉求，正在从“算力数量”转向“系统能力”——包括数据接入、预处理、多模态调度以及训练全流程的工程化效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推理侧的变化更加显著。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说训练侧的变化仍然是渐进的，那么推理侧的变化则更为直接和激烈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个被反复引用的数据，来自火山引擎在其公开发布会上的披露：平台每日Token调用量已达到50万亿级别。这是当前国内少数被明确对外公布的Token规模数据之一，也成为行业理解推理负载的重要参考。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，多个面向大众或企业的模型产品正在持续扩大推理需求。例如豆包、通义千问以及近期加大投入的腾讯元宝，都在不同程度上推动Token消耗快速增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些产品并不完全运行在同一云平台上，但它们共同指向一个事实：推理阶段正在成为智算需求增长的主要来源，且这种增长具备明显的外溢性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在所有推理场景中，编程类应用被反复强调。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘涛指出，2025年一个尤为显著的变化在于：编程相关请求正在成为Token消耗的主力场景之一。这一判断并非孤立，而是与海外模型使用结构的统计结果高度一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Vibe Coding”成为一个关键词。一个广为流传的事实是，Claude Code的大量代码本身，正是由Claude Code参与生成的。这意味着模型不再只是辅助工具，而是深度介入软件生产过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从全球Token调用结构来看，编程类请求在多家模型服务商中长期占据最高比例。金山云也观察到了同样的趋势：代码生成、重构和理解能力的提升，正在显著改变程序员的工作方式，并直接放大推理侧算力需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在具体应用层面，互联网客户仍然是智算需求的重要来源，但其需求形态已经发生变化。刘涛提到，当前互联网场景呈现出三个明显特征：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其一，多模态需求显著增长。视频生成、视频理解以及复杂推理任务，带动了训练与推理负载的持续上升；&lt;/p&gt;&lt;p&gt;其二，模型参数规模不再单向膨胀，而是围绕具体任务进行结构性调整；&lt;/p&gt;&lt;p&gt;其三，Vibe Coding在头部互联网公司中已较为普及，使用更强的商用模型进行代码开发，正在成为常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些变化意味着，互联网客户对智算平台的期待，已经从“算力服务”升级为对模型生命周期管理和工程体系的整体依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了满足更多元化的需求，刘涛表示，2025年，智算平台金山云星流已完成从资源管理平台向一站式AI训推全流程平台的战略升级。从训推平台、机器人平台到模型API服务，升级后的金山云星流平台构建了从异构资源调度、训练任务故障自愈到机器人行业应用支撑、模型API服务商业化落地的全链路闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实现三维进阶，智算云AI势能全释放&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管各行各业大规模应用AI还处于早期探索阶段，但定位行业助力者的金山云，多年来持续打磨全栈AI能力。从2023年的智算网基础设施，到2024年智算云的平台化和Serverless化，再到2025年的一站式AI训推全流程平台，通过提升平台效率、突破行业边界、加速推理布局，金山云为迎接AI应用爆发做好了充分准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在平台效率方面，金山云星流训推平台提供从模型开发、训练到推理的完整生命周期管理，具备开发、训练、推理和数据处理四大模块能力，通过降低多模块协同复杂度，能实现“开箱即用”的AI开发体验。自研的GPU故障自愈技术结合任务可观测性设计，可实时监控硬件健康状态与任务进程，自动触发故障迁移与任务重调度，降低算力中断风险，保障长周期训练任务稳定运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为面向机器人开发与落地的全链路云原生平台，金山云星流机器人平台深度融合数据采集、存储、标注、模型开发、训练、部署与仿真等核心环节，打造具身场景专属的数据、模型、仿真一体化引擎。平台率先实现具身智能数据工程领域采集、标注、管理的全链路闭环，可高效服务具身智能行业模型训练、仿真应用场景分析等核心需求，助力客户快速完成从算法研发到真实场景部署的全流程落地，最终推动机器人产业的智能化升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面向大模型应用开发者和企业用户，金山云星流平台模型API服务提供高可用、易集成的模型调用与管理能力，覆盖模型调用的全生命周期。该服务支持高并发推理与多模型管理，能够帮助用户高效接入多种模型资源，助力大模型应用落地。目前，金山云星流平台模型API服务已积累诸多行业客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，金山云星流平台的模型生态也在持续丰富。目前，平台已支持近40种不同模型，包括DeepSeek、Xiaomi MiMo、Qwen3、Kimi等。客户通过一站式访问，即可高效接入多种模型，在畅享稳定高效云服务的同时，更加聚焦AI业务创新和价值创造。&lt;/p&gt;</description><link>https://www.infoq.cn/article/ELmQulBO3oXOzC1F76It</link><guid isPermaLink="false">https://www.infoq.cn/article/ELmQulBO3oXOzC1F76It</guid><pubDate>Tue, 27 Jan 2026 03:58:35 GMT</pubDate><author>李冬梅</author><category>芯片&amp;算力</category></item><item><title>让机器人“看清”三维世界，蚂蚁灵波开源LingBot-Depth模型</title><description>&lt;p&gt;空间智能迎来重要开源进展。1月 27 日，蚂蚁集团旗下具身智能公司灵波科技宣布开源高精度空间感知模型LingBot-Depth。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该模型基于奥比中光Gemini 330 系列双目 3D 相机提供的芯片级原始数据，专注于提升环境深度感知与三维空间理解能力，旨在为机器人、自动驾驶汽车等智能终端赋予更精准、更可靠的三维视觉，在“看清楚”三维世界这一行业关键难题上取得重要突破。这也是蚂蚁灵波科技在2025外滩大会后首次亮相后，时隔半年在具身智能技术基座方向公布重要成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在NYUv2、ETH3D等权威基准评测中，LingBot-Depth展现出代际级优势：相比业界主流的 PromptDA与PriorDA，其在室内场景的相对误差（REL）降低超过70%，在挑战性的稀疏SfM 任务中RMSE误差降低约47%，确立了新的行业精度标杆。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9a2618eab00ecdf730c1e675d8d0e97.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：在最具挑战的稀疏深度补全任务中，LingBot-Depth性能整体优于现有多种主流模型。图中数值越低代表性能越好。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在家庭和工业环境中，玻璃器皿、镜面、不锈钢设备等透明和反光物体物体十分常见，但却是机器空间感知的难点。传统深度相机受制于光学物理特性，在面对透明或高反光材质时，往往无法接收有效回波，导致深度图出现数据丢失或产生噪声。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对这一行业共性难题，蚂蚁灵波科技研发了“掩码深度建模”（Masked Depth Modeling，MDM）技术，并依托奥比中光Gemini 330 系列双目 3D 相机进行 RGB-Depth 数据采集与效果验证。当深度数据出现缺失或异常时，LingBot-Depth 模型能够融合彩色图像（RGB）中的纹理、轮廓及环境上下文信息，对缺失区域进行推断与补全，输出完整、致密、边缘更清晰的三维深度图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，LingBot-Depth 模型已通过奥比中光深度视觉实验室的专业认证，在精度、稳定性及复杂场景适应性方面均有良好表现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验中，奥比中光Gemini 330&amp;nbsp;系列在应用LingBot-Depth 后，面对透明玻璃、高反光镜面、强逆光及复杂曲面等极具挑战的光学场景时，输出的深度图依然平滑、完整，且物体的轮廓边缘非常锐利，其效果显著优于业内领先的3D&amp;nbsp;视觉公司Stereolabs 推出的 ZED Stereo Depth 深度相机。这意味着在不更换传感器硬件的前提下，LingBot-Depth&amp;nbsp;可显著提升消费级深度相机对高难物体的处理效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8dadb4233f1ed629b1f22f8dcd6d2043.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：搭载 LingBot-Depth 后，奥比中光Gemini 330 系列在透明及反光场景下深度图的完整性和边缘清晰度明显提升）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9a/9aa57018c78962b8705eeeb561d13089.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：其效果优于业界领先的ZED 深度相机）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LingBot-Depth 的优异性来源于海量真实场景数据。灵波科技采集约1000 万份原始样本，提炼出 200 万组高价值深度配对数据用于训练，支撑模型在极端环境下的泛化能力。这一核心数据资产（包括2M 真实世界深度数据和 1M 仿真数据）将于近期开源，推动社区更快攻克复杂场景空间感知难题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，蚂蚁灵波科技已与奥比中光达成战略合作意向。奥比中光计划基于LingBot-Depth 的能力推出新一代深度相机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WVlzEl33IYR1SH7mBssb</link><guid isPermaLink="false">https://www.infoq.cn/article/WVlzEl33IYR1SH7mBssb</guid><pubDate>Tue, 27 Jan 2026 03:23:16 GMT</pubDate><author>蚂蚁集团</author><category>生成式 AI</category></item><item><title>OpenCost回顾2025年的里程碑事件，并制定2026年发展路线图</title><description>&lt;p&gt;&lt;a href=&quot;https://opencost.io/&quot;&gt;OpenCost&lt;/a&gt;&quot;项目——一个由&lt;a href=&quot;https://www.cncf.io/&quot;&gt;云原生计算基金会&lt;/a&gt;&quot;（CNCF）托管的开源成本和资源管理工具——&lt;a href=&quot;https://www.cncf.io/blog/2026/01/12/opencost-reflecting-on-2025-and-looking-ahead-to-2026/&quot;&gt;发布&lt;/a&gt;&quot;了一份年终回顾，回顾了项目2025年的开发进展，并概要介绍了2026年的优先事项。本次更新凸显了活跃的发布节奏、功能扩展（包括支持AI的MCP服务器）、通过导师计划和贡献推动的社区发展，以及扩展项目数据模型和成本追踪功能的计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，OpenCost社区发布了11个版本，增强了可用性并扩展了功能。重要新增特性包括：通过环境变量配置实现无需&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt;&quot;即可运行OpenCost的能力；Beta版Collector数据源（一个用于成本数据导出的通用框架）；具备健康监测与导出功能的诊断系统。OpenCost还优化了多云成本追踪能力，在Oracle和DigitalOcean等供应商的贡献下，扩展了追踪云及多云指标的能力。这些版本旨在让成本透明度在Kubernetes环境中更具可操作性，使项目用户和贡献者均能从中受益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年的一个重要里程碑是引入了&lt;a href=&quot;https://github.com/opencost/opencost&quot;&gt;OpenCost MCP&lt;/a&gt;&quot;服务器，使AI代理能够使用自然语言实时查询成本数据。这种集成有助于自动分析跨命名空间、Pod和节点的支出模式，使团队不必手动查询即可生成成本报告和建议。MCP服务器是作为一个默认组件引入的，能够输出清晰、分步骤的成本优化建议，将云计算成本管理与新兴的AI生态系统相结合，满足自动化程度更高的FinOps工作流程的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社区活动同样表现突出。OpenCost项目通过&lt;a href=&quot;https://lfx.linuxfoundation.org/&quot;&gt;Linux基金会的LFX计划&lt;/a&gt;&quot;持续推进导师培训工作，受训者为企业就绪性贡献了集成测试，并推进了OpenCost数据模型2.0（KubeModel）的开发——该模型为跨动态Kubernetes资源实现可扩展的精确成本追踪奠定了基础。贡献者们还致力于完善文档、优化用户体验以及扩大社区参与度，进一步强化了OpenCost的开发者友好性及其生态系统的成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望2026，项目计划增强对AI使用成本跟踪的支持，因为机器学习工作负载对云计算支出的影响越来越大。围绕成本数据的供应链安全改进也是一个优先事项，同时，为了更好地反映Kubernetes资源行为的复杂性，项目计划对&lt;a href=&quot;https://opencost.io/blog/introducing-kubemodel/&quot;&gt;KubeModel框架&lt;/a&gt;&quot;进行迭代完善。参加即将到来的KubeCon+CloudNativeCon会议仍然是项目策略的一个关键组成部分，目的是提高项目在云原生从业者中间的认知度和采用率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为CNCF孵化项目，OpenCost在云原生领域的重要性日益凸显，因为成本可视化与资源调度已成为运维和财务治理的核心环节。该项目标准化了Kubernetes成本报告并整合了AI驱动的工具链，旨在帮助财务运维团队与工程团队应对2026年及之后日益复杂的多云工作负载的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/opencost-roadmap-2026/&quot;&gt;https://www.infoq.com/news/2026/01/opencost-roadmap-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6JfQa58o09NmOujZLf2C</link><guid isPermaLink="false">https://www.infoq.cn/article/6JfQa58o09NmOujZLf2C</guid><pubDate>Tue, 27 Jan 2026 03:12:45 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>理想汽车内部会曝光：必做人形机器人！全网急聘“最好的人”、连跳槽的前员工都要揪回来？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月26日，理想汽车CEO李想召开了一场两个小时的线上全员会。据多位内部员工反馈，李想强调，2026年是所有想要成为AI头部公司上车的最后一年；最晚2028年，L4一定能落地；最终全球布局基座模型、芯片、操作系统、具身智能等业务的公司不会超过3家，理想会努力成为其中一家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来，理想会进一步强化具身智能的品牌定位，而不仅仅是创造移动的家。在汽车之外，理想一定会做人形机器人，并会尽快落地亮相。”而接下来，理想为了迎接新一轮的AI竞争，公司将对研发进行新一轮的组织变革，将研发团队按照基座模型团队、软件本体团队、硬件本体团队等进行划分，其中汽车、机器人等都归为硬件本体团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，李想表示，要去招聘最好的人，把原来那些去到机器人创业公司的人再招回来。在此之前，已经有不少智驾核心技术人员从该公司离职，去具身智能赛道创业了。2025年下半年，前理想自动驾驶研发负责人贾鹏、量产负责人王佳佳与前CTO王凯等核心高管一起创办了具身智能公司至简动力，且很快就拿到多家头部美元基金和互联网科技公司的投资意向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，理想已在官网社招页面放出多个人形机器人研发岗位。从招聘信息可以看出，其研发项目几乎覆盖了人形机器人从核心部件到系统集成的全流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9d/9d9e57482830f408f388164b235df66c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2025年三季度业绩会上，李想公开表示，现在电动车行业拼参数已经拼到死胡同了，做智能终端又容易变成把手机应用搬到车里，属于重复建设，所以理想选了第三条路：把车定义成“具身智能”产品，让它从单纯的交通工具，变成有感知、有大脑、有神经、有心脏、有身体的“机器人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，早在2024年底的AITALK上，李想就说过，理想做人形机器人是肯定的，但还没到合适的时机。然而，此前，因为技术跟不上、人形机器人供应链不成熟等问题，理想暂停了人形机器人自研项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但理想在泛机器人领域的布局也一直在进行。2025年6月还有消息称，理想成立了“空间机器人”和“穿戴机器人”两个二级部门，都归高级副总裁范皓宇带领的产品部管，智能眼镜Livis是首款产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/mlWrTAeJsCJSREa1H4gZ</link><guid isPermaLink="false">https://www.infoq.cn/article/mlWrTAeJsCJSREa1H4gZ</guid><pubDate>Tue, 27 Jan 2026 03:08:30 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>DoorDash运用AI提升聊天与通话安全，将安全事件减少50%</title><description>&lt;p&gt;DoorDash构建并部署了&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;一个AI驱动的安全系统SafeChat&lt;/a&gt;&quot;，用于审核配送员与顾客在应用内聊天、发送图片及进行语音通话时的互动内容。SafeChat运用机器学习技术，可近乎实时地检测并响应不安全内容，筛查通信中的冒犯性或不当信息，并支持立即采取行动，如举报问题或取消配送任务。该系统专注于安全保障而非用户互动或自动化。它以AI为核心基础设施，旨在维护平台的信誉及保障配送员与顾客的利益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SafeChat使用了分层AI架构，结合了机器学习模型与人工审核。该系统每天处理数百万次互动，对文本消息、图片和语音通信内容进行分类。其文本审核分为两个阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在第一阶段，DoorDash工程师采用了一个三层方案。第一层是审核API，提供了一个低成本、&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;高召回率&lt;/a&gt;&quot;的过滤器，以极小的延迟自动清除了大约90%的消息。随后，未被清除的消息进入一个速度快、成本低的大型语言模型（LLM），它能以更高的精确度将99.8%的消息识别为安全信息。剩余的消息由一个更精确、成本更高的LLM进行评估，对包含不敬言语、威胁和性内容的消息进行评分。这些评分可以为采取安全行动提供支持，例如在检测到高风险消息时允许配送员取消订单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash基于第一阶段的大约1000万条消息训练了一个内部模型，第二阶段因此能够采用一种双层方案。内部模型为第一层，用于自动清除大多数消息。只有被标记的消息才会进入精确的LLM进行详细评分。第一层的响应时间在300毫秒之内，而被标记的消息可能需要长达三秒钟。该系统处理了99.8%的流量，提高了可扩展性并降低了成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1f/1f79a022d47d8b89de2cea30a1b1dd0b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DoorDash双层文本审核架构（图片来源：&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;DoorDash技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;图片审核由计算机视觉模型负责处理，模型的选择依据是吞吐量和粒度。为了减少误报和漏报，他们通过反复的人工审核调整了阈值和置信度分数。该系统每天可以处理数十万张图片，而且延迟可以满足实时互动的要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语音审核最初以仅观察模式部署，用于校准置信度分数。待阈值验证完成后，系统即可自动执行干预措施，例如中断通话或限制后续通信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/085bcba234e0f7a17b610efba0201418.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DoorDash语音审核架构（图片来源：&lt;a href=&quot;https://careersatdoordash.com/blog/doordash-safechat-ai-safety-feature/&quot;&gt;DoorDash技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上，作为&lt;a href=&quot;https://www.linkedin.com/posts/doordash_safechat-doordashs-ai-powered-safety-feature-activity-7393757928467562496-SGMo?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;特性发布公告&lt;/a&gt;&quot;的一部分，DoorDash表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;SafeChat通过AI创新和精心设计，使平台上的每个人都建立起信心和信任，为所有用户打造一个更安全顺畅的使用体验。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SafeChat的执行层根据违规的严重性和反复性采取相应的行动。它可以阻止或删除不安全的消息，终止通话，限制通信，或将问题升级给人类安全代理。重复出现或严重的违规行为会触发账户审查或暂停。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据DoorDash工程师的报告，通过将分层AI模型与&lt;a href=&quot;https://en.wikipedia.org/wiki/Human-in-the-loop&quot;&gt;人类反馈循环&lt;/a&gt;&quot;相结合，SafeChat系统得以实现大规模运行并保持近乎实时的响应速度。按照该公司的说法，该系统自部署以来，已使中低严重程度的安全事件减少了约50%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/&quot;&gt;https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2UiF1yejWi2urcT4jhtO</link><guid isPermaLink="false">https://www.infoq.cn/article/2UiF1yejWi2urcT4jhtO</guid><pubDate>Tue, 27 Jan 2026 02:51:01 GMT</pubDate><author>作者：Leela Kumili</author><category>AI&amp;大模型</category><category>安全</category></item></channel></rss>