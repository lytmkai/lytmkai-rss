<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Mon, 19 Jan 2026 04:05:28 GMT</lastBuildDate><ttl>5</ttl><item><title>阿里云 CIO 全面深度解析：企业 AI 大模型落地实践「 RIDE 方法论」</title><description>&lt;p&gt;据麦肯锡发布的《The state of AI in 2025》全球调研报告揭示， 88% 的企业已在至少一个业务职能中常规使用 AI（如 IT、营销、知识管理），但  62% 仍处于实验或试点阶段，仅有少量实现企业级的规模化部署。我们可以理解为，当下企业的 AI 落地正呈现“高采用、低价值”的典型特征，多数企业卡在试点到规模化之间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/84bd4dfab1e1871019a3cc5867d459b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;麦肯锡《The state of AI in 2025》报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 应用进入深水区，竞争的核心已经转向规模化的落地能力，而非技术本身。这也指向一个重要问题：当下的 CIO 群体，想真正实践 AI 大模型在企业的有效落地，实现规模化价值，要化解过程中的诸多坑点与难点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自阿里云智能集团副总裁、 CIO 蒋林泉在 AICon 2025 年 8 月所分享的 “阿里云大模型应用落地实践之路”，并完整呈现他对企业AI落地的经典方法论“RIDE”和数字人案例。文中，通过规模化上线的 28 个数字人的成功实践经验，分享从组织共识挑战、业务机会识别，到 AI 指标衡量，再到产品工程落地的体系化思考，以蒋林泉的第一视角，解析企业 AI  真实落地的系统路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第一视角观察&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我自担任阿里云 CIO 三年以来，第一次对外发表演讲。此次分享浓缩我过去三年在阿里云带领团队推进数字化与智能化进程中沉淀的案例与经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在担任 CIO 之前，我主要负责阿里云飞天核心系统的产品和研发工作，当时对外的演讲内容更多围绕飞天和阿里云的产品，角色也更偏向于“乙方”的产研身份。而今天，以阿里云 CIO 的身份首次对外演讲，更多是站在“应用开发者”的角度，分享如何在企业内部场景中推进数字化和智能化落地的一些实践与体会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b2533ae16f00775108b486dba8de950.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;阿里云智能集团副总裁、 CIO 蒋林泉&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两三年，我带领团队致力于推动 AI 大模型在企业各类场景中的落地应用，在这个过程中有很多感触。想先谈一下，在这个阶段里的一些观察和思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今时代，我们常常会思考一个问题：一个人或者一个组织发展得这么好，到底是时代的原因，还是自己努力的原因？其实最主要还是时代的原因。我们能够发展到今天，很大程度上是因为坐在了一个很好的“电梯”。比如搭上了中国这个电梯，中国互联网的电梯，以及我所在的阿里巴巴这个平台的电梯。平台本身发展得很好，在上面自然也发展得很好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，在电梯里做俯卧撑，还是在平地上做俯卧撑？两者达到的高度是不一样的。个人努力固然重要，但更重要的是平台。我认为，在这个时代，AI 就是那个最大的电梯。无论是组织还是个人，有没有搭上 AI 这趟电梯，将直接决定在未来能够达到的高度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8c8ae65217f6c22f5268e44ab58a999.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;ARK INVEST 报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据 ARK INVEST 以往的一份调研报告预测，到 2030 年，算力性能相较于现在将增长 1000 倍。这是什么概念？在 AI 时代之前，我们常常讲摩尔定律，技术性能大约每 18 个月翻一番。而在 AI 时代，技术发展的速度被极大地加快了。如果不能及时搭上 AI 这趟高速电梯，大概率会落后于时代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这样的认识，我们发现，无论是企业还是个人，都开始逐渐意识到 AI 的重要性。意识到这一点后，许多企业，包括 CEO 和业务部门，开始变得焦虑起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就涉及到，这一轮科技革命与以往的科技革命最大的不同之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个信息技术产业中，无论是 PC 互联网还是移动互联网时代，技术在企业中的应用过程是一个渐进的过程，非常循序渐进。那个时候，企业的 CEO 看到业界的炒作、厂商的炒作，都比较冷静，可以慢慢来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，这一次的情况却截然不同。我觉得这是第一次，企业 CEO 和业务部门比 IT 团队、比供应商还“上头”。因此，我们可以说，现在企业内部最大的矛盾，就是业务部门在社交媒体、PR 渠道里看到的 AI，往往呈现出一些“炸裂”、“梦幻”的效果，而 IT 部门或者说 CIO，在实际生产力上的发展却是不均衡、不充分的。这种矛盾体现得非常突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阿里巴巴集团内部，以及我与业界几十位 CIO 交流的过程中，观察到，在企业内部，这种现象大量存在。企业中会涌现出很多 Idea，做出很多 Demo，上线很多技术平台，一个团队里，恨不得要搭好几套 Dify 平台，各种智能体平台都在搭建。但是，在这些过程中，还是技术主导比较突出，更多是拿着平台去做 Demo，业务方的参与往往比较浅层。这类现象在企业里是比较过剩的，可以说整个企业都充斥着类似的情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/78d6cdd7990881aa70079cdefc1c6083.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，我在企业中普遍观察到，很多方面的投入都严重不足：是否真正深入到业务本身去做价值识别，或去正确地定义产品，以及如何开展知识工程（注意，这里我们不再仅仅是传统的软件工程，而是知识工程），还有我们强调的业务专家知识动员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们认为，如果要在企业里真正用好 AI，并且产生实际的业务结果，就要做非常大的投入。恰好，在这个领域，我们做了很多探索和实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;阿里云企业大模型应用实践落地&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，想向大家展示阿里云内部企业 AI 大模型业务落地的全景图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这张图中，我们可以看到很多“数字人”，无论是在阿里云的官方网站、CRM（客户关系管理系统）、业务支撑系统，还是在内容管理系统、人事管理系统中，这些数字人都已经广泛地落地应用，并在原来的业务中发挥真实的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过程中，我们已经落地了大约 28 个数字人项目，从中挑几个有代表性的例子来分享，让大家更有体感。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ed/edf0d08a52ae28b3e83b7faee353744c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI 翻译数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家都知道，翻译是大模型非常擅长的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在阿里云，我们遇到过很大的挑战。作为一家公共云服务提供商，为客户提供服务时，文档的作用至关重要（ ToB 的服务非常依赖文档）。阿里云拥有 300 多个产品，十几万篇文档，涉及上亿文字。其中有一个非常大的痛点在于“出海”，我们要出海到日本、美国、欧洲、印尼，还有土耳其，而我们的开发者要高度依赖文档，来操作云计算服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题在于，我们缺乏既懂本地语言，又懂云计算的人才，技术类的翻译必须同时具备这两方面的能力。但即使有足够的资金，也很难招聘到这样的人才。过去，我们只能选择“忍”，仅翻译了英文文档，以及部分日文文档，而其他语言的翻译工作基本停滞不前，这也导致海外开发者的反馈不佳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一轮 AI 技术突破之前，我们尝试过用传统 NLP 来做翻译，但效果根本不行。到了 ChatGPT 3.5 版本，我们发现自然语言处理技术，仍然无法满足我们的需求。而到了 ChatGPT 4 版本，我们再次尝试发现，翻译质量终于能和那些“既懂技术又懂本地语言”的专业译者打平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，当时也做了计算（时间在一年多前），每篇文档的翻译成本，仅为当初专业技术翻译团队的 1/200。从那时起，我们开始大量使用大模型进行翻译工作，到现在，我们已经完成了印尼语的全部翻译工作。这意味着，解决了原本靠资金也无法解决的组织问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用专业的评分来看，过去，用懂本地语言、懂技术的专业翻译团队来翻译，评分大约为 4.12 分（满分 5 分）。现在，我们用 AI 来翻译，评分能够达到 4.68 分。在海外市场，我们发现海外网站的用户体验以及 NPS（净推荐值）都得到了显著提升。因此，这不仅仅是一个成本问题，更是通过 AI 解决了过去无法解决的难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;技术文档验证数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才提到，阿里云有十几万篇文档，覆盖三百多个产品。其中，有一半是操作指南和解决方案，客户需要完全依照这些文档进行操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里一个很大的问题是：传统IT产品可能是半年或一年一个版本，文档和产品可以同步开发。但我们是互联网模式的 IT 系统，我们的情况是，线上功能不停迭代，功能的迭代和我们文档的一致性，就要实时保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原来，也是依赖外包团队进行文档验证和测试，由于“带宽”限制，只能解决中文文档的验证问题。每六个月会把所有文档“跑”一遍，去验证它们和线上功能是否一致，经常会发现有很多版本不一致的问题。但这个过程本身就有很大问题：首先一轮验证就需要六个月时间，当第一个月验证并修复好的内容，到第六个月，验证可能又变得不一致了。原来，我们一直没能把这个问题解决，导致客户经常会遇到功能与文档不符而操作不下去的问题，这就要求我们提供最新内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在我们是怎么做的呢？用 AI 来模拟这个过程：它会左边打开技术文档，右边操作浏览器里同步打开阿里云网站，然后严格按照文档里的步骤进行操作。过程中，AI一旦卡住或无法继续，就大概率意味着文档和实际功能不匹配。虽然少数情况是云产品控制台本身的问题，但绝大部分的确是文档与功能不一致。当AI发现不一致时，它会立刻把不一致的“单拎”出来，并自动创建一个Aone需求单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们后续还有一个“文档修复数字人”，它会“接手”这个Aone需求单，分析实际情况与文档描述的差异，并做修复。然后，它会把这个修复好的文档，给到我们technical writer做确认，确认后就能上线了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这之后，过去需要六个月才能完成一轮的验证工作，现在只要一个星期。同时，我们现在也把这套验证机制应用到日文、英文以及其他语种上，确保国际站的功能和文档也能保持一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去靠人工验证时，一致率到底是多少？验证质量好不好？覆盖度够不够？这些其实都是一个“unknown”的状态。而现在，一切都变得清晰、可量化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;网站 AI 助理数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三个案例是网站AI助理。阿里云有几百万客户，那我们的自服务模式是怎样的呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看一组数字：每天大约 97% 的客户访问阿里云，都是通过自助操作，只有 3% 的客户会选择“提工单”。而在这3%的客户中，百分之七八十的任务也还是由自己解决的，只有极少数最终会变成需要人工介入的工单。所以，我们的客户绝大部分是自服务的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便如此，由于我们的客户基数太大，这“漏”进来的一小部分工单，依然需要我们服务团队投入大量人力去处理。在这些工单里，有一半都属于“咨询工单”。什么是咨询工单呢？就是客户遇到问题直接提问，我们的小二在后台查文档、翻知识库（Knowledge Base），找到答案再回复给他。这类工单纯粹是信息问答，不涉及操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类工单主要有几个问题：第一是一半的工单服务成本很高，第二是个时效问题。我们统计过，过去一个咨询工单的平均关闭时间，绝大部分要到5个小时左右。也就是说，一个客户平均想要解决这种咨询问题，需要花费大量时间才能解决。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网站 AI 助理上线后，大量的咨询问题已经由 AI 直接回答了，而平均响应时间是 10 秒左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们正在和服务团队合作，与服务团队共同承担全年工单降量，我们一起努力，希望通过 AI 在网站自服务的深入应用和渗透来实质拓展服务带宽，更重要的是，能够一起提升阿里云的客户服务体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;智能电销辅助数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才讲的是服务，探讨了如何帮助客户解决咨询工单和自助诊断的问题，把服务体验提升了。现在来看另一个场景：销售。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云要服务上百万的企业，无法对每一家企业都用直销的方式去覆盖。因此，我们有很大一块业务是面向中小企业（SMB），通过电话销售来帮助我们客户实现售前咨询，以及售前购买的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;电话销售小二的日常工作，主要分为话前、话中、话后三个环节。话前： 小二需要做计划，规划当天要打哪些电话、了解客户的商机、准备话术，并排好优先级。需要这样一个准备过程，才能保证一天的工作有序高效；话中： 就是与客户的实际沟通；话后： 需要复盘，记录通话小结，整理哪些需要follow-up，哪些需要申请折扣。需要处理的问题都要记下来，这样才能闭环到后续的业务处理，形成一个完整闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，我们在这三个环节都提供了 AI 数字人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话前”，由AI来完成通话计划，包括怎么打，话术是什么。过去小二自己排计划要花半个多小时，现在一上班，计划就已经生成好了，可以直接开工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话中”，我们提供了一个智能辅助提醒。当小二与客户通话时，系统会根据对话内容，在工作台右侧实时提醒他如何回答，比如客户在说他想要这个，建议你这么回答。目前已经在辅助小二去解答客户非常复杂的一些云计算咨询问题。目前话中提示小二的采纳率已经达到了50%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;● 在“话后”，像通话复盘、撰写小记、follow up，包括后续的通话质检，这些工作都交给了一个自动化的AI数字人来完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们的小二可以从繁杂的事务性工作中解放出来，集中精力在真正的销售沟通上，大幅拓宽了我们销售的服务带宽。同时，AI 的智能计划、实时辅助和后续复盘，也极大地提升了我们服务客户的质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;智能质检数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 应用到电话质检之前，这几乎是一个原理上无解的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原来我们大规模的外呼电话作业过程，是非常难被知晓的。比如中间过程是否按照公司的作业规范进行？与客户的沟通是否足够礼貌？更有时候，有的外呼人员可能会把客户引导到私下公司去联系、去成交。但原来，我们是很难去做这个电话质检的，因为这是语音作业，很难管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而现在，我们用 AI 把所有的电话语音全部智能化，从而识别里面所有的这些问题，再通过统一的质检标准，就能够得到一个规模化的质检。于是，这个AI质检能够大规模地提升我们的服务质量与效率，覆盖全量业务场景，关键还能控制我们的业务风险（避免产生额外的风险）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，这件事我们原来几乎是无法搞定的，因为原来是靠抽样，也就是人工抽样去听那些电话录音，如果抽样抽到了问题，再去一个个处罚，但效率是非常非常低的。它的抽样完整性、抽样覆盖度都几乎是没法被使用的（覆盖度仅有2%），不同质检员的判断差异也很大，对人力的消耗也很厉害。所以，现在通过AI质检数字人，能够让覆盖度提升到100%，质检的准确率也远高从前，带来的最终效果是非常好的，这使得整个服务质量能够规模化地提升上去。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;智能外呼数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才我们讲到AI如何辅助做事，这里则是一个能直接进行智能外呼的数字人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;众所了解，云计算本身是非常复杂的，如何招聘到足够多的外呼坐席人员，让他们既具备相关技能，又熟悉云计算知识，同时还能够耐心地每天坐在工位打一天的电话，这对我们来说是一个巨大的痛点。因为招聘和能力培养难度很大，人员流动率非常高，这使得无论是销售服务，还是电话服务的质量，都存在明显的短板。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本质来看，这是一个短线影响业务增长，长线影响服务满意度与企业品牌塑造的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在前期已经有一定的知识积累，包括语音、多模态等方面的经验，因此，我们通过 AI 的方式直接引入智能外呼。它直接上场，与我们的客户沟通，挖掘销售商机，交付给服务团队去做主动的服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，在潜在客户的线索清洗、免费试用、转生产、以及产品即将到期的续费提醒等主动外呼场景中，这个数字人已经上线运行了。目前，我们还在开发场景包括产品到期的主动关怀、NPS 调研等，上线后，预计可以拓展出“能交付结果的”上百个 HC 的服务带宽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数字 AI 客服的外呼，还有些不一样的特征。首先，它可以灵活、快速地按需扩容，而且，它的声音可以做得更甜美，也可以做得更有情商。更重要的是，在技术的不断加持下，这个AI小二解决问题的能力，可能已经超过了原来人类员工的平均水平，而且还在不停地提升。目前，我们的智能外呼数字人可以像“金牌销售”一样工作，非常接近真人体验。未来会有更多的想象空间，让它能够更好地服务阿里云客户，提升我们的服务质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;直销辅助数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享了很多电销案例，这里谈谈“直销”场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阿里云的直销业务中，我们面临着一个核心挑战：销售如何变得更加专业和高效，促进公司业绩增长？在实际工作中，我们的销售团队遇到了两大业务痛点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一个痛点：云计算销售要求高、招聘难、培养成本高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云计算销售不仅需要具备良好的客户拓展能力，还需要深入理解云计算技术与行业应用场景。复合型人才稀缺，招聘难度大、周期长，新人从入门到胜任，需要经历数月的培训与实战积累，培养成本居高不下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个痛点：销售运营专业服务带宽不足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;销售运营、数据BI、财务、法务等运营中台的服务带宽，无法充分支撑前线销售需求，难以及时响应每一位销售人员的专业支持诉求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这些问题，我们将整个销售流程分为“拜访前”和“拜访后”两个关键环节，在每个环节都提供AI数字人的全方位支持。核心围绕销售作业的有效性展开，让直销过程实现“在线化”，全面提升销售过程的辅助效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;拜访前：销售“一键”获取客户“谈参”，了解客户用云信息、技术类型、解决方案、竞对情况等全面画像。过去，销售自己从各渠道去查询要花1个多小时，现在，10分钟就能查询到，而且信息质量更优、内容更全面，有效促进了与客户key person的高质量拜访。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;拜访后：我们提供AI对拜访过程的全方位复盘，包括商机要点是什么，客户对阿里云品牌表现出的情感倾向是什么，建议后续怎么推进客户成单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI软硬结合的优势，我们让直销的销售过程实现“在线化”，高质量拜访小记达到100%全面覆盖，新销售也能通过高质量在线信息资产快速学习，上手周期缩短50%，大幅降低新人培养成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式，相当于拓展数百位专业销售运营为销售团队“贴身辅助”，销售人员得以从繁琐的流程性工作中解放出来，能够更专业、更高效地服务客户，大幅提升了销售有效性，有力促进了公司业绩增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;合同风险审核数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ToB 业务的一大特征，是有大量的政企和大客户，他们通常不会使用我们的标准合同。这些合同金额巨大，需要进行严格的风险审核，涵盖财税法、风控、信控等多个方面的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去，要完成这样的风险审核，我们需要专业的法务、财务等领域的精英人士，他们大多来自国际四大会计师事务所。然而，鉴于我们业务规模庞大，不可能招聘到足够多的精英来从事这项工作。因此，我们在合同风险审核方面遇到了巨大瓶颈，审核时间过长，最长甚至可达 5 个月，平均也需要两周到一个月。这极大地拖累了业务效率，包括服务大客户的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们培养了一大批“数字人”，包括财务数字人、信控数字人和法务数字人。并且，把这些数字人送到合同撰写端，让他们在销售和客户沟通、合同拟定的瞬间，就能够实时识别潜在风险并提示谈判方案，而不是等到审核端后才发现问题，再回过头去处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在合同审核端，我们通过审核标准数字化、专家经验数字化，用统一的标准执行，极大提升了准确率。而AI也正是实现知识工作线下流程线上化的体现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI技术，我们不断拓展中后台的服务带宽，解决商业拓展流程中的效率瓶颈。后续，我们也期望它在风险拦截上的能力，能够持续提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;员工服务数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么特别提到员工服务数字人？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为大型企业里，HR 系统有一个显著特征，就是非常分散。比如请假、体检、福利、在职证明等，各式各样的流程和服务都散落在不同的系统里。与此同时，各类政策也同样分散，包括公司内部的福利政策、外部的人才政策等等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;员工在需要获取这些信息或使用这些系统时，会遇到两个难点：第一，这些服务是低频使用的；第二，它们分散在不同地方，获取难度非常大。由于是低频服务，无法配备一个庞大的服务团队来支持，所以 HR 团队的负担很重，而员工的服务体验也不足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们将这些低频、分散的服务全部整合到一个智能体中，通过钉钉平台打造了一个“云小宝”（数字人），为员工提供统一的智能服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们发现，通过引入智能体，折算下来相当于节省或新增了几十名员工在为大家服务。更重要的是，员工的体验得到了极大提升，比如，我们服务员工的响应时长已经从平均7.2分钟缩减到5秒。再比如，员工只需要用自然语言输入，如“下周一请假”、“国庆前后两天请假”或“为父亲预约体检”，系统就能迅速响应并完成操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;面试智能辅助数字人&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个场景，我们聊聊招聘。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们对外招聘，核心是描述我们需要什么样的人。从这个角度出发，前置是OKR，我们通过AI分析每个部门日常在做什么，目标是什么，根据日常目标和事情，去看清楚招聘的JD（职位描述）是不是合理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再者，从JD开始，根据岗位要求，再结合当前的候选人简历信息，在面试的时候就会生成面试计划。面试时，结合岗位要求，面试官应该问哪些问题？根据最佳实践，怎么去考察候选人？这些专业问题在面试前，已经帮面试官提供好。面试中，通过对话过程，发现应该追问哪些问题，以及面试后，怎么总结面试过程中候选人是不是qualified这个岗位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过AI，我们可以更结构化、体系化地来做这件事，使得面试过程管理，面试质量，以及对面试人评价的客观性，都得到很大的提升。这也彻底改变了原来仅仅通过电话形式的面试，因为它的过程是一个黑盒逻辑，而“黑盒”最大的问题是无法提升过程的质量，包括保持长期的、闭环的有效性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对一家公司来说，招聘是件非常严肃的事情，我们经常讲，如果招错一个人，会导致后面的事情是非常糟糕的。所以本质上来讲，面试智能辅助数字人，提升了我们整个组织在招聘进人方面的有效性。这不只是效率问题，而是能够规模化促使我们在面试过程中的专业性、面试评价的专业性得到质的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;28个数字人全面上岗，真正产生业务价值&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3c3e51ab6dca7d2a3779dedeac199a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们有二十几个场景实现了数字人的智能化服务，这里只是挑选了10个来举例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些数字人应用背后的评估衡量，有一个共同逻辑：&lt;/p&gt;&lt;p&gt;一是折算拓展了多少人力；&lt;/p&gt;&lt;p&gt;二是业务效率提升了多少；&lt;/p&gt;&lt;p&gt;三是业务效果提升了多少。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们非常注重这一结构，因为每一个数字人上线落地，都必须衡量其对原来业务是否真正拓展了服务带宽 ，并且，是否比原来人工操作的效率和效果更好，这是非常关键的，与外界所谓的众多智能体最大的区别，就在于此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些智能体最终都是在对应的岗位上实际工作的。在我们的 HR 系统中，这些数字人被分配到对应的业务部门，向对应的业务团队汇报工作，与我们从外部招聘的员工没有任何区别。所以，它们必须在对应的岗位和业务团队中，发挥超过一定人数的实际任务执行作用，才能真正融入团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我们的钉钉系统以及内部工作系统中，这些数字人与普通员工一样，拥有工号和头像。唯一的区别在于，它们的工号以“AI”开头，如 AI001、AI002，目前我们已经有大概 28 个智能体上线，后续还有更多智能体在排队等待上线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，在过去两年，带领团队推进业务落地的过程中，我也深刻体会到，真正将技术应用于业务并取得成效，没有那么简单。特别是，真正在业务中产生价值和仅仅做出一个 Demo 之间，是天壤之别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，想和大家进一步分享，我们在这一过程中遇到的困难，以及总结出的一些解决方法，希望能对大家有所帮助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型 E2E 落地坑点与解法 —— RIDE&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家可能听过红杉提出的一个概念叫&amp;nbsp;RaaS，即“结果即服务”。这一概念的核心在于，如果仅仅提供工具和产品，让企业自行落地是不够的。所以，我们特别重视真正上线，并产生业务结果的项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我作为 CIO 所带的团队，在企业内部为业务部门提供的，就是这种&amp;nbsp;“以交付结果为导向的服务”。在推进 RaaS 的过程中，也总结出一套方法论，叫&amp;nbsp;RIDE。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e4/e49d732c30605558b4469bd17b2a70da.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RIDE 包括四个关键步骤：Reorganize（重组组织与生产关系）、Identify（识别业务痛点与 AI 机会）、Define（定义指标与运营体系）、和 Execute（推进数据建设与工程落地）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是&amp;nbsp;Reorganize。在 AI 时代，新的生产力下，原来的生产关系是非常不适应新生产力的发展，这种不适应会在每个毛孔里面表现出来，然后阻碍 AI 的发展和落地，所以要求我们要重新调整生产关系。第二，是&amp;nbsp;Identify。也就是我们需要精准地识别出企业中哪些问题适合用 AI 来解决，这要求我们首先明确问题的定义，然后结合 AI 的能力和业务需求，确定哪些问题可以通过 AI 得到有效的解决。然后是&amp;nbsp;Define。在明确了问题和 AI 的能力之后，我们需要精准地定义产品及其运营指标，进行准确的指标跟踪。最后才是&amp;nbsp;Execute。执行阶段是一个金字塔结构，上面是业务目标，下面是工程数据和评测，中间是工程应用算法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这套我们称之为 RIDE 的方法论，并非在做 AI 转型的第一天就有了，而是在二十多个智能体真正有效落地业务的过程中，我们发现，如果不遵循方法论中的这些步骤，项目很可能会失败。遵循这些步骤，虽然不能保证 100% 的成功，但至少可以提高成功的概率。这是一套用两年时间、用血泪经验总结出来的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Reorganize ｜重组组织与生产关系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;书同文、车同轨 ：AI时代的通识教育&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们首先从&amp;nbsp;Reorganize&amp;nbsp;开始讲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在落地第一年，我发现了一个问题：无论是业务团队还是我们自己的团队，对大模型的能力边界、发展程度、具体原理等基本概念的理解都存在差异，甚至在我自己的团队，产品经理、算法、工程团队内部都无法拉齐概念认知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这一问题，我们发起了一个行动，叫&amp;nbsp;“书同文、车同轨”。&amp;nbsp;我们要求全员参加&amp;nbsp;AI 大模型的认证培训。最主要的原因，是要解决大家在基本功和认知逻辑上的差异。我称之为&amp;nbsp;“AI 时代的通识教育”，相当于要在团队里重新走一遍“高中的教育”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一培训分为两类：大模型ACA认证（面向非技术人员）、 大模型ACP认证（面向技术人员），因为我们不仅需要技术人员之间能够对齐话语，也希望非技术人员和技术人员对齐话语。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种通识教育对于团队的协作至关重要，首先在我们 CIO 线内部已经完成了全员的认证，后面，我们的业务方，也就是我们的财务、人力、销售、中后台等都在做&amp;nbsp;全员认证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，整个阿里巴巴集团都在用这个方法来做 AI 转型的基础教育，重新建立大家的基础认知。不然就会出现这种情况：大家都在谈论同一个概念，但其实理解的内容和现实完全不同。如果没有做过深入工作，很难体会到那种无力感，一旦通过通识教育统一认知，沟通效率就会显著提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/32/3210b1f4ab3dd2665737f14423be6824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云大模型ACA认证：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://edu.aliyun.com/certification/aca13?spm=a2cwt.28380597.J_1564692210.17.28813487dUqGKW&quot;&gt;https://edu.aliyun.com/certification/aca13?spm=a2cwt.28380597.J_1564692210.17.28813487dUqGKW&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;阿里云大模型ACP认证：&lt;a href=&quot;https://edu.aliyun.com/certification/acp26?spm=a2cwt.28380597.J_1564692210.18.28813487dUqGKW&quot;&gt;https://edu.aliyun.com/certification/acp26?spm=a2cwt.28380597.J_1564692210.18.28813487dUqGKW&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;「企业免费体验」大模型认证：&lt;a href=&quot;https://edu.aliyun.com/learning/topic/llm-free-trial&quot;&gt;https://edu.aliyun.com/learning/topic/llm-free-trial&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样的基础上，又设计了两个比赛。一个是产研提效比赛，一个是业务提效比赛。和其他大赛最大的不一样，我们的比赛是真正以 E2E 为衡量标准的。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如产研比赛，我们要看的，是原来 E2E 同样粒度的一个需求，需要多少“人月”完成，而现在能减少到多少人月。而不是看代码采用率，因为代码采用率很容易“灌水”，而且它往往只能补全那些最容易写的代码，最难的代码可不容易补全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在业务 E2E 方面，我们的比赛就是要真正进入业务场景，帮助业务去拓展，而且效果和效率都要超过原来。所以，这两件事非常重要，第一，是做“书同文，车同轨”的通识教育，因为 AI 时代的知识在不断发生巨变，每个月都在变，现实的实践知识和原来的基础知识都有大量的不同；第二，是“以赛促练”，整个组织通过正确目标下的比赛，大家会发现短板，发现相互之间可以学习的地方，就能够激发组织不断地去创新、去提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;数字员工 ：业务方与IT方 联合培养&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说说我们的数字员工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有一个非常关键的安排：我们的这些数字人最后都是汇报给业务部门的。这不仅关乎形式，更重要的是心理。我们不能让业务部门觉得，AI 技术会威胁到他们的工作，而是要让他们明白，AI 技术是来帮助提效的。如果这个关系没处理好，就会遇到无数的暗礁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/88e14865fc7aa21a7640472b6bec9423.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，我们把自己定位为数字人供应商，业务部门是 AI 先进组织，业务部门可以雇佣我们的数字员工，并与我们一起联合培养。&amp;nbsp;这样，业务部门会更愿意接受 AI 技术，减少阻力。所以这是第一点，我们把自己退到一个外包供应商的位置上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二点，我们还发现，AI 数字员工是不能扛责任的，也不能给它打“3.25”（低绩效）。这意味着，数字员工在系统里执行任务出了问题，谁来承担的问题。我们将 AI 数字员工汇报到业务部门，属于业务部门的人（让他们放心），并一起参与 AI 员工的培养过程，同时数字员工也会受到正式员工的监督，来承担相应业务领域的责任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;定标准 ：AI要与人比，不与“神”比&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我们经常听到一句话：ToC 还好，但 ToB 的大模型有幻觉，做不到 100% 正确。但实践经验告诉我们，其实人也有幻觉，而且人的幻觉还很大。如果认真看，在很多任务里，人其实也是不靠谱的，也经常会失败，只是企业没发现而已。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们强调的一点是：如果 AI 项目和业务部门真正达成了共识，并且通过培养逐步磨合，就必须认真回头来看，AI 的要求标准到底是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要求 100% 正确，其实就是把 AI 拿来和“神”比。但如果是和原来人做事的效果和准确率去对比，那就是和“人”比。所以，追求比人做得更好、更准，才是真正有意义的对标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那怎么避免和“神”比呢？回到前面所说，解决生产关系的问题，处理好内部业务的逻辑、目标和关系，这样才能真正实现 AI 和人比，而不是和“神”比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fc/fc59a744ddd066e6b79446f598c0a184.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个&amp;nbsp;Reorganize&amp;nbsp;的过程中，我们还发现，要把数字人汇报到业务部门，对 HR 部门来说，这就等同一个“正式员工”。注意，我们是真的把它当作正式员工来看待的，用它能否产出真正的业务结果来度量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们在内部与 CPO（HR 负责人）沟通时，讨论过：怎么去度量 AI 数字人是否真的发挥了一个正式员工的效果？最后，我们确定了一个方向：AI 数字人必须有一个目标，就是在原有具体的业务流程里，接管一个重复且有价值的任务，并且能够折算出“相当于拓展出多少人力”，这就是唯一的目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但要&amp;nbsp;真正让数字员工上线、上岗，必须满足两个标准条件：&amp;nbsp;一是数字人执行原来任务的效率，一定要比原来提升一定百分比，一定要比原来执行任务的人效率高；二是数字人执行任务的效果，同样，也要比原来提升一定百分比。只有当数字人做到效率高、效果好时，才能“正式上岗”，进入业务部门工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Identify ｜识别业务痛点与 AI 机会&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从三个特征，挖掘AI机会&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;刚才讲的是&amp;nbsp;Reorganize，如果不解决 Reorganize 的组织问题就会不断遇到暗礁，甚至没法往前走。但解决了组织的问题后，业务部门会说，好，我们来联合培养数字员工。那从哪里开始呢？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以第一件事就是业务机会的识别（Identify）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2496e59b238364c5d2bc83c77059b1e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这轮 AI 革命的核心其实是 LLM（large language model），所以，我们在内部有一个逻辑：所有以 language 为中心的工作，都将被大模型深刻影响。比如电销、客服、招聘、OKR、文档、翻译、合同审核，还有研发类的 C language、Java language、SQL language 等，这轮以 language 为中心的工作受影响最大。所以第一个特征是&amp;nbsp;Language 类&amp;nbsp;工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二个特征是被重复执行、规模化执行。因为 AI 是自动化的，越大规模、越重复的任务，AI 越有机会去做。第三个特征是，如果本身缺人，甚至有人投诉效率低，那这个地方就是个大的机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三个特征，是我们与业务部门一起来 Identify，识别哪些业务是可以着手的。这也是我们在内部形成共识后，如何去识别机会、定义机会的关键点。因为只有把问题定义清楚了，后面做事才会顺畅。如果解决错了问题，那投入就白白浪费了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;数字员工，以“单任务”为核心换算&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，我们刚才讲到，数字员工要在对应的任务里拓展目标，也就是拓展对应岗位的人力，实际面对各种场景具体怎么处理，又怎么核算？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f33e3f605ada391bc70ba68a0fa1f467.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的经验是，首先，有些“单任务岗位”，比如技术翻译，我们是按字收费的，那么，AI 翻译一个字多少钱，就可以直接线性替换了。一个人一天的产能可能是翻译 2 万字，那我们就差不多折算成 “2 万字的产能”等同于“一个人”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果是“多任务岗位”，比如产品经理，他一会儿做 PRD，一会儿分析工单，一会儿画 Demo，一会儿又去客户那里访谈。这种多任务岗位，我们发现往往有些任务是重复的、繁琐的，也不是高价值的。为了提效，非常适合将这些低价值任务，拆分成一个个“单任务岗位”，如工单分析岗位、产品原型设计岗位等，让数字员工去做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，原岗位上的人就从繁琐工作中卸载出来，可以聚焦在更高价值的主线工作上，他们的幸福感也会爆棚。在换算方面，最终也都是”以单任务岗位为核心进行HC换算”，逻辑清晰明了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种方式原先主要是由外包承接，但受制于外包员工管理难度大、成本构成多、招聘周期长、稳定性低、用工风险高、能力上限低（薪资因素）等诸多原因，多方面都受到约束，无法大面积展开。当我们有了数字员工之后，自然解锁了这些约束，&amp;nbsp;这件事就变得更加切实可行。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Define ｜AI 的产品度量与运营度量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;准确率是AI产品核心&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过了&amp;nbsp;Identify&amp;nbsp;这一步，下面就是&amp;nbsp;Define。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个时代和以前做产品有很大不同。我们前面提到的一些产品大多都类似，比如都有交互、体验。在这个流程里，其实和上一轮移动互联网的产品没有区别。但&amp;nbsp;AI 产品有一个特别关键的点，就是“准确率”。&amp;nbsp;当然，除了准确率之外，还有响应时效性和安全合规等非功能性指标，比如在电销过程中，和客户实时对话，延迟必须非常低，不然客户会觉得交流效率不高，像机器人说话一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9b45111fb0badee9130ac4bc4cf4cb6f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，实时性和准确性非常关键。如果准确性不够好，客户根本无法使用，也根本不可能真正上岗。所以，准确率是 AI 项目的第一核心指标，整个项目组都必须盯住它，这也是产品定义中最核心的部分，必须重新去&amp;nbsp;Redefine。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;运营与产品指标「协同度量」，才不掉坑&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，运营指标同样至关重要。如果只有产品指标和准确率指标，那大概率会掉到“坑里”。即使是在对内的业务项目里，原来移动互联网那些基本功也不能丢，比如：&lt;/p&gt;&lt;p&gt;DAU（每日活跃用户数）；用户提问数；渗透率，即目标客户的覆盖率；留存率（最关键）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果同一个客户今天用了，下周还愿意继续用，说明这个 AI 智能体真正帮他解决了问题。如果客户只用了一次就不再回来，那么无论前面的产品指标再漂亮，都没有意义，那可能就是定义错了问题。运营指标就是用来兜底的，如果不紧盯这些指标，很容易让产品、工程和算法团队陷入“自嗨”。什么叫自嗨？就是他们说“我的指标很好”，结果客户根本不用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子，在阿里云官网的 AI 助理中，我们就设定了这样的度量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如下图所示，左图展示了准确度的度量指标，时间线大约覆盖从去年到今年的一年时间。蓝色区域代表表现良好的部分（精准解决了客户的咨询问题和任务），黄色区域为中等水平（虽解决了任务，但伴有大量无关信息），红色区域则是表现差劲的部分（回答与客户问题完全不相关）。中间图展示了 DAU 和客户问题数，右图则是留存率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/69/69aea369bbf9588752408a3a661b13a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们的留存率实际上已经达到了一个相当高的水平（PPT中并未刷新数字）。从图中可以清晰看到，随着准确度的持续提升，DAU 和留存率也在稳步上升。但是反之，如果 DAU 和留存率始终停滞不前甚至下滑，即使你的工程和算法团队声称准确率很高，那无疑是自欺欺人的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实际上，很多工程算法团队成员，可能并未意识到上述这一点。之所以能明确指出，是因为在左图的准确度指标上，我也曾经被多次误导，但这也并非团队有意为之。在如今的信息环境中，随便搜索公众号就能发现大量类似“用这一招，你的准确率能提升到 95%”的文章，但这些文章往往存在误导性，它背后都有一个前提条件，即在某个狭窄的小场景下，准确率能够达到 95%，然而在面对海量问题时，这一指标却难以提升（这一点稍后会详细分享）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Execute ｜推进数据建设与工程落地&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;掌握「产品研发工程金字塔」&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;定义好了产品和运营指标（Define），往下走才是执行（Exectute）阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Exectute&amp;nbsp;阶段的关键在于：一定要用产品和业务目标来拉动。因为在牵引拉动的过程中，才能充分动员领域知识专家的参与和评测。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果没有知识专家的深度参与和强大的评测能力，大模型的应用上限是很难提升的，这是第一点。第二，如果项目目标缺乏价值，或者没有真正的痛点，那么会发现得不到资源的“祝福”。也就是说，一方面难以获得其他团队的配合，另一方面自身团队的价值感也难以维持，这将直接影响项目的推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b32b199df39f232cd6c8a4b1f77f6b2d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在整个执行逻辑中，金字塔最下面是工程的数据与评测，我把这个放成最大的一块底座，因为这是基石——业务数据、业务 API 以及评测能力是大模型应用的基础，对这一部分的投入必须充足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一基础上，才是工程应用算法、预训练（Pre-training）、RAG 以及微调等等，这些在媒体报道里面出现的技术热词，并非不重要，但这些只是&amp;nbsp;“必要条件”。我观察到，大多数产研团队在这部分（工程 - 应用与算法）投入了 80% 至 90% 的时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但想强调的是：这些只是必要条件，仅靠这些无法解决企业 E2E 落地的问题。哪怕你在必要条件上投入再多，再加 10 倍的努力，也无法实现真正的 E2E 落地。因此，必须设法补齐真正实现 E2E 落地所需的充分条件。&amp;nbsp;如果无法做到这一点，项目成功的希望将十分渺茫。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;常见的LLM AI应用范式：翻译、Agent&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在与业务团队沟通以及处理各种复杂问题的过程中，我们总结出了几种常见的模式：&amp;nbsp;首先是基础设施层面，涉及知识和数据的构建；中间是编排和调度，无论是大家熟悉的工作流编排，还是智能体自主规划编排，或是两者的结合；最上面是对客的产品与运营。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里，重点讲述图中深蓝色部分的两种模式：第一个是翻译模式，第二个是 Agent 模式，我认为主要分为这两种典型的应用模式。其中，翻译模式最容易取得成效，因为它相对简单；而智能体模式则较为复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7d/7d19b513a0cc83ac769f6db55472de5a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;翻译模式：关键在“蛋糕坯”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;先谈谈翻译模式。&amp;nbsp;在公司内部，我们将所有翻译类模式统称为 AI 领域的“低垂果实”，这类模式相对容易实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一轮的大型语言模型背后的算法是 Transformer。Transformer 最早是 Google 为了翻译任务而开发，在不停做翻译的过程中衍生出了 Transformer 算法。随后，预训练模型如 BERT 也大量应用于翻译领域。所以，大模型的原理 Transformer 特别擅长做翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;翻译又可以分为狭义翻译和广义翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;狭义翻译指的是中译英、英译中等语言之间的转换。而广义翻译则涵盖更广泛的形式，比如：自然语音转成文本，再转成语音；自然语言转成 SQL 语言；自然语言转成 Java 语言；甚至让一篇论文用自然语言“翻译”成中学生能听懂的表述，这些都属于广义翻译范畴。无论是狭义翻译还是广义翻译，Transformer 都特别擅长，因此这是最容易出结果的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a03ff9f85ee03396d4af733c98d5a773.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这里有一个坑：&amp;nbsp;虽然（图中）左边的翻译能力已经具备，但如果右边原有的系统还没准备好（not ready），就会出现问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Chat BI 来说，为什么 Chat BI 在企业里没什么成功的案例呢？其实很大一部分原因在于，Chat BI 的逻辑无外乎就是：用自然语言翻译成 SQL，然后在后台的数据库或大数据系统里执行，再把执行结果取出来，再翻译成自然语言返回给人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Chat BI 的实质，就是自然语言 → SQL → 执行结果 → 自然语言，这本质上还是一种翻译。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我们会发现一个很有意思的问题：很多企业说要上 Chat BI，但如果原本数据库和里面的业务逻辑、数据口径积累不足，甚至连人都写不出对应的 SQL 来，那自然语言也一样翻译不出来。因为后台本身没有可执行的基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我认为，企业里绝大部分在 Chat BI 上踩的坑，都来自于一开始就想做一个过于“宽”的东西。但是做了这个翻译之后，如果原来的系统 API 没准备好，数据没准备好，甚至连原来的人都无法执行这些操作，那自然语言翻译也没法落地。这就是最大的误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此我们在内部的逻辑是：要先Identify 原系统具备哪些能力。比如，如果你原来的 ODPS、数据库和数据中台本身已经有 BI 和运营，能够在某个领域里不断取数、用 SQL 分析数据，而且业务场景也很丰富，那么，那些高频的 SQL 语句才是真正值得作为翻译目标的部分，而不是盲目地去做一个 Chat BI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以很关键的是要分成两个部分 ：一部分是翻译，一部分是原来系统的语言处理能力。我习惯这么来形容：原来的系统就是“蛋糕坯”，大模型翻译就是上面的“樱桃”。如果你现有的蛋糕坯是 ready 的，我放一个樱桃上去，你就可以吃樱桃蛋糕了。但是如果原来的蛋糕坯都没有，你让我做一个樱桃蛋糕，是做不出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里非常重要的一点是：要能够识别出原来的蛋糕坯是不是 ready ，然后在上面放上你的樱桃，而不是直接拿一个樱桃就装作是樱桃蛋糕。这个地方往往就是个误区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;翻译模式是“低垂的果实”，容易做，但里面其实有非常多的坑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 模式：关键在意图与知识空间&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说&amp;nbsp;Agent 应用模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家可以注意这样一句话：所有的 Agent 应用模式都是始于用户意图，终于意图满足。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你不是从用户意图出发，最后又不是以是否满足客户意图来作为度量标准，去看待你的智能体，那一定会失败，没有任何成功的可能性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我发现团队，甚至整个业界，最容易出现的问题。因此我们引出了一个方法，这是我在内部做智能体时，一定要去践行的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一件事情，要找到这个领域的“意图空间”。&amp;nbsp;当一个客户在智能体里和你交互时，他一定是带着意图的。那么这些意图都有哪些？比如客服场景里，客户会提出各种咨询问题，这些问题本质上就是一个空间、集合。所以，第一步就是要搞清楚这个集合的&amp;nbsp;边界和完整性。如果你不知道它的完整性，就无法去度量。只有在建立了完整的意图空间之后，才能继续往下做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，第一件事要建立意图空间。然后，当清楚地知道了意图空间，就要基于这个意图空间来准备&amp;nbsp;知识工程。也就是说，你的知识、文档是否完备？API 和结构化数据是否具备？能否真正满足客户的这些意图？我认为这是最基础的必要条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ec59798c52a3434dbee1855bf4222dd3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再者，有了知识、意图空间，接下来才能带着意图去做评测。&amp;nbsp;因为既知道用户的意图，也掌握了知识，这样才能真正开展工作。如果意图不清楚、知识不具备，其实就是“空转”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的经验是：在客服场景里构建意图空间，从原来就在满足意图的领域出发，从&amp;nbsp;工单&amp;nbsp;里去分析和构建意图空间。有了意图空间之后，就可以对意图进行分类。分类完成后，再根据不同类别去检查和补全知识，做好知识工程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，当&amp;nbsp;意图空间&amp;nbsp;和&amp;nbsp;知识空间&amp;nbsp;都建立好了，才有可能开展评测，也才知道如何去度量你的 Agent。只有具备了度量能力，才有可能进一步做工程和算法迭代，这个是原理决定的。这也是我们在内部做智能体的一个必修课。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里，简单总结一下两个模式： 翻译模式是樱桃，一定要先找到原来的蛋糕坯在哪里，再把樱桃放上去。如果蛋糕坯不 ready，只放个樱桃一定会失败。而 Agent 模式的关键则是：始于用户意图，终于意图满足。&amp;nbsp;这是一系列完整的逻辑方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 落地要点：意图空间、品味&amp;amp;评测&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我们就展开讲这个稍微复杂一些的 Agent 模式，看看在业务体系里实现 E2E 落地的一些关键要点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b387cad2e57fd680552b6106c750368.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，对意图空间的投入进行 ROI 评估。做一个 Agent，它的 ROI 高不高？这取决于意图空间的大小。如果工程所需的知识量庞大，意图也非常多、非常宽，那么所需要的投资就会非常大。意图空间越大，为满足这些意图所需要的知识、工程和迭代的投入也就越大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以有一个非常清晰的结论：第一件事情，就是要控制意图空间的规模。如果不控制规模，会导致失败，因为后续的投入很难支撑。这里要记住一句话：如何去控制一个智能体的意图空间？如果没有控制好，或者不清晰，那么 ROI 根本算不出来。而一个算不出 ROI 的项目，成功的可能性将大打折扣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二，&amp;nbsp;我们经常讲，最近大家肯定也听说过，在 AI 领域里经常提到一个词叫“品味”。AI 时代里，品味非常重要。&amp;nbsp;那么品味来源于哪里？我自己猜测，要追溯到 1995 年乔布斯（Jobs）的一次采访。当时记者说：听说你比较粗暴、独裁，你怎么知道你的决定就是对的？乔布斯想了大约 10 秒，回答道：“归根结底，最后是品味决定的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;品味和这一轮 AI 的关键问题——评测——高度相关。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e5d44b291935bf9a2538dfc67f28ffe.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一轮和上一轮 AI 革命最大的区别在哪里？&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上一轮深度学习主要是计算机视觉。那时候的数据评测怎么做？一张图给猫、狗、交通灯、汽车、人等等打圈，数据打标就是这么来的。所以评测时，只需要看分类对不对（猫有没有被错分成狗？对了就好）。ImageNet 就是这样做的，李飞飞当年找了很多外包团队来做标注，这种标注工作很适合外包，找普通人就能做。原因很简单，猫狗识别不难，就算是一些专家领域，比如故障识别、次品检测，标注也相对容易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这一轮情况完全不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型的输入是小作文，输出也是小作文。在专业领域尤其如此，很难直接度量。这就是为什么要强调品味——因为没有标准答案。我们都是经历过高考的。高考作文有没有标准答案？没有。开放题，比如写一篇中心思想总结，有没有标准答案？也没有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型的评测正是如此。所以，这一轮大模型最关键的区别在于：度量数据、评测没有标准答案。既然这是没有标准答案的，意味着成本最高，也就成为落地的瓶颈。&amp;nbsp;如何解决这个瓶颈？只能重投入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 落地要点：如何做好「评测」&amp;nbsp;&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这里讲的“品味”，就是如何做评测的问题。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/954a3b39d3b94c4660a5d04eb8f29e4a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们怎么去评测？评测是一件非常重的事情，这包括业务效果的评测能力，也包括评测本身的工程化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，在人工评测中，我们如何去解决分类的标准问题？什么是“好”，什么是“中”，什么是“差”？如何能够确保，评测对真实业务意图的覆盖度是足够的？如果覆盖度足够好，标准也足够清晰，我们又如何通过工程化的方式，对系统的迭代和变动进行自动化评测？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于人工评测和度量，很多时候就像写一篇小作文，它是非标的，是没有标准答案的东西。相反，为什么现在编程发展很快？因为数学和编程都有标准答案，可以被编辑器校验，但是纯文本是没有标准答案的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，评测这项工作非常耗时，也很容易成为整个项目的瓶颈，是需要极大加强的。如果不去加强，那么整个项目的基石就可能动摇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在评测的过程中有一个非常重要的点，叫&amp;nbsp;E2E 归因。因为在智能体的过程中会有非常多的环节，在这么多工作流和智能体的编排逻辑中，如果一个意图没有被满足，我们必须要有能力确定这个 Bad case 的问题到底出在哪个环节。当每一个 Badcase 都应该归因到工程里的具体环节，才能对具体的原因进行聚类和改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b4959fc8d9887488b8ad3f85288eebeb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果从产品宏观功能体系来看，体系的最底层，必须要有两样东西：第一，是业务评测；第二，是全链路的归因分析能力。我把这两项放在最底层，就是因为它们太重要了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图这是个大概率的经验总结，也就是说，如果具备度量能力，会发现&amp;nbsp;大部分问题都出现在数据层面，出现在非结构化、结构化数据 API。如果基本能力不具备，这就是智能体失败的主要原因。部分问题可能出现在知识预处理、意图识别、上下文检索，以及后续的意图识别总结等环节。数据极为重要，但没有评测也就谈不上数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b260a7e00c9eec4a1b1814491691a22.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;引出一个经常被讨论的问题：是否需要引入模型训练？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的观点非常明确：必须在白盒方式下使用基模 API，注重评测和数据，并进行 E2E 归因迭代。只有当数据质量和评测能力具备时，才能引入训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因很简单，如果数据和评测能力不 Ready，投入在训练上的每一分钱都是浪费。如果数据不够好，那就是“garbage in, garbage out”。这些问题，都不是训练本身能够帮助解决的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，训练的周期长、成本高、迭代速度慢，如果没有能力评估训练结果的好坏，也没有足够的数据进行训练，这种投入是不明智的。因此，只有在必须使用训练，且基模无法解决问题时，我们才会引入预训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/af4dcf4c9bf5a8fb2b7d24b1f94954ce.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;写在最后：AI+云的「大电梯」&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，为大家回顾一下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在阿里云内部推进 AI 转型，本质上是需要为业务提供 Result as a Service（RaaS）。我们也是当前时点为数不多的，能够真正大规模实现 E2E 落地，给业务交付结果的实践团队。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而我们实现 Result as a Service 的方法叫&amp;nbsp;RIDE，RIDE 分别代表 Reorganize、Identify、Define 和 Execute。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要特别注意的是，在必要条件上再努力，也解决不了充分条件的问题，所以这个 RIDE 方法论的核心是在提醒大家：只有把落地所需要的充分条件补齐，才能真正开展 AI 企业有效落地的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a0118e1b86a2b596ba7940d8b986889a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;呼应最开始讲的“电梯”，想表达的是，冰山之上，我带着团队一直在做业务的数字化转型，之所以能够实现，是因为冰山之下，有强大的阿里云作为底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是涵盖通义千问在内各种模型服务的 MaaS 百炼，还是 PAI，ODPS，数据库等 PAAS 服务、或是底层 IaaS 比如 ECS、灵骏、存储、网络服务，都是我们依赖的企业应用的有力支撑武器。而且，这些能力的成本在不断下降，功能也在持续拓展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，当企业选择了一个强大的技术底座，随着技术水平的增长和成本的下降，企业的数字化转型也就能够搭上一部更好的“电梯”。我自己认为，阿里云就是这样一部“大电梯”，企业上云后，这部电梯持续为企业实现数字化转型，提供源源不断的上升动力。&lt;/p&gt;</description><link>https://www.infoq.cn/article/BKMR19Rj75XwLR1ysM4B</link><guid isPermaLink="false">https://www.infoq.cn/article/BKMR19Rj75XwLR1ysM4B</guid><pubDate>Mon, 19 Jan 2026 03:23:05 GMT</pubDate><author>籍云</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>微软介绍了TypeScript 7的更新</title><description>&lt;p&gt;微软近日分享了&lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt;&quot;&amp;nbsp;7（代号为Corsa项目）的最新进展，披露了对TypeScript编译器的一次根本性重构。该更新&lt;a href=&quot;https://devblogs.microsoft.com/typescript/progress-on-typescript-7-december-2025/&quot;&gt;发布于2025年12月&lt;/a&gt;&quot;，详细介绍了团队将TypeScript编译器用Go语言重写的宏伟计划，他们承诺构建速度最高可提升10倍，并显著降低内存的占用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款名为tsgo的全新原生编译器充分利用了Go语言的性能优势，带来了大幅度的速度提升。据TypeScript团队表示，与旧版本相比，完整构建速度最高可提升10倍，并具备高效的多项目并行处理能力。为编辑器功能（如代码补全、跳转定义、重构等）提供支持的原生语言服务目前已基本稳定，可供日常使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户现在就可以试用这一预览版：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;npm install -g @typescript/native-preview&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TypeScript 7最重要的变化之一是默认启用&lt;a href=&quot;https://github.com/microsoft/TypeScript/issues/62333&quot;&gt;严格模式（strict mode）&lt;/a&gt;&quot;，这是一项与以往版本不兼容的破坏性变更。这一转变体现了团队对类型安全的坚定承诺，也符合行业最佳实践，但可能要求从旧版本升级的项目进行相应调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;选择Go作为实现语言在开发者社区引发了广泛讨论。团队在一份&lt;a href=&quot;https://github.com/microsoft/typescript-go/discussions/411#discussioncomment-12464988&quot;&gt;详尽的FAQ&lt;/a&gt;&quot;中解释说，Go提供了自动垃圾回收机制，同时又是目前最贴近“原生优先”理念的语言。此外，现有TypeScript代码库采用高度函数式的编程风格，几乎不使用类，因此Go的函数与数据结构范式比面向对象语言更为契合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=43332830&quot;&gt;Hacker News&lt;/a&gt;&quot;上，开发者们对性能提升表现出了极大的热情。一位用户评论说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇，这太震撼了！10倍的速度提升对我们这类大型TypeScript项目将是颠覆性的。我一直在等待这样的改进，我们团队的项目在CI上的类型检查耗时极长，并严重拖慢了IDE的响应速度。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，也有开发者对依赖TypeScript编译器API的工具迁移路径表示担忧：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;……对于我们这些工具作者来说，这个原生编译器将如何分发？我猜会通过WebAssembly（WASM）？编译器API是否兼容？比如转换器（transforms）、抽象语法树（AST）、LanguageService、Program、SourceFile、Checker等等？&amp;nbsp;我非常担心工具生态的迁移可能会异常困难。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些开发者已经上手尝试。Reddit上有&lt;a href=&quot;https://www.reddit.com/r/typescript/comments/1pcgmrj/comment/ns0frwz/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;用户&lt;/a&gt;&quot;称其类型检查时间减少了75%。&lt;a href=&quot;https://www.reddit.com/r/webdev/comments/1pcqzn3/progress_on_typescript_7_december_2025/&quot;&gt;还有人&lt;/a&gt;&quot;对默认开启严格模式表示欢迎：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;默认启用严格模式真是太棒了。我们以前经常在项目中工作到一半才发现严格模式没启用，结果要修复一大堆问题，非常令人头疼。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于重度依赖编译器的开发工具而言，TypeScript 7的原生实现使其与其它以原生语言编写的高性能JavaScript工具站在了同一赛道。例如，用Go编写的&lt;a href=&quot;https://esbuild.github.io/&quot;&gt;esbuild&lt;/a&gt;&quot;，以及用Rust编写的SWC和oxc，均已证明原生实现能带来显著的性能优势。TypeScript团队此次转型不仅验证了这一架构方向的正确性，同时也确保了与TypeScript语言规范的完全兼容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TypeScript是由微软开发和维护的一种强类型编程语言，它在JavaScript基础上增加了静态类型定义。自2012年发布以来，TypeScript可编译为纯JavaScript，运行于任何支持JavaScript的环境，包括浏览器、Node.js及其他JavaScript运行时。通过其类型系统，开发者能在编译阶段而非运行时捕获错误；借助智能代码补全、重构等特性，IDE支持也得到了显著增强，同时，显式的类型契约使大型代码库更易于维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/typescript-7-progress/&quot;&gt;Microsoft Share Update on TypeScript 7&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ev8UP654Oux2DreuC6T0</link><guid isPermaLink="false">https://www.infoq.cn/article/ev8UP654Oux2DreuC6T0</guid><pubDate>Mon, 19 Jan 2026 02:42:41 GMT</pubDate><author>作者：Daniel Curtis</author><category>微软</category><category>编程语言</category></item><item><title>谷歌发布适用于多智能体的八种设计模式</title><description>&lt;p&gt;谷歌近期发布了一份指南，&lt;a href=&quot;https://developers.googleblog.com/developers-guide-to-multi-agent-patterns-in-adk/&quot;&gt;详细介绍了多智能体系统（Multi-Agent Systems, MAS）的八种核心设计模式&lt;/a&gt;&quot;，涵盖从顺序流水线到人工介入（human-in-the-loop）架构等多种范式。该指南不仅对每种模式都提供了清晰的解释，还附带了使用谷歌Agent Development Kit（ADK）实现的示例代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌指出，构建复杂且可扩展的智能体应用需要采用与其他软件系统相同的工程化方法，因为依赖单一实体会形成性能瓶颈，并使调试变得非常困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;可靠性来源于去中心化与专业化。多智能体系统（Multi-Agent Systems,MAS）相当于AI领域的微服务架构。通过为各个智能体分配特定角色（比如，解析器、评判器、调度器），开发者可以构建出天然更具模块化、可测试性和可靠性的系统。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于ADK提供的三种基础执行模式,即顺序（sequential）、循环（loop）和并行（parallel），谷歌归纳出八种基本架构（或称为“模式”），帮助开发者以结构化方式设计多智能体系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;顺序流水线（Sequential Pipeline）是最简单的模式，智能体像装配线一样依次处理任务，每个智能体将其输出传递给下一个智能体。谷歌表示，这种模式“线性、确定性强，并且调试起来非常直观，因为你能够始终清楚数据来自何处”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;协调器/分发器（Coordinator/Dispatcher）模式是顺序流水线的一种变体，其中一个智能体作为决策者，接收请求并将其分派给下游的专用智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;并行扇出/聚合（Parallel Fan-out/Gather）模式在多个智能体同时执行各自职责时非常有用。例如，在审查PR代码的场景中，主智能体可并行启动多个子智能体分别处理代码风格检查、安全审计和性能分析。随后，一个合成器（synthesizer）智能体汇总所有输出，决定批准或拒绝该PR。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;层次分解（Hierarchical Decomposition）模式适用于更复杂的场景，高层智能体将复杂的目标拆解为子任务，并委派给其他智能体执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;生成器与评判器（Generator and Critic）模式在输出可靠性至关重要的情况下使用，其中一个智能体负责生成内容，另一个智能体负责验证，并且可选择性地提供反馈，促使生成器迭代优化其输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迭代精进（Iterative Refinement）模式是“生成器与评判器”模式的泛化形式，生成器的输出被送入评判器（critique）和精进器（refiner）智能体，二者协同工作，多次迭代以持续改进原始输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工介入（Human-in-the-Loop）适用于具有不可逆后果或高风险的决策场景（比如，金融交易、生产环境部署、敏感数据操作）。此时，一个审批工具（approval tool）智能体会在必要时暂停执行，等待人工审核者批准或否决建议的操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;复合模式（Composite Pattern）允许组合上述任意多种模式。例如，使用协调器路由请求、并行智能体加速处理，再结合生成器/评判器循环确保输出的质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如指南所述，&lt;a href=&quot;https://google.github.io/adk-docs/agents/multi-agents/&quot;&gt;谷歌为每种模式都提供了详细的架构图和ADK代码片段&lt;/a&gt;&quot;，请参阅该文档以获取更多细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，如果想要了解其他使用ADK构建多智能体系统的思路，请参考&lt;a href=&quot;https://medium.com/@shins777/adk-workflow-the-core-logic-of-ai-agent-8ce4be5c1c40&quot;&gt;Hangsik Shin撰写的指南&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/multi-agent-design-patterns/&quot;&gt;Google’s Eight Essential Multi-Agent Design Patterns&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pTGMtm4Yxej0pWXJB4BN</link><guid isPermaLink="false">https://www.infoq.cn/article/pTGMtm4Yxej0pWXJB4BN</guid><pubDate>Mon, 19 Jan 2026 02:14:17 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>ChatGPT 将测试广告投放，AI 信任危机一触即发</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;事件背景&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月16日，OpenAI通过官方X（原Twitter）账号正式宣布，将在未来数周内开始在ChatGPT的免费版和新推出的ChatGPT Go（$8/月）中测试广告投放。与此同时，Plus（$20/月）、Pro（$200/月）及企业版将继续保持无广告体验。这一决策迅速引发了科技圈的广泛关注和激烈讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/5234f70811ad7bcdaf7e463ddfda46df.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;OpenAI官方推文宣布广告计划，并发布广告原则说明 | 来源：X @OpenAI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，OpenAI同时发布了一份详尽的&quot;广告原则&quot;（Our Ad Principles），试图向用户保证广告不会影响ChatGPT的回答质量和隐私保护。然而，这份承诺并未能平息用户的担忧——社交媒体上的反应呈现出高度两极分化的态势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI的广告原则解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7b/7b43057e190743086666c7694c159ab2.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;OpenAI发布的广告原则框架：强调使命对齐、答案独立、对话隐私、用户控制与长期价值&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📋 OpenAI 官方承诺清单&lt;/p&gt;&lt;p&gt;✓答案独立性：ChatGPT的回答始终基于客观有用性，广告不会影响答案内容&lt;/p&gt;&lt;p&gt;✓对话隐私：不向广告商出售用户数据，对话内容保持私密&lt;/p&gt;&lt;p&gt;✓用户控制：用户可随时关闭个性化广告，清除广告相关数据&lt;/p&gt;&lt;p&gt;✓付费保护：Plus、Pro、Business、Enterprise等高价层级永不显示广告&lt;/p&gt;&lt;p&gt;✓未成年保护：18岁以下用户不会看到广告&lt;/p&gt;&lt;p&gt;✓敏感话题禁区：政治、健康、心理健康等敏感话题禁止广告投放&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;商业化背后的财务压力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从华尔街日报、彭博社等主流财经媒体的报道来看，OpenAI此举并非心血来潮，而是面对真实财务压力的&quot;不得已之举&quot;。据公开数据显示：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/87/8774f103fa85bde59577b3b6aa2d7e02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在如此悬殊的付费转化率面前，广告变现被多家媒体评价为&quot;不可避免&quot;的选择。富国银行预测，ChatGPT在搜索市场的占比将从2025年底的17%增长到2030年的三分之一，这为广告业务提供了巨大的潜在市场空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;社交媒体的激烈反应&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;公告发布后，X平台上的评论区迅速沦陷。从截图可见，用户反应从嘲讽、愤怒到直接引用Sam Altman此前的反广告言论，形成了鲜明的对比和讽刺效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bb/bb77475c3007a83e6b3e36055c4cda50.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;X平台用户对OpenAI广告公告的部分反应 | Grok引用了Altman 2024年称广告是&quot;反乌托邦&quot;的言论&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Sam Altman在2024年曾称将广告嵌入ChatGPT回复是一种&quot;反乌托邦&quot;的想法：&quot;很容易想象那种未来的反乌托邦场景——你问ChatGPT一个问题，它回答说&#39;你应该考虑买这个产品&#39;或者&#39;你应该去这里度假&#39;之类的。&quot;—— 来源：Grok @grok 引用 Altman 2024年采访&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种前后矛盾的表态成为用户攻击的焦点。有用户直言：&quot;直接说你们需要更多钱不就得了&quot;（Just say you guys need more money），简洁而犀利地戳破了官方话术的包装。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;四大核心担忧&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;1. 答案中立性与商业影响的矛盾&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户普遍担忧：一旦AI提供的建议与商业利益相关联，就很难保证答案仍然是纯粹基于&quot;客观有用性&quot;的判断。有用户形象地比喻：&quot;感觉就像在心理咨询师办公室里竖起了广告牌。&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;2. 数据隐私与&quot;监听&quot;恐惧&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管OpenAI承诺&quot;不会出售用户数据给广告商&quot;，但用户对此类承诺持谨慎态度。有Reddit用户反映，在ChatGPT中讨论特定话题后，很快在其他平台看到相关广告，这加深了数据被滥用的担忧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;3. 前科问题：App Recommendations事件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年12月，ChatGPT Plus付费用户在对话中看到来自Target、Peloton等品牌的&quot;推荐&quot;。OpenAI最初辩称这不是广告，只是应用发现功能，但最终在用户强烈反对下关闭了该功能。首席研究官Mark Chen道歉承认公司&quot;做得不够好&quot;。这一事件严重损害了用户对OpenAI承诺的信任。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;4. Instagram模式类比的逻辑悖论&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CEO Sam Altman提到欣赏Instagram的广告模式，但用户尖锐地指出：Instagram之所以成功，正是因为Meta大规模收集和出售了用户的个人数据——这与OpenAI声称的&quot;隐私优先&quot;立场本质矛盾，形成了无法调和的逻辑悖论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;广告形态预览&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据OpenAI展示的概念图，广告将以&quot;Sponsored&quot;标签的形式出现在ChatGPT回复的底部，与回答内容明确分离。在下图的示例中，当用户询问墨西哥晚宴菜谱时，系统在给出食谱建议后，底部会显示相关食材的赞助商购买链接。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/71/714ddc9a44c97e5f72129b0e6daee51e.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;ChatGPT广告投放概念设计：广告以&quot;Sponsored&quot;标签形式出现在回复底部，与答案内容分离&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计理论上可以降低用户对答案被&quot;污染&quot;的担忧，但批评者指出，长期来看广告逻辑一旦被引入系统，算法污染可能是微妙且难以察觉的——即使不是故意为之。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;前科回顾：信任的裂痕&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2024年 Altman 公开反对广告&lt;/p&gt;&lt;p&gt;Sam Altman在采访中称将广告嵌入ChatGPT回复是&quot;反乌托邦&quot;的想法，表示更倾向于订阅模式以避免用户成为产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年12月 App Recommendations 争议&lt;/p&gt;&lt;p&gt;ChatGPT Plus付费用户发现对话中出现Target、Peloton等品牌推荐。OpenAI先是否认为广告，后在舆论压力下关闭该功能并道歉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月16日正式宣布广告测试&lt;/p&gt;&lt;p&gt;OpenAI官宣在免费版和Go版本中测试广告，同时发布&quot;广告原则&quot;框架，承诺付费用户永远不会看到广告。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一系列事件的累积效应是：用户现在不再轻易相信OpenAI关于&quot;高价订阅永远不会有广告&quot;的承诺。Reddit社区中大量评论指出，这正是流媒体巨头采用过的老套路——&quot;先在免费端试水，再慢慢侵入付费端&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;有条件的宽容声音&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;💡部分理性用户的接受条件&lt;/p&gt;&lt;p&gt;并非所有反应都是负面的。部分用户认为，如果OpenAI能够做到以下几点，免费用户看广告是一种合理的交换：&lt;/p&gt;&lt;p&gt;1. 透明性：广告必须明确标注为&quot;Sponsored&quot;，不能伪装成自然回答&lt;/p&gt;&lt;p&gt;2. 相关性：广告应与当前对话相关，而非完全无关的干扰&lt;/p&gt;&lt;p&gt;3. 可控性：用户可以关闭个性化广告设置，或清除用于投放广告的对话记录&lt;/p&gt;&lt;p&gt;4. 底线：高价订阅（Plus/Pro）必须永远保持无广告体验&lt;/p&gt;&lt;p&gt;这类&quot;有条件宽容&quot;的声音提醒我们，用户并非完全不能接受商业化，关键在于执行的边界和信任的维护。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业视角：竞争压力与战略转向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从更宏观的行业视角来看，OpenAI的这一决策也反映了AI领域日益激烈的商业化竞争。谷歌的Gemini和Meta的AI产品已经内置广告机制，OpenAI不想在市场份额争夺中落于下风。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Marketing AI Institute的分析尤其指出，OpenAI内部正面临巨大的商业化压力。公司聘请前Facebook和Instacart高管Fidji Simo担任应用业务CEO，这一人事任命本身就暗示了公司的战略方向——从技术研究机构向消费级商业平台的全面转型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI的创新尝试在于&quot;对话语境驱动的广告&quot;（contextual ads triggered by current conversation），理论上这种做法可以降低隐私风险。但实践中，用户很难确信系统不会进行隐蔽的数据关联。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论：信任与商业化的钢丝行走&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社交媒体反应以担忧和怀疑为主，核心议题围绕信任、隐私和&quot;前科&quot;问题。用户普遍采取了&quot;show me&quot;的态度——可以测试，但任何迹象表明承诺被破坏就会转向竞品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主流媒体的评价则务实与批判并存：认可这一决策的商业必然性，但广泛质疑其能否在不伤害信任的前提下成功。最尖锐的评论来自社区用户的讽刺——AGI实际上是&quot;Ads Generating Income&quot;（广告创造收入）。这反映了一个更深层的焦虑：开放人工智能的使命（AGI造福全人类）与商业化压力之间可能存在根本性冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI正在走钢丝——既要维持无广告体验的付费用户的付费意愿，又要通过免费/低价层的广告收入覆盖高昂的运营成本。这个平衡能维持多久，将决定ChatGPT是否会重蹈社交媒体平台从纯净到被商业完全入侵的老路。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pt1L2jiAHb7tAcB1lAmz</link><guid isPermaLink="false">https://www.infoq.cn/article/pt1L2jiAHb7tAcB1lAmz</guid><pubDate>Mon, 19 Jan 2026 01:25:13 GMT</pubDate><author>AI创新中心</author><category>AI&amp;大模型</category></item><item><title>Java 近期资讯：Spring Shell、JReleaser、TornadoInsight和Apache Camel</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;连续第二周，JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问版本&lt;/a&gt;&quot;仍为&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B29&quot;&gt;Build 29&lt;/a&gt;&quot;。更多详情请参阅其&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样，JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问版本&lt;/a&gt;&quot;当前仍为&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-27%2B3&quot;&gt;Build 3&lt;/a&gt;&quot;。详细信息可查阅其&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Framework&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Spring Shell 4.0.0正式发布&lt;a href=&quot;https://spring.io/blog/2025/12/30/spring-shell-4-0-0-ga-released&quot;&gt;GA版本&lt;/a&gt;&quot;，包含缺陷修复、文档改进、依赖项升级以及多项新特性，包括，命令编程模型重构，在使用Spring Boot时，不再需要@EnableCommand或@CommandScan注解，并修复了@Command注解的意外行为；全新升级的DSL，解决了CommandRegistration.Builder实例与Spring Security的SecurityFilterChain接口在新构建器格式下的匹配问题；与Spring Framework 7.0和Spring Boot 4.0对齐；新增对&lt;a href=&quot;https://jspecify.dev/&quot;&gt;JSpecify&lt;/a&gt;&quot;的空安全（null safety）支持。更多细节请参见&lt;a href=&quot;https://github.com/spring-projects/spring-shell/releases/tag/v4.0.0&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JReleaser&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://jreleaser.org/&quot;&gt;JReleaser&lt;/a&gt;&quot;&amp;nbsp;1.22.0&lt;a href=&quot;https://andresalmiray.com/jreleaser-1-22-0-has-been-released/&quot;&gt;发布&lt;/a&gt;&quot;，这是一个用于简化Java项目发布流程的工具，本次更新包括缺陷修复、文档改进、依赖项升级以及新功能，包括，&lt;a href=&quot;https://jreleaser.org/guide/latest/reference/signing.html&quot;&gt;Signing&lt;/a&gt;&quot;模块全面重构，支持同时使用多种方法对构件（artifacts）进行签名；新增对&lt;a href=&quot;https://jedisct1.github.io/minisign/&quot;&gt;Minisign&lt;/a&gt;&quot;（一个用于文件签名和验证的工具）的支持；支持在部署构件到Maven Central时跳过等待期。更多详细信息请见&lt;a href=&quot;https://github.com/jreleaser/jreleaser/releases/tag/v1.22.0&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TornadoVM&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tornadovm.org/&quot;&gt;TornadoVM&lt;/a&gt;&quot;团队&lt;a href=&quot;https://www.tornadovm.org/post/tornadoinsight-compatibility-with-tornadovm-sdk-2-0-configuration-guide&quot;&gt;宣布&lt;/a&gt;&quot;，其开源IntelliJ 插件&lt;a href=&quot;https://github.com/beehive-lab/tornado-insight/blob/main/README.md&quot;&gt;TornadoInsight&lt;/a&gt;&quot;（旨在提升TornadoVM的开发体验）现已兼容&lt;a href=&quot;https://www.infoq.com/news/2025/12/tornadovm-20-gpu-llm&quot;&gt;最新发布的TornadoVM 2.0&lt;/a&gt;&quot;。相关配置指南也已同步更新。关于TornadoInsight的更多信息，可参考InfoQ的&lt;a href=&quot;https://www.infoq.com/news/2024/01/introducing-tornadoinsight/&quot;&gt;新闻报道&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Apache Camel&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/&quot;&gt;Apache Camel&lt;/a&gt;&quot;&amp;nbsp;4.14.3&lt;a href=&quot;https://camel.apache.org/blog/2026/01/RELEASE-4.14.3/&quot;&gt;发布&lt;/a&gt;&quot;，包含缺陷修复、依赖项升级及功能改进，包括，在使用&lt;a href=&quot;https://camel.apache.org/manual/camel-jbang.html&quot;&gt;Camel JBang&lt;/a&gt;&quot;时，可通过--repos命令为&lt;a href=&quot;https://camel.apache.org/camel-k/2.9.x/kamelets/kamelets.html&quot;&gt;Camel Kamelet&lt;/a&gt;&quot;相关操作指定Maven仓库；&lt;a href=&quot;https://camel.apache.org/components/4.14.x/neo4j-component.html&quot;&gt;Camel Neo4j&lt;/a&gt;&quot;组件改进了消息体的检测逻辑，避免内部错误；修复了&lt;a href=&quot;https://camel.apache.org/components/4.14.x/netty-component.html&quot;&gt;Camel Netty&lt;/a&gt;&quot;中SSL客户端证书主题名称（subject name）从可读字符串表述被错误转换为晦涩的&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc2253&quot;&gt;LDAP&lt;/a&gt;&quot;格式的问题。更多详情请查阅&lt;a href=&quot;https://camel.apache.org/releases/release-4.14.3/&quot;&gt;发布说明&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-dec29-2025/&quot;&gt;Java News Roundup: Spring Shell, JReleaser, TornadoInsight, Apache Camel&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PRKRY1HB7C2ipJFoJBHy</link><guid isPermaLink="false">https://www.infoq.cn/article/PRKRY1HB7C2ipJFoJBHy</guid><pubDate>Mon, 19 Jan 2026 00:00:00 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>烧掉数万亿 Token、数百 Agent 连跑一周：Cursor“从零写浏览器”，结果是拼装人类代码？</title><description>&lt;p&gt;现在，大模型可以独立写完整整一个浏览器了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor CEO Michael Truell 最近分享了一项颇为吸睛的实验：他们用 GPT-5.2 让系统连续不间断运行一周，从零构建出一个“可用”的 Web 浏览器。按他的描述，产出规模达到：超过 300 万行代码、横跨数千个文件，全部通过这套 AI 驱动的编程平台生成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/09a68fe91c9f8b726b597d4a49b03612.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数百个 Agent “从零”写了一个浏览器？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照他的说法，这个项目并没有依赖现成的渲染引擎，而是用 Rust 从零实现了一整套渲染引擎，其中包括 HTML 解析、CSS 级联规则、布局计算、文本排版（text shaping）、绘制（paint）流程，甚至还实现了一个自定义的 JavaScript 虚拟机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Truell 也坦言，这个浏览器目前只是“勉强能用”，距离 WebKit 或 Chromium 等成熟引擎还有很大差距；但团队依然“感到震惊”，因为简单网站在它上面渲染得很快，而且整体效果在很大程度上是正确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，Cursor 还发布了一篇博客文章，题为《Scaling long-running autonomous coding》（扩展长时间运行的自主编程）。文章回顾了一系列实验：让“编程 agent 连续自主运行数周”，目标是“理解在那些通常需要人类团队耗费数月完成的项目中，agentic coding 的能力边界究竟可以被推进到什么程度”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这篇文章里，他们重点讲的是多 Agent 如何协同：如何在单个项目上同时运行数百个并发 Agent、如何协调它们的工作，并观察它们写出超过一百万行代码和数万亿个 token 的过程与经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor 先承认了单个 Agent 的局限：任务规模一大、依赖一复杂，推进速度就会明显变慢。并行化看似顺理成章，但他们很快发现，难点不在并发，而在协同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“学习如何协同：我们最初的方法是让所有 agent 具有同等地位，并通过一个共享文件自行协同。每个 agent 会检查其他 agent 在做什么、认领一个任务并更新自己的状态。为防止两个 agent 抢占同一项任务，我们使用了锁机制。&amp;nbsp;这一方案在一些有趣的方面失败了：&amp;nbsp;agent 会持有锁太久，或者干脆忘记释放锁。即使锁机制正常工作，它也会成为瓶颈。二十个 agent 的速度会下降到相当于两三个 agent 的有效吞吐量，大部分时间都花在等待上。&amp;nbsp;系统非常脆弱：agent 可能在持有锁的情况下失败、尝试获取自己已经持有的锁，或者在完全没有获取锁的情况下更新协调文件。&amp;nbsp;我们尝试用乐观并发控制来替代锁。agent 可以自由读取状态，但如果自上次读取后状态已经发生变化，则写入会失败。这种方式更简单、也更健壮，但更深层的问题依然存在。&amp;nbsp;在没有层级结构的情况下，agent 变得非常规避风险。它们会回避困难任务，转而做一些小而安全的修改。没有任何一个 agent 承担起解决难题或端到端实现的责任。结果就是工作长时间在空转，却没有实质性进展。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这一问题，Cursor 最终引入了更明确的角色分工，搭建一条职责清晰的流水线：将 Agent 分为规划者和执行者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“规划者（Planners） 持续探索代码库并创建任务。他们可以针对特定区域派生子规划者，使规划过程本身也可以并行且递归地展开。&amp;nbsp;执行者（Workers） 领取任务并专注于把任务完成到底。他们不会与其他执行者协调，也不关心整体大局，只是全力处理自己被分配的任务，完成后再提交变更。&amp;nbsp;在每个周期结束时，会有一个评审 Agent 判断是否继续，然后下一轮迭代会从干净的初始状态重新开始。这样基本解决了我们的协同问题，并且让我们可以扩展到非常大的项目，而不会让任何单个 Agent 陷入视野过于狭窄的状态。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此基础上，Cursor 把这套系统指向一个更具挑战性的目标：从零构建一个浏览器。他们表示，Agent 持续运行了将近一周，在 1,000 个文件中写出了超过 100 万行代码（原文如此，跟Michael Truell说的300万行不同），并将源码发布在 GitHub 上供外界浏览。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fc/fc8f661989eab13452b0fd8157d33f1b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cursor 进一步宣称：即便代码库规模已经很大，新启动的 agent 仍然能够理解它并取得实质性进展；同时，成百上千个 worker 并发运行，向同一个分支推送代码，而且几乎没有冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一场“全民打假”的开始？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次实验之所以引发强烈反应，很大程度上是因为：Web 浏览器本身就是软件工程里公认的“地狱级”项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/de/de92607bb9b36483a2a05e52c367cb12.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它难的不只是“写代码”，而是工作量的量级、模块之间的高耦合，以及兼容性这条几乎看不到尽头的长尾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 上，有人顺手抛了一个问题：“开发一个浏览器最难的地方是什么？”很快就有人给出一个类比：“说句真心话，这个问题几乎等同于：开发一个操作系统最难的地方是什么？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为现代浏览器是千万级代码量的系统，能够运行非常复杂的应用。它包含网络栈、多种解析器、frame 构建与回流（reflow）模块、合成（composite）、渲染（render）与绘制（paint）组件、前端 UI 组件、可扩展框架等等。这里面每一个模块，都必须同时做到：既支持 30 年前的旧内容，也支持复杂得离谱的当代 Web 应用。同时，它还得在高性能、高安全前提下尽可能少占用系统资源，并且往往要跨 Mac、Windows、Linux、Android、iOS 等多个平台运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有人提到，最难的是那张超长的任务清单。浏览器里包含多个高复杂度模块，每一个单拎出来都可能要做很久；更麻烦的是，它们之间还要通过一套相当“啰嗦”的 API 连接起来——很多接口你必须实现，至少也得先把壳子（stub）搭出来，否则系统就会崩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对这个浏览器项目，Cursor 在博客中写道：“虽然这看起来像是一张简单的屏幕截图，但从头开始构建一个浏览器是非常困难的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而如果外界自己去尝试编译这个项目，会很快意识到：它离“功能齐全的浏览器”还差得很远，甚至看起来在公开代码状态下，连最基本的构建都很难稳定通过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从仓库公开信息来看，近期 main 分支的多次 GitHub Actions 运行结果显示失败（其中还包括工作流文件本身的错误）；不少开发者的独立构建尝试也报告了数十个编译错误。与此同时，最近的一些 PR 虽然被合并，但 CI 仍处于失败状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更有开发者表示自己回溯 Git 历史，往前翻了约 100 个提交后表示，依然没能找到一个可以“干净编译通过”的版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也引出了一个问题：这些被 Cursor 描述为在代码库中长期并发运行的“agent”，在工程链路上到底做到哪一步？至少从当前公开状态看，它们似乎并没有把“能编译、能检查”当成最基础的收敛目标——因为无论是 cargo build 还是 cargo check，都会立刻暴露出成片的编译错误和大量警告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而Cursor 的博客文章除了提供代码仓库链接外，既没有提供可复现的演示，也没有提供任何已知的有效版本（标签/发布/提交）来验证截图。无论如何，这文章本身给人一种原型功能完备的错觉，却忽略了此类声明应有的基本可复现性特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/18f5eca698dcba5b3804f2d1b922713f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人在Michael Truell 的LinkedIn上直接把结果抛了回去：“构建直接失败，报了 32 个错误，代码本身就是坏的；没有任何 release、没有 tag，CI 也在持续失败，我们甚至连这个所谓‘可用的浏览器’都没法编译、没法试跑。这更像是一场营销活动，而不是一次真正的 agentic 实验。”Michael Truell 至今没有回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c2/c267ef2f437e4f201c1ec9d34b3fb436.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前唯一一个在社交平台上明确分享“复现成功”的人，是前浏览器开发者 Oliver Medhurst。他表示自己花了大约两个小时修复编译错误和漏洞，才把项目跑起来。至于性能，他的评价也很直接：有些页面加载要整整一分钟，“不算好”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个更敏感的追问也随之出现：“所以这真的是从零开始写的吗？”他给出的回应更像一句反转预告：“剧透：不是。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/69/69169227bd6dc8d62e0d4cf0637cbb88.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更多网友通过翻看仓库依赖发现，这个项目直接引入了 Servo （一个最初由 Mozilla 开发的基于 Rust 的浏览器）项目的 HTML 与 CSS 解析器（html parser、css parser），以及 QuickJS 的 Rust 绑定（rquickjs），并非所有关键组件都是自行实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再加上 selectors、resvg、wgpu、tiny-skia 等一系列成熟库，这个“浏览器实验”更像是直接调用了人类编写的代码，而不是“从零开始”的一整套渲染与执行引擎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64b6b900a7d742bf32dafd9f963fe8a5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e7/e716eb0006daa5131dc89b27d7892306.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/85/85491f9fdccde9acf59685ff7790b541.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更搞笑的是，Cursor 这里用的还是一个发布于 2023 年 6 月的wgpu 0.17这种非常旧的老版本，而当前最新版本已经是 28（发布于 2025 年 12 月）。大概因为大模型写代码时往往会直接改版本管理文件（如 package.json、Cargo.toml），而不是通过 npm add、cargo add 这类构建工具来引入依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c15a8df25de61642d98e04625fc169a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也不怪网友骂他们：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这简直是胡扯。应用根本跑不起来，功能也缺得厉害。LLM 更像是在把它训练过的现成代码拼起来做个浏览器——毕竟 Chromium 本来就是开源的。最后堆出了 300 万行‘看起来很多’但没有价值的代码，结果还不能用，更谈不上什么新产品。折腾到最后，你还是得让开发者花大量时间去调试、排查安全漏洞，才能把它打磨得像一个早就存在的成熟产品。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“两周时间、数百个 agent，V8 和 Blink 又都是开源的。说到底，这就是在浪费 GPU 和电力。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/95/95a7b886c9f136fd2ea711159c08cd8d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后值得一提的是，这个实验还暴露出一个不容忽视的问题：成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人翻回 Cursor 的原帖指出，他们还在跑类似实验，比如一个 Excel 克隆项目（&lt;a href=&quot;https://github.com/wilson-anysphere/formula&quot;&gt;https://github.com/wilson-anysphere/formula&lt;/a&gt;&quot;）。GitHub Actions 的概览数据很夸张：累计触发了 16 万多次 workflow 运行，但成功的只有 247 次——失败的主要原因不是代码本身，而是超出了支出上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，Agent 并不在乎预算；但在真实的软件工程里，可复现的构建、可持续的成本、可验证的产出，才决定一个系统最终能不能被信任、被维护、被继续推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://cursor.com/cn/blog/scaling-agents&quot;&gt;https://cursor.com/cn/blog/scaling-agents&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46646777&quot;&gt;https://news.ycombinator.com/item?id=46646777&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/&quot;&gt;https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7417328860045959169-PFuT/&quot;&gt;https://www.linkedin.com/posts/activity-7417328860045959169-PFuT/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://xcancel.com/CanadaHonk&quot;&gt;https://xcancel.com/CanadaHonk&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/t0rpY0X2G9RBmXf9SK6g</link><guid isPermaLink="false">https://www.infoq.cn/article/t0rpY0X2G9RBmXf9SK6g</guid><pubDate>Sat, 17 Jan 2026 12:00:00 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>告别“刀片利润”，AI如何帮中国数百万中小工厂构筑新护城河？</title><description>&lt;p&gt;“今年上半年我在山东临沂见了一位满头白发的90后老板，他们公司的年销售额超过3亿元，但利润却不到1000万。”&lt;a href=&quot;https://www.infoq.cn/article/xLUE7sWGby4MF5GKpgas&quot;&gt;1688&lt;/a&gt;&quot;商家发展中心总经理王强在日前接受媒体采访时讲道，“白天睡觉、晚上陪客户，这是他们为此生意的主要方式。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，这不是个例，而是中国数百万中小工厂主的真实缩影——规模在增长，利润在萎缩；订单在增加，确定性在流失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种悖论背后，是一场深刻的结构性撕裂，是当前B2B产业面临的系统性挑战：合规成本持续攀升，环保、税务、用工等要求日益刚性；与此同时，供给极端碎片化、需求高度非标化，交易决策链冗长，产业经验变得难以沉淀和复用。这使得过去依靠压价和人情关系维系的生意模式，在今天已经难以为继。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，中国产业带的底层逻辑正在被彻底重构，“K型复苏”成为新常态，头部企业加速扩张，尾部企业加速出清，一个尖锐的问题摆在所有中小工厂面前——出路究竟在哪？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1688在刚刚发布的《2025中国产业带发展趋势报告》（以下简称“报告”）中给出了他们的答案：2025年，中国产业带开始迈向AI原生时代。这不是又一次简单的工具升级，而是一场历史性跨越：从“数字化”向“智能化”的范式转变。在这场变革中，AI不再是可有可无的“外挂”，而是决定生死存亡的“操作系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该报告基于1688平台26年产业带深耕经验，整合覆盖全国70%一级产业带、超百万家源头厂商的真实交易大数据，系统分析了AI在产业带的演进路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“卷成本”到“拼确定性”，中国产业带的游戏规则正在被重写&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说过去二十年产业带的竞争关键词是“规模”与“成本”，那么今天，“确定性”正迅速取代“低价”，成为新的护城河。所谓确定性，不只是按时交货，更包括产品品质稳定、服务响应及时、需求预测精准、合规风险可控。并且，这些变化的覆盖范围不仅限于国内市场，更是全球性的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告指出，产业带“江湖规矩”和“权力地图”正被彻底重写，“外转内”与“内转外”并行，产能外迁与产业内移共振，工厂正从“代工者”蜕变为“品牌主”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而AI，成了构建确定性的核心引擎，基于 AI 原生的下一代供应链呼之欲出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据王强分享，深圳一家3C配件厂商深圳众鑫通泰曾深陷“低价、欠款、无客”的死循环，但他们通过AI分析亚马逊上的用户差评发现，一款手机支架的核心痛点是“粘不牢、价格高”，并据此开发出了采用真空磁吸技术的新品，同时，借助AI生成的油管爆款视频进行推广，这家工厂最终实现了跨境业务占比突破70%，毛利率远超同行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一家位于安徽芜湖的一个6人鞋企也在2024年借助AI工具实现了惊人跃升：上新效率提升4倍，支付转化率提升41%，全年销售额达1.5亿元。他们没有设计师，就用AI完成换色、场景图和视频生成；没有客服团队，就部署7×24小时自动响应系统；甚至通过AI分析TikTok热门搜索词，捕捉全球潮流趋势。“AI已经是趋势了，等别人都试完再上，你就被淘汰了。”芜湖苏禾鞋业张云这样说。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;类似的故事正在更多产业带上演。报告显示，和这些工厂一样，越来越多的中小企业正通过AI将内贸积累的柔性快反、品控能力“翻译”为跨境竞争力。AI不再只是巨头的游戏，它同样成为了小微商家对抗规模劣势的“杠杆”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，AI已深度融入商家端的三大核心场景：选品方面，通过全球电商平台评论与社媒热词，反向定义产品；小单快返方面，基于柔性供应链使得响应周期大幅缩短；智能质检方面，用图像识别替代人工目检，使得效率大幅提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1688公共事务部总经理范敏强调，这些变化背后指向了这样一种进化逻辑——AI正在驱动三大“位移”：&lt;/p&gt;&lt;p&gt;第一，决策机制位移，从依赖“老师傅经验”转向依赖“AI产业大脑”；第二，组织形态位移：从“人盯人”管理转向“AI调度+人机协同”；第三，核心竞争力位移，从“模具和产能”转向“数据驱动的快速迭代能力”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如报告所示，2026年起，增长将向能稳定交付、直连用户、自主开款的源头工厂集中。未来的赢家，不是规模最大、也不是成本最低的，而是将AI融入血液，构建起“效率×合规×确定性”新护城河的企业” 。在这个新规则下，“确定性”本身就成了最稀缺的资源，能否用好AI，则成了企业穿越周期的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;超越“生意搭子”，AI从来不只是一个工具&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这场跃迁并非坦途。&lt;a href=&quot;https://www.infoq.cn/article/jNFPJE9aRZi37seSRNwc&quot;&gt;数据孤岛&lt;/a&gt;&quot;、模型泛化能力不足、复合型人才短缺仍是大多数企业在AI应用落地过程中遇到的主要瓶颈。尤其在传统产业带，许多工厂连基础的ERP系统都没能打通，导致AI系统缺乏高质量、结构化的数据输入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对这些现实约束，企业可以选择从最小可行场景切入，以业务价值反推技术建设。以深圳众鑫通泰为例，他们并不是从一开始就试图构建“全厂智能大脑”，而是聚焦一个具体痛点——“用户为什么差评我们的手机支架？” 通过抓取公开电商平台评论这一无需内部系统打通的外部数据源，快速验证了AI选品的价值，再逐步将成功经验延伸至生产排程与质检环节。这种“由外而内、由点及面”的路径，有效绕开了初期数据孤岛的制约。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对模型泛化难题，目前市场上已经有平台开始探索基于跨商家、跨品类的聚合数据，提炼具有行业共性的智能能力。比如1688依托其覆盖全国70%一级产业带的交易网络，正尝试把成功案例中从选品洞察、质检规则到履约优化的AI应用逻辑，抽象为可借鉴的方法论甚至工具原型，以降低单个工厂的试错成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而破解人才困局的关键，在于构建“人机协同”的新工作流，而非追求全能型个体。拿芜湖苏禾鞋业这个6人企业来说，他们并没有AI工程师，也没有技术背景，仅仅通过应用AI工具，就实现了设计、营销与客户服务的全面提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;王强表示：“未来的AI的卷应该是，你是不是让AI把你做成一套的体系或系统？而&lt;a href=&quot;https://www.infoq.cn/article/MhBpL7m3y873tY1qmm0I&quot;&gt;不仅仅是一个工具&lt;/a&gt;&quot;。” 换言之，AI的价值不在于炫技，而在于能否系统性解决已知问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告中对中国产业带进化进行了三阶段划分，随着AI应用深度的变化，企业将从“AI外挂”进入“AI共生”乃至“AI原生”。而这也恰恰是企业应对以上一系列挑战的底层逻辑，企业不应该把AI当作一个孤立的技术项目，而是将其视为重构业务流程、组织协作与客户价值的契机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于身处这一变革浪潮中的企业，报告还给出了三条行动建议：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，要信仰AI，用AI做生意，把工厂变成真正的“硅基工厂”——让每一条产线都听得懂需求、看得见订单、控得住质量；第二，要做足确定性，品控要硬、履约要稳、服务要好；第三，要布局双循环，AI正在模糊内需与跨境的边界，因此企业需要一套AI系统，通做全球生意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当AI不再是“生意搭子”，而成为产业运行的“智能中枢”，中国制造业的下一轮红利，才真正拉开序幕。这场变革不会一夜完成，但方向已然清晰：谁能把不确定性转化为确定性，谁就能在新秩序中占据主动。而AI，正是那把最关键的钥匙。&lt;/p&gt;</description><link>https://www.infoq.cn/article/uPMimwjtFsRSB8thczFi</link><guid isPermaLink="false">https://www.infoq.cn/article/uPMimwjtFsRSB8thczFi</guid><pubDate>Sat, 17 Jan 2026 11:37:25 GMT</pubDate><author>高玉娴</author><category>阿里巴巴</category><category>工业</category><category>AI&amp;大模型</category><category>数字化转型</category></item><item><title>IDE消亡之年？Steve Yegge 两句狠话：2026 年还用 IDE 就不行，每天烧 500–1000 美元 Token 才合理</title><description>&lt;p&gt;虽然我并不认同“IDE 会在 2026 年消亡”这种绝对说法，但 Steve Yegge 和 Gene Kim 在分享中抛出的判断，依然值得认真对待：在他们的推演里，从 2026 年 1 月 1 日起，继续依赖传统 IDE 的工程师，会被更快拉开差距。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们认为这不是“工具升级”，而是“生产方式换代”：工程师的竞争力，越来越取决于你能否用好新一代 AI 开发方式，以及你愿不愿意为它付出真实成本——例如把每天的 token 开销重新定价到“接近日薪”的量级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更刺耳的是，他们转述了 OpenAI 的 Andrew Glover 的一项观察：是否使用 Codex，可能会让同级别工程师之间的生产力差距被拉到 10 倍，这让管理层“非常惊慌”，“因为他们甚至可能不得不裁掉 50% 的工程师”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心观点如下：&lt;/p&gt;&lt;p&gt;现在的模型本质上是派一个潜水员下去，让它在代码库里四处探索，即使给它更大的氧气瓶，比如一百万 token，它仍会耗尽。正确做法应该是派多个角色，而不是寄希望于一个超大的单一潜水员。如果你在2026年 1 月 1 日后还在使用 IDE，那你就是一个“不好的工程师”。作为工程师，我每天花在 Token 上的费用应该与我的日薪相当，也就是每天 500 到 1000 美元。Claude Code 走错了方向，他们造出一只巨大、耗能、高成本的“肌肉蚂蚁”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve Yegge：今天的时间会过得很快，我将讨论明年(2026年）开发工具的样貌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在所有人都迷恋 Claude Code，市面上大概有四十个竞争者，但 Claude Code 并不是答案，代码补全也不是。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然我每天使用它十四个小时，但开发者并未真正采纳。核心问题是这些工具使用难度过高，认知负担重，而且常常“撒谎、作弊、偷懒”。因此大多数开发者并不喜欢这样的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我逐渐认识到，Claude Code 很像电钻或电锯。对于没有受过训练的人，它既能帮上忙，也能造成巨大损伤。未受训练的工程师使用 Claude Code，与一个新手拿着电锯差不多：既可能“切到脚”，也可能在熟练后完成极其精细的工作。然而软件世界无限广阔，而我们的野心也同样无限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此我想用一个类比说明：明年将是从“手持电锯、电钻”转向“数控机床（CNC）”的一年。CNC 在给定坐标后能自动执行极其精确的操作，这项技术我们已经使用了数百年，也不会在今年停止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c95d80dbac1bae36cd549db265cdec73.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有人说“模型已经触顶了”，你的工程师们可能也这么说。即使如此，我们仍然等同于刚发现蒸汽和电力，还需要时间去驾驭它。现在的问题已经主要是工程问题。一年到一年半内，所有代码都将由大型自动化“磨床”式系统生成，工程师不再直接查看代码。这将是一个全新的世界，而我们正走向那里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gene 和我曾与 OpenAI 的 Andrew Glover 交流过，他说公司内部出现了明显的分化：部分工程师使用 Codex，而更多人没有用（拒绝使用工具的人主要是资深与 Staff 级工程师），产能差距巨大，导致绩效评估出现警报。两个同级别的工程师，其生产力可能相差十倍，这让管理层非常惊慌，因为他们甚至可能不得不裁掉 50% 的工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/22a874b661acbb71717e4f083fa6ce92.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种情况类似瑞士机械表产业的衰落：经历数百年的辉煌，却被石英表在短短几年内颠覆，当时的工匠与今天坚持传统方式的资深工程师反应如出一辙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f1fa5c038fae21cd13a5793d3802507.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来需要的是一种全新的 UI，不是传统 IDE，而是新的 IDE。事实上，Replit 已经走得最前，他们的方向非常值得称赞。我们不该再继续追着旧形态、构建各种命令行界面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/89/8982b2a872bb79323b772dde65419d58.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，Claude Code 及其竞争者都走错了方向——它们像在打造“世界上最大的蚂蚁”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的朋友、澳大利亚联邦银行的 Brendan Hopper 说得很好：自然界靠蚁群协作，而 Claude Code 却造出一只巨大、耗能、高成本的“肌肉蚂蚁”。无论是要分析整个代码库，还是只是问“我的git ignore 还在吗”，它都调用最昂贵的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/598c2514f47467c5c48c87d0de6b3ed5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是我想到了“潜水员隐喻”：上下文窗口就像氧气瓶。现在的模型本质上是派一个潜水员下去，让它在代码库里四处探索，即使给它更大的氧气瓶，比如一百万 token，它仍会耗尽。正确做法应该是派多个角色：产品经理潜水员、开发潜水员、代码审查潜水员、测试潜水员、合并潜水员等，而不是寄希望于一个超大的单一潜水员。可没人这么做，大家都在造“大潜水员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3a1a5c011d89fc53ca8fc584af27b027.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;未来的构建方式将是工程师熟悉的：任务分解、逐步细化、组件化、黑盒化，并依赖大量协作的智能体，而不是单一智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/26/269c82af09b2d0b65f15c65fb57e01aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在此之前，我的建议还是：学习 Claude Code来适应新方式，并放弃你的 IDE。如果你在明年 1 月 1 日后还在使用 IDE，那你就是一个“不好的工程师”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97077c1a104c8143ba383e9d41ff0aec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Gene Kim：我研究高绩效技术组织已有 26 年，这段旅程始于我作为 Tripwire 的技术创始人。我们致力于研究那些表现卓越的技术组织——它们在项目交付、运维稳定性、安全合规方面都处于领先。我们想理解这些组织如何实现“从优秀到卓越”的转变，以及其他组织如何复制这些成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这 26 年中我经历了许多意外，其中最大的意外之一，是这项研究最终将我带到了 DevOps 运动的中心。DevOps 改变了测试、运维、信息安全等角色的协作方式。我曾以为这会是我职业生涯中最激动人心的经历，直到我在今年 6 月首次与 Steve Yegge 见面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我和Steve有许多共同点，其中之一就是对人工智能的热爱，以及都认为 AI 将从底层重塑软件开发的方式。我们相信，AI 对技术组织的影响，可能比十年前敏捷、云计算、CI/CD 和移动化所带来的变革大上百倍。而这些技术突破不仅会改变组织，也会重塑整个经济，让经济结构围绕更先进的生产方式重新排列。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去一年半，我们观察了许多案例，让我们提前看到未来技术组织的雏形。有人可能熟悉 Adrian Cockcroft，他曾是 Netflix 的云架构师，主导了 2009 年将 Netflix 整个基础设施从自建机房迁移到云端。他在几个月前写道，2011 年有人提出“无运维（NoOps）”时，引发了基础设施和运维团队的强烈反对，但现在类似的事情再次发生，只不过这次可能叫“无开发（NoDev）”。如今看来，这似乎不再好笑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们从 Zapier 的分享中看到，支持团队能发版，设计师能发版，UX 设计也能直接发版。过去被开发者告知“排队、等一个季度、等一年、甚至永远等不到”的人，现在突然能够自己把功能“对话式地”写进生产环境。这不仅改变技术组织，也可能改变整个经济。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Steve 和我很幸运能看到部署方式的改变带来什么影响。十年前，我写了《The Phoenix Project（凤凰项目）》，讲述灾难性的部署流程。当时许多组织一年只发布一次版本，难以想象。后来我参与了 DevOps 状况研究，这项跨行业研究在 2013–2019 年间覆盖了 36,000 名受访者。我们发现，高绩效团队每天能多次部署，并能在一小时内完成一次发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 2009 年，多次每日部署被视为鲁莽、不负责任甚至“不道德”，但如今却是常态。若想保持高可靠性、缩短平均修复时间，就必须更频繁地进行更小规模的部署。现在我们看到的案例表明，不再手写代码，而是运用新的方式进行开发，可能是一种价值更优的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们在《Vibe coding》一书中提出的定义是：只要不是靠双手在 IDE 里敲代码的方式，都可以称作“Vibe coding”。有些人还像在暗房里冲洗照片一样，依旧习惯在昏暗环境里手动输入代码。但 Anthropic 联合创始人兼 CEO Dario&amp;nbsp;Amodei 给了我们更好的定义：Vibe coding 是由反复对话推动的、由 AI 生成代码的过程。他说这个词很美，能表达一种全新的开发方式，但也略带戏谑。不过对他们而言，这已经是“唯一的方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是编程语言领域的重要人物 Erik Meijer 博士，他参与过 Visual Basic、C#、Haskell，也在 Meta 推出了 Hack 编程语言，在一年内迁移了数百万行 PHP 代码，引入静态类型检查。他说，我们可能是最后一代手写代码的开发者，所以应该享受这一段最后的旅程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8dd568affd763d1ec9ca2c3125a1308f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一件事是这样的：去年 11 月开始，我一直在观察Steve，他每天在编码代理上花掉几百美元。这在当时看起来非常奇怪。他不仅把各种月度订阅都用到了上限，实际上还远远超出了这些额度。&lt;/p&gt;&lt;p&gt;但现在我们听到的一种说法是：作为一名工程师，我的工作本身就应该要求我每天在 token 上的花费，和我的日薪大致相当。也就是说，大概每天 500 到 1000 美元。因为这些工具带来的，是一种机械优势和认知优势。作为工程师，我会挑战自己，去榨取这种投入所能带来的最大价值，把成果交付给真正重要的人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/2344e7d9a259a019f8dd14122b7348a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在书中，我们把人们为何愿意这样做总结成一个缩写：FAAFO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aabd819a0f0cd76ab59fe64dd1e3548a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一个 F 是Faster（更快），但这是最表层的理由。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是A-Ambitious（雄心），AI 让我们得以完成过去无法实现的雄心项目，把不可能的事情变得可能。在另一端，琐碎麻烦的小任务也几乎变成了零成本。我非常喜欢 Claude Code 团队中的一段采访，Katherine 说，以前客户问题会被放进 Jira 的待办项，在梳理会议中争论，一拖数周；而现在我们直接在当下修复，并在 30 分钟内发布。记录依然会做，但协调成本几乎完全消失了。也就是说，不可能的事情变得可能，麻烦的小事变得免费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个A 是Able（能力），代表“更独立”，更能单独完成工作。这里有两类协调成本正被 AI 消除。第一类协调成本来自“等待”。如果你需要开发者或一个团队帮你做事，你必须沟通、协调、同步、排优先级、游说、升级……总之必须让他们“和你一样在乎这个问题”。而现在，依靠这些近乎奇迹般的新工具，你可以自己完成许多工作。第二类协调成本来自“理解”。即使别人愿意像你一样重视某件事，他们也无法读你的心。但我们发现，LLM 是惊人的“协作中介”。仅通过一个 LLM，你就能以 Markdown 文档的形式与不同职能顺畅协同。这当然不是最终形态，但它让高带宽的理解成为可能。因为要想实现共同的成果，就必须先有共同的目标与共同的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个 F，是 Fun（好玩）。正如 Steve 所说，Vibe coding 具有成瘾性。我们见过两个人原本以为“写代码的黄金时代已经过去了”，结果却意外发现现实恰恰相反。我现在常常玩得太投入，不逼自己去睡就会写到凌晨两三点。它不是只有好的一面，但肯定比无聊、枯燥甚至痛苦要好得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;O是Optionality（可选项）。我们非常重视“创造期权价值”。模块化之所以强大，也因为它能创造更高的期权价值。Vibe coding 能让你同时进行更多实验、更多尝试，因此它是极具经济价值的工具。Steve Yegge 说，对于已经经历“顿悟时刻”的人来说，本能反应往往是：如何让团队中所有人都获得与你现在同等的生产力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面我分享一些让我们看到未来形态的案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，Travelopia 的产品与技术负责人 Sree Balakrishna 的分享。Travelopia 是一家年营收 15 亿美元的旅行企业。他们曾用一个小团队，在 6 周内替换一套传统系统。按过去的方式，需要 8 人（6 个开发、1 个 UX、1 个产品负责人）；而现在，也许只需要一个开发与一个领域专家，正如 Kent Beck 所说，“一个有问题的人加一个能解决问题的人”。这种团队规模的变化会深刻影响组织未来的运作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e3/e3aadc82b15766130f889bbc3eccdd58.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我最兴奋的案例来自 Dr. Tapabrata Pal。他在 Capital One 推动过 DevOps，如今在 Fidelity 负责一个关键应用，用来查询公司 2.5 万个应用中哪些受 Log4j 影响。过去他的团队总说重新做这个工具需要 5 个月，并需招聘前端工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终他自己花 5 天 Vibe coding 出了一个版本，并上线生产。他只是想证明：事情完全能做，而且可以更快完成。后续更戏剧的是：他为应用找维护者，资深工程师们都不愿接手，最后是团队中最年轻的工程师成为维护者，并正在快速成长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，这个应用的内部用户数量增长了 10 倍，他也因此获得更多人手。这些变化是任何人都没预料到的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再分享一个例子，我重返 Google Cloud 团队做的 Dora 研究，其中一项未进入正式报告的发现是关于“AI 信任度”。我们采用的信任定义是：你能多大程度预测对方（AI）的行为？越信任，就能给更大请求，用更少词语，减少反馈需求。结果显示：使用 AI 的时间越长，信任越高。那些说“我试了一下，它写代码很差”的人，多半只用了 1 小时。显然，AI 的掌握是可训练的技能，需要实践，而不是一次性体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/18/180a259b65eb420133f3397dc1ab0a39.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，我们的责任之一，是帮助他人获得“顿悟时刻”，并协助他们不断练习，从而真正掌握这些强大的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;六周前，Steve 和我为领导者们做了一次 Vibe coding 工作坊。三小时内，完成率 100%，每个人都做出了成果。还有一位，他说自己 15 年没写代码了，却在短时间内做出一个自动帮自己抢 Southwest 登机位的工具（直到被反机器人系统封掉），你从他脸上的表情就能看到那种久违的创造力被重新点燃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dcb9564856bcd6cc931edd02150191a7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，当支持团队、领导者能编码并上线时，技术组织必然会重塑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个技术领导者说，当他告诉团队他写了一个应用，其中 6 万行代码都是 AI 写的，而他自己一行没看时，团队看他的眼神仿佛“希望他不存在”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e51f4bb833719de52a99ed553bc6f89.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个例子，一些存在十年的遗留系统问题，团队集合资深工程师，用 AI 生成修复方案并提交 Pull Request。这次被接受了，而不像过去那样被污名为“AI生成的低质量内容（AI slop）”。还有团队说，他们现在的代码提交速度如此之快，以至于每个代码仓库只能容纳一个工程师，否则合并冲突会让协作成本爆炸。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=cMSprbJ95jg&amp;amp;t=4206s&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SJNt2c2Sh5AgO4LbiSC8</link><guid isPermaLink="false">https://www.infoq.cn/article/SJNt2c2Sh5AgO4LbiSC8</guid><pubDate>Sat, 17 Jan 2026 05:42:26 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>腾讯云ADP国内首发AI原生Widget：一句话秒级生成交互组件，重塑Agent使用体验</title><description>&lt;p&gt;智能体对话正在告别“纯文本时代”！近日，腾讯云智能体开发平台（ADP）重磅上线国内首个“AI原生Widget”，面向企业客户提供“富交互任务交付”能力，只需自然语言描述，就能实时生成表单、按钮等交互组件。该能力还同步在腾讯元器（一站式AI智能体创作与分发平台）生态侧落地，支持创作者一键生成交互卡片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，这一功能还兼容OpenAI 生态的 Widget 接入规范，外部 Widget 可依据标准协议直接导入复用，进一步拓展智能体能力边界与生态扩展空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI原生Widget”是一种面向智能体任务交付的“富交互组件形态”，模型输出结构化描述（JSON Schema)，平台自动渲染为可操作的表单、按钮，并将用户交互结果回传智能体，实现任务闭环执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/43/40/4396b1bc36e225b054f57d50fdf6ea40.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;自然语言秒级生成智能体交互组件&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在传统的大模型对话中，文本输出是主要形式。海量的文字堆砌，不仅抬高理解成本，而且完成单一任务需要多轮来回沟通，效率低且体验不佳。Widget作为可嵌入式的自定义展示组件，能在智能体对话流中，灵活融入图表、表单、按钮等“富交互”模块，将对话界面升级为沉浸式任务平台，引导用户按步骤操作，大幅提升信息传递与任务执行效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前国内的智能体平台构建Widget时，普遍采用传统“拖拉拽式低代码+手动配置字段/数据源映射关系”的方式，流程繁琐、耗时久、稳定性一般，难以适配高效开发的需求。针对这一痛点，腾讯云ADP推出的AI原生Widget，提供了模版创建、代码创建、自然语言生成等多种方式，降低开发门槛。即使非专业前端开发者，只需用语言描述需求，或调用现成Widget模板，一分钟内就能够生成对应组件，真正实现“所想即所得”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/32/a2/32fdcca20ce95b897452acb94ee92da2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持多种Widget开发模式&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如用户想要搭建一个“健身小助理”智能体，通过AI原生Widget，输入提示词后，一键就能生成对应卡片。当用户询问“我要跑步”时，系统会弹出预设卡片，引导用户点选运动频率、强度等习惯信息，再根据用户的选择，快速生成“跑步训练周计划”卡片，包含每周运动安排、单次运动内容、时长和强度建议等核心信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/9e/8a/9ef0f98b4d200e849bb79d5df795068a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;从纯文本对话到富交互任务执行&lt;/p&gt;</description><link>https://www.infoq.cn/article/KXHUrhczo8le9KpyyNjr</link><guid isPermaLink="false">https://www.infoq.cn/article/KXHUrhczo8le9KpyyNjr</guid><pubDate>Sat, 17 Jan 2026 05:36:06 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>AI 的下一个十年：从技术拐点到工程落地的路线图</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术发展史上，总会出现一些被反复回望的“拐点时刻”。在 Snowflake 首席执行官 Sridhar Ramaswamy 看来，我们正身处这样的关键节点之中——多年来机器学习与深度学习的研究积累、Transformer 等关键架构的突破，以及云计算规模能力的成熟，在这一刻汇聚，推动人工智能走向真正的产业化阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bb51e554cb4794fec827c8518aa96f2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一背景下，Snowflake 邀请了两位深度参与并塑造这一进程的核心人物，共同展开了一场关于 “未来十年 AI 蓝图” 的对话：堪称全球最具影响力的人工智能教育者和先驱者、LandingAI 执行董事长、DeepLearning.AI 创始人吴恩达（Andrew Ng），以及亚马逊云科技 Agentic AI 副总裁Swami Sivasubramanian，他曾主导 Amazon SageMaker 与 Amazon Bedrock 的构建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场对话并未停留在对模型能力的抽象讨论，而是围绕竞争优势、商业模式、工程架构、数据治理以及开发者未来等关键问题，勾勒出一条从战略到落地的清晰脉络。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;竞争焦点正逐渐脱离模型本身&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕“AI 时代的护城河从何而来”这一核心问题，讨论首先打破了一个常见误区：竞争优势并不必然源于模型本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在吴恩达看来，ChatGPT 这类产品在消费者层面形成的品牌认知，本身就构成了防御壁垒；但在更多行业场景中，护城河往往取决于行业结构，而非 AI 技术能力。例如，借助 AI 加速构建双边市场的平台，其持久性来自平台机制本身，而不是底层模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30596c95056a50fa39ca49c9a7644e72.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个重要变化是，软件护城河正在被削弱。过去需要多年、大规模团队才能构建的软件系统，如今在 AI 辅助编程的加持下，其可复制性显著提高。API 调用的灵活性也使开发者能够迅速切换工具，这让“API 即护城河”的逻辑变得愈发脆弱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 从企业市场的视角补充道：在真实的企业环境中，竞争焦点正从“谁的模型更强”，转向“谁能通过 API 和服务，以更优的性价比，帮助企业真正提升收入或降低成本”。在这个意义上，真正的“最佳模型”，往往是企业自身的商业模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从订阅制到按量计费：AI 正在重塑软件商业逻辑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在商业模式层面，圆桌讨论也触及了一个正在发生的结构性变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十余年，SaaS 以订阅制为核心，其背后依赖的是软件接近零边际成本的特性。但在 AI 尤其是智能体场景中，这一前提正在发生变化——推理成本真实存在，且可能随使用规模非线性增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 指出，当 AI 系统开始代表用户执行任务，且工作负载与用户数量脱钩时，更接近云服务的按量计费模式将变得合理且必要。吴恩达则从开发者体验出发，分享了一个直观感受：AI 编程工具的效率如此之高，以至于开发者愿意为其消耗更多算力和费用，因为由此带来的生产力提升是实实在在的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/757bce25dccf7a8bd6499ab85e2184a2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并非简单的定价方式变化，而是意味着 AI 正在重新定义“软件价值如何被衡量和付费”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成功的 AI 架构：产品先行，为不确定性留出空间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当讨论从战略转向工程实践，三位嘉宾形成了高度一致的共识：产品市场契合（PMF）始终优先于成本优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴恩达强调，在早期创新阶段，最大的挑战不是控制成本，而是打造用户真正热爱的产品。当 PMF 出现后，工程手段总能在后续阶段将成本曲线重新压低。关键在于，在架构设计之初，就为模型可替换性和技术选择权留出空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 从大量初创企业的实践中总结出一条清晰路径：&lt;/p&gt;&lt;p&gt;初期采用通用基础模型快速验证产品；随着真实负载显现，通过微调、蒸馏、提示缓存优化等手段应对非线性成本；将模型选型视为可演进的工程问题，而非一次性决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，掌控自身数据层被反复强调。将数据牢牢掌握在企业自身体系内，而不是被封装进供应商的“云端密匣”（box in a cloud），是确保未来技术与合作可选性的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;非结构化数据的真正解锁：从 PDF 开始&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈及 AI 应用的下一个增长点时，吴恩达将注意力投向了一个长期被忽视的领域：非结构化数据。&lt;/p&gt;&lt;p&gt;在他看来，企业中最具价值、却最未被充分利用的隐性数据，正大量存在于 PDF 文档之中。无论是金融领域复杂的报表，还是医疗行业的各类表单，过去人们对 PDF 的主要交互方式，往往只是简单的关键词搜索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而如今，借助智能体驱动的文档解析能力，AI 已能够理解复杂表格结构、提取语义信息，并将其转化为可分析、可计算的数据资产。这一变化，正在迅速催生大量新的企业级应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给开发者的长期建议：回到基础，拥抱创造&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在圆桌的最后，讨论回到了一个更具情绪张力的话题：年轻开发者在 AI 浪潮下的焦虑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Swami 指出，行业在某种程度上混淆了“编程”与“计算机科学”。即便 AI 能生成大量代码，对底层原理的理解，编译器、数据库、系统架构、数学与统计基础，依然不可替代。历史经验表明，每次技术变革初期都会经历短暂低谷与普遍焦虑，当前正处在类似阶段，但最终带来的是更大规模的创造者群体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴恩达则将这一判断推向更积极的方向：这是一个前所未有的创造窗口期。构建产品所需的时间和成本正在大幅降低，而 AI 辅助编程让“学习编程”本身变得更具现实意义和乐趣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 Sridhar Ramaswamy 在圆桌结束时表示，未来无需被动等待，当下的我们比以往任何时候都更有能力去进项创造 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/wLeViF4hSl0WQrw3r2cQ</link><guid isPermaLink="false">https://www.infoq.cn/article/wLeViF4hSl0WQrw3r2cQ</guid><pubDate>Fri, 16 Jan 2026 12:02:04 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>云计算</category><category>AI&amp;大模型</category></item><item><title>受够了Copilot的“霸王条款”？GitHub全球宕机遭怒骂，引爆开发者“大逃离”！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几个小时前，有大批开发者反馈：GitHub大面积宕机了，社交平台上充斥着“粉色独角兽”的截图和相应的控诉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/196132f0521e4f6f1f124188610a35ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于此次故障的原因，目前GitHub还未给出详细分析报告。然而，不少开发者们的猜测已把矛头指向了Copilot。而在近期，也有企业和许多个人开发者们在“逃离”GitHub并将代码库迁移到了其他平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;GitHub宕机遭怒骂：拖垮全世界开发流程&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“所有用户都被强制登出，我自己也登不进去，因为整个服务器都宕机了。”“我以为是自己眼花了，但GitHub真的宕机了。很多团队都在周四收尾迭代冲刺，这次故障怕是要导致大量工作项顺延。”“更新即将发布时，登录网站就崩溃了。”“尴尬的时刻：你意识到自己过去一个小时一直在本地提交代码，却无法推送。”“天哪，我现在该怎么给经理汇报啊。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一次GitHub的宕机，很快就在各大平台引发诸多抱怨。在发布的事件报告中能看到，GitHub承认，在宕机期间，“多项服务性能下降，特别是问题报告、拉取请求和 API。”历经大约两小时的故障排查与修复工作后，GitHub恢复服务和功能运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而GitHub也在解决问题后在X上作出回应，“本次故障已完全解决。感谢各位在问题处理期间的耐心与理解。故障根本原因的详细分析报告，将在完成后第一时间对外公布。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d75e6c0f283f5e1b475199d0e19607b2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于此事，网友纷纷表示，“GitHub 的风险太大了。一家公司不应该有能力拖垮全世界的整个开发流程。”“为什么这么多项目都只托管在 GitHub 上？没有一个镜像站点，所有功能都依赖于 GitHub，成千上万的人甚至都没有考虑过其他方案…”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人怀疑，“问题可能出在 Copilot 上。”也有人说道，“我们目前尚不能确定这是 Copilot 的问题。不能仅仅因为微软正愈发强制其开发者使用 GitHub Copilot，还吹嘘有大量内部代码由 GHCP 编写，且在此期间 GitHub 发生了数次严重宕机，就认定这些宕机与 Copilot 有关。我们不妨静观其变，等微软宣称此事与 GHCP 毫无关联时，我们就能确定GitHub 宕机和 Copilot 真的没关系了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Gentoo Linux将迁出GitHub，导火索直指Copilot&amp;nbsp;&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，已有企业在因&amp;nbsp;GitHub 强制推行 Copilot 工具的举措而放弃这一平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，Gentoo Linux 当前正计划将其仓库从 GitHub 全面迁移。而迁移的导火索，正是GitHub 试图“强制代码库使用 Copilot”。Gnoppix 是领先的开源 AI Linux 发行和服务提供商，以其通过 Portage 实现高度可定制的基于源码的包管理系统而闻名，他们一直依赖 GitHub 托管其主要的 git 仓库，包括作为数千个 ebuild 上游源代码的关键 gentoo.git 树。这一基础设施支持了一个全球开发者和用户社区，他们欣赏Gentoo在编译针对特定硬件架构和优化标志软件方面的灵活性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gentoo 社区的讨论中，提及了一个关键事件：GitHub 会在代码仓库页面自动弹出横幅提示，敦促贡献者 “启用 Copilot”，并警告不配合的仓库将面临曝光度降低或功能受限的后果。Gentoo 开发者表示，这类干预行为严重干扰了正常开发流程，在合并请求与代码评审环节强行插入未经请求的 AI 代码建议。Gentoo 理事会一名成员发文称：“GitHub 正公然试图强迫我们的代码仓库启用 Copilot，而这是我们明确反对的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以 Gentoo 维护者为代表的批评者认为，该工具的运作模式在开源许可证的精神与条文层面均构成侵权。Gentoo 的大量软件包构建脚本（ebuild）及专属代码，均采用 GNU 通用公共许可证（GPL）或知识共享协议的衍生版本授权，这类协议通常要求衍生作品需以兼容的授权条款进行共享。而 Copilot 的 “黑箱” 训练流程，使得外界无法判断其生成代码是否构成衍生作品，这就可能导致专有软件在未经署名或未遵循互惠原则的情况下，挪用开源代码成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“再见了，Github，欢迎Codeberg。”1月5日，Gentoo Linux在发布的2025 年度项目回顾报告中披露迁移细节：受持续遭遇的 GitHub Copilot 强制启用相关争议影响，Gentoo 目前正评估并计划将代码仓库镜像及合并请求贡献渠道迁移至 Codeberg 平台。Codeberg 是一个基于 Forgejo 搭建的代码托管网站，由非营利组织维护，服务器部署于德国柏林。Gentoo 核心的 Git 代码库、问题工单系统等基础设施仍由官方自行托管，且暂无变更计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，有外媒报道称，Gentoo 的迁移计划将分阶段推进。初期工作将聚焦于 gentoo.git 核心代码仓库，目标在数月内完成迁移。Gentoo 基础设施团队已在多款备选代码托管平台完成镜像原型部署，评估维度涵盖 Git 托管可靠性、问题追踪系统集成能力、持续集成 / 持续部署（CI/CD）流水线支持以及网页端代码浏览体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移备选方案还包括GitLab、SourceHut，以及在 Gentoo 内部服务器部署 Gitea 等同类平台。其中，自托管方案可实现对代码资产的完全掌控，但需投入额外的运维成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;个人开发者成批“逃离”，Copilot疑似承认再利用开源代码&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;个人开发者们对GitHub的类似不满也十分严重。在各大开发者社区及讨论平台上，都有愤怒的GitHub用户表示，他们应该摆脱强制使用的Copilot功能。过去一年来，GitHub 平台上的开发者们最热门的讨论之一也是：是否应该阻止微软的AI服务Copilot自动在代码仓库中生成问题和拉取请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月6日，有开发者在 GitHub上提交一项讨论，称其发现“Copilot似乎公开承认其对开源代码进行了再利用（或盗用？），且未遵守署名规定”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c83d109dac605fbae649f4f0173c8212.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，微软似乎拒绝禁用这些功能，导致许多开源软件开发者开始质疑是否存在其他替代方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者Andi McClure，在去年1月向微软Visual Studio Code代码仓库提交请求，抗议自己卸载 Copilot 扩展程序后，VS Code 界面中却再次出现 Copilot 图标。一位名叫 Constantine 的开发者在 McClure 的帖子下写道，“今天我拒绝了 Copilot 为我的 PR 生成的两条代码建议，这非常令人不安，所以我开始搜索，然后找到了这个讨论。拒绝使用AI对我来说是原则问题，所以如果这种情况继续下去，微软又不尽快提供让我选择退出代码库AI的方法，我会把我的代码迁移到自托管的解决方案上，并且永远不会再回到 GitHub。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年9月，McClure在一封邮件中表示，“此前每当 Copilot 干扰我的 GitHub 使用体验时，我都会在 GitHub 社区反馈区提交问题工单。我强烈反感的是，Copilot 表面上似乎在未经许可的情况下，利用我发布在 GitHub 上的代码进行训练，违反了我设定的开源许可协议；而 GitHub 还要在我眼前反复推送这款我绝不会使用的工具，简直跟广告没两样。既然这件事已经对我造成困扰，我认为没有理由再保持沉默。我觉得，我们之所以会被迫接受一些大家都不认同的事物，部分原因就是我们选择了忍气吞声。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;McClure还表示，自 GitHub 从微软旗下一家独立子公司，被划归至微软核心AI部门（CoreAI group）后，“开源社区的态度似乎已从抱怨Copilot变为开始主动远离GitHub。”据称，其在开源社区的不少同仁都在讨论从 GitHub 迁移至 Codeberg，或是自建基于 Forgejo 的托管平台（Codeberg 正是基于 Forgejo 搭建）的计划。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“微软对Copilot的推广似乎完全是自上而下的，高层似乎已经彻底忘记了客户留存等传统目标。不管出于什么原因，他们只想提升‘AI’指标，把客户群仅仅当作提升这些指标的工具。”McClure认为，人们已经开始厌倦这种情况，如果持续下去，将会削弱开发者与GitHub之间的网络联系，加速开发者进一步迁移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.githubstatus.com/incidents/q987xpbqjbpl&quot;&gt;https://www.githubstatus.com/incidents/q987xpbqjbpl&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.gentoo.org/news/2026/01/05/new-year.html&quot;&gt;https://www.gentoo.org/news/2026/01/05/new-year.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://forum.gnoppix.org/t/gentoo-linux-plans-migration-from-github-over-attempts-to-force-copilot-usage-for-our-repositories/3934&quot;&gt;https://forum.gnoppix.org/t/gentoo-linux-plans-migration-from-github-over-attempts-to-force-copilot-usage-for-our-repositories/3934&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/H16Z6V1Cz3Sf1qeb4fwr</link><guid isPermaLink="false">https://www.infoq.cn/article/H16Z6V1Cz3Sf1qeb4fwr</guid><pubDate>Fri, 16 Jan 2026 10:00:00 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>拒绝传统Router“瞎指挥”，多智能体如何实现智能任务分配？</title><description>&lt;p&gt;企业级多智能体（Multi-Agent）系统最大的瓶颈，往往不是Agent不够强，而是负责分发任务的Router（路由器）太“傻”。传统Router只会做简单的单选分类，面对复杂的企业级故障经常“瞎指挥”，在企业运维的十字路口，我们需要一个更聪明的“交警”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去一年里，Multi-Agent架构正在成为企业AI的新基建。我们忙着造更强的SQLAgent、更快的检索Agent，但却发现运维系统的十字路口却越来越拥堵了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和想象中的Agent们“游刃有余”的自动协同、分工协作不同，因为传统Router的上限太低、智能程度有限，很难跟上Agent们“匆匆忙忙”的脚步。在未来的企业&amp;nbsp;AI&amp;nbsp;系统中，Agent越来越多，能力边界越来越模糊，系统必须具备“承认不确定性并协作解决”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，腾讯云正式开源TCAR（Tencent Cloud Andon Router）——一个只有4B参数，但学会了“先想清楚，再选择”的智能路由模型，它专为解决跨域、冲突和模糊问题而生，为企业AI应用提供Reasoning-centric Routing+Multi-Agent Collaboration的基础形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么传统Router在企业运维场景里“玩不转”了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里可以看几个非常常见的场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1、不同agent可能能解决一样的问题，传统Router通常为单标签分类，只考虑选择一个agent，导致无法给出最优解决方案。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59f3d3f5d8df472e3793060967380b4a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;2、新业务、新Agent随时上线，传统Router对这些“新同事”完全不了解，需重新训练，也就无法快速分配给他们最合适的工作。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/89/890d442348431d5d2f3f2804da184552.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;3、用户描述模糊、不完整。例如用户提到“网站访问延时”，传统Router就无法确定不确定是CDN、COS还是网络的问题。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c101e510649ddacd2de749050e74ad2e.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;4、传统Router缺乏可解释性，黑盒决策，一旦路由错了，没法快速修复badcase，后面Agent再强也救不回来。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/1972e99ed6a4ef6584029842603b9095.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;总结来说，传统Router面对企业场景有三大硬伤：搞不定跨域、解不了冲突、跟不上变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;TCAR的解法：像人类专家一样“先想后做”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCAR（TencentCloudAndonRouter）的核心很简单，但在Router中几乎没人认真做过——把路由从直接预测标签，变成先推理再选择Agent集合。这时候，Router不再是一个收发任务的转接系统，而是变成了一个具备推理能力的“决策者”。它把路由过程从单项选择变成了“写分析报告+组建任务组”；它的工作职能从挑选队列最前面的agent完成任务，到在专家梯队中找到最合适的那个人选来完成任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它就像是一个拥有顶尖专家团队的，高度聪明且能够自我决策的“项目经理”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能力一：Reason-then-Select（拒绝黑盒，把思考过程写出来）&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c8/c898827dd2a674d104464ca6a3e4c1a2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;TCAR在输出Agent之前，会先生成一段自然语言推理链，明确说明问题可能涉及哪些技术栈，不同Agent的职责边界，为什么多个Agent执行是合理的，这让路由不再是黑盒，而是可解释、可Debug、可持续优化Agent描述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能力二：从单挑到团战&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/accbee1a4fa500c49d98485ba40d8f61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;在TCAR中路由结果不再是one-hot，而是一个Agent子集，这一步直接解决了企业系统中最棘手的Agent冲突问题：不强行压缩决策，而是保留不确定性，交给后续协作解决。当然，这也要建立在对指令聪明且充分的理解力上。&lt;/p&gt;&lt;p&gt;能力三：专家会诊，择优输出&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/7944b8f50fc5ca92c5a34c6b8437a3c7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;当TCAR选出多个候选Agent后，每个Agent独立给出自己的专业答案，而后由一个RefiningAgent负责对比、消歧、融合，最终输出一个完整、无冲突的答案，这套模式在排障类问题上效果尤其明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;覆盖全面、命中精准，硬核且强大&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TCAR不是一个简单的Prompt工程产物，为了让它具备上述能力，我们做了两件比较特别的事情：&lt;/p&gt;&lt;p&gt;一是两阶段训练+特殊融合，兼顾推理能力和选择精度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶段一SFT（监督微调）：教会模型结构化推理，学会输出Agent集合，通过Slerp方法融合模型。阶段二RL（强化学习/DAPO）：重点调教模型“选得对不对”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;二是专门针对多Agent设计奖励函数，把路由当成一个集合预测问题，在模型覆盖率和精确度之间形成稳定平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;R1奖励（类似精确率Precision）：你选出来的Agent里，有多少是真正干活的？（防止选了一堆没用的配角）R2奖励（类似召回率Recall）：关键的那几个Agent，你有没有漏掉？（防止漏掉主角）长度惩罚：防止模型为了求稳把所有Agent全选上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，经过CLINC150、HWU64、MINDS14、SGD、Qcloud五个数据集的评测，TCAR&amp;nbsp;在企业高冲突数据上全面超过当前主流大模型&amp;nbsp;Router，在高歧义、跨域问题中更稳定，4B&amp;nbsp;参数量推理速度快成本低，更重要的是下游多Agent + Refining Agent的整体成功率显著提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;腾讯云还提供了全套的完整开源范式，包括：TCAR&amp;nbsp;路由模型（4B）、Prompt&amp;nbsp;规范（Router / Refining Agent）、训练方法与实验细节、可直接落地的多&amp;nbsp;Agent&amp;nbsp;路由范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相关链接：&lt;/p&gt;&lt;p&gt;HuggingFace：https://huggingface.co/tencent/TCAndon-RouterGitHub：https://github.com/Tencent/TCAndon-RouterPaper：https://arxiv.org/pdf/2601.04544&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cYlRMETcNGxhDvBqCDII</link><guid isPermaLink="false">https://www.infoq.cn/article/cYlRMETcNGxhDvBqCDII</guid><pubDate>Fri, 16 Jan 2026 09:20:05 GMT</pubDate><author>腾讯云</author><category>腾讯</category><category>生成式 AI</category></item><item><title>聚焦 AI Agent 系统诊断，智能运维助手 SysOM MCP 正式开源</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AIOps 新范式：说句话就能做运维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，操作系统运维面临复杂架构、依赖关系混乱、故障定位难、依赖人工经验、工具碎片化、监控不足及自动化欠缺等挑战。为应对以上难题，阿里云结合大语言模型（LLM）、智能体（Agent）与模型上下文协议（MCP），实现了自然语言驱动的智能运维：LLM 理解指令，Agent 自主执行任务，MCP 连接底层诊断工具。这三者的协同，使 AI 助手能自动诊断系统问题，生成报告与修复建议，显著提升效率，推动运维向主动智能演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云操作系统控制台（以下简称操作系统控制台）是一站式操作系统运维管理平台，提供了内存、I/O、网络、内核崩溃等强大的系统诊断能力，SysOM是操作系统控制台的运维组件。但这些功能通常需要用户登录控制台，并具备一定的运维经验才能有效使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 助手（如 Qwen Code）的普及，用户更希望用自然语言一句话解决问题，比如“为什么 CPU 变高了？”为此，SysOM 将原有诊断能力通过 MCP（Model Context Protocol）进行标准化封装，推出开源项目&amp;nbsp;SysOM MCP。SysOM MCP 脱胎于阿里云操作系统控制台，把复杂的运维操作转化为 AI 可直接调用的标准工具，让 AI Agent 能像专业工程师一样“动手”诊断系统问题——用户无需懂命令，只需用自然语言提问，即可获得精准的系统级分析。如今，SysOM MCP 正在推动自然语言成为操作系统诊断的新入口，让智能运维真正走向普惠与高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 项目开源地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/sysom_mcp&quot;&gt;https://github.com/alibaba/sysom_mcp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SysOM MCP：用自然语言驱动系统诊断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统运维依赖命令行和专家经验，而通用 AI 虽能“说”却不能“做”。SysOM MCP&amp;nbsp;的出现填补了这一鸿沟——通过 MCP 协议，AI 不仅能理解问题，还能自动执行真实诊断，实现从“问答”到“行动”的闭环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 项目内置超过 20 个生产级诊断工具，全部通过标准 JSON-RPC over stdio/SSE 暴露，包括：&lt;/p&gt;&lt;p&gt;内存分析：内存全景诊断、Java 内存诊断、OOM 内存诊断；IO 诊断：IO 一键诊断、IO 流量分析诊断；网络排查：网络丢包诊断、网络抖动诊断；调度诊断：系统负载诊断、调度抖动诊断；磁盘诊断：磁盘分析诊断；宕机诊断：宕机诊断（dmesg 分析）、宕机诊断（vmcore 深入分析）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目支持&amp;nbsp;--stdio&amp;nbsp;（本地嵌入）和&amp;nbsp;--sse（HTTP 服务）两种模式，轻松集成各类 AI 客户端。&lt;/p&gt;&lt;p&gt;要在支持 MCP 协议的 AI Agent 平台（如 Qwen Code）中使用 SysOM MCP，首先需将项目代码克隆到本地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;git clone https://github.com/alibaba/sysom_mcp.git
cd sysom_mcp&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再在配置文件中添加如下配置，就可以让 AI 助手能以自然语言驱动操作系统及运维操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;{
&amp;nbsp;&amp;nbsp;&quot;mcpServers&quot;: {
&amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;sysom_mcp&quot;: {
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;command&quot;:&amp;nbsp;&quot;uv&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;args&quot;: [&quot;run&quot;,&amp;nbsp;&quot;python&quot;,&amp;nbsp;&quot;sysom_main_mcp.py&quot;,&amp;nbsp;&quot;--stdio&quot;],
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;env&quot;: {
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;ACCESS_KEY_ID&quot;:&amp;nbsp;&quot;your_access_key_id&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;ACCESS_KEY_SECRET&quot;:&amp;nbsp;&quot;your_access_key_secret&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;DASHSCOPE_API_KEY&quot;:&amp;nbsp;&quot;your_dashscope_api_key&quot;
&amp;nbsp; &amp;nbsp; &amp;nbsp; },
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;cwd&quot;:&amp;nbsp;&quot;&lt;sysom mcp项目目录=&quot;&quot;&gt;&quot;,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;timeout&quot;:&amp;nbsp;30000,
&amp;nbsp; &amp;nbsp; &amp;nbsp;&amp;nbsp;&quot;trust&quot;: false
&amp;nbsp; &amp;nbsp; }
&amp;nbsp; }
}&lt;/sysom&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最佳实践：谈话间揭秘隐藏内存泄漏&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;amp;mid=2247489171&amp;amp;idx=1&amp;amp;sn=478ea3261ce3d329ed55e05331c080a6&amp;amp;scene=21#wechat_redirect&quot;&gt;OS Copilot&amp;nbsp;&lt;/a&gt;&quot;是阿里云基于大模型构建的操作系统智能助手，支持自然语言问答、辅助命令执行、系统运维调优等功能，帮助您更好地使用 Linux 系统，提高 Linux 的使用效率。目前，操作系统控制台上的 OS copilot 已接入 SysOM MCP，用户只需在操作系统控制台中以自然语言与 OS Copilot 对话，即可自动触发操作系统问题的根因排查。整个诊断过程无需人工干预，结果以结构化形式清晰呈现，大幅降低运维门槛，让复杂问题“一问即解”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文以隐蔽的内存泄漏为例，展示 SysOM MCP 的诊断功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5f/5f472f9db24e87b31f8202d62b8823fc.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/932801e5554e51e1584245c35663a8f9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们可以看到上图的对话中，OS Copilot 给出了可能的泄漏原因。同时也可以点击图中下方的诊断报告，在操作系统控制台查看更详细的诊断结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/92/9246802fc8116d85f45d2778d5c1b60f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 脱胎于阿里云操作系统控制台，诊断工具已在大规模生产环境验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下载地址&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目采用现代 Python 工具链（uv + Python 3.11+），安装简单：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;git clone https://github.com/alibaba/sysom_mcp.git
cd sysom_mcp &amp;amp;&amp;amp; uv sync
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持一键启动：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;uv run python sysom_main_mcp.py --stdio &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;# 供本地调用 &amp;nbsp;
uv run python sysom_main_mcp.py --sse --port 7140 &amp;nbsp;# 启动 HTTP 服务 &amp;nbsp;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用场景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SysOM MCP 可接入各种 AI agent，帮助您打造具备系统诊断能力的智能助手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开源共建&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🌟&amp;nbsp;GitHub 地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/sysom_mcp&quot;&gt;https://github.com/alibaba/sysom_mcp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欢迎 Star、Fork、提交 Issue，一起构建 AI 原生运维新生态！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;联系我们&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若想使用更全面的 SysOM 功能，请登录阿里云操作系统控制台体验，地址：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://alinux.console.aliyun.com/&quot;&gt;https://alinux.console.aliyun.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您在使用操作系统控制台的过程中，有任何疑问和建议，可以扫描下方二维码或搜索群号：94405014449&amp;nbsp;加入钉钉群反馈，欢迎大家扫码加入交流。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b25287aa24b6e95b58056b5056ca0927.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;操作系统控制台钉钉交流群&lt;/p&gt;</description><link>https://www.infoq.cn/article/285HkzS0EM8tpHg9mBgY</link><guid isPermaLink="false">https://www.infoq.cn/article/285HkzS0EM8tpHg9mBgY</guid><pubDate>Fri, 16 Jan 2026 08:46:00 GMT</pubDate><author>万瑞萍</author><category>阿里巴巴</category><category>操作系统</category></item><item><title>亚马逊云科技发布第五代 Graviton 处理器，M9g 实例同步登场</title><description>&lt;p&gt;亚马逊云科技近日宣布了&lt;a href=&quot;https://www.aboutamazon.com/news/aws/aws-graviton-5-cpu-amazon-ec2&quot;&gt;全新的 Graviton5 处理器&lt;/a&gt;&quot;，并预览了首批基于该处理器的 EC2 实例——通用型 M9g 实例。据亚马逊云科技介绍，Graviton5 相比 Graviton4 最高可提升 25% 的性能，并首次引入 Nitro 隔离引擎（Isolation Engine），同时配备更大的 L3 缓存，在延迟、内存带宽和网络吞吐量方面均有所改善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据新闻稿，基于 Arm 架构的 &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/m9g&quot;&gt;EC2 M9g 实例&lt;/a&gt;&quot;单实例最多支持 192 个 CPU 核心。更高的核心密度使核间延迟最多降低 33%，并提升了带宽表现，从而改善了数据库、分析型负载、应用服务器、游戏以及电子设计自动化（EDA）等工作负载的扩展能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Graviton5 将 Nitro 隔离引擎纳入 Nitro 系统。这是一个全新的引擎，利用形式化验证来证明不同工作负载之间，以及工作负载与亚马逊云科技运维人员之间的隔离性。该引擎拥有规模较小且经过验证的代码库，亚马逊云科技还将向客户开放其实现及相关证明，以供审查。亚马逊云科技内核和虚拟化工程师 Mohamed Mediouni &lt;a href=&quot;https://www.linkedin.com/posts/mohamed-mediouni-kaos_graviton5-introduces-the-nitro-isolation-activity-7402401265080664064-o_cD/&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一个厂商自研的 hypervisor（说白了，就是对 KVM 的替代）本身就很有意思。至于这条路最终会走向哪里，还有待观察。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在最近的 re:Invent 大会上，亚马逊云科技还介绍了主题为 《&lt;a href=&quot;https://www.youtube.com/watch?v=hqqKi3E-oG8&quot;&gt;引入 Nitro 隔离引擎：用数学证明实现云端透明安全&lt;/a&gt;&quot;》 的演讲，目前已在油管上线。亚马逊副总裁兼杰出工程师 &lt;a href=&quot;https://www.linkedin.com/posts/a-saidi_aws-reinvent-2025-introducing-nitro-isolation-activity-7403510493107126272-1b-O?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAABaQ5R4B1z_TPIVzQKBvbJ9SpDn29zaiJcY&quot;&gt;Ali Saidi 表示&lt;/a&gt;&quot;：“Nitro 隔离引擎利用 Rust 和形式化验证，打造了一个经过数学证明的云 hypervisor，为云安全树立了新的标准。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技客户对 Graviton 的采用率持续增长。根据官方数据，亚马逊云科技新增 CPU 容量中已有超过 50% 来自 Graviton；在最近一次亚马逊会员日活动中，Amazon.com 使用的 EC2 计算资源中，有超过 40% 由 Graviton 提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在硬件层面，Graviton5 的 L3 缓存容量是 Graviton4 的五倍，单核可用 L3 缓存增加至原来的 2.6 倍，降低了内存访问的延迟。其内存速度也高于基于 Graviton4 的 M8g 实例，对大规模、内存密集型工作负载更为有利。官方还指出，网络带宽平均提升可达 15%，EBS 带宽最高提升 20%，整体网络吞吐能力相比上一代实例最高可提升两倍。不过在 Hacker News 上，用户 diath &lt;a href=&quot;https://news.ycombinator.com/item?id=46171008&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;没有基准测试，没有 FLOPs，也没有和通用硬件的对比……“9 比 8 快，8 比 7 快，7 比 6 快，……1 最慢，但具体多快谁也不知道。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Reddit 上的讨论则褒贬不一，主要质疑集中在技术细节披露不足，以及 M9g 目前仅在部分区域以预览形式提供。用户 Ill-Side-8092 &lt;a href=&quot;https://www.reddit.com/r/aws/comments/1penqtp/comment/nsfr0v9/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;作为一次自然的增量更新，这当然是件好事，而且 Annapurna 团队算是当前 AWS 中少数仍在进行真正创新的团队之一。不过现实是，在大厂之间，自研芯片如今已经成了“入场门槛”——谷歌、苹果和微软也都有相当成熟的自有芯片方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有用户指出，亚马逊云科技在不同区域之间的服务和功能一致性仍然存在明显差距。用户 Rude_Walk &lt;a href=&quot;https://www.reddit.com/r/aws/comments/1penqtp/comment/nsljsh4/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;要不先把 Graviton4 全面铺开？新加坡这种重要区域现在都还没支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技表示，Graviton5 的早期采用者包括 Adobe、Epic Games 和 Pinterest 等公司。目前，亚马逊云科技已开放 M9g 实例预览版的申请页面，供用户&lt;a href=&quot;https://pages.awscloud.com/AWSGraviton5-AmazonEC2M9ginstances-Preview.html&quot;&gt;报名体验&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/xtXzsEPA5rVb6ZGfpUyh</link><guid isPermaLink="false">https://www.infoq.cn/article/xtXzsEPA5rVb6ZGfpUyh</guid><pubDate>Fri, 16 Jan 2026 08:23:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>框架</category></item><item><title>OpenAI前团队创业内乱，CTO泄密竞对遭开除！翁荔火速发文</title><description>&lt;p&gt;过去一年里，前 OpenAI 首席技术官 Mira Murati 创办的 Thinking Machines Lab，一度被视为“OpenAI 体系外最值得关注的实验之一”。然而，这家成立时间并不长、却已完成 20 亿美元种子轮融资的明星公司，如今正迎来成立以来最关键的一次人员震荡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间周三，Murati 在社交媒体上宣布了公司联合创始人兼 CTO Barret Zoph的离职。“我们已与Barret Zoph分道扬镳，”Murati在X平台上发帖称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Soumith Chintala 将出任Thinking Machines的新任 CTO。他是一位才华横溢、经验丰富的领导者，十多年来为人工智能领域做出了重要贡献，也是我们团队的重要成员。我们非常高兴他能承担起这项新的职责。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Murati 声明中的用词较为克制，仅强调“双方已分道扬镳”，并未对离职原因、过程或分歧作出任何解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9a/9a65a758cca557efb2e70169a5d27057.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据有人获悉的内部消息，此次是由于 Barret Zoph 个人的不道德行为（向竞争对手公司泄漏了公司机密），Thinking Machines Lab 才解雇了他。Mira Murati 甚至是在全体员工大会上宣布了这一消息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/835dfa3bbd8a99f4a7d779a7c6fff42c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但不到一小时后，另一则公开信息迅速改变了外界对这一事件的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 应用部门 CEO Fidji Simo同样在 X 平台发文，宣布欢迎 Barret Zoph、Luke Metz 以及 Sam Schoenholz “回归 OpenAI”，并明确表示这一安排“已筹备数周”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Simo在发文中称，“Barret 将向我汇报工作；Luke 和 Sam 将向 Barret 汇报工作。关于他们未来的工作重点，敬请期待！”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f4/f458d4cd3910684874a5d4c0177cc084.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;短短 58 分钟内，两个高度相关却几乎没有交集的公开声明，构成了一次罕见而耐人寻味的“信息对冲”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从结果看，这不仅是 Thinking Machines Lab 失去了一位联合创始人，更是一次系统性的“回流”：三位核心技术人物，几乎同时从 Murati 的创业公司转向 OpenAI。对于一家尚未正式推出核心产品、却以“重塑通用人工智能研究方式”为愿景的初创公司而言，这一变化的象征意义，远大于人员数量本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着事情的发酵，Thinking Machines Lab 联合创始人、前 OpenAI 安全研究副总裁翁荔（Lilian Weng）也在X上发声了，她的发文中没有明确就此事展开评论，而是发了一些个人感慨：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/ac30f030960f27c584c284ece3bba8a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从 OpenAI 核心到创业阵营&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要理解这次事件的分量，必须回到几位关键人物的背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mira Murati 曾长期担任 OpenAI 的首席技术官，是 GPT-4 等核心模型研发阶段的重要管理者之一。她在 2024 年 9 月离开 OpenAI，此举一度被外界解读为 OpenAI 高层频繁变动背景下的又一次重要分叉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;离职后不久，她便与 Barret Zoph、Luke Metz 等人共同创立 Thinking Machines Lab，并亲自出任首席执行官。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a0/a01fd91c64633279d997c7ed5145f7a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Thinking Machines Lab CEO、OpenAI 前CTO Mira Murati&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Zoph 的履历在 AI 研究界极具分量。他曾担任 OpenAI 研究副总裁，在此之前，他在谷歌担任研究科学家长达六年，是深度学习与架构搜索方向的重要研究者之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 OpenAI 期间，他负责后训练（Post-Training）研究，这包括模型对齐、工具调用、评估体系、ChatGPT 性能提升、搜索功能以及多模态能力开发等关键项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其博客个人介绍中也明确指出，他在 OpenAI 工作期间 “他的团队负责对齐、工具使用、评估、ChatGPT、搜索和多模态等功能，并训练用于 ChatGPT 与 API 的模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些职责意味着 Zoph 在当时直接参与了 OpenAI 核心模型（例如 ChatGPT 与大型语言模型系列）从研究阶段到产品化前训练阶段的关键工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/31/310edd24ffe4ad0adff6520b3e035213.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Thinking Machines Lab 前 CTO、 OpenAI 前研究副总裁 Barret Zoph&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Luke Metz 同样曾在 OpenAI 工作多年，曾是 OpenAI 团队的创始成员之一。他与 John Schulman、Barret Zoph、Liam Fedus 以及其他许多人一起，开发了ChatGPT。在此之前，他曾在 Google Brain 担任研究科学家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在进入行业之前，Metz 就已活跃于机器学习研究领域，其学术作品涉及优化、元学习、生成模型等多个方向。从他在 ResearchGate 和 Semantic Scholar 上的发表记录可以看出，他参与了多篇关于深度学习、学习优化策略和元学习的论文，这些论文的引用量很高，表明其研究在社区内部具有较强影响力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e8/e8598c453903bbdfd6c54c89ff28734b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 前核心成员 Luke Metz&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sam Schoenholz 也有 OpenAI 工作经历，在 OpenAI 工作期间，他参与了多个生成式模型研发相关的体系贡献，但官方报道并未具体列出他的单项项目头衔或单独负责成果。他的 LinkedIn 页面在事件发生后仍显示其供职于 Thinking Machines Lab。在加入OpenAI之前，他也曾供职于谷歌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5da470f9ef0afb80027ba6771f07cde1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI前研究员 Sam Schoenholz&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然 Thinking Machines Lab 的官方公告未单独提到 Schoenholz 的离职，但 OpenAI 的声明中明确包括他，表明他确实从Thinking Machines Lab 转向 OpenAI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因如此，Thinking Machines Lab 从一开始就被视为一次“前 OpenAI 核心技术团队的集体外溢”。这一判断并非空穴来风。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;高调的AI创企，首个产品没砸出任何水花&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司自成立之初就定位于 构建通用、更可定制、更易理解的 AI 系统，希望让 AI 能适应人类多种需求，并让研究社区与开发者更容易使用最前沿的模型能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成立后不到半年，Thinking Machines Lab 在 2025 年 7 月完成了规模高达 20 亿美元 的种子轮融资，由风险投资机构 Andreessen Horowitz（a16z） 领投，多家机构参与，包括 Nvidia、AMD、Accel、Cisco、ServiceNow 和 Jane Street 等。该融资使公司估值达到约 120 亿美元，成为当时成立尚短、尚无收入、尚未明确推出产品的 AI 初创公司中估值最高的一例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一融资规模之所以备受关注，主要来自两个方面：一是资本对核心人才的押注：资本市场对包括 Murati 在内的“OpenAI 系创业团队”给予了极高的信任，认为该团队能够开拓新一代 AI 研究与产业方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;二是对 AI 未来路径的押注：投资者愿意用“种子轮”阶段资金支持一个尚未推出产品的团队，反映出市场对 AI 基础研究与基础设施工具方向的高度预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这个阶段，公司极少透露技术细节、产品计划或战略路线图，外界对其业务进展了解甚少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到 2025 年 10 月，Thinking Machines Lab 才首次对外推出公司开发的 第一个产品——Tinker。这是公司成立之后首次公开可用于外界访问的软件工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Tinker 是一个 面向 AI 开发者和研究者的 fine-tuning（微调）工具 API，旨在让用户无需管理复杂的训练基础设施，就能对大型语言模型进行定制训练。具体来说：它为语言模型提供微调的 API 接口，让开发者可以通过几行代码选择不同模型（如 LLaMA、Qwen 等），进行任务定制。 官方宣传称该产品旨在“将前沿 AI 能力民主化，让更广泛的研究者和开发者有机会实验与定制模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，首款产品问世后，市场反馈保持审慎甚至怀疑的态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些用户认为，目前许多大模型平台（包括 OpenAI、Anthropic、Meta 等）已有 fine-tuning 工具，因此 Tinker 并不构成明显的“突破性产品”，存在独特性不足的短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，根据少量外媒评述，Tinker 的用户基础尚处于探索期，没有展现出“行业级爆款”效果，而只是一个在开发者社区和实验环境中被尝试的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推出时间距离融资完成有较长滞后，且产品本身尚处于早期阶段，并未显现出对现有大模型平台的明显替代或补充效应，因此外界对Thinking Machines Lab的评价愈发消极。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种情况下，往往更容易暴露出创始团队的理念分歧。因此也就有了如今创始团队“内讧”的局面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/56/5632762a27f2a490f96c3193fa6f299b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截图源自网络&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;资本充足，为什么还会出现人员流失？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wired 的相关报道指出，Zoph 与 Thinking Machines Lab 的分手“并不友好”。虽然报道并未披露具体冲突细节，但这一描述本身，已经与 Murati 在公开声明中所采用的克制措辞形成了明显对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，Murati 的声明中并未提及 Luke Metz 与 Sam Schoenholz 的去向，也未就公司内部是否发生更广泛的人事变动作出说明。而 OpenAI 方面的表态，则一次性点名三人，并强调这一回归“已筹备数周”。这意味着，至少在时间线上，相关沟通并非临时决定，而更可能是在 Thinking Machines Lab 内部结构尚未对外公开调整之前，就已基本敲定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一时间差，为外界留下了足够的解读空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从 OpenAI 的角度看，这次人员回流同样耐人寻味。近年来，OpenAI 在模型研发、产品化和组织治理层面都经历了快速变化。应用部门的地位不断上升，通用模型之外，如何将能力转化为可持续的产品与平台，成为公司内部的重要议题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Fidji Simo 的表态，释放了一个清晰信号：这些回归并非偶发，而是被视为战略性补强。Zoph 曾负责 OpenAI 的研究工作，Metz 与 Schoenholz 也熟悉公司内部体系。在一个研发复杂度不断提高、组织规模持续扩张的阶段，重新吸纳“熟悉文化、具备研究深度”的技术骨干，本身就是一种降低组织摩擦的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与之形成对照的是，Thinking Machines Lab 仍处于“高度依赖创始人协作”的阶段。当联合创始人之间在研究方向、组织节奏或权责边界上出现分歧时，其冲击往往更直接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次事件的特殊性，还在于其发生在一家资金极其充裕的初创公司内部。20 亿美元的种子轮融资，本应为团队提供充足的试错空间。但事实表明，资金并不能消解所有结构性张力，尤其是在高密度智力劳动与强价值主张并存的组织中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次事件已经为外界提供了一个极为罕见的观察窗口：当顶级 AI 人才、超级资本与宏大愿景同时汇聚时，真正决定组织命运的，往往不是融资规模或履历光环，而是能否在高度不确定的探索中，维持稳定而清晰的共同方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，以TechCrunch 为代表的多家外媒已联系 Thinking Machines Lab 与 OpenAI 寻求进一步置评，但截至报道时，双方尚未提供更多公开说明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友怎么看？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此事发生后迅速在社交媒体引发热议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Reddit平台，有用户评论称这次“AI 人才流转实在太正常了”，OpenAI 有能力快速重新吸引之前的员工，因为其资源和品牌影响力强大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位用户用半开玩笑的语气说他“想象 Zoph 像是 OpenAI 的间谍，被抓住后跑回去了（笑）”，表明社区对事件背后动机的好奇与揣测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54427aaae3eb86e16beab8831e071018.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次 Thinking Machines Lab CEO 与其他成员之间“不体面的分手”也让一些网友嗅到了OpenAI内部层级架构之间的不足之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在X平台，有用户表示，“OpenAI 在人工智能领域是开放的，但在组织结构方面则不然。&lt;/p&gt;&lt;p&gt;层级结构依然存在。大型团队就是这样运作的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0165efc93224540ee7aed3ccbab2e16.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://dataconomy.com/2026/01/15/openai-rehires-top-talent-as-muratis-12b-startup-loses-co-founders/?utm_source=chatgpt.com&quot;&gt;https://dataconomy.com/2026/01/15/openai-rehires-top-talent-as-muratis-12b-startup-loses-co-founders/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/01/14/mira-muratis-startup-thinking-machines-lab-is-losing-two-of-its-co-founders-to-openai/&quot;&gt;https://techcrunch.com/2026/01/14/mira-muratis-startup-thinking-machines-lab-is-losing-two-of-its-co-founders-to-openai/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Luke-Metz?utm_source=chatgpt.com&quot;&gt;https://www.researchgate.net/profile/Luke-Metz?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://lukemetz.com/about/&quot;&gt;https://lukemetz.com/about/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/jP3NTaQpjTjP7U7FrSRW</link><guid isPermaLink="false">https://www.infoq.cn/article/jP3NTaQpjTjP7U7FrSRW</guid><pubDate>Fri, 16 Jan 2026 07:13:46 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>AWS CloudWatch成为一个统一的可观测性平台，支持Apache Iceberg</title><description>&lt;p&gt;亚马逊云科技宣布，&lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;Amazon CloudWatch&lt;/a&gt;&quot;实现重大增强，从一个基本的监控服务转变为一个统一的可观测性平台，能够整合多账户环境中的操作、安全和合规日志。本次更新解决了企业中一个长期存在的挑战：分散的日志管理需要多个工具和数据副本，每一个都会增加成本和复杂性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次更新的关键创新是&lt;a href=&quot;https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/s3-tables-integration.html&quot;&gt;通过Amazon S3表以兼容Apache Iceberg的方式访问日志数据&lt;/a&gt;&quot;，使组织能够在不使用ETL管道的情况下就地查询日志，同时保持与第三方分析工具的兼容性。这种方法，结合对&lt;a href=&quot;https://docs.aws.amazon.com/security-lake/latest/userguide/open-cybersecurity-schema-framework.html&quot;&gt;Open Cybersecurity Schema Framework&lt;/a&gt;&quot;（OCSF）和&lt;a href=&quot;https://opentelemetry.io/&quot;&gt;Open Telemetry&lt;/a&gt;&quot;（OTel）标准的原生支持，使CloudWatch成为像Splunk和Datadog这样的成熟的可观测性平台的潜在替代品（至少对于以亚马逊云科技云为中心的组织来说是这样）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudWatch现已原生支持跨账户和区域聚合预置日志，通过AWS Organizations整合AWS CloudTrail、Amazon VPC Flow Logs和AWS WAF访问日志等。此外，该服务还支持第三方数据源，包括：CrowdStrike、Okta、Wiz、Zscaler、Microsoft Office 365以及ServiceNow CMDB。CloudWatch为各类数据源提供了托管的OCSF转换服务，并&lt;a href=&quot;https://zerogrok.com/grok-patterns/&quot;&gt;通过Grok实现自定义解析与字段级操作&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudWatch将日志管理简化为一个内置治理功能的单一服务，消除了在不同工具中复制多个数据副本的需求。统一数据存储减少了ETL管道的复杂性，降低了运营成本和管理开销。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以在CloudWatch中使用自然语言或流行的查询语言，如LogSQL、PPL和SQL，通过单个界面运行查询。此外，他们可以通过兼容Apache Iceberg的表，使用自己喜欢的分析工具查询数据。新版&lt;a href=&quot;https://davidmorrill.github.io/facets/facets_ui.html&quot;&gt;Facets界面&lt;/a&gt;&quot;允许通过源、应用程序、账户、区域和日志类型简便地进行筛选，并通过智能参数推断实现跨账户和跨区域查询。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a6/a681e0f444f22651035122687c0d3965.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：&lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-cloudwatch-introduces-unified-data-management-and-analytics-for-operations-security-and-compliance/&quot;&gt;亚马逊云科技新闻博客&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上的一篇&lt;a href=&quot;https://www.linkedin.com/posts/suresh-rajashekaraiah_amazon-cloudwatch-just-changed-the-game-activity-7404201064591167490-Owc5&quot;&gt;博文&lt;/a&gt;&quot;中，Mphasis架构师Suresh Rajashekaraiah指出，多年来企业一直苦于运营日志与安全日志存储分散的问题，这使得故障排查和合规流程变得复杂。然而，通过增强Amazon CloudWatch，这个问题得到了解决。该服务提供了一个统一的日志平台，整合并规范了来自其云服务和第三方的数据，支持更高效的查询。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，Corey Quinn通过他的AWS Snarkbot在Bluesky上发了这样一个&lt;a href=&quot;https://bsky.app/profile/aws-snarkbot.lastweekinaws.com/post/3m6zgsivyvx22&quot;&gt;帖子&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;CloudWatch如今做到了Splunk十五年前做的事，但每句话里云服务名称的数量远超实际内容。“统一数据存储”=S3加上额外的步骤和咨询账单。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然Splunk提供了跨Azure、GCP和本地环境等平台的可见性，但亚马逊云科技押注其原生集成和“零ETL”成本，这可以为他们赢得以亚马逊云科技云为中心的组织的青睐。此外，虽然像Datadog和Dynatrace这样的竞争对手提供了深入的应用性能监控和混合云UI，但它们产生的出口和索引费用通常高于亚马逊云科技采用的S3表“就地查询”模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开源替代方案，如ELK技术栈（Elasticsearch、Logstash、Kibana）和Grafana Loki，提供了供应商无关的统一日志管理，并且由社区驱动创新，不过它们需要组织管理自己的基础设施和运营复杂性。CloudWatch的托管服务方法消除了这种运营负担，但会将组织更紧密地绑定到亚马逊云科技的生态系统，对于寻求多云灵活性的团队来说，这会带来供应商锁定的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，除了AWS GovCloud（美国）区域和中国区域，Amazon CloudWatch的增强功能在所有AWS区域都已提供。要了解Amazon CloudWatch的定价详情，可查看&lt;a href=&quot;https://aws.amazon.com/cloudwatch/pricing&quot;&gt;定价页面&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-cloudwatch-unified-logs/&quot;&gt;https://www.infoq.com/news/2026/01/aws-cloudwatch-unified-logs/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/siglzhFYdV4bfKPfplrj</link><guid isPermaLink="false">https://www.infoq.cn/article/siglzhFYdV4bfKPfplrj</guid><pubDate>Fri, 16 Jan 2026 07:03:00 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>框架</category></item><item><title>首个智能体商业信任协议来了！支付宝携手千问App、淘宝闪购等发布AI商业协议ACT</title><description>&lt;p&gt;1月16日，支付宝联合千问App、淘宝闪购、Rokid、大麦、阿里云百炼等伙伴，正式发布ACT协议（Agentic Commerce Trust Protocol，智能体商业信任协议）。这是中国首个面向Agent 商业需求设计的开放技术协议框架，为 AI 与电商、外卖等服务平台的协同打造一套 “通用语言”，让跨终端、跨系统、跨平台的 AI 任务执行，变得更便捷、更高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e0d550b6327306c0a821430007191f9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以千问App为例，依托 ACT协议 ，千问App成功打通淘宝闪购与支付宝 AI 付：用户只需向千问发出指令 “帮我点杯珍珠奶茶”，千问基于用户地理位置，智能推荐附近符合需求的商品，同步完成比价与优惠券自动核销。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户仅需点击 “选它”，确认支付宝付款，即可一键完成结账。整个购物流程以对话式、自动化、不跳端的方式推进，千问化身专属 “购物助手”，包办繁琐操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当AI 的能力边界不断拓展，从“聊天对话”延伸至购物付款等“办事时代”，新的问题也随之浮现：AI 操作是否获得用户明确授权？资金交易过程是否足够安全？更换设备或应用后，服务体验能否保持连贯？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ACT 协议的诞生正是为破解这些问题而来。支付宝为其搭建了 “委托授权域”“商业交互域”“支付服务域”“信任服务域” 四大核心基础设施标准，实现 AI 操作全流程可追溯、可验证，让人更放心；支持自动化交易流程，减少不必要的人工干预，提升服务效率；统一多平台服务标准，避免体验的割裂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9df1ea4382f50cb3b8946230068cb42.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统付款模式不同，在ACT协议的规则框架下，AI 仅承担下单操作的执行角色，付款环节始终由用户主导或自主授权。在保障资金安全的前提下，为用户大幅节省时间成本。而对商家而言，未来接入 AI 原生应用时，只需按照协议标准配置统一接口，即可对接全渠道入口，无需单独进行复杂的 API 开发，大幅降低对接成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，ACT 协议可使用在AI 代买、企业自动化采购等多元场景，并提供两种付款模式：一是即时付款，用户与 AI 实时对话，基于推荐列表自主决策，确认后完成付款授权与身份验证，适用于 AI 点外卖、日常购物等高频场景；二是委托授权，用户可提前设定时间窗口、金额上限、商家范围等条件，即便离线无指令，AI 也能自动监测商品动态并完成下单结算，适用于机票、酒店预订等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该协议最大限度遵循兼容性、隐私性、开放性三大原则，全面适配现有商业与支付系统，并将伴随AI 行业技术发展持续优化。支付宝同时表示，正积极推动更多支付服务商、商家与平台、AI 开发者、智能终端生态厂商加入，共同完善协议内容，共建 AI 商业信任新生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着AI 原生应用能力的持续升级，“AI 代办” 服务日渐普及，支付作为其中特殊且关键的环节，正成为全球科技企业的布局焦点。此前，OpenAI 联合 Stripe 推出协议以支持 ChatGPT 结账功能；近期，谷歌也发布 AI 购物全流程通用商务协议（Universal Commerce Protocol，简称 UCP），将实现用户在 Gemini 内直接下单。&lt;/p&gt;</description><link>https://www.infoq.cn/article/DE0ifdKyd4Oevz5Bgr3X</link><guid isPermaLink="false">https://www.infoq.cn/article/DE0ifdKyd4Oevz5Bgr3X</guid><pubDate>Fri, 16 Jan 2026 03:11:39 GMT</pubDate><author>李冬梅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>谷歌发布 Gemma Scope 2，深化对 LLM 行为的理解</title><description>&lt;p&gt;&lt;a href=&quot;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&quot;&gt;Gemma Scope 2 是一套旨在解释 Gemini 3 模型行为的工具&lt;/a&gt;&quot;，使研究人员能够分析模型的突发行为，审核和调试 AI 代理，并针对越狱、幻觉和阿谀奉承等安全问题制定缓解策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;可解释性研究旨在理解 AI 模型的内部工作机制和学习算法。随着 AI 变得越来越强大和复杂，可解释性对于构建安全可靠的 AI 至关重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌将 Gemma Scope 描述为大型语言模型（LLM）显微镜。它结合了稀疏自编码器（SAEs）和转码器，让研究人员能够检查模型的内部表示，查看它“思考”的内容，并理解这些内部状态如何塑造了其行为。一个关键的应用场景是检查模型输出与其内部状态之间的差异，按照谷歌的说法，这可能有助于发现安全风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gemma Scope 2 针对 Gemma 2 模型家族从多个方面扩展了原先的 Gemma Scope。最值得注意的是，它在 Gemini 3 模型的每一层中重新训练了其 SAEs 和转码器，包括 &lt;a href=&quot;https://arxiv.org/abs/2501.18823&quot;&gt;kip-transcoders&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot;&gt;cross-layer transcoders&lt;/a&gt;&quot; 。这些转码器旨在使多步计算和分布式算法更容易解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/Gemma_Scope_2_Technical_Paper.pdf&quot;&gt;解释说&lt;/a&gt;&quot;，增加层数直接增加了计算和内存需求。为了保持复杂性随层数线性增长，这需要设计专门的稀疏内核。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，谷歌采用了&lt;a href=&quot;https://arxiv.org/abs/2503.17547&quot;&gt;一种更先进的训练技术&lt;/a&gt;&quot;，使 Gemma Scope 2 有更强的能力来识别更有用的概念，同时也解决了初版实现中已知的几个缺陷。最后，Gemma Scope 2 引入了专门针对聊天机器人进行分析的工具，使研究人员能够研究复杂的多步行为，如越狱、拒绝机制和思维链忠实度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;稀疏自编码器使用一对编码器和解码器函数来分解和重建所有 LLM 输入。另一方面，经过训练后，转码器能够稀疏重建多层感知器（MLP）子层的计算过程，即学习如何对给定输入进行输出近似。这使其能够识别各层及子层中哪些部分（更精确地说是哪些激活模式）是由单输入令牌或令牌序列触发的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了应用于安全领域外，&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1eh4wja/comment/lfykb9o/&quot;&gt;Reddit 用户 Mescalian 预测&lt;/a&gt;&quot;，这项研究还可以：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;指导其他领域的最佳实践，未来可能会被用来监控智能程度更高的 AI 的内部推理。不过目前，它最适用于通过对权重进行微调及其他修改来调整模型能力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与谷歌类似，&lt;a href=&quot;https://www.infoq.com/news/2025/04/anthropic-ai-microscope/&quot;&gt;Anthropic&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://www.infoq.com/news/2020/04/open-ai-microscope/&quot;&gt;OpenAI&lt;/a&gt;&quot; 也针对他们的模型发布了自己的“ AI 显微镜”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌已在 Hugging Face 上&lt;a href=&quot;https://huggingface.co/google/gemma-scope-2&quot;&gt;发布&lt;/a&gt;&quot;了 Gemma Scope 2 的权重。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-gemma-scope-2/&quot;&gt;https://www.infoq.com/news/2026/01/google-gemma-scope-2/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QAVo2JuWaAElyqUA6joF</link><guid isPermaLink="false">https://www.infoq.cn/article/QAVo2JuWaAElyqUA6joF</guid><pubDate>Fri, 16 Jan 2026 02:36:30 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>FACTS基准测试套件问世，用于评估大型语言模型的事实准确性</title><description>&lt;p&gt;&lt;a href=&quot;https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/?utm_source=ALL&amp;amp;utm_medium=social&amp;amp;utm_campaign=&amp;amp;utm_content=&quot;&gt;FACTS基准测试套件&lt;/a&gt;&quot;发布，这是一个旨在系统性评估大型语言模型事实准确性的全新行业基准。该套件由FACTS团队与Kaggle联合开发，扩展了早期事实基础研究相关的工作，并引入了一个更广泛的多维度框架，用于衡量语言模型在不同使用场景下产生事实正确响应的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FACTS基准测试套件基于原先的FACTS Grounding Benchmark，并增加了三个新基准：参数化（Parametric）、搜索（Search）和多模态（Multimodal）。结合更新后的Grounding Benchmark v2，该套件可以从反映现实世界常见模型使用场景的四个维度评估事实性。该基准测试总共包括3513个精选示例，分为公共和私有评估集两部分。Kaggle负责管理保留的私有数据集，评估参赛模型，并通过公开排行榜发布结果。总体性能以FACTS评分的形式呈现。该分值是通过所有基准测试以及两部分数据集的平均准确率计算得出的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参数化基准测试侧重于模型仅凭内部知识（无需外部工具）回答基于事实的问题的能力。问题形式类似于常见的知识问答题，通常可通过维基百科等来源找到答案。搜索基准测试评估模型能否通过标准的Web搜索工具准确地检索并整合信息，通常需要多步检索才能完成单个查询。多模态基准测试在回答图像相关的问题时检验事实准确性，需要结合背景知识进行正确的视觉解读。更新后的Grounding Benchmark v2评估响应是否基于提供的上下文信息进行了合理推演。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初步结果既凸显了进展，也揭示了接下来要面对的挑战。在评估的模型中，Gemini 3 Pro以68.8%的总体FACTS评分位居首位，其参数化事实性与搜索事实性较前代模型均有显著提升。然而，评估的所有模型总体准确率均未突破70%，多模态事实性成为各模型普遍面临的难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e7c0336b403b9988fa4cff728f7bc8f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：谷歌DeepMind博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基准测试的结构引起了从业者的关注。资深iOS工程师Alexey Marinin在评论此次发布时&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7404597456069369856?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7404597456069369856%2C7404784965244301312%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287404784965244301312%2Curn%3Ali%3Aactivity%3A7404597456069369856%29&quot;&gt;指出&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这种四维视角（知识、Web、基础、多模态）感觉更接近人们日常实际使用这些模型的方式。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;FACTS团队表示，该基准旨在支持正在进行的研究，而不是作为模型质量的最终衡量标准。通过公开数据集并规范评估标准，该项目旨在为衡量语言模型的事实可靠性提供一个共同的基准，以适应其持续演进的发展需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/facts-benchmark-suite/&quot;&gt;https://www.infoq.com/news/2026/01/facts-benchmark-suite/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zQZlFUuAIBS7HfoIK2CD</link><guid isPermaLink="false">https://www.infoq.cn/article/zQZlFUuAIBS7HfoIK2CD</guid><pubDate>Fri, 16 Jan 2026 01:53:49 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>全靠Claude Code 10天赶工上线，Cowork 删用户11G文件不含糊！核心研发：长时间打磨再发布很难成功</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 发布 Claude Cowork 研究预览版没多久，就被曝出了删用户文件、窃取文件等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，博主James McAulay在测试Cowork 功能中，选择“整理文件夹”这一基础且高频的场景，同时还与Claude Code进行对比。当James正在对比两款工具的整理进度时，Claude Cowork 突然触发了致命错误：在整理过程中擅自删除了约11GB文件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更令人崩溃的是，这些文件并未进入回收站，而是被执行了“rm -rf”不可逆删除命令。James紧急让 Claude Cowork 导出操作日志，确认该命令的执行记录后，咨询 Claude Code 能否恢复，得到的却是“无法恢复，属于致命操作”的回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事后复盘发现，James在 Claude Cowork 询问文件操作权限时，点击了“全部允许”或“始终允许”，但没有预料到它会无视明确的“保留文件”指令，更没想到会执行不可逆删除操作。万幸的是，此次被删除的均为过往上传记录，并非核心重要文件，未造成严重损失，但这一安全隐患足以让用户对其望而却步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e1b1ebaee3574961ce9b86f7d2044b21.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;James还指出，Cowork 与Claude Code相比，存在两点不足：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是交互的繁琐性。发出“整理文件夹”的指令后，Claude Cowork 并未直接行动，而是要求先启动新任务并手动选择目标文件夹；Claude Code则直接定位文件夹并开始分析，仅需授予一次权限即可推进。Claude Cowork 通过反复交互确认整理细节，比如询问“文件按什么维度分类”“用户数据文件夹如何处理”，即便明确回复“用户数据文件夹暂不删除、保留”，它仍在待办清单中标记“删除用户数据文件夹：已完成”，虽后续未实际执行该删除操作，但也暴露了指令响应的漏洞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是效率的滞后性。整理过程中，Claude Cowork 运行命令多次停顿，节奏拖沓；而同期用 Claude Code 整理“音乐文件夹”，智能体快速给出“专辑和迷你专辑、单曲、Demo、翻唱”的分类建议，确认后即刻推进整理，全程仅需数十秒。即便两者均搭载 Opus 4.5 模型，Claude Cowork 的响应速度和执行效率仍明显落后，甚至让简单的文件夹整理变成了“持久战”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，AI 安全公司PromptArmor还发现，由于 Claude 代码执行环境中存在已知但未解决的隔离缺陷，Claude Cowork 易受通过间接提示注入实施的文件窃取攻击。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，这是一个最早由 Johann Rehberger 在 Cowork 尚未出现之前、于 Claude.ai 聊天环境中发现的漏洞，已经扩展到 Cowork中。Anthropic 对该漏洞进行了确认，但并未进行修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 提醒用户：“Cowork 是一个研究预览版，由于其agentic的特性以及可访问互联网，存在独特风险。”官方建议用户警惕“可能表明存在提示注入的可疑行为”。然而，由于该功能面向的是普通大众而非仅限技术用户，PromptArmor表示认同 Simon Willison 的观点：“要求普通、非程序员用户去警惕‘可能表明提示注入的可疑行为’，这是不公平的！”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d221638382f4de83263c0417be5d498b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此前，Every 团队提前获得权限，Dan Shipper、Kieran Klaassen 直播测试了该产品并分享了使用体验。期间，Anthropic Claude Cowork 项目核心成员 Felix Rieseberg 参与解读了产品设计思路。Felix 介绍，Cowork 是一个快速上线、先交给大家看怎么应用的产品，只用了 1.5 周就完成了开发，Felix 表示未来将以用户反馈为核心快速迭代。此外，工程师 Boris Cherny 还在X 上透露，该产品的全部代码都是由 Claude Code 编写的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在直播中，Felix表示，产品工作流可拆分为 “非确定性（依赖模型智能）” 和 “稳定可重复（编写工具）” 两类，按需取舍。Skills 是平衡 “模型灵活性” 与 “工作流稳定性” 的关键，能沉淀可复用知识，还能催生涌现能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，未来 Agent 类应用界面会趋简，用统一的 “泛化入口” 覆盖更多场景，而非专用化输入框堆砌。下面是三人对话部分内容，我们进行了翻译，并且在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一周半冲刺、先上线再说&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这是我们团队做的产品。我们在最近大概一周半的时间里全力冲刺，把它做出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：一周半？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：对，不过我想澄清一下：其实很多人早就有一个共识：如果能有一个“给非程序员用的 Claude Code”，那一定会非常有帮助、也很有价值。我们真正想做的，是帮助人把事情做完，不管是生活里还是公司工作中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这之前，我们其实已经做过好几个原型，尤其是在圣诞节前。但假期期间我们观察到一件事，我相信很多人也注意到了：越来越多的人开始用 Claude Code 做几乎所有事情，某种程度上，大家是在用它“自动化自己的人生”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们就在想：有没有一个足够小、足够早期的形态，可以先做出来给大家用，然后和用户一起快速迭代，真正搞清楚什么样的用户体验才是对的、我们到底应该构建什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在你们看到的这个就是答案。它是一个 research preview，非常早期的 alpha 版本，有很多不完善的地方、很多毛糙的边角，你们已经看到不少了，这些我们都会很快改进。但这就是我们的尝试：在开放状态下构建产品，和外部的人一起打磨。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我太喜欢这种方式了，能不能讲讲你们做的一些设计决策？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这是个很好的问题。我个人有一个判断：不只是 Anthropic，而是整个 Agent 类应用的用户界面，在接下来一两年里都会发生非常大的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在我们看到的，是为不同任务设计的高度专用化输入框，以及围绕特定任务搭出来的一整套脚手架。但随着模型能力不断提升、整个行业对“泛化问题”的理解逐渐加深，我认为未来我们会用更少的界面，覆盖更广的使用场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在当下，我们之所以把 Cowork 单独拆出来，是因为我们想非常透明地告诉用户：这是一个“施工中的区域”。某种意义上，我们是在邀请你走进我们的厨房。我们希望能和用户一起工作，几乎每天都上线新功能、修 bug、尝试新想法。所以这个独立的 Tab 本身就是实验性的，可以说是在前沿、甚至是“流血边缘”。它节奏更快、打磨得没那么精致，这也是我们把它单独拎出来的主要原因之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，也有一些技术层面的原因。比如现在这个 Cowork 是运行在你本地电脑上的，所以里面的对话是本地的，不会在多设备之间同步。同时，我们给了 Claude 更激进的一些 Agent 能力。综合这些因素，才决定做成现在这个形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：同一个应用里，一边是云端的聊天，一边却是在自己电脑上跑的 Agent。怎么让用户真正理解“这两者不一样”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：是的，我心里有一个梦想，我相信很多人也有同样的想法：最终这些其实都不重要，代码到底跑在什么地方，应该只是一个技术实现细节。对用户来说，它应该就跟你访问纽约时报网站时会不会用 WebSocket 一样，谁会在乎呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我们来说，现阶段这样做的好处是，可以跑得更快、发布得更快，也能和真正使用这个产品的人更近距离地一起共创。我一直很坚定地认为，一个人关起门来是很难做出好产品的。那种“躲进山洞里干一年，最后拿出来”的方式，其实很难成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也经常提醒大家：就连第一代 iPhone，都缺了很多我们现在觉得是“理所当然”的功能。所以，这确实是一个不小的门槛，但我们暂时可以接受，因为我们希望现在选择用这个产品的人，本身就是带着明确意图来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我觉得这是一个非常有意思的模式，先极快地把东西做出来，以一个“新入口”的形式放在应用里，让相对更少的人点进来。这样就能在真实世界里快速迭代，而不是一开始就追求完美。尤其是在你刚才说一周半就能做出一个版本，简直疯狂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“现在的状态是，先看看大家怎么用”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：但在你们脑海里，这个产品“真正的形态”是什么样的？你们接下来想往哪里走？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我太喜欢这个问题了，因为说实话，我也想反过来问你们两个同样的问题：你们希望它变成什么？你们想用它做什么？我已经听你们提到过，比如想让它能访问整台电脑，还有多选交互是不是可以更灵活一些之类的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我现在更多的状态是，先看看大家怎么用，然后疯狂尝试各种可能性。里面肯定有很多是错的，也会有一些是对的。对我来说，真正有意思的不是我个人的愿景，而是用户真正想拿它干什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我过去做过的产品几乎都是这样：你心里以为用户会这么用，结果他们找到了完全不同的用法，然后你顺着那个方向继续做下去。所以我特别希望我们能搞清楚：人们现在到底想要什么、喜欢什么、不喜欢什么。肯定也会有人明确说不喜欢某些地方，那我们就根据这些反馈不断调整、迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：这又回到一个老问题了。比如 Boris 就非常擅长把 Claude Code 做成一种让用户在使用过程中逐渐发现“自己到底想要什么”的工具。那你们在 Cowork 里有没有类似的策略？比如给我们一些“积木式”的东西？能不能加自己的插件或 Skills？Claude Code 很酷的一个地方在于它特别好 hack、特别可塑，你们面向非程序员的 Cowork 是不是也有类似理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：对，非常强调可组合性。你刚才提到 Boris 推动 Claude Code 早发布、快迭代、看用户怎么用，其实特别巧，我们之所以能这么快上线，很大程度上也是 Boris 在推动我说，“你应该早点给大家看看，看他们会怎么用”。（注：Boris Cherny&amp;nbsp;是Claude Code核心创作者）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于可组合这一点，过去几周、甚至最近两个月里，我自己感受最深的，是我越来越依赖 Skills。以前我可能会去写 MCP 工具，或者为 Claude 专门做一套很定制化的东西，现在我更多是直接写 Skills。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时候我还是会写一个二进制程序，但我随后就会在一个 Skill 文件里用 Markdown 描述：Claude，如果你要做这件事，请遵循这些规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我最近在给自己做一个马拉松训练计划。我写了一个小程序，从不同平台抓取我的运动数据；然后在一个 Skill 里写清楚：如果你要帮我做训练计划，请按这些原则来。现在，只要你在 Claude AI 里装过的 Skill，都会自动加载到 Cowork 里。而且我觉得这只会越来越重要，尤其是模型越来越聪明，比如Opus 4.5 版本，对 Skills 的遵循能力真的非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以目前来说，Skills 大概是我们最主要、也最“可 hack”的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;统一的“泛化入口”趋势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：太棒了。你刚才提到未来会有更少的 UI 形态。这是不是也意味着，围绕“聊天是不是 AI 的最终形态”这个争论，你其实是在押注自然语言会长期存在？也就是说，我们最终不会有越来越多复杂的 UI，而是更少的界面，人只需要和一个 Agent，或者一个能调度其他 Agent 的 Agent 对话？你们现在推动的方向，某种程度上是不是就类似今天 Claude Code 所展现出来的那种形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：是的，这个问题现在仍然存在很大的争论空间，而且肯定不存在什么“Anthropic 官方立场”。老实说，就算是在我这个并不算大的团队里，大家也未必能在整体上达成一致。每个人对于未来人类将如何与 AI、与模型交互，都有非常不同的想象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果只从我个人的角度来说，我大概坚信两件事。第一是：聊天式输入及其各种变体——不仅仅是模型意义上的聊天，而是更广义的那种“我想要点什么”的输入框——会比我们想象中存在得更久。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把它抽象开来看，不管是 Google 首页，还是 Chrome 的地址栏，本质上都是一个“我想要某样东西”的输入框，我认为这种形态会长期存在，我们会继续拥有某种看起来很像搜索框的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题是，我们到底需要多少个这样的输入框？你会有一个专门写代码的框吗？一个用于个人娱乐的、一个处理医疗相关问题的？我并不确定未来会存在这么多彼此割裂的输入框。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我再拿 Google 做类比。过去你可能记得，Google 会为不同需求提供不同的搜索入口和子产品。但现在，越来越多时候，你只是直接在 Chrome 的地址栏里输入你想要的东西。你不会真的先想清楚“我现在是在购物模式”，然后再专门去打开 Google Shopping。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，如果我们未来看不到一种更聪明的、能理解你想做什么的“泛化入口”，我会很意外。当然，后端可能仍然会分流，比如它理解你想要做的是 X，于是给你呈现一个适合 X 的界面，但入口本身很可能是统一的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;产品设计中的取舍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：我觉得一个很有意思的反例是 Microsoft Excel。某种程度上，它和 AI 的工作方式其实也很像：这是一个通用型产品，上手极其简单，但你可以在里面把事情做到无限复杂。而且，Excel 甚至某种程度上催生了后来的 B2B SaaS 浪潮，很多 SaaS 本质上就是把Excel 里的复杂工作流“产品化”了。所以也有另一种可能：你先有一个极其通用的工具，然后人们在里面发现了高价值、高强度的工作流，最后这些工作流再被拆分成独立产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我觉得 Excel 真的是一个极其漂亮的例子。对很多开发者来说，Excel 其实处在一个有点“边缘化”的位置，但如果你比较一下 Excel 的日活用户数量和全球开发者的数量，那是一个非常惊人的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 Excel 身上看到的一个很有意思的点是：它的重度用户，其实并不太在意那种“边际效率提升”，或者 UI 上一点点的小优化。他们更在意的是对这个产品的深度熟悉和肌肉记忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里面是有教训的。我在很多产品表面上都见过这种情况：作为开发者，你会觉得“如果我单独给你做一个更贴合这个场景的小工具，你的工作流会更好”。但结果往往是，用户并不会去用那个新工具，而是继续在他们已经非常熟悉的产品里，把事情做完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，这是我在 Slack 工作多年反复学到的一课：你可以做很多你自认为更适合某个使用场景的独立服务，但用户最后往往还是选择就在聊天里完成这件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：说到这里，虽然今天的主题更偏向非开发者，但我感觉现在有不少开发者在看。你正好是那种“真的把这个东西做出来了”的人，对 Agent native 应用的构建理解非常深。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们一直在思考 Agent-native 应用的核心原则。比如其中一个原则是“对等性（parity）”：用户通过 UI 能做的事情，agent 也应该能做。我在 Cowork 里已经能看到这一点。另一个是“粒度（granularity）”：工具应该尽量处在比功能更底层的层级，而“功能”更多存在于 prompt 或 Skill 中，这样你就能以开发者没预料到的方式去组合工具。这会自然带来第三个原则“可组合性（composability）”，而可组合性最终会产生第四个：涌现能力（emergent capability）。也就是用户开始用它做你完全没想到的事情，你看到了潜在需求，然后再围绕它构建产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在我看来，几乎就是 Claude Code 的工作方式。我很好奇，这一套在你听来是否成立？或者从你们在 Anthropic 大规模落地的经验来看，有没有什么能让大家把 Agent native 应用做得更好的建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这套说法对我来说非常有共鸣。而且我觉得，“涌现能力”里隐藏着一个非常重要的事实：无论是个人还是在孤立的小团队里，我们几乎不可能提前预测一个 Agent 最终会在哪些地方变得极其有用，尤其是当你只给了它一些相对原始的工具时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把工具尽可能下沉、做成通用形态，是一件非常强大的事情。工具越可组合、越通用，你就越能从模型智能的持续提升中获益。我和很多开发者聊过一个感受：模型智能提升、以及模型“正确调用工具”的能力，增长速度往往远快于你新增工具、或者教育用户理解这些工具的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以如果你退一步思考：“我能不能先做一个高度通用的工具？”那你构建出一个可以适应未来新场景的产品的概率，其实会大得多。这一点，我非常认同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那在这些原则之下，你怎么看其中的取舍？比如工具设计本身的权衡问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：对，我觉得把东西放进 prompt 里、再配合工具，本身是很棒的。但问题在于，我们现在突然需要去创建一些“能读取 Skills 的工具”，或者类似的东西。于是就出现了一个新的“元层”。Skills 本质上就像是一种即时的 prompt 注入，但你得先把这个体系搭出来。现在所有在做这些东西的人，如果不是直接用 Claude Code 或 Cloud SDK，那基本都得自己从头构建一整套。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是就出现了一种拉扯：你到底是把行为直接描述在一个 tool 里？还是再包一层 tool，让它去调用别的东西？这中间是有摩擦成本的。当然，可组合性是很好的。比如一开始你可能会有五个 tool：搜索邮件、读取邮件、做这个、做那个。但你也可以说：不，我只提供一个 execute tool，然后用 Skills、MCP，或者某种抽象层来完成这些事情。现在正处在这样一个转变期，而 Claude Code 和 Claude SDK 显然是在推动这个方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我确实能感受到这种摩擦。我猜你也一定感受到了。所以我很好奇：你有没有什么最佳实践，能给那些还停留在“传统 AI 应用思维”的人一些建议？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：我不确定我能给出什么“来自山顶的智慧”，会比你已经拥有的经验更有价值。但你说的那点，确实非常戳中我。我觉得你必须做一个取舍：哪些输出你愿意让它是非确定性的、哪些地方你愿意依赖模型的智能。而且一旦你依赖模型智能，每当你换一个更便宜、或者“更笨”的模型，那些地方的质量就会下降。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我会把整个工作流拆成两类：一类是非确定性的；一类是可重复、稳定的。如果某个部分非常可重复，而且你可以非常确信它“永远不会变”，而且就算模型变聪明了，你也得不到任何额外收益，那我会觉得，这正是写一个工具的好地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实我们已经在这么做了。你完全可以给 Claude 一个极其通用的“汇编级”工具，比如：“直接调用 GCC，你想怎么编就怎么编。”但我们并没有这么做，因为那样就太疯狂了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Skills 与可组合性实践&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那已经是粒度的极限了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：不过我也想说一句：当我和很多开发者聊的时候，我发现即便这个“是否要给模型工具”的基本假设，也正在被挑战。我不会把太多赌注压在这个假设上。比如，我们到底是不是还需要给 Claude 工具？还是说，某一天它只需要靠记忆和权重，直接把 0 和 1 写到世界里？这是一个非常有意思、也非常难判断的问题，没人真的知道答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你们已经在实践中学到了一些东西。你们之所以创造了 Skills，就是因为仅靠 Slash command 或子 Agent 已经不够了，对吧？我们需要 Claude.md 更强，但现实是 Skills 正是为了解决这个问题而诞生的，而且显然它们效果很好。我完全认同你说的，Skills 太棒了。我现在几乎每天都在写 Skills，而且真的很爱用。所以这里面一定有些什么。但问题是：什么时候应该用 Skill？什么时候又不该？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：这真的是一场特别有意思的对话。有一个你以后真的应该跟 Barry 聊聊。在公司内部，至少在某种程度上，Skills 这个概念就是他提出来的。从根本上说，Skills 正是你刚才描述的那种张力的自然产物。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我们想让公司内部的人能很容易地拿到各种仪表盘。我们用的是一家主流数据服务商，很多数据都在那儿。一开始我们在想：要不要做一堆非常具体的工具，专门去拉数据、压缩成固定格式。最早那几版仪表盘，其实效果并不理想（那还是 4.5 之前）。大概每三四个里面，就有一个看起来很拉胯。于是，我们开始想：要不要把参数卡死，直接做一个“固定模板”的仪表盘？Claude 只负责往里面填新数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这个过程中，我们突然发现了一件事：如果你只是告诉 Claude 如何正确地查询这个数据源、可以使用 SQL、以及生成仪表盘时需要遵循哪些设计原则，突然间，它就能稳定地产出质量很高的结果，而且是“几乎每一次”都很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，这就打开了“涌现能力”的大门。因为你还可以对 Claude 说：“我知道你在遵循这些仪表盘原则，但我想换一种图表类型”，或者“我想把它和另一份数据结合起来。”就在这一刻，事情真正开始变得有趣了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：这真的很有意思。我觉得为什么要用 Skill，而不是只给它 GCC、让一切都即兴发生，其中一个关键原因在于：你需要把一些可重复的、可分享的知识，变成一个大家都能讨论、都能复用的东西。并不是所有事情都应该是“即时生成”的。有些事情，你就是希望一个团队能长期、反复地用同一种方式来做。而这，本质上就是 Skill。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：而且这其实也很符合人类本身的工作方式，对吧？比如我刚加入一家公司时，总有人教我怎么订机票、怎么订会议室。从某种意义上说，我们每个人，都是靠着一堆 markdown 文件在工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得差不多该下线了，但在走之前，我想让你们两个各自给我一个建议：你们最希望我们改的一件事是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dan：那我先来一个最简单的：给我对整台电脑的完全访问权限。还有就是，让我更清楚地知道它现在到底是在我本地电脑上运行，还是在云端以聊天的形式运行；以及，让它在手机上用起来更顺畅。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kieran：我也支持移动端。但我最想要的是能让我添加自己的插件。我有一个插件市场，我只想把它接进来直接用。现在我得在一个应用里加东西，再拷贝到这里，有点绕。可能也能凑合用，但如果能原生支持插件市场、直接添加插件，那真的会非常棒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Felix：好，明白了。谢谢你们，这些反馈都非常有价值。我们会把这些带回去，跟团队一起讨论。也欢迎大家把想法发给我们。我们真的很希望听到大家的反馈，并据此调整路线图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;测试总结：理念可以，做得一般&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，我们总结了Every团队的测评结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude Cowork 的核心定位是为非技术用户提供 Claude Code 级别的 AI 协作能力，其最显著的突破在于重构了 AI 使用逻辑，从传统“发提示词→等回复”的一问一答模式，升级为“异步协作”模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与普通 Claude 聊天相比，Claude Cowork 专为“长时间工作”设计，具备持续推进任务直至完成的能力。直播中展示的典型案例包括：审计过去一个月的日历并分析与目标的匹配度、抓取 PostHog 数据统计按钮点击量、分析 Every 咨询业务的竞品、整理下载文件夹、校对 Google Docs 文案等。这些任务均需 AI 持续“浏览”、推理，部分任务耗时可达一小时左右，远超普通 AI 聊天的响应速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品的场景适配性极强，尤其适合需要深度研究和数据处理的岗位。用户只需连接 Chrome 浏览器，AI 即可直接使用用户已登录的各类服务，无需重复认证，轻松完成 Twitter 时间线热点分析、竞品信息搜集等需多平台联动的任务。同时，它支持生成文档、Excel、PPT、PDF 等多种产出物，可应用于简历优化、会议发言起草等日常工作场景，大幅提升增长团队、咨询人员、写作者等群体的工作效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在交互设计上，产品右侧设置了待办任务列表，清晰展示任务进度与当前阶段，用户可直观掌握 AI 工作状态。其“询问用户”功能还配备了可视化交互界面，支持多选项快速响应，进一步降低了操作门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据测评，Cowork具备较强的可扩展性，支持加载用户已安装的 Claude Skills，这也是其最具“可玩度”和“可定制性”的核心入口。用户可通过 Skills 封装专业知识与操作逻辑，实现个性化需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;测评团队也指出了产品当前存在的争议与不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最核心的争议在于“单独设置 Cowork 标签页”的设计：部分用户认为应在同一标签页内根据任务自动切换模式，避免额外的选择成本；但也有观点认为，独立标签页能明确提醒用户切换使用心态：从“实时对话”转向“异步托付”，尤其对非技术用户而言，这种明确的区分有助于适应全新的协作范式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外在体验细节上，产品仍有诸多优化空间：一是 UI 打磨不足，任务列表仅按时间排序，缺乏视觉区分度，部分内容存在“懒加载”导致展示不及时；二是权限管理不够直观，普通用户难以清晰判断 AI 是在本地还是云端运行，文件夹访问权限需手动配置易造成困惑；三是“询问用户”功能存在逻辑缺陷，可能在用户未响应时自动跳过问题，且选项数量和字符数存在限制；四是对复杂应用（如 Google Docs）的适配尚不完善，相关操作容易失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对不同用户，测评团队给出了针对性使用建议：非技术用户可将其视为“升级版聊天功能”，用日常任务直接尝试，逐步适应异步协作模式；重度用户可尝试通过 Skills 定制个性化功能，探索组合使用的可能性。他们表示，所有用户均需保持好奇心，忽略“三个月前 AI 做不到”的固有认知，在每一次产品更新后重新尝试核心需求，毕竟 AI 能力每隔几个月就会发生巨大迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终，测评团队给出的评分结论为：“理念绿牌，当前执行黄牌”。理念层面，产品开创性地将 Claude Code 级别的异步协作能力开放给非技术用户，推动了 AI 协作范式的转变，具备极高的探索价值；执行层面，因 UI 粗糙、部分功能逻辑不完善等问题，当前体验仍有较大优化空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_6C9nMvQsGU&quot;&gt;https://www.youtube.com/watch?v=_6C9nMvQsGU&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oPBN-QIfLaY&quot;&gt;https://www.youtube.com/watch?v=oPBN-QIfLaY&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files&quot;&gt;https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WDcTxr8g5TT064NaQyp3</link><guid isPermaLink="false">https://www.infoq.cn/article/WDcTxr8g5TT064NaQyp3</guid><pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖</title><description>&lt;p&gt;2026 年，AI 真正“下地干活”的第一战，被阿里打响了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1 月 15 日，在杭州阿里园区举行的千问 App 发布会上，阿里巴巴集团总裁吴嘉做了一次并不复杂、却很直观的演示：他用千问给现场嘉宾点了 40 杯“伯牙绝弦”奶茶。整个过程没有人工介入。千问自行匹配附近奶茶店，下单，并调用支付宝完成支付。没一会儿，淘宝闪购的骑手把奶茶送进会场。发布会的气氛，也在这一刻被彻底点燃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事后，有杭州的网友恍然大悟“怪不得刚刚西溪附近叫不动外卖！”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba72fd559beaf5ece58490ff5c1ae8b3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比 PPT 上的参数和模型指标，这个场景更容易被理解：AI 第一次在公开场合，完整地替人把一件现实中的事情办成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次更新中，阿里将千问定位成&amp;nbsp;“每个人的生活助手”。路径也很明确：不从新场景做起，而是直接接入阿里现有的业务体系，让 AI 先把眼前的事干好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;日常生活&amp;nbsp;层面，千问首批接入了&amp;nbsp;淘宝闪购、支付宝、淘宝、飞猪和高德&amp;nbsp;五大业务，可以一句话&amp;nbsp;点外卖、买东西、订机票、订酒店、查路线，这些原本需要在多个 App 之间来回切换的操作，现在可以交给一句话来完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6c/6c659e25b3b2f5fd57a8386e902d2613.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;“办事”&amp;nbsp;这一层，千问的能力被进一步拉长。它开始尝试处理更复杂的任务，比如打电话订餐厅、整理调研资料、处理财务文件、辅助搭建网站等。这类功能目前仍处于定向邀测阶段，&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吴嘉在发布会上表示：“AI 在拥有超强大脑之后，正在长出能够触达真实世界的手和脚，在生活中实实在在地替用户‘干活’。&amp;nbsp;千问的优势在于‘最强的 Qwen 模型’与‘阿里最完整的商业生态’的结合。AI 办事的时代才刚刚开始，我们会持续探索，把千问打造成真正有用的个人 AI 助手。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自千问上线两个月以来，月度活跃用户已突破 1 亿。&amp;nbsp;吴嘉认为，随着 AI coding、全模态理解以及超长上下文等关键能力逐步成熟，AI 正在走出手机屏幕，进入更复杂、也更真实的生产与生活场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把阿里折叠进千问中，&amp;nbsp;通过统一的 AI 入口，让千问拥有&amp;nbsp;400&amp;nbsp;余项办事能力，在&amp;nbsp;生活、办公、教育&amp;nbsp;等方面全场景覆盖，让千问成为 AI 时代的超级应用入口，这正是阿里的野心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;办事之上如何理解需求，才能判断是不是一个合格的助手&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随着模型能力的跃迁，思考让 Agent 做事，已经是近几年行业的集体共识。但&amp;nbsp;干的活好不好，这才是能否放心 AI 当助手的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里此次的更新方向，既在意料之中，又有些意料之外的惊喜，这个惊喜的落脚点就在于&amp;nbsp;对需求的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在对千问用户数据观察中，用户主动询问商品推荐的月环比高达 300%，这引起了阿里的注意，利用好千问与淘宝的链接，让千问拥有更可用的商品推荐能力，这确实踩中了不少人的真实需求，也成为千问区别其他通用 Agent 的功能独特切入点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d77d96dc23f662ce274acc00f1023a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅发挥了阿里在电商上的传统优势，也让庞大的商品供给和相对成熟的推荐体系真正被用起来。用户只需一句话，就能完成从商品推荐到下单的完整流程。其背后，是&amp;nbsp;阿里各业务接口的打通和协同调用，用起来足够顺，也足够省事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但更令人惊喜的是&amp;nbsp;对决策层面的关注，这也是&amp;nbsp;模型深入理解真实需求的表现，如何调用工具做更好的决策，体现了阿里强大的整合能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，现场展示了要给老人购买一款家庭扫地机，并且家里还养了一只猫，预算在 2000-4000 左右。千问在综合产品的价格与能力之上，还进一步老人的便捷需求与对猫毛的清洁效果，在综合这些复杂的条件后，给出推荐产品与相关理由，这正是大模型方便人类决策的一个虚拟需求感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a1/a141ee5ec5f8bdcb211d4a17eb9197e7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一个徒步推荐的方案中，千问不仅推荐出行路线，结合天气情况给出建议，还将徒步需要的产品直接发送到了千问界面上，确实让人看到 AI 未来融入世界的真实摸样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94c795db2602bc1789406c2e8d10bbac.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不是只做简单的一件事，而是将好多事做好，形成闭环，阿里已经迈出第一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者能想到的弊端，可能就是如何避免大模型被商家刷的假好评和广告垃圾数据污染，根据错误数据给出错误推荐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在一个全家人考虑去三亚出行的案例中，千问综合了路线、预算、老人与孩子的需求等，给出了路线选择，并给出三套酒店方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fe5cc6ca26ffb34e7e474aba35271405.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，酒店的均价都在两三千左右，不少人吐槽这恐怕没人住得起，方案不适用，不接地气，这或许是笔者认为的阿里迈出的是“半步”，还需要进一步的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现场还有一个小惊喜是，千问演示现场定饭店的时候，有一段与老板确定需求的打电话环节，从包间大小，价格，有小朋友等需求进行多方拉扯沟通，直到最后，电话结尾说，“我是千问 AI 助手在与你沟通”，大家才恍然大悟，原来是千问的语音功能在完成订酒店的“最后一公里”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是各种多模态打通后，AI 能做到的程度，留给人更多想象空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种好用，同时体现在在对办公需求上，在更专业的场景上，需要更好的交付结果，要求也更难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;千问可以集成各种复杂工具，完成做表格、整理数据、处理报表、汇报 PPT 等各种具体业务。从如何处理资料到最后成品展现，从效果来看，确实还不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07fa85f8f5b6844bf016d30842fb6e9a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次，阿里找来了专业人士来验收干活效果，千万财经博主小 Lin 说，亲自下场演示了用千问生成一份《2026 毕业生就业报告》，从信息汇总，消化资料，角度分析，文章演示到 PPT 的生成，千问干了一个完整的活。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，如果把千问当做个工作三年内的大学生，来干这些活，效果还是不错的，如果要求更高，可能就是把控 PPT 的内容重点质量，PPT 的设计是否美观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/40fb2d9103811e218d0cafb2f183e743.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在教育领域，千问也做出一些精心设计，令人印象深刻的是在各种题目中，除了思路的讲解，还会生成一段动态视频进行图示演说，能随时对话沟通，给出思路和解法，并且多模态展示，这让千问更像一个人一样解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/feaccba85e81a081dd48331badff811f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;笔者也亲自进行了一个上手测评，一个是用千问点奶茶，还有一个是用千问询问如何落户问题，千问都给出了较为实用的操作结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b1/b1c582f476f9719185aa2a395af5352d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总体来看，千问并没有试图一下子把所有事都做好，而是在尝试把复杂的事做得更完整、更贴近人的真实需求。它距离“完全可靠的 AI 助手”还有距离，但已经明显走出了聊天框，开始进入决策和执行的真实环节。而对干活质量的进一步打磨，恐怕正是阿里下一步要发力的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在几家最受关注的 AI 巨头中，字节跳动&amp;nbsp;选择从系统层切入，通过豆包手机助手借助操作系统能力，去调度第三方应用，与现实世界建立连接；阿里&amp;nbsp;的路线则更为直接，依托自身已高度成熟的电商、支付、物流、出行等业务体系，将这些能力整体接入千问，形成一个以自有生态为核心的闭环。腾讯&amp;nbsp;目前尚未对外展示完整方案，但从近期在 Agent 和多模态方向上的密集招聘来看，其下一步布局大概率仍将围绕微信这一超级入口展开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c08da37ff9047adf91ec2784c9765e8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;表面上看，Agent 之争比拼的是模型能力，但更深层的竞争，实际上取决于谁能更稳定、更规模化地承接真实世界的复杂需求。&lt;/p&gt;</description><link>https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</link><guid isPermaLink="false">https://www.infoq.cn/article/JrwSzMTgarBg4Ims5vmr</guid><pubDate>Thu, 15 Jan 2026 11:13:35 GMT</pubDate><author>高允毅</author><category>阿里巴巴</category><category>生成式 AI</category></item><item><title>“商业版 HTTP”来了：谷歌 CEO 劈柴官宣 UCP，Agent 直接下单，倒逼淘宝京东“拆家式重构”？</title><description>&lt;p&gt;谷歌把“Agent 购物”这件事，推到了一个更标准化的层面：Universal Commerce Protocol（UCP）正式亮相。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日（1月11日），谷歌 CEO Sundar Pichai（绰号“劈柴”） 首次登上 NRF（美国零售联合会年会），在题为“人工智能平台转型及零售业的未来机遇”的主题演讲中宣布了该协议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照谷歌的说法，UCP 是一项新的开放标准，目标是让 Agent 能够在线上直接买东西。在实现机制上，UCP 通过定义一组“代理商务的构建模块”，把端到端的购物流程拆解成可复用的能力组件：既覆盖推动商品发现与购买的关键动作，也延伸到下单后的体验与服务等环节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，这套设计将让生态系统在同一套标准下实现互操作，使任何 Agent 都能与任意商家进行对话，并自主完成从商品发现到结账的完整购物流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该标准采用 Apache 2.0 开源许可证发布：&lt;a href=&quot;https://github.com/Universal-Commerce-Protocol/ucp&quot;&gt;https://github.com/Universal-Commerce-Protocol/ucp&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8ac749381cca480abba528ff41699a7b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人一看到这条消息就意识到：大事可能真要来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;风险投资人 Linas Beliūnas 在LinkedIn 上评论称：“谷歌刚刚对‘商业’做了一件类似 HTTP 当年对 Web 所做的事情。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他看来，UCP 的野心，是把电商 20 年来那条固定链路，“搜索—广告—商品页—结账”——压缩成“意图—Agent 推理—购买”：用户不再需要点击跳转，不再被迫参与 SEO博弈，也不再被传统的转化漏斗一层层“导流”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;进一步说，Beliūnas 认为，UCP 试图成为商业领域的“HTTP”——也就是所有由 AI 介导的交易背后，那层看不见、但不可或缺的基础设施，“品牌不再争夺用户注意力，他们将竞相争取被Agent选中。网站变得可有可无。这就是非人类商业的开端。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2b1750b564c1fe87991e9db7109580b5.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期关注零售的连续创业者 Scott Wingo 甚至把谷歌这次在 NRF 上的一系列动作形容为一次“震撼与威慑（shock and awe）式”的进攻。他感叹自己在这个行业干了 30 年，“从来没见过现在这样的场面，真的太疯狂了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Wingo 看来，NRF 过去一直带着点“昏昏欲睡”的气质：讨论的多是收银系统、收银机、POS，以及超市自助结账的传送带这些传统议题。而如今，它几乎已经变成了一场围绕 Agent Commerce（智能体商业）展开的大会。“这种变化，是我做梦都想不到的。”他说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;统一零售界的新标准？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，UCP 到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单说，UCP 的目标是让 Agent 能够贯穿用户购买流程的各个环节：从商品发现、对比，到下单结账，再到购买后的支持服务，都可以在同一套标准下衔接起来。它想解决的核心问题是：用一个统一标准承载这些流程能力，而不是让商家和平台为不同 Agent、不同系统反复做一遍又一遍的对接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/affc2226740f2bd401efbcd843d05752.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从谷歌给出的设计图可以看到整体思路：左侧是各种消费者触点——消费者在这些地方与 Agentic Commerce 交互。在谷歌的世界里，这些包括 Google AI Mode、核心搜索、Gemini 等。右侧是后台系统——零售商后台需要的订单管理、库存管理等能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六项能力：产品发现、购物车、身份绑定、结账、订单，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中间是六个圆角矩形，其中三个是实线框，三个是虚线框。实线框的，是已经宣布、可用的能力。尚未上线的三项是：产品发现、购物车，以及其他垂直能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;围绕这六项能力，Scott Wingo 也给出了更具体的解读：&lt;/p&gt;&lt;p&gt;产品发现（Product Discovery）：目前官方并没有披露太多细节，但他判断，这很可能会与后续对 Google Shopping Feed 规范的扩展绑定在一起。未来 UCP 可能会提供类似“开关”的机制：商家可以决定哪些商品对 Agent 开放，Agent 也可以通过协议以不同方式拉取商品信息——某种程度上，这有点像 Stripe 的 Agentic Commerce 套件思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;购物车（Cart）：这是他认为“最值得盯”的部分。谷歌在图里用虚线框把它标出来，像是在释放一个强信号：UCP 可能要去挑战电商的“圣杯”——跨商家、多商品、由商家作为交易主体（merchant-of-record）的统一购物车。一句话：“一个购物车管全网”。他认为 ChatGPT/ACP 可能也有类似目标，但谷歌这次等于把这个方向直接摆到台面上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;身份绑定（Identity Linking）：他推测这会涉及“识别你的 Agent”（某种 know your agent 的机制）、银行卡 token 化等能力，类似 Link 或 ShopPay 那套：如果系统能把你的身份与支付凭据映射成 token，就有机会实现自动填充信用卡信息等体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结账（Checkout）：谷歌准备把 “Buy for Me” 做一次大升级——新结账入口将同时出现在搜索 AI Mode 和 Gemini 应用的符合条件商品页中，流程被压成三步“商品 → 确认订单 → 下单完成”，并将率先在美国上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;订单（Order）：一旦开始“在对话里结账”，就必须有一套双向的订单体验。一边是面向消费者：查看订单、取消、退货等；另一边是面向商家：拉取订单、处理履约、上传物流信息，并完成一整套购买后流程（退货、评价等）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他垂直能力（Other Vertical Capabilities）：这部分目前更像一个“兜底项”，官方也没有给出更多细节。他猜测它可能用于未来扩展到更多品类/行业，比如汽配、生鲜、B2B 等。当天新闻里被提到的客户之一是 Papa Johns（达美乐/披萨这种即时零售/本地履约场景），因此也不排除这块会成为一种“插件位”，让类似“ChatGPT App”式的体验从 UCP 的侧边接入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这些能力下方，还有三个模块，代表底层通信方式：API、MCP，以及 A2A。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌同时强调，UCP 并不是一套孤立协议，它可以与其他 Agent 协议协同使用，例如其在去年发布的 Agent Payments Protocol（AP2）、Agent2Agent（A2A） 以及 Model Context Protocol（MCP）。Agent 与商家可以根据自身需求，灵活选择和组合协议中的不同扩展模块。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中，MCP 更像是一个“工具与上下文协议”，用于让 Agent 安全、标准化地访问各类工具；A2A 是谷歌推出的多 Agent 通信协议，用来支持 Agent 之间的协作与任务分工； 而 AP2 是去年底发布的，聚焦在支付层，试图为 Agent 执行交易提供可验证、可授权的支付机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 UCP，看起来就是在这些协议之上的一次延伸，专门聚焦在零售这一层。可以说，谷歌这段时间在 Agent 协议这件事上确实是在“加班加点”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/243fc7c4415ac7a479eb2f2b3b8b95ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，谷歌并不是第一个做这件事的。OpenAI 的 Agent Commerce Protocol&lt;/p&gt;&lt;p&gt;几个月前，OpenAI 其实也推出过一个 Agent 商业相关的协议，主打“即时结账”，帮助 Agent 发现商品并完成购买。而谷歌的一个巨大优势在于：绝大多数零售商本来就非常熟悉谷歌——比如 AdWords、广告投放，以及一整套谷歌企业服务。谷歌正在尽可能地利用这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;UCP 真正要解决的问题：可发现性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 的核心想法，是用一套协议建立“通用兼容性”。商家只需要一次性把“我卖什么、我怎么卖”按标准描述清楚，理论上就能在不同平台、不同 Agent 之间通用。而它真正想啃下的硬骨头，是 “可发现性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这对传统零售网站而言，意味着一次不小的变革：页面不再是交易的唯一入口，商品数据本身开始成为入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，谷歌也在补“数据底座”。在扩展产品数据源部分，谷歌还在其 Merchant Seller 工具中为用户提供新的“数据属性”，以便品牌可以优化其产品列表，提升 AI 搜索排名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要知道，在 AI / LLM 时代，我们过去 20 年一直在为“关键词 + 四五个要点”优化商品页，但这恰恰是 AI 最不需要的东西。这些系统需要的是：内容爆炸 + 上下文，缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子：一个自行车脚踏。几乎所有线上商品都可以有 50–100 个属性：螺纹结构、反光片数量、材质、重量、兼容标准……这叫“内容”。而“上下文”是：它更适合山地还是公路？兼容哪些车型？能不能和某些配件一起用？内容和上下文就像阴与阳，缺了任何一边，Agent 都很难可靠地做判断、更难可靠地下单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去那套 Google 商品数据规范，更像一条长满杂草的碎石路；而 Agentic Commerce 需要的，是一条 30 车道的信息高速公路——是光纤，不是拨号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果谷歌继续用旧的商品 Feed 规范来做 Agentic Commerce，在发现环节一定会失败。Gemini 拿不到足够的信息。这次他们终于开始补这一块：新增描述性文本属性、产品规格、Q&amp;amp;A、评论、特性列表、形态、口味、主题、兼容性信息、推荐配件、替代品等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;官方说法是“新增数十个字段”。在 Scott Wingo 看来，这个数量大概会在 24–60 个之间；即便今天只先放出 20 个，也一定会很快扩展到 30、40 个——因为所有人都会意识到：这才是决定可发现性的关键。这些数据仍然通过 Merchant Center 上传，本质上可以理解为 Google Shopping Feed 2.0。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他对所有品牌和零售商的建议只有一句：尽可能“疯狂”地扩展你的商品级内容与上下文。这将直接决定你在 AI 时代能不能被 Agent 选中、能不能“占领 Buy Box”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谁站队了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP 在发布之初，就集结了科技与金融领域的一批重量级玩家，包括 Shopify、Walmart、Target、Etsy、Wayfair、Visa、Stripe、Adyen 等。首日即吸引了 20 多家合作伙伴加入，这正是标准胜出的典型路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/134c45edcab47ba358657acb357548a6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从已公开的信息来看，这些合作方大致可以分为两类：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一类是零售商与电商平台，包括 Etsy、Wayfair、Target、Best Buy、Macy’s、Kroger、Home Depot、Gap Inc.、Sephora、Ulta、Zalando、Chewy、Carrefour、Flipkart、Shopee 等；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类则是支付与清算体系，如 PayPal、Stripe、Adyen、Visa、Mastercard、American Express、Worldpay。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，有网友注意到，蚂蚁金服（ANT Financial） 也已经出现在 UCP 的合作名单中。有人评论称：“蚂蚁已经接入 UCP，但阿里巴巴推出自己的 Agentic Commerce 平台和 AI 协议，恐怕只是时间问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而从阿里最近的动作来看，这个判断并不突兀。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 15 日（今天），阿里千问 App 上线全新 AI Agent 能力“任务助理”，并打通淘宝、闪购、飞猪、高德与支付宝等应用：用户只需一句“我要两杯奶茶”，Agent 就能自动完成选店、选地址、选商品并生成订单，最后一步再由用户确认支付。延伸阅读：《&lt;a href=&quot;https://mp.weixin.qq.com/s/WXM2h4Z9DXrhVaoCjnt-ew&quot;&gt;刚刚，阿里园区被奶茶包围，都是千问点的！西溪叫不动外卖了&lt;/a&gt;&quot;》&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体看下来，一个趋势已经很难忽视：走到 2026 年，Agent 不再是大厂用来展示技术实力的“玩具”，而是开始被当成真正的赚钱工具。Agent 正在明显加速进入真实的应用场景，尤其是交易和服务这些最硬的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说得更激进一点：AI 很可能会把“社交 + 电商 + 服务”这套组合重新洗牌一遍。虽然“重做一遍”这个说法已经被用烂了，但眼下发生的变化，确实不像是在原有体系上打补丁，而更像是在重写入口、链路和分发规则——估计淘宝、京东这种级别的平台，迟早都得跟着重构一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且，这种变化最近已经变得非常明显了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&quot;&gt;https://blog.google/products/ads-commerce/agentic-commerce-ai-tools-protocol-retailers-platforms/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OXUn970YHVo&quot;&gt;https://www.youtube.com/watch?v=OXUn970YHVo&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&quot;&gt;https://www.finextra.com/pressarticle/108486/ant-international-embraces-googles-universal-commerce-protocol&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</link><guid isPermaLink="false">https://www.infoq.cn/article/5uNUBZjeBEhLY24tNdG9</guid><pubDate>Thu, 15 Jan 2026 11:09:21 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>谷歌推出Conductor：一款面向Gemini CLI的上下文驱动开发扩展</title><description>&lt;p&gt;谷歌发布了新的Gemini CLI预览扩展&lt;a href=&quot;https://developers.googleblog.com/conductor-introducing-context-driven-development-for-gemini-cli/&quot;&gt;Conductor&lt;/a&gt;&quot;，为AI辅助软件开发引入了结构化、上下文驱动的方法。该扩展旨在解决基于聊天的编码工具的一个常见限制：跨会话丢失项目上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor将开发上下文从瞬态会话中转移到直接存储在存储库中的持久Markdown文件中。这些文件定义了产品目标、架构约束、技术选择和工作流偏好，并作为开发人员和AI智能体的共享真相来源。其目的是使AI辅助开发随着时间的推移更加可预测、可审查和可重复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor鼓励的不是直接从提示到代码的转换，而是规划优先的工作流。开发人员在调用代码生成之前定义规范和实现计划，并且这些构件在特性的整个生命周期中仍然是代码库的一部分。这种方法旨在支持更大的任务，如特性开发、重构和在已建立的项目上工作，在这些任务中，理解现有的结构和约束是至关重要的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor的一个核心概念是轨迹，它代表了一个离散的工作单元。每个轨迹包括一个书面规范和一个面向任务的计划，该计划被分解为阶段和子任务。只有在计划被评审之后，实施才能继续进行，并在计划文件中直接跟踪进度。由于状态存储在存储库中，因此可以暂停、恢复或修改工作，而不会丢失上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期用户强调了基于轨迹的工作流，认为这是对临时提示的实际改进。Forrester的工程和产品负责人Devin Dickerson&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407465019967238146?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7407465019967238146%2C7408224250060378112%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287408224250060378112%2Curn%3Ali%3Aactivity%3A7407465019967238146%29&quot;&gt;说&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对于这个扩展我最喜欢的特性是轨迹的概念。在这次发布之前，我一直在使用自己构建的Conductor开源版本，我最终构建了自己的特性切片。现在轨迹已经内置了，我可以扔掉那个了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor还支持团队范围的配置。项目可以一次性定义共享标准配置，如测试策略、编码约定和工作流程偏好，并将它们一致地应用于所有AI辅助的贡献。这使得扩展不仅适用于个人开发人员，也适用于寻求跨贡献者和机器一致性的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;试用预览版的开发人员指出，它强调了明确的规划和测试驱动的工作流。Navid Farazmand&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7414757320267575297?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7414757320267575297%2C7415108568544247808%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287415108568544247808%2Curn%3Ali%3Aactivity%3A7414757320267575297%29&quot;&gt;描述道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当Gemini CLI发布时，我立即尝试用.md文件创建类似的东西。Conductor要好得多——特别是它采用的测试驱动开发方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Conductor是Gemini CLI的预览扩展，可以从其公共&lt;a href=&quot;https://github.com/gemini-cli-extensions/conductor&quot;&gt;GitHub&lt;/a&gt;&quot;仓库安装。谷歌将这次发布定位为初始步骤，随着开发人员和团队的反馈指导未来的迭代，计划进行进一步的改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-conductor/&quot;&gt;https://www.infoq.com/news/2026/01/google-conductor/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</link><guid isPermaLink="false">https://www.infoq.cn/article/RjIZD2dC9ZX3ROmzpsST</guid><pubDate>Thu, 15 Jan 2026 07:28:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>AI&amp;大模型</category><category>性能优化</category></item><item><title>手握30亿、被蚂蚁狂挖人，转型被骂惨的王小川，真的翻身了？</title><description>&lt;p&gt;在“大模型六小虎”成为历史后，王小川终于等来了自己的风口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，国内外大厂在医疗领域动作频繁。1月8日，OpenAI高调入局，除了推出ChatGPT Health，还收购了医疗保健初创公司Torch。几乎同期，Anthropic、英伟达、苹果等都有产品和合作发布。国内，蚂蚁阿福自发布后短期内月活用户突破3000万，单日提问量超千万。资本市场上，AI 医疗板块逆势走强，成为最近市场热点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此前大模型竞争激烈的当口，AI 医疗并不是一个很性感的话题。那种不信任来自百川内外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年成立的百川在一年后战略收缩，决定聚焦医疗，成为国内较早专注到医疗的大模型创企。但内部“没有足够传达在医疗上的决心和路径要求，没有让每个团队在医疗价值创造中深度思考why和how，进而导致部分团队工作目标出现了摇摆和偏差。”“去年中途转过来时被骂惨了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不只内部，业界对AI医疗也存有疑虑，连带着对百川的路线选择也有质疑。“2024年跟医生谈AI，大家都不信。”王小川直言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到2025年，大家看到DeepSeek真的比百度靠谱很多；年末阿福发布，投了10亿来砸广告，看到了技术和应用进展；今年1月8日，OpenAI Health 正式上线，Anthropic 也发布了自己的两个技术能力：医疗计算和Agent，两个巨头都开始进入医疗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“所以，从市场判断来看，医疗作为AI‘皇冠上的明珠’这样的高级阶段，已经开始进入应用范畴。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6cdda9230d6c5e944f95f7509736824.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从发布反思信至今9个月过去，王小川向 InfoQ 表示，百川如今的护城河主要有三个：一是模型结构的优先级，“医疗安全性”和“诊断准确性”始终是首位；二是切入点选择，百川聚焦严肃、高价的医疗场景，区别于其他企业的健康类打法，这类场景的壁垒更高，且有明确的付费意愿；三是产品形态的差异化，百川身份差异化服务和决策辅助能力，是现有产品不具备的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川尤其提到，大厂和创业公司不一样，他们有职业团队，需要的是更安稳的方案。“大创新靠小厂，小创新靠大厂，必须切入我们认为有高价值的事情，共识不是我们优先的突破点，而大厂更多的是注重共识，路线图和产品形态是不一样的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模型要低幻觉、能问诊，多模态非主战场&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“去年8月发布的M2作为百川重新聚焦医疗之后的主力模型，在行业得到很多好评。典型现象就是蚂蚁开始疯狂挖人，从技术人员到财务人员，所以属于小圈子认可技术路线图。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨天，百川正式开源了新一代医疗大模型 Baichuan-M3。据百川智能模型技术负责人鞠强介绍，Baichuan 系列采用SCAN框架，实现临床医生层级的推理与问诊。其核心在于不仅询问疾病类型，更通过定量问题将模糊主诉转化为可定位、可量化的临床证据；并且突破单一症状的局限，进行跨系统关联推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，团队高度重视并主动防控大模型在医疗中的“幻觉”，坚持正确知识并进行原子级事实检验：在模型推理过程中进行逐层事实核查，确保结论基于真实输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;鞠强介绍，在模型训练中，抑制“幻觉”与提升推理能力之间存在明显的“跷跷板效应”，容易陷入两种极端：若过度追求推理表现，其生成内容会更丰富、答对率上升，但幻觉也难以控制；若强力抑制幻觉，模型则会趋向过度保守，回答变得拘谨甚至回避问题，导致实用性下降。这也是团队在Baichuan-M3训练中重点攻克的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为破解这一矛盾，研发团队引入了 Fact-aware 强化学习技术。该技术核心在于，在强化训练过程中，既对幻觉进行充分压制，又确保推理能力不受损，反而同步提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果显示，相比前代模型M2，百川正式开源新一代医疗大模型 Baichuan-M3 的幻觉率大幅下降，同时在医疗专业评测HealthBench上的推理能力得分从34分显著提升至44分，位列榜首。在不依赖工具或检索增强的纯模型设置下，医疗幻觉率3.5，超越GPT-5.2。“这验证了我们通过强化学习方法，在抑制幻觉与增强推理之间取得了有效平衡。”鞠强表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e6/e66ada51aa81717429a27588ca063a4c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Hugging Face 地址：&lt;a href=&quot;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://huggingface.co/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/baichuan-inc/Baichuan-M3-235B&quot;&gt;https://github.com/baichuan-inc/Baichuan-M3-235B&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，模型深度集成的问诊能力，从日常症状中识别风险。团队设计了防御性思维追问，以甄别背后潜在的系统性疾病，还会进行组合症状敏锐识别，比如用户描述“情绪激动时左牙疼”时，模型能会关联“牙痛+情绪症状”，优先建议排查心脏系统问题，从而排除重大隐患，而非直接推荐牙医或止痛药。该能力已集成至产品，服务于医生与普通用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在AI医疗中，除了文字，还有影像等信息。不过，王小川认为，多模态并非当前AI主战场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他解释道，ChatGPT之所以令人震撼，正是因为它展现出一种“智力”，而智力的本质，是将具体事物进行抽象的能力，其核心在于符号系统。在这一逻辑下，智能主要依托于三种形式语言：自然语言、数学语言与代码语言。至今，评估一个模型能力的强弱，本质上仍是检验其符号处理与逻辑推理的水平，功能可用并不等同于智力高超。在医疗领域，这一观点尤为关键。医疗的核心是决策，而不仅仅是感知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，未来医学影像的初步解读可由专用小模型完成，许多厂商也已具备相应的图像引擎。但真正的价值在于：将影像符号化之后，如何用语言模型进行综合推理与判断。因此，感知模型与认知模型必须结合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，当前的一些工作，比如将CT影像转化为报告，或是专注于胰腺癌筛查的视觉模型，固然有其价值，但它们更像是“挂在智力之树上的叶子”，是整体流程中的一环，而非驱动智能演进的主战场。真正的突破，仍在于如何通过符号与语言，构建能够进行复杂医疗决策的认知核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“在中国To C比To B更好”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来巨大的增量是在院外，不在院内。”王小川说道。其核心是直接服务患者，而不是通过服务医生间接服务患者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;反观OpenAI的入局是靠打造“个人超级助手”，Anthropic则从合规性与临床效率上做B端突围。对此，王小川的评价是：“美国是To C和To B都可以干，但在中国To C比To B更好。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;王小川认为，国内的医疗现状是医生供给不足，互联网虽能连接信息却无法创造供给；医患权力不均，双方容易沟通不畅、患者无助；患者更倾向三甲医院，致使基层医疗薄弱；医疗知识分散于各科室，复杂病症往往缺乏整体视角。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于此，他的设想是AI 可以“造出高质量医生”，但不是要AI取代医生。“在某些维度上，AI超过医生是必然的，比如信息收集的完整性、医学知识的储备量、循证的精准度等。但AI不会取代医生的核心执行能力，比如手术、查体等。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在不取代医生的情况下，AI 可以推动“权力让渡”，即帮助患者理解病情与方案，获得更多参与权和知情权。另外，居家通过AI进行初步咨询，让“居家首诊”可能，减轻医疗系统负担。此外，复杂问题需要跨科室会诊，以前就是入院即入组，即进入某个科研队列，有了AI后能够做到“看病即入组”，更有机会做好生命模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在实现的产品形态上，百川目前主打还是百小应App，不过用户进入后可以选择医生和患者两种身份，给出的结果是不一样的：医生版更像OpenEvidence，答案更加专业、更加强调循证，引用的文章在系统中100%存在，让其能够做决策、信息够充分；患者版本则强调补充信息，进入启发式端到端的问诊，也给到患者决策能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们与OpenEvidence的区别在于，OpenEvidence只是服务于医生，百川是可复数、可懂、可决策、可行动、能够服务到患者的，这样的产品定位在全球是独一无二的。”王小川补充道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在其看来，做To C产品，重点是让产品价值触达真正的目标人群，即有严肃医疗需求、愿意为决策辅助付费的患者。他举例称，达摩院做的胰腺癌平扫CT模型，虽然技术门槛高，但解决了核心临床痛点，就有明确的付费方；而泛健康类服务看似覆盖广，但价值不突出，反而难以找到稳定的付费用户。百川目前的做法就是基本全覆盖，重点放在儿科、慢病和肿瘤，优先突破有明确痛点的领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;收费模式上，王小川认为，不是只赚医院或医生的钱，还可以向患者收费，也可以形成服务包，后面的医疗资源和药械以服务包形式收费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我倒不担心商业模式本身，确实要过了这个门槛、为用户创造价值，之后不管直接收费还是生态收费都是很容易的事情。”王小川说道。目前，百川账上还有 30 亿人民币，这也留给了王小川证明的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据王小川透露，今年上半年，百川会完成两款产品的发布和推广，核心是回归决策层面，帮助用户（包括患者和医生）做出更好的医疗决策，最终实现“医生时刻陪伴式”的健康管理。“我们第二个产品已经可以当成院外医生来看了。”此外，百川也有计划硬件产品发布和出海计划，具体日程未定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了培养用户心智，百川未来也会增加一定的广告宣传投入，另外会重视医生对产品的认可度。“阿福跟我们的路线不一样，老医生都是无感的。我们希望医生和患者一体两面，共享一款产品，要让专家点头，而不只是患者鼓掌。产品做好以后确实能够取得一定的口碑效应。”王小川说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“今年上市的两家主要还是踩在通用模型技术红利和政策支持的基础上，但目前他们的市值和商业化能力并不匹配，但AI医疗今天也是大模型竞争中的一个范式，虽然它的成熟会晚一点，在后面我们肯定也是奔着上市去的。”王小川给了自己两年的时间再看看。&lt;/p&gt;</description><link>https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</link><guid isPermaLink="false">https://www.infoq.cn/article/YK5s1sA4dEkP15fKNrkc</guid><pubDate>Thu, 15 Jan 2026 06:51:01 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>LangGrant推出LEDGE MCP服务器，赋能企业数据库启用代理式AI</title><description>&lt;p&gt;&lt;a href=&quot;https://www.langgrant.ai/&quot;&gt;LangGrant&lt;/a&gt;&quot;推出了LEDGE MCP服务器，这是一个新的企业平台，旨在让大语言模型在复杂的数据库环境中进行推理，而无需直接访问或暴露底层数据。该版本旨在消除组织在将代理式AI应用于受受控生产数据时面临的一些最大障碍，即安全限制、失控的token成本和不可靠的分析结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司表示，LEDGE MCP服务器允许LLM跨&lt;a href=&quot;https://www.oracle.com/&quot;&gt;Oracle&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;SQL Server&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;Postgres&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.snowflake.com/en/&quot;&gt;Snowflake&lt;/a&gt;&quot;,等数据库生成准确、可执行的多步骤分析计划，同时将数据完全保留在企业边界内。通过依赖模式、元数据和关系而不是原始记录，该平台消除了将大型数据集推送到LLM的需要，从而大大减少了token的使用并防止敏感数据泄漏。根据LangGrant的说法，通常需要数周手工编写查询和验证的任务现在可以在几分钟内完成，并具有完全的人工审查和可审计性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant首席执行官、首席技术官兼联合创始人&lt;a href=&quot;https://www.linkedin.com/in/rameshpar/&quot;&gt;Ramesh Parameswaran&lt;/a&gt;&quot;表示：“LEDGE MCP服务器消除了LLM和企业数据之间的摩擦。”他指出，企业现在可以安全、经济地将代理式AI直接应用于现有的数据库生态系统，而不会损害治理或监督。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在许多组织中，上下文工程和代理式AI正从实验阶段进入生产环境。许多企业已经接受了AI助手，但在操作数据库方面却停滞不前。安全策略通常禁止直接访问LLM，在分析原始数据时token和计算成本会激增，开发人员和业务用户都在努力应对企业模式的规模和复杂性。即使使用AI辅助编码工具，工程师也经常花费数周时间手动将部分上下文输入模型，以生成可用的查询和管道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LangGrant将LEDGE定位为一个全面解决这些问题的编排和治理层。MCP服务器管理LLM如何与企业数据交互，确保符合访问控制和策略。分析和推理使用数据库上下文而不是数据有效负载来执行，以降低成本并减少幻觉风险。该平台还可以自动创建可由人工团队检查、批准和执行的多阶段分析计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，LEDGE支持按需克隆和容器化类似生产的数据库，为智能体开发人员提供安全、隔离的环境来构建和测试AI工作流。通过跨异构系统自动映射模式和关系，该平台使LLM能够跨多个数据库进行推理，而无需读取底层数据本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了LEDGE MCP服务器，LangGrant认为企业对AI的采用将更少地依赖于更大的模型，而更多地依赖于安全的编排、治理和成本控制。该公司认为，通过保持数据原位，同时为LLM提供全面的上下文理解，企业最终可以准确、安全、大规模地将AI应用于其最有价值的数据资产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多公司正在采用MCP风格的服务器，在不暴露原始数据的情况下为AI智能体提供安全、结构化的环境，但它们的重点领域有所不同。 &lt;a href=&quot;https://www.infoq.com/news/2025/04/github-mcp-server-public-preview/&quot;&gt;GitHub&lt;/a&gt;&quot;的MCP服务器以开发人员的工作流程为中心，允许LLM在执行访问控制的同时对存储库、问题、拉取请求和CI元数据进行推理。同样，微软的&lt;a href=&quot;https://www.infoq.com/news/2025/07/azure-devops-mcp-server/&quot;&gt;Azure DevOps MCP&lt;/a&gt;&quot;向AI智能体公开结构化项目和管道上下文，以支持规划、故障排除和交付自动化，而不是深度分析数据处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了开发者平台，MCP概念也出现在基础设施和运营中。&lt;a href=&quot;https://www.infoq.com/news/2025/11/buoyant-linkerd-mcp-support/&quot;&gt;Linkerd&lt;/a&gt;&quot;等服务网格项目正在探索MCP集成，为AI智能体提供对服务流量、遥测和策略执行的安全可见性。云提供商还通过他们的AI服务（如&lt;a href=&quot;https://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt;&quot;和&lt;a href=&quot;https://cloud.google.com/?hl=en&quot;&gt;谷歌云&lt;/a&gt;&quot;）提供类似MCP的上下文层，这些服务允许智能体查询基础设施元数据和操作信号，而无需将敏感数据直接传递给模型。这些方法侧重于操作意识，而不是数据分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与这些产品相比，LangGrant的LEDGE MCP服务器以专注于企业数据库和分析而脱颖而出。总之，这些平台显示了MCP如何成为一种基础模式，每个实现都针对企业堆栈的特定层进行了定制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&quot;&gt;https://www.infoq.com/news/2026/01/langgrant-ledge-mcp-server/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</link><guid isPermaLink="false">https://www.infoq.cn/article/oNxDI4OWZTmvoXrZXMvA</guid><pubDate>Thu, 15 Jan 2026 06:42:00 GMT</pubDate><author>作者：Craig Risi</author><category>AI&amp;大模型</category><category>数据库</category></item><item><title>QCon 北京 2026 启动｜Agentic AI 时代的软件工程重塑</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年，我们分别在&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing&quot;&gt;北京&lt;/a&gt;&quot;、&lt;a href=&quot;https://qcon.infoq.cn/2025/shanghai/&quot;&gt;上海&lt;/a&gt;&quot;举办了两场 QCon 全球软件开发大会。过去一年里，我们和大量一线技术团队、工程负责人、开发者持续交流，感受到一个很明显的变化：大家讨论的重点，正在从“AI 能做什么”，转向“AI 怎么在生产系统里稳定运行、可控交付、持续产生价值”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不是热度退去，而是行业进入了更难、也更关键的阶段——从演示走向长期运行，从能力展示走向工程兑现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，越来越多团队开始回到同一个问题：当 AI 真正进入业务流程后，系统能不能“长期跑得住”？成本能不能算得清？质量、风险、合规能不能兜得住？组织的协作方式要不要跟着变？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，智能体（Agentic AI）&amp;nbsp;成为不少团队正在尝试的新方向：它不只是一次回答或一次推理，而是把感知、工具调用、任务执行、反馈迭代串成一个可运营的流程，逐步嵌入研发、交付与业务链路。可以预期，进入 2026 年，这类探索会从局部试点走向更体系化的工程建设：不仅是“加一个 AI 功能”，而是软件系统、研发流程乃至组织协作方式都要随之调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;软件工程正在发生的变化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们看到的变化不只是工具升级，更像是一套工程范式在被重写：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统架构开始围绕「智能体协作」重新设计工程方法论从「确定性流程」迈向「人机协同闭环」研发组织面临角色重塑与能力重构产品与交互从“界面驱动”走向“意图与行动驱动”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从基础设施、推理与知识体系，到研发与交付流程，再到前端、客户端与应用体验——AI 正在以更工程化的方式进入软件生产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;QCon 北京 2026 的核心主线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot;&amp;nbsp;将以&amp;nbsp;「Agentic AI 时代的软件工程重塑」&amp;nbsp;作为大会核心主线，把讨论从&amp;nbsp;「AI For What」，走向真正可持续的&amp;nbsp;「Value From AI」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一主线，我们将从六个关键维度系统性展开探索：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;前沿技术雷达（Future Tech）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来 1–2 年最值得提前布局的方向：Agentic AI 的新形态、下一代模型、交互范式与系统架构演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;架构设计与数据底座（系统可演进）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论如何构建可扩展、可演进、可复用的 AI 系统：Agent 架构、数据治理、知识体系与工程实践，回答“能不能长期跑”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;效能与成本（拒绝盲目烧钱）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注让 AI “跑得起、跑得快、跑得稳”的工程方法：在算力、推理、工程效率与 ROI 之间，寻找真正可持续的平衡点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;产品与交互（体验提升）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚焦前端、客户端与产品层的 AI 原生改造：人机协作、意图驱动交互、任务闭环体验，以及 Agent 参与下的产品新范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;可信落地（守住底线）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讨论 AI 带来的新风险。从 Demo 到 &amp;nbsp;Production 的“最后一公里”信任危机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;研发组织进化（长期主义）&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关注未来团队如何生存与进化：重塑研发角色分工、协作模式与工程文化，构建面向 AI 时代的组织能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;我们希望在 QCon 北京 2026 呈现的&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026 想呈现的，不只是“又一场关于 AI 的大会”，而是这轮变化真正落到工程与组织之后的全景：哪些方向已经走通，哪些正在付出真实成本，哪些系统必须被重构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，北京站部分专题已上线，我们期望持续挖掘来自一线生产环境的长期实践，呈现 Agentic AI 融入软件工程后的真实样貌——成功经验、工程妥协与关键取舍并存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/9d/9def3983a8e8e8f60f08f3560ccf39b9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多嘉宾邀请进行中&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也欢迎你带着真实问题与实践加入其中，与更多同行一起，把这场正在发生的软件工程重塑讲清楚、做扎实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;QCon 北京 2026，期待与你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📍&amp;nbsp;会议官网：&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;https://qcon.infoq.cn/2026/beijing/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;📩&amp;nbsp;演讲申请：&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;https://jinshuju.com/f/Cu32l5&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲评审标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;观点：是否清晰、有判断力，能否帮助听众形成有效认知实践：内容须来源于真实工程或业务实践深度：是否具备可复用的方法论或经验价值专业声誉：演讲者在相关领域的实践背景与影响力不做广告：QCon 不是厂商宣传舞台听众所得：听众能带走什么，是我们最关注的标准&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲嘉宾福利&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🎟&amp;nbsp;免费参会：自由参加大会全部课程💸&amp;nbsp;专属折扣：提供特别优惠码，方便同事与朋友购票📰&amp;nbsp;独家报道：有机会接受 InfoQ / 极客时间的深度采访🏨&amp;nbsp;免费住宿：为外地嘉宾提供酒店入住✈️&amp;nbsp;无忧差旅：承担嘉宾往返会场的交通费用&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/a4/a4b836a91798f84c96c46e2f4c7ec73a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</link><guid isPermaLink="false">https://www.infoq.cn/article/xDvcxhcebTPsfrciN2MA</guid><pubDate>Thu, 15 Jan 2026 03:17:38 GMT</pubDate><author>Kitty</author><category>AI&amp;大模型</category><category>软件工程</category></item><item><title>中了！极客时间入围中国移动培训服务一采供应商</title><description>&lt;p&gt;极客时间企业版（极客邦控股（北京）有限公司）成功入围中国移动 2026–2028 年培训服务集采项目，正式成为其一级供应商。在技术、市场及政企、培训资源开发三大标段中均取得优异成绩，彰显了公司在 IT 与数智化培训领域的深厚实力和生态优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;中标概览：三大赛道，全面突破&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在本次集采中的表现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版在“标包 7（培训资源开发）”中勇夺魁首，依托成熟的课程研发体系与知识产品化能力，彰显了其在高质量、体系化数智课程开发方面的硬核实力；在“标包 2（技术）”中位列三甲，体现了在 AI、云计算、大数据等前沿技术培训领域的扎实积淀；同时强势入围“标包 3（市场及政企）”，进一步验证了其助力企业业务增长与数智化转型的全面解决方案能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7f/7f41da54f421e75a37be20573c31cdde.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2ecdf1eff746c6b8b31dcf1de2c9e170.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;能力解读：“媒体+产品+生态”的复合优势&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;极客时间企业版之所以能快速响应不同标包的需求，根源在于公司长期以来打造的“内容+产品+生态”模式：&lt;/p&gt;&lt;p&gt;极客时间企业版则凭借培训平台与课程产品，将培训需求转化为可落地、可衡量的学习成果。InfoQ 极客传媒提供前瞻行业洞察，精准把握人才培养方向。TGO 鲲鹏会链接高端产业资源与实战智慧，构建协同发展的高管智库。&lt;/p&gt;&lt;p&gt;依托公司各业务板块的协同效应，极客时间企业版将持续为包括中国移动在内的广大合作伙伴提供“严选内容、高效转化”的培训服务，践行“助力客户成功”的价值承诺，提升企业人才发展的综合回报。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/52a9b1971f02f614f0ec5511ccc54aff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0898ab691c26366e1602f2a158deff38.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;时代召唤：AI 浪潮下的企业人才变革&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当 AI 重构千行百业，企业对“懂技术、会落地、能创新”的数智人才需求，已从“可选”变为“刚需”。极客时间企业版始终致力于将前沿数智技术与实战知识体系深度融合，此次入围正是对公司在应对时代命题、推动产业人才升级方面能力的高度认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深化创新，践行使命&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;立足新起点，极客时间企业版将以此次合作为引擎：对内，持续深化课程内容与服务创新；对外，将集采所带来的资源与平台优势，探索数智人才培养的新模式、新场景。我们坚信，专业的培训服务不仅是知识的传递，更是产业的赋能。未来，极客邦科技将继续秉持“推动数智人才全面发展，助力数智中国早日实现”的使命，与中国移动及所有伙伴一道，用人才之力点亮数智未来！&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a405ee19565ac3e822dd33d1fee8d4a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;（图为：极客时间企业版产品服务概览）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;合作咨询&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欢迎联系极客时间企业版，我们将按照您的企业场景、业务目标和人才发展要求，提供专属人才培养解决方案，助力您的企业致胜 AI 时代。敬请点击“阅读原文”访问官网，或扫描下图二维码&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d51c0dec780f590df20bbbd672190d33.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</link><guid isPermaLink="false">https://www.infoq.cn/article/WzMGM6CbcAcl8SCUhcd7</guid><pubDate>Thu, 15 Jan 2026 02:59:38 GMT</pubDate><author>极客时间企业版</author><category>数字人才培养</category></item><item><title>实测谷歌Veo 3.1：新增原生竖屏模式和4K画质，换个语言翻车到离谱？</title><description>&lt;p&gt;刚刚，谷歌更新了其 Veo AI 视频生成器，新增原生竖屏视频生成与 4K 分辨率支持功能。此次对 “文生视频” 功能的调整，旨在提升画面清晰度的同时，确保不同场景中的主体元素保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bd/bd8233a10f0980d44fe46156c0a3fe68.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Veo 3.1 的更新，解决了生成式视频领域一项长期存在的挑战：保持镜头间的视觉一致性。谷歌表示，新款模型在场景切换时能更好地保留人物特征与背景纹理，从而更容易重复使用特定的视觉元素，或在多场景叙事中贯穿同一主题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fee0a6257754c1b1378f71f8e313eac7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最显著的改进是对“素材到视频”工具的重大优化。用户只需添加三张参考图片：一张用于主体，一张用于背景，一张用于展现所需的视觉效果或风格。然后，只需添加一些文字即可开始制作。即使提示信息较短，Veo 3.1 也能在提供参考图像后生成角色表情和动作更生动的视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;移动创作者是本次更新的核心受众。升级后的 Veo 可直接生成 9:16 比例的原生竖屏视频，创作者无需对横屏素材进行裁剪，也不必牺牲画质，就能制作出适配 YouTube Shorts 等平台的全屏内容。针对更专业的创作流程，谷歌还新增了 1080P 至 4K 的画质提升选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，这些新功能已率先在 Gemini 应用、YouTube Shorts 及 YouTube Create 工具中上线，并将逐步覆盖谷歌旗下更多创作者工具与企业级服务。为区分生成内容与真实拍摄素材，谷歌会在视频文件中嵌入肉眼不可见的 SynthID 数字水印。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有体验用户反馈，Veo 3.1似乎存在不同语言版本表现差距太大的问题。“巴西葡萄牙语的人物音频存在音画不同步、台词错乱的问题，其他语言版本的表现则相对更佳。我曾指令其生成一段鹦鹉以沙哑嗓音鸣叫的音频，但该需求最终未能实现。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/32/32c77f6b5acf1e71fc9768effdd53ba3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，此次更新距苹果与谷歌官宣合作、计划在下一代 Siri 中集成 Gemini 模型仅过去一天。与此同时， OpenAI 已达成合作，计划将迪士尼角色引入 Sora 平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</link><guid isPermaLink="false">https://www.infoq.cn/article/cZX1jtxGLwkZmsU4CbsL</guid><pubDate>Thu, 15 Jan 2026 02:43:36 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>估值1亿的“死了么”APP有多好抄？5分钟AI就能复刻，去年有人一下午做出原型</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，上了热搜、又在苹果付费软件排行榜的榜首上挂了多日的&quot;死了么&quot;APP，突然宣布更名为Demumu。据其称，“经团队审慎决策，‘死了么’APP将于即将发布的新版本中，正式启用全球化品牌名Demumu。继昨日获得BBC报道后，我们的服务在海外实现了爆发式增长。未来，Demumu将继续秉持安全守护的初心，把源自中国的守护方案带向世界，服务全球更多独居群体。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自推出后，该APP的热度剧增，下载量一度暴涨100倍。虽开发成本仅1000多元，但获得不少头部投资机构的青睐，现在的估值已经飙到了1亿元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而最值得一提的是，这一APP的完整原型竟然在去年初就有了，可做出来的却不是同一批人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;痛失1亿的“原作者”，5分钟复刻出海外版&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“好消息：我去年做了一个‘死了么’APP；坏消息：我只做了产品设计和UI并发了文，但我当时觉得这只是个用来博眼球的噱头。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在死了么APP爆火后不久，一位ID为“饼干哥哥AGI”的数据分析师公开表示，去年3月，他从小红书上经常看到这个需求，随后花了不到一个下午的时间用Cursor和Claude 3.7生成了完整APP原型，并且将提示词模版和操作步骤都发布了出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;提示词by @花叔：&amp;nbsp;我想开发一个类似外卖APP「饿了么」，APP叫「死了么」，用于养老的，每天问一句，以防独自一个人死在家里没人发现。APP也有骑手，哪里有人死了就去接单收尸。 注意这是专门为独居90后的年轻人设计的。风格要求清新好看、APP内的文案多用搞怪的网络用语。&amp;nbsp;现在需要输出高保真的原型图，请通过以下方式帮我完成所有界面的原型设计，并确保这些原型界面可以直接用于开发：1、用户体验分析：先分析这个 APP 的主要功能和用户需求，确定核心交互逻辑。2、产品界面规划：作为产品经理，定义关键界面，确保信息架构合理。3、高保真 UI 设计：作为 UI 设计师，设计贴近真实 iOS/Android 设计规范的界面，使用现代化的 UI 元素，使其具有良好的视觉体验。4、HTML 原型实现：使用 HTML + Tailwind CSS（或 Bootstrap）生成所有原型界面，并使用 FontAwesome（或其他开源 UI 组件）让界面更加精美、接近真实的 APP 设计。拆分代码文件，保持结构清晰：5、每个界面应作为独立的 HTML 文件存放，例如 home.html、profile.html、settings.html 等。- index.html 作为主入口，不直接写入所有界面的 HTML 代码，而是使用 iframe 的方式嵌入这些 HTML 片段，并将所有页面直接平铺展示在 index 页面中，而不是跳转链接。- 真实感增强：&amp;nbsp; - 界面尺寸应模拟 iPhone 15 Pro，并让界面圆角化，使其更像真实的手机界面。&amp;nbsp; - 使用真实的 UI 图片，而非占位符图片（可从 Unsplash、Pexels、APPle 官方 UI 资源中选择）。&amp;nbsp; - 添加顶部状态栏（模拟 iOS 状态栏），并包含 APP 导航栏（类似 iOS 底部 Tab Bar）。请按照以上要求生成完整的 HTML 代码，并确保其可用于实际开发。&amp;nbsp;操作步骤：1.打开Cursor编辑器（确保版本足够新，支持Claude 3.7）2.选择编辑Agent模式3.选择Claude 3.7 Sonnet作为模型，最好是用thinking4.粘贴上述提示词，填入你需要的APP类型5.等待生成完成，可能需要3-5分钟&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术已经不值钱了。现在再重新做的话，可能一个下午的时候都够从0到设计到APP STORE上架了。”这位数据分析师感叹道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之后，他又花了5分钟就复刻出了&quot;死了么&quot;APP海外版。据称，这次他没敲任何代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;三名95后共同开发，APP收费已涨了8倍&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个引爆全网的APP，到底有什么魅力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，该应用是为独居人群打造的低成本安全工具，核心功能十分简单。用户无需注册登录，首次使用只需填写姓名和紧急联系人，每天打开应用完成签到即可；若连续2天未签到，系统将于次日自动发送邮件告知紧急联系人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最开始，“死了么”APP的收费只要1元，现在涨到了8元。团队表示，这是为了让项目能够健康、持续地发展，并覆盖日益增长的短信、服务器等成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该应用由三名95后共同创立并独立运营，在走红后，其背后的创始人们也陆续“现身”并对外披露了项目相关情况。“死了么”APP创始人之一郭先生介绍，团队内只有三名95后成员，且各自有自己的本职工作，通过远程方式进行协作。项目大约在2025年年中立项，开发时间不到一个月，初始投入成本仅1000多块钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位创始人吕先生表示，早在两三年前他们就在社交平台上留意到相关需求，“近几年大家都会讨论‘什么APP是每个人都需要的，并且一定会下载的’，就有网友提到‘死了么’APP，这个创意出来之后有很大的讨论度，我们看到了其中的需求，并且这件事本身也很有意义，于是我们就尝试去注册这个名字，发现可以注册，后续又用了一个月时间完成了开发。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据创始人郭先生透露，增长从2026年01月09日左右开始，短短两天内，下载量相比之前暴涨100倍以上，且仍在持续攀升。吕先生表示，现在APP的下载量不太方便透露，但“确实增长速度非常快”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据点点数据，除中国市场外，目前“死了么”APP在多国苹果应用商店霸榜第一：在新加坡付费榜位居总榜第一，在比利时、荷兰、瑞典等国付费工具榜排名第一，在英国、澳大利亚、美国等10多个国家付费工具榜排名第二。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公开信息显示，“死了么”APP由月境（郑州）技术服务有限公司开发，这家成立于2025年03月10日的公司，注册资本仅10万元，法定代表人为郭孟初，由其100%持股。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前有消息称，该公司已经接触到投资意向，正计划以100万元出让公司10%的股份。也就是说，这时其估值已达到1000万元。当前的最新消息是，如今该APP用户已增长800倍。并且，随着与六七十家投资方的深入接触和洽谈，短短两日间，“死了么”APP的估值飙升至近1亿元。但目前，其团队仍维持出让公司10%股份的计划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;遍地都是“活了么”，免费版卷疯了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“死了么”APP爆火后，陆续有网友提出优化建议，有人提出可以改成通过短信通知紧急联系人、优化签到形式等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，其团队已透露了后续的发展规划：接下来将把主要精力投入到产品打磨中，例如丰富短信提醒功能、考虑增加留言功能，并探索推出更适老化的新产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于为软件带来广泛共鸣与关注的名字，也开始迎来争议。不少网友认为，“死了么”不好听，建议改成“活着么”。苹果官方客服也于1月9日作出回应，称用户若对APP名称不满，可提供应用基础信息，客服将同步至相关业务部门，协调联系开发者沟通处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，“死了么”APP已改名为Demumu。有网友认为，“死了么APP之所以火，这名字最起码占一半功劳。改名，大概率算是把魂给丢了。只有当它从工具属性进化到社区属性，它才有可能活下来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而另一款名为“活了么”的APP已上架苹果应用商店，其功能与“死了么”相似，但目前是免费版的。此外，在苹果应用商店，有十几个类似名称与功能定位的APP也扎堆上线了，如“活着么”、“还活着么”“我还在”“我还活着呢”“我还好”等。其中，“活着么”目前就有9个，大部分都是免费版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da56da2e0e8757d5e369b1f2807e55d7.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&quot;&gt;https://mp.weixin.qq.com/s?__biz=MjM5NDI4MTY3NA==&amp;amp;mid=2257491168&amp;amp;idx=1&amp;amp;sn=b91be37bdd74cf7f27be5bef4bc337b7&amp;amp;scene=21&amp;amp;poc_token=HCkEZ2mjYvHT95lST7JpKli28nPa1gzdzMOTMQy0&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</link><guid isPermaLink="false">https://www.infoq.cn/article/SvMjPz85LESWG9NgCNME</guid><pubDate>Thu, 15 Jan 2026 02:36:04 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>亚马逊云科技为S3 Tables添加智能分层存储和复制功能</title><description>&lt;p&gt;亚马逊云科技最近宣布为&lt;a href=&quot;https://aws.amazon.com/s3/features/tables/&quot;&gt;S3 Tables&lt;/a&gt;&quot;引入两项新功能，第一项功能是新的智能分层存储类，该存储类能够根据访问模式自动优化成本，第二项功能是支持跨AWS区域和账户自动维护一致的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html&quot;&gt;Apache Iceberg&lt;/a&gt;&quot;表副本的复制功能，该过程无需手动同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/tables-intelligent-tiering.html&quot;&gt;智能分层存储类&lt;/a&gt;&quot;会将数据自动分配到最具成本效益的三个低延迟层级之一，即Frequent Access、Infrequent Access或Archive Instant Access。据公司介绍，最后一种是最低成本的层级，比Infrequent Access层级便宜68%。亚马逊云科技的主任开发者倡导者Sebastian Stromacq这样写到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在无访问达30天后，数据会被移动到Infrequent Access层级，在90天后，则会迁移到Archive Instant Access层级，这一过程不会对应用程序造成影响或性能降低。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;默认情况下，表使用标准存储类，但创建表时可以指定智能分层（Intelligent-Tiering）作为存储类，用户也可以在表存储桶级别配置默认存储类。用户可以将智能分层设置为表存储桶的默认存储类，如果在创建表时未指定存储类，那么表将自动存储在智能分层中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以利用&lt;a href=&quot;https://aws.amazon.com/cli/&quot;&gt;AWS命令行界面（AWS CLI）&lt;/a&gt;&quot;，通过put-table-bucket-storage-class和get-table-bucket-storage-class命令来更改或验证其S3表格存储桶的存储层级。相关命令如下所示：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;shell&quot;&gt;aws s3tables put-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \
   --storage-class-configuration storageClass=INTELLIGENT_TIERING


# Verify the storage class
aws s3tables get-table-bucket-storage-class \
   --table-bucket-arn $TABLE_BUCKET_ARN  \


{ &quot;storageClassConfiguration&quot;:
   {
      &quot;storageClass&quot;: &quot;INTELLIGENT_TIERING&quot;
   }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;来自Imperious Enterprise的AWS架构师Adefemi Adeyemi在LinkedIn的&lt;a href=&quot;https://www.linkedin.com/posts/adefemi-adeyemi_if-you-are-working-with-apache-iceberg-on-activity-7402006733004406784-SMSW&quot;&gt;帖子&lt;/a&gt;&quot;中指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数分析数据集在一段时间内是“热”的，但随后会逐渐“冷却”。借助S3 Tables的智能分层功能，你无需不断调整Iceberg数据的生命周期策略。该服务会根据访问模式自动将对象移至更便宜的存储层级，这对长期存在的数据湖来说是一大优势。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，S3 Tables的&lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-replication-managing.html&quot;&gt;复制功能&lt;/a&gt;&quot;可以帮助用户跨AWS区域和账户维护表格的一致性只读副本。当声明目标表格的存储桶时，服务会创建只读的副本表格，并以时间顺序复制所有更新，同时保持父子快照关系。这些副本表格将在源表格更新后的几分钟内得到更新，并支持独立于源表格的加密和保留策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Stromacq说到：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;用户可以通过&lt;a href=&quot;https://aws.amazon.com/sagemaker/unified-studio/&quot;&gt;Amazon SageMaker Unified Studio&lt;/a&gt;&quot;或任何兼容Iceberg的引擎（包括&lt;a href=&quot;https://duckdb.org/&quot;&gt;DuckDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://py.iceberg.apache.org/&quot;&gt;PyIceberg&lt;/a&gt;&quot;、&lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;&quot;和&lt;a href=&quot;https://trino.io/&quot;&gt;Trino&lt;/a&gt;&quot;）查询副本表格。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助AWS Management Console、API或&lt;a href=&quot;https://aws.amazon.com/tools/&quot;&gt;AWS SDK&lt;/a&gt;&quot;，用户可以创建和维护表格副本。此外，他们可以指定用于复制源表格的目标表格存储桶。当用户启用复制功能时，S3 Tables会在这些存储桶中创建只读副本，使用最新状态进行回填，并持续监控更新以保持同步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在同一篇LinkedIn帖子中，Adeyemi指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对复制功能的原生支持让你能够快速创建只读副本，这些副本在几分钟内即可与源表保持同步，并且可作为Iceberg表进行查询。减少了自定义集成的工作量，让你有更多时间真正使用数据。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用户可以通过&lt;a href=&quot;https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html&quot;&gt;AWS Cost and Usage Reports&lt;/a&gt;&quot;和&lt;a href=&quot;https://aws.amazon.com/cloudwatch/&quot;&gt;Amazon CloudWatch&lt;/a&gt;&quot;指标跟踪各访问层的存储使用情况。配置智能分层无需额外费用，用户仅需支付各层的存储成本。至于S3 Table的复制，用户需支付目标表格的S3 Table的存储费用、复制PUT请求的费用、表格更新（提交）以及复制数据的对象的监控费用。更多详情可参见&lt;a href=&quot;https://aws.amazon.com/s3/pricing/&quot;&gt;定价页面&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/s3-tables-intelligent-tiering/&quot;&gt;&amp;nbsp;AWS Adds Intelligent-Tiering and Replication for S3 Tables&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</link><guid isPermaLink="false">https://www.infoq.cn/article/QF4H8hz11CO0coRgWL5I</guid><pubDate>Thu, 15 Jan 2026 02:34:41 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>数据库</category></item></channel></rss>