<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Fri, 30 Jan 2026 09:05:23 GMT</lastBuildDate><ttl>5</ttl><item><title>效率狂飙数倍后：Coding Agent已然成熟，但开放世界仍是“无人区”</title><description>&lt;p&gt;如果说 2024 年是属于大模型的“奇迹之年”，那么刚刚过去的 2025 年，则可以被定义为 Agent（智能体）的“工程落地元年”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术圈的语境里，大模型正经历从“被动问答”到“主动干活”的范式转移。过去没有 Agent 的时候，大模型是在被动地回答问题；Agent 诞生后，它变成了主动执行。这不仅仅是业务模式的变化，更是从聊天程序到生产组件的根本性变革。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这种根本性的变革却不是一蹴而就的，而是由多个重要的、里程碑式的“协议”和框架强力驱动的。这一点与早期互联网协议定义推动 Web 应用爆发式增长的逻辑类似——标准化不是为了漂亮的规范，是要真正去解决跨系统、跨场景、跨团队协作的实际工程痛点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2025 年，两大协议推动了 Agent 应用的爆发&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云容器计算服务 ACS Agent Sandbox 技术负责人黄涛在最近的一次深度对谈中将其归结为三个关键事件：MCP 协议的爆火、A2A 协议的发布，以及多智能体协同的工程化实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是 Anthropic 发布的 MCP（Model Context Protocol，模型上下文协议），旨在定义 AI 模型如何访问外部工具、数据库和服务的标准化方式。与过去每个模型厂商根据自有接口定制不同，MCP 提供了类似 “USB-C 接口” 的统一协议，使得智能体能跨平台访问外部能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，MCP 能够让智能体通过统一的协议访问数据库、库存系统、工作流 API 等，而不需要针对每种服务和接口写特定的适配代码。 这种标准化的好处，在企业级应用场景中尤为显著：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;减少集成成本：应用方不再为大量不同的 API 写 glue code。提升可靠性与一致性：统一格式、统一调用流程让错漏减少。加速自动化能力落地：智能体能快速理解系统数据并据此执行任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;起初，这看起来只是一个单纯的技术协议，但在刚刚过去的一整年，它彻底引爆了应用层。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以阿里集团内部为例，为了加速 AI 在电商领域的渗透，阿里孵化出了名为“TMCP”的电商 MCP 网关平台。大量业务方通过编写 MCP Server，将复杂的供应链、库存数据、用户信息等通过标准化协议“喂”给 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“MCP 解决的是 Agent 看世界、调工具的‘语言统一’问题。”黄涛解释道。以前 Agent 调用工具需要针对每个接口做定制，现在有了标准网关，Agent 可以更快速地理解客户需求，从一个只会聊天的程序，变成真正能调度阿里复杂业务逻辑的“超级组件”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，另一款协议也值得重点关注。Agent-to-Agent（A2A）是由谷歌发布的，其核心目标是定义智能体间的“通用语言”和协作规范，使不同背景、不同厂商或不同开发框架下的智能体，可以像微服务一样，通过标准化方式互相发现、协商任务、交换状态、协调工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这类似于 Web 发展的历史中 HTTP、REST API 为服务间通信提供标准一样——如果没有可互操作的通讯协议，大规模协作系统无法自然形成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去，不同功能的 Agent 之间想要对话，往往需要开发者编写极其复杂的“粘合代码”。而 A2A 标准的出现，意味着不同背景、不同厂商的 Agent 可以像人类员工一样，通过一套标准的交互准则进行协作。&lt;/p&gt;&lt;p&gt;协议能力上看，MCP 与 A2A 都可以用于描述智能体之间的交互，但二者的设计侧重点存在差异。MCP 更强调通用的调用与连接能力，统一智能体与外部工具、系统乃至其他智能体的交互方式；相比之下，A2A 在设计上更聚焦于多智能体场景本身，试图为智能体之间的协作、状态同步与交互模式提供更直接的抽象支持。因此，在早期多 Agent 系统实践中，即便采用了 MCP 这类通用协议，智能体之间的协调逻辑仍常常依赖开发者手工实现，难以随着系统规模的增长而自然演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，Manus 等框架提出的多智能体协同概念，不仅停留在交互层，更深入到了底层的基座能力。比如安全沙箱（Sandbox）技术的引入，解决了 Agent 在执行代码或处理敏感数据时的隔离问题，让“协作”不再是裸奔。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;繁荣背后的工程陷阱：多 Agent 协作的“收敛性”困局&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管应用层呈现爆发趋势，但当 Agent 真正走进企业级生产环境时，工程性挑战接踵而至。最让开发者头疼的，莫过于多 Agent 协作中的“低效”与“幻觉”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI CEO 奥特曼曾描绘过一个超级个体带领一堆 Agent 协作的未来。但在实际操作中，守辰发现了一个尴尬的现实：Agent 之间会产生大量“无效沟通”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“多个 Agent 协作时，经常会出现不聚焦的情况，聊着聊着就聊开了。”阿里云智能容器服务高级专家， OpenKruise Agents 项目发起人张振举例说，有些框架下，Agent 之间会互相委派任务，甚至出现死循环。这种“社交式发散”直接导致了 Token 消耗的激增，但最终得到的推理效果却可能不如一个定义明确的单 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种成本不仅仅是金钱上的，更是算力资源的浪费。对于企业而言，如何量化 Agent 之间的协作模式，识别并固化有效的沟通路径，避免像人类会议一样的“低效扯皮”，是目前的重难点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个挑战在于 Agent 的“自制能力”尚浅。在传统的 BPM（业务流程管理）或 RPA（机器人流程自动化）领域，追求的是强约束、强工程化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前的 Agent 虽然有灵性，但离完全自制还有很大差距。黄涛认为，现阶段 Agent 与 BPM 的关系并非“替代”，而是“融合”。“我们要给 Agent 定义清晰的边界和子系统，明确它的输入、输出和约束，而不是把它当作一个泛化的、人格化的机器人。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阿里的实践中，开发者尝试在现有的工具流中加入 Agent 节点，让它处理那些“不那么确定”的子任务，而将确定性的逻辑依然留给脚本或流程引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄涛的这一观点，为 Agent 当前的发展阶段进行了精准锚定。它摒弃了不切实际的科幻幻想，转而拥抱一种务实、可工程化的演进路径：Agent 并非一个从天而降、全知全能的“取代者”，而是一个需要被精心设计和集成到现有生产体系中的“增强组件”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“融合”思维，决定了 Agent 价值的兑现方式——它必须深入具体业务的血肉之中，在解决真实痛点、优化既有流程的过程中证明自己。那么，Agent 究竟在哪些场景里产生了真实价值？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内普遍认为，最先被 Agent 攻陷的堡垒是编程和运维。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI Coding 是目前 Agent 落地最成熟、收益最可观的领域。黄涛分享了自己的体感：“以前写一段代码需要一个小时，现在 Agent 一分钟生成，我再改个十来分钟，效率提升是巨大的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更显著的变化发生在自动化运维。2024 年的运维 AI 更多是基于 RAG 查手册，而 2025 年的 Agent 则学会了“模仿工程师经验”。当系统报错时，Agent 会自动执行一系列命令去定位问题，甚至能感知真实的运行环境并做出反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张振对 2026 年最期待的突破点是“开放世界训练”。随着 Agent 被装进手机（如字节跳动与中兴的合作）或机器人（如宇树机器人），它面临的是未知的、非实验室的环境。一个典型的挑战是：Agent 操作某个 App 时被封禁了，它该怎么办？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“让 AI 知道自己不知道，是走向真智能的关键一步。”张振提到。阿里云正在通过发布像 OpenKruise Agents 这样的基础设施，提供检查点（Checkpoint）和克隆功能，来加速这种在开放世界中的训练效率。值得一提的是，OpenKruise Agents 是阿里云容器计算服务 ACS 的 Agent Sandbox（ACS Agent Sandbox）逐步开源的能力之一。与 OpenKruise Agents 不同，ACS Agent Sandbox 面向企业级 AI Agent 应用规模化落地，内存级休眠唤醒与 checkpoint 克隆能力 ，支持结合云端弹性调度与微虚拟化隔离，以缩短沙箱启动与恢复时间，提升并行探索效率以及降低训练成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Agent 的终极形态：超级自动化还是数字员工？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从攻克编程与运维的确定性堡垒，到勇敢迈向充满未知的开放世界训练，Agent 的能力边界正在实践中被不断拓展和重新定义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种从“专用工具”到“适应环境”的演进路径，自然引发了更深层次的思考：Agent 进化的终点究竟在何处？是成为无所不能的超级自动化智能，还是先成为我们身边协同工作的可靠伙伴？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 Agent 的终极形态，黄涛和张振两位专家给出了略有分歧但互补的视角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄涛的视角更偏向“高度自制的智能体”：他认为 Agent 最终会演化成在家庭助理、工厂、无人驾驶等场景中完全自主运行的实体。它能完美感知环境差异，自主决策，彻底解放人类。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而张振的视角则更务实，倾向于“数字员工”：他认为短期内，AI Agent 会以数字员工的身份在企业中入职。“员工”这个角色方便企业进行 KPI 评估，也方便人类与之协作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管愿景不同，但共识已成：Agent 将不再是特定领域的应用，而是一种像数据库、中间件一样的“新兴基础设施”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一年，我们经历了对 Agent 能力的盲目崇拜，也正在经历对其工程化落地的痛苦磨合。当 MCP 协议把业务的大门敲开，当沙箱技术把安全的篱笆扎紧，当开放世界训练让 AI 开始学会“思考”，Agent 就不再是 PPT 上的概念，而是真正开始改变生产力逻辑的底层变量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如张振所强调的那样，AI 可能无法立即成为那个“超级智能体”，但它会以无数个“数字员工”的身份，渗透进代码的每一行、运维的每一次报警、以及每一个复杂的商业决策中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这才是 Agent 时代的真实叙事：不在于取代，而在于进化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采访嘉宾简介：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄涛 ，阿里云容器计算服务 ACS 技术负责人张振，阿里云智能容器服务高级专家， OpenKruise Agents 项目发起人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0gUgQyIFDDrgJaXlESUg</link><guid isPermaLink="false">https://www.infoq.cn/article/0gUgQyIFDDrgJaXlESUg</guid><pubDate>Fri, 30 Jan 2026 08:59:12 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>OCR竞争加剧！百度开源新一代SOTA OCR模型，性能超越DeepSeek-OCR2？</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;1月29日，百度正式发布并开源新一代文档解析模型PaddleOCR-VL-1.5。该模型以仅0.9B参数的轻量架构，在全球权威文档解析评测榜单OmniDocBench V1.5中取得全球综合性能第一成绩，整体精度达到94.5%，超过Gemini-3-Pro、DeepSeek-OCR2、Qwen3-VL-235B-A22B、GPT-5.2等模型。&lt;/p&gt;&lt;p&gt;﻿&lt;/p&gt;&lt;p&gt;值得关注的是，PaddleOCR-VL-1.5 全球首次实现OCR模型的“异形框定位”能力，使机器能够精准识别倾斜、弯折、拍照畸变等非规则文档形态，首次让“歪文档”实现稳定、可规模化解析。该技术解决了传统OCR模型在移动拍照、扫描件变形、复杂光照等真实场景中因文档形变导致的识别失败问题，可广泛应用于金融票据处理、档案数字化、政务文档流转等场景。&lt;/p&gt;&lt;p&gt;﻿&lt;/p&gt;&lt;p&gt;PaddleOCR-VL-1.5 基于文心大模型进行开发，在 OmniDocBench V1.5多个关键指标上取得领先表现。其中，表格结构理解（92.8 分）和阅读顺序预测（95.8 分）两项核心指标上均位列第一，分别领先 Gemini-3-Pro、DeepSeek-OCR 等主流模型 2–5 分不等。在文档阅读顺序预测任务中，其版面逻辑解析错误率仅为同类其他模型约一半。这表明，PaddleOCR-VL-1.5 在复杂文档结构还原与版面逻辑理解方面具备更高稳定性，在合同、财报等高复杂度业务场景中拥有更高可用性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/9d/77/9dfbbc9e3fc80bbba7174cdbe6fe7177.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在线使用/API：&lt;a href=&quot;https://www.paddleocr.com/&quot;&gt;https://www.paddleocr.com&lt;/a&gt;&quot;﻿&lt;/p&gt;&lt;p&gt;开源项目地址：&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&quot;﻿&lt;/p&gt;&lt;p&gt;模型下载地址：&lt;a href=&quot;https://huggingface.co/PaddlePaddle/PaddleOCR-VL-1.5&quot;&gt;https://huggingface.co/PaddlePaddle/PaddleOCR-VL-1.5&lt;/a&gt;&quot;﻿&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年10月16日，百度首次发布并开源 PaddleOCR-VL模型，在 OmniDocBench V1.5 榜单中取得全球SOTA成绩，并连续五天登顶 HuggingFace全球模型总趋势榜与ModelScope全球模型总趋势榜双榜第一。&lt;/p&gt;&lt;p&gt;﻿&lt;/p&gt;&lt;p&gt;相比于上代，在功能层面，PaddleOCR-VL-1.5 进一步集成印章识别、文本检测与识别等任务能力，关键指标持续领跑；同时针对特殊场景与多语种识别进行系统优化，在生僻字、古籍文献、多语种表格、下划线与复选框等复杂结构识别方面显著提升，并新增对藏语、孟加拉语等语种的支持。模型还支持跨页表格自动合并与跨页段落标题识别，有效解决长文档解析中的结构断裂问题。&lt;/p&gt;&lt;p&gt;﻿&lt;/p&gt;&lt;p&gt;近半年来，全球主流模型厂商密集布局OCR 领域。1月27日，深度求索发布新一代 OCR 模型 DeepSeek-OCR-2，引入“因果流查询”机制，并将语言模型融入视觉编码，在OmniDocBench V1.5中实现91.09%精度。与此同时，Mistral AI、字节跳动、腾讯等企业也相继推出新一代 OCR 模型，行业竞争持续加剧。&lt;/p&gt;&lt;p&gt;·&lt;/p&gt;</description><link>https://www.infoq.cn/article/US8DFAjTKWuEUkRBEFSj</link><guid isPermaLink="false">https://www.infoq.cn/article/US8DFAjTKWuEUkRBEFSj</guid><pubDate>Fri, 30 Jan 2026 08:34:01 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>商汤开源SenseNova-MARS，突破多模态搜索推理天花板</title><description>&lt;p&gt;今天，商汤正式开源多模态自主推理模型：SenseNova-MARS（8B/32B 双版本）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS，是首个支持动态视觉推理和图文搜索深度融合的Agentic VLM（ Vision-Language Model，视觉-语言模型）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所谓Agentic VLM，就是既能看图、又能像Agent一样行动的多模态模型。在工程上，Agentic VLM通常是“一个核心模型+一层轻量控制逻辑”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和很多Agentic VLM主要依赖外部框架来“指挥流程”不同，SenseNova-MARS把部分记忆判断能力，放进了模型自己的思考过程里——而且把“视觉细节+搜索结果”，当作推理过程中的持续变量，而不是把这些信息当作一次性输入或外部状态被简单传递。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样做的好处在哪？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，就是模型的推理不再是线性的，而是带反馈的闭环过程。可以显著降低错误累积，并提升复杂任务的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来看看更直观的结果：在多模态搜索与推理这一组核心测试中，SenseNova-MARS 拿到了69.74分，超过了Gemini-3-Pro（69.06 分）&amp;nbsp;和&amp;nbsp;GPT-5.2（67.64 分）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d59e3e801fd91e41cf2e0c83792b9c75.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再看细项，在MMSearch、HR-MMSearch、FVQA、InfoSeek、SimpleVQA、LiveVQA等基准测试中，SenseNova-MARS取得开源模型中的SOTA 成绩，还超越Gemini-3.0-Pro、GPT-5.2等顶级闭源模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更多细节可见技术报告（https://arxiv.org/abs/2512.24330）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;全能冠军，自主解决复杂问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在MMSearch 榜单（图文搜索核心评测）中，模型以 74.27 分登顶，超越GPT-5.2（66.08 分）；HR-MMSearch（高清细节搜索评测）中 54.43 分领先，显著拉开与闭源模型的差距。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ee/ee360ad6b9e7a1c18c12111f17be378e.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;HR-MMSearch的测试题目堪称“AI界的奥林匹克”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采用305张2025年最新的4K超高清图片，确保AI无法依赖旧知识“作弊”；所有问题都针对图片中占比不到5%的细节，比如小标志、小字、微小物体，必须用图像裁剪工具才能看清；覆盖体育、娱乐文化、科学技术、商业金融、游戏、学术研究、地理旅行等八大领域，60%的问题都需要至少使用三种工具才能解答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单说，无论是需要“查遍全网”的知识密集型任务，还是需要“火眼金睛”的细粒度视觉分析，它都是当前的“全能冠军”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53574430ae73cb193eb1344d4827f15f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用组合拳，解决真实场景问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS还能实实在在落地到我们生活和工作的场景，解决需要“多步骤推理+多工具协作”的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;普通AI的工具调用，要么只能搜文字，要么只能看图片，遇到需要“先放大细节、再识别物体、最后查背景”的复杂任务就束手无策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e721ddf42290eb3d1b494efef9c4d06.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对识别赛车服微小 logo + 查询公司成立年份 + 匹配车手出生年月 + 计算差值’的复杂任务，SenseNova-MARS 可自主调用图像裁剪、文本 / 图像搜索工具，无需人工干预完成闭环解答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1bce7ce95d3d9aaa60d3d464daa9a2a6.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS能从产品和行业峰会的照片中，识别企业的标志，快速搜集产品、企业的信息，以及时间、数量、参数等细节要素，辅助分析行业情况和格局。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f2/f26dbef3bdeb01be2048393ea5d3d1b3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS能从赛事照片中识别画面中的logo、人物等信息，追溯比赛或人员背景信息，帮助快速补充重要细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/51/5160df7753308658f84b55092d602833.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS甚至能够轻松处理，这类超长步骤的多模态推理，和超过三种工具调用，自动裁剪分析细节、搜索相关研究数据，快速验证假设，得出关键判断。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;拥有这种“自主思考+多工具协作”的能力，SenseNova-MARS能够自动解决“细节识别 + 信息检索 + 逻辑推理”复杂任务，帮助实现工作效率提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图像裁剪：能精准聚焦图片上的微小细节，哪怕是占比不到5%的细节——比如赛车手衣服上的微小logo、赛事照片里观众席的标语，都可通过裁剪放大清晰分析。&lt;/p&gt;&lt;p&gt;图像搜索：能在看到物体、人物或场景，的瞬间自动匹配相关信息——比如识别出赛车手的身份，或是某款冷门设备的型号。&lt;/p&gt;&lt;p&gt;文本搜索：能快速抓取精准信息——无论是公司成立年份、人物出生年月，还是最新的行业数据，都能秒级获取。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53574430ae73cb193eb1344d4827f15f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从练中学，形成“经验”和“直觉”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SenseNova-MARS采用了“因材施教”的训练方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一阶段：打基础。&lt;/p&gt;&lt;p&gt;针对跨模态多跳搜索推理训练数据稀缺的痛点，创新性的提出了基于多模智能体的自动化数据合成引擎，采用细粒度视觉锚点+ 多跳深度关联检索的机制，动态挖掘并关联跨网页实体的逻辑，自动化构建高复杂度的多跳推理链路，同时引入闭环自洽性校验来去除幻觉数据，构造出具备严密逻辑链条与高知识密度的多跳搜索问答数据。用精心筛选的“高难度案例”做教材，每个案例都标注了“该用什么工具、步骤是什么”，让AI先学会基本的“破案逻辑”。这些案例都是从海量数据中挑出的“硬骨头”，确保AI一开始就接触真实复杂场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二阶段：练实战。采用“强化学习”——就像侦探在一次次破案中积累经验，AI每做对一次决策（比如选对工具、步骤合理）就会获得奖励，做错了就调整策略。为了避免AI“学偏”，研究团队还加了个“稳定器”——BN-GSPO算法，让它在处理简单题和复杂题时都能保持稳定进步，不会出现“偏科”。 这种基于双阶段归一化的优雅机制有效平滑了动态工具调用返回分布多样性带来的优化波动并确保了学习信号分布的一致性，从而成功解决了跨模态多步多工具智能体训练过程中的收敛性难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过这样的训练，AI不仅学会了用工具，更培养&quot;工具使用直觉&quot;——知道在什么情况下应该使用哪些工具，以及如何将不同工具的结果有机结合起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;商汤日日新SenseNova-MARS模型、代码、数据集全开源，支持 Hugging Face 直接下载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Github 仓库：&lt;/p&gt;&lt;p&gt;https://github.com/OpenSenseNova/SenseNova-MARS&lt;/p&gt;&lt;p&gt;模型仓库：&lt;/p&gt;&lt;p&gt;32B：&lt;/p&gt;&lt;p&gt;https://huggingface.co/sensenova/SenseNova-MARS-32B&lt;/p&gt;&lt;p&gt;8B：&lt;/p&gt;&lt;p&gt;https://huggingface.co/sensenova/SenseNova-MARS-8B&lt;/p&gt;</description><link>https://www.infoq.cn/article/3x5oE24DW1X5eFYeMGTE</link><guid isPermaLink="false">https://www.infoq.cn/article/3x5oE24DW1X5eFYeMGTE</guid><pubDate>Fri, 30 Jan 2026 08:08:48 GMT</pubDate><author>木子</author><category>AI 工程化</category></item><item><title>苹果 Siri 压力大了？Google 甩出 FunctionGemma 杀手锏，手机本地也能流畅调 API</title><description>&lt;p&gt;Google 正式发布了&amp;nbsp;&lt;a href=&quot;https://blog.google/innovation-and-ai/technology/developers-tools/functiongemma/&quot;&gt;FunctionGemma&lt;/a&gt;&quot;，这是其 Gemma 3 270M 模型的一个全新轻量化版本。该模型经过专门微调，能够将自然语言指令精准转化为结构化的函数和 API 调用，从而让 AI 代理超越“空谈”，具备真正的执行力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;&lt;a href=&quot;https://developers.googleblog.com/en/introducing-gemma-3-270m/&quot;&gt;Gemma 3 270M&lt;/a&gt;&quot;&amp;nbsp;发布数月后，为了响应开发者日益增长的需求，Google 赋予了该模型原生的函数调用（Function Calling）能力，使其进化为 FunctionGemma。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本地化运行赋予了该模型双重身份：它既可以作为一个独立的代理，处理私密且离线的任务；也可以充当“智能流量调度员”，将更复杂的请求路由至更大规模的远程模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这一特性在端侧（On-device）应用中尤为引人注目。AI Agent 可以借此实现从设置提醒到切换系统设置等一系列复杂的多步工作流自动化。为了在边缘计算场景中实现这一目标，模型必须足够轻量以支持本地运行，同时又必须具备极高的专业化程度以保证可靠性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google 解释称，FunctionGemma 的初衷并非用于零样本提示（Zero-shot prompting），而是旨在让开发者进行深度定制，从而构建出快速、私密且能将自然语言转化为可执行 API 操作的端侧代理。这种方法是模型达到生产级性能的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在 Google 的“移动操作（Mobile Actions）”测试评估中，微调技术显著提升了模型的可靠性，将其准确率从 58% 的基准线大幅拉升至 85%。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在硬件适配方面，该模型专为手机和 NVIDIA Jetson Nano 等资源受限的设备设计。它利用 Gemma 家族的 256k 词表，能够高效地对 JSON 数据和多语言输入进行分词处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;FunctionGemma 支持 Google 所称的“统一行动与对话”模式。这意味着模型既能生成用于调用工具的结构化代码或函数，又能无缝切换回自然语言，向用户解释执行结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google 同时指出，FunctionGemma 拥有广泛的生态系统支持。开发者可以使用 Hugging Face Transformers、Unsloth、Keras 或 NVIDIA NeMo 等框架进行微调，并通过 LiteRT-LM、vLLM、MLX、Llama.cpp、Ollama、Vertex AI 或 LM Studio 等平台进行部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对开发者，Google 明确列出了 FunctionGemma 的最佳适用场景，包括：拥有明确的 API 接口、愿意进行模型微调、优先考虑本地化部署，或正在构建结合端侧与远程任务的复杂系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了展示模型的实战能力，Google 发布了多个演示项目，包括 Mobile Actions、TinyGarden 和 Physics Playground。这些演示均可通过 Play 商店中的&amp;nbsp;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery&quot;&gt;Google AI Edge Gallery 应用&lt;/a&gt;&quot;进行体验：&lt;/p&gt;&lt;p&gt;Mobile Actions：解析诸如“为明天的午餐创建一个日历行程”、“将 John 添加到联系人”或“打开手电筒”等自然语言指令，并将其映射到相应的操作系统级工具调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TinyGarden：一款语音控制游戏。玩家给出“在顶排种下向日葵并浇水”等指令，模型会将其分解为带有坐标目标的 plantCrop 和 waterCrop 等具体函数调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Physics Playground：一个交互式物理益智演示。它使用自然语言指令控制游戏内的模拟动作，并利用 Transformer.js 展示了客户端 JavaScript 的集成能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，FunctionGemma 已在&amp;nbsp;&lt;a href=&quot;https://huggingface.co/google/functiongemma-270m-it&quot;&gt;Hugging Face&lt;/a&gt;&quot;&amp;nbsp;和&amp;nbsp;&lt;a href=&quot;https://www.kaggle.com/models/google/functiongemma&quot;&gt;Kaggle&lt;/a&gt;&quot;&amp;nbsp;上线。此外，Google 还提供了&amp;nbsp;&lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/%5BFunctionGemma%5DFinetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb&quot;&gt;Colab 笔记本&lt;/a&gt;&quot;和&amp;nbsp;&lt;a href=&quot;https://huggingface.co/datasets/google/mobile-actions&quot;&gt;mobile-actions 数据集&lt;/a&gt;&quot;，以帮助开发者更轻松地对模型进行专业化训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/functiongemma-edge-function-call/&quot;&gt;https://www.infoq.com/news/2026/01/functiongemma-edge-function-call/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QRiKGWvWxwUh7wAwODjK</link><guid isPermaLink="false">https://www.infoq.cn/article/QRiKGWvWxwUh7wAwODjK</guid><pubDate>Fri, 30 Jan 2026 08:00:00 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>凌晨三点写代码、10个 Agent 同时跑！ClawdBot 创始人自曝 AI 上瘾史：Claude Code 入坑，Codex 成主力</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Clawdbot（现名：Moltbot）火了到国内，社交平台上到处都是部署教学、使用教学和使用展示。国内的腾讯云、阿里云等也相继宣布上线 Clawdbot 云端极简部署及全套云服务，钉钉也在 Github 上开源了 Moltbot 接入方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;项目背后的创始人 Peter Steinberger 也红极一时，他的构建方式成为很多人的学习对象。Peter之前就是一位非常出色的开发者，打造了一个被用在超过十亿台设备上的 PDF 框架。后来他经历了严重的职业倦怠，卖掉股份，整整三年从科技圈消失。今年，他回来了，而他现在的构建方式、正在做的事情，已经和传统软件开发完全不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter 近期在“The Pragmatic Engineer”节目中，用近两个小时的时间分享了他的开发经历。他解释了，为什么他现在发布的代码，大部分自己都不再逐行阅读，而这其实并没什么大不了；他具体是如何打造了 ClawdBot 这个看起来就像 Siri 未来版本的个人助手的；他如何利用“闭环原则”，高效进行 AI 编程；为什么代码评审已经过时，PR 应该改名叫 Prompt Request等，他还分享了很多关于软件工程工作流在未来几年可能发生的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter 可以称得上是“AI 重塑开发方式”的最佳实践者之一。我们整理翻译了这期干货满满的对话，并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;怎么入行的？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这次终于线下见到你了，太棒了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是啊，差点还搞砸了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：怎么回事？是忘了时间吗？你经常这样吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：其实不太常见。只是最近这个时间点比较特殊，因为我最近的项目 ClawdBot 突然火了。说实话，有点睡不够了。但这种感觉也很有意思，我从来没经历过一个社区在这么短时间内爆发。真的非常好玩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在聊 ClawdBot 之前，我们先把时间拉回去。你做的 PSPDFKit，据说被用在超过十亿台设备上，基本上你看到一个 PDF 被渲染，很可能背后就是它。那在更早之前，你是怎么进入技术行业的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：天哪，这得从很早说起了。我来自奥地利一个小地方，一直比较内向，经常被欺负。那时候，夏天总会有客人来家里，其中有个电脑迷，我迷上了他的机器，天天盯着这台机器研究，最后求妈妈给我买了一台。从那以后，我就彻底陷进去了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那时候你还在读中学？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：差不多吧，大概十四岁。我最早做的事情之一，是从学校“顺”了一张老 DOS 游戏的软盘，然后自己写了个拷贝保护，好拿去卖。加载一次要两分钟，但我当时觉得这事特别酷。当然也打了很多游戏，不过对我来说，做东西本身就像在玩游戏。说实话，现在做事带来的成就感，比通关游戏还爽。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始我看的是类似Windows的bash脚本，然后做网站，写一点 JavaScript，虽然完全不知道自己在干嘛。真正系统性地学“怎么构建东西”，是上大学之后。我从没见过我父亲，家里也很穷，所以我一直要打工，学费生活费都得自己赚。别人放假的时候，我就在公司全职上班。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我第一份正式工作在维也纳，本来只打算干一个月，结果他们留了我六个月，后来我在那家公司工作了大概五年。第一天他们给了我一本厚厚的书，上面写着“Microsoft MFC”，到现在我做梦还会被吓醒。我当时心想，这也太糟了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我干脆悄悄用 .NET，也没跟他们说。过了几个月我才摊牌，说我顺便做了点“技术栈现代化”。反正木已成舟，他们居然也一直留着我，大概因为事情确实做成了。我实际上还挺喜欢.NET 2.0的泛型，不过应用启动慢得要命，第一次跑基本要等很久，老 Windows 用户应该都懂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你后来是怎么接触到 iOS，又是怎么想到做PSPDFKit 的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：那是后面的事了。上大学时，有个朋友给我看了 iPhone。我就摸了一分钟，立刻决定要弄一台。那一刻真的像被雷劈了一样，完全不一样，完全是另一个层级的体验。但当时我其实还没想过要给它做应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那大概是2009、2010年左右？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：差不多。后来有一次，我在地铁上用一个交友网站，用的是 iPhone OS 2。我打了一大段很走心的消息，刚点发送后车进隧道了，JavaScript禁用了发送按钮，然后直接报错。那时候没有复制粘贴、没有截图、页面还不能滚动，那段话就这么没了。我当时气炸了，觉得这简直不可接受。回到家我就把那个网站黑了，用正则去解析 HTML。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在看当然完全不该这么干，后来我硬做了一个 App。我用的是 iPhone OS 3 的 Beta 版，Core Data 也是 Beta，还用改过的 GCC，把 blocks 编译器移植进来。各种 Beta 技术一锅炖，我自己其实也不知道在干嘛，折腾了很久才跑起来。我给那家公司写信说我做了个 App，问他们怎么看，没人理我，我就直接丢到 App Store上架了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这就是那个交友 App 的客户端？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，本质上就是把 HTML 当 API 用，纯解析页面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：现在看挺野的，但在当年确实没人这么干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我定价五美元，第一个月就赚了一万美元。当时我完全不知道流程有多复杂，Apple 的系统也很原始。我甚至把收款账户填成了我爷爷的。有一天我爷爷打电话问我，说怎么 Apple 给他打了一大笔钱，我跟他说“这是我的，你千万别动”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来有一次我在夜店里，看到有人在用我的 App，我特别骄傲，差点冲过去跟他说这是我做的，最后还是忍住了。没多久，我就跟工作了五年的公司说，我要全力做这个项目。老板当面嘲笑我，说这是个一时的风口，肯定不长久。那一刻我心里就憋了一口气：总有一天，我要做一家比你们值钱的公司。结果这花了我八年的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有点成瘾性格，一旦投入就停不下来。我疯狂打磨这个 App，高速学习，也是那段时间我开始用 Twitter，那些对我职业发展影响巨大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来有一天凌晨三点，我在派对上喝得有点多，然后接到了一个电话，对方说他是 Apple 的 John，说我的应用有问题，有人举报不当图片。电话挂了，我的 App 也就此下架。我刚辞了工作，心态直接炸裂，开始接零散的活儿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在旧金山的一家酒吧里，我被介绍为“奥地利最好的 iOS 开发者之一”。就这样，我拿到了美国的工作机会，搬过去待了一阵子。后来去了 Nokia Developer Days，那真是史前时代了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在那里，有人找我，说他们在东欧做了一个杂志阅读 App，经常崩溃。那会儿 iPad 刚出来，Steve Jobs 说它是出版业的救世主，大家都在做杂志 App。我一听觉得这是个不错的短期项目，就接了。我一打开代码，整个人都懵了。那是我见过最糟糕的 iOS 代码，整个项目只有一个文件，几千行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还是 Objective-C？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，是 Objective-C，而且他们把 Windows 当成 Tab 来用。我都不知道这能行。我很惊讶这居然能用，但感觉像个纸牌屋。我试着“外科手术式”地修补问题，但基本上是动一处、坏一片。最后我好不容易把它稳住了，就跟他们说，“这太疯狂了，我要重写”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们原本预计要半年，我说我一个月就能搞定，最后花了两个月，也不算差太远。接下来我就一直在解决各种 PDF 相关的技术问题。这个领域谈不上多性感，但每个领域里都能找到真正有挑战的点。比如一个 C 语言调用渲染PDF可能要30MB，但整个系统只有64MB，如果你不够小心、不够聪明，系统随时就把你干掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我那段时间完全沉迷在“把它做到极致”这件事上，比如屏幕旋转时页面的动画效果，这种细节我会反复打磨，花了远超合理的时间。所以原本一个月的活，最后干了两个月，但结果是好的。之后我又跟他们合作了一段时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来有个朋友给我发消息，说他在做一个杂志应用，PDF 那块特别难。我跟他说，我刚好做过，对方就问我能不能把代码给他，我说可以。先把那套杂志 App 里和 PDF 有关的部分抽出来，确认对方也没意见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后我突然想到，既然有人需要，为什么不试着卖给更多人？我用一个 WordPress 模板，硬改成能跑在 GitHub Pages 上。然后用fastlane流程最后得到一个Dropbox链接，里面有源代码。当天晚上我就发了条推文。一周之内，有三个人买了，大概两百美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在当时对我来说，这已经很不可思议了。不只是有人付钱，还有十封邮件在抱怨，说他们也想买，但这个产品还没有他们想要的功能。比如有人问，为什么不能选中文本？几个月后我才真正意识到，这功能到底有多难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：PDF 里的文本选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，尤其是这个。你知道那句话吗：公司是由年轻人建立的，因为他们不知道有多难。我当时完全没概念，后来才发现这简直是疯了一样复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;直到现在，前几周还有人给我写邮件，说他们在做 PDF 相关的事情，想找我帮忙。我基本都会回一句：不好意思，我已经把这辈子该懂的 PDF 知识都学完了，远远超过一个正常人该承受的量，祝你好运。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过当时，这个项目真的起飞了。我一边等签证，一边继续维护。买的人越来越多。那是夏天，我躺在湖边晒太阳，邮箱里突然又进来一封邮件，说又有人买了，六百美元、八百美元。随着功能变多，我不断涨价。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;等我真的去旧金山那家公司上班时，这个项目赚的钱，已经超过我在那里拿的工资了。但我那时的想法还是：我得去那家公司看看，于是还是去了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说你搬到了 San Francisco。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，而且很有意思的是，那家公司后来也让我用自己的框架帮他们做东西。创业公司当然不可能只干八小时，我的本职工作很忙，个人项目也一样，睡的自然越来越少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;三个月后，我的经理 Sabine 把我叫过去，问我一句话：“Peter，你还好吗？”公司给了我一个选择：要么继续在这家公司工作，把个人项目停掉；要么反过来。他给我一周时间决定，而且因为签证问题，如果不留下，就得离境。这个决定其实一点都不难。我很清楚，我想做自己的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那时候你已经看出来了，这是一个真正的生意，至少能给你带来和美国工作差不多的收入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我从来不是被钱驱动的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你真正的驱动力是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我想做那种让别人觉得“太棒了”的东西。我特别迷恋细节，迷恋那些小小的惊喜感。并不是因为这个领域没有竞争，相反，竞争很多。但我心里一直憋着一股劲：我要做一个像 Apple 自己会做出来的产品，充满关怀、打磨、克制，还有那些行业里很多人已经不在乎的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以哪怕有竞争对手功能更多、做得更早，我的产品依然更成功。因为开发者试过之后，都会觉得我的用起来最好。我一直觉得，产品的“感觉”比功能列表重要得多。我们为什么买 Apple？不是因为它功能最多，而是因为它用起来就是更舒服。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从卖组件到创建公司&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你是怎么从“一个人在卖 PDF 组件”，走到开始招人的？你什么时候意识到这件事可以做得更大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我回到维也纳之后，决定彻底 all in，开始和一些自由职业者合作。说实话，我招人其实招得太晚了，完全可以更早迈出这一步，但这一步真的很难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从那时起，这个产品开始有了自己的生命。我职业生涯里差不多有13年都在打磨这个名字奇怪的产品。名字我一直没改，当初想名字只花了几分钟，就叫 PSPDFKit。后来改过一次，但说实话，要不是不得不改，我可能还是不会动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：名字确实有点绕，但非常独特。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：如果你写 Objective-C，你就会觉得这个名字很合理，因为它本质上就是个命名空间。我的营销策略也一直很简单：我只关心开发者。虽然最终拍板的是管理层，但只要我能说服公司里的工程师，他们就会替我去内部推广、游说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们从来不做冷邮件，也不搞侵略式销售。所有客户都是自然找上门的。我们只做三件事：把产品做好、写真正有价值的技术博客，以及参加大量开发者大会。对我来说，最重要的是让大家明白，这个产品背后的人是真的懂技术、也真的热爱这件事。而这一点，会直接体现在产品里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：PSPDFKit 底层用的是什么技术？最早是 Objective-C 吗？后来转成 Swift？有没有用到 C 之类的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：一开始确实是 Objective-C，后来逐步覆盖到所有平台。真正一次大的转折，是我们把 Apple 自带的渲染器换掉了，那个东西当时问题很多，之后改成了一个大型的 C++ 渲染器，后来所有平台基本都共用这一套核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们在 Web 这块也做得非常早，是最早一批跑在 WebAssembly 上的 PDF 框架之一。当时我做了一件现在看来还挺聪明的事：在一切刚开始的时候，我们做了一个性能基准测试。后来这个 benchmark 被 Google、Microsoft、Apple 等公司拿去用，成了他们内部的参考指标之一。结果就是，这些大公司为了跑得比我们快，反过来不断把他们的渲染器优化得更快，而测试用的内容，其实就是我们自己的渲染场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;创业后，分享公司的“核心秘密”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：厉害。随着公司规模变大，我对 PSPDFKit 的一个深刻印象是，你们写了大量博客。记得有一篇文章，讲的是团队怎么运作：每个功能都要从 proposal 开始，因为 API 很大、用户很多，所以你们非常保守；还有类似 Boy Scout Rule 那样的重构原则。团队从十几个人发展到几十个人，这种文化是怎么建立起来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我卖股份的时候，公司大概七十人，现在已经接近两百人了。一开始我就很清楚，在维也纳不可能招到我需要的所有人，所以我们从一开始就是 remote first，后来又变成了一种混合模式，反而更复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多东西都是边走边学。我从来没有“我要当 CEO”的执念，我一直在写代码，我会找合适的人来帮我做公司的其他部分。业务我能做，也做得还可以，但我真的不喜欢那种企业销售电话，你得去琢磨一个“魔法数字”，看对方可能愿意付多少钱。这就是企业销售，真的很折磨。但说到底，这种模式可能是唯一行得通的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你是说企业销售本身？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：很多开发者去厂商官网，看不到价格，只看到“联系我们”或者“预约演示”，都会很不爽。为什么一定要这样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：原因很简单，我们会看你的公司情况，然后大概判断你能接受的价格，再定一个数。听起来确实很糟糕，但当你的产品没办法简单拆成一个统一定价时，这是现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个自由职业者，和一家财富五百强公司，用法完全不同，获得的价值也完全不同。如果统一收费，要么把小客户挡在门外，要么让大客户觉得价格可疑。价格定低了，大公司采购流程都走不起来；定高了，小团队直接流失。所以这个过程看起来不公平，但在某些产品上，反而是最公平的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件大致可以分成四个象限：容易或困难，有趣或无聊。我们处在“又难又无聊”的那一块。&lt;/p&gt;&lt;p&gt;如果只构建每个开发者都想构建的东西，卖起来一定很难。卖给开发者本来就难，如果一个东西既简单又有趣，那基本没戏。但如果是那种“我真不想碰，而且还特别难”的，反而是个好位置。我找到了这样一个细分领域，里面有无限多复杂问题可以解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那解析 PDF 到底难在哪？有规范啊，我是工程师，照着规范做不就行了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：举个最简单的例子。PDF 里有链接，比如目录，点一下跳到某一页。我一开始的假设是，可能有一两百个链接。我就按这个规模设计了整个数据模型。后来来了一个付费很高的客户，说他们的 PDF 打开要四分钟，我一看是一份五万页的文本圣经，每一页上有上百个链接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那就是五十万个链接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，我的模型直接爆炸了。假设差了三个数量级。但这时候你已经是一个成熟产品了，还有稳定的 API。你要怎么彻底重构内部，又不破坏所有用户？所有东西都得改成 lazy loading。以前加载100个对象没问题，现在不行了。我花了整整两个月重写内部结构，同时还要保证对外 API 看起来还是“简单的”。用户不需要知道哪些是立即加载的、哪些是延迟加载的，引用关系也必须保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这些引用必须还能连得上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。我其实非常喜欢做支持，这也是公司能成功的重要原因之一。如果你提一个工单，结果 CEO 直接回你，还帮你解决问题，那感觉是完全不一样的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直有个策略：支持一定要快。五分钟内回，和两天后回，体验差别巨大。这个问题就是其中一个例子，我花了两个月把它彻底解决，最后跑得非常顺，那种满足感真的很强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那时候你自己还写很多代码，对吧？虽然团队已经很大了，但你仍然会深入细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：当然。我有一支非常棒的团队，有些模块我参与得更多。移动端一直是我最上心的部分，但我也会深度参与技术、市场和业务。业务上我有 Jonathan 帮忙，市场和销售也有很优秀的人。其实，持续写博客、写你是怎么解决这些复杂问题的，会帮你吸引同样想解决复杂问题的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是我对 PSPDFKit 最深的印象之一。你们的博客不只是营销，而是真的好看。说实话，我并不做 PDF，但如果要说起 PDF 框架，第一个想到的就是 PSPDFKit，因为只有你们会写这么有意思的技术文章。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在回头看，会不会也觉得奇怪，为什么更多公司不这么做？还是说，这本来就需要创始人本身是个喜欢写、喜欢拆解问题的工程师？你当时写这些文章，是出于“这对公司有用”，还是单纯因为你自己想把解决过的难题记录下来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我喜欢分享，也喜欢启发别人。有时候团队内部也会纠结，要不要写这些内容，毕竟算是一些“秘密武器”，但我从来没太在意这些声音。还有一点很重要：写下来本身，就是加深理解。你觉得自己懂了，但当你要教别人时，才会发现自己是否真的懂。所以对我来说，这也是一种复盘和保存。我解决了一个很难的问题就想把它留下来，顺便帮到别人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我也享受关注。但更重要的是，有时候我自己过一年再回头看这些文章，会发现这就是公司最好的文档，是我自己的“技术笔记本”。它在很多方面都很有用。很多大公司流程太重，而且不少开发者本身不喜欢写东西，所以我后来干脆规定，每个月给所有人一整天，只干一件事：写一篇博客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那天不用干别的活，只写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，就写。一天的时间其实已经很多了，现在我写一篇文章也就几个小时。我不想过多谈论公司增长阶段，但我觉得公司最有意思的阶段，是刚开始以及快速成长的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来人多了，流程多了，更像是在“养护花园”，而不是疯狂 hack。事情变得更迭代化，也没那么刺激了。人一多，内部摩擦、情绪问题也多，这些我并不享受。那段时间我真的被烧干了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“停更”，赋闲&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得是什么让你最终人力交瘁的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我只是工作太猛了，几乎每个周末都在工作，还要处理大量管理事务。CEO 本质上就是“兜底的人”，凡是别人没处理好、处理不了、或者搞砸的，最后都得你来收拾。而且很孤独，你不能随便讲很多事情。哪怕公司已经很开放了，你也不能一直表达负面情绪，就算真的发生了很糟糕的事，你也得扛着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得有个周末，合伙人凌晨给我打电话，说一家大型飞机制造公司，因为我们的软件崩了，飞机停飞了。那是个非常“刺激”的周末，最后我拆了他们的应用，证明是他们外包代码乱改，触发了授权回退逻辑。但那种时刻，你会觉得公司随时可能完蛋，而这种压力只是所有压力中的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些事情你能撑一阵子。但我也相信，burnout 不完全是因为工作太多，更是因为你开始不再真正相信自己在做的事情，或者内部冲突太多。我们当时在管理团队里争论也很多，我还犯了一个错误，以为公司应该用一种过度民主的方式来管理。这些都消耗了我。但即便如此，我一点都不后悔这段经历。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：从外人的角度看，你卖掉了股份，赚到足够多的钱，按理说已经不用再工作了。很多刚起步、或者未来想创业的人，都会觉得这简直是终极梦想。既然已经“通关”，是不是就该停下来、享受生活了？现实是，大多数人走不到那一步。但一旦走到了，好像任务就完成了，就像攀岩爬到顶，敲响铃铛，游戏结束。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：外界看，你博客更新停了好几年。那段时间你在做什么？又学到了什么？也就是在你回归到现在之前，那几年到底发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我真的花了很长时间让自己“降压”，去填补那些我以为错过的人生体验，花了不少钱。有几个月，我甚至连电脑都没开过。那段时间，我完全没有“接下来该干嘛”的感觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，那种状态挺违和的，你这么早就“退休”，或者说有一个好到不需要再工作的退出，这件事本身就会把人搞懵。那几年对我来说，其实挺难熬的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来有一天，大概在四月，我突然想起一个很多年前只是当副业做过的项目，我心想还是想把它继续做完吧。于是，三年多之后，我重新坐回电脑前，开始写代码。那个项目是个 Twitter 分析工具，用 Swift 和 SwiftUI 写的。其实当年我就知道，这东西如果做成 Web 会好很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以这是一个你一直放在心里的老想法？跟 Twitter 有关的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，算是分析工具。最开始只是我自己想用，因为市面上根本没有。三年后再看，还是没有。现在勉强算有点类似的，但我中途也被别的事带跑了。我当时想用 Web 技术重写，但说实话，在公司里我从来没碰过那一块。那一整套技术栈一直是 Martin 在负责，他很厉害，所以我完全不用操心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以你其实一直没怎么亲手下场？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。等我再回来自己做的时候，我才发现，“哇，这一层真的很深”，而且这其实是个陷阱：你在某一套技术上越熟练，跳到另一套时就越痛苦。不是做不到，是太折磨人了。我在 Apple 那套技术里，闭着眼都能写代码；可一换栈，连最基础的东西都要去 Google，一下子就感觉自己又成了新手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：而且经验越多，越讨厌这种感觉。效率下降，明明知道自己本可以更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。所以我回来的时候就在想：那 AI 到底是什么？CI、AI 那些大家都在吐槽的东西，到底值不值得看一眼？老实说，我某种程度上反而要感谢那三年几乎没碰电脑的日子，因为你们那时候已经把 AI 看过一轮了，知道它当时有多烂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;回归即上手Claude Code，“上瘾了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，你错过了 GitHub Copilot 的早期测试版，那种“高级自动补全”的阶段。后来有了 GPT-3.5，再到 GPT-4，才是真正的飞跃。所以你回来之后，第一个用的是什么工具？你等于是直接跳过了两年开发者一边用、一边嫌弃 AI 的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是 Claude Code。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你一上来就用它？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。我记得它刚发布不久，之前就有 Beta。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，你休息了一段时间回来，直接打开 Claude Code，前面的演进全都没经历。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：没错。我记得我拿了一个以前写得很乱的副项目，又用我自己做的一个浏览器插件，把整个 GitHub 仓库转成一个 1.3MB 的 Markdown 文件。我把它丢进 Google AI Studio，用 Gemini 之类的模型，敲了一句：“给我写一份 spec。”它直接生成了四百多行代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我再把这份 spec 拖回 Claude Code，说一句“照这个做”，然后我去干别的事了。等我回来，它告诉我：“已经百分之百可以用于生产环境了。”我一跑，直接崩了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我又给它接了 MCP，让它能用浏览器，我记得 MCP 当时已经有了。它又跑了几个小时，最后居然做出了一个 Twitter 登录页，还能跑点流程。说实话，效果不算好，但它真的“做出了点东西”。那一刻对我来说，简直是被震住了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那是在去年四月、五月左右，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。已经好到让我看清方向了。我立刻意识到：这就是未来。从那之后，有好几个月我都睡不好觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得有一次我凌晨五点在 Twitter 上给你发私信，你马上就回了。我还问你怎么这么早，你说这是常态，你基本都没睡。我问你在干嘛，你说一直在用 Claude，特别上瘾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：真的，就跟赌场一个道理，它就是我的小老虎机。你敲下一个 prompt，要么啥也没发生，要么一坨垃圾，要么突然给你个让人头皮发麻的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：而且你是一个经验非常丰富的开发者，对你来说，被“震撼”并不容易。你见过好代码、烂代码，心里是有一个标准的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：所以才好笑。我以前在公司时，花了大量的时间在所谓“抠细节”上。现在回头看，我都会想：我当时在干嘛？客户根本感知不到这些。当然，代码要可靠、要快、要安全，这些是底线。但我当年真的抠太多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但另一方面，你刚才也说过，大家之所以喜欢 PSPDFKit，正是因为它打磨得最好、最稳定。你不觉得那种“抠细节”其实是在控制技术债吗？某种程度上，正是这种偏执才让产品性能和质量都站得住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的，这么说也没错。到现在我也还是这样。我上一篇博客，其实就是在“忏悔”，我承认我开始在主分支上直接提交 AI 写的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，我其实还是花了大量时间在做结构重构。就拿最近来说，我特别想把一个 PR 合进去，那是一条接近一万五千行的改动链。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一个项目里，我把所有东西都迁移到了插件化架构，这件事让我非常兴奋。我真的很在意整体结构。但我没有把每一行代码都读一遍，因为很多代码说白了就是枯燥的“管道工程”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你看，大多数应用本质上都差不多：数据从 API 进来，是一种形态；你解析、封装，变成另一种形态；存进数据库，又是一种形态；再读出来，又变一次；最后变成 HTML 或别的形式，你在页面里输入，它又变了。大部分软件，其实就是在应用里不断“揉捏”数据，我们本质上就是高级的数据搬运工，而真正难的部分，如Postgres 这种东西三十年前就被一群天才解决了。这就是现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，总会有一些有意思的地方，但我真的不需要关心每个按钮怎么对齐、每个 Tailwind class 怎么写。有些细节很无聊，有些细节很有趣，但整体来说，更重要的是系统架构，而不是逐行读代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;日常如何用AI 编程工具工作？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们跳到现在。你现在用 Claude 相关工具写代码时，日常工作流是怎样的？你用终端吗？几个终端？都用哪些工具？你刚才说你不太做逐行代码审查，但又一直在想做架构。如果你要跟一个即将加入团队的开发者解释，你的一天大概是什么样的，会怎么说？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：这个过程挺有意思的。稍微回顾一下，一开始是 Claude Code，然后我就彻底上头了。接着有一段时间我用 Cursor，又试了 Gemini 2.5，后来又用了 Opus 4。我还把不少朋友也拉进来了，比如我在越南认识的 Armin 和 Mario，他们都是被我“传染”的。我当时状态真的很上头，搞得他们也开始试，然后大家一起凌晨五点不睡觉。我把这群人戏称为“黑眼圈俱乐部”。这也是为什么我后来在伦敦搞了一个 meetup，名字就叫 Claude Code Anonymous。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正把我震住的，是一个认知上的变化：我突然意识到，我现在几乎什么都能做了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前做副业要慎重挑选，因为写软件真的很难。现在也不轻松，但那种“摩擦”感变了。过去是“我在这个技术栈里很强，在那个栈里很菜”，现在我会想：算了，直接上 Go 吧。我完全不懂 Go，但我有系统层面的理解。一旦你有了这种理解，就会慢慢形成一种感觉，知道什么是对的、什么是错的。这本身就是一种技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得有人发推说，写代码时你能“感觉到摩擦”，而正是这种“摩擦”帮你做出好的架构。我现在 prompt 的时候也有同样的感觉：我能看到代码刷刷地生成、能感知它花了多久、能感觉到模型是不是在“顶你”，也能判断生成的东西是乱的，还是有章法的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在发出 prompt 的那一刻，心里其实已经有个预期：这事大概要多久。如果明显比预期慢，我立刻就知道有问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你等于是在“感觉”模型的状态，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，我觉得这是一种共生关系。我在学着更好地“跟它说话”，甚至可以说是一种新的、半死不活的语言。同时，我用这些工具的能力在提升，模型本身也在进化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从四月到现在，我觉得真正的拐点是在夏天：那时它已经强到，你几乎可以不手写代码，就把软件做出来。但真正让我彻底服气的，其实是 GPT-5.2。我觉得它被严重低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我其实不太理解，为什么还有那么多人主要用 Claude Code。当然，我能理解那是一种不同的工作方式。但我现在用的这一套强得离谱，几乎每一个 prompt都能给我想要的结果。这在 Claude 上是很难想象的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我最近的一个项目常常在Codex 上同时跑五到十个 agent。如果你是典型的 Claude Code 用户，你得忘掉不少“为了哄它出好结果”的小技巧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也见过 Claude Code 团队，他们确实开创了一个新类别。Claude Code 是一个定义品类的产品，用来做通用电脑工作非常棒、用来写代码也很好，我现在几乎每天还在用。但一旦进入复杂应用的代码编写，Codex 就强太多了。Claude Code 往往只读三四个文件，就自信满满开始写代码，你得不断拉着它，让它多读、多看，理解整个代码库，才能把新功能编进去。Codex 则会安静地读文件，可能读十分钟。如果你只用一个终端，这体验确实会让人崩溃，我完全理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我更喜欢那种，你不用事无巨细地告诉它该怎么做，我和模型更像是在对话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我会说：“我们一起看看这个结构，有哪些可能性？你有没有考虑过这个功能？”因为每一次 session，对模型来说都是从零开始理解你的产品，你有时候只需要给它一点点提示，让它往不同的方向探索。我不需要什么Plan模式，只是一直聊，直到我说“那就这么建吧”，它才会真的开始动手。当然，它们都挺“容易被触发”的，但只要我说的是“讨论”“给我选项”，它就不会直接写代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以你大量的 prompting，其实是在和 agent 一起做规划？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。比如我会提醒它“我们需要文档，那放在哪里合适？”它可能会建议“这应该单独成一页。”系统设计是我在做的，因为我对产品整体形态有清晰的理解。我不需要逐行理解代码，那是 Codex 在做的事，但架构师是我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来有点像很久以前的一种模式：有一个“Architect”，以前也是开发者，但不再亲手写代码，而是负责系统蓝图，下面有一群工程师实现。这种模式在很多现代公司已经不流行了，大家更偏向资深工程师一起协作。不过在一些银行之类的地方，还是能看到这种“大写的 Architect”。问题是，这种模式往往很让人讨厌：设计的人不用值班，不直接为结果负责，最后在现实里容易失效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而你现在的状态，倒像是你是 architect，但手下是一群 agent。区别在于，你依然是独立贡献者，代码是你的、责任也是你的。如果你推了个 bug 把 ClawdBot 搞挂了，就像最近那次，你是要负责的。以前在公司里，architect 往往被流程和人层层保护，不太需要直接面对结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我其实不太喜欢“architect”这个词，我更愿意叫自己 builder。我发现，能不能把 AI 用好，人群之间差异非常明显。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像我关心的是结果、是产品，我很在意它的感觉、体验。我关心结构层面的骨架，但不会抠那些小细节。而另一类人，特别喜欢写硬核代码、研究算法，他们不太喜欢产品、市场这些东西。他们更享受解决“难问题”。而偏偏，这正是 AI 最擅长的部分，所以这类人往往会抗拒 AI，或者感到非常失落。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多时候，我只是给模型一点提示，但老实说，我去年在软件架构和系统设计上学到的东西，比过去五年加起来都多。这些模型里装着海量知识，一切都只差一个“问对的问题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;像我那个 Twitter 项目到现在还没完成，我也很希望能回去继续做。所有东西一度都曾跑得很好，但用着用着就开始卡、变得奇怪，然后又莫名其妙恢复。这类问题特别难 debug，因为很难复现。基本就是：你用得越多，它就越慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我发现，是 Postgres 里有一些在特定 insert 时触发的逻辑把数据库拖得很忙。模型看不到这一层，因为抽象太远了。问题出在一个文件里的一个函数，名字也不明显。我一直没问对问题，直到我问了一句：“这里有没有副作用？”才把它挖出来，然后改掉了。所以说，一切真的都只差在能否问一个对的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但前提是，你得有足够的知识和经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，这正是关键。那些对内部实现执念很深、又不太在乎“能不能先做出来”的人，往往会抗拒 AI；而那些更兴奋于“把东西做出来”的人，反而进展飞快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一点对我帮助很大：以前我开公司带团队，可以盯着每个人的代码，要求他们写成我想要的样子。但很多没管过人的开发者，没有学会放手，接受“这段代码不是我理想中的样子，但它能让我更接近目标”。不完美的地方，永远可以之后再改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我非常相信“迭代式改进”。当年在公司里，我就是花了很长时间学会一点点放手。所以，当我开始用 Claude Code 的时候，感觉就像我手下有了一群工程师：有时候很不完美，有时候甚至有点蠢，但偶尔又异常聪明。我需要引导他们，一起朝着一个目标前进。某种程度上，这感觉就像又当了一次老板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;高效率的秘诀&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：挺有意思的一点是，在之前，你用一种非常传统的方式做了十几年的软件，甚至不止十几年。你不仅把产品打磨得很扎实，也非常擅长带团队、设立高标准，对“工程本身”这件事非常在意。而现在，你用 agent、用 AI 写代码有一年左右的时间了。对比这两种阶段，你觉得真正改变了什么？又有哪些东西，其实并没有变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我不太喜欢“vibe coding”这个说法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你更愿意怎么称呼？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：现在大家基本都这么叫了吧。我自己对外会说，我做的是“Agentic Engineering”。现在我往往是凌晨三点开始写代码。那些枯燥、机械的编码工作基本都被自动化掉了，我的速度快了很多，但与此同时，我需要思考的事情也多得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我依然能进入那种心流状态，感觉和以前几乎一样，但精神消耗其实更大，因为我不是在管理一个工程师，而是同时管五个、十个 agent。我在不同模块之间来回切换：这边是一个子系统，那边是一个功能点，我心里大概知道这个功能交给 Codex 可能要跑四十分钟到一个小时，那我就先把方案想清楚再丢给它去做，然后我转头去做别的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个在跑、那个也在跑，我要过一会儿回来看看这个、再切到另一个，脑子里一直在做上下文切换。我其实挺不喜欢这种状态的，也觉得这是一个过渡期的问题。将来模型和系统更快之后，我可能就不用并行这么多。但为了保持 flow，我现在必须高度并行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通常会有一个主项目占据我的主要注意力，旁边还有几个“卫星项目”，可能我只花五分钟交代一下、它跑半小时，我回来看看结果就行，对脑力占用不算大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听你这么说，我想到两种画面。一种是那种经营类游戏，要管厨房里的员工，看着一道道菜出炉，你得不停切换。另一种是看国际象棋大师同时下二十盘棋，他们走到一块棋盘前看一眼，立刻做决定。有的棋要想久一点，有的扫一眼就走。你就像在不断扩展自己的“并行带宽”，只要你还能顺畅地切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：区别在于，用 Claude Code 的时候，你确实得换一种工作方式。它很快，但第一版产出经常是跑不通的。比如它写了点东西但你忘了同步改另外三个地方的话，程序就崩了。真正高效的秘诀在于：你必须把闭环做完整，让 agent 能自己 debug、自己测试。这是最大的秘密，也正是我后来效率暴涨的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但老实说，在 Claude Code 那一套下，很多时候你还是得回去修修补补，迭代次数也不少，所以总体并不一定快多少，只是更“互动”。现在用 Codex，几乎一次就对。我的基本策略永远是：做一个功能，一定让它写测试，确保能跑起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：至少要能跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。哪怕是写一个 Mac 应用也是一样。就像我前两天在 debug 一个问题：同样的 TypeScript 代码，在 CLI 里能找到远程网关，但在 Mac app 里不行。Mac app 的 debug 特别烦，你得编译、启动、点来点去才知道不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我干脆说：“你给我做一个 CLI 调试工具，走完全相同的代码路径，我可以直接调用。”然后就让它自己跑、自己改。它跑了一个小时，最后告诉我这是一个 race condition 和一个配置错误。听起来也很合理。我不需要亲眼看它怎么写代码，只要闭环跑通了就行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你其实是因为搭好了验证闭环，所以你信任它。这和在大公司里做项目有点像，所有测试都过了，并不代表百分百没问题，但已经是一个很强的信号了，至少有人替你想过、测过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：即便在我最新的项目里，也照样会有 bug。比如 Antigravity 在工具调用的循环格式上有些奇怪的行为，你得做过滤。我一开始被折腾了很久，后来突然意识到：我为什么不把这事自动化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我直接跟 Codex 说：“设计一套 live test，起一个 Docker 容器，把整个系统装起来，跑一个完整 loop，用指定文件里的 API key，然后让模型读一张图片，生成一张新图片，再反过来分析结果。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个过程跑得很慢，但它把我所有 API key 都测了一遍，从 Anthropic 到 OpenAI 再到 GLM，所有细节问题全修了，因为闭环是完整的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你说的“闭环”，本质上就是让 agent 能验证自己的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：没错。这也是为什么现在这些模型特别擅长写代码，但写创意内容反而一般，因为代码是可验证的：能编译、能 lint、能跑、能看输出，只要你设计得好，就能形成一个完美的反馈回路。我甚至会把核心逻辑都设计成可以用 CLI 跑，因为浏览器那一套循环太慢了，你要的是快速反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以有些东西其实没怎么变：比如后端、业务逻辑这种，本来就更容易验证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：反而有个挺反直觉的点：用这种方式写代码，会让你变成一个更好的工程师。因为你必须把架构想清楚，才能更容易验证，而验证正是把事情做对的关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这其实和 AI 之前是一样的。做复杂系统的人，一开始就会想怎么让它可测试、接口怎么设计、要不要 mock、要不要端到端测试。这些都是非常困难、而且一旦做了就很难改的决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：软件还是软件。我现在可以很坦然地说，我不再亲手写代码了，但我写的代码质量比以前更好。而以前我已经写得很好了。在公司那会儿，测试常常很痛苦，各种边界条件、分支爆炸。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：除了像 Anders 这种我非常尊敬坚持 test-first 的人，大多数开发者其实都不爱写测试。我自己也是。测试和文档对我来说从来不是一种创作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：现在完全不一样了。我最近一个项目的文档质量是我职业生涯里最好的，但我一行都没写。我只是跟模型讲清楚设计权衡：为什么这么做，然后让它写给新手看的部分，再在后面加上更技术化的细节，效果好得惊人。测试也是一样。每做一个功能，我就会自然地问：这个怎么测？如果换一种结构，是不是更好测？因为我脑子里始终只有一件事：怎么把闭环关上。模型必须能自己验证结果，这会反过来逼我做出更好的架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么开发者AI编程玩不溜？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得，为什么还有很多经验丰富的开发者，对 AI 这套东西依然很抗拒？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：前阵子我看到一篇博文，作者是我非常尊敬的人。他测试了好几个模型，其中甚至包括一些本来就不适合写代码的模型。他的做法听起来像是随便写个 prompt，在网页上点发送，拿结果就跑，甚至都不编译，结果当然很失望。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但问题是：你觉得自己第一次写代码就能没 bug 吗？这些模型，本质上是人类知识的幽灵。它们不可能一次就对，所以你必须有反馈闭环。你也不能只发一个 prompt，而是要开始一段对话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还抱怨模型用了旧 API。但你没告诉它 macOS 版本，它当然会默认用老 API。模型训练的数据里，旧数据本来就比新数据多。你越理解这些“小怪兽”是怎么思考的，你的 prompting 就越好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他可能只玩了一天，就下结论说这东西不行。这就好比你会弹吉他，我把你放到钢琴前，你随便敲两下说“这不行，我还是回去弹吉他吧”。这是另一种构建方式，另一种思维方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不知道我凌晨三点对着 Claude Code 吼过多少次。后来我慢慢搞明白了：它真的就是严格按我说的话在做事。甚至有时候你可以直接问它：你为什么这么理解？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在最近一个项目里，我感觉自己更像一个“人肉合并按钮”。社区很活跃，我几乎一直在 review PR。一开始它经常只 cherry-pick 一部分就关 PR，我被气得不行。后来我问它为什么，它会说：因为你之前这么说过，我就这么理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;慢慢地，我学会了这门“机器语言”，调整我的表达，现在它几乎每次都能给我想要的结果。这和任何技能一样，是可以练出来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这和 Simon Willison 说的也很像：用得越久，越能意识到自己还能做得更好。那我们来做个更极端的假设。你现在做的ClawdBot 很火、用户很多，但还不是像 PSPDFKit 那样直接承载大量收入的业务。如果今天 PSPDFKit 从世界上消失了，你要从零重建它，手上有现在这些 agent，你会怎么做？你会把什么交给 AI？什么一定要自己把控？团队结构会变成什么样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：今天的话，我大概用三成的人就能跑起一家公司。但前提是，这些人必须非常资深，既懂系统又敢于放手，知道哪些地方重要，哪些地方可以“vibe”一下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一点我在 AI 圈里其实不太常见。Twitter 上太多声音很大、但明显不知道自己在干什么的人，还有很多我觉得挺荒唐的概念。比如某些用来绕 Opus 限制的复杂流程，Codex 根本用不着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件开发很少是那种“列一个超长任务清单，然后全部自动执行”的问题。我看到很多人搭了一整套复杂的编排系统：自动建 ticket、agent 处理 tickets、agent 互相发邮件，最后搞出一团复杂系统。图什么？这本质上就是瀑布模型，我们早就知道它不好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，开发必须从一个模糊的想法开始。我甚至会故意少给 prompt，让 agent 先做点“不太对”的东西，因为可能八成都是垃圾，但那剩下的两成会给我新的启发，然后我不断迭代、塑形。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我得点它、用它、感受它。好软件需要“品味”，而这是 AI 现在最欠缺的部分。但好处是，现在做一个功能太容易了，不行就扔掉，或者重新 prompt。我的构建方式几乎总是向前的，很少回滚。就像雕塑一样：你拿着一块石头，一点点敲，慢慢地，形状就从大理石里浮现出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：回过头看软件工程的变化，好像有一个很明显的转折点。过去没有 AI、没有这些agent的时候，前期规划非常重要。你觉得现在这种变化，是因为写代码本身的成本大幅下降了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我现在还是会做规划，但投入的精力没以前那么多了。因为现在试错太便宜了，你可以直接做出来看看效果，再判断“这个形态行不行”“是不是需要微调”，甚至“干脆完全换一条路”。相比过去，这一切的成本低到一个程度，让整个过程变得更像是在玩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，就像以前哪怕是交给一个刚毕业的新人或者实习生，一件事也得花一两天。现在不是一天两天，而是分钟级。就算是比较长的任务，最多也就是十几二十分钟。而且你还不是干等着，一个任务在跑，另外几个也在并行跑，所以试错本身几乎不算浪费。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的。最早我在 Claude 里其实假设只有一个 agent，后来变成多个；一开始假设只有一个 provider，比如 WhatsApp，后来又变成支持多个。这种改动，如果是我自己手写代码，简直是灾难，要把逻辑贯穿整个系统重新织（weaving）一遍。但 Codex 花了大概几个小时就搞定了，这要是我自己来至少得两周。所以以前那种“前期一定要一次想对”的心态是现实所迫，现在我知道，很多东西是可以改的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也让技术债的处理变得轻松很多。你可以一边做，一边重新理解项目本身，一边调整你的思路。所以我其实不太相信那种“按 spec 写完，机器跑完就结束”的模式。你在真正开始构建之前，根本不可能完全知道自己要做什么。很多关键认知，都是在构建过程中才出现的，它们又会反过来影响系统最终的形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对我来说，这更像一个循环，你不是直线爬山而是绕着走，有时候还会偏离一下路径，但最终还是会到达山顶。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;ClawdBot 来了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们换个话题。你已经连续几个月几乎不间断地在做 ClawdBot。其实有一个想法很早就把你拉回来了，对吧？你一直想做一个“超个人化”的助理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，而且不是那种每天早上给你发“早安，这是你今天三件待办事项”的助理。&lt;/p&gt;&lt;p&gt;我想要的是一个真正理解我的东西，比如我见了一个朋友回家后它会问我：“刚刚那次见面感觉怎么样？”或者有一天提醒我：“你已经三周没给 Thomas 发消息了，我注意到他最近在城里，要不要打个招呼？”又或者它会发现某些模式，比如“你每次提到这个人语气都会变，为什么？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那是一种极度个人化的东西，几乎是反 CRM 的存在，有点像电影《Her》，但这确实是技术发展的方向。模型对文本的理解能力非常强，上下文越大它们看到的模式就越多。即便它们本质上只是矩阵计算、没有灵魂，但很多时候给人的感觉已经完全不一样了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当时我甚至为这个想法注册了一家公司，叫 Amantus Machina，意思是“有爱的机器”。但去年夏天我真正深入做的时候，发现模型还差一点。能跑起来也有一些惊喜，但整体上还站在我需求的边缘之外。不过这反而让我很兴奋，因为 AI 的进展太快了，我很清楚这个想法可以晚点再回来做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个判断是，我相信所有大型公司都在做个人助理。未来每个人都会有一个“最懂你的朋友”，它是台机器，了解你的一切、可以替你做事、能主动提醒你。当然，这会非常消耗算力，但凡是负担得起的人，都会想要一个。然后随着系统效率提高、芯片进步，这种能力一定会逐步下沉。这几乎是确定的趋势，而且现在已经能看到一些雏形了，比如 OpenAI 推出的一些偏生产力的功能。但现在算力还不够，把这种东西真正作为产品推出来非常难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且还有一个问题是，我其实更希望它跑在我自己的电脑上，数据真正属于我自己。把邮件、日历、约会软件全部交给 OpenAI 或 Anthropic，说实话，挺吓人的，很多人已经在把这些模型当作心理咨询师用了，而且效果出奇地好。它非常会倾听，能理解你的困扰，只要不是某些明显差劲的版本，它真的能提出很有洞察力的问题，哪怕只是帮你复述和反思，你都会感觉被理解了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我一直有这个助理的想法，只是当时技术还没到位。与此同时，我也做了很多别的有趣的东西。在职业里绕一点“vibe 的弯路”，不断给自己造工具，优化自己的工作流，这几乎是成为一个真正工程师的必经阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但“超个人化 agent”这个念头一直没消失。最近几个月，我终于开始认真把它做出来。一开始它的规模其实很小，我甚至叫它 WhatsApp Relay，本意只是通过 WhatsApp 触发我电脑上的一些操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我去摩洛哥给朋友过生日，一整天都在外面，就一直用 WhatsApp 跟这个 agent 聊天。它帮我指路、开玩笑，还能用我的身份给其他朋友发消息。那一刻我真的被震住了。最早的实现非常粗糙，我甚至没用正式的方式传图片，只是丢了个字符串，让它自己用工具去读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一次我随手发了一条语音，其实我根本没实现语音功能。结果过了半分钟，它居然回了我一条语音。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我问它怎么做到的，它说：你发了一个文件，我看了 headers，发现是 Ogg 格式，就用 ffmpeg 转了一下；然后我找你电脑上的语音识别工具，没装，但我发现了一个 OpenAI 的 endpoint，于是用 curl 调了接口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那一刻我真的觉得不可思议。这就是 Opus 的能力，它太“能自己想办法”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我开始彻底上瘾。我让它叫我起床，它跑在伦敦的 Mac Studio 上，通过 SSH 连到我在摩洛哥的 MacBook，帮我开音乐，因为我没回应就一直把音量调大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我还加了一个 heartbeat。这简直疯了，你每隔几分钟就给一个模型发“想点酷的事情，给我点惊喜”的 prompt，这可能是史上最贵的闹钟，但它真的“懂”了。我那段时间脚受了伤需要很早起床，却一直没回应，它的推理过程非常清楚：“Peter 没回复，他必须起床，不能再睡了。”我把这个东西给朋友们看，所有人都被吸引住了，觉得这太神奇了。我自己也一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我发到 Twitter 上，反而反响很冷，因为很多人完全看不懂这是什么。我感觉，这可能是一种全新的产品类别，大家还没有形成认知。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这有点像你当年第一次接触 iPhone 的经历。广告、电视、各种宣传你都看过了，但真正的变化，其实还是在你亲手用上它之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，必须得用。我真正全力投入也就是最近这几个月，一开始它还叫 VA Relay，后来我自己都觉得这个名字不对劲了，因为功能早就不止这些了，已经接了 Telegram，还有一堆别的东西，再叫 Relay 完全不贴切。所以我给它改了个名字，叫 ClawdBot。算是个内部玩笑，我很喜欢《Doctor Who》，而且这个名字域名更好，也更能解释这个产品是什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，我也在悄悄搭建我的“军队”。要让这套东西真正跑起来，核心原则就是：一切都得是 CLI。所以我写了大量 CLI 控制 Google、床、灯、音乐，所有东西都变成命令行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么是 CLI，不是 MCP&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那为什么是 CLI？为什么不是 MCP？你怎么看 MCP 这套东西？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：说实话，MCP 更像是一根拐杖。它最大的正面价值是逼着公司去开放更多 API。但整个设计思路本身是有问题的：你得在会话一开始，就把所有工具、所有函数、所有说明一次性塞进上下文，然后模型还得精确地构造一大坨调用参数，再接收一大坨返回。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题是，模型其实特别擅长用 bash。举个例子，你要一个天气服务，模型先问“有哪些城市”，接口一次性给你几百个城市；模型没法过滤，只能全吃进去。然后你再问“给我 London 的天气”，返回温度、风速、降雨、几十个你根本不关心的字段，最后上下文里全是垃圾信息。但如果是 CLI，模型可以直接用 jq，只拿它真正需要的那一小部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来问题并不是 MCP 本身，而是所有东西都必须提前塞进上下文。如果能按需发现、按需调用，理论上是能解决的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：现在大家确实在往这个方向做，但还有一个根本问题：你没法“链式组合”。&lt;/p&gt;&lt;p&gt;我不能写一个脚本说：“找出所有温度超过二十五度的城市，再过滤字段，再把结果打包成一个命令。”因为 MCP 本质上都是孤立的工具，没法脚本化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但这听起来更像是时间问题。就像现在我们做一个天气应用，本来就要选 API、比较价格、覆盖范围，然后再把不同 API 的结果串起来。这套事情在没有 AI 的年代已经解决过了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的，AI 时代迟早也会解决。只是形式还没定。我自己干脆写了个小工具，叫 Porter，用 TypeScript，把 MCP 转成 CLI，直接打包用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以你的结论是至少现在，CLI 的效率更高？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。ClawdBot 里我其实根本没直接支持 MCP，但通过 Porter，几乎可以用任何 MCP。你甚至可以在手机上说：“用 Vercel 的 MCP 做这个事情。”它会自己去网站找 MCP、加载、按需使用。而现在很多 MCP 方案，甚至还得重启 Claude Code，用户体验非常糟，所以我就一路把自动化堆起来，工作量非常大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Taylor 前几天还做了个视频，说“这个人疯了”，因为现在支持的东西已经多到离谱。但我自己在用 agent 的过程中只会不断冒出一个念头：我还想让它多做一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前段时间我干了一件“非常不理智”的事：我建了一个 Discord，把我的 agent 加了进去。有人给项目贡献了 Discord 支持，我当时其实很犹豫要不要合并，但最后还是合并了。结果就是我把一个拥有我电脑完整读写权限的 agent，扔进了一个公开的 Discord。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;把复杂度隐藏到让人觉得“理所当然”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来完全不像是个好主意。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，简直疯狂。但后来有人进来，看到我用它检查摄像头、做家庭自动化、帮我放音乐。我在厨房里，跟它说“看我的屏幕”，它就真的看到了。因为它有完整权限，可以点终端、替我打字、执行命令。你甚至可以对它说“做这个做那个”，它就照着屏幕操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在还在优化，理想状态当然是纯文本流，但现在这种方式已经能跑了，而且是后台默默在跑。任何体验过几分钟的人都会上瘾，项目的 star 数一周内从一百涨到三千多，我已经合并了500多个 PR。所以，我现在感觉自己就是个人肉 merge 按钮，整个人状态都有点散。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这正是它的美妙之处：技术本身消失了。你只是拿着手机，像跟一个朋友聊天。这个“朋友”无所不能：能访问你的邮件、日历、文件，能给你搭网站、能做行政工作、能爬网页、能给朋友打电话，甚至能帮你打电话给商家订位。我正准备合并通话功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你完全不用关心算力、上下文、子 agent。它们在后台疯狂运转，只为了让你觉得“一切都很简单”。我还有一套记忆系统，当然不完美，但已经足够让人觉得像是魔法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在我走在路上，看到一个活动，随手给 Claude 发张照片。它不仅能告诉我这个活动的评价，还会检查我日历有没有冲突、朋友有没有提过。因为它掌握了大量上下文，给出的回答，已经完全不是那种“各自待在小盒子里的工具”能比的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来你已经做出了 Apple 想让 Siri 成为、却始终没做到的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：老实说，我可能是 Anthropic 最好的销售。我都不知道有多少人因为 ClawdBot 去买了200美元的订阅，有些人甚至多开了一个账号。不是因为模型“浪费 token”，而是大家太爱用了，用得太频繁。而且由于复杂性被完全隐藏，他们根本感觉不到后台有多少子 agent 在忙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正难的地方在工程上：如何把复杂度隐藏到让人觉得“理所当然”。这才是魔法的来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但这也很有意思。你在架构上投入了这么多思考。现在这个项目已经跑了几个月，也确实爆了。在你脑子里，你是不是很清楚 ClawdBot 的结构？哪些地方该改、哪些地方要重构？你会不会开始担心内存、token、效率这些问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：Token 更多是 prompt 和 memory 结构的问题。说到底，这就是 TypeScript 在搬 JSON。大模型给我文本，我存盘；我再把文本发到 WhatsApp、Slack、Discord、Signal、iMessage，还有更多渠道在接入。现在结构确实有点乱，但本质上只是文本在不同形态间流动。有多 provider、多 agent、有 agent loop、有配置、有大量 plumbing，但没有哪一块是真的“难”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：更多是碎片化的复杂，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。真正的难点是：怎么让它“看起来像魔法”。我花了大量时间在安装和引导体验上。你只需要敲一行命令，我会检查你有没有 Homebrew、有没有 Node，自动安装依赖，兼容老版本；然后引导你选模型，能自动识别你已经装了什么，基本就是一路按回车。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接着你填个手机号，WhatsApp 就能直接用。我会问你要不要“给它起名字”，然后终端里会弹出一个 TUI，让你完成这一步。我还加了一个 bootstrap 阶段：模型一开始不会假装自己“有灵魂”，而是通过一轮对话慢慢理解你；然后它会把 bootstrap 文件删掉，生成 user.md、soul.md、identity 文件，记录你的偏好、价值观、内部玩笑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些文件不是静态的，它们会随着你们的互动不断演化。等这一切结束，你只是用 WhatsApp 跟它聊天，但你已经不再觉得自己是在跟“GPT 某个版本”说话，而是在跟一个真正的“朋友”交流。配置不需要你手改，因为 agent 能改自己。你甚至可以对它说“更新一下自己”，它就会拉最新版本、更新完再回来告诉你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是我说的魔法：当复杂度被隐藏到极致，体验才会真的发生变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这听起来其实很像你当年做 PSPDFKit 的思路，对吧？你把 PDF 那套复杂性完全“融”掉了，用户只需要直接用，旋转、编辑，一切都很自然地发生。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，甚至在当年的 API 层面就是这么想的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;你的工作流程，公司能套用吗&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们把话题拉回到软件工程本身。你现在做的已经是一个真实在跑的产品了，是生产软件，大家在用，你也在不断 merge PR。回头看 PSPDFKit 那样的公司，几十人、上百人的团队在维护成熟系统。基于你现在构建 ClawdBot 的方式、你用的这些工具，你觉得大型公司的软件工程方式会发生什么变化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我明显感受到一个割裂：像你这样的个人，AI 对生产力的提升是巨大的，你完全掌控；但在团队或公司层面，尤其是有大量历史代码的情况下，一切就慢很多。不是说他们不用 AI，而是感觉两个世界之间有一道鸿沟。你当过 CEO，你怎么看？这是结构性问题，还是只是时间问题，就像每一代新技术，先被一小撮人玩明白？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我觉得，大多数公司要高效采用 AI，会非常非常难，因为这不仅是工具问题，而是要求你重新定义“公司是怎么运作的”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你想想，在 Google 这种地方，你要么是工程师，要么是经理；你想顺手决定一下 UI 什么样？对不起，不行。要么你写代码，要么你做设计，角色边界非常清楚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在这个新世界里，你需要的是有完整产品视角的人，能把事情从头做到尾。这样的人数量会少得多，但要求极高：高自主性、高能力。极端一点说，公司规模可能只需要现在的三成。这听起来就很吓人了，经济上也一定会带来巨大的冲击，很多人会发现自己在这个新世界里找不到位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我一点都不意外，现有的大公司用不好 AI。他们当然也在用，但只是“用到一点”。要真正发挥作用，你得先来一次大重构，不只是代码层面的，也是组织层面的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在设计代码库，已经不是为了“对我来说顺不顺手”，而是为了“对 agent 来说顺不顺手”。我优化的是模型摩擦最小、跑得最快的路径，而不一定是我个人最偏好的写法。因为最终是它在跟代码打交道，不是我。我负责的是整体结构和架构，这部分我还是按自己的方式来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在所有东西都要“可被解析”。PR 在我眼里，越来越像是 Prompt Request。有人提了一个 PR，我很少直接在那个 PR 上改。我会先说声谢谢，理解这个功能想干嘛，然后拉着 agent 从这个 PR 出发，把功能按我理解的方式重新设计一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代码可能会复用一点，但更多是把“目标”传达清楚。有些 PR 在定位 bug 时确实很有帮助，但说实话，现在很多 PR 的整体代码质量在下降，因为大家在疯狂 Vibe Coding。可真正要把功能做对，还是得对整体设计有深刻理解，否则你连怎么引导 agent 都不知道，结果自然就会很糟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，没有一个完整的反馈闭环，质量肯定会出问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的，对我来说，这种方式效率极高。我记得在 PSPDFKit 的时候，一个 PR 可能要做一周，评论、来回切换上下文、等 CI 四十分钟……现在不一样了。我把代码丢给模型，它会主动提醒我“这个地方可能会影响到别的模块”。我自己也会有判断，然后我们一起把它“重塑”成符合我愿景的形态，再把代码织进去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，我现在写代码用的动词都变了，“把代码织进现有结构里”，有时候甚至要先改结构，才能让它装得进去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那如果你现在招一两个人，变成一个小团队，你觉得代码评审、CI、CD 这些东西会怎么变化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我其实没那么在意 CI 了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你以前在 PSPDFKit 可是非常在意这些的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：以前是，现在测试本身我还是在意的，但我用的是本地 CI。我现在有点“异端”了。&lt;/p&gt;&lt;p&gt;agent 会跑测试，我不想每次推个后端 API，都等十分钟 CI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但你已经在 agent 那里等了不少时间了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：只要本地测试过了，我就 merge。偶尔 main 会出点小问题，但通常很接近正确状态。&lt;/p&gt;&lt;p&gt;我现在管完整流程叫 “gate”。full gate 就是 lint、build、全测试跑一遍。它就像一道门，代码出去之前必须过这关。我甚至开始用 agent 的语言了：“提交之前，跑一下 full gate。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那如果多一个人一起做，你可能也不会做传统的 code review 了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我们不会讨论具体代码，而是讨论架构、讨论大的决策、讨论风格。比如最近有个 PR 加了语音通话功能，现在我可以直接对 ClawdBot 说：“帮我给这家餐厅打电话，订两个位置。”这功能很酷，但它是一个很大的模块，影响面很广。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我当时就有点犹豫：这是不是开始变成臃肿软件了？然后我又回到老套路：把它做成一个 CLI。我之前有个没做完的项目正好相关，于是我打开 Codex，说：“你看看这个 PR，再看看那个项目，能不能把这个功能织进去？”对，我又用了“织”这个词。对我来说，这已经成了一种工作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：就这么继续往前推了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对，就这么继续。我们能不能把这个功能织进 CLI 里？利弊分别是什么？然后他们会跟我说可以这样做、那样做，也会给我很坦诚的意见。听下来我会觉得，这个功能其实是适合放进项目里的，而且确实能带来一些如果做成外置 CLI 就拿不到的好处。但我心里还是会有警惕：我不喜欢臃肿，这会不会开始变成 bloatware？那能不能搞一个插件式架构？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一个用 AI 的“隐藏技巧”是：多引用别的产品。我经常直接跟 agent 说，你去看这个文件夹，我当初在那儿已经把问题解决过了；或者去看那个地方，之前的思路都在那里。这样它就能直接理解我当时是怎么想的，我也不用重新解释一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为如果我再解释一次，很可能反而会引入偏差，没法完全表达我脑子里的原始想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有个人叫 Shitty Coding Agent的项目，名字虽然这么叫，其实一点也不 shitty。他里面有一套插件架构，可以通过 Git 加载代码，而且全是 TypeScript。我就跟 agent 说，“你去看看这个文件夹、那个文件夹。”结果它受到启发，直接给我设计出了一套非常炸裂的插件架构。所以本质上还是一种直觉驱动的过程。我昨晚基本上就是在干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：听起来，你的整个工作流已经和传统方式完全不一样了。PR 在你这里的角色变了，CI 也不一样了，测试还在，但更重要的是反馈回路。你用的是“织代码”，而不是“写代码”，讨论的是架构和品味。这对我来说是一个非常大的转变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那我们假设接下来你要招一两个、三个工程师，把这个项目变成一个真正的团队，甚至一个业务,你会看重什么样的能力？如果现在有一个资深工程师，你会被什么样的品质吸引？你会期待他们做过什么项目，或者具备什么特质，才能适应、或者快速学会这种工作方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我会找那种活跃在 GitHub 上、做开源的人。更重要的是，我要感觉到他们“爱玩这个游戏”。在这个新世界里，学习方式就是不断尝试，它真的很像一个游戏：你越玩越熟练，就像学乐器一样，得一直练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在能做到这么快、这么高效，连我自己都觉得有点不可思议。前几天我一天之内提交了600多个 commits，简直疯狂。但它是能跑的，不是那种“看起来很糟”的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，这背后是大量的技能积累。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的，但真的很累，你必须去玩这些技术、去学习。一开始一定会很挫败，就像你第一次去健身房又累又痛，但很快你就会变强，你会感觉到工作流在加速，能明显看到进步，然后你就慢慢上瘾了。所以，一边玩，一边拼命干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你现在投入的时间，明显比以前多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我从来没像现在这么拼过。就算当年我有公司，也没这么拼。不是因为我必须这么做，而是因为这件事太上瘾、太好玩了。再加上现在正好有势能，有一群人在推着我往前走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;年轻人的建议&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：是不是也因为你商业嗅觉很好？你能看出来什么时候有机会、什么时候窗口期打开了。&lt;/p&gt;&lt;p&gt;你刚才提到，现在“公开做事”这件事本身就很新颖。你也说，就算你现在想招人，其实也很难，因为真正公开、高频使用这些工具的人并不多。但可能两三年后，大家都会这么做，这个优势也就没了。还有一群人很焦虑的，是应届生、没什么经验的新人。毕竟你是在成为资深工程师之后，AI 才出现的，你有大量积累可以借力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把自己放回到那个阶段，基于你现在知道的一切，你会建议他们去做什么？是打好软件工程基本功，还是直接拥抱 agent，还是两者结合？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我会建议他们保持无限的好奇心。毫无疑问，进入这个市场一定会更难，你必须通过不断做东西来积累经验。我不觉得一定要写很多代码，但你得去接触复杂的开源项目，去读、去理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你现在有一台无限耐心的机器，可以把任何东西给你讲清楚，你可以不停地问：为什么要这么设计？为什么当初要这么做？慢慢建立起系统级理解。但这一切都依赖真实的好奇心，而我不觉得现在的大学真的很擅长教这个。通常，这种能力都是在痛苦中学会的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对新人来说不会轻松，但他们也有一个优势：他们没有被“旧经验”污染。就像小孩子一样，他们会用 agent 做出我们根本想不到的事情，因为他们不知道“这事不该这么做”。而等他们这么做的时候，往往已经能跑通了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：而且他们身边的朋友也都在用这些工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：对。前几天我有个小的菜单栏应用，用来统计 Cursor、Claude Code 这些成本，跑得有点慢。我本能反应是：好，打开 Instruments，开始点。结果他们直接在终端里把 profiling 全做了，连 Instruments 都不用开，就直接给我提了优化方案，还顺带把性能搞快了。我完全被震住了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得我们可能低估了进入这个行业的年轻人的能力和资源整合水平。很多伟大的公司创始人都非常年轻，当时经验也不多，但热情极强。对我来说，最冲击的还是你提到的这些变化：不再依赖 PR，不做传统 code review。这些东西陪伴了你十五年以上，也是 PSPDFKit 成功的重要基石。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：是的，现在需要一整套新东西。哪怕现在有人给我提 PR，我更关心的其实是 prompt，而不是代码。我会让大家把 prompt 也一起提交，有些人会这么做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我读 prompt 的时间，比读代码还多。因为那是更高信号的信息：你是怎么得到这个结果的？你问了什么问题？中间做了多少引导？这比最终代码本身更能帮我判断质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果有人想要一个新功能，我甚至直接要一个“Prompt Request”。你把需求写清楚，我就能把 issue 指给 agent，让它直接去做。真正的工作，其实是在想清楚系统应该怎么运作、细节是什么。如果别人已经帮我把这些想清楚了，我基本可以直接说一句“build”，然后它就能跑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，如果有人只提了一个很小的修复 PR，我反而会请他们别这么做，因为我花在 review 上的时间，可能是我直接在 Codex 里敲一句“fix”再等几分钟的十倍。现在我们已经可以有一行命令就启动。但在最近两周项目开始真正有热度之后，我干脆让大家直接用 agent 指向仓库来做配置。所以我们没有传统意义上的 Onboarding，而是 Claude Code 驱动的 Onboarding。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的 agent 会自己 clone 仓库、读文档、写配置、帮用户把环境全搭好，甚至设置 Launch Agent，全程不需要人工步骤。这在以前完全不可想象，但现在不是优先级问题了，因为 agent 可以替你做这些事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且这个产品本身就是 agent 构建的，所以它的结构、命名方式，完全符合 agent 的“直觉”。模型权重里本身就编码了某些对命名和结构的偏好，它在这个项目里导航起来非常顺。所以我没有把太多精力放在 Onboarding 上。以后我当然也想做成很魔法的体验，但当下更重要的是信息传得通、系统别炸。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;小彩蛋&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：好，那我们用几个快问快答收尾。第一个：有没有一个你会推荐的工具？不是 CLI，也不是 IDE，可以是实体设备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：我买过很多小玩意，大多数都挺一般。但有一个不贵、看起来也挺糙的东西，给了我几乎无限的快乐。它是一个用 Android 跑的电子相框，可以上传照片。它有一个邮箱，朋友可以直接给它发照片，之后就会自动显示。我家里放了好几个。技术上说，它很糟糕，动画也很简陋，但它就是不停地给我展示生活中那些快乐的瞬间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它大概两百美元，但说实话它给我的满足感，比我新买的 iPhone 还大。我买了 iPhone 17，到现在都还没拆封，因为我一想到要换卡、迁移数据就觉得太麻烦，完全没有“非换不可”的理由。但这个小相框，真的让我很开心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在科技之外，有什么事情能让你充电、让你远离屏幕？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Peter：健身房，最好是和教练一起，把手机锁在柜子里。那一个小时里，我完全活在当下，没有通知，没有冲动去摸手机。有时候我甚至出门散步，把手机直接留在家里。一开始会非常恐慌，好像手机已经变成身体的一部分了，但这种感觉反而让我觉得特别爽。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8lF7HmQ_RgY&quot;&gt;https://www.youtube.com/watch?v=8lF7HmQ_RgY&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/XW07axk5vWGgzz4bFqol</link><guid isPermaLink="false">https://www.infoq.cn/article/XW07axk5vWGgzz4bFqol</guid><pubDate>Fri, 30 Jan 2026 07:30:53 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>就差两个字符！亚马逊云科技自家 GitHub 仓库险被攻破，供应链安全亮红灯</title><description>&lt;p&gt;亚马逊云科技近日发布了一则安全公告，确认其部分由亚马逊云科技管理的热门开源 GitHub 仓库存在配置问题。该高危漏洞被命名为 CodeBreach，可能导致恶意代码被引入仓库，甚至使依赖 AWS CodeBuild 的仓库遭到接管。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Wiz Security 的研究团队发现，一部分仓库在为 AWS CodeBuild&amp;nbsp;&lt;a href=&quot;https://www.wiz.io/blog/wiz-research-codebreach-vulnerability-aws-codebuild&quot;&gt;配置 Webhook 过滤规则&lt;/a&gt;&quot;时，使用了正则表达式来限制可信的触发者 ID，但这些过滤规则并不充分，导致攻击者可以利用可预测获取的 actor ID 获得管理员权限。此次受影响、并对 AWS Console 供应链构成风险的仓库共有四个，分别是：AWS SDK for JavaScript v3、通用加密库&amp;nbsp;&lt;a href=&quot;https://github.com/aws/aws-lc&quot;&gt;aws-lc&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/corretto/amazon-corretto-crypto-provider&quot;&gt;amazon-corretto-crypto-provider&lt;/a&gt;&quot;，以及&amp;nbsp;&lt;a href=&quot;https://github.com/awslabs/open-data-registry&quot;&gt;awslabs/open-data-registry&lt;/a&gt;&quot;（一个可通过 AWS 资源访问的公开数据集仓库）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Wiz 的漏洞研究员&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/yuval-avrahami-25139416b/&quot;&gt;Yuval Avrahami&lt;/a&gt;&quot;&amp;nbsp;与 Wiz 漏洞研究负责人&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/nir-ohfeld-b534b010a/&quot;&gt;Nir Ohfeld&lt;/a&gt;&quot;&amp;nbsp;解释称：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;该漏洞源于仓库中 AWS CodeBuild CI 流水线在处理构建触发条件时存在一个极其细微的缺陷。正则表达式过滤规则中仅仅缺失了两个字符，就足以让未认证的攻击者进入构建环境，并泄露高权限凭据。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，用于校验哪些 GitHub 用户可以触发构建的 ACTOR_ID 过滤规则，缺少了起始符号（^）和结束符号（$），这使得任何只要在用户 ID 中“包含”受信任 ID 作为子串的用户，都可以绕过限制。由于 GitHub 用户 ID 是按顺序分配的，研究人员通过创建自动化的 GitHub App，从构建缓存中捕获凭据，最终获得了受影响仓库的完整管理员权限。鉴于 AWS SDK for JavaScript 被打包进 AWS Console，一旦攻击成功，可能会危及无数 AWS 账户所依赖的控制台供应链。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;亚马逊云科技在确认漏洞存在并感谢 Wiz Security 研究团队发现该问题的同时表示，其他由亚马逊云科技 管理的开源仓库并不存在类似的错误配置。受影响仓库中的问题在首次披露后的 48 小时内即已完成修复。Avrahami 与 Ohfeld 进一步指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这一问题延续了近年来多起供应链攻击中常见的模式，例如 Nx S1ngularity 事件：细微的 CI/CD 配置错误，却可能引发影响巨大的攻击。就在去年 7 月，一名威胁行为者还曾滥用类似的 CodeBuild 问题，对 Amazon Q VS Code 扩展的用户发起供应链攻击。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着此类攻击日益频繁，Wiz 呼吁各组织进一步加固自身的 CI/CD 流水线，确保所有基于 ACTOR_ID 的访问控制都经过严格限定与正确配置，仅允许白名单中的身份触发构建。Reddit 用户 hashkent&amp;nbsp;&lt;a href=&quot;https://www.reddit.com/r/aws/comments/1qeh9wh/codebreach_supply_chain_vuln_aws_codebuild/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;现在感觉要保护源代码变得越来越难了，外面的世界真的有点吓人。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一事件以及近期的其他攻击，再次凸显了一个重要原则：绝不能让不受信任的贡献触发具有高权限的 CI/CD 流水线。The Duckbill Group 首席云经济学家 Corey Quinn 也&lt;a href=&quot;https://www.lastweekinaws.com/newsletter/&quot;&gt;评论&lt;/a&gt;&quot;称：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是过去一年里第二起重大的 CodeBuild 安全失误了。那边是不是“水土有问题”？友情提示：如果连亚马逊云科技都没能把自家安全配置好，你可能更应该好好检查一下自己的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据披露，CodeBreach 漏洞最早由 Wiz 于 8 月 25 日向亚马逊云科技报告。亚马逊云科技随后在 8 月 27 日为存在问题的 actor ID 过滤规则补齐了锚点，并吊销了 aws-sdk-js-automation 的个人访问令牌。9 月，亚马逊云科技 还进一步加固了安全措施，以防止非特权构建通过内存转储方式访问项目凭据。不过，该事件直到 1 月 15 日才正式对外公开。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-github-vulnerability/&quot;&gt;https://www.infoq.com/news/2026/01/aws-github-vulnerability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Uo24zvaQqEsDyK752BYQ</link><guid isPermaLink="false">https://www.infoq.cn/article/Uo24zvaQqEsDyK752BYQ</guid><pubDate>Fri, 30 Jan 2026 06:00:00 GMT</pubDate><author>Renato Losio</author><category>亚马逊云科技</category><category>云安全</category></item><item><title>如何破解大模型生产级部署「最后一公里难题」？</title><description>&lt;p&gt;破局大模型落地最后一公里：AI Serving Stack荣膺2025中国技术力量年度卓越奖 #2025年度中国技术力量卓越奖&lt;/p&gt;
</description><link>https://www.infoq.cn/article/CDqEUTowOKzrfddKqqbK</link><guid isPermaLink="false">https://www.infoq.cn/article/CDqEUTowOKzrfddKqqbK</guid><pubDate>Fri, 30 Jan 2026 04:00:00 GMT</pubDate><author>郭佳浠</author><category>AI&amp;大模型</category></item><item><title>DoorDash通过多臂老虎机增强A/B测试</title><description>&lt;p&gt;DoorDash工程师Caixia Huang和Alex Weinstein说，尽管实验至关重要，但传统A/B测试可能过于缓慢且成本高昂。为了消除这些限制，他们&lt;a href=&quot;https://careersatdoordash.com/blog/experimentation-at-doordash-with-a-multi-armed-bandit-platform/&quot;&gt;采用了“多臂老虎机”（MAB）方法来优化实验&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们的实验目标是，最小化因向用户子集提供效果较差的功能变体而造成的机遇成本或遗憾。传统A/B测试依赖于固定的流量分割和预先确定的样本大小，并且在整个实验过程中保持不变。这样做的结果是，即使早期出现了明显的优胜版本，实验也会继续进行，直到达到预定的停止条件。更糟糕的是，随着同时进行的实验增多，机会成本会不断累积，而鼓励团队按顺序开展实验以减少遗憾则会显著减慢迭代速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多臂老虎机方法提供了一种基于性能自适应分配流量的方法，可以在加速学习的同时减少浪费。其基本工作原理是：它反复在多个选项（仅部分属性已知）中做选择，并随着实验进行收集到更多证据时细化这些选项：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;就我们的目的来说，这种策略根据实验期间收集的持续反馈将实验流量分配给表现更好的功能变体。其核心思想是：自动化的多臂老虎机（MAB）代理会不断地从一组动作池（即多个操作选项）中做选择，从而最大化预设的奖励值，同时在后续迭代中通过用户反馈不断学习优化策略。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种策略实现了探索（即了解所有候选选项）和利用（即优先考虑最佳表现选项）之间的平衡，直到实验收敛到最佳选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照Huang和Weinstein的说法，MAB有助于降低实验成本，方便快速评估许多不同的想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash的MAB方法核心是汤普森采样，这是一种贝叶斯算法，以其卓越的性能和对延迟反馈的鲁棒性而闻名。简而言之，该算法通过从后验奖励分布（即决策周期结束后）采样来决定资源分配，并在新数据涌入时更新奖励预期以准备下个决策周期。在每个决策周期中，预期奖励将被用于确定选项分配方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DoorDash工程师表示，采用MAB方法并非没有挑战。特别是，对奖励函数中未包含的指标进行推断变得更加困难，而这反过来又在鼓励团队选择更复杂的奖励指标，以便捕捉尽可能多的洞察。相比之下，传统A/B测试允许在实验结束后对任何指标进行事后分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，由于MAB会更积极地调整分配，所以它可能会导致用户在多次与同一功能进行交互时产生不一致的用户体验。DoorDash计划通过采用上下文老虎机、利用贝叶斯优化和实施粘性用户分配来解决这些限制，增强整体用户体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多臂老虎机的概念来自概率论和机器学习。它使用老虎机的类比描述了这样一个问题：一个赌徒面对多个老虎机，必须决定玩哪个，多久玩一次，以什么顺序玩，以及何时尝试另一台机器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/multi-armed-bandits-doordash/&quot;&gt;https://www.infoq.com/news/2026/01/multi-armed-bandits-doordash/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/X7i8hMBrPd7hKvgkwKaC</link><guid isPermaLink="false">https://www.infoq.cn/article/X7i8hMBrPd7hKvgkwKaC</guid><pubDate>Fri, 30 Jan 2026 03:06:22 GMT</pubDate><author>Sergio De Simone</author><category>可观测</category></item><item><title>Ramp构建的内部编码代理支撑着30%的工程拉取请求</title><description>&lt;p&gt;Ramp&lt;a href=&quot;https://builders.ramp.com/post/why-we-built-our-background-agent&quot;&gt;分享&lt;/a&gt;&quot;了Inspect的架构。在公司前后端存储库的合并拉取请求中，这个内部编码代理的采用率迅速达到了约30%。这家金融科技公司分享了一份详细的技术规范，解释他们如何创建了一个系统，使AI代理能够像人类工程师一样访问开发环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Inspect的真正创新之处在于，它让编码代理能够完全访问Ramp的所有工程工具。与只让代理编写基本代码不同，Ramp的系统在Modal的沙盒虚拟机中运行，能与数据库、CI/CD管道、监控工具（如Sentry和Datadog）、功能标志以及Slack和GitHub等沟通平台无缝衔接。该代理可以编写代码，并通过工程师每天使用的测试和验证流程来确保代码可以正常运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp的工程团队表示，这种验证循环标志着对旧式代码生成工具的重大变革。该代理可以运行测试，检查监控仪表板，查询数据库进行验证，并参与代码审查。这有助于弥补影响许多AI编码助手的验证不足问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp选择基于Modal的基础设施进行构建，这对Inspect的性能特征至关重要。该平台能近乎即时地启动会话，并支持无限并发会话。这使得多名工程师可以同时使用独立的代理实例而不会导致资源争用。Modal的沙箱隔离功能与文件系统快照机制确保了代码执行的安全性，并且支持快速迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该架构使用Cloudflare Durable Objects进行状态管理。在交互过程中，这可以保持对话上下文和开发会话状态稳定。这种有状态的设计有助于代理跟踪它们的工作，就像人类工程师在开发时记住代码库一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp实现了多个客户端接口，使Inspect能够从不同的工作流程中访问。工程师可以使用许多工具与代理互动：用于快速聊天的Slack机器人，用于详细任务的Web界面，以及用于编辑可视化React组件的Chrome扩展。这种多模态方法表明，不同的任务需要不同的交互模式才能发挥最佳的效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该系统支持团队成员之间协同工作。他们能够同时观察并引导智能代理的操作。这一功能解决了人们对自主编程工具的普遍担忧。它既能确保有效的人工监督，又能让团队受益于自动化带来的效率提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp明确主张构建而不是购买现成的编码代理解决方案。其工程团队相信，与商业产品相比，自主工具可以实现更强大的集成。这主要是因为内部工具可以与外部供应商无法触及的专有系统、数据库和工作流程深度连接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司承认，这种方法需要大量的工程投资。Ramp分享了详细的实施规范，希望能够给他人带来一些启发。其中包括执行环境、代理集成模式、状态管理和客户端实现的细节。这显示了Ramp的自信，即竞争优势来自于执行，而不是隐藏架构细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最引人注目的也许是Inspect的部署，Ramp并没有强制推广。合并拉取请求占比30%是工程师自主选择采用代理的结果。他们发现，它在质量、速度或便利性方面均可媲美手动编码。持续增长的趋势表明，人们越来越了解该系统的能力与局限性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其团队还指出，Inspect简化了代码贡献工作。它为非工程师提供了与专业开发人员相同的工具访问权限。也就是说，该代理可能允许产品经理、设计师和其他人直接添加代码。这可能会改变跨职能团队的合作方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ramp的工程团队知道，会话速度和质量仍然主要取决于模型的智能程度。即使有最好的工具和设置，编码代理也会受限于当今的语言模型。这些模型仍然会犯错误，会产生幻觉，难以进行复杂的推理，并且需要人类监督。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司知道，他们的构建而非购买的建议可能不适用于每个组织。要实现类似的系统，需要强大的AI基础设施技能和工程资源。规模比较小的团队或有些组织可能没有那么多资源，或者认为不值得投入那么多资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着编码代理的不断演进，Ramp的技术规范和采用指标提供了清晰的数据支撑。这有助于企业评估自身的自动化战略。研究表明，在具备适当环境、工具和验证机制的前提下，AI编码代理能够大规模地提升工程生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/ramp-coding-agent-platform/&quot;&gt;https://www.infoq.com/news/2026/01/ramp-coding-agent-platform/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/gf8Vj1s4sAA0svzVYke3</link><guid isPermaLink="false">https://www.infoq.cn/article/gf8Vj1s4sAA0svzVYke3</guid><pubDate>Fri, 30 Jan 2026 03:00:49 GMT</pubDate><author>作者：Claudio Masolo</author><category>软件工程</category></item><item><title>谷歌通用商务协议（UCP）赋能智能代理购物</title><description>&lt;p&gt;谷歌推出了&lt;a href=&quot;https://developers.googleblog.com/under-the-hood-universal-commerce-protocol-ucp/&quot;&gt;通用商务协议&lt;/a&gt;&quot;（UCP），这是一个旨在提升AI驱动平台商业体验的开源标准。UCP为智能购物提供了一种通用语言，可以实现消费者、企业与支付服务商之间的无缝交互。该协议既能与现有的零售基础设施集成，又能通过智能支付协议（&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol&quot;&gt;AP2&lt;/a&gt;&quot;）保障支付安全，并通过API及Agent-to-Agent通信（&lt;a href=&quot;https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/&quot;&gt;A2A&lt;/a&gt;&quot;）为企业提供了灵活的连接方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP是与Shopify、Etsy、Wayfair、Target和沃尔玛等主要的行业参与者合作开发的，并得到了全球20多个合作伙伴的支持。该协议旨在服务于整个商业生态系统：商家可以控制自己的产品和结账流程，AI平台可以快速上线商家，开发者可以基于中立的开放标准进行构建，支付提供商获得互操作性，消费者享受流畅的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该协议解决了当今商业基础设施面临的一个主要挑战：N乘以N的集成问题。传统系统需要为每个平台或销售渠道建立单独的连接，这可能会减慢智能代理购物体验的推广速度。UCP提供了一个安全层，标准化了从产品发现到结账和订单管理的整个商业工作流程。它允许代理动态发现商家能力和可用的支付选项，支持多种通信方法（包括API、Agent2Agent和MCP），而且，为了与广泛的支付提供商合作，实现了支付工具与处理程序的分离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，UCP允许智能代理通过标准化请求访问商家的服务、启动结账流程并应用折扣。谷歌的参考实现通过搜索功能的AI模式和Gemini等AI界面展示了这一功能，支持使用谷歌钱包或其他兼容的支付方式完成购买。企业可通过商家中心账户访问库存信息，不用单独集成每个平台即可让智能代理访问他们的产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在LinkedIn上，首席创新官兼AI大使Andy Reid提出了一个&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7417105773681614848?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418624068054052864%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287418624068054052864%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&quot;&gt;问题&lt;/a&gt;&quot;，探讨这对小品牌的潜在影响：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果UCP允许Gemini将整个购物旅程压缩成一个“支付”按钮，这是否加速了向“默认经济”的转变？在这种经济模式下，每一项交易只有一个品牌作为最终答案出现。如果协议倾向于最相关“默认选项”而不是提供多种选择的市场环境，小品牌该如何生存？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌AI负责人James Massey&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7417105773681614848?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418624068054052864%29&amp;amp;replyUrn=urn%3Ali%3Acomment%3A%28activity%3A7417105773681614848%2C7418679761016619008%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287418624068054052864%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&amp;amp;dashReplyUrn=urn%3Ali%3Afsd_comment%3A%287418679761016619008%2Curn%3Ali%3Aactivity%3A7417105773681614848%29&quot;&gt;回应&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andy Reid，这个角度很有趣。我认为，尽管“支付”按钮简化了最后一步，但UCP是作为一个开放标准而不是封闭市场来设计的。在我看来，这实际上可能对小品牌更有利。使用UCP让自己成为AI代理的“可发现”对象，可以帮他们节省传统搜索所需的巨额广告预算，如果他们的产品是最相关的，无论品牌大小，协议都允许Gemini将它们作为主要选项呈现。实际上，这和数据质量有关。不过，我认为需要过段时间才能看到结果！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UCP的开放架构使开发者能够探究付款、折扣和订单管理等功能，使用基于Python的SDK进行快速实现。谷歌及其合作伙伴旨在通过共享的代理商务标准简化集成并增强消费者和商家的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通用商务协议已作为开源规范发布于GitHub平台。该项目鼓励人们通过提交拉取请求和参与讨论来为社区做贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/google-ucp/&quot;&gt;https://www.infoq.com/news/2026/01/google-ucp/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/48dPcqMKE6lcxtp9OLH6</link><guid isPermaLink="false">https://www.infoq.cn/article/48dPcqMKE6lcxtp9OLH6</guid><pubDate>Fri, 30 Jan 2026 02:58:14 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>Google</category><category>云计算</category></item><item><title>Rust 重写代码格式化器，Oxfmt 宣称比 Prettier 快 30 倍，前端工具链要“统一口径”了？</title><description>&lt;p&gt;&lt;a href=&quot;https://voidzero.dev/&quot;&gt;VoidZero&lt;/a&gt;&quot;&amp;nbsp;近日宣布推出&amp;nbsp;&lt;a href=&quot;https://voidzero.dev/posts/announcing-oxfmt-alpha&quot;&gt;Oxfmt&lt;/a&gt;&quot;&amp;nbsp;的 Alpha 版本。这是一款基于&amp;nbsp;&lt;a href=&quot;https://rust-lang.org/&quot;&gt;Rust&lt;/a&gt;&quot;&amp;nbsp;实现的代码格式化工具，面向 JavaScript 与 TypeScript 项目，目标是在大幅提升性能的同时，保持与 Prettier 输出结果的高度一致。作为 VoidZero 更大规模 Oxc 工具链计划的一部分，Oxfmt 在官方测试中展现出比 Prettier 快 30 倍以上的格式化速度，同时与&amp;nbsp;&lt;a href=&quot;https://prettier.io/&quot;&gt;Prettier&lt;/a&gt;&quot;&amp;nbsp;的兼容度超过 95%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Oxfmt 试图解决 JavaScript 生态中一个长期存在的矛盾：性能与习惯之间的冲突。一方面，Rust 工具链在性能上优势明显；另一方面，Prettier 已成为事实上的格式化标准。Oxfmt 将两者结合，既利用 Rust 带来的性能提升，又严格对齐 Prettier 的格式化风格，使其可以作为 Prettier 的“即插即用”替代方案，开发者迁移时几乎不需要承受格式差异带来的成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Oxfmt 的开发动机，部分来自 VoidZero 在 2025 年初发布&amp;nbsp;&lt;a href=&quot;https://oxc.rs/docs/guide/usage/linter.html&quot;&gt;Oxlint&lt;/a&gt;&quot;&amp;nbsp;之后收到的大量用户反馈。根据官方公告，用户反复提出对“样式类能力”的需求，例如 import 排序。VoidZero 团队对此采取了明确的工具边界划分：Lint 工具负责逻辑问题，Formatter 只关注代码风格。通过同时提供 Oxlint 与 Oxfmt，团队希望减少配置复杂度，并避免在多个工具之间反复关闭重叠规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在性能方面，官方&lt;a href=&quot;https://github.com/oxc-project/bench-formatter&quot;&gt;基准测试&lt;/a&gt;&quot;显示：在无缓存的首次运行中，Oxfmt 的速度约为 Biome 的 3 倍、Prettier 的 30 倍。Oxfmt 构建在 Oxc 编译器栈之上，刻意规避了现有格式化工具中常见的架构瓶颈，因此在大型代码库和 CI 场景下表现尤为突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 Prettier 迁移到 Oxfmt 对多数项目来说相当简单。开发者只需将现有的 .prettierrc 配置文件重命名，即可直接使用 Oxfmt。当前版本已支持包括 singleQuote、printWidth 在内的多项主流 Prettier 配置，&lt;a href=&quot;https://oxc.rs/docs/guide/usage/formatter&quot;&gt;完整列表&lt;/a&gt;&quot;可在官方文档中查阅。虽然 Oxfmt 目前通过了约 95% 的 Prettier JavaScript 和 TypeScript 测试用例，但 VoidZero 也在持续向 Prettier 提交 Bug 报告和 Pull Request，以进一步缩小两者之间的差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者 Ryan Leichty 在 X（原 Twitter）上回应作者&lt;a href=&quot;https://x.com/ryanleichty/status/1979971666127032701&quot;&gt;相关帖子&lt;/a&gt;&quot;时表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们已经切到 oxlint 了，oxfmt 真的等不及了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参数状态管理工具&amp;nbsp;&lt;a href=&quot;https://x.com/nuqs47ng&quot;&gt;nuqs&lt;/a&gt;&quot;&amp;nbsp;的官方账号，则在评论 Oxfmt 新增 Tailwind CSS 支持时写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对 Biome 来说，直接被秒。很期待用 oxfmt 替换 Prettier（顺便也可能把 oxlint 一起试了）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上，也有用户围绕 Oxfmt 与 Biome 的性能差异&lt;a href=&quot;https://www.reddit.com/r/javascript/comments/1pbid6b/first_alpha_of_oxfmt_the_rustbased/&quot;&gt;提出疑问&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;不错啊，但有点好奇，他们是怎么做到比同样是 Rust 的 Biome 快这么多的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，有人回应称，关键区别在于两者的架构设计：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;架构完全不一样，而且对性能这件事是真的“较真到偏执”。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从更广泛的工具生态来看，Oxfmt 与&amp;nbsp;&lt;a href=&quot;https://biomejs.dev/&quot;&gt;Biome&lt;/a&gt;&quot;、Prettier 一同构成了 JavaScript 和 TypeScript 领域的主要格式化工具选择。Prettier 仍然是采用最广泛的事实标准；Biome 则通过将 lint 与 format 合并到单一工具中逐渐获得关注。Oxfmt 的差异化路径在于：在保持 Prettier 兼容性的前提下，提供超越两者的性能表现。与 Biome 类似，Oxfmt 也构建在 biome_formatter 的一个分支之上，VoidZero 在公告中特别致谢了 Biome 与 Rome 团队的基础性贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望即将到来的 Beta 版本，VoidZero 正在推进多项实验性能力的稳定化工作，包括内置 import 排序、CSS-in-JS 的嵌入式语言格式化等功能。同时，团队也在研究为 Vue、Svelte、Astro 等主流框架提供插件支持。开发者可以通过项目的&amp;nbsp;&lt;a href=&quot;https://github.com/oxc-project/oxc/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;&quot;&amp;nbsp;提交问题和反馈，或加入官方&amp;nbsp;&lt;a href=&quot;https://discord.gg/9uXCAwqQZW&quot;&gt;Discord 社区&lt;/a&gt;&quot;参与讨论。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/oxfmt-rust-prettier/&quot;&gt;https://www.infoq.com/news/2026/01/oxfmt-rust-prettier/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sDC0oXfPeGcqk1CokzsU</link><guid isPermaLink="false">https://www.infoq.cn/article/sDC0oXfPeGcqk1CokzsU</guid><pubDate>Fri, 30 Jan 2026 02:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>一项 20 年前的 Oracle 排序算法专利到期，开源数据库集体受益</title><description>&lt;p&gt;近日，一篇文章披露，Oracle 公司一项关于高速排序方法的&lt;a href=&quot;https://smalldatum.blogspot.com/2026/01/common-prefix-skipping-adaptive-sort.html&quot;&gt;专利已经到期&lt;/a&gt;&quot;，这意味着开源数据库可以自由使用这一算法。该排序算法的发明者&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/mdcallag&quot;&gt;Mark Callaghan&lt;/a&gt;&quot;&amp;nbsp;指出，这种诞生于 20 年前的技术，能够显著加速对相似数据的排序过程，有望让数据库系统在性能和效率上实现进一步提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这项编号为&amp;nbsp;&lt;a href=&quot;https://patents.google.com/patent/US7680791B2&quot;&gt;US7680791B2&lt;/a&gt;&quot;&amp;nbsp;的专利于 2010 年授予 Oracle Corporation，描述了一种利用“公共前缀字节”进行数据排序的方法。Callaghan 建议将这一排序算法称为 “Orasort”。该方法的核心目标，是解决排序过程中反复比较相似键值前缀所带来的效率问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体而言，该算法融合了多种技术手段，包括：在比较键值时跳过公共前缀、在快速排序（quicksort）与基数排序（radix sort）之间自适应切换、缓存键值子串以减少缓存未命中，以及在排序尚未完全结束时提前输出部分结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于排序过程中数据会被拆分为更小的分组，组内键值往往共享更长的前缀。该算法会记录这些共享部分，在比较时直接跳过它们；在合适的情况下切换到更高效的排序方式；并预先加载下一步所需的字节，从而减少无效计算、提升整体性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Callaghan 曾先后任职于 Oracle、Google 和 Facebook，是资深数据库专家。他回顾了这一专利的诞生过程，并解释了其当下重新受到关注的原因：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我是在 Oracle 工作期间发明了这个算法，它最终被集成进 10gR2 版本中，官方宣称相比 Oracle 之前使用的排序算法，性能提升约 5 倍。我一直希望有一天能看到它的开源实现。这项专利对算法的描述非常清晰，比大多数专利都更容易阅读。值得庆幸的是，负责知识产权的律师充分利用了我当时撰写的功能和设计文档。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一消息迅速引发社区关注，开发者开始讨论如何将该算法引入并优化 MySQL、PostgreSQL 等数据库系统。Flooid.in 的 CTO、ScaleArc 前创始人 Varun Singh 表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;细节完整到这种程度，你甚至可以把它和专利文档一起丢进一个 AI agent 里，直接开始实现。Mark 太厉害了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一条讨论中，Google 的数据库工程师 Hannu Krosing 尝试借助 Gemini，分别使用 Python、C 和 C++ 对该算法进行了实现。文章指出，Oracle 内存排序算法在当年实现了约 5 倍于旧方案的性能提升，甚至因此收到了 Oracle 创始人 Larry Ellison 的致谢邮件。Callaghan 回忆道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当我把它集成进 Oracle DBMS 后，就能直接与旧排序算法对比。新算法通常快了大约 5 倍。后来我又把它和 SyncSort 做了比较。我不记得他们是否有 DeWitt Clause（限制公开对比结果的条款），所以不便透露具体数据，但可以说，Oracle 的新排序算法在对比中表现非常出色。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，Charles Thayer 评论道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我以前从没认真考虑过，一个排序算法在什么时候可以输出第一个结果，以尽早开始响应流、降低延迟。（快速排序在这方面应该相对有优势。）这项工作很有意思。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;截至目前，Oracle 共持有超过 52,000 项专利，其中仍包含大量与数据库技术相关的专利，例如自管理数据库架构、数据库性能优化方法等，涵盖自动调优、高效数据存储等数据库管理的多个关键领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/oracle-patent-sorting-databases/&quot;&gt;https://www.infoq.com/news/2026/01/oracle-patent-sorting-databases/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cMYZm5ujJag5Y2SPxTG2</link><guid isPermaLink="false">https://www.infoq.cn/article/cMYZm5ujJag5Y2SPxTG2</guid><pubDate>Fri, 30 Jan 2026 00:00:00 GMT</pubDate><author>Renato Losio</author><category>数据库</category></item><item><title>世界模型混战，蚂蚁炸出开源牌</title><description>&lt;p&gt;作者｜陈姚戈&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;世界模型领域迎来了一个重要开源模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，蚂蚁集团旗下的具身智能公司“蚂蚁灵波”，正式发布并开源其通用世界模型LingBot-World。与许多闭源方案不同，蚂蚁灵波选择全面开源代码和模型权重，而且不绑定任何特定硬件或平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;去年DeepMind发布的Genie&amp;nbsp;3，让人们看到了世界模型能够根据文本或图像提示，实时生成一个可探索的动态虚拟世界。LingBot-World沿袭了这条路线，并在交互能力、高动态稳定性、长时序连贯性以及物理一致性等维度取得了突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更令人惊喜的是，LingBot-World呈现出从“生成”到“模拟”的跨越。随着模型规模的扩大，灵波团队观察到，LingBot-World开始表现出远超普通视频生成的复杂行为，涌现出对空间关系、时间连续性和物理规律的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以看到，鸭子腿部蹬水的动作、水面对扰动的响应、以及鸭子身体与水之间的相互作用都比较符合物理规律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这显示出模型不仅记住了视觉表象，还在某种程度上理解了流体力学等基础物理机制。同时，水面对扰动的反应，显示出模型对因果关系的理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户切换视角后再回来时，环境中的智能体（比如这只猫）仍能保持持久记忆。智能体即使没有被观察到，也能持续行动。这确保了当视角回归时，世界状态会自然推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当环境中智能体（这只猫）碰到沙发后，没有穿透沙发，反而向空地走去。可以看到，LingBot-World遵循了空间的逻辑，让智能体运动具有物理的合理性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一个长达9分20秒的视频，没有经过任何剪辑和拼贴。视频为用户第一视角，从一座破旧的古希腊神庙出发，沿城市小径前行，经过一座新古典主义建筑，再向左进入一片复原的古希腊建筑群。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近十分钟内，画面保持了较为稳定的物理状态和视觉质量，这在目前的视频生成模型和世界模型中都比较罕见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，在视频最后几分钟，建筑之间的位置关系似乎被模型遗忘了。在7:00，新古典主义建筑和复原式古希腊建筑群是连接在一起的；但7:31，从复原式古希腊建筑群望向新古典主义建筑时，新古典主义建筑消失了。8:30回到新古典主义建筑时，它成为了一栋孤立的房子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管存在这些细节瑕疵，LingBot-World&amp;nbsp;的进步依然显著——单次生成接近10分钟的连贯视频，很可能刷新了当前视频/世界模型的长度纪录。作为对比，Veo&amp;nbsp;3&amp;nbsp;和&amp;nbsp;Sora&amp;nbsp;2&amp;nbsp;的单次生成上限分别为8秒和25秒，Runway&amp;nbsp;Gen-3&amp;nbsp;Alpha&amp;nbsp;为40秒，Kling&amp;nbsp;最长支持2分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与其他交互世界模型相比，LingBot-World在开源、提供720p分辨率的情况下，还保证了高动态程度和长生成跨度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/28/282a90e7af1f10973a9978f4f3ff0c6b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在VBench测试中，LingBot-World全面领先于Yume-1.5和HY&amp;nbsp;World-1.5等先进开源模型，证明了自己不仅是一个视频生成器，更是一个强大的交互式模拟器。通过接收用户输入的动作指令，它能够生成高度动态且物理一致的视觉反馈，保持在高动态度下的整体一致性，使视频内容在长时间段内始终与最初的提示保持一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a4d518073a166cd7b25a615d22b7c76.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在看到大语言模型的局限后，世界模型成为火热赛道。Google、李飞飞、Yann&amp;nbsp;LeCun以及众多科学家纷纷指出，LLM无法很好地理解物理世界、因果关系，而“世界模型”是AI走向真实物理世界深度理解的一个解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于“世界模型”究竟该长什么样，行业至今尚无统一标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞飞的Marble正专注理解空间关系；英伟达把世界模型细分为预测模型、风格迁移模型、推理模型；DeepMind团队的Genie&amp;nbsp;3，则试图在同一个模型中，实现端到端的实时渲染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;路线的分歧，也反应了行业需求的多样性，以及寻找解决方案的困难——无论是智能驾驶、具身智能，还是游戏，都在寻找各自需要的智能方案，以及合适的开发范式和入口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁灵波的世界模型方案更接近Genie&amp;nbsp;3，旨在成为一个通用模型，为Agent、具身智能、游戏、仿真等领域提供理解世界物理规律的基础设施平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过开源其训练方法、模型权重等内容，蚂蚁灵波不仅展示了其在具身智能领域的战略布局，也为行业提供了探索世界模型更多可能性的契机，帮助降低验证世界模型的门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一周，蚂蚁灵波对外集中发布和开源模型研究成果，相继发布并开源空间感知模型LingBot-Depth、具身大模型LingBot-VLA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今，随着LingBot-World的发布，蚂蚁灵波正从幕后走向台前。蚂蚁灵波的目标是打造一个开放、通用的智能基座，与越来越多行业和厂商共建生态。这一次，它用开源的方式，向世界抛出了自己的世界模型范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;构建世界模型的梦想和努力&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在深入探讨蚂蚁团队通用世界模型的细节之前，我们需要花点时间，回顾一下1990年世界模型的开始。这将帮助我们更清楚地理解过去30多年中“世界模型”研究的变与不变、当前世界模型技术路线之争的焦点，从而更好地理解蚂蚁是在怎样的方向和基础上努力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界模型40年，变与不变&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1990年，强化学习领域奠基人、2024图灵奖获得者Richard&amp;nbsp;S.&amp;nbsp;Sutton&amp;nbsp;在人类认知学习过程的启发下，在论文《Dyna,&amp;nbsp;an&amp;nbsp;Integrated&amp;nbsp;Architecture&amp;nbsp;for&amp;nbsp;Learning,&amp;nbsp;Planning,&amp;nbsp;and&amp;nbsp;Reacting》中提出了一个开创性架构：智能体不应只靠真实世界试错学习，而应构建一个内部世界模型，在“脑海”中模拟动作后果，低成本地进行规划与策略优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a807cbddb837a572be625ae5eb02f6bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图片来自Dyna论文。&lt;/p&gt;&lt;p&gt;图片呈现的是Dyna框架的核心逻辑，智能体的目标是最大化其在时间维度上累积获得的总奖励。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;Dyna&amp;nbsp;框架中，世界模型也被称为动作模型，它被视为一个“黑盒子”，输入当前的情境和动作，输出对下一个情境和即时奖励的预测。模型的作用是模拟现实世界，Agent&amp;nbsp;通过与现实世界的持续互动产生经验，并利用这些经验通过监督学习方法来改进模型，使其更接近真实的物理规律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2026年回顾这篇36年前的论文，会发现这份古早的研究为理解当下复杂的技术路线之争提供了共同的根基——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对世界模型的探究，起源于对人类、机器，以及更广泛的智能体如何学习和行动的好奇。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而“世界模型”作为一种方法，提出的解决方案是在模拟出的世界中，让智能体学习、行动、获得反馈和迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dyna这篇论文的核心理念，成为了今天世界模型的研究的底层思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不管是NVIDIA&amp;nbsp;Cosmos、World&amp;nbsp;labs、Google&amp;nbsp;Genie，还是LingBot-World，都沿袭了Dyna的核心理念：世界模型是为智能体提供“模拟经验”的内部环境，使得智能体可以在一个虚拟的环境中进行规划和策略训练。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在不同方向的探索中，我们可以得到的共识是：世界模型从多样化的输入数据中学习对真实世界环境的内部表征，包括物理规律、空间动态和因果关系等。这些表征帮助模型预测未来状态，模拟动作序列，并支持复杂的规划与决策，而不需要反复进行真实世界的实验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;36&amp;nbsp;年过去，我们正站在大语言模型的阴影和语境中讨论世界模型。LLM在理解真实物理世界、及模拟/预测未来后果等方面的局限，正加速科研和商业领域对世界模型的探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年的一次访谈中，Dyna的创作者&amp;nbsp;Richard&amp;nbsp;S.&amp;nbsp;Sutton强调，LLM已经走到了瓶颈。他指出，LLM的核心缺陷在于，它们仅仅是在模仿人类行为，而无法理解世界、预测现实世界中的未来事件。他提倡放弃基于LLM的路径，转而开发基于强化学习、拥有世界转换模型（Transition&amp;nbsp;model&amp;nbsp;of&amp;nbsp;the&amp;nbsp;world）。这种世界模型不仅能学习奖励，还能从所有感官信息中获取环境的丰富理解，最终能够预测“如果做某事，后果将是什么”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大语言模型在理解真实物理世界的不足，以及模拟/预测未来后果的不足，让一批科学家转向，在世界模型中寻找解法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李飞飞认为&amp;nbsp;LLM&amp;nbsp;缺乏对物理世界的感知，提出“空间智能”（Spatial&amp;nbsp;Intelligence）是&amp;nbsp;AI&amp;nbsp;的下一个北极星，AI&amp;nbsp;需要理解三维空间、几何、物理规则以及因果关系，才能从“理解文本”迈向“理解并作用于物理世界”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Yann&amp;nbsp;LeCun则批评&amp;nbsp;LLM&amp;nbsp;依赖文本概率预测，感知学习世界的方式背道而驰。为此，他推广&amp;nbsp;JEPA（联合嵌入预测架构），并成立&amp;nbsp;AMI&amp;nbsp;Labs，通过世界模型的路径实现AGI，探索如何让AI&amp;nbsp;系统具备理解物理世界、持久记忆、逻辑推理以及复杂任务规划能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;DeepMind联合创始人兼CEO&amp;nbsp;Demis&amp;nbsp;Hassabis&amp;nbsp;在今年1月的对谈节目中强调，目前的&amp;nbsp;AI&amp;nbsp;系统还不能理解物理世界、因果关系、行为如何影响结果，而精确的世界模型是实现科学发现或理论创新的关键。他表示，Genie这样的模型还只是“胚胎期世界模型”，Genie体现出的，生成关于世界的内容的能力，某种程度上体现了模型理解了世界的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Google&amp;nbsp;AI团队深度押注了世界模型的发展，并认为它会在2026年赢得重大发展。Hassabis在谈及2026年的突破和期待时提到，“最令我兴奋的，莫过于进一步推动‘世界模型’的发展，提升其运行效率，从而使其能够真正被用于我们通用模型中的‘规划’环节。”这可能意味着，未来世界模型将融入Gemini这样的基础模型中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界模型的路线分歧&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在探索AGI的道路时，蚂蚁集团也看到了世界模型的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为蚂蚁集团旗下的具身智能企业，蚂蚁灵波的定位是“智能基座公司”，致力于打造一个能够理解世界、物理规律以及时空演化的AI系统。而世界模型正是实现这一目标的重要方式之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管各方都将世界模型视为未来的关键技术，然而不同公司选择的路径却各不相同。总体上，这些路径可以分为生成式和非生成式两类，两种路径的核心区别在于预测空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;NVIDIA&amp;nbsp;Cosmos、DeepMind&amp;nbsp;Genie和World&amp;nbsp;Labs都是生成式路径的代表。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cosmos和Genie主要使用由像素构成的观测空间，利用大规模高维视觉数据训练，通过特定的时空架构设计，让模型产生对三维物理世界的理解。Genie&amp;nbsp;3官网中特别提到“Genie&amp;nbsp;3&amp;nbsp;的一致性是一种涌现能力……Genie&amp;nbsp;3&amp;nbsp;生成的世界更为动态和丰富，因为它们是基于世界描述和用户动作逐帧创建的。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;World&amp;nbsp;Labs则另辟蹊径，将预测空间设定为在3D空间中带有位姿的帧，通过查询待生成帧的位姿来生成新图像。其发布的RTFM模型表明：“模型对世界的记忆（存储在各个帧中）具备了空间结构；它将带有位姿信息的帧视作一种‘空间存储’，这赋予了模型一种弱先验——即所建模的世界是三维欧几里得空间，而无需强迫模型显式预测该世界中的物体几何结构。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非生成路径的代表是Yann&amp;nbsp;LeCun的联合嵌入预测架构（Joint&amp;nbsp;Embedding&amp;nbsp;Predictive&amp;nbsp;Architecture,&amp;nbsp;JEPA）。JEPA&amp;nbsp;通过编码器将输入转化为潜空间（Latent&amp;nbsp;Space），并在该空间内预测未来抽象表征（Embeddings），从而无需进行像素级的重建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁灵波的LingBot-World&amp;nbsp;选择了类似Genie的路径，试图在此基础上解决从视频生成到世界模拟之间的技术障碍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;拆解&amp;nbsp;LingBot-World&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在前文的案例和分析中，我们看到蚂蚁灵波的&amp;nbsp;LingBot-World沿袭了Gienie的生成式路线，同时在交互能力、高动态稳定性、长时序连贯性以及物理一致性上表现惊艳。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，蚂蚁灵波选择开源代码和模型权重，并在论文中完整披露了从数据采集到训练部署的全链路设计，鼓励社区测试、使用和复现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即使是在近10&amp;nbsp;分钟的超长视频中、或是快速运动下，画面中的物体依然保持了较为稳定的几何物理特性，没有出现视频生成模型常见的崩坏。这种稳定性，源于其独特的数据引擎和模型架构设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据引擎&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;许多从视频生成模型切入世界模型研发的团队，很快会撞到数据瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;互联网上浩如烟海的短视频大多是“被动”记录，缺乏因果链条。对于世界模型而言，它需要理解的是动作和后果之间的关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如：“按下&amp;nbsp;W&amp;nbsp;键向前走，门是否会打开？”“绕到建筑背面，窗户是否依然存在？”这类智能体动作与环境反馈之间的因果闭环，在普通视频中几乎不存在，在真实世界中规模化采集的成本也很高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了构建“动作-反馈”的闭环，LingBot-World&amp;nbsp;打造了从采集、处理到标注的流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World的数据包含通用视频、游戏数据和合成渲染数据，以确保训练语料的丰富性、高质量和交互性。为游戏数据，灵波团队还开发了专门的平台，捕获&amp;nbsp;RGB&amp;nbsp;帧并严格对齐用户的输入和相机参数。合成数据由&amp;nbsp;Unreal&amp;nbsp;Engine&amp;nbsp;生成，带有精确相机数据和自定义轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97fcf2abdd59e576054679e2bc9125e4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;数据处理和标注流程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据处理层面，灵波团队首先对原始视频进行质量筛选与切分，生成结构清晰的视频片段；然后借助VLM视频的视觉质量、场景类型和视角等，结合几何标注提供必要的3D结构先验，产出元数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，团队引入三种不同粒度的描述标注，涵盖视频全过程的宏观描述、去除了动作和相机数据的静态描写，以及带有时间标注的描述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;模型构建和训练&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;将世界模型定义为一个条件生成过程，模拟由智能体动作驱动的视觉状态演化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从模型构建和训练过程，我们可以看到，LingBot-World&amp;nbsp;是从“视频生成模型”起步，通过不同阶段训练，让模型从“生成”走向“模拟”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从目标函数上看，这种模拟本质上是一种概率预测。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;的目标函数明确表达了这一思想：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;即在最大化给定历史帧&amp;nbsp;()&amp;nbsp;和动作序列&amp;nbsp;()&amp;nbsp;的条件下，预测下一帧状态&amp;nbsp;()&amp;nbsp;的似然概率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，就是让模型学会根据过去看到的画面和执行过的动作，尽可能准确地预测下一帧画面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了避免直接从零训练导致的计算开销和模式崩塌，LingBot-World采取了分阶段的训练策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预训练负责建立稳健的通用视频先验，确保高保真开放域生成；中训练注入世界知识和动作可控性，使模型能够模拟具有一致交互逻辑的长期坚持动态；后训练使架构适应实时交互，采用因果注意力和少步蒸馏以实现低延迟和严格因果性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/06/06c308590e1819dc8efa59b49ac3b639.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;模型训练流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从“生成视频”到“模拟世界”，LingBot-World带来的可能性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;的意义绝不仅在于生成一段精美的视频，而在于它提供了一个高保真的物理交互沙盒，成为具身智能、自动驾驶与虚拟现实等下游任务的通用基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World&amp;nbsp;最直观的突破在于它赋予了通过自然语言控制模拟过程。例如，通过输入“冬季”或“夜晚”，模型会渲染出城堡结冰或夜晚灯光变化的物理效果，同时支持向“像素风”或“蒸汽朋克”等风格的切换。还可以在具体场景中精确注入特定物体。例如，在城堡上空触发烟花，或在喷泉中生成鱼和鸟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在环境中生成烟花效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;改变环境整体风格&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在自动驾驶训练中，这种能力极具价值。算法团队可以人为制造“鬼探头”、极端天气或突发交通冲突，构建出严苛的因果推理环境，从而低成本地解决智驾中的长尾问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;深层物理特性的稳定性，则为这种模拟提供了实际应用的底座。得益于模型展现的长程记忆，生成的视频序列具备了较高的&amp;nbsp;3D&amp;nbsp;一致性，这使得视觉信息可以直接转化为场景点云，从而服务于&amp;nbsp;3D&amp;nbsp;重建或高精度仿真任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LingBot-World具有很好的3D一致性。可以看到，视角变化的情况下，房间结构和物理性状仍然保持稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种稳定性试图触及具身智能训练中的一个核心痛点：机器人的导航或复杂操作往往涉及跨越长时序的决策序列。LingBot-World&amp;nbsp;展现的&amp;nbsp;10&amp;nbsp;分钟级别生成能力，在理论上为多步骤任务提供了更稳定的物理一致性。如果这种长程模拟能有效控制累积误差，将有助于机器人在虚拟环境中进行高频次、深度、低成本试错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，LingBot-World&amp;nbsp;与&amp;nbsp;LingBot-VLA（视觉-语言-动作模型）的结合，勾勒出了一种具身大脑的闭环方案。在这种设定下，世界模型充当了机器人的“内部模拟器”：在&amp;nbsp;VLA&amp;nbsp;模型输出最终指令前，系统可以在虚拟空间中先行演练不同的动作轨迹，评估其物理后果，从而筛选出更符合物理规律且具备安全性的执行路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;令人惊喜的是，利用训练LingBot-World的数据，蚂蚁灵波团队还微调出了动作智能体。智能体可以被置于&amp;nbsp;LingBot-World&amp;nbsp;打造的环境中，Agent&amp;nbsp;的动作改变会实时重塑环境状态，而环境的演变则反过来决定Agent的下一步决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;灵波团队利用LingBot-World相同数据训练处的自主智能体，能在生成的世界中自主规划并执行动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种互动揭示了世界模型在“模拟沙盒”之外的另一种可能——它不仅能理解环境对智能体变化的响应，也具备预测智能体动作流的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，世界模型未来或许不仅仅是训练智能体的工具，也有可能成为驱动智能体（包括机器人）的底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目官网：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://technology.robbyant.com/lingbot-world&quot;&gt;https://technology.robbyant.com/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;论文连接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2601.20540&quot;&gt;https://arxiv.org/abs/2601.20540&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码和模型权重下载:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/robbyant/lingbot-world&quot;&gt;https://github.com/robbyant/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/robbyant/lingbot-world&quot;&gt;https://huggingface.co/robbyant/lingbot-world&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam&quot;&gt;https://www.modelscope.cn/models/Robbyant/lingbot-world-base-cam&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hmKcZ2hdjDk3SspgikfV</link><guid isPermaLink="false">https://www.infoq.cn/article/hmKcZ2hdjDk3SspgikfV</guid><pubDate>Thu, 29 Jan 2026 11:59:46 GMT</pubDate><author>陈姚戈</author><category>AI&amp;大模型</category></item><item><title>如何大规模构建、部署和管理智能体</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着大语言模型能力的成熟，围绕 AI 智能体的讨论正在迅速升温。构建一个能够执行任务、调用工具的 Agent，已经不再是少数团队的专属能力。但在这场技术热潮之下，一个更现实的问题逐渐浮出水面：当智能体不再停留在演示环境，而是被放入真实业务系统中运行时，会发生什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CrewAI 创始人兼 CEO Joao Moura 以实践者的视角，在 BUILD 2025 大会上系统梳理了 AI 智能体从概念、原型走向生产环境过程中，所面临的一系列关键问题。这场分享的核心，并不在于“如何快速做出一个 Agent”，而在于如何让 Agent 在复杂系统中长期、稳定地工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“会生成”到“会决策”：重新理解智能体的能力边界&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分享中，Joao 首先回到一个基础问题：什么才是 AI 智能体真正的能力来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他指出，很多人已经非常熟悉大语言模型在内容生成上的表现，例如生成文本、改写表达、调整语气。这些能力本质上仍然是“输出导向”的，模型根据输入，生成一段结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而智能体的出现，源于另一类能力的被系统性使用：决策能力。当模型不仅要给出答案，还需要在多个选项之间做出判断，并说明为什么选择其中一个时，它开始参与“思考过程”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a5/a5e31ab04b331a193c0acde05d979ea6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，当系统为模型提供可调用的工具，并赋予其一个明确目标，模型就不再只是被动响应请求，而是开始围绕目标不断判断下一步行动。这种行动可能包括调用内部系统、获取业务数据、更新状态，甚至触发后续流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体并不是某种全新的技术形态，而是一个围绕目标进行持续决策与行动的系统。理解这一点，是后续讨论生产化问题的前提。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;真正的分水岭：为什么原型和生产完全是两回事&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到智能体落地时，Joao 明确指出了一个现实情况：从原型到生产，并不是一次线性升级，而是一道本质不同的门槛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原型阶段，团队关注的往往是“能不能跑起来”；而进入生产环境后，关注点会迅速转向“能不能持续运行”。这时，模型本身反而不再是唯一变量，系统层面的复杂性开始占据主导。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/94f0feee4fcd61857bb572dbc119f48f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体一旦被放入真实业务系统，就意味着它将与 ERP、CRM 等核心系统交互，其行为可能直接影响业务流程。在这种情况下，系统是否稳定、决策是否可控、行为是否可预测，都会变成不可回避的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多阻碍智能体进入生产的因素，并不来自 AI 本身，而是来自工程、架构和系统集成层面的现实约束。这也是为什么不少 Agent 项目停留在 Demo 阶段，却迟迟无法真正上线的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;决策、工具与执行：Agent 在系统中的运行方式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个智能体并不是简单地“调用模型”，而是需要在决策、工具调用和执行之间形成闭环。模型负责判断当前状态下应该采取什么行动，而系统则需要确保这些行动能够被安全、准确地执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体需要调用外部工具时，问题并不止于“能不能连上接口”，而在于调用是否可控、结果是否可追踪、失败是否可恢复。这些因素，都会直接影响智能体是否具备进入生产环境的条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种结构下，Agent 更像是一个被嵌入到系统中的“决策节点”，而不是一个独立存在的智能模块。它的价值，取决于整个系统是否为它提供了稳定的运行土壤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;当数量上升：规模化带来的管理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当智能体不再是单点实验，而是开始成批部署时，另一个问题随之出现：如何管理这些 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规模化并不意味着简单复制更多实例。随着智能体数量的增加，部署、运行、监控和管理本身会迅速成为新的复杂系统。如果缺乏系统性的设计，智能体越多，整体风险反而越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到整场分享的核心，Joao 传递出的判断其实非常清晰：AI 智能体的真正价值，并不在于是否足够聪明，而在于是否能够被可靠地使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当讨论从“能不能做”转向“值不值得用”，从原型转向生产，智能体面临的已经不是技术炫技的问题，而是工程与系统成熟度的检验。也正是在这个阶段，智能体才真正开始进入创造长期价值的轨道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</link><guid isPermaLink="false">https://www.infoq.cn/article/WYSZ0iMoqpfYfVDiGKdu</guid><pubDate>Thu, 29 Jan 2026 10:26:09 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>不要再纠结 LLM 准确率了：从“回答对不对”到“系统是否值得信任”</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每个人都在努力提高大语言模型的精准度。但真正的挑战并非精度，而是上下文理解能力。在 BUILD 2025 大会上，Hex 合作伙伴工程负责人 Armin Efendic 探讨了为什么传统的方法，如评估套件或合成问题集往往不够有效，以及成功的 AI 系统是如何通过随着时间推移逐步积累上下文来构建的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由 Snowflake Cortex 提供支持的 Hex，启用了一个新的对话式分析模型，每次交互都让模型变得更聪明。通过 Hex 的 Notebook Agent 与 Threads 功能，业务用户可直接定义核心问题，而数据团队则将这些问题精炼、审计并转化为持久且值得信赖的工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个模型中，测试用例不再由数据团队闭门设计，而是由业务需求驱动并在数据工作流中自动实施，最终形成一个具有生命力的上下文系统，而非一成不变的提示词或测试集，它能随着组织共同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c3f33c248d2dee34e327d20cde14222a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;准确率不是终点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Armin 开场就把矛头对准了一个常见做法：把业务用户会问的问题合成成一批样例，甚至进一步转成 SQL 查询，然后把这些喂给 LLM，用类似单元测试的方式去衡量它的准确率、稳定性与一致性。他不否认“准确性是顶层关注”，但他强调，把 LLM 当作传统软件组件来做单元测试，本身就是一个不合适的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因在于，当你把业务问题硬转换为一组 SQL，并据此去构建样例与评估集时，你很难覆盖真实业务中不断变化的语义、不断扩张的问题空间，以及不同用户在不同语境下对同一指标的不同问法。更重要的是，即使你做出了一个看似通过率很高的测试集，也依然回答不了企业最在意的那件事：当它在真实环境中生成了一个结论，你如何知道它不是在胡编？你又如何知道它到底做了什么才得到这个结论？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，Armin 把正确性从结果层拉回到系统层：你需要的不是一个靠样例证明自己正确的聊天机器人，而是一套可审计的系统，它能够随着时间变得灵活、可塑，能够让业务用户在使用中不断收敛可回答的问题类型，也能够让系统拥有被“硬化”的路径：哪些能力可以放开，哪些问题必须收紧，哪些定义需要固化，哪些数据应该进入上下文、哪些不应该。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，真正有效的路线是：从一套能运行、能被观察的系统出发，让系统在使用中暴露问题、沉淀模式，再反过来加固上下文。这种思路听起来不如直接做评估来得爽快，但它更接近企业系统的真实生长方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;对话式分析如何变成“可审计的系统”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了把“可审计”讲得具体，Armin 用 Hex 的产品演示展示了对话式分析在真实系统中应该是什么形态。演示从一个非常典型的业务问题开始：假设我是营销经理，我想让系统分析销售机会的“首次触达来源”（first touch source），并做营销归因视角的拆解。这里一个很关键的动作，是他先在系统里配置模型提供方：通过密钥对（key pair）连接到 Snowflake 实例，使用 Snowflake Cortex 内托管的 Claude，并强调这是一个“walled garden”的私有网络环境。这样做的直接意义是：模型驻留在数据所在的环境里，数据可以传递给模型，同时也能让 IT 团队对数据出入边界更放心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入线程后，Hex 并不是立刻吐出一句“结论”，而是在后台进行一系列用户不可见但决定可信度的步骤：它会先围绕可访问的元数据“思考”，查看平台上已有的 Hex 项目、仪表板或资产，判断是否存在可复用的内容；它会拉取来自数据仓库的表描述、列描述等元数据，并强调这些可以自动导入、不需要复杂配置；如果企业已经有 dbt 元数据，也可以进一步带入；随后它形成一个“漏斗式”的收敛过程：从广义元数据到相关表、再到更具体的模型信息与底层数据，最后才开始把 SQL 单元格、可能的 Python 单元格、图表与可视化逐步组织起来，用以回答最初的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也解释了他在演示里专门强调的一个点：这种模式一开始会“慢”，但这是刻意设计的。因为此时系统面对的是生产数据仓库，它需要把大量上下文带进来，需要推理与迭代，而这类深度思考天然会以时间为代价。换来的收益是：它可以生成更细致、接近数据科学家或嵌入式数据分析师水平的分析过程。Armin 也提到，未来会有更偏“快速、短促回答”的迭代版本，可能更多依赖语义模型，而不是每次都在全量上下文里深挖。但在这个阶段，他们优先解决的是“在没有分析师介入的情况下，业务用户也能得到一份扎实的分析报告”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当线程生成结果后，界面里不仅有图表，还能继续做探索：拖拽维度与度量、查看底层表格数据、检查异常、做更深的切片。这时“可信度焦虑”就会自然出现：这么多信息暴露给业务用户，我怎么知道它没有幻觉？我该不该信这些 SQL？我如何让它更确定？Armin 的回答不是“相信模型”，而是把系统的底座亮出来：在 Hex 里，每一个线程、每一个项目，背后都由笔记本支撑。把线程保存为项目后，你可以在笔记本里看到完整对话以 Markdown 的形式呈现；更重要的是，你能看到它实际运行的 SQL、过滤条件、连接逻辑、图表生成过程，以及它如何一步步构建出整份报告。对于负责准确性与治理的数据团队来说，这种“把对话落到可审计的笔记本”非常关键——它让系统从一开始就具备被审核、被追责、被修正的可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，Armin 进一步展示了一个更现实的协作场景：业务用户提出问题后，不一定要立刻去找数据团队提工单，而是先在对话线程里得到初步洞察；如果需要更深入的分析（比如进一步做季节性拆解），技术用户可以把笔记本智能体（notebook agent）限定在这个项目范围内，和智能体一起继续规划、推理、生成图表，并在生成的“待处理变更”中逐条审核、决定保留哪些结果。分析由此变成一种可协作、可迭代、可沉淀的工作流，而不是一次性、不可解释的问答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从一次性对话到可复用资产&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果到这里为止，Hex 展示的是“可观察性”，那么 Armin 在后半段想讲的，是上下文如何变成系统能力，如何从一次性对话沉淀为可复用资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他先展示了一个从笔记本走向应用（app builder）的路径：当某些分析内容需要“持久化”，例如营销与销售负责人希望随时看到季节性分析或关键指标，而不是每次回来重新提问，那么就可以把笔记本中已经生成的图表、文本等资产拖拽到应用构建器里，做成一个仪表板、报告或更像 BI 的交互界面。这里的核心并不是“又做了一个 BI”，而是强调：即便呈现形态变成 BI 风格，背后依然由笔记本驱动，仍然保留 SQL、Python、Snowpark 等灵活性；同时，笔记本与应用这两种范式始终连接，资产是可回溯的。换句话说，展示层可以更友好，但底层逻辑并不会因此变成不可审计的黑箱。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;紧接着他抛出了“连接胶水”的问题：当我们有线程、有笔记本、有应用，如何让它们构成一个一致的策略？答案是语义模型——它是 Armin 所谓“上下文引擎”的关键组成部分。原因也很务实：企业里那些精心构建的报表与仪表板，通常包含大量转化逻辑、业务口径、SQL/Python 查询，这些恰恰是 LLM 最需要、也最容易误解的上下文。如果不能把这些上下文结构化，LLM 的确定性就无从谈起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在演示里，语义模型有两条路：一是导入已有的 Snowflake semantic view。Hex 可以浏览生产仓库、发现可访问的语义视图，然后快速引入，例如引入一个 B2B sales model，让 enriched metadata 直接在 Hex 中可用。另一条路更贴近多数团队的起点：不是先有语义视图，而是先有一堆被业务反复使用的仪表板项目。Hex 的语义建模工作台里有一个“建模智能体”（modeling agent），它能理解 Hex 的语义建模能力，并且能针对某个具体项目（例如 sales and marketing dashboard）去阅读项目里包含的 SQL 单元格、DataFrame 操作、joins、函数与过滤条件，形成建模计划，做错误预防，推断表关系，把“项目里已经存在的业务逻辑”烘焙进语义模型中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一段其实回答了一个关键的企业问题：语义模型从哪来？它不一定需要从零凭空设计，它可以从企业已经在用的分析资产中被抽取、被规范、被版本化。建好之后，语义模型还能用一种“拖拽式”的方式被检查：你可以选择维度、度量，查看聚合、查看系统生成的 SQL，在发布之前把模型硬化到你满意的程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更进一步，Armin 也回应了“供应商锁定”的担忧。他明确表示，Hex 不希望用专有 YAML 把用户锁死，并提到两个方向：其一是和 Snowflake 等一起推动“开放语义交换”（Open Semantic Interchange），一个由约 18 家甚至更多公司组成的联盟，目标是让语义模型信息能在不同系统之间互换，以促进 LLM 采用并避免 vendor lock-in；其二是更近期开启“写回”能力，让在 Hex 中构建的语义模型可以写回到 semantic views 中，保证不同系统间“友好共存”。这些内容在分享里出现得很明确：终点不是锁定格式，而是让用户愿意因为体验与工作流而持续使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当语义模型准备好后，线程侧的使用方式也随之变化：你可以把对话线程限定为“只使用语义模型”，而不是访问整个生产数据仓库。Armin 强调，这会让系统随着时间更确定：当你不断硬化语义模型、补充上下文，它会越来越稳定、越来越可控。也正因此，他再次回到开场的观点：把精力放在构建上下文系统上，而不是试图用合成样例把原型聊天机器人测到“看起来准确”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;规模化审计与上下文飞轮&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分享的最后一部分，Armin 把问题推到最现实的规模化挑战上：当系统从一个人试用扩展到五十、一百个用户时，你如何监控它？你如何知道 LLM 系统到底在做什么，业务用户到底拿它解决什么问题？这时，“可审计”就不能停留在某个线程或某个项目，而必须成为一套能覆盖全局的治理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他提到 Hex 的“上下文工作室”（context studio），目前处于少数 Alpha 合作伙伴的 Alpha 阶段，但他之所以专门强调它，是因为它承载了上下文系统最关键的一环：理解使用行为，反过来指导上下文如何演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来说，你可以看到平台总体使用情况：用户更常用笔记本还是线程？创建了多少语义模型？也可以按对话量看用户分布，查看某个用户使用线程的频次、提问的类型。更重要的是，当你下钻到“问题类型”时，Armin 给出了一个很强的判断：这些真实问题才是你的单元测试。不是你在上线前试图一次性“破坏一切”并用评估集兜住，而是看清业务用户到底在问什么，再回去硬化你需要硬化的上下文与问题类型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕“如何策划上下文”，他在分享里给出了三个层次的抓手。最直接的是规则文件（rules file）：你可以在里面定义 SQL 的数据质量防护、业务定义、偏好的 SQL 风格、杂项信息，以及希望系统使用的可视化方式，并且这些内容可以即时编辑、保存或导出。第二层是“经认可的数据”（endorsed data）：由数据团队或所谓“金层”背书的数据资产，可以在 Hex 的语境下被定义清楚，决定哪些数据可以喂给 LLM。第三层则是更成熟、也最关键的做法：语义项目（semantic projects）。随着审计能力增强，你不仅能看到语义模型被使用的次数，还能观察是否有多个语义模型被同时使用、是否需要在某些场景中合并；你也能判断哪些项目最常被引用，从而决定是否需要对下游数据做更多建模，或者是否需要补充列描述、表描述等元数据来改善上下文质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些细节共同指向同一个结论：上下文不是一次性设计出来的，它是被真实使用不断“磨”出来的。你从稍微宽的范围起步，抽取一两个语义模型，让业务用户用起来；再通过审计看到真实问题与真实路径，回去修规则、补语义、加背书数据、完善元数据。如此循环，系统才会越来越确定、越来越可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场分享最有价值的地方，在于它没有把“可信”简化为一个指标，也没有把“准确率”当作唯一的归宿。Armin 反复强调的其实是另一套思维：企业要的不是一个在评估集上表现漂亮的聊天机器人，而是一套能持续吸收上下文、可审计、可治理的系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从线程到笔记本的可观察性，从笔记本到应用的资产化，从项目到语义模型的上下文结构化，再到面向规模化使用的审计与上下文工作室——这些环节被串成一个整体，目的只有一个：让 LLM 在真实业务里变得更确定，并且在需求增长时仍然能保持可控与可信。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原视频地址：&lt;a href=&quot;https://www.snowflake.com/en/build/americas/agenda/?login=ML&quot;&gt;https://www.snowflake.com/en/build/americas/agenda/?login=ML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;🔥【活动推荐】2 月 2 日-6 日，&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;Snowflake Discover&lt;/a&gt;&quot; 重磅上线！这是一场免费、线上、可实时互动的技术活动，旨在帮助您全面提升数据与 AI 能力，深入了解如何更高效地管理、整合与分析数据。4 天时间 18 场技术干货分享，由来自亚太地区的一线技术专家亲自分享与讲解～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/edm/resource/image/85/9a/852e6196c25c9abab4e7a7ee2767159a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/about/webinars/snowflake-discover-apac/?utm_source=InfoQ&amp;amp;utm_medium=Social&amp;amp;utm_campaign=discoverAI-China-InfoQ&quot;&gt;点击报名 Discover&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</link><guid isPermaLink="false">https://www.infoq.cn/article/TZrHpojJxuCmLCP0uRSO</guid><pubDate>Thu, 29 Jan 2026 09:02:56 GMT</pubDate><author>王玮</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>Aspire 13.1带来了MCP集成、CLI增强和Azure部署更新</title><description>&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;作为增量更新发布，它基于Aspire 13引入的多语言平台基础。此次发布专注于通过增强命令行界面、更深入地支持AI辅助开发工作流程、改进仪表板体验以及更清晰的Azure环境部署行为来提高开发者的生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据团队报告，此次更新旨在使日常开发任务更可预测、更易于自动化，并与现代AI编码工具更好地对齐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aspire 13.1中的一个核心新增功能是通过与模型上下文协议（&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-mcp-for-ai-coding-agents&quot;&gt;Model Context Protocol&lt;/a&gt;&quot;，MCP）集成，扩展了对AI编码智能体的支持。一个新的命令允许项目在初始化时支持MCP，使兼容的AI工具能够发现Aspire集成、检查应用程序结构并与运行中的资源交互。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;aspire mcp init&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接后，AI智能体可以查询应用程序状态、查看日志并通过暴露的端点检查跟踪。这种集成旨在简化开发过程中AI助手的使用，而无需为每个工具进行自定义设置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-cli-enhancements&quot;&gt;Aspire CLI&lt;/a&gt;&quot;进行了几次更新，旨在减少创建、运行和维护项目时的摩擦。如前所述，项目创建命令现在可以选择通道，并且一旦选择，将全局保持，确保新项目的行为一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CLI还能检测到已经运行的实例，并在启动新运行之前自动停止它们，从而避免常见的冲突。安装脚本现在支持一个选项来跳过修改系统 PATH，这在受控环境中非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布的仪&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-dashboard-improvements&quot;&gt;表板更新&lt;/a&gt;&quot;专注于清晰度和可见性。新的参数标签允许直接从资源详情中查看和管理配置值。GenAI可视化器已增强，以更好地显示工具定义、评估和相关日志，并支持预览音频和视频内容。仪表板的几个稳定性问题也得到了解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e063e5a2395fca657719db177ab3ec1c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（GenAI可视化器工具定义，来源：&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#genai-visualizer-enhancements&quot;&gt;官方Aspire文档&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#%EF%B8%8F-azure-improvements&quot;&gt;Azure&lt;/a&gt;&quot;改进方面，Aspire 13.1引入了更清晰的命名和更强大的验证。Azure Redis集成已重命名，以更好地匹配底层服务，并且在部署过程中更早地执行额外检查，以便尽早发现配置问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Azure资源现在暴露出标准化的连接属性，这些属性在支持的语言中通用，使得非.NET应用程序能够使用一致的设置进行连接。还增加了对Azure App Service中部署槽的支持和对默认角色分配的更精细控制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过引入通用容器注册表资源，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-container-and-docker-compose&quot;&gt;容器和部署&lt;/a&gt;&quot;工作流得到了改进，允许开发者锁定Azure容器注册表之外的注册表。容器镜像推送现在更加明确和可预测，特别是在部署到Azure容器应用时。Docker Compose支持已得到改进，以增强可移植性并减少并行构建期间的竞争条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还包括针对&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/#-javascript-and-frontend-support&quot;&gt;JavaScript和前端开发的更新&lt;/a&gt;&quot;，例如一个新的起始模板，该模板结合了ASP.NET Core后端和基于Vite的前端，改进了开发中的HTTPS处理，并修复了与包管理器相关的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;证书处理得到了简化，新增了配置HTTPS和在支持的容器中终止TLS的新API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Aspire 13.1还稳定了之前预览版中的几个集成，包括Dev Tunnels、端点代理支持和Azure Functions。模板已更新以反映一致的模式，并且广泛的错误修复集提高了跨平台的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;Aspire 13.1&lt;/a&gt;&quot;需要.NET 10 SDK或更高版本。建议从早期版本升级的开发者查看已记录的重大变更，特别是围绕Azure Redis API和重命名的连接属性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于感兴趣的读者，&lt;a href=&quot;https://aspire.dev/whats-new/aspire-13-1/&quot;&gt;完整的发布说明&lt;/a&gt;&quot;和详细文档可在&lt;a href=&quot;https://github.com/dotnet/aspire&quot;&gt;官方Aspire存储库&lt;/a&gt;&quot;和文档渠道中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&quot;&gt;https://www.infoq.com/news/2026/01/dotnet-aspire-13-1-release/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</link><guid isPermaLink="false">https://www.infoq.cn/article/7yEzcHRZxLIIVR4zMV4k</guid><pubDate>Thu, 29 Jan 2026 09:00:00 GMT</pubDate><author>作者：Almir Vuk</author><category>微软</category><category>云计算</category></item><item><title>LangChain 创始人警告：2026 成为“Agent 工程”分水岭，传统软件公司的生存考验开始了</title><description>&lt;p&gt;过去几十年，软件工程有一个稳定不变的前提：系统的行为写在代码里。工程师读代码，就能推断系统在大多数场景下会怎么运行；测试、调试、上线，也都围绕“确定性”展开。但 Agent 的出现正在动摇这个前提：在 Agent 应用里，决定行为的不再只是代码，还有模型本身——一个在代码之外运行、带着非确定性的黑箱。你无法只靠读代码理解它，只能让它跑起来、看它在真实输入下做了什么，才知道系统“到底在干什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在播客中，LangChain 创始人 Harrison Chase 还把最近一波“能连续跑起来”的编程 Agent、Deep Research 等现象视为拐点，并判断这类“长任务 Agent”的落地会在 2025 年末到 2026 年进一步加速。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也把问题推到了台前：2026 被很多人视为“长任务 Agent 元年”，现有的软件公司还能不能熬过去？就像当年从 on-prem 走向云，并不是所有软件公司都成功转型一样，工程范式一旦变化，就会重新筛选参与者。长任务 Agent 更像“数字员工”——它不是多回合聊天那么简单，而是能在更长时间里持续执行、反复试错、不断自我修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这期与红杉资本的对话中，Harrison 抛出了一个判断：构建 Agent，已经不只是把软件开发“加一层 AI”，而是工程范式本身在变。为什么他说“光读代码不够了”？为什么 tracing、评估、记忆这些原本偏“辅助”的东西，突然变成主角？他在对话里给出了非常具体的解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而更现实的问题是：如果范式真的在变，那些靠数据、流程、产品形态建立壁垒的传统软件公司，优势还能不能延续？它们手里握着的数据与 API 可能依然是王牌，但能否把这些资产变成 Agent 时代的生产力，取决于一套全新的工程打法。Harrison 的观察与判断，都在下面的完整对话里：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：AI 领域的变化速度快得惊人。当前最受关注的话题，我觉得没有人比你更合适来聊。我们会先谈 长任务 Agent（Long Horizon Agents） 和 Agent Harness（智能体运行框架）。&lt;/p&gt;&lt;p&gt;接着，我们会讨论：构建长任务 Agent 与构建传统软件到底有什么不同，以及你如何看待 LangChain 在整个生态系统中的角色。最后，我想和你聊聊未来。你怎么看红杉资本这篇关于Long Horizon Agents的文章？哪些观点你认同，哪些地方你不太同意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/acbe71eff021a48fa8459ecebfaa8b62.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“在去年的一篇文章中，我们曾提出：推理模型（reasoning models）是 AI 领域最重要的新前沿。而“长任务 Agent”（long-horizon agents）则在这一范式之上更进一步——它们不只是思考，还能够采取行动，并在时间维度上不断迭代。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://sequoiacap.com/article/2026-this-is-agi/&quot;&gt;https://sequoiacap.com/article/2026-this-is-agi/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：你们这个概念命名得非常好，那篇文章也写得很棒。我整体上是认同的——长任务 Agent 终于开始真正“跑起来”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始对 Agent 的设想，本来就是让一个 LLM 运行在一个循环里，自主决定接下来该做什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AutoGPT 本质上就是这个想法，这也是它当初能迅速走红、抓住那么多人想象力的原因：一个 LLM 在循环中运行，完全自主地决定行动。但当时的问题在于：模型还不够好，围绕模型的 scaffolding（支架）和 harness（框架）也不够成熟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这几年，模型本身变得更强了；与此同时，我们也逐渐搞清楚了，什么样的 harness 才是“好”的。于是现在，这套东西开始真正奏效了。最明显的例子是在编程领域，Agent 的突破首先发生在那里。之后，这种能力正在向其他领域扩散。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，你仍然需要告诉 Agent 你想让它做什么，它也需要配备合适的工具。但现在，它确实可以持续运行更长的时间，而且表现越来越稳定。所以，用“长时序”来描述这一类 Agent，我觉得非常贴切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最喜欢的长任务 Agent 案例有哪些？你觉得它们正在呈现出哪些形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：目前最成熟、我自己用得最多的，还是 编程 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再往外延一点，我觉得非常优秀的一类是 AI SRE。比如 Traversal（我记得它是一家红杉投资的公司），他们的 AI SRE 可以在更长的时间跨度内运行。再往抽象一点，其实这类 AI SRE 本质上属于“研究型 Agent”。比如：给它一个事故，它会去翻日志、分析上下文、追溯原因。研究任务本身非常适合 Agent，因为它们最终产出的往往是一个“初稿”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent 的问题在于：它们还达不到 99% 的可靠性，但它们可以在较长时间内完成大量工作。所以，只要你能把任务框定为：让 Agent 长时间运行，产出一个初步版本，由人来审阅，这在我看来就是目前长任务 Agent 最“杀手级”的应用形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程就是一个例子：你通常是提交 PR，而不是直接推到生产环境（当然，vibe coding 现在也在不断进步）。AI SRE 也是一样：结果会交给人来 review。报告生成也是如此：你不会直接发给所有用户，而是先看一遍、改一改。我们在金融领域也看到了大量这样的用法，这是一个非常大的研究机会。客服领域同样如此。最早的客服 Agent 主要是做“第一响应”：用户一发消息，马上给出回复，这类用法现在也做得很好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现在开始出现新的形态，比如 Klarna&amp;nbsp;这个产品：人类和 AI 协同工作。当第一层自动回复失败后，不是简单地转交给人工，而是让一个长任务 Agent 在后台运行，生成一份完整的事件报告，然后再交给人工客服处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里“agent”这个词在客服语境下会变得有点混乱，但核心逻辑是一致的。总结来说，这些应用的共同点是：先由 Agent 生成一个“初稿”，再由人类接管。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那么，“为什么是现在”？你觉得主要是因为模型本身变得足够强，还是因为人们在 harness 侧做了非常聪明的工程设计？在回答这个问题之前，能不能先帮听众梳理一下：在一个 Agent 系统中，模型、框架和 harness 各自扮演什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：当然可以。我也顺便把“框架”这个概念一起带进来。一开始，我们把 LangChain 描述为一个 Agent Framework，现在我们又推出了 Deep Agents，我更愿意称它为一个 Agent Harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人都会问，这两者有什么区别。模型很简单，就是 LLM：输入 token、输出 token。框架（Framework） 是围绕模型的一层抽象，让你更容易切换模型，封装工具、向量数据库、记忆等组件，本身比较“无偏好”，强调灵活性，更像是基础设施。Harness 则更“有主张”。以 Deep Agents 为例：我们默认就提供一个 规划工具（Planning Tool）；这个工具是直接内建在 harness 里的，带有明确的设计立场：我们认为这是“正确”的做法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还做了 上下文压缩（Compaction）。长任务 Agent 会运行很久，哪怕上下文窗口已经很大，也终究是有限的，总会有需要压缩的时候。怎么压缩？压缩什么？这是一个正在被大量研究的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，几乎所有 Agent Harness 都会提供文件系统交互能力，不管是直接操作，还是通过 bash。这一点其实很难和模型本身完全分开，因为模型训练数据里已经大量包含了这类操作。&lt;/p&gt;&lt;p&gt;如果回到两年前，我不确定我们是否能预见到：基于文件系统的 harness 会成为最优解之一。那时模型还没被充分训练过这些模式，而现在模型和 harness 是在一起“共同进化”的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结来说，这是一个组合效应：模型本身确实在变强，推理模型带来了巨大提升。同时，我们也逐渐摸索出了 compaction、planning、文件系统工具等一整套关键原语。这两者缺一不可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;设计范式的演进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得在我们第一次对谈时，你把 LangGraph 描述为 Agent 的“认知架构”。现在来看，这是不是也可以理解为 harness 的一种形态？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的，这个理解是对的。我们现在的 Deep Agents 是构建在 LangGraph 之上的。可以把它看作是一个非常具体、非常有主张的 LangGraph 实例，更偏向通用目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期我们讨论过“通用架构”和“专用架构”的区别。现在我们观察到一个很有意思的变化：过去需要写进架构里的任务特异性，正在转移到工具和指令里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;复杂性并没有消失，只是从结构化代码，转移到了自然语言中。因此，prompt 的设计、修改，甚至自动更新，正在成为系统的一部分；而 harness 本身，反而变得更加稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在你看来，harness 工程中最难做对的是什么？你觉得单个公司是否真的有可能在这一层形成显著优势？有没有你特别佩服的团队？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：说实话，目前在 harness 工程上做得最好的，基本都是编程类公司。Claude Code 就是一个非常典型的例子。我认为它能如此受欢迎，很大程度上是因为它的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是否意味着：harness 更适合由模型公司来做，而不是第三方创业公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我不确定。比如 Factory、AMP 这些编程公司，也都做出了非常强的 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实存在一个现实：harness 往往和模型家族绑定得比较紧密。不一定是某一个具体模型，而是一整个模型体系。Anthropic 的模型会针对某些工具进行微调，OpenAI 则针对另外一些。这和 prompt 类似：不同模型，需要不同的 prompt；同样，不同模型家族，也需要稍微不同的 harness。当然，它们也有很多共性，比如几乎都会使用文件系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我自己也没有一个确定答案。但一个很明显的现象是：几乎所有做编程 Agent 的公司，现在都在自研自己的 harness。你去看 Terminal Bench 2 这样的榜单，会发现他们不仅展示模型，还展示 harness。Claude Code 并不总是在榜首。这说明：性能差异并不完全来自模型，而来自对“模型如何在 harness 中工作”的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得，排行榜上表现最好的 harness，究竟在哪些地方做得特别好？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：首先是对模型训练偏好的理解。比如 OpenAI 的模型对 Bash 非常熟悉；Anthropic 提供了显式的文件编辑工具。顺着模型的“母语”来设计 harness，本身就能带来性能收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次是 上下文压缩（Compaction）。随着任务时间跨度变长，如何处理上下文窗口溢出，已经成为一个核心问题。这显然也是 harness 的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还有 skills、子 Agent、MCP 等机制。目前这些能力还没有被系统性地训练进模型中，仍然属于比较新的探索方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们的 harness 中，一个典型挑战是：主 Agent 如何与子 Agent 高效通信。主模型需要把所有必要信息传递给子 Agent，同时还要明确告诉它：最终只需要返回一个“最终结果”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们见过一些失败案例：子 Agent 做了大量工作，最后却返回一句“请查看我上面的分析”，而主 Agent 根本看不到那些内容，于是完全不知道它在说什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，如何通过 prompt 设计让这些组件协同工作，是 harness 工程中非常重要的一部分。&lt;/p&gt;&lt;p&gt;如果你去看一些公开的 harness prompt，它们往往有几百行之长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我想从演进角度问一个问题。你一直站在模型“如何落地”的最前沿。如果用一种简化视角来看过去五年的几个关键拐点：ChatGPT 带来了预训练的拐点；o1 带来了推理能力的拐点； 最近，Claude Code + Opus 4.5 带来了长任务 Agent 的拐点。但从你这个“围绕模型做设计”的世界来看，拐点会不会是另一套划分？从认知架构到框架、再到 harness，这中间经历了哪些真正的跃迁？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我大概会把它分成三个阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一阶段：最早期。那时 LangChain 刚刚出现，模型还是“纯文本输入、纯文本输出”，甚至还不是 chat 模型。没有工具调用，没有 reasoning，没有结构化输出。人们主要做的是单一 prompt 或简单 chain。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二阶段：工具与规划开始进入模型。模型开始支持 tool calling，也尝试学会“思考”和“规划”。虽然还不够强，但已经能做出基本决策。这时，人们大量使用自定义的认知架构，通过显式提问来引导模型行动，但整体仍然依赖大量外部 scaffolding。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三阶段：长任务 Agent 的真正起飞。大概是在今年 6～7 月，我们看到 Claude Code、Deep Research、Manus 等产品同时爆发。它们在底层使用的是同一个核心算法：让 LLM 在循环中运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的突破来自于 上下文工程：压缩、子 Agent、技能、记忆——所有这些，都是围绕上下文展开的。这正是我们开始做 Deep Agents 的时间点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于很多程序员来说，Opus 4.5 可能是一个心理上的分水岭。也可能只是碰巧遇上假期，大家回家开始大量使用 Claude Code，突然意识到：它真的很好用。无论是 2025 年初还是 2025 年末，总之在某个时间点，模型“刚好强到足以支撑这种形态”，于是我们从 scaffolding 迈向了 harness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Coding Agent 是通用 AI 的终局形态吗&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：接下来会发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也希望我知道答案。这个“让 LLM 在循环中运行、让它自己决定要拉什么上下文进来”的算法，本身极其简单、也极其通用。这正是 Agent 从一开始的核心设想，而我们现在终于走到了“它真的能工作”的阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，可能会有大量围绕上下文工程的技巧出现：有些手动设计的部分可能会消失；比如压缩类的，现在仍然高度依赖 harness 作者的决策。Anthropic 已经在尝试让模型自己决定何时压缩上下文，虽然目前用得还不多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个我们非常关注的方向是 记忆（Memory）。从本质上说，记忆也是一种上下文工程，只不过是跨更长时间尺度的上下文。核心算法本身已经非常清晰：运行 LLM 循环。未来的进步，很可能来自更聪明的上下文工程方式，或者让模型自己参与上下文管理。模型当然也会继续变强，越来越擅长长时序任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我目前思考最多的一个问题是：我们看到的大多数 harness 都是高度偏向编程任务的。这是长任务 Agent 最先爆发的领域。但即便是在非编程任务中，你也可以认为：写代码本身是一种非常强的、通用的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我本来想问你：编程智能体（coding agents）到底算不算一个子类别？还是说编程智能体就是智能体本身？换句话说，智能体的工作，本质上是想办法让计算机去做一些有用的事情，而“写代码”本来就是让计算机做有用事情的一种很好的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我也不确定。但有一点我非常非常坚信：现阶段只要你在做长时序智能体，你就必须给它文件系统的访问能力。因为文件系统在“上下文管理”方面能做的事情太多了。比如我们说 compaction（上下文压缩），一种策略是把内容总结掉，但把完整的消息都放进文件系统里，这样如果智能体后续需要回查，它还能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一种策略是，当你遇到很大的工具调用结果时，不要把全部内容都塞回模型上下文里；你可以把结果放进文件系统，然后让智能体需要的时候再去查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这些操作，其实不一定需要真实的文件系统，也不一定要让它真的去写代码。我们有一个概念叫“虚拟文件系统”：它底层可能只是 Postgres 之类的存储，扩展性更强。当然，“真实代码”能做的事情，虚拟文件系统做不了。比如你没法在虚拟文件系统里直接运行代码。所以写脚本在很多场景下确实非常有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也认为编程智能体有潜力成为通用智能体，但我不确定这是否意味着“今天的编程智能体”就是通用智能体——如果你能理解我这句话。因为我觉得现在很多编程智能体还是为编程任务做了大量优化的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以“一个通用智能体可能长得像编程智能体”，但反过来，“今天的编程智能体就是通用智能体”，这件事我并不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;传统软件面临的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们能不能转到另一个话题：构建长时序智能体和构建传统软件之间的差异？你能不能先描述一下“1.0 时代”的软件开发栈是什么样的，然后说说现在到底哪里不一样？我记得你在 X 上写过一篇很不错的文章，也许你可以总结一下核心结论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e1/e1839d7404649a0013d16b7f7d6b4a61.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来源：&lt;a href=&quot;https://x.com/hwchase17/status/2010044779225329688&quot;&gt;https://x.com/hwchase17/status/2010044779225329688&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我这段时间一直在反复想这个问题：我们经常说“做智能体和做软件是不同的”，而且很多人也同意。但问题是：到底哪里不同？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得很容易、也很偷懒地说“不同”，但“具体不同在哪里”才是关键。下面这些可能听起来很显然，但也许显然是好事，希望它们不太有争议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你在做传统软件时，所有逻辑都写在代码里，你能直接在软件代码中看到它。但当你在做智能体时，你的应用如何工作的“逻辑”，并不全部在代码里，其中很大一部分来自模型本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着：你不能只看代码，就判断智能体在某个具体场景下会做什么。你必须真的把它跑起来。而我认为，这就是最大的不同：我们引入了这种非确定性系统，它是一个黑箱，它在代码之外。我觉得这就是核心差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个直接后果是：为了弄清楚应用到底在做什么，你不能看代码，你必须看它在真实运行中做了什么。这也是为什么我们做的产品里，最受欢迎的之一是 LangSmith。LangSmith 的一个核心能力是 tracing（追踪/执行轨迹）。为什么 trace 这么受欢迎？因为它能把智能体每一步内部发生的事情都清清楚楚地展示出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这跟传统软件里的 trace 又不一样。传统软件里，你的系统在那边跑，它会吐出很多日志和事件；你通常是在出现错误时才去看，而且你不需要“每一步的全部细节”。而且本地开发时，你可能直接打个断点就够了；很多时候日志追踪是上线到生产环境后才会更重度开启。但在智能体里，人们从一开始就会用 trace 来理解“底层到底在发生什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它在智能体里的影响力，远大于在单一 LLM 应用里的影响力。因为在单一 LLM 应用里，如果模型回答得不好，你知道你的 prompt 是什么，也知道输入上下文是什么（由代码决定），然后你得到一个输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，它在循环中运行、不断重复。你并不知道第 14 步时上下文里到底有什么，因为前面 13 步可能会把任意东西拉进上下文。所以，“上下文工程（Context Engineering）”真的是一个非常好的词。我真希望这是我发明的。它几乎完美描述了我们在 LangChain 做的一切——只是当时我们并不知道这个术语已经存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;trace 的价值就在于：它能直接告诉你此时此刻上下文里到底有什么，这太重要了。那这又意味着什么？这意味着：对传统软件来说，“真相的来源（source of truth）”在代码里。但对智能体来说，真相来源变成了代码与 trace 的组合——而 trace 是你能看到真相的一部分地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术上说，真相当然也“存在于模型的数百万参数里”，但你基本没法直接对参数做什么。所以现实上，trace 就成了你可以抓住的“事实载体”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，trace 也会成为你开始思考测试的地方。你仍然可以对 harness 的某些部分做单元测试，也可以离线做一些 unit test，但要获得真正的测试用例，你很可能需要用 trace 来构建。而且在智能体里，在线测试（online testing） 可能比传统软件更重要，因为行为不会在离线环境里完整显现出来，只有在真实世界输入驱动下、系统被真正使用时，行为才会“涌现”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也看到 trace 正在成为团队协作的中心：如果出了问题，不再是“去 GitHub 看代码”，而是“去看那条 trace”。我们在开源项目里也一样。有人说：“Deep Agents 这里跑偏了，发生了什么？”我们的第一反应是：“把 LangSmith trace 发给我们。”如果没有 trace，我们基本没法帮你 debug。过去大家会说“把代码给我看看”，但现在已经转变了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是我写在 X 上那篇文章的核心内容，反馈很好。我也还在琢磨怎么把它表达得更精确，但我觉得这一点很关键。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外一个点我也还在继续想：我觉得构建智能体是一个更偏迭代式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们过去也会这么说，但我以前会有点翻白眼，因为软件开发本来也是迭代式的：你发布、收反馈、不断迭代，这就是软件开发的常态。但我觉得差别在于：在传统软件里，你的迭代是围绕“你希望软件做什么”来进行的。你有一个想法，你发布，你收反馈。比如“这个按钮让人困惑”，或者“用户其实想做 X 而不是 Y”。但你在发布之前，其实你是知道软件会怎么运行的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在智能体里，你在发布之前并不知道它到底会怎么做。你当然有一个预期，但你并不能在发布前真正确定它会做什么。因此，为了让它更准确、让它更“对”、让它能通过某种“概念上的单元测试”，你需要更多轮次的迭代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个基础上，我也认为记忆（memory）非常重要。因为记忆就是在从这些交互中学习。如果你的开发过程变得更迭代、更难，那么作为开发者，我为了让系统表现正确，可能需要反复改系统 prompt——这种频率甚至可能比我改代码还高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是记忆进入的地方：如果系统能够以某种方式自己学习，那就能减少开发者必须进行的迭代次数，让构建这类智能体变得更容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，这是我认为“构建智能体确实不同于构建软件”的另一个角度。我也承认，这么说有点老套，所以我一直在逼自己想清楚“到底不同在哪里”，目前我总结出来的就是这两点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我也很想追问这一点。现在公开市场上有一个很大的争论：现有的软件公司还能不能熬过去？如果类比当年从本地部署软件（on-prem）转向云（cloud），实际上真正成功转型的公司并不多，因为事实证明，“做云软件”和“做本地软件”确实差异很大。你现在处在“人们如何用 AI 构建产品”的核心地带。你怎么看这件事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不是要问公开市场的投资问题，而是想问：这个变化到底有多大？你有没有看到很多人：过去很擅长“旧方法做软件”，现在也能很擅长“新方法做软件”？还是说更像是：你要么在“新方法”里长大，要么就很难真正理解它？你觉得人能跨越这个鸿沟吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我注意到现在有很多年轻创始人，这让我觉得，也许年轻人因为没有太多对“旧软件开发方式”的先入之见，反而可以更快把这些东西学起来、用起来。而且我们确实一再听到一个现象：很多在做 agent engineering 的团队成员，反而是更初级的开发者、更初级的构建者——他们确实没有那些先入之见。我们内部的应用 AI 团队，确实整体更偏年轻一些。我觉得这里面既有“人的因素”，也有“公司的因素”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;先说公司层面：数据依然非常非常非常有价值。如果你从 harness 的角度去看——顺便说一句，我其实不认为长期来看大多数人都会自己去写 harness，因为它比做 framework 难太多了。所以我觉得大家最终会用我们提供的 harness，或者用别人的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那一个 harness 里面有什么？主要就是：prompt、指令，以及它连接的工具。而现有公司在这方面最大的资产之一，是他们已经拥有数据和 API。如果你过去在这块做得不错，那么把这些东西接入到 agent 上，其实会非常容易产生真实价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们前阵子和金融行业的人聊，他们就说：数据的价值只会越来越高、越来越高、越来越高。所以如果你是一个传统软件厂商，你手上有这些高价值数据，你应该能够把它暴露给智能体，让智能体去用，从中拿到很大的收益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过这里还有另一部分：关于“如何使用这些数据”的指令（instructions），这一块可能更偏“新增”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你作为软件厂商也许一直对“怎么用这些数据”有一些想法，但你并没有把这些想法系统化、固化成可执行的“操作说明”，因为过去这件事更多是由人来完成的——很多智能体现在在做的事情，本来就是人类会做的事情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你当然会给人配工具，但你以前不会、或者也很难成功地把它完全自动化。而到了“智能体”这一代，这部分才真正变得可行。所以我觉得这块是新的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们也看到大量需求来自“垂直领域创业公司”。Rogo 就是一个很好的例子：他们团队有人有金融行业经验，把这种行业知识带进了智能体系统里，而这之所以有效，是因为很多智能体的驱动力来自“知识”——但不是那种通用世界知识，而是如何执行特定流程、特定模式的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以问题就变成：做传统软件的人是不是做智能体的合适人选？我觉得我们确实看到很多非常资深的开发者在采用 agentic coding，所以某种程度上这更像是“心态问题”。但确实也可能会呈现出一种“年轻化倾向”。而公司层面，则很大程度取决于它手上的数据资产。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以看起来，你认为 trace 是这个新世界里 agent 开发的核心“产物”，LangSmith 在这方面帮助很大。那你觉得还有哪些核心的“产物”——或者说，可能“产物”这个词不对，应该说组件（components）？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，组件。我觉得构建软件与构建智能体之间另一个差异是：评估软件时，你可以相当可靠地依赖程序化测试和断言。但智能体做的很多事情，本质上是“人类会做的事情”。因此要评估它，你必须把人的判断引入进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我们在 LangSmith 里努力解决的问题之一：你已经有了这些 traces，那么你怎么把人类判断带到 traces 上？最直接的方法当然就是：把人引进来。所以我们也看到数据标注类创业公司做得很好。我们在 LangSmith 里有一个概念叫 annotation queues（标注队列），就是把人带进来参与。因此，实际的、真实的人类判断，是其中非常重要的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这里的“人工标注”的trace，比如，智能体做了这些步骤，这是好还是不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：有时候，人会给自然语言反馈：这很好、这很差、这里应该怎么做。有时候，人会直接“纠正它”：把正确步骤完整地写出来。这具体怎么做取决于用例，而且对做 RL 的模型公司，和对做 agent 应用的公司来说，也可能不一样。但核心就是：把人类判断带进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也看到另一条路：尝试为这种人类判断建立一些“代理指标”（proxy）。这就是 LLM-as-a-Judge 这类方法的来源：你可以跑一个 LLM 或其他模型，让它承担某种“类似人类判断”的角色，去给那些本来需要人类判断的东西打分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们一直在思考的一件事是：怎么让“构建 judge”这件事变得容易。因为 judge 的关键很大一部分在于：它必须和你的人的判断、人类偏好保持一致。如果做不到，那你的 grader（评分器）就很糟糕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们在 LangSmith 里做了一个概念叫 align evals：人类先去标注一些 traces，然后基于这些标注，构建一个 LLM judge，使它在这些样本上被校准（calibrated）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为关键就在于：你要把人类判断引入进来；如果你要用 proxy 来替代它，那就必须确保这个 proxy 校准得足够好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有意思。我记得我们最开始和你做业务合作的时候，还在邮件里讨论过：LLM-as-a-Judge 到底是否可行。看起来它已经进步很多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：是的。LM-as-a-Judge 其实有几个不同层面的用法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最常见的一种，是用于 eval：拿一条 trace，直接给它一个分数，比如 1 到 0，或者 0 到 10。这个我认为是可行的，而且很多人确实在做。他们会离线做，也会在线做，因为有些判断并不需要 ground truth（标准答案）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但我觉得另外一个更重要的场景，是你在 coding agents 里也能看到的：coding agent 往往会先工作到某一步，然后遇到错误，触发纠错。它实际上是在“评判自己刚才做的工作”。我们也在 memory 上看到同样的模式：记忆很大一部分就是反思 traces，然后更新某些东西。所以问题是：LLM 能不能去反思 traces——无论是它自己的 trace、以前 session 的 trace，还是别人的 trace？我觉得完全可以。我们在 eval、纠错、记忆里到处都能看到这种模式，本质上其实是一回事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Eval是 RL 的奖励信号，还是工程反馈机制？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我明白了。那接下来就很自然会问：你有了所有 traces，也有了 eval。那么这些 eval 到底是什么？它是强化学习的 reward signal？还是一种反馈机制，让工程师去改进 harness、让 agent 工程师去优化 harness？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：因为现在大家都不再手动写太多代码了，大家都在用这些 agent 工具。我们观察到一个很重要的模式：我们有一个 LangSmith MCP，也有 LangSmith fetch（一个 CLI）。因为 coding agents 特别擅长用 CLI。你把这些给智能体，它就能把 traces 拉下来，诊断哪里出了问题，然后把这些 traces 带进代码库里，从而修复它。这是我们正在看到的真实模式，而且我们非常非常非常想支持这种模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在这一点上，相比“用 eval 做强化学习奖励信号”，我对“把 eval 当作工程反馈、用于改 harness”的路径更乐观——至少对今天做 agent 应用的公司来说是这样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这听起来像是递归自我改进啊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得是，但还是有一个人类在环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到前面那个点：当它产出“初稿”时效果最好——它改 prompt，然后人类 review，这能让系统保持不跑偏。但我们确实……我们最近发布了 LangSmith Agent Builder，这是一个 no-code 的 agent 构建方式。其中一个很酷的功能就是 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在 memory 的工作方式是这样的：当你和 agent 交互时（注意它还不是后台自动跑的那种；它不会自己拉 traces），如果你对它说：“你不该做 X，你应该做 Y”，它就会去改自己的指令——这些指令本质上就是文件——然后直接编辑这些文件。这样未来它就会按新的方式表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是一种“自我改进”的形式。我们确实还想加入另一种机制：比如每天晚上跑一次任务，查看当天所有 traces，更新自己的指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：就是那种“做梦”的机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，“睡眠时间算力（sleep-time compute）”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;记忆与自我改进会成为护城河吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们再多聊聊未来。你现在最兴奋的是什么？听起来你聊了很多 memory。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我很看好memory。我觉得让智能体去改善自己，这非常酷，而且在很多场景下也很有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但也不是所有场景都用得上。比如 ChatGPT 加了 memory 功能，我其实用得不多，我也不觉得它显著增加了我对产品的粘性。我觉得原因之一是：我去 ChatGPT 时，大多数问题都是一次性的。我不太会反复做同一件事：我可能问软件，也可能问吃的、旅行……都很零散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 agent builder 里，你通常是为特定任务构建特定工作流。比如我有一个 email agent。而且我其实……它已经给我发邮件两年了。我之前在 agent builder 之外就有一个 email agent，它带有 memory。后来我们做了 agent builder，我想把它迁移进去，但它没有我之前的那些 memories。即便它的起始 prompt 一样、工具也一样，但因为缺了记忆，它现在的体验就明显差很多。我到现在都还没完全切过去，因为它现在确实不如之前那个好用——说白了，它现在“有点烂”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，如果我持续和它交互，它会变好，它会不那么烂。但这也恰恰说明：memory 可能会成为真正的护城河（moat）。而且我绝对相信，我们已经到了一个阶段：LLM 可以看 traces，然后改变自己代码里的某些东西。问题在于：怎么把这件事做得安全、并且在用户层面可接受。但我认为，在一些特定场景里（不是所有场景），我们会越来越多看到这种能力。至于 ChatGPT 这种通用聊天产品，我仍然不确定这种形态的 memory 是否有用，至少目前我不确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你觉得和长时序智能体一起工作的 UI 会如何演化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：我觉得大概率需要同步模式（sync）和异步模式（async）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长时序智能体运行时间可能很长，默认应该是异步管理：如果它要跑一天，你不会一直坐在那里等它结束。你很可能会启动一个、再启动一个、同时跑很多个。所以这里会涉及到异步管理：我觉得像 Linear、Jira、看板，甚至 email，都可以作为 UI 设计的参考——如何去管理一堆异步运行的 agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但与此同时，很多时候你又会想切换到同步交流。因为 agent 最后给你返回一份研究报告，你可能需要立刻指出：它这里写错了，你要给反馈。聊天界面在这方面其实已经挺不错的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我唯一想补充的是：现在很多 agent 不仅是在“对话”，它还会去修改文件系统里的文件。所以你必须有一种方式去查看“状态”（state）——也就是它改了什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这在编程领域尤其明显：IDE 依然被使用，是因为当你想手动改代码时，你需要看见那个“当前状态”。即便我启动 Claude Code，它跑完后，我有时也会打开来看它到底写了什么代码。所以“能看到状态”这件事很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 在 Claude “co-work”（这里指那类协作式工作流）里做了一个很酷的设计：你设置它时要选择一个目录，等于你在告诉它：“这就是你的环境。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在编程里当然也是常态：你打开 IDE 到某个目录。但我觉得把它明确成一个心智模型很有帮助：这就是你的 workspace（工作区）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个 workspace 也不一定非得是本地目录：它可以是 Google Drive、Notion 页面，或者任何能存储状态的地方。你和 agent 就是在这个状态上协作：你启动它，让多个任务异步跑；然后切到同步模式，在 chat 里和它讨论，但同时你还能看到它正在协作的“状态”。这就是我目前看到的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：所以这也就是你说的“agent inbox”的想法：为了进入 sync 模式，agent 需要能联系到你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase：对，没错。我们大概一年前发布过 agent inbox，理念是“ambient agents”：它们在后台跑，必要时来 ping 你。但第一版其实没有 sync 模式：它 ping 你，你回一句，然后你就等它下一次再 ping 你。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但很多时候，我切到邮件去回复它时，我其实只回很短的话，而且我不想再切出去然后干等——我（对方）很重要，所以我更想直接进入一种“同步对话”的模式，跟 agent 把这个问题当场聊完。所以我们后来做了一个关键改动：当你打开 inbox 时，会直接进入 chat，而 chat 是非常同步的。这是一个很大的 unlock（突破点）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在认为：只有 async 模式，目前还不太够。也许未来如果 agent 强到你几乎不用纠正它，那么纯异步会更可行。但至少现在，我们看到人们在 async 和 sync 之间来回切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人： 你怎么看 code sandboxes（代码沙箱）？是不是每个 agent 最终都会配一个 sandbox？也包括“能用电脑”、能上网用浏览器这种能力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Harrison Chase： 这是个特别好的问题，我们也一直在想。就目前的经验来看，“写代码/跑代码”这条路明显比“直接操作浏览器”更成熟、更好用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以短期内，如果要在这些能力里挑一个最可能成为标配的，我更看好的是代码执行（code execution）——也就是给 agent 一个能安全运行脚本、验证结果的环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，文件系统（file system）我几乎是“坚定派”：不管是本地目录、还是背后用数据库实现的“虚拟文件系统”，agent 总得有个地方能存状态、存中间结果、随时回查，这对上下文管理太关键了。比如：&lt;/p&gt;&lt;p&gt;做 compaction（上下文压缩）时，把完整内容丢到文件里，需要再查就去读；工具调用返回特别长时，不塞进上下文，改成写文件、让 agent 自己按需读取。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于“coding”（让 agent 真正去写代码），我没那么绝对，但我大概90% 站在“需要”这一边。因为很多长尾任务里，写脚本依然是最通用、最强的手段——你很难找到同等级的替代品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也可能出现另一类场景：如果你做的是高度重复、流程固定的事情，未必每次都要写很多代码；但即使这样，文件系统仍然重要，因为重复流程会不断产生上下文和状态，你还是要做上下文工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再说浏览器使用（browser use）：从我们目前看到的效果来说，模型还不够稳定。也许可以让 coding agent 通过 CLI 的方式“间接”完成一些浏览器相关任务（算是一种近似解），我确实见过一些挺酷的实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而所谓 computer use（直接操作电脑界面）则更像是介于两者之间的混合形态，目前还有不少不确定性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以总结一下：我非常喜欢 code sandboxes，我觉得它会成为 agent 能力栈里很关键的一块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太棒了。Harrison，真的非常感谢你今天来参加节目。你一直都能在 agent 这条路上看到未来，能和你聊“上下文工程如何演化到今天的 harness 与长时序智能体”，真的特别过瘾。感谢你推动这个未来，也感谢你一直愿意和我们聊这些。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Harrison Chase：谢谢邀请。我希望未来还能再来一次，然后证明我今天说的全部都是错的。因为预测未来真的很难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=vtugjs2chdA&amp;amp;t=1s&lt;/p&gt;</description><link>https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</link><guid isPermaLink="false">https://www.infoq.cn/article/2XfMOshHpdVVKjB2hxms</guid><pubDate>Thu, 29 Jan 2026 08:18:49 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>Linus 之后的 Linux？内核社区终于写下“接班预案”</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/94/945c92e1d75cd68d349118286393a8f5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Linus Torvalds 常开玩笑说自己会“活到永远”。但以防万一，Linux 内核社区现在也准备好了一套交接方案——只是这份方案并没有点名具体的接班人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果 Torvalds 发生意外，或者哪天决定退休，Linux 不再把一切寄托在“到时候再说”。核心内核社区已经正式起草了一份项目连续性计划：一旦顶层维护者出现空缺，应该如何在最坏情况或有序过渡中，选出新的顶层维护者（可能是一人，也可能是多人），确保项目长期稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 本人则明确表示自己暂无退休打算。被问到未来是否会交棒时，他依旧以一贯的幽默回应，暗示自己更倾向于“继续干下去”。随后他又补充了一个更现实的理由：家里人同样不希望他突然闲下来，尤其是太太，大概更不想每天被一个无所事事、没事找事的丈夫缠着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这份新的“为计划而写的计划”由资深内核贡献者 Dan Williams 起草，并在最近于东京举行的 Linux Kernel Maintainer Summit 上讨论。Williams 介绍它时还自嘲：这是个“与我们终将走向死亡相关、但很振奋的话题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不指定唯一继承人&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 也解释了这次为何会把“接班”议题正式摆上台面：部分原因是他此前与 Linux 基金会的合同在去年第三季度到期，基金会技术顾问委员会的人都知情。虽然合同随后已续签，但这段时间确实促使大家把风险管理讨论得更具体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;计划并没有给出一个“唯一继承人”。相反，它明确了一套选择流程：一旦需要交接，由社区召集一次类似“秘密会议”的讨论机制，集中权衡候选人或候选团队，尽量做出对项目长期健康最有利的决定。有维护者开玩笑说，干脆学选教皇：把人都锁在房间里，等决定出来再放出一缕白烟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文件提到一个开源圈常说的“公交车系数”（bus factor）梗：假设项目的关键人物哪天突然“消失”（比如出了意外），项目还能不能照常运转？因为 Torvalds 仍是顶层合并与发布的最终把关人，所以从风险角度看，Linux 在这一环节几乎等同于“系数为 1”——也就是关键节点过度依赖一个人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过在现实中，大家也大致心照不宣：真要临时接手，“企鹅之王”的角色多半会落到 Greg Kroah-Hartman 身上——他是 Linux 内核稳定分支的维护者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Torvalds 还在 2024 年和好友 Dirk Hohndel（Verizon Open Source 负责人）聊过这个话题。Hohndel 认为，要成为 Linux 的主维护者，需要极其丰富的经验；而目前最自然的“备份选项”就是 Greg Kroah-Hartman。Torvalds 的看法则更偏向长期视角：关键不在于某个人，而在于谁能获得社区的信任；这种信任通常来自长期参与、稳定协作，以及社区对其工作方式的充分了解，但“资历够久”并不意味着必须三十年如一日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kroah-Hartman 也确实曾短暂顶上过。2018 年 Torvalds 一度暂离内核工作、反思并改善自己对待其他开发者和维护者的方式时，Kroah-Hartman 曾临时承担顶层职责。不过，他的年龄甚至比 Torvalds 还大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;或许会由多人共同接棒&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此也有人提出，与其再找一位新的“终身仁慈独裁者”（BDFL），不如把顶层维护者的职责拆分给多位值得信赖的开发者共同承担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;56 岁的 Torvalds 仍然是几乎所有进入 torvalds/linux.git 变更的最终裁决者。他常自嘲 Linux 的核心圈子正在“变老”。而维护者疲劳、以及核心子系统负责人后继乏人等问题，让这种紧迫感越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以确定的是：Torvalds 并不会在短期内让位。他仍会继续监督主线开发，并一直做到自己“做不动”为止。只是至少现在，那个终极的“Linus 依赖”风险终于有了明确的处理流程——等到真正需要的那一天，可以直接套用，而不必临时抱佛脚。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&quot;&gt;https://www.zdnet.com/article/linux-community-project-continuity-plan-for-replacing-linus-torvalds/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</link><guid isPermaLink="false">https://www.infoq.cn/article/rxKQhGxLH5lYkeo51kCZ</guid><pubDate>Thu, 29 Jan 2026 08:12:34 GMT</pubDate><author>Tina</author><category>开源</category></item><item><title>不跟英伟达走老路，这家GPU公司的技术架构藏着哪些关键解？</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;采访嘉宾 | 天数智芯 AI 与加速计算技术负责人 单天逸&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于国产 GPU 行业来说，没有哪个时间节点比当下更宝贵。在政策支持硬科技企业上市的背景下，国产 GPU 迎来了难得的上市黄金窗口期。但上市并非终点，在敲钟的那一刻，下一战场大幕已经拉开——GPU 厂商的技术路线、产品能力和长期判断，被放到了更公开也更严苛的舞台上，谁能撑起资本市场和大众期待，谁就能撑起市值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么，天数智芯上市后的首场发布会能够在业内形成广泛讨论。它以极其务实的工程师表达方式，把架构放回到国产 GPU 技术叙事的中心。在 1 月 26 日召开的天数智芯“智启芯程”合作伙伴大会中，围绕架构层的创新与思考占据了相当比重。基于这些创新点与思考，天数智芯公布了过去一代以及未来三代的架构路线图：&lt;/p&gt;&lt;p&gt;2025 年，天数天枢架构已经超越英伟达 Hopper，在 DeepSeek V3 场景中实测性能数据超出 20%；2026 年，天数天璇架构对标 Blackwell，新增 ixFP4 精度支持；2026 年，天数天玑架构超越 Blackwell，覆盖全场景 AI/加速计算；2027 年，天数天权架构超越 Rubin，支持更多精度与创新设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/93a5511a47ea59c34947fa5622e43f57.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;国产 GPU，开启 AI++ 计算新范式&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据天数智芯公布的架构路线图及阶段发展目标，在 2027 年之前，天数智芯将通过多代产品完成对英伟达的追赶；在 2027 年之后，将转向更富创新性的架构设计，聚焦更具突破性的超级计算芯片架构设计。看似宏大，但对于仍处于爬坡阶段的国产 GPU 行业来说，这条路径实际上相当务实——只有在工程化能力上完成对标甚至是超越，国产 GPU 才有资格进入更大规模的生产环境中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在规模化落地阶段的竞争，焦点早已从峰值性能指标转向有效计算能力。当 Token 成为 AI 时代最基本的生产资料，当算力消耗开始对标真实业务产出，无论是国际顶尖 GPU 厂商还是国内 GPU 企业，核心命题都只有一个：如何在真实业务中，把算力转化为有效的 Token。这似乎又将大家都拉到同一起跑线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一命题，天数智芯提出了两条明确的架构判断：其一，回归计算本质；其二，提供高质量算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;回归计算本质，核心在于“不设限”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年，规模的快速扩张带来了阶段性的产业繁荣，也使得算力实现野蛮增长。但这种粗放式发展，也带来了能效比失衡、算力资源严重浪费等问题。背后的根因十分复杂。以开车行驶为例，路途中可能会遇到雨雪冰雹天气、崎岖道路等各种复杂情况。物理、芯片、系统世界也是如此，计算、通讯、存储都会带来各种障碍。所以，幻想奔跑在平坦的赛道上毫无意义，产业真正需要的，是能够翻山越岭的全能越野车。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64709d562cae987749119744a750d556.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;广义上，芯片可分为专用芯片和通用芯片：专用芯片类似“应试教育”，它的优势和边界都很清晰，能加速特定算法、特定指令，比如矩阵乘法、Softmax 这些主流任务，但一旦计算范式发生变化，适应空间就会迅速收紧；通用芯片的设计哲学，不是为了押中某一类算法，而是回归计算本质，覆盖更广泛，甚至全新的计算需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是天数智芯坚持推出并量产通用 GPU 的根因。在其看来，硬件与算法的关系本来就不应该相互掣肘，算力的僵化不应限制算法的进化，而是通过通用算力为探索未知算法提供一个坚实的底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑探索未来算法的关键，实则就是“不设限”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这一判断，天数智芯的芯片设计哲学，在计算层面追求的是覆盖几乎所有的数学运算图谱，而非某一类、某一种计算：从 Scalar、Vector、Tensor 到 Cube，支持从高精度科学计算到 AI 精度计算，从 MMA 到 DPX，不管是 AI 的 Attention 机制、前沿的科学计算，还是未来的量子计算相关模拟，天数智芯全都支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行层面，追求的是更高的算力利用率：大、中、小任务会被精准分配到不同的计算单元中执行，配合高密度的多任务核心设计，算力可以被拆解、调度得更加精细，从而减少算力浪费，提高计算效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d21f6e06061719cd4a9a199eb6e5fed0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“不设限”的设计哲学，让天数天枢架构得以实现三大创新，这也是天枢能够超越英伟达 Hopper 架构的根因：&lt;/p&gt;&lt;p&gt;TPC BroadCast（计算组广播机制）设计：不是简单粗暴地放大带宽，而是从单位带宽的使用效率入手，存在相同地址的数据时，芯片内部的 load store 单元不会进行重复、无用的访问，而是在上游进行 BroadCast，减少不必要的内存访问次数，从而有效降低访存功耗，等效提升访存带宽，用更小的功耗和面积实现相同的功能。Instruction Co-Exec（多指令并行处理系统）设计：在指令执行层面，通过 Instruction Co-Exec 设计实现了多种指令类型的并行执行能力，不仅支持 Tensor Core 与 Vector Core 的并行协同，还将 Exponent 计算、通信等操作一并纳入统一调度。在天数 IX-Scheduler 模块中，通过极低的成本增强了不同指令之间的并行处理能力，无论是 MLA、Engram，还是面向更复杂模型场景的计算需求，都可以在这一并行框架下被同时处理，从而提升整体执行效率。Dynamic Warp Scheduling（动态线程组调度系统）设计：随着 MoE 架构在大模型中被广泛采用，模型厂商普遍面临推理效率低等现实挑战。为提升并行度，微架构层面允许芯片中同时驻留更多 warp，但 warp 的增加也意味着对计算资源的竞争更为激烈。为此，天数智芯首创了 Dynamic Warp Scheduling 机制，通过动态调度让不同 warp 在资源使用上实现有序协作，避免计算资源闲置，也减少了对同一资源的无序争抢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三项设计的出发点本质上都指向相同的目标：高性能与高效率。数据显示，这些创新让天数天枢的效率较当前行业平均水平提升 60%，基于这些效率优势，实现在DeepSeek V3 场景平均比 Hopper 架构高约 20% 性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这三项设计中可以看出，天数智芯在架构层面的创新，并不是围绕某一个具体模型或算子展开，而是试图打破 GPU 通用范式边界。天数智芯 AI 与加速计算技术负责人单天逸在接受采访时表示，在天数智芯提出 Dynamic Warp Scheduling 设计之前，几乎没有人从调度机制的角度去思考，还能为 MoE 带来哪些性能空间。从更深层次意义来看，这类微架构层面的调度和优化，一直是英伟达、AMD 等巨头保持领先的“内功”，天数智芯在这些单点上的突破，实际上也是国产 GPU 向顶级玩家看齐的重要一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;提供高质量算力：高效率、可预期、可持续&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在天数智芯的架构语境中，回归计算本质并不是一个抽象的口号，而是实现高质量算力的前提条件。只有当 GPU 从底层开始真正对计算负责，高质量算力才成为可能。基于这一判断，天数智芯将高质量算力拆解为三个核心维度：高效率、可预期与可持续。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7ace836faccd159427d7c71b330df3ca.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;高效率意味着能为客户创造最优的 TCO（总体拥有成本），节省使用成本；可预期则通过精准的仿真模拟，让客户在拿到芯片、部署算力之前，就能清晰预判最终的性能表现，做到所见即所得；可持续指的是从现在主流的 CNN、RNN，到当下火热的 Transformer，再到未来还未诞生的全新算法，算力始终能无缝适配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这三个方向，天数智芯在架构及系统设计上，选择从多任务并行处理、长上下文 IX-Attention 模块、IX-SIMU 全栈软件仿真系统以及 IXAI++ 算力系统多个层面同步推进。这几项，其实哪个都值得单独展开探讨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，基于“不设限”的设计理念，在当前 PD 分离的架构下，天数智芯的 GPU 不只做计算，还支撑通信、KV 数据传输这些关键任务，通过打造 Ⅸ 并行任务处理模块，GPU 能精准调度 KV 传输、多路多流、计算与通信等各类任务，让它们并行不冲突。在真实业务场景中，该模块成功帮助头部互联网客户实现了端到端 30% 的性能跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b473b037acf6e72c5b667dd838e26e33.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了提高算力可持续性，天数智芯统一了芯片内、外，来构建算力系统，并通过不断更新的软件栈和软件系统，三类库共同支持和保障多场景的高效运行。其中，AI 库、通讯库（ixccl）、加速计算库是基石，在基石之上，直接支撑各类神经网络模型CNN、Transformer、LSTM 与高性能计算的各个领域，并以此提供各类 AI 应用，包括支持 AI4Sci 的相关应用，如蛋白质结构预测（AlphaFold）、医疗影像分析（Clara）、气候模拟（Earth2）等，以及量子计算的平台 cudaQ、分子动力学 Gromacs，大规模方程组求解器 HPL 等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套算力系统被命名为 IXAI++，寓意为自我迭代，不止于 AI。其最终的目标是，成为一座连接算法创新与物理世界的桥梁，带领人类科技通往未知探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但给业内带来最多惊喜的，是 IX-Attention 模块和 IX-SIMU 全栈软件仿真系统。前者解决的是当前大模型推理中最具代表性的效率难题，后者解决的是企业部署算力系统最头疼的不可控难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型推理场景中，长上下文被普遍认为是最具代表性的效率难题之一。即便是在国际主流 GPU 架构上，Attention 的执行效率依然不高，如果不对其进行针对性优化，首字延迟将明显偏高，模型响应速度差，推理成本高昂，最终影响大模型在真实业务中的可用性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一痛点，天数智芯设计了 Ⅸ Attention 模块，从底层对 Attention 的执行路径进行重构：Attention 底层涉及 exponent、reduce、MMA、atomic 等多类指令与算子，Ⅸ Attention 模块的核心思路，是将这些分散的组件有机地拼装到一起，如同指挥一支乐队一般，确保多种乐器能够和谐共鸣。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/1931cf31a98c3a57d61ca5cbaf8caa44.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“其中的技术难点在于调度，多种乐器需要同时演奏，任何一个环节拖慢节奏，都会成为整个系统的瓶颈”，单天逸表示，在实际的长上下文推理中，Ⅸ Attention 模块有效改善了 Attention 的执行效率，带来了约 20% 的提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对企业部署算力系统最头疼的不可控难题，天数智芯搭建了 IX-SIMU 全栈软件仿真系统，这套仿真系统的目标，就是零意外、可预期。通过对芯片等硬件与软件执行策略的联合仿真，能精准输出任意模型的性能表现，提升算力在真实场景中的可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c15d79ed9346cf49898ee0193b585926.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单天逸表示，在算力系统的仿真与评估中，最难建模的是指令级别的硬件行为。IX-SIMU 的核心能力在于，能够对底层指令执行进行精细建模。在实际使用中，用户只需输入软件代码，IX-SIMU 便会自动整合 GPU、CPU、网卡、PCIe 等硬件组件，匹配网络拓扑，再结合软件策略、投机策略、Streaming LLM 策略、前缀匹配等各类策略，最终精准输出 Deepseek、千问等任意模型的性能表现，实现从单卡到万卡集群的 “精密扩展”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕高效率、可预期、可持续三大判断，天数智芯在算力侧从硬件架构到系统设计进行了整体布局，并用未来三代架构路线图提前回答下一个问题：当算力僵化开始掣肘未来计算，架构层还能怎么演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;决定上限的，最终还是应用和生态&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;架构代表的其实是下限，决定上限的，最终还是应用和生态。数据显示，截至 2025 年年底，天数产品已在互联网、大模型、金融、医疗、教育、交通等超过 20 个行业落地应用，服务客户数量超过 300 家，并通过软硬件协同优化，完成 1000+ 次模型部署，让产品能力真正达到商用级别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑这些场景应用的，早已不是一个产品的能力范畴，而是“产品 + 解决方案” 双轨模式，这一模式其实与英伟达定位非常相近，聚焦的都是解决方案落地。在大模型深入产业应用的当下，这套组合打法相当务实，毕竟应用落地才是唯一真理，谁能在企业真实业务场景中快速部署、持续稳定运行，谁就能赢得先机。在速度和兼容性上，天数智芯也交出了一份不错的答卷：国内新的大模型发布当天便能跑通，目前已稳定运行 400 余种模型、数千个已有算子与 100 余种定制算子，数千卡集群稳定运行超 1000 天。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这次发布会上，天数智芯面向物理 AI 场景落地，一口气发布了四款边端算力产品“彤央”系列：包括边端 AI 算力模组 TY1000、TY1100，以及边端 AI 算力终端 TY1100_NX、TY1200。 据了解，“彤央”系列产品的标称算力均为实测稠密算力，覆盖 100T 到 300T 范围。数据显示，在计算机视觉、自然语言处理、DeepSeek 32B 大语言模型、具身智能 VLA 模型及世界模型等多个场景的实测中，彤央 TY1000 的性能全面优于英伟达 AGX Orin。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在发布会中，天数智芯展示了“彤央”系列产品在具身智能、工业智能、商业智能和交通智能四大边端核心领域的落地应用：具身智能领域，为格蓝若机器人提供高算力、低延迟的“大脑”支撑；在工业智能领域，落地园区与产线，推动产线自动化升级；在商业智能领域，瑞幸咖啡数千家门店部署彤央方案，高效处理视频流、挖掘消费数据价值；在交通智能领域，与“车路云一体化”20 个头部试点城市合作，验证车路协同方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，天数智芯走的路线虽然是底层技术自研，但在生态上并非封闭。在生态建设上，天数智芯与硬件厂商、解决方案提供商等多家生态伙伴签署战略合作协议，进一步完善国产 AI 算力生态闭环。通过兼容主流开发生态，持续开放底层能力，降低开发者迁移和使用门槛。未来，天数智芯还会持续增加在生态共建上的资本与人力投入，从应用到芯片与开发者一同优化 AI 应用系统，共同为应用落地提供性能、性价比与生态易用的价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从底层架构到产品，从应用到生态，国产算力正在实现完整闭环，这种从芯片到生态的协同能力，不仅让国产算力更可用、更可持续，也为行业探索新模式提供了更多想象空间。&lt;/p&gt;</description><link>https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</link><guid isPermaLink="false">https://www.infoq.cn/article/hR5WX4alMiNZumPR5ukC</guid><pubDate>Thu, 29 Jan 2026 06:54:56 GMT</pubDate><author>凌敏</author><category>企业动态</category><category>芯片&amp;算力</category></item><item><title>智源多模态大模型登Nature，确立自回归成为生成式人工智能统一路线</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月28日，智源多模态大模型成果&quot;Multimodal learning with next-token prediction for large multimodal models（通过预测下一个词元进行多模态学习的多模态大模型）&quot;上线国际顶级学术期刊Nature，预计2月12日纸质版正式刊发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Nature编辑点评这项研究：Emu3 仅基于预测下一个词元（Next-token prediction），实现了大规模文本、图像和视频的统一学习，其在生成与感知任务上的性能可与使用专门路线相当，这一成果对构建可扩展、统一的多模态智能系统具有重要意义。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/4104e358f0c6b33a3c481f57a18d6760.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41586-025-10041-x&quot;&gt;https://www.nature.com/articles/s41586-025-10041-x&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2018年以来，GPT采用 “预测下一个词元（Next-token prediction，NTP）”的自回归路线，实现了语言大模型重大突破，开启了生成式人工智能浪潮。而多模态模型主要依赖对比学习、扩散模型等专门路线，自回归路线是否可以作为通用路线统一多模态？一直是未解之谜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源这项成果表明，只采用自回归路线，就可以统一多模态学习，训练出优秀的原生多模态大模型，对于确立自回归成为生成式人工智能统一路线具有重大意义。在后续迭代的Emu3.5版本，确实证明了这一范式的可拓展性，并达成预测下一个状态（Next-state prediction)的能力跃迁，获得可泛化的世界建模能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从语言到多模态：“预测下一个词元”的潜力与未解之问&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“预测下一个词元”彻底改变了语言模型，促成了如 ChatGPT等突破性成果，并引发了关于通用人工智能（AGI）早期迹象的讨论。然而，其在多模态学习中的潜力一直不甚明朗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在多模态模型领域，视觉生成长期以来由结构复杂的扩散模型主导，而视觉语言感知则主要由组合式方法引领 ，这些方法通常将CLIP编码器与大语言模型（LLMs）结合。尽管已有一些尝试试图统一生成与感知（如Emu和Chameleon），但这些工作要么简单将LLM与扩散模型拼接在一起，要么在性能效果上不及那些针对生成或感知任务精心设计的专用方法。这就留下了一个根本性的科学问题：单一的预测下一个词元框架是否能够作为通用的多模态学习范式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就此，智源提出了Emu3，基于“预测下一个词元”的全新多模态模型，将图像、文本和视频统一离散化到同一个表示空间中，并从零开始，在多模态序列混合数据上联合训练一个单一的 Transformer。这一架构证明了仅凭“预测下一个词元”，就能够同时支持高水平的生成能力与理解能力，并且在同一统一架构下，自然地扩展到机器人操作以及多模态交错等生成任务。此外，研究团队还做了大量消融实验和分析，验证了多模态学习的规模定律（Scaling law）、统一离散化的高效性、以及解码器架构的有效性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/58/5876defd768a77c0f403e3b928a37a11.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Emu3 架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实验显示，Emu3在生成与感知任务上的整体表现可与多种成熟的任务专用模型相媲美：在文生图任务中，其效果达到扩散模型水平；在视觉语言理解方面，可以与融合CLIP和大语言模型的主流方案比肩。此外，Emu3还具备视频生成能力。不同于以噪声为起点的扩散式视频生成模型，Emu3通过自回归方式逐词元（token）预测视频序列，实现基于因果的视频生成与延展，展现出对物理世界中环境、人类与动物行为的初步模拟能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;从模型到范式：Emu3对多模态学习的启示&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同于 Sora 的扩散式视频生成，Emu3&amp;nbsp;采用纯自回归方式逐词元（token） 生成视频，能够在给定上下文下进行视频延展与未来预测，并在文本引导下生成高保真视频。此外，Emu3 还可拓展至视觉语言交错生成，例如图文并茂的菜谱生成；也可拓展至视觉语言动作建模，如机器人操作VLA等，进一步体现了“预测下一个词元”的通用性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智源研究团队对相关研究的多项关键技术与模型进行了开源，以推动该方向的持续研究。其中包括一个稳定且通用的视觉分词器（tokenizer），可将图像与视频高效转换为离散词元来表示。同时，研究通过大规模消融实验系统分析了多项关键技术的设计选择，例如：分词器（tokenizer）码本尺寸、初始化策略、多模态dropout机制以及损失权重配置等，揭示了多模态自回归模型在训练过程中的动态特性。研究还验证了自回归路线高度通用性：直接偏好优化（DPO）方法可无缝应用于自回归视觉生成任务，使模型能够更好地对齐人类偏好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究有力表明了预测下一个词元可作为多模态模型的核心范式，突破语言模型的边界，在多种多模态任务中展现了强劲性能。通过简化复杂的模型设计、聚焦统一词元，该方法在训练与推理阶段均展现出显著的可扩展性，为统一多模态学习奠定了坚实基础，有望推动原生多模态助手、世界模型以及具身智能等方向的发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此研究基础上，悟界·Emu3.5进一步通过大规模长时序视频训练，学习时空与因果关系，展现出随模型与数据规模增长而提升的物理世界建模能力，并观察到多模态能力随规模扩展而涌现的趋势，实现了“预测下一个状态”的范式升级。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</link><guid isPermaLink="false">https://www.infoq.cn/article/Xr6XspENqsQuZuVvwQqQ</guid><pubDate>Thu, 29 Jan 2026 06:47:46 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“AI在一线：开发人员如何重塑软件开发流程” ｜圆桌讨论</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从代码生成到自动化文档，人工智能已经开始渗透到软件开发生命周期的几乎每个阶段。但除了炒作之外，实际上发生了什么变化？我们询问了一群工程师、架构师和技术领导者，AI辅助工具的兴起如何重塑软件开发的既定节奏，以及他们在现实世界中采用AI后学到了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;讨论嘉宾&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mariia Bulycheva——Intapp高级机器学习工程师Phil Calçado——Outropy首席执行官Andreas Kollegger——Neo4j高级开发者倡导者May Walter——Hud.io创始人、首席技术官&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：AI辅助工具的兴起对你们组织的软件开发过程有何影响？它们是否改变了你们对软件架构的思考方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：AI辅助工具加速了原型设计，并减少了在重复编码任务上花费的时间，使我们的团队能够更多地关注架构决策和设计复杂的在线实验，这对于大规模迭代改进复杂的推荐系统至关重要。从数字平台典型的大量多模态数据中获得初步洞察也变得更快、更顺畅、更一致，因为我们可以将初始数据分析委托给了AI。&amp;nbsp;我们工作的另一个非常重要的方面是跟上我们领域科学发展的快速步伐。每年，在顶级会议上都会发表数千篇新的研究论文，过去阅读它们并确定哪些可能与我们团队的日常ML任务相关是非常耗时的。今天，AI工具提供了高质量的摘要，甚至突出显示哪些方法可能适用于我们的用例。这已经导致了几个新建模想法的快速实现，否则我们可能需要花费数周甚至数月的时间来发现和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：绝对有。我们运行的是一个消费者参与平台，其功能之多，任何正常人都无法全部记住。例如，最近，我们需要改变我们处理时区的调度方式。代码变更本身可能只有10行，但真正的工作是深入数百个涉及调度的地方，弄清楚每个地方的假设，并添加单元测试以断言调用站点不会中断，而是行为的变化。我们原以为这将是一个为期六个月的项目，因为我们需要逐步研究并进行小的更改。&amp;nbsp;有了像Cursor和Claude Code这样的工具，我们大大缩短了这个时间。它们帮助我们找出所有受影响的位置，为每个位置生成单元测试，并将推出分成按子系统分组的小PR。每个PR都带有对所属团队的上下文敏感的描述——不仅仅是“修复调度，请审查”，而是解释为什么以及在他们的世界中预期的影响。&amp;nbsp;因此，尽管我们像其他人一样看到了原始代码输出的增加，但在我们这样成熟的、超大规模的系统中，最大的提升在于AI如何帮助我们研究自己的代码库，将无聊但必不可少的安全检查整合在一起，使系统性变更变得不那么可怕。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在我们组织中，所有员工现在都可以使用AI辅助工具。对于表面层级的界面设计，这些工具帮助我们更快地迭代，探索新想法，并解锁新方法，如氛围编码，专注于更高层次的设计和策略。&amp;nbsp;但我们也确实遇到了AI的局限性。像许多组织一样，我们发现大语言模型（LLM）在需要深厚领域专业知识和全局架构整体视图的高度专业化代码上挣扎。我们的代码库本身就超过了任何LLM上下文窗口的容量，而这些模型本身也没有在其中的独特复杂性上进行训练。简而言之，AI不能发明它不理解的东西。因此，我们有意采取了一种以人为中心的方法：虽然AI帮助我们加速和增强，但推动软件架构突破的是我们工程师的专业知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：AI辅助工具极大地缩短了从想法到工作代码的路径。一旦意图明确，迭代周期就会显著缩短。开发人员正在从代码的唯一作者转变为更像是管理者的角色——指导代理，验证输出，并确保需求得到真正满足。&amp;nbsp;AI之前，架构是关于团队之间的所有权和可扩展接口。这引入了一个新的维度：上下文架构——设计代理生成生产就绪代码所需的输入、脚手架和护栏。上下文工程正在成为系统的核心部分，它简化了在复杂环境中快速构建的能力，如分布式和基于事件的系统。&amp;nbsp;但速度带来了一个新的瓶颈：为生产准备AI生成的变更。即使审查有了AI辅助，挑战也不再是关于发现语法错误，而是在大型、大规模的系统中验证意想不到的后果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：人工智能的采用如何影响团队内部的入职流程？你们团队或组织中的初级开发人员是否受到软件开发过程中采用人工智能的影响？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：人工智能工具可以通过提供即时的代码示例、文档摘要和测试建议，显著加快学习过程，这些都支持了初级开发人员。在处理个性化和推荐系统等复杂领域的团队中，这一点尤其有用，因为现在初级人员可以更快地探索新的代码库，而不必总是依赖高级工程师。同时，我们将他们与更有经验的同事配对，以确保他们学习潜在的基本建模和系统设计原则，而不仅仅是捷径。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我们刚刚让暑期实习生展示了他们的项目，几乎每个人都把人工智能称为救星。进入一个有10年历史的Rails代码库，其中包含数千个可移动的部分，这是令人生畏的。但是能够对Cursor或Claude Code说，“我是一名懂Python和C++的大三学生，请用我熟悉的方式解释这个Rails代码”，这意味着他们可以在几周内提高效率，而不是仅仅把时间花在弄清楚基础知识上。&amp;nbsp;而且，不仅仅是实习生。在这个庞大的系统中，即使是高级工程师也需要比在小公司更多的准备时间。AI并没有消除对系统的实际理解，但它确实减轻了“我们在哪里处理认证？”或“我们是否已经有了观察者模式的实现？”这类问题的压力。&amp;nbsp;当然，这里有一个问题。生成式AI擅长复制模式，这通常意味着我们不希望再看到的遗留风格和架构。因此，我们不得不适应。我们正在使我们的工作流程和架构更加适应AI，并且我们已经开始将当前的指导方针直接嵌入到Claude Code和Cursor的智能体中。这样，当AI提供帮助时，它会引导人们走向现在，而不是过去。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：人工智能的采用增强了我们的入职流程，特别是对于新接触图数据库的初级开发人员。虽然人工智能不能取代经验丰富的导师的指导，但它通过帮助新员工更快地上手，补充了我们现有的入职资源。&amp;nbsp;入职培训不仅仅是教授编码技能。它是关于建立领域专业知识的。编码能力很重要，但更重要的是理解要编写什么代码以及为什么。这就是为什么我们的入职开发人员，他们对代码库及其架构有深入的了解，在向初级团队成员传授专业知识和上下文方面发挥着关键作用。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：人工智能降低了贡献的障碍。现在，一个新开发人员可以在他们的第一天就写出可用的代码——这与早期工作仅限于样板或错误修复的日子相比，是一个戏剧性的转变。但真正的机会不在于速度；而是在于能力和范围的深度。&amp;nbsp;我最常听到的担忧是，人工智能有使入职变得肤浅的风险——初级人员可以在不理解代码为何以某种方式行为的情况下生成代码。我的经验恰恰相反。当代码生成与运行时反馈配对时，初级开发人员从一开始就接触到系统思维：架构在负载下的行为如何，依赖项如何相互作用，以及变化如何波及到业务结果。工程师成为智能体代码生成过程中的业务大使。&amp;nbsp;他们不再需要花几个月的时间来处理低价值的工作，而是能够处理团队的更多任务。如果做得好，这不会跳过步骤——它会加速步骤。有了正确的文化和期望设定，初级工程师可以更快地发展成为全面发展的工程师，因为他们不仅学习如何编写代码，还学习了为什么它在系统环境中很重要。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在你们团队或组织中，你们是否测量过AI辅助开发对生产力或质量的影响？你们学到了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们在样板代码和单元测试生成方面看到了明显的生产力提升，甚至在为推荐系统设置模拟实验方面也是如此。然而，当处理影响大规模客户体验的关键系统时，真正的好处来自于将AI辅助与深度工程师参与结合起来。我们了解到，虽然AI提高了生产力，但质量仍然取决于仔细验证和清晰的指标和测试。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado:：没有正式的测量。坦率地说，我不相信大多数抛出的“生产力”数字。在软件领域，你可以操纵指标，直到它们说出你想要的任何东西，而AI的炒作周期使这变得更糟。事实上，人们再次认真计算代码行数，只是为了增加一轮融资或提高股价，这是令人尴尬的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：在前端方面，我们看到了生产力的提升，特别是对于那些使用Cursor等工具的工程师。我们的许多工程师已经使用AI支持来更快地理解、进行表面编码和测试我们的代码库，但我们从AI中看到的真正影响是对开发人员体验的影响。通过使用AI工具来支持他们的一些活动，我们的工程师现在有更多的时间发挥创造力，并最终改进他们解决问题的方式，并为他们的工作创造新的方法。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：是的，我们了解到的第一件事是，大多数常见的度量标准并没有太大意义。公认的代码行数、提交次数、PR：AI可以立即夸大这些数字，但它们只是工程生产力的虚荣指标。&amp;nbsp;真正的信号存在于下游。发布稳定性、事故频率、随叫随到的时间，甚至代码变更率，都能告诉我们是否真的在加快速度，或者只是在制造更多的脆弱性。AI将速度转移到了管道的前端，但除非验证循环紧密，否则债务会在后期显现——以缺陷、回归和精疲力竭的团队的形式。&amp;nbsp;从第一天开始，通过持续的生产反馈，我们可以看到真相所在：功能开发变得更快，但审查周期变得更长了，部署后的错误也出现了。&amp;nbsp;教训是，AI生产力需要一个学习曲线和迭代方法。一旦度量，可以逐步改进采用，以捕捉优势——同时避免因快速交付但存在稳定性问题窒息的陷阱。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：为了有效地使用AI工具，你们的团队或组织中有哪些非技术方面的东西需要改变？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：最大的变化是在心态上。团队必须从期望AI建议是“正确”的，转变为将它们视为需要彻底验证、讨论和测试的起点。这种文化转变鼓励了实验和跨学科合作，将对确定性的关注转变为对探索的关注。在大规模个性化工作中，我们还需要与产品和法律团队就负责任的数据使用和可复制性达成一致。这些协议创造了护栏，使工程师能够安全地探索和部署AI辅助解决方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：我认为关于生成式AI工具的最大事情是：这超越了编码。是的，像任何其他工具一样，你必须注意副作用。生成式AI可以很容易地生成内容：代码、PR评论、技术规范、电子邮件、Slack消息。它也使得总结大量文本并过滤掉非必要的内容变得非常容易。&amp;nbsp;这两个特性的结合创造了一个奇怪的激励：人们生成了大量的低信噪比内容，然后其他人再次使用AI将其过滤回去。这是极其无效的。我们已经开始内部讨论在制作内容时正确使用AI的方式。剧透：这不是让AI为你写作，而是使用AI帮助你写得更好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：我们在AI的早期阶段建立了一个AI伦理委员会，组织中的代表们更好地理解和指导AI如何影响我们的业务的每一个方面。所有技术都可以成为一股善的力量，但它也需要有意识的思考、行动和指导。&amp;nbsp;因为我们信任客户数据，我们的开发人员需要在引入AI作为助手的任何领域都应用更高的敏感性，从简单的计划文件和电子邮件线程到代码库本身。随着我们采用、集成和扩展AI，我们所有的开发人员都必须确保人类判断，而不是AI，指导和监督每一步。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：最大的变化不是技术上的，而是文化上的。开发人员自然会单独采用工具，但当AI被视为个人生产力技巧时，它的效果并不好。只有当它成为共享流程的一部分，有一致的验证步骤和清晰的责任时，它才会有效。此外，AI工具在缺乏上下文时不会失败，而是产生不准确的回应，这可能会损害用户的信任并增加变更的摩擦。&amp;nbsp;在10名工程师的情况下，每个人都可以以自己的方式进行实验。在100名工程师的情况下，这种方法就会崩溃。不同的智能体独立生成代码会造成分裂和风险。我们转向了共同的设置和共享的工作流程，这样AI不仅仅是帮助个人更快地移动，而是使整个团队更快地移动。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们在管理AI辅助编码方面设置了哪些护栏（文化、道德或技术），以及你们如何管理个人、团队和组织对AI输出的信任问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：我们将AI输出视为重复性或样板任务的“初稿代码”，这些代码总是经过单元测试和同行评审。在文化方面，我们强调责任：提交代码的开发人员负责，无论是否有AI辅助。对于机器学习工作流程，我们不信任AI直接生成模型，相反，在任何模型更改甚至可以考虑用于生产之前，我们依赖于针对既定基线的自动离线评估。这确保了AI驱动的贡献达到了与人类编写的代码相同的质量标准。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil calado：这仍然是一个非常初级的实践，所以我们一直在尝试不同的护栏和工具。在安全和合规方面，我们的立场从一开始就很明确：作为一个处理世界上一些最大品牌数据的上市公司，我们必须将相同的治理实践应用于AI编码工具，就像我们其他地方所做的一样。几年前，这意味着落后于曲线，但今天大多数供应商都有坚实的企业计划，所以我们可以安全地使用最先进的模型，而不会妥协安全性或可审计性。&amp;nbsp;文化上，我们很早就设定了期望：仅仅因为一个AI工具编写了变更，并不意味着它不是你的代码。你仍然拥有它，你需要把每一行都当作是你自己打出来的一样。这与使用IntelliJ的提取方法重构没有什么不同，在这种情况下，它可能自动化了机械操作，但你仍然需要理解并验证结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：大型企业软件可以提供防止AI生成错误的保障，但更高的准确性、上下文和可追溯性是使AI输出可解释和可验证的关键，而不仅仅是性能。这就是为什么我们引入了一个广泛的测试计划，涵盖了从单个单元测试到详尽的生产级验证的所有内容。&amp;nbsp;与此同时，我们的工程师在纪律和创新之间保持平衡至关重要。我们鼓励工程师尝试各种想法，探索可能尚未准备好投入生产的项目。这种环境允许快速迭代和创造力，同时确保只有最有价值和经过充分测试的创新才能过渡到生产。其结果是一种独特的平衡：保持客户的信任和稳定，同时不断推进图驱动的创新，使AI更准确、透明和可解释。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：我们必须赢得对AI输出的信任，而获得信任的唯一方法便是创造上下文。每个AI生成的变更都经历了与人类编写的代码相同的标准——审查、测试、验证——但有一个额外的要求：它必须在运行时证明自己。&amp;nbsp;对我们来说，信任不是来自对模型的信念；而是来自观察代码在现实世界条件下的行为。新版本的性能和旧版本一样吗？它是否引入了新的错误或在负载下改变了性能？当运行时上下文持续可用时，AI就不再是黑盒。它变成了一个可以信任的伙伴，因为它与工程师依赖相同的信号进行推理。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：你们认为软件开发团队低估了AI编码工具的哪些方面？你们认为有哪些当前的AI增强型开发工作流程或模型被过度炒作，哪些仍然没有得到充分利用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Mariia Bulycheva：许多团队低估了上下文管理的重要性，因为AI的效果取决于你提供的上下文（代码库、文档、架构、在线测试的实验设置）。在大型系统中，这意味着不仅要管理代码片段，还要管理模型性能数据、日志和实验历史，以有效指导AI工具。过度炒作：AI据说取代了工程判断的“一键式开发”。未充分利用：AI辅助调试、实验设置和复杂ML工作流的文档记录，这可以大幅降低长期维护成本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Phil Calçado：现在AI中有很多空洞的炒作，很难挑出一个罪魁祸首。但大多数团队都低估了的这一点是：AI编码工具不是一个单程的魔法盒子。你不能只是向它们抛出一个提示，就期望得到一致、正确的结果。&amp;nbsp;这是任何真正构建AI产品的人都知道的痛苦教训。无论你的提示工程有多聪明，有效使用LLMs来自于结合工作流程并确保正确的上下文在正确的时间可用。否则，你只是在掷骰子。&amp;nbsp;我在以前为一个流行的代码审查工具构建AI管道时亲眼目睹了这一点。模型可能已经记住了所有写过的Python书籍，但如果你问10个开发人员“正确的方法”去做某事，你会得到11个答案。如果没有你的代码库、组织标准和实际目标的上下文，LLM就不知道哪个适用。这就是为什么你会得到完全不同的解决方案，甚至是对立的——这取决于当你提问时，概率之神想要倾向于哪一边。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Andreas Kollegger：许多软件开发团队低估了AI编码工具可以简化开发人员最不喜欢的任务，比如编写测试和文档。虽然AI编码演示承诺低代码和无代码，通常看起来微不足道或不可靠，但它们展示了AI如何将自然语言和代码之间进行转换，这对于自动化繁琐的任务和重复的设置是理想的。类似地，有一种专门用于项目初始化和代码生成的编码工具。&amp;nbsp;有一种工作流程被夸大了，但却没有得到充分利用，那就是让编码智能体通宵运行，并在早上检查他们的工作。我不建议在无人监督的情况下重构新产品功能或大量代码，但编码智能体非常适合一个定义良好的GitHub问题，它有良好的讨论，一个孤立且可重现的例子，以及一个可测试的修复。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;May Walter：大多数团队低估的是，模型已经足够好（并且正在变得更好）——缺失的成分是组织上下文。等待“更好的模型”是一种分心。真正的挑战是设计提供生成生产级代码所需的上下文的系统：你的架构、编码标准、数据边界和业务优先事项。如果没有这些，即使是最好的模型（或工程师）也会表现不佳。&amp;nbsp;另一方面，今天被过度炒作的是原始代码生成和静态代码审查。这些工作流程在演示中看起来令人印象深刻，但它们并没有解决大型组织中软件工程最难的部分：调试和质量保证。智能体仍然缺乏运行时上下文，并且很少有工具来评估哪些更改在业务影响方面真正关键。&amp;nbsp;这个差距很重要，因为更快的代码生成意味着更多的更改流入生产环境——而且没有更强大的流程来决定要监控什么，团队冒着为了脆弱性而牺牲速度的风险。未充分利用的前沿不是更快地编写代码，而是构建验证循环和运行时感知工具，以在这些更改部署之前增加确定性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这次讨论中得出的第一个，也许是最重要的结论是，尽管在软件开发过程中采用AI工具无疑降低了贡献的门槛，但它仍然是一个乘数，而不是灵丹妙药。只有与强大的组织环境相结合，AI才能增强生产力。基于AI的工程有潜力成为软件开发的核心，就像CI/CD管道曾经一样。然而，架构、编码标准和实验脚手架是成功采用AI的支撑支柱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，随着AI工具的发展，组织中开发人员的角色也从代码作者转变为系统编排者。新采用的策划、验证和集成AI输出的过程并没有取代软件工程这门手艺；相反，它增强了它。批判性思维和架构意识比以往任何时候都更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，采用任何新技术都会带来陷阱，对于AI和基于AI的工具也是如此。降低贡献的进入门槛也意味着增加了浅薄理解和生产次品代码的风险，这可能对初级开发人员的职业发展和整个组织产生负面影响。指导和运行时反馈是重要的护栏，以及文化和伦理保障：AI输出必须被视为初稿，人类必须对其负责。当涉及到AI时，信任不是授予的：它是一个过程，通过测试、同行评审、运行时验证和透明度赢得。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成功的指标也必须重新思考，因为AI夸大了所有传统的生产力指标。有意义的信号来得更晚：稳定性、变动、事件，以及有多少时间可以释放给创造力和架构。将AI扩展视为一个协作过程，而不是个人生产力的提升，这需要协调的工作流程和对周围流程的更高成熟度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论好坏，很明显，AI带来的变化已经到来，正在重塑软件开发的工艺。仍然有未被充分利用的方面，但上下文设计和运行时感知工具已经是下一个架构前沿。从长远来看，AI竞赛的赢家将是那些将其整合到具有问责制、信任和能够以负责任的方式共同发展的团队级流程中的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&quot;&gt;https://www.infoq.com/articles/ai-developers-rewriting-software-process/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</link><guid isPermaLink="false">https://www.infoq.cn/article/btdQGESEtDXJQPTxVCYF</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>作者：Arthur Casals</author><category>AI&amp;大模型</category></item><item><title>半年处理 1 亿笔支付！x402 V2 升级，让支付更简单</title><description>&lt;p&gt;在经历了为期六个月的真实场景应用之后，&lt;a href=&quot;https://www.x402.org/writing/x402-v2-launch&quot;&gt;开放支付标准 x402&lt;/a&gt;&quot;&amp;nbsp;迎来了重要更新。本次升级显著拓展了协议能力，使其不再局限于“单次请求、固定金额”的支付模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;新版协议新增了多项关键能力，包括：基于钱包的身份识别、自动 API 发现机制、动态支付接收方，以及通过 CAIP 标准实现的多链与法币扩展支持。同时，x402 还推出了一个完全模块化的 SDK，用于支持自定义网络和支付方案。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 是一次重量级升级，使协议在通用性、灵活性和跨网络扩展能力上均有显著提升。新版规范更加简洁、模块化，并与 CAIP、IETF Header 等现代标准保持一致，从而实现了一个可同时覆盖链上与链下支付的统一接口。在具体能力上，x402 V2 提供了一个统一的支付接口，可在多条区块链上支持稳定币和代币支付，包括 Base、Solana 等网络；同时也保持了对传统支付体系的兼容性，如 ACH、SEPA 以及银行卡网络。此外，协议还引入了“按请求路由”的能力，支持将支付定向至特定地址、角色，或基于回调逻辑进行分发，从而实现更复杂的多步骤支付工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 V2 的另一项重要改进，是清晰地区分了三类角色：协议规范本身、SDK 实现，以及负责链上验证和结算的facilitators。这种分层设计显著提升了协议的可扩展性，并为插件化、模块化架构奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新标准还引入了基于钱包的访问机制、可复用会话（reusable sessions）以及模块化付费墙（modular paywalls）。钱包支持让客户端在支付流程上拥有更高灵活性，可减少已购项目的重复交互与延迟；而模块化付费墙则使开发者能够更容易地集成和扩展后端支付逻辑，推动整个生态向更开放的方向发展。&lt;/p&gt;&lt;p&gt;在开发者体验方面，x402 V2 也进行了系统性优化。通过模块化设计简化配置流程，新增了同时选择多个 facilitators 的能力，并大幅减少了“胶水代码”和样板代码的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;x402 是一套开放、原生于 Web 的支付标准与协议，其目标是让“支付”成为互联网的一等公民。它支持微支付、按使用付费（pay-per-use）以及机器对机器（machine-to-machine）支付，使 Web 应用、API 以及自治代理（如 AI 机器人）能够直接通过 HTTP 为服务付费，而无需传统账户体系、订阅模式或复杂的支付流程。在推出后的短短几个月内，x402 已在 API、Web 应用和自治代理等场景中处理了超过 1 亿次 支付流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该协议利用了一个长期被忽视的 HTTP 状态码 —— 402（Payment Required），用于在需要付费时返回支付指令。借助 x402，支付可以直接嵌入在 HTTP 请求—响应流程中，无需将用户跳转至外部支付页面，也不再依赖 API Key 或个人账户体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为 x402 Foundation 的最初合作伙伴之一，Cloudflare 与 Coinbase 一同推动了该协议的落地。Cloudflare 已将 x402 集成进其&lt;a href=&quot;https://developers.cloudflare.com/agents/x402/&quot;&gt;开发者工具和基础设施&lt;/a&gt;&quot;中，包括：Agents SDK：帮助开发者构建能够自动完成 x402 支付的智能代理；MCP servers：向外暴露支持 x402 的工具，使服务能够返回 402 Payment Required 响应，并接收来自客户端的 x402 支付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&quot;&gt;https://www.infoq.com/news/2026/01/x402-agentic-http-payments/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</link><guid isPermaLink="false">https://www.infoq.cn/article/zfep8dP77z3KOWM6bUBi</guid><pubDate>Thu, 29 Jan 2026 06:00:00 GMT</pubDate><author>Sergio De Simone</author><category>软件工程</category></item><item><title>刚完成“卖身”重组，TikTok就瘫了！Oracle 背锅：暴风雪吹坏了数据中心</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle数据中心断电，引发 TikTok 大面积瘫痪&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，短视频平台 TikTok 在美国出现了一次短暂的服务中断。值得玩味的是，这次中断的时间点，恰好卡在 TikTok 刚完成一项美国业务重组安排之后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据这份重组安排，由 Oracle 与一组美国本土投资者共同组建的新合资实体，将接管 TikTok 在美国的运营相关事务，并被称为 TikTok USDS。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok USDS 承诺将用户数据通过 Oracle 公司拥有的数据中心进行传输。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚重组完没几天，TikTok就出现了大面积瘫痪，许多美国用户反映，他们无法上传视频到TikTok，也无法观看大多数新视频，包括美国以外用户成功上传的新视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一些用户表示，他们的算法似乎“重置”了，但目前尚不清楚这是否也与停电有关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事情不断发酵，逼得 TikTok&amp;nbsp;USDS 不得不出面回应了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TikTok&amp;nbsp;USDS 发言人 Jamie Favazza在给The Verge的一封电子邮件中指出，该公司在其新创建的X账户上发布了一份声明，声明称，由于美国数据中心发生电力中断，影响了TikTok和我们运营的其他应用程序，公司一直在“努力恢复服务”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/20e4e23cbb346864a484a62806ea02f0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然问题出在了数据中心，数据中心当然也要出来回应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Oracle 回应：完全怪天气&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对不断升温的质疑，Oracle 公司于当地时间 1 月 27 日通过电子邮件向媒体作出正式回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle 发言人迈克尔·埃格伯特（Michael Egbert）表示，上周末美国遭遇的一场强烈冬季风暴，导致Oracle 一处数据中心发生了暂时性停电，从而影响了 TikTok 在美国的服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“上周末，Oracle 数据中心因天气原因发生暂时性停电，影响了 TikTok 的服务。” 埃格伯特在声明中写道。他进一步解释称，美国 TikTok 用户在停电后所遇到的问题，主要源于恢复过程中出现的技术故障，目前Oracle 正与 TikTok 合作，尽快修复相关问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一回应明确否认了服务异常与内容审查之间存在直接关联，并将原因归结为基础设施层面的突发事故。Oracle 方面的说法，也与当时美国多地遭遇极端冬季天气的事实相吻合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在随后的声明中，TikTok 指出，其工程团队正在持续推进恢复工作，并在 1 月 27 日表示，已在恢复美国系统方面取得“重大进展”，但仍提醒用户，某些技术问题可能在短期内持续存在，尤其是在发布新内容时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64663332726be7240cc1f4098dedf7cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为此次事件中的关键基础设施提供方，Oracle 的角色也受到资本市场关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Benzinga Pro 报道，Oracle 公司股票在事件曝光当日收于 174.90 美元，下跌 4.13%，但在盘后交易中回升 1.16% 至 176.93 美元。Benzinga 的 Edge 股票排名显示，Oracle 股票在动量和价值维度上的评分均处于较低水平，反映出其在短期至长期内的价格趋势承压。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/254b1fa735b06cff92bf69b81ae8c593.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 TikTok USDS 合资企业逐步接管美国业务，其基础设施稳定性、内容审核机制以及与地方和联邦监管机构的互动方式，仍将持续受到审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：不只可以怪天气，还可以怪AI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着最终达成的合资协议，被外界普遍视为 TikTok 在美国“生死攸关”的一次妥协安排。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，此次技术中断发生之际，正值 TikTok 更新其美国隐私政策之后。新政策与合资架构调整相配套，但其中关于可能收集的数据类型的表述，引发部分用户不安。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;市场情报公司 Sensor Tower 向 CNBC 提供的数据显示，在过去五天内，美国地区 TikTok 的每日应用删除量较此前三个月的平均水平增长了近 150%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Reddit 上，一条关于 Oracle 数据中心与 TikTok 服务中断的帖子吸引了大量关注，不少网友在评论区提出了各类猜测、调侃与个人经验分享，这些反馈在一定程度上折射出技术社区对事件的怀疑态度，以及对 TikTok 内容机制与 Oracle 云服务能力的长期刻板印象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位ID名为 transcriptoin_error 的用户提出了一种“看似合理的推测”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，如果平台在系统中新增了内容过滤机制，那么在将相关流量迁移到新系统的过程中，确实有可能引发故障。他指出，在大规模系统迁移或数据转移时，出现配置错误或小规模失效并不罕见，尤其是在新旧系统并行、过滤规则叠加的情况下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论获得了数十次点赞，被不少用户视为“至少在工程逻辑上说得通”的一种解释，但评论者本人也并未声称这是事实，而是明确将其界定为推测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4a/4a7c72d3511f47000b48430ba2da565c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在另一条高赞的长篇幅评论中，该用户进一步构建了一套完整但高度假设性的系统模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他设想，如果 TikTok 平台不愿直接改动现有代码，以避免引发更大规模的系统崩溃，那么新增的内容过滤功能很可能会被设计成一个独立服务，甚至可能基于人工智能模型运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种设想下，所有潜在“敏感内容”都会被发送至一个新的 AI 服务进行判断，只有在得到“允许发布”的反馈后，内容才会正常上线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9fa27b6e1350a4f8df70b50cb109e9c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该用户进一步推测，如果这一 AI 服务发生宕机，而系统默认策略又是“未通过即阻止”，那么大量内容就可能被一并拦截，从而在用户侧表现为算法行为的“剧烈变化”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条评论虽然点赞不高，但在讨论中被多次引用，成为部分网友解释“为什么技术故障会影响内容分发”的逻辑模板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有网友对上述推测持明显怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位在评论区拥有较高影响力的用户指出，至今仍然没有人能够清楚解释，为什么一次服务器层面的故障，会导致推荐算法或内容分发逻辑出现如此明显的变化。在他看来，如果问题仅限于数据中心断电或服务恢复过程中的技术瑕疵，那么算法层面的“性格突变”仍然缺乏合理解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了针对 TikTok 的讨论，Oracle 本身也成为 Reddit 用户情绪的集中投射对象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位用户直言，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“能力并非问题所在，科技圈里没人能忍受 Oracle。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/33/336724bda8cb17fd3aa87f2658e0017c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还引用了一句在技术圈流传已久的说法：“Oracle 没有客户，只有囚犯。”这类评论并未直接指向此次事件的具体责任，但反映出 Oracle 在开发者与工程师群体中的长期口碑问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&quot;&gt;https://economictimes.indiatimes.com/tech/technology/oracle-says-data-center-outage-causing-issues-faced-by-us-tiktok-users/articleshow/127667105.cms?from=mdr&amp;amp;utm_source=contentofinterest&amp;amp;utm_medium=text&amp;amp;utm_campaign=cppst&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&quot;&gt;https://slate.com/technology/2026/01/tiktok-outage-oracle-ice-shooting.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&quot;&gt;https://www.reddit.com/r/news/comments/1qpbtv5/oracle_says_data_center_outage_causing_issues/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</link><guid isPermaLink="false">https://www.infoq.cn/article/Z0YrtK1zR7C4OcP5CV17</guid><pubDate>Thu, 29 Jan 2026 05:21:46 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Rust贡献者推出新语言Rue，探索AI辅助编译器开发</title><description>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/steve-klabnik/&quot;&gt;Steve Klabnik&lt;/a&gt;&quot;是《&lt;a href=&quot;https://doc.rust-lang.org/stable/book/&quot;&gt;Rust编程语言&lt;/a&gt;&quot;》的作者，并且在过去的13年里对Rust项目做出了贡献，他宣布了Rue，这是一种系统编程语言，它在没有垃圾回收的情况下探索内存安全性，同时优先考虑开发人员的人机工程学，而不是Rust的复杂性。该项目是在Anthropic的&lt;a href=&quot;https://claude.com/product/overview&quot;&gt;Claude AI&lt;/a&gt;&quot;的大力帮助下开发的，目标是填补高性能系统语言和垃圾回收替代品之间的未充分服务的设计空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在使用Rust 13周年之际，Klabnik在一篇博客文章中解释了他的动机：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我一直在想我是否应该尝试创造自己的语言。我真的很喜欢它们！这就是为什么我最初参与Ruby，然后是Rust的部分原因！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;语言名称遵循他的“Ru”前缀模式（Ruby、Rust、Rue），同时保持双重解释——既是&lt;a href=&quot;https://en.wikipedia.org/wiki/Ruta_graveolens&quot;&gt;花&lt;/a&gt;&quot;又是&lt;a href=&quot;https://redkiwiapp.com/en/english-guide/synonyms/rue-regret&quot;&gt;遗憾的表达&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik的核心设计问题是：“如果Rust不试图与C和C++竞争最高性能会怎么样？”如果我们愿意为了易用性而使性能稍微降低，但不要太低，会怎样？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术方法的核心是消除Rust的标志性——&lt;a href=&quot;https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html&quot;&gt;借用检查器&lt;/a&gt;&quot;。考虑一个典型的Rust代码，其中你试图在持有对其中一个元素的引用的同时修改一个向量。编译器拒绝此操作，因为引用可能会无效。Rue通过使用“inout”参数来暂时转移所有权，从而避免了整个问题，类似于Swift。在Rust中，试图在迭代当量时修改它会在编译时失败。Rue的inout参数允许你临时传递可变引用，但防止将它们存储在数据结构中；在保持内存安全的同时，通过更简单的限制消除了对生命周期跟踪的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;函数可以就地修改值，但这些值不能作为引用存储在堆分配的结构中。不需要生命周期注释。权衡是什么？某些模式变得无法表达。正如设计文档所承认的，Rue无法支持从其容器借用的迭代器；它们必须消耗它们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News社区的反应既有兴趣也有怀疑。一位评论者&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;捕捉&lt;/a&gt;&quot;到了这个挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Rust之所以成功地制造出没有垃圾回收的内存安全语言，是因为它引入了显著的复杂性（这是一种权衡）。没有人真正知道除此之外的合理方法，除非你还想放弃通用系统编程语言的要求。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据GitHub仓库中的设计提案，Rue实现了四种不同的所有权模式：值类型、仿射类型、线性类型和引用计数类型。Klabnik在回应中承认，“这必然会导致一些表现力的丧失。没有万能的解决方案。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发方法代表了一个实验，解决了Klabnik多年来一直在思考的&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;问题&lt;/a&gt;&quot;：“没有资金或团队，一个人还能构建一门编程语言吗？”这种方法标志着Klabnik的转变，他形容自己直到2025年都是AI怀疑论者。他第一次尝试在没有有效利用AI的情况下构建Rue，经过几个月的工作后不得不放弃。这一迭代，更有效地使用Anthropic的Claude AI，仅用两周时间就产生了大约70,000行Rust编译器代码，远远超过了他之前几个月的尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种协作超越了典型的编码协助。在Klabnik和Claude共&lt;a href=&quot;https://medium.com/codeelevation/do-you-still-need-a-team-to-build-a-programming-language-a-rust-core-contributor-tried-with-claude-da13aab912b4&quot;&gt;同署名&lt;/a&gt;&quot;的博客文章中，AI描述了编写大部分实现代码，而Klabnik指导架构并做出设计决策。Klabnik强调，有效使用AI工具需要大量的技能：“仅仅知道如何编写代码实际上不足以真正使用大模型。它们是它们自己的新类别的工具。”他的方法涉及迭代实验，编写简短的代码片段，开始对话，并测试不同的提示策略。这种模式是否能消除历史上资助语言项目的大量投资，还有待观察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue仍处于早期开发阶段，具有基本的控制流、函数和非泛型枚举。它通过自定义后端而不是LLVM编译为本地可执行文件，通过简化的语义实现快速编译时间。堆分配正在进行中，而语言服务器协议支持、包管理和并发模型尚未实现。该项目使用Buck2而不是Cargo进行未来的编译器引导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Klabnik保持着适度的期望：“我不指望它能发展成我的业余项目。”尽管如此，他指出，PHP和Rust的创造者Rasmus Lerdorf和Graydon Hoare也是从个人实验开始的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI辅助开发工具重塑软件工程，这项实验正在进行。虽然GitHub Copilot和类似的工具协助增量编码，Klabnik使用AI进行编译器的架构级工作的方法代表了不同级别的合作。如果成功，它可能表明，传统上需要大型团队的复杂基础设施项目，在AI的帮助下，对于熟练的个人来说可能是&lt;a href=&quot;https://news.ycombinator.com/item?id=46348262&quot;&gt;可行的&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真正的考验将是那些对Rust的学习曲线感到沮丧但又不愿意采用垃圾回收机制的开发人员是否能接受Rue的权衡。正如一位Hacker News评论者所说：&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果他们能在设计空间中找到一个全新的未被探索的点，我会非常感兴趣，但目前，我仍然持怀疑态度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rue语言的文档可在&lt;a href=&quot;https://rue-lang.dev/&quot;&gt;rue-lang.dev&lt;/a&gt;&quot;上找到，源代码在&lt;a href=&quot;https://github.com/rue-language/rue&quot;&gt;GitHub&lt;/a&gt;&quot;上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&quot;&gt;https://www.infoq.com/news/2026/01/steve-klabnik-rue-language-ai/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</link><guid isPermaLink="false">https://www.infoq.cn/article/amMNKG5vmN7LkiIA4ObY</guid><pubDate>Thu, 29 Jan 2026 03:05:14 GMT</pubDate><author>Tim Anderson</author><category>编程语言</category></item><item><title>Spring近期资讯：Boot、Security、Integration、Modulith和AMQP首个里程碑版本发布</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Boot&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot; 4.1.0的首个里程碑版本提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 @AutoConfigureWebServer 注解用于在支持 @SpringBootTest 注解的特定类和随机端口下启动Web服务器；以及通过Spring AMQP和Spring Kafka中定义的配置bean的自动配置，改进了可观测性和指标支持。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了缺陷修复、文档改进和依赖升级，Spring Boot 4.0.2，&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-0-2-available-now&quot;&gt;即第二个维护版本&lt;/a&gt;&quot;，还提供了一个值得注意的更改，即从 spring-boot-jetty 模块中移除了对 org.eclipse.jetty.ee11:jetty-ee11-servlets 模块的依赖，因为它未被使用并被确定为不必要。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Security&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：在 PasswordEncoder 接口中定义的 encode() 方法添加了空值契约；以及使用Spring Framework DefaultParameterNameDiscoverer 类中定义的 getSharedInstance() 方法，而不是创建该类的单独自定义实例。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Integration&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot; 7.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和新功能，例如：新的 spring-integration-cloudevents 和 spring-integration-grpc 模块分别支持&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/cloudevents.html&quot;&gt;CloudEvents&lt;/a&gt;&quot;转换和&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1-SNAPSHOT/grpc.html&quot;&gt;gRPC&lt;/a&gt;&quot;协议；以及新的 GrpcInboundGateway 和 GrpcOutboundGateway 类，作为gRPC客户端调用的入站和出站网关。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-integration/reference/7.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Modulith&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot; 2.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和改进，例如：在集成测试运行后重置 TimeMachine 类实例中的位移的能力；以及 spring.modulith.test.on-no-changes 属性的两个新属性值 execute-all 和 execute-none ，提供了在未检测到更改时跳过所有测试的能力。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AI&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot; 2.0.0的第二个&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、文档改进、依赖升级和许多新功能，例如：在 McpServerAutoConfiguration 类中添加了新的接口 McpSyncServerCustomizer 和 McpAsyncServerCustomizer ，解决了非web应用程序环境中MCP自动配置的问题；以及添加了来自&lt;a href=&quot;https://aws.amazon.com/s3/features/vectors/&quot;&gt;Amazon S3&lt;/a&gt;&quot;、&lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock Knowledge Base&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.quarkiverse.io/quarkus-langchain4j/dev/rag-infinispan-store.html&quot;&gt;Infinispan&lt;/a&gt;&quot;的向量存储后端。关于该版本的更多细节，包括重大变更，可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring Batch&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-batch&quot;&gt;Spring Batch&lt;/a&gt;&quot; 6.0.2，即&lt;a href=&quot;https://spring.io/blog/2026/01/21/spring-batch-6-0-2-available-now&quot;&gt;第二个维护版本&lt;/a&gt;&quot;，提供了缺陷修复、文档改进、依赖升级和一个新功能，引入了两个新类 ZonedDateTimeToStringConverter 和 OffsetDateTimeToStringConverter ，以支持 JobParameters 类的类型。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring AMQP&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot; 4.1.0的&lt;a href=&quot;https://spring.io/blog/2026/01/22/spring-boot-4-1-0-M1-available-now&quot;&gt;首个里程碑版本&lt;/a&gt;&quot;提供了缺陷修复、依赖升级和新功能，例如：新的 AmqpMessageListenerContainer 类实现了一个类似于 RabbitAmqpListenerContainer 类的容器；以及新的 @EnableAmqp 注解用于导入 AmqpDefaultConfiguration 类的实例，带有方便的基础设施bean。关于该版本的更多细节可在&lt;a href=&quot;https://github.com/spring-projects/spring-boot/releases/tag/v4.1.0-M1&quot;&gt;发布说明&lt;/a&gt;&quot;和这个&lt;a href=&quot;https://docs.spring.io/spring-amqp/reference/4.1/whats-new.html&quot;&gt;新功能页面&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</link><guid isPermaLink="false">https://www.infoq.cn/article/O8POeIhNkAxY3Vm7RoV2</guid><pubDate>Thu, 29 Jan 2026 03:01:25 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>TTC 完成千万美元级新一轮融资，厚雪资本领投、百度战略投资</title><description>&lt;p&gt;近日，北京才多对信息技术有限公司（ True Talents Connect ，以下简称“ TTC ”）宣布完成 A 轮千万美元级融资。本轮融资由厚雪资本领投，百度战略投资。此前，TTC 自研的 AI Agent 产品“小麦招聘”获第三届百度“文心杯”创业大赛一等奖，此次融资标志着其正式融入百度生态的新阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本轮融资将主要用于强化 AI 大模型与 Agent 技术研发，持续升级“小麦招聘”产品体验，并深化在 AI 及硬科技赛道的人才服务专业团队建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自创立以来，TTC 聚焦 AI 及前沿科技赛道，围绕“ AI +猎头”模式，为企业提供关键人才解决方案，致力于用科技（ AI ）高效连接人才与机会。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心团队兼具猎头基因与 AI 技术视野：TTC 创始人兼 CEO 肖玛峰 Max 拥有 17 年高端猎头行业经验，曾参与打造国内领先的中高端猎头品牌；联合创始人兼 CTO 宁辽原具备微软（美国）及字节跳动技术背景，长期深耕 AI 架构与模型落地，为公司 AI 能力与产品化提供持续支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年被视为 TTC 的 AI 元年。其推出核心产品「小麦招聘」，将顶尖猎头的判断逻辑与行业知识解码为结构化模型，赋能 AI Agent ，实现求职及招聘全链路的智能化升级，推动“人人都有一个专业 AI 猎头顾问”成为可能。产品上线仅 4 个月，已获得百度「文心杯」、量子位「 AI 100 创新产品」、创业北京「创业创新大赛」等多个行业奖项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随 AI 能力在业务中的全面落地，TTC 2025 年累计服务超千家 AI 领域科技企业及头部大厂，推荐岗位达数万次，年营收同比增长超 50 %，业务增长势头强劲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;厚雪资本创始合伙人侯昊翔 Roger ：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“投资 TTC ，源于我们一个核心判断：在 AI 时代，人才是终极的基础设施。TTC 所做的，正是帮助企业科学地组合与迁移关键人才，通过精准配置与协同设计，让不同能力在合适的时点形成共振，产生‘ 1+1&amp;gt;11 ’的创造力。更打动我们的是团队对‘激发人’的深刻理解——他们清楚地认识到，顶尖的人才服务是专业能力与判断艺术的结合，是成为点燃人才潜力的‘催化剂’。与此同时，Max 和团队拥有顶尖的行业经验，却始终保持着创业者的谦逊与初心，这让我们相信他们能走得更远。对厚雪而言，这同样是一次重要的战略协同。作为一家同样具有创业心态的机构，我们看好 TTC‘ AI 赋能顾问’与‘平台直接连接’的双轨模式。这不仅是对传统招聘痛点的革新，也为我们共同深入 AI 产业核心人群、持续积累认知与资源提供了新的入口。我们期待与这样的长期主义者同行，共同见证 AI 人才服务从‘高端配置’到‘ AI 时代企业基石’的变革。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百度：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们始终认为，只有当 AI 被内化为一种原生的能力，才能真正在各行各业引爆生产力革命。TTC 正是这一理念在人力资源行业的出色实践者。作为第三届百度‘文心杯’创业大赛一等奖项目，「小麦招聘」没有简单地将 AI 作为工具，而是将顶尖猎头的专业服务能力重构为原生 AI Agent ，这让高效、精准的人才匹配从‘高端服务’变成了可广泛触达的智能基础。百度此次战略投资并开放生态资源，正是希望与 TTC 携手，加速 AI 原生能力在人力资源领域的深度渗透，共同将‘智能红利’转化为支撑万千企业发展的‘人才红利’，进而转化为‘社会红利’。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，TTC 将继续秉持“用科技（ AI ）高效连接人才与机会”的使命，深化 AI 技术与人才服务的融合，让每一次人才连接，都更智能、更精准、更值得信赖。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink&quot;&gt;https://jxog8b3tny.feishu.cn/file/DxjWbNenpoe17NxaZ9zcPE1Jnlh?from=from_copylink &lt;/a&gt;&quot;&amp;nbsp;（点击链接下载）&lt;/p&gt;</description><link>https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</link><guid isPermaLink="false">https://www.infoq.cn/article/eHJs3ook7AQ5Doge7CEh</guid><pubDate>Thu, 29 Jan 2026 03:00:00 GMT</pubDate><author>小麦招聘</author><category>百度</category><category>AI 工程化</category></item><item><title>Java近期资讯：Oracle关键补丁更新、Grizzly 5、Payara Platform、GraalVM、Liberica JDK</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenJDK&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JEP 527，&lt;a href=&quot;https://openjdk.org/jeps/527&quot;&gt;TLS 1.3的后量子混合密钥交换&lt;/a&gt;&quot;，在JDK 27中已从Proposed提升Targeted状态。这个JEP提议使用正在由互联网工程任务组（IETF）起草的TLS 1.3规范中的混合密钥交换，增强RFC 8446，&lt;a href=&quot;https://datatracker.ietf.org/doc/rfc8446/&quot;&gt;传输层安全（TLS）协议版本1.3的实现&lt;/a&gt;&quot;，与JEP 496，&lt;a href=&quot;https://openjdk.org/jeps/496&quot;&gt;量子抵抗模块格的密钥封装机制&lt;/a&gt;&quot;，其在JDK 24中交付。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Oracle发布了JDK的25.0.2、21.0.10、17.0.18、11.0.30和8u481版本，作为2026年1月季度&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;关键补丁更新咨询&lt;/a&gt;&quot;的一部分。关于该版本的更多细节可以在&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/25-0-2-relnotes.html&quot;&gt;25.0.2&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/21-0-10-relnotes.html&quot;&gt;21.0.10&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/17-0-18-relnotes.html&quot;&gt;17.0.18&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/11-0-30-relnotes.html&quot;&gt;11.0.30&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.oracle.com/java/technologies/javase/8u481-relnotes.html&quot;&gt;8u481&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 26&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B32&quot;&gt;Build 32&lt;/a&gt;&quot;在上周发布，包括从Build 31的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JDK 27&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本Build 6也在上周发布，包括从Build 5的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GlassFish Grizzly&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/blob/main/README.md&quot;&gt;GlassFish Grizzly&lt;/a&gt;&quot;5.0.0 &lt;a href=&quot;https://x.com/OmniFishEE/status/2013611675639841050&quot;&gt;GA&lt;/a&gt;&quot;版本发布，这是一个旨在扩展&lt;a href=&quot;https://docs.oracle.com/en/java/javase/25/docs/api/java.base/java/nio/package-summary.html&quot;&gt;Java NIO API&lt;/a&gt;&quot;能力的框架，带来了显著的变化，如：JDK 21基线；在Grizzly线程池中使用新的 virtualthreadexexecutorservice 类来支持虚拟线程；并支持&lt;a href=&quot;https://jakarta.ee/specifications/servlet/6.1/&quot;&gt;Jakarta Servlet 6.1&lt;/a&gt;&quot;规范。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/eclipse-ee4j/glassfish-grizzly/releases/tag/5.0.0-RELEASE&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Jakarta EE&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在每周的&lt;a href=&quot;https://www.agilejava.eu/&quot;&gt;Hashtag Jakarta EE&lt;/a&gt;&quot;博客中，Eclipse基金会的Jakarta EE开发者倡导者&lt;a href=&quot;https://se.linkedin.com/in/ivargrimstad&quot;&gt;Ivar Grimstad&lt;/a&gt;&quot;提供了Jakarta EE 12的&lt;a href=&quot;https://www.agilejava.eu/2026/01/25/hashtag-jakarta-ee-317/&quot;&gt;更新&lt;/a&gt;&quot;，他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Jakarta EE的每个主要版本都被赋予了一个主题，或者是一个特征性的口号。对于Jakarta EE 9，关键词是“低准入门槛-创新平台-轻松迁移”，对于Jakarta EE 10，它是“现代化-简化-轻量级”。Jakarta EE 11的口号是“开发人员的生产力和性能”。&amp;nbsp;当我们讨论即将发布的Jakarta EE 12版本该使用什么口号时，选择落在了“健壮和灵活”上。无论我们谈论的是哪个版本，这都非常适合Jakarta EE，但更适合Jakarta EE 12，因为它现在比以往任何时候都更加健壮，这是其转移到Eclipse基金会以来的第四个主要版本。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通往Jakarta EE 12的道路包括四个里程碑版本，其中第一个版本已于2025年12月交付，计划于2026年7月发布GA版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;BellSoft&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与Oracle的2026年1月的关键补丁更新（&lt;a href=&quot;https://www.oracle.com/security-alerts/cpujan2026.html&quot;&gt;Critical Patch Update&lt;/a&gt;&quot;，CPU）同时，BellSoft为&lt;a href=&quot;https://bell-sw.com/pages/libericajdk/&quot;&gt;Liberica JDK&lt;/a&gt;&quot;的25.0.1.0.1、21.0.9.0.1、17.0.17.0.1、11.0.29.0.1、8u481 7u491和6u491版本发布了CPU补丁，以解决这个&lt;a href=&quot;https://docs.bell-sw.com/security/search/&quot;&gt;CVEs&lt;/a&gt;&quot;列表。此外，包含CPU和非关键修复的补丁集更新（PSU）版本25.0.2、21.0.10、17.0.18、11.0.30和8u481也已发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BellSoft表示，他们总共有1217个修复和回溯，参与消除了所有版本中的21个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;GraalVM&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同样，GraalVM 25.0.2，第二个维护版本也与Oracle的2026年1月CPU同时发布，解决了一些显著的问题，如：JDK Flight Recorder中的Translation-Lookaside Buffer（TLB）事件的内存泄漏；以及循环向量化的误编译，导致结果不正确。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队还放弃了对macOS x64的支持。这个新版本只支持macOS AArch64。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关于该版本的更多细节可以在&lt;a href=&quot;https://www.graalvm.org/release-notes/JDK_25/&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Spring框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于Spring来说，这是忙碌的一周，因为各个团队都发布了第一个里程碑式的版本：Spring Boot；&lt;a href=&quot;https://spring.io/projects/spring-boot&quot;&gt;Spring Boot&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-security&quot;&gt;Spring Security&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-integration&quot;&gt;Spring Integration&lt;/a&gt;&quot;、&lt;a href=&quot;https://spring.io/projects/spring-modulith&quot;&gt;Spring Modulith&lt;/a&gt;&quot;和&lt;a href=&quot;https://spring.io/projects/spring-amqp&quot;&gt;Spring AMQP&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://spring.io/projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt;&quot;的第二个里程碑版本。更多细节可以在InfoQ的&lt;a href=&quot;https://www.infoq.com/news/2026/01/spring-news-roundup-jan19-2026/&quot;&gt;新闻报道&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Payara&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Payara发布了其2026年1月版的&lt;a href=&quot;https://www.payara.fish/&quot;&gt;Payara Platform&lt;/a&gt;&quot;，其中包括社区版7.2026.1，企业版6.34.0和企业版5.83.0。除了缺陷修复和组件升级，这三个版本都专注于两个CVE的解决方案，即：&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2020-5258&quot;&gt;CVE-2020-5258&lt;/a&gt;&quot;，这是&lt;a href=&quot;https://dojo.io/&quot;&gt;Dojo&lt;/a&gt;&quot;中的一个漏洞，允许攻击者将属性注入到JavaScript中现有的语言构造原型中，并通过注入其他值来操纵这些属性以覆盖或&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Security/Attacks/Prototype_pollution&quot;&gt;污染&lt;/a&gt;&quot;JavaScript应用程序对象原型；以及允许攻击者通过恶意URL有效载荷接管Payara管理帐户的漏洞。关于这些版本的更多细节可以在社区版&lt;a href=&quot;https://docs.payara.fish/community/docs/Release%20Notes/Release%20Notes%207.2026.1.html&quot;&gt;7.2026.1&lt;/a&gt;&quot;、企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/Release%20Notes/Release%20Notes%206.34.0.html&quot;&gt;6.34.0&lt;/a&gt;&quot;和企业版&lt;a href=&quot;https://docs.payara.fish/enterprise/docs/5.83.0/Release%20Notes/Release%20Notes%205.83.0.html&quot;&gt;5.83.0&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenXava&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openxava.org/&quot;&gt;OpenXava&lt;/a&gt;&quot; 7.6.4版本的发布包含了缺陷修复、文档改进、依赖升级和新功能，例如：改进了嵌入式Apache Tomcat的启动时间；以及在 Strings 类中定义了一个新的 toString(Locale, Object) 方法，该方法与其他重载的 toString() 方法一起，用于转换具有本地化意识的字符串。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/openxava/openxava/releases/tag/7.6.4&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JetBrains Ktor&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JetBrains &lt;a href=&quot;https://ktor.io/&quot;&gt;Ktor&lt;/a&gt;&quot; 3.4.0版本的发布提供了缺陷修复和新特性，例如：一个新的API， describe ，它与一个新的编译器插件一起动态生成并记录OpenAPI端点；以及一个新的 ktor-server-compression-zstd 模块，支持&lt;a href=&quot;https://man.archlinux.org/man/zstd.1.en&quot;&gt;Zstd&lt;/a&gt;&quot;压缩算法。关于该版本的更多细节可以在&lt;a href=&quot;https://github.com/ktorio/ktor/releases/tag/3.4.0&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&quot;&gt;https://www.infoq.com/news/2026/01/java-news-roundup-jan19-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</link><guid isPermaLink="false">https://www.infoq.cn/article/cDEARSKm18EAokOm6icy</guid><pubDate>Thu, 29 Jan 2026 02:57:21 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>多次全球性中断后，Cloudflare推出了“Code Orange: Fail Small”韧性计划</title><description>&lt;p&gt;Cloudflare最近发布了一项名为“&lt;a href=&quot;https://blog.cloudflare.com/fail-small-resilience-plan/&quot;&gt;Code Orange: Fail Small&lt;/a&gt;&quot;”的详细韧性计划，以防止过去六周内连续发生的两次重大网络中断导致的大规模服务中断再次发生。该计划优先考虑受控发布、改进故障模式处理以及简化应急流程，以使其全球网络更加稳健，并减少因配置错误而造成的脆弱性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的网络在2025年&lt;a href=&quot;https://www.infoq.com/news/2025/11/cloudflare-global-outage-cause/&quot;&gt;11月18日&lt;/a&gt;&quot;和&lt;a href=&quot;https://blog.cloudflare.com/5-december-2025-outage/&quot;&gt;12月5日&lt;/a&gt;&quot;遭受了两次严重的中断。第一次事件导致流量交付中断了约2小时10分钟，而第二次事件则影响了其网络背后约28%的应用程序，持续了约25分钟。这些事件发生在即时的全球配置更改之后，尽管这些更改旨在提高安全性或机器人检测能力，但它们在数百个数据中心迅速传播了错误的设置，从而引发了广泛的服务故障。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Code Orange: Fail Small”计划规定，配置更改必须以受控的、分阶段的方式进行，类似于Cloudflare现有的软件发布流程&lt;a href=&quot;https://blog.cloudflare.com/safe-change-at-any-scale/&quot;&gt;Health Mediated Deployment(HMD)&lt;/a&gt;&quot;，其中包括分阶段验证和自动回滚机制。历史上，配置更新（如DNS记录或安全规则）会通过内部的&lt;a href=&quot;https://blog.cloudflare.com/quicksilver-v2-evolution-of-a-globally-distributed-key-value-store-part-1/&quot;&gt;Quicksilver系统&lt;/a&gt;&quot;在几秒钟内向全球范围传播，当错误的更改传播过快时，这就成为了一个隐患。在新策略下，配置更新需要通过监控门禁并采用渐进式部署，以便在问题影响到大范围基础设施之前尽早发现它们并降低影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare还计划审查和改进网络流量处理系统中的所有故障模式，旨在确保每个组件在错误条件下都能做出可预测的响应，并且不会将故障级联到不相关的服务。这包括验证关键产品之间的接口契约，并建立合理的默认值，以便即使依赖的子系统发生故障，流量也能继续流动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，该公司正在彻底改革紧急访问程序和内部工具的访问权限，以减少在过去的中断事件中拖慢事件响应速度的循环依赖。增强的培训和简化的应急访问协议旨在帮助工程师更快地应对关键故障，同时不损害安全防护措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的计划正在逐步推进，通过单独的更新以改善整体的性韧性，而不是一次性地进行大规模更新。该公司预计到2026年第一季度末，所有生产系统都将使用增强后的HMD配置流程，故障模式将得到更好的定义和测试，应急响应访问也将得到改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些努力是在日益严格的审查背景下进行的。Cloudflare的中断事件引起了&lt;a href=&quot;https://www.theguardian.com/technology/2025/dec/05/another-cloudflare-outage-takes-down-websites-linkedin-zoom&quot;&gt;广泛的关注&lt;/a&gt;&quot;，事件影响了LinkedIn、Zoom和Shopify等主要网站，并引发了关于集中式互联网基础设施风险的讨论。尽管社区的一些&lt;a href=&quot;https://www.reddit.com/r/CloudFlare/comments/1pr1twp/code_orange_fail_small_our_resilience_plan&quot;&gt;反应&lt;/a&gt;&quot;表达了不满，但许多讨论平台上的用户也对Cloudflare坦诚承认问题及其结构性改进的承诺表示了欢迎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare正在努力重建信心，“Code Orange: Fail Small”计划凸显了该公司向更谨慎的部署实践的转变，并对故障的出现做出更强的预期，以便在问题升级为扰乱互联网生态系统大范围的全球中断之前将其控制住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-resilience-plan/&quot;&gt;Cloudflare Launches ‘Code Orange: Fail Small’ Resilience Plan After Multiple Global Outages&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</link><guid isPermaLink="false">https://www.infoq.cn/article/lKcqZxO13IGomsA22W0N</guid><pubDate>Thu, 29 Jan 2026 02:53:04 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category></item><item><title>Vercel 开源 Bash 工具：基于本地文件系统的上下文检索</title><description>&lt;p&gt;Vercel &lt;a href=&quot;https://vercel.com/changelog/introducing-bash-tool-for-filesystem-based-context-retrieval&quot;&gt;开源了 bash-tool&lt;/a&gt;&quot;，这个工具为 AI 智能体提供了一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bash_(Unix_shell)&quot;&gt;Bash&lt;/a&gt;&quot; 执行引擎，让它们可以直接运行&lt;a href=&quot;https://cycle.io/learn/linux-filesystem-commands&quot;&gt;基于文件系统&lt;/a&gt;&quot;的命令，帮模型获取上下文信息。这个工具的设计初衷，就是让 AI 智能体在处理大量本地上下文时，不用把整个文件塞进模型提示词里，可以直接通过像 find、grep、jq 这样的 &lt;a href=&quot;https://google.github.io/styleguide/shellguide.html&quot;&gt;shell 命令&lt;/a&gt;&quot;，直接在文件夹里操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 给智能体提供了三大核心操作：bash（解释并执行 Bash 脚本）、readFile（从预加载的文件系统读取文件）、writeFile（更新文件）。这个引擎基于 just-bash（一个用 TypeScript 写的解释器），不会新开 shell 进程，也不会随便执行二进制文件。它既能用内存文件系统，也能跑在隔离的虚拟机里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际用起来时，开发者可以在创建工具时先把一批文件加载进去，智能体就能随时对这些文件运行命令。比如，你可以把一个 JavaScript 源码文件交给 bash-tool，智能体就能查找或操作文件系统，而不用把整个文件内容塞进提示词里。如果需要真正的 shell 和文件系统，也可以在 &lt;a href=&quot;https://vercel.com/docs/vercel-sandbox&quot;&gt;Vercel 的沙盒&lt;/a&gt;&quot;环境下用这个工具，支持完整的虚拟机隔离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个工具的诞生，是因为大家都想让大模型的上下文窗口别太臃肿，同时又希望智能体能精准获取文件里的关键信息。只拿 shell 命令的结果，不嵌入整个文件，智能体就能省下不少 token，把注意力集中在真正有用的小片段上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者可以把 bash-tool 和 &lt;a href=&quot;https://ai-sdk.dev/&quot;&gt;Vercel 的 AI SDK&lt;/a&gt;&quot; 一起安装，就能开始开发用文件系统操作做检索的智能体了。它既能用内存文件系统，也能跑在沙盒环境里，部署起来很灵活，而且还不会暴露不安全的执行路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者们在&lt;a href=&quot;https://x.com/vercel/status/2009769470194266327&quot;&gt;早期讨论&lt;/a&gt;&quot;时就发现，用 Bash 风格的接口让智能体检索上下文，其实是很贴合大多工具和模型已经熟悉的 Unix 工作流。Vercel 让智能体能用 find、grep 这些经典命令，等于直接用上了 shell 的语义，让模型能高效地查找和提取结构化信息，而不是只靠向量检索或者把整个文件塞进提示词。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者 &lt;a href=&quot;https://x.com/asimgilani/status/2009787196736450887&quot;&gt;Asim Gilani&lt;/a&gt;&quot; 说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;能不用复杂的上下文管理，真的太爽了。让模型自己查文件，比每次都喂它一堆碎片强多了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/benjaminshafii/status/2009798694594588754&quot;&gt;Benjamin Shafii&lt;/a&gt;&quot; 也表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Unix 50 年前就把抽象做对了。只要你能把设备、进程、数据都当成文件看，你就只需要一种抽象和一个 API。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;bash-tool 的出现，可能会影响未来 AI 驱动的开发系统如何处理本地上下文，更加注重精准检索和与软件工程常见文件系统语义的深度结合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/01/vercel-bash-tool/&lt;/p&gt;</description><link>https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</link><guid isPermaLink="false">https://www.infoq.cn/article/UzyGmH8QWc2eHSvS6Yta</guid><pubDate>Thu, 29 Jan 2026 02:50:23 GMT</pubDate><author>作者：Daniel Dominguez</author><category>开源</category></item><item><title>又一款世界模型宣布开源！对标 Genie 3、10分钟长视频无损生成</title><description>&lt;p&gt;1 月 29 日，继连续发布空间感知与VLA基座模型后，蚂蚁灵波科技再次刷新行业预期，开源发布世界模型LingBot-World。该模型在视频质量、动态程度、长时一致性、交互能力等关键指标上均媲美Google Genie 3，旨在为具身智能、自动驾驶及游戏开发提供高保真、高动态、可实时操控的“数字演练场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/6630b4123c11260f615d99fa40469a9f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：LingBot-World在适用场景、生成时长、动态程度、分辨率等方面均处于业界顶尖水平）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开源地址：https://github.com/Robbyant/lingbot-world?tab=readme-ov-file&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对视频生成中最常见的“长时漂移”问题（生成时间一长就可能出现物体变形、细节塌陷、主体消失或场景结构崩坏等现象），LingBot-World&amp;nbsp;通过多阶段训练以及并行化加速，实现了近&amp;nbsp;10 分钟的连续稳定无损生成，为长序列、多步骤的复杂任务训练提供支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;交互性能上，LingBot-World可实现约16 FPS 的生成吞吐，并将端到端交互延迟控制在 1 秒以内。用户可通过键盘或鼠标实时控制角色与相机视角，画面随指令即时反馈。此外，用户可通过文本触发环境变化与世界事件，例如调整天气、改变画面风格或生成特定事件，并在保持场景几何关系相对一致的前提下完成变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/90/90fa3c806fd7811440c1a441d9a54231.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：一致性压力测试，镜头最长移开60秒后返回，目标物体仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a4/a492f32211511f5d7e12fdc7c2045f3e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：高动态环境下，镜头长时间移开后返回，车辆形态外观仍保持一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/80/80819d2c0847929314e4151a3a1be453.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：镜头长时间移开后返回，房屋仍存在且结构一致）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型具备Zero-shot 泛化能力，仅需输入一张真实照片（如城市街景）或游戏截图，即可生成可交互的视频流，无需针对单一场景进行额外训练或数据采集，从而降低在不同场景中的部署与使用成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为解决世界模型训练中高质量交互数据匮乏的问题，LingBot-World 采用了混合采集策略：一方面通过清洗大规模的网络视频以覆盖多样化的场景，另一方面结合游戏采集与虚幻引擎（UE）合成管线，从渲染层直接提取无UI 干扰的纯净画面，并同步记录操作指令与相机位姿，为模型学习“动作如何改变环境”提供精确对齐的训练信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;具身智能的规模化落地面临一个核心挑战——复杂长程任务的真机训练数据极度稀缺。LingBot-World&amp;nbsp;凭借长时序一致性（也即记忆能力）、实时交互响应，以及对&quot;动作-环境变化&quot;因果关系的理解，能够在数字世界中&quot;想象&quot;物理世界，为智能体的场景理解和长程任务执行提供了一个低成本、高保真的试错空间。同时，LingBot-World支持场景多样化生成（如光照、摆放位置变化等），也有助于提升具身智能算法在真实场景中的泛化能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着“灵波”系列连续发布三款具身领域大模型，蚂蚁的AGI战略实现了从数字世界到物理感知的关键延伸。这标志着其“基础模型-通用应用-实体交互”的全栈路径已然清晰。蚂蚁正通过InclusionAI&amp;nbsp;社区将模型全部开源，和行业共建，探索AGI的边界。一个旨在深度融合开源开放并服务于真实场景的AGI生态，正加速成型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，LingBot-World 模型权重及推理代码已面向社区开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</link><guid isPermaLink="false">https://www.infoq.cn/article/wPVrPmGCbw49Nxtct4RW</guid><pubDate>Thu, 29 Jan 2026 02:44:20 GMT</pubDate><author>蚂蚁集团</author><category>生成式 AI</category></item></channel></rss>