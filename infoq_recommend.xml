<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Wed, 11 Feb 2026 02:05:52 GMT</lastBuildDate><ttl>5</ttl><item><title>模力工场 032 周 AI 应用榜：桌面 Agent 强势来袭，阶跃登顶本周榜首</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;邀你用 AI 一键生成新年财运红包封面！2月5日至25日，设计松鼠 × 模力创意红包，即可赢金币参与多轮现金抽奖。扫码进群，马上开启你的开年好运！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/cafd85344e3eb0dfc0714172e79b1126.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;032 周上榜应用精选（附用户热评）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场 32 周 AI 应用周榜来啦～本周共有 25 款应用上架新榜，所有排名均来自用户真实使用、测评与社区讨论热度。本期用户讨论最高的是：桌面 Agent 形态的出现。AI 正在从“对话框里的助手”，走向“接管桌面的执行者”。AI 开始在真实桌面环境中，操作网页、处理本地文件、生成办公文档，甚至能把多个应用里的任务一口气跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们从中精选出十款最具声量的应用，聚焦五大垂直领域，为你更详细地解读榜单背后的 AI 行业风向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、桌面 Agent 类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：接管桌面｜跨应用操作｜真实执行&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/stepfun-desktop?utm_source=20260210infoQ&quot;&gt;阶跃AI桌面伙伴 📍上海&lt;/a&gt;&quot;：一个更懂中文办公的国产桌面 AI 伙伴，无需复杂设置，全平台支持，深度整合钉钉、飞书等本土工具，用截图提问、智能整理、定时任务等贴心功能，为你打造真正懂中文、懂场景、懂流程的下一代智能工作台。&lt;/p&gt;&lt;p&gt;    &lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3660388f8cfbad8960e33fd0215d44f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/workany?utm_source=20260210infoQ&quot;&gt;WorkAny 📍广州&lt;/a&gt;&quot;：艾逗比开发的开源跨平台桌面智能体，可以通过安全沙盒执行各类脚本，无缝处理文件整理、文档生成、网页制作等办公任务，更支持自定义模型与并行处理，用本地订阅打造你的专属 AI 生产力中心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72fd4ee7cf65a17a99ff9138019d141c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、学习 / 知识管理类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：结构化学习｜知识转写｜理解与记忆&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/chatglm?utm_source=20260210infoQ&quot;&gt;智谱清言 AI 学习搭子&lt;/a&gt;&quot;：植入在智谱清言生态中的学习辅助模块，擅长把教材、文档和概念转化为知识地图、卡片和讲解内容，并配套随堂测试，更偏“陪伴式学习”和知识消化。&lt;a href=&quot;https://agicamp.com/products/thetawaveai?utm_source=20260210infoQ&quot;&gt;Thetawave AI&lt;/a&gt;&quot;：偏重输入端的学习整理工具，支持录音、视频、文档、网页等多源内容转写，并生成结构化笔记、思维导图和测验，适合学生和知识工作者做系统性复盘。&lt;a href=&quot;https://agicamp.com/products/notebooklmgoogle?utm_source=20260210infoQ&quot;&gt;Notebook LM&lt;/a&gt;&quot;：Google 推出的研究型笔记工具，更偏“资料理解与问答”。围绕用户上传的 PDF、网页、视频等材料进行摘要、提问和交互式研究整理，适合研究与长期项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、内容与视频创作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：内容工业化｜全流程生成｜效率提升&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/daoying?utm_source=20260210infoQ&quot;&gt;道影 AI 📍杭州&lt;/a&gt;&quot;：AI 视频全链路生产平台，面向短剧、漫剧等专业内容创作者。从剧本到成片一体化设计，强调流程贯通与规模化生产，而非单点创意工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、开发 / 编程协作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：Vibe Coding｜一体化开发｜任务式编程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/zaizhi?utm_source=20260210infoQ&quot;&gt;OpenCode&lt;/a&gt;&quot;：为 Vibe Coding 场景设计的 AI 编程工具。把聊天、代码编辑、文件树和终端放在同一界面，支持 skill 封装与多模型切换，对编程新手非常友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/0398aaa68bffa221cef5510894d6bccb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;五、专业与底层能力类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：专业生成｜算力平台｜企业与垂直场景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/mureka?utm_source=20260210infoQ&quot;&gt;Mureka V8&lt;/a&gt;&quot;：昆仑万维推出的 AI 音乐生成平台，从自然语言或歌词直接生成结构完整、编曲成熟、人声自然的音乐作品，面向专业音乐创作场景。&lt;a href=&quot;https://agicamp.com/products/Prism?utm_source=20260210infoQ&quot;&gt;Prism&lt;/a&gt;&quot;：OpenAI 的 Prism 是一个不错的学术写作结构梳理与格式排版工具。它尤其适合在开题与文献综述阶段，帮你将思路系统化、可视化，并接手繁琐的 LaTeX 排版与参考文献管理。&lt;a href=&quot;https://agicamp.com/products/lanyun?utm_source=20260210infoQ&quot;&gt;蓝耘元生代 📍北京&lt;/a&gt;&quot;：以自研 MetaGen 智能算力操作系统为核心，面向企业提供集算力调度、模型服务与数据生成于一体的智算云平台，支撑 AI 应用落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;榜单之外但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【应用名称】：&lt;a href=&quot;https://agicamp.com/products/Flora?utm_source=20260210infoQ&quot;&gt;Flora&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：节点式创作｜无限画布｜创意工作流&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【模力小A推荐】：Flora 是一款节点式创意 AI 平台，通过“无限画布”把文本、图像和视频生成串成可复用的工作流，适合品牌视觉、广告概念等跨媒介创作场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从本期榜单可以清晰看到一个信号：AI 的主战场正在从“会不会回答问题”，转向“能不能把事做完”。桌面 Agent 的集中出现，是这一变化最直观的体现。相比以往停留在对话框里的助手，本周讨论热度最高的产品，已经开始直接接管桌面环境，真实操作网页、处理本地文件、生成办公文档，甚至跨多个应用连续执行任务。用户关注的核心不再是模型能力，而是执行稳定性、流程完整度和对真实工作场景的适配程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，学习、内容创作和编程类应用的演进路径也在发生变化：它们不再强调“单次生成”，而是围绕结构化理解、完整流程和长期使用进行设计。无论是学习工具对多源资料的系统整理，还是内容平台对从创意到成片的全链路打通，本质上都在向“可持续使用的生产力工具”靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，本期周榜反映出的并非某一个爆款应用，而是一种明确趋势：AI 正在从能力展示，进入到执行与交付阶段。谁能真正嵌入用户的工作流，承担连续、可验证的任务，谁才更有可能成为下一阶段被长期留下来的 AI 应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后再介绍一下模力工场的上榜机制和加入榜单的参与方式，欢迎大家继续积极参与提交 AI 应用～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场AI 应用榜并非依靠“点赞刷榜”，而是参考以下权重维度：&lt;/p&gt;&lt;p&gt;评论数（核心指标，代表社区真实反馈）&lt;/p&gt;&lt;p&gt;收藏与点赞（次级指标）&lt;/p&gt;&lt;p&gt;推荐人贡献（注册推荐人可直接为好应用打 Call）&lt;/p&gt;&lt;p&gt;加入榜单的参与方式：&lt;/p&gt;&lt;p&gt;如果你是开发者：上传你的 AI 应用，描述使用场景与核心亮点；&lt;/p&gt;&lt;p&gt;如果你是推荐人：发现好工具，发布推荐理由；&lt;/p&gt;&lt;p&gt;如果你是用户：关注榜单，评论互动，影响榜单权重，贡献真实声音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One More Thing，对于所有在模力工场上发布的 AI 应用，极客邦科技会借助旗下各品牌资源进行传播，短时间内触达千万级技术决策者与开发者、AI 用户：&lt;/p&gt;&lt;p&gt;InfoQ 全媒体矩阵&lt;/p&gt;&lt;p&gt;AI 前线全媒体矩阵&lt;/p&gt;&lt;p&gt;极客时间全媒体矩阵&lt;/p&gt;&lt;p&gt;TGO 鲲鹏会全媒体矩阵&lt;/p&gt;&lt;p&gt;霍太稳视频号&lt;/p&gt;</description><link>https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</link><guid isPermaLink="false">https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</guid><pubDate>Tue, 10 Feb 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>为 ChatGPT 和 Claude 提供“地基”的那家公司，在担心什么</title><description>&lt;p&gt;过去一年，关于 AI 的讨论出现了一种明显的反差：一边是模型能力不断刷新上限，另一边却是越来越多企业开始质疑——为什么真正落地依然这么难？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从概念验证到生产系统，从 60% 的“看起来可用”到 99.99% 的“不得不可靠”，企业级 AI 面对的从来不是算力或参数规模的问题，而是数据、决策责任、合规流程以及现实系统复杂性。而这些恰恰是大多数新闻叙事里最容易被忽略的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样的背景下，这期播客给出了一种罕见的“现实视角”。对话并没有继续渲染模型能力的指数级增长，而是把焦点放在一个更基础、也更棘手的问题上：AI 要真正进入企业和关键业务流程，还缺什么？答案指向了一个长期被低估的环节——数据，以及数据背后的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本期节目的对话者，正是站在这一环节核心位置的人。他所领导的公司，长期为几乎所有一线大模型实验室提供训练所需的基础数据；而在加入这家公司之前，他曾把一个看似边缘的想法，在极短时间内推演成一家年收入 200 亿美元的业务，也亲身经历过科技创业中最极端的法律与商业风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场对谈中，他系统性地拆解了几个被反复误解的问题：&lt;/p&gt;&lt;p&gt;为什么大模型至今仍然离不开人类专家？&lt;/p&gt;&lt;p&gt;为什么绝大多数企业数据对 AI 来说毫无价值？&lt;/p&gt;&lt;p&gt;以及，当模型开始转向“智能体”和决策能力时，真正的瓶颈到底在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客整理翻译：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;精华摘要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您在数据、标记、训练等领域都是当之无愧的技术先驱，您能不能展望一下AI领域在过去一年半以来的发展轨迹？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：一年半之前，大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;收购之后，Scale 还是 Scale 吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：欢迎今天到场的嘉宾Jason Droge，请先简单向大家介绍一下您的背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是Scale AI的新任CEO，这也是我在接替Alex Wang接受Meta收购之后参与的首次采访。Alex现在领导Meta旗下的超级智能团队。在加入Scale AI之前，我与Travis Calendar曾共同创立一家公司，再向前追溯还在Uber等几家初创公司工作。我最知名的成果应该是创立并领导了Uber Eats，跟团队的同事一道把这个点子培养成了如今市值数十亿美元的企业。在COVID期间，Uber Eats几乎是以一己之力支撑起了因社交隔离而陷入瘫痪的Uber业务体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这次采访的主题是AI模型如何拥有真正的智能。您觉得Scale AI在其中扮演了什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们在ChatGPT和Claude身上看到了诸多改进。目前各个前沿领域都存在领军级别的模型，各家实验室则聘请专家填补这些大模型的知识空白，校正其对于事物运作方式的理解。而Scale是这一领域的先驱，也可以说是创造了这种业务形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：我们都很关心Scale的近况以及被Meta收购之后的变化。Scale目前情况如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：Scale仍然是一家完全独立的公司。在此次交易中，Meta投入140多亿美元以换取Scale公司49%的无投票权股份，且未获得新的董事会席位。Scale的董事会保持不变，治理结构几乎未肥影响，Meta对于Scale的任何资源也都不具备优先访问权。我们跟Meta一直在数据业务方面保持着长期合作关系，随着双方关系更进一步，各方面合作也有望持续扩大。但我们与其他各方的合作不会受到影响，Meta无法访问任何之前不对其开放的信息，例如隐私和数据安全政策等。事实上，此次交易只涉及约15位员工的变动，而Scale共拥有约1100名员工。现在我们旗下拥有两大业务部门，其营收都达到了数亿美元规模。公司内部相当于两家独角兽，支撑着每月的业务增长。总之，我们很高兴能够继续构建并交付数据，维持之前的工作模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据不是苦力活：标注为何变成专家工作&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到 Scale主要面向AI数据市场，那能不能解释一下数据标注工作是怎么从当初的低成本劳动力转向如今的专家处理形式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：没问题。首先，我认为其他竞争对手目前的定位是错误的，所以我先从这个角度切入，再逐渐延伸其他方面。这里我先花点时间介绍一下Scale的发展史，还有自2016年以来的发展脉络。Alex很早就意识到，数据对于模型来说至关重要。那时候他只有19、20岁，但他已经在考虑要如何围绕这个基本前提建立业务。他最初选择的方向是为自动驾驶做标注。标注数据的质量越高，汽车的行驶表现也会更好。之后这股浪潮演变成了计算机视觉，我们开始跟国防部门建立合作关系，为他们提供标注服务，到这里时间已经来到2020年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来大模型的性能越来越强、愈发完善，需要的数据类型也更为丰富。所以我们一直在不断调整以提供所需的数据类型。在此期间，行业本身也在不断变化。记得两、三年前这些大语言模型刚出现的时候，经常会闹出幻觉问题，比如给出特别浅显的错误答案等等。但情况变化很快，我们也一直在随之改变。Scale一直走在前沿，开始为更复杂的任务提供专家级的数据标注服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要聊过去一年半的情况我其实不太够格，因为我才加入公司13个月。刚加入时，我一直在做模型性能的测试。那时候大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的专家中，有80%的同事拥有学士学位证书，这跟其他竞争对手的定位完全不同。其中约有15%的员工对相关行业有深入了解。这些高知人群通过为模型添加标签、贡献专业知识来赚取丰厚的收入。我很喜欢我们这种以专家级别进行数据标注的业务定位，这能帮助公司与研究人员保持联系、了解他们的需求。我们内部也很早意识到大模型在高度专业的领域上表现欠佳，所以我们会主动联系开发基座模型的大厂，表示我们注意到了这个问题，而且有专家团队可以搞定。我喜欢这种与众不同的定位，这跟竞争对手的想法完全不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那Scale是怎么接洽并挽留这些专家贡献群体的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：确实不太容易接触得到，所以得制定相应的策略。具体的方式肯定不止一种，最主要的就是让专家们相互内推，而且他们很喜欢这种用自己的专业知识为AI做贡献的感觉，很酷。比如一位特定领域的博士在面对具体主题时，发现大模型的表现根本无法令人满意。那这时候他就可以通过有偿的方式提供专业建议，并借此赚取数百甚至数千美元。当然，我们也会推动校招，直接跟学校里的教授和学生们交流，询问有没有人愿意参与进来。当然，LinkedIn等传统渠道也是开放的，但效果最好的还是线下接洽和内推网络。这样能够为参与者提供良好的体验，因为他们的贡献一方面是为了赚钱，但更多是出于为AI模型做贡献的成就感。而且这个过程也是在替他们自己解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能体要学会做事：没有捷径，只有环境&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有报道认为整个AI经济生态都将转向强化学习，您对此有何看法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：强化学习当然非常重要，我觉得这种趋势性判断也很有道理。强化学习环境就相当于AI智能体的沙箱，它们可以在沙箱中学会如何达成目标。我们在这方面也尝试了一年有余。比如在Salesforce实例当中，AI智能体要如何实现导航？襳中包含哪些需要识别的数据？这要求智能体执行一套可靠性极高的业务流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，智能体还得知道如果无法完成预期任务，或者判断正确完成任务的可能性较低，那要怎么向人类反馈以获取指引。所有这些都需要训练，而且不存在什么神奇的捷径。唯一的办法就是把AI智能体放进能代表人类正确操作的环境当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以想象，现实世界中此类环境的数据和其中的不同目标可以说是无穷无尽，所以我们花了一年多跟模型开发商保持良好的合作关系，共同观察在不同任务/环境下的通用性表现。很明显，这类环境、软件系统、配置、数据类型、规模和用户数量各有不同，复杂性也差异巨大。这&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就要求我们制定一种策略，让模型开发商能够收集到足够的通用数据以支撑广泛用例，而不必直接面对上万亿种任务和环境指标的排列组合。有些工作和数据之间具有更强的通用性，可以用一种简单的方式完成任务——比如在日历上找到要参与的访谈，让智能体浏览日历内容并弹出相应提示。接下来要做的，就是把智能体推广到一切日历搜索和日程管理操作。总之数据通用性越强，价值也就越大。我们的工作就是为模型开发商提供最有价值的数据，从而确保智能体尽可能为最终用户提供良好服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;企业AI不是演示：从 95% 到‘五个9’，差着一个世界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那您能举例聊聊具体向模型实验室提供哪些数据吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以。比如说我们的业务分为两个方面：其一是向模型开发商提供数据，也就是出售数据。其二则是向医疗保健系统、保险系统之类客户出售应用程序和服务解决方案。比方说我们跟一家医疗保健系统开发商合作，这套系统目前存在很多问题，部分专家需要定期处理少数罕见病例。因为专家人手不够，所以罕见病例会大量积压。这家医疗保健机构希望接诊更多病人，提供更好的护理体验并减少复诊次数，也就是在第一时间给出准确的诊断并制定治疗方案。如果没有AI的帮助，医生得耗费大量时间阅读长达两、三百页的病历文件。而我们开发的工具能帮助他们阅读这些文件，并指出其中最值得注意的五到十条内容。举例来说，某些过敏症状看似不起眼，但却可能跟医生开具的治疗药物发生冲突。AI工具可以快速提取这种人脑难以容纳的关联性要素，表现出相当完善的诊疗能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这方面，现成的模型肯定会有一定局限性，迫使医疗保健系统内部的人员亲自进行数据标注。不少企业乃至政府部门也会这么做，但仅靠现成模型加上一些零散数据没办法实现特别好的效果。毕竟很多银行或者医院一年的数据量就多达上百PB，他们自己根本没法判断哪些数据对模型有用。大多数数据都没价值，可怎么从中挑选出少数有价值的？而作为专业服务商，我们在数据的评判、挑选和专业知识储备方面非常出色，能够帮助客户顺利攻克这些瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：要保证AI变得越来越聪明，咱们人类到底还要参与多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：首先，数据标注本身就是一段不断创新的历程，就跟自动驾驶汽车一样。现在我们需要的数据标注量已经远远少于过去了，什么时候我们不再需要外部数据、模型的训练不再需要人工数据时，那就已经发展到新的阶段了。换言之，这意味着那时人类提供的一切技能和知识都已经不重要，没办法推动模型的进一步提升了。但对于Scale这样的企业，我们一直在研究如何刹那起能够发现新需求，并与贡献者网络合作的运营体系。我们会邀请专家贡献者来挖掘这些数据和信息。另外，他们的很多才能并不会第一时间就表现出价值。比如一年前很多知识对模型没用，但现在却突然有了大用。这是个不断进步的过程，需要将越来越多的数据输入到模型当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;出于经济动机，我们相信人类永远会在其中占据一席之地。这不只是种商业判断，更是我的个人信念，就像AI系统永远要为人类服务一样。我甚至觉得随着脑力岗位的逐渐消失，这就是未来知识工作者的主要转型方向。而且从我对部分客户身上观察到的情况，这种转变很可能在未来一到两年内发生。我当然希望这种颠覆别来得太快，但目前来讲确实是一切皆有可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于从长远来看，新技术总会替代旧方案，比如收音机淘汰了现场播讲之类。人类还是很善于适应这种变化的，技术的发展史就是人类适应新模式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您经常会提到“评估”这个词。那评估工作在专家们的日常工作中占比多少？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：占比还是相当高的。在企业和政府客户中，大部分业务内容就是评估，因为需要有人来设定“好”的基准。以之前提到的医疗保健来讲，医生在工作中就会评估病历报告和记录内容，然后对“好”做出明确的定义。这样慢慢累积起来的“好”和“正确”，就会让模型变得越来越可用。当然，AI的能力仍然有局限。对于那些人工流程的准确率很低的场景来说，AI就特别重要，因为能够切实帮上大忙。如果AI在其中能够达到50%、60%甚至70%、80%的准确率，那大家就乐疯了。但对于剩余的情况，比如人工流程的准确率能够达到98%，希望AI能解决余下的2%，那就很困难了。正因为如此，我们才需要明确定义“好”，让自己构建的系统能够代表使用者做出判断。在这样的设计思路下，AI系统就能像人类一样根据当前的信息尽可能给出最佳建议或者行动方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：很多人觉得AI是基于海量历史数据训练出来的，那AI在智能水平上怎么超越人类呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这种理解对，但也不对。首先，跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。而且这些铺设工作总得有人去做。负责任地讲，作为从业者，我对于如今大模型在一致性和准确性方面的提升仍然感觉喜出望外。现在大家可能已经习惯了大模型越来越先进也越来越靠谱，但短短三年前这个问题还相当复杂，需要综合考虑多种因素。总之，大模型的发展是算力、模型本体和数据改进的共同产物，而这三条确实在同时进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：大家对这个问题讨论得倒是不少，新闻报道也是层出不穷。具体怎么理解，要看我们选择怎样的视角。目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策。一旦到了这个阶段，我们之前提到的应用环境就非常重要了。比如怎么让智能体在医疗保健系统内正确导航、如何在手机上的天气应用中导航，又怎么替我们做出决策。目前这一切才刚刚起步，我也期待看到后续的更多变化。而这也是大家相对不太了解的层面，对于改进的方式也是莫衷一是。如果选择最乐观的判断，那这一切就是经济体系下的又一波正常变动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，AI普及不再是技术问题，而是人力和政策层面的问题。虽然目前还没到这个程度，但我确实相信未来两到三年之内，AI技术会发展到中心让治理层和政策制定者认真对待的阶段。现在已经离那个状态不远了，也就是两到三年的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：主要还是炒作得太狠了。我们的工作是打造出真正能够为客户创造价值的产品，并真正在解决复杂性上有所突破。还是以医疗保健系统为例，我们就在为医保公司提供理赔流程管理。这样的财务决策其实就是个可以自动化的过程，但在具体落地上学问就大了。很多人觉得概念验证能达到60%、70%的成功率就行，但这跟规模化应用还差得远。以数据中心为例，正常运行时间、可靠性和备份稳定性方面每增加一个“9”对应的都是又一个数量级的投入。比如四个“9”基本就是大学生自架服务器的水平，而五个“9”看起来只高了一点点，但其实完全是另一个世界。比如说很多人认为95%是个挺高的标准，但一旦用这个标准处理采购订单，那必然会面对无穷无尽的故障和投诉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总之，企业在应用AI时需要一步步完成法律批准、政策批准、监管批准和变更管理等各个环节，确保精度能让所有人满意。因此，AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。所以大家一定要分得清新闻炒作跟实践落地。就像我自己的教育背景，博士这个头衔说起来轻，但背后需要付出的努力远超大多数人的想象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;所有好生意，都是在不确定中被验证出来的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您参与建立了Uber Eats，还创办过其他几家初创企业。关于获客户这个问题，您有没有什么独家心得可以分享？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是那种愿意尝试一切新鲜事物的人，而且我总觉得创业是个非常清晰而且可把握的过程。我自己的思路是这样：在实际行动之前，先质疑自己听到的每一句话。我不会从字面上理解客户的表达，而是从产品管理的角度来审视。这个大家已经讨论得足够多了，比如说别按他们说的做、而要按他们预期的效果来做，这才是真正值得关注的问题。总之我会关注客户的潜在动机，而这种动机并不总是经济性质的，也往往跟自尊心和职业发展相关。比如说如果我们要向某人推销企业软件，那就得让对方相信你的软件能帮他们做好工作、建立起信任让对方接受你参与到大的项目中来。这个过程中重要的不只是产品，更要思考他们想得到怎样的建议、需要我们提供什么、需要怎么做才能找到正确的产品实现方向等等。我知道这话听起来有点陈词滥调，但只要让我准确把握住对方的真实动机，我就能拿出正确的结果。我再举个例子，当初在发布Uber Eats之前，我有认真考察业务。在获客方面，我们其实还给不出餐厅导览的功能，对餐饮行业也一无所知。但在Uber，我们最想解决的是接下来该拓展哪些其他业务。在考察了大量企业之后，我们觉得外卖业务最值得尝试，结果也证明这是正确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了搜集数据，我们找了一家餐厅供应商并拿到一份基础目录，比如说一份典型的餐食要用多少火腿、多少奶酪、多少面包和多少片生菜，再据此推断食材成本有多少、人工成本是多少，进而建立起基准数据。把这些因素综合起来，我们就能把餐食品类建立起清晰的认知。我们发现食材在每份餐食中的成本大约占20%到30%，人工又占20%到30%，10%是房租和其他开销。总之这就是一种链条，而结合核算出的附加价值之后，我们决定收取账单总价的30%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;餐厅刚开始会觉得这个数字太高了，本能性地想要拒绝。我们解释了自己的核算方式，说服对方做起来试试。事实证明餐厅的判断是正确的，这个比例确实太高了，最终确定下来的抽成是25%——跟我们的判断也相差不远。在这样的基础之上，我们再分析餐厅的主要价值实现形式是什么。对于游客型餐厅来讲，增加需求就是最关键的。在固定的餐厅店租、人力支出和食材成本都不变的前提下，需求增加三倍并不会增加人力支出，单纯的食材用量增加可以让产品的毛利率提升至70%到80%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这样的洞察，我们有信心在抽取一定费用之后店家和消费者都能接受。这就是市场经济的基本逻辑——不会只满足单独一方的所有需求，各方牺牲一点利益来保障自己余下的利益。Uber Eats就是这样的典型案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以善于独立思考闻名，这种能力为什么如此重要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：从创始人的角度来论述，在Uber我确实享受到了很多优势。我其实不算真正的创始人，只是参与了创业的流程。创业是涉及很多要素的，比如在97年创办第一家公司时，体验其实也就那样。但现在的创业可不一样了，每个人都在做自己的探索。但问题是，我们的研究方向多少都要受到周围言论的影响，那就没办法拥有独立的洞察力。所以最重要的独立思考，坚定去践行自己的判断。因此从创业的角度讲，我认为独特性非常重要。所以核心考验的就是人的洞察力，至于为什么我会幸运地拥有这种洞察力。这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。也许是因为我有点“遗世独立”，也可能是因为我是那种擅长逆向思维的人，总会在寻求其他人不相信的真理，有时候这也挺有效的。而且最难的部分是，我们愿不愿意在自己的判断上押上五到十年时间？人们总会犯错，只能尽量跟客户沟通，试着解决困扰他们的问题。创业就是这样，我们必须有这种强烈的自我表达意愿，不断摧毁自己曾经坚信的东西。更难的一点在于，我们又要有能力超越自己的观点，不能自大到总认为自己就是全世界最了不起的独立思考者。这个事很辩证，但最终就是要靠结果来证明和支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有时候最靠谱的创业方向，并不在于所谓最好的思路和机会。当然，对于我这种职业生涯已经超过25年的从业者，那选择空间和容错面会大得多。我认为一种业务的成功可以有两种途径：其一，也可以说是最重要的一种，就是创始人长期保持一股自我迭代和更新的力量。但这种年复一年的坚持其实很艰难。第二点是，可以直接去照搬其他人的经验，比如什么是好的商业模式、什么是差的商业模式、什么是好的市场定位、什么是差的市场定位。哪怕是拥有再强大的知识储备，哪怕理论上要进入的是一种比较差的市场定位，那只需要全身心投入，那随着时间推移也会逐渐显现回报。当然，我个人不会选择这种方式，我认为还是要根据市场需求走。纵观顶尖风投企业投资的项目，就会发现他们的投资组合是有规律的，至少是在对应价值数百亿美元的商业模式方面是有共性的，而且是具有网络效应的。规模大的业务就是比规模小的业务更有价值。比如我在Uber做过的新业务筛选，淘汰不好的想法其实挺快的，费不了多少时间。至于在筛选剩下的业务中，那就可以根据自己的直觉和热情去推动了。总而言之，我觉得大多数人对于哪些业务有机会增长到千亿美元规模缺少基本的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您推动了Uber Eats的上线。那在确定选择外卖赛道之前，你还探索过哪些其他想法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我肯定不是那种绝顶聪明的人，所以我会尽可能长期保持自己开放的态度，直到把各种有价值的元素都融合起来。有些想法刚开始看也许觉得比较差，但只需要不断挖掘，最终的对错判断很可能会反转。有一天，我在旧金山到处乱逛，看到了711之类的便利店。我就会想，大家要买到自己想要的东西需要转多少个拐角？难道不能直接把想要货品直接放进购物车吗？比如按下购物车上的按钮，它就直接把想要的货品送过来。毕竟叫“便利”店嘛，就得足够便利。所以我们在华盛顿特区推出了这项服务，在路上投放了大约十辆这样的卡车，里面装了大约250个吐货口。刚开始的情况很糟糕，根本就没多少人来买。于是我们意识到自己在下意识地找痛点，并不清楚便利店的核心吸引力是什么。我们的卡车不卖烟、不卖啤酒、不卖豌豆泥，我们不清楚大家最想买哪些商品。但说实话，工会的力量太强了，所以我天然地认为别用人工是最安全、成本最低的。我们很出色地解决了这个非经济学层面的问题，实现了便捷交付。但结果呢？我们做了Uber Spot还是什么，但跟点对点配送的Uber Direct一样，刚起步就表现不好。也就是说，消费者并没这方面需求，企业才有这种需求。2014年我们刚做尝试时，就没找到市场需求。后来我们持续更新了15个版本，最终发现外卖业务的表现才更出色，也拥有可靠的经济回报。这是个很酷的问题，我们可以用这些工具来支持夫妻店，让他们具备跟大企业竞争的资格。我们可以把房地产因素排除在外，让店面选址不再决定一切。只要你的餐食好吃，就完全可以吸引更多顾客。所以我觉得这是个很有趣的问题，真正促进当地经济发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：Uber Eats最终在Uber的危机时刻拉了母品牌一把，现在业务规模发展到多大了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们是2015年12月在多伦多推出了这项服务，大约两小时内销售额就达到了2万美元，简直是疯狂。我们很快意识到这个路子是对的，而且经济效益很好。我在Uber待了六年左右，用了一年半左右才把这个项目真正做起来，并在四年半之后把销售额做到了200亿美元。必须承认，Uber非常擅长扩大业务规模，但竞争激烈的市场上其他友商也做得不错。我们击败了很多对手，也有一些对手确实压我们一头。目前业务规模正在向着800亿美元迈进，时间才过去了短短四年半。我想COVID在其中也发挥了很大的推动作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：众所周知，您曾经反对麦当劳加入Uber Eats。能分享一下这个故事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这事说起来就有意思了。或许有时候不想太多反而会让人意外找到正确的原因。我们发布的Uber Eats在全球范围内获得了广泛成功，而其中的基本愿景很简单：让小餐馆也能跟大规模连锁餐厅正面竞争。以巴黎来说，大家去巴黎旅行肯定不想吃大牌连锁餐厅，而更想发掘本地特色小店。这是现实需求，我们也决定参与其中。但后来麦当劳联系了我们，表示想跟我们一起做外卖业务。我们拒绝了，哪怕对方强调他们的日均消费者高达8000万。在拖了四、五个月之后，我们团队觉得我肯定是疯了，他们想促成这件事、而且愿意为之倾力投入。最终，我们还是跟麦当劳建立了独家合作关系，获得了大量连锁店客户。那时候大家都担心每单收益还能不能保证，毕竟订单规模到了单日几千万级别，这肯定是笔大钱。面对现实问题，Uber的企业文化就是缩小配送半径、在必要时提高某些食物的价格，反正总有办法解决的。三个月之后又有新的问题出现，业务再次陷入困境……总之很多同事觉得我在跟麦当劳合作方面太固执了，但我觉得最终至少还是达成了一笔很棒的交易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你一直很关注毛利率，能不能具体说说？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是这样的，当然这只是我的评判标准之一。当然，也有不少企业本身很好，但毛利率也不高，比如说好市多、沃尔玛之类。亚马逊也有类似的情况。但总的来说，高毛利率加上相对较低的客户流失曲线，对企业来说肯定是个很健康的运营信号。毕竟生意的本质就是增加价值，这就像一块客观的试金石。我们在开展新业务的时候肯定也受到过毛利率问题的困扰，比如刚开始先试试毛利率40%的方案，发现可行再试着提升到60%——这时候商家就觉得不能接受了，大家再坐下来交流。至于离岸外包公司，他们的毛利率是多少？查了一下，大概在20%，而且已经运营了很长时间。那按这个规律来讲，我们的毛利率最终也将不可避免地从40%下降到20%，除非真能找到差异化的突破，否则必然要陷入这个巨坑。所以我认为毛利率只是个很粗糙的指标，远不能算是完美的工具。但至少它可以是种快速高效的过滤器，可以考查突然跳出来的想法能不能通过初步评估。比如说核算之后发现毛利率很低，就只能通过后续销量来弥补，那这事恐怕就不大行得通。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到“不输”是获得成功的先决条件，能不能给我们具体解释一下这个理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们的科技文化是由投资者来建立投资组合，很多叙事是由投资者掌控。坦率地讲，创始人肯定也会参与其中。这种情况当然是比较理想的，只是我们没法确定自己会不会有这份运气。如果真说自己人生中只有一次尝试的机会，那我肯定不会轻易去行动，必须得三思而后行。虽然没有这方面数据做支撑，但我发现自己朋友圈里的企业家和最出色的创业者，会审视自己做决策时的风险状况，并在整个过程中都持续做出均衡和积极的决策调整。很多时候我们会忘记决策背后是对应着风险的。这里还有很多可以讨论的部分，因为我觉得用高风险决策最终取得成功是种特别不可取的文化现象，连培训当中都会认可这种思路。毕竟高风险决策必然带来巨大的波动，这对创始人最重要的特质——也就是坚持下去的能力是种直接挑战。大多数人在寻求最佳时机、与客户建立良好关系和将合适产品推向市场之前，就已经放弃退出了。而科技行业瞬息万变，我们确实可能在短时间内从平凡之人变成技术英雄，但大概率这会是个漫长的过程，得先活下来才能谈成功。而当前我们正身处炒作周期，每个人都想尝试、全力投入，但却没意识到客户会一直在，希望自己的问题能得到解决。总之，生存是一切的前提，我们要尽量别把业务发展置于危及的境地。当然，我也不是说完全拒绝任何冒险，这是个需要认真权衡利弊的大问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快问快答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您肯定也经历过失败，能不能分享一条从痛苦经历中汲取到的教训？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我还是认为多花点时间提前思考，可以避免后续的很多麻烦。我之前确实做过一些尝试，但结果一般就不细聊了。在2001年互联网泡沫破裂之后，我曾打算筹资创办一家公司、而且是能赚钱的公司，成果就是Scour。坦白讲，当时我没在科技行业发现什么好机会，所以我开始在网上卖二手高尔夫球杆，还真赚了不少钱。那会我才22岁，考虑得并不周全，而且我的预期也不高，因为我觉得这生意什么人都能做。但我确实赚了很多钱，甚至想过把全美国所有的二手高尔夫球杆都买下来，直接操控整个市场的交易价格。我太年轻、太自负了，根本没认真思考过这件事的可行性。总之我就这样进入了这个行业，还靠这个赚了几百万美元。但整个过程都让我很痛苦，因为&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您对招聘人才和组建团队很有见解，能不能聊聊自己的理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：最近我对这个问题又有了更具体的理解。比如某些职位，就必须要有对当前市场丰富的观察和理解。毕竟市场发展太快，没时间慢慢去培训，所以要找的就是那些哪怕其他条件弱些、但真正理解市场和能够跟客户建立良好关系的人。有了这个前提，其余的部分才能跟公司共同成长，并建立起理想的职业发展轨迹。当然，这类职位只能在公司中只占5%，但它们对于产品的快速上市非常重要。比如在面试当中，我就只考查三点：对于解决问题是否抱有足够的好奇心，是不是擅长把自己的想法准确表达出来，还有能不能很好地跟其他人合作、特别是扮演好领导者。我相对不那么看重专业知识，毕竟我自己肯定有能力边界，不可能在所有专业知识上都做出准确判断。但只要能成功做好这几点，对方的成功几率就相当高。面对世界的持续变化，我们需要的就是具备极强适应能力的人。以Uber Eats为例，当初组建项目管理团队时，我总会通过招人把团队设计成一个优势互补扔 机体，同时尽量减少团队中除运营以外的大部分冲突。而且从一无所有到高达200亿美元的估值，我的这个理念始终没有动摇。我一直坚信团队成员间了解彼此的优势和劣势，而且能够相互弥补，这比传统上的各种考核标准都要靠谱。换句话说，我必须得学会相信员工，因为我不可能亲自掌控一切。当然，人事系统是非常复杂的，不可能基于我简单的几句话就生搬硬套。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：在日常生活和工作当中，你发现了哪些AI应用方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：说实话，我在加入Scale之前是在消费电子领域工作，也参与过政府层面的一些应用项目。AI这个领域发展太快了，每当有新概念出现我都会认真学习，也会向公司里的其他同事请教技术细节，比如说数据和产品的技术特征。但他们的时间也有限，更多新概念还得靠自己主动学习。大家可能不相信，我的主要工作并不是处理跟AI相关的工程问题，而是管理这个组织。为了避免频繁打扰同事，很多时候我也会直接跟AI学习，在上下班的路上跟它聊天。这已经变成我生活中的一种习惯，也是我从业以来见证过的最不可思议的奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如之前采访Perplexity创始人的时候，对方就介绍他们要求同事在提出任何问题之前，必须先请教AI。那时候这还是种很疯狂的工作方式，但现在看来他们的领先恰恰体现在这里。至于在工作当中，我会上传一份内部文件，然后边亲自阅读边比较它的提炼结果。让人震惊的是，AI的表现真的非常出色，而且帮我节约了大量时间。在大规模组织中，我们经常会遇到这样的难题：我不知道你想让我说什么，我也不知道自己需要了解什么，这就导致大家各有议程、自说自话。那面对这样的传播挑战，AI确实能帮上大忙，太神奇了。我现在会用它来处理法律文件，比如快速了解对手打算怎么对付我、我又该怎么应对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您还有什么想跟听众们分享吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有啊。我想向大家强调Scale团队的卓越贡献。我们一直非常努力，持续为客户提供巨大的价值。任何语言在这份努力面前都显得苍白无力，更无法体现客户在此基础之上解决的无数问题。我认为这份付出配得上一切尊重和回报。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最近发现什么自己特别喜欢的产品了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：应该说是Veo 3吧，虽然不算全新产品。高中时我曾梦想当个编剧，还认真写过剧本。所以这次我找出第一页、拍下照片，再上传到Veo 3。结果真的让我震惊，居然一张剧本照片也能生成相应的视频画面。现在我在考虑怎么把这些工具用来生成家庭录像，再利用其他工具让内容更加生动。虽然还有进一步迭代的空间，但这种体验真的很有趣。这类工具真的会改变人们的情感生活，比如让祖父母、亲戚或其他很久没见的人在照片中动起来，这会产生很大的情感冲击。训练出这套模型的技术人员很厉害，而帮助他们做专家级数据标注的服务者也很厉害。这种直接把文字转化成光景的能力很棒，也很酷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最喜欢的人生格言是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：终点永远不是终点。这是我最喜欢、也深深牢记心中的格言。这跟之前关于生存才是第一要务的观点差不多，只有先活下来才有机会获得成功。纵观自己的创业历程，我觉得这条的指导意义最大。每个人都会经历艰难的旅程，但只要能在这段旅程中坚持五年，那大家的精神承受力绝对会比99.9%的人强。更具体地讲，我们在努力工作时会深切感受到这句话的意义。有时候我们觉得自己太累了，想要停下来，但事实上只要继续前进，似乎就又可以坚持下去了。我牢记这句话，提醒自己任何一个节点都不是真正的终点，仍然有更远的标的有待探寻。所以无论当下的解决方案到底完不完美，我们都可以先勇敢接受，然后抖擞前行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果想要与您交流或者了解更多关于Scale的信息，应该怎么安排？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以，大家可以关注我的邮箱，随时了解最新动态。如果是招聘方面的诉求，可以直接访问scale.com、进入我们的招聘页面，目前公司开放了250个空缺职位。我们的业务仍在扩展，包括应用程序业务、数据业务和服务业务都在疯狂增长。我们需要更多人手来帮助我们推进这段旅程。我们还刚刚跟政府签下了巨额合同，金额是21亿美元——而且不是一份，而一个月内签了两份。我们的政府业务做得很好、企业业务做得很好、国际政府业务同样做得很好。公司市场需求很大，也让不少销售人员拿到了丰厚的佣金。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&quot;&gt;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</link><guid isPermaLink="false">https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</guid><pubDate>Tue, 10 Feb 2026 11:43:36 GMT</pubDate><author>核子可乐,Tina</author><category>生成式 AI</category></item><item><title>Java探索载体类以扩展面向数据编程</title><description>&lt;p&gt;OpenJDK的&lt;a href=&quot;https://openjdk.org/projects/amber/&quot;&gt;Amber项目&lt;/a&gt;&quot;发布了一份全新的设计说明，名为“&lt;a href=&quot;https://openjdk.org/projects/amber/design-notes/beyond-records&quot;&gt;Java面向数据编程：超越记录类（Record）&lt;/a&gt;&quot;”，阐述了一种探索性的方案，以便将类似记录类的特性拓展至更灵活的类设计中。该文档引入了载体类（carrier class）与载体接口（carrier interface）的概念，目标是提炼记录类的核心优势并进行通用化适配，同时不再强加严格的表述规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Java 16引入了记录类，为不可变数据载体的建模提供了简洁的方式。如下这种记录类的声明：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;record Point(int x, int y) { }
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译器会自动为其生成规范的构造器、访问器方法，以及equals、hashCode和toString方法的实现。记录类还支持解构（deconstruction）模式，可以配合instanceof和switch关键字使用。结合密封类与模式匹配特性，记录类能实现Java中代数数据类型的建模。例如，HTTP客户端或网关可以按如下方式定义不同的响应类型：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;public sealed interface HttpResponse permits HttpResponse.Success, HttpResponse.NotFound, HttpResponse.ServerError {
    record Success(int status, String body) implements HttpResponse {}
    record NotFound(String message) implements HttpResponse {}
    record ServerError(int status, String error) implements HttpResponse {}
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这样的响应类型层级，我们可以通过穷举式模式匹配进行统一处理：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;static String handle(HttpResponse response) {
   return switch (response) {
       case Success(var code, var body) -&amp;gt; &quot;OK (&quot; + code + &quot;): &quot; + body;
       case NotFound(var msg) -&amp;gt; &quot;404: &quot; + msg;
       case ServerError(var code, var err) -&amp;gt; &quot;Error (&quot; + code + &quot;): &quot; + err;
   };
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此示例中，编译器会强制检查是否覆盖了所有允许的响应类型。若新增一种响应类型，必须同步更新该switch表达式，从而降低不完整错误处理的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期的一次讨论中，甲骨文公司的Java语言架构师&lt;a href=&quot;https://www.linkedin.com/in/briangoetz/&quot;&gt;Brian Goetz&lt;/a&gt;&quot;指出，这些特性的组合虽然能实现强大的数据建模能力，但实际落地却经常会受到长期形成的面向对象设计习惯制约。他发现，即便现代语言特性已能大幅减少间接代码，开发人员仍会习惯性地设计用于中介数据访问的API。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明重点聚焦于记录类无法适用的场景。实际开发中，许多数据类型需要派生值或缓存值、可选的内部表示形式、可变性或继承特性。在这种情况下，开发人员只能退而求其次，使用传统类，并重写大量的样板代码。文档将这种转变形容为“断崖式回落”，对记录类的基准模型做微小调整，就会导致代码量的大幅增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了缓解这一问题，文档提出了载体类的设计思路。载体类以类似记录类头信息的状态描述作为开头，其余行为则与普通类完全一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，状态描述用于定义类的逻辑组件，编译器可基于这些组件自动生成访问器、对象方法及解构模式。与记录类不同，载体类不要求将所有状态仅存储在这些组件中，这也是其核心灵活性所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种灵活性能实现记录类难以表达的设计模式，例如缓存派生值的场景：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
    private final double norm;

    Point { norm = Math.hypot(x, y); }
    double norm() { return norm; }
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此例中，派生值norm在构造阶段计算完成，并且未纳入状态描述，但该类仍能借助编译器为其组件自动生成的方法，减少样板代码编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;载体类同样设计为可与模式匹配深度集成，用法与记录类一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;if (obj instanceof Point(var x, var y)) {
    // use x and y
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设计说明中还进一步探讨了载体类与未来重构特性的兼容性，例如，针对记录类的&lt;a href=&quot;https://openjdk.org/jeps/468&quot;&gt;JEP 468&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除载体类外，该提案还引入了载体接口的概念，接口可声明自身的状态描述，并且所有实现类都能参与统一的模式匹配：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;interface Pair&lt;t, u=&quot;&quot;&gt;(T first, U second) { }

switch (pair) {
    case Pair(var a, var b) -&amp;gt; ...
}
&lt;/t,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计能简化日常开发中常见的元组式抽象，同时保留Java强类型的优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明将载体类置于Java向面向数据编程整体转型的背景下，通过结合记录类、密闭类型、模式匹配，再加上潜在的载体类，Java正逐步引导开发者直接建模数据结构，而非依赖层级繁杂的API。Goetz认为，当前的核心挑战在于，帮助开发者意识到，在将“数据”作为首要抽象时，大量的支撑性代码都可被省略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，“超越记录类”还属于探索性的文档，官方尚未公布具体的语法定义、JEP提案及版本发布时间表。但这份文档释放了明确的信号，那就是Amber项目将持续推进相关研发，进一步减少Java的样板代码，并将现代语言特性拓展至更复杂的类设计中，而这些探索，也许会将影响未来版本中Java开发者构建以数据为核心的 API的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/java-beyond-records/&quot;&gt;Java Explores Carrier Classes to Extend Data-Oriented Programming Beyond Records&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</link><guid isPermaLink="false">https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</guid><pubDate>Tue, 10 Feb 2026 11:26:42 GMT</pubDate><author>作者：A N M Bazlur Rahman</author><category>编程语言</category></item><item><title>谷歌推出托管AlloyDB连接池</title><description>&lt;p&gt;谷歌云&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/release-notes#December_18_2025&quot;&gt;正式发布&lt;/a&gt;&quot;AlloyDB for PostgreSQL通用托管连接池，将类似PgBouncer的功能直接集成到数据库服务中。按照谷歌的说法，与直接连接相比，这一特性能够提供3倍多的客户端连接和高达5倍的事务吞吐量，帮助开发者解决了运行高并发工作负载时面临的扩展挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接池并不是什么新鲜事。多年来，为了重用数据库连接而不是为每个请求创建新的连接，开发者们将&lt;a href=&quot;https://www.pgbouncer.org/&quot;&gt;PgBouncer&lt;/a&gt;&quot;或&lt;a href=&quot;https://www.pgpool.net/docs/latest/en/html/&quot;&gt;pgpool&lt;/a&gt;&quot;作为单独的基础设施进行了部署。现在，AlloyDB可以自动完成这些工作了。开发者可以通过控制台复选框或API调用来启用它，连接池使用6432端口，而常规连接使用5432端口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;托管连接池会缓存预先建好的连接，将它们分配给传入请求，并在使用完成后将它们返回给连接池，而不是关闭它们。谷歌表示，这可以消除“运维负担”，作为AlloyDB实例的一部分，连接池会自动升级和扩展。连接池和数据库之间的通信在谷歌云的网络内运行，可能比外部连接池设置的延迟小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Cloud Run或Cloud Functions上的无服务器部署，其优势更为显著。这些平台会启动多个实例，每个实例都会打开数据库连接，在流量高峰时往往会超出PostgreSQL的连接限制。对于这种情况，连接池是一个很好的缓冲，它利用现有的连接处理请求，而非强制数据库同时处理数百个新的连接尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UKG高级首席架构师Jeff Bogenschneider在早期测试期间描述了其影响：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;AlloyDB的架构使我们能够在单个集群中部署的数据库数量远超其他Postgres托管服务。此前我们曾担心连接限制问题，而托管连接池可以帮助我们确保全球的客户都能获得最佳的性能，让我们得以自由地扩展业务，而不用担心在高峰使用时段遇到连接限制问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;运行微服务的开发者应该考虑将应用端连接池与AlloyDB的托管连接池配对。在&lt;a href=&quot;https://medium.com/google-cloud/elastic-microservices-rigid-databases-connection-exhaustion-8cdc558f212a&quot;&gt;Medium&lt;/a&gt;&quot;上，Adarsha Kuthuru和Kumar Ramamurthy详细描述了这种“双池”模式：像HikariCP这样的应用连接池为每个实例维持5-10个到AlloyDB连接池的连接，后者通过多路复用将这些连接连接到数量更少的后端数据库连接。这个方案可以避免为50个微服务实例各建立20个连接时，1000个并发连接冲击数据库的场景。作者建议为每个vCPU配置15-20个连接器连接，并协调各层的超时设置，避免连接重置错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能提供两种连接池模式。事务模式（默认）通过为每个事务分配独立的连接来最大化可扩展性；会话模式完全兼容PostgreSQL的功能。开发者可以通过AlloyDB API中的标准PgBouncer参数调整连接池规模、超时设置及空闲阈值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能存在一些限制。托管连接池不适用于AlloyDB Auth Proxy或语言连接器——开发者需要直接连接。这妨碍了依赖身份验证代理进行凭据轮换或简化TLS配置的部署模式。在2024年11月前部署的实例上启用连接池功能时，由于要更新VPC设置，会引发短暂的网络中断（持续时间少于15秒）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于已经单独运行PgBouncer的开发者而言，迁移至托管连接池主要在于整合基础设施——减少一个需要打补丁的组件。对于新增部署，尤其是无服务器或高并发工作负载，启用该功能所需的投入极少，却能防患于未然，在扩展问题爆发前将其及时化解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌提供了&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling&quot;&gt;配置托管连接池&lt;/a&gt;&quot;的文档和在现有实例上&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling#enable-managed-connection-pooling&quot;&gt;启用该特性&lt;/a&gt;&quot;的最佳实践。对于双池模式，发表在Medium上的博文提供了一份部署指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&quot;&gt;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</link><guid isPermaLink="false">https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</guid><pubDate>Tue, 10 Feb 2026 11:21:10 GMT</pubDate><author>Steef-Jan Wiggers</author><category>Google</category><category>大数据</category></item><item><title>突发！继杨格过劳病离职后，xAI又一位联创出走，疑单独创业</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;刚刚，xAI 又损失一位华人联创&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;几个小时前，全球首富马斯克旗下人工智能公司 xAI 再迎联合创始团队成员离职。xAI 公司联合创始人 Yuhuai (Tony) Wu（音译：吴玉怀）在x上发文称，今天正式从 xAI 辞职了。他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这家公司——以及我们之间如同家人般的情谊——将永远铭刻在我的记忆中。我会深深怀念这里的人们、作战室，以及我们并肩作战过的所有战役。&amp;nbsp;我的人生新篇章即将开启。这是一个充满无限可能的时代：一支配备人工智能的小团队可以移山填海，重新定义一切皆有可能。”&amp;nbsp;致埃隆 &lt;a href=&quot;https://x.com/elonmusk&quot;&gt;@elonmusk&lt;/a&gt;&quot;，感谢你们相信我们的使命，也感谢你们带给我们这段毕生难忘的旅程。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f34f72fba29a82e49db7cc369f6d2cdc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 LinkedIn 资料和媒体相关报道，吴是著名人工智能研究者与企业家，因联合创立 xAI 而广为业界所知。吴在 xAI 的任职期间被视为技术与研究团队核心成员之一，负责推动推理与数学智能相关方面的研发工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据吴的LinkedIn个人资料显示，在加入该公司之前，他曾在谷歌工作近两年，担任研究科学家（Research Scientist），参与与神经网络、数学推理相关的大型语言模型等研究项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;博士阶段曾分别在 DeepMind 工作约 11 个月，并在 OpenAI 担任过科研实习岗位（数月）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在学术贡献上，他是多个顶级国际会议论文的作者或共同作者，例如关于大语言模型与数学推理、定理证明等的研究成果。其部分成果被视为推动 AI 数学与符号推理能力前沿的重要贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e3/e3feb6b2078bfe074af6af03e07f2213.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，吴玉怀是过去一年中第四位离开公司的联合创始人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他离职之前，xAI 公司的另外几位创始人 Christian Szegedy 于去年2月离职，Igor Babuschkin 于去年8月离职，而杨格上个月表示，由于健康原因，他已暂时退出公司事务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;给马斯克工作，压力太大？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先提出离职的是 Christian Szegedy，但他并没有在x上透露过多关于未来去向的信息，也未明确解释离职原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但他离职后，去年8月的 Igor Babuschkin 在离职时在x上发了长文感慨和马斯克一同创业的时光，他首先回顾了2023 年初，几位创始人创建公司的初心。他们确信：人类正在逼近通向超级智能的“配方”。一切迹象都在表明，AI 很快就可能在推理能力上超越人类。那随之而来的问题是——人类该如何确保，这项技术被用于善的方向？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多年来，马斯克始终警示强大 AI 所潜藏的风险。正是在这样的背景下，他们发现彼此拥有完全一致的愿景：让 AI 造福全人类。于是，他们集结了一群志同道合的工程师，xAI 正式启程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Igor Babuschkin还首次揭秘的创业时的艰辛，并称自己从马斯克身上学到了两条无价的准则：&lt;/p&gt;&lt;p&gt;第一，永远不要畏惧亲自下场解决最棘手的技术问题；&lt;/p&gt;&lt;p&gt;第二，保持一种近乎偏执的紧迫感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在帖子的结尾，Igor Babuschkin 表达了自己离职的根本原因不是挫折或失败，而是个人使命的聚焦与升华。他表示自己已经创办了新公司，名为： Babuschkin Ventures，希望获得更多关注和支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;帖子翻译如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我依然清楚地记得第一次见到埃隆的那一天。我们围绕人工智能以及它可能塑造的未来，连续聊了好几个小时。那次交谈中，我们达成了一种几乎无需言说的共识：这个世界，需要一家使命不同、方向不同的全新 AI 公司。&amp;nbsp;构建真正推动人类前进的人工智能，是我一生的梦想。&amp;nbsp;苏联解体后，我的父母离开俄罗斯联邦，踏上移民之路，只为给孩子寻找一个更好的未来。作为移民，生活从来谈不上轻松。但即便在最艰难的时刻，他们依然坚信：人类的价值是无价的——勇气、同理心，以及对理解世界的永恒好奇。&amp;nbsp;童年时期，我仰慕理查德·费曼、马克斯·普朗克这样的科学家。他们不懈地推动物理学的边界，只为更接近宇宙的真理。后来，我在 CERN 攻读粒子物理博士，满怀激情地希望自己也能为这一使命贡献力量。然而，寻找“新物理”变得越来越困难——需要更庞大的对撞机，却换来越来越稀少的突破。&amp;nbsp;于是我开始思考：解开宇宙之谜的钥匙，或许并不是更大的对撞机，而是超级智能。&amp;nbsp;AI 是否能够构建一套自洽的量子引力理论？AI 是否有一天能证明黎曼猜想？&amp;nbsp;2023 年初，我逐渐确信：我们正在逼近通向超级智能的“配方”。一切迹象都在表明，AI 很快就可能在推理能力上超越人类。那随之而来的问题是——我们该如何确保，这项技术被用于善的方向？&amp;nbsp;多年来，埃隆始终警示强大 AI 所潜藏的风险。正是在这样的背景下，我们发现彼此拥有完全一致的愿景：让 AI 造福全人类。于是，我们集结了一群志同道合的工程师，xAI 正式启程。&amp;nbsp;xAI 的早期并不轻松。质疑者告诉我们：我们入局太晚了，从零开始打造一家顶级 AI 公司几乎不可能。但我们选择相信“不可能”。&amp;nbsp;从零创业，意味着事无巨细、亲力亲为。最初，我亲手搭建了公司大量底层工具，用于启动和管理模型训练任务。后来，我负责统筹公司相当一部分工程工作，涵盖基础设施、产品以及应用型 AI 项目。&amp;nbsp;xAI 的人，是我见过最投入、最坚定的一群人。&amp;nbsp;在血汗与泪水中，我们以惊人的速度建成了孟菲斯超级算力集群，并以前所未有的节奏交付了前沿模型。&amp;nbsp;从埃隆身上，我学到了两条无价的准则：第一，永远不要畏惧亲自下场解决最棘手的技术问题；第二，保持一种近乎偏执的紧迫感。&amp;nbsp;xAI 的执行速度，快到近乎疯狂。&amp;nbsp;业内资深人士曾断言：在 120 天内建成孟菲斯超级集群，根本不可能。但我们依然选择相信“不可能”。&amp;nbsp;在期限临近时，集群节点之间的 RDMA 通信频频出现诡异问题。埃隆决定亲自飞往数据中心，我们随即跟上。基础设施团队在深夜抵达孟菲斯，几乎没有休息，立刻投入排查。&amp;nbsp;在翻阅了数万行 lspci 输出后，我们终于锁定了罪魁祸首——一个错误的 BIOS 设置。埃隆一直陪着我们奋战到深夜。当训练任务终于跑通时，他在凌晨 4:20 发帖庆祝，那一刻我们忍不住大笑出声。&amp;nbsp;我永远不会忘记那一夜的肾上腺素飙升，也不会忘记那种“我们真的在一起并肩作战”的情感联结。那晚入睡时，我们都清楚地意识到：自己正身处人生中最激动人心的时刻。&amp;nbsp;我对 xAI 这个大家庭，怀有无比深厚的感情。&amp;nbsp;你们是我合作过的最投入、最顽强的一群人。能够如此迅速追赶并站上技术前沿，靠的不是奇迹，而是每一个人的拼劲与团队精神。&amp;nbsp;感谢每一位与我并肩走过这段旅程的人。我想向你们的付出、时间与牺牲致敬——这些从来都不容易。我会永远记得那些灯火通明的深夜，记得我们一起熬过的每一次极限冲刺。&amp;nbsp;今天，当我驱车离开时，心情就像一位送孩子远行上大学的父母——骄傲、欣慰，眼眶湿润。我会继续注视着这家公司成长、成熟。&amp;nbsp;迈向人生的下一章节时，我再次想起父母当年的移民选择——为了让下一代生活在更好的世界。不久前，我与“未来生命研究所”创始人 Max Tegmark 共进晚餐。他给我看了自己年幼儿子的照片，然后问我：“我们该如何安全地构建 AI，才能确保我们的孩子真正繁荣成长？”&amp;nbsp;这个问题深深触动了我。&amp;nbsp;在更早的职业生涯中，我曾担任 DeepMind 的 AlphaStar《星际争霸》智能体技术负责人，亲眼见证了强化学习在规模化后所释放的惊人力量。随着前沿模型在更长时间尺度、更广任务范围内变得愈发“具备代理性”，其能力也将不断放大——这使得 AI 安全研究变得前所未有地重要。我希望继续自己的使命：推动安全、对人类有益的人工智能。&amp;nbsp;今天，我正式宣布创立 Babuschkin Ventures，专注支持 AI 安全研究，并投资于推动人类进步、探索宇宙奥秘的 AI 与智能体系统初创公司。&amp;nbsp;如果你愿意交流，欢迎通过 ventures@babuschk.in 联系我。奇点正在逼近，但人类的未来依然光明。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;再然后就是前不久，1月21日，xAI 的另一位联创 Greg Yang (音译：杨格）也在x上发文称已经离职。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨此前曾在微软公司工作，是马斯克 2023 年人工智能初创公司的创始成员之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨在x上发文表示，他可能在一段时间前感染了莱姆病，症状是在 xAI 高强度工作期间变得明显的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种疾病是由蜱虫叮咬引起的，会导致炎症。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/04/046ac82a85ffa5fa401966fa7d1e264a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杨在x 上发文称，其实自己生病的症状在很久以前就已经感染了，只是一直到高强度投入 xAI 的研发构建、免疫系统被持续消耗之后，症状才真正显现出来。这里很容易读出他的言外之意——超高强度工作，伤害了身体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他表示从整体来看，反而觉得自己是幸运的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;莱姆病是一种严重的疾病，拖得越久，治疗难度越大。很多患者在五六十岁时才被发现，情况往往要艰难得多。它甚至可能让人长期卧床、丧失行动能力。而他，至少现在仍然可以正常生活，照顾好自己。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杨还表示：“所以，尽管有人对我说不该把自己逼得这么狠。但我并不后悔。正因为我曾那样拼命，我才得以及早发现问题；而现在，我可以修复它——这样，当我重新站起来时，就能比以往走得更远。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得玩味的是，尽管&amp;nbsp;Igor Babuschkin离职后发表了长篇大论解释了离职原因，但在离职后，他也公开吐槽了科技公司对工程师缺乏耐心：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;许多AI公司未能给工程师足够的时间和心态去做出最好的工作，导致代码和系统不可靠。良好的公司文化，注重卓越、专注和足够休息，能带来更好的成果。早期Google就是这种文化的典范，创始人们应该借鉴他们的策略。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，就是今天刚刚宣布离职的吴，但从他发文中可以隐约提到的将开启人生新篇章，并表示这是一个充满无限可能的时代，一切皆有可能，外界猜测他离职的原因是要单独创业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;世界首富也睡过车间地板&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在科技圈乃至大众媒体中，马斯克既被视为颠覆行业的创新者，也常因其极端的工作和管理方式而成为争议焦点。无论是在特斯拉、SpaceX，还是他于 2022 年收购后的微博（Twitter，后更名为 X）、以及最新的 xAI，马斯克对效率、速度和结果的近乎苛刻追求，塑造了一种鲜明而强烈的企业文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克本人对生产和执行的标准极高，这一点体现在多个层面：无论是火箭发射、汽车量产，还是 AI 平台的快速迭代，他都要求以超出常规的节奏推进。对他而言，工作不是常规的职业任务，而是一种总体使命的极致实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克长期以身作则，亲自展示“全员投入”的文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在特斯拉 Model 3 产能冲刺阶段，他曾公开表示自己多次睡在工厂地板上，与团队同吃同住，以身作则推动生产进度。此举被他本人解释为希望自己的处境比其他员工更“糟糕”，以此激发团队极限投入的精神。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他接手推特后，类似的高强度工作节奏再次出现。据报道，高管和员工为了赶项目上线与平台改造，不得不在办公室过夜，有人甚至将办公室布置成临时卧室。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种文化也延续到了新的业务单位。在 xAI，有员工因此张贴自己连续 36 小时未睡工作的照片，并获得同行与马斯克本人的回应，成为“极致奉献”的象征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些事例并非孤立现象，而是马斯克管理体制的核心体现：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;全员以任务完成为唯一衡量标准，在不惜个人生活成本的条件下追求快速执行。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克对组织流畅和成本效率的执念，也体现在他接手推特后大规模裁员与重新设定公司节奏的做法上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他接管后短时间内削减了约 50% 的员工，以期通过快速精简来降低成本并重塑团队结构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，他的内部沟通中强调“长时间高强度工作是继续留任的前提”，并要求员工亲自回到办公室工作、放弃远程安排。这样的政策在推特内部引发了大量讨论与反弹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种以极限 KPI 和严格劳动投入作为衡量绩效指标的方式，反映出马斯克对“成果优先、短期快速推进”的坚定信念，但也因此产生了显著的压力文化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;长期以来，马斯克管理方式的成功也伴随着争议。批评者认为他的严苛要求置工作效率于健康和心理福祉之上，尤其是在后疫情时代的职场环境中，这种风格显得格格不入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，马斯克在推特上曾要求员工在特定期限内选择接受“高强度工作”或离职与三个月遣散费的方案，这种二选一的选择在劳动力市场中引发了关于员工权利与企业管理伦理的广泛讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管马斯克支持者认为这种做法有助于推动快速创新和执行效率，但批评者指出，这种过度强调短期指标和工作时长的文化，可能会导致高离职率、身心健康问题，以及长期人才流失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管存在争议，马斯克模式背后却有其一致性逻辑：他不满足于常规的“业务增长”，而试图推动技术、生产、产品乃至整个人类文明的极限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论是加速电动汽车普及、实现火箭可复用、还是构建被他视为下一个关键技术节点的人工智能系统，所有这些目标在他眼中都不容许“慢与保守”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他本人也强调领导者的角色不仅仅是分配任务，更是“培养能思考的人”，希望员工不仅知道“做什么”，更要知道“如何思考”以解决复杂问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种极端使命感驱动的管理哲学，既是他能够成功推进多个行业边界的动力来源，同时也是造成高压力工作文化的重要根源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://yuhuaiwu.github.io/?utm_source=chatgpt.com&quot;&gt;https://yuhuaiwu.github.io/?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.businessinsider.com/elon-musk-xai-loses-cofounder-tony-wu-2026-2?utm_source=chatgpt.com&quot;&gt;https://www.businessinsider.com/elon-musk-xai-loses-cofounder-tony-wu-2026-2?utm_source=chatgpt.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/VLJ1Pkm0fYtW0iYTN9vg</link><guid isPermaLink="false">https://www.infoq.cn/article/VLJ1Pkm0fYtW0iYTN9vg</guid><pubDate>Tue, 10 Feb 2026 10:54:35 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>星海图首席科学家许华哲创业，刚获内部投资！目标：“让机器人做一道松鼠鳜鱼”？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，有消息称，星海图孵化首席科学家许华哲创业，新公司将会切入具身智能C端应用赛道，已获得星海图种子轮投资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“让机器人做一道松鼠鳜鱼”，是许华哲在多次公开提到的具身智能终极设想。他曾表示，处理活鱼、改刀、油炸到摆盘，其复杂的物理交互是验证机器人智慧程度的最好指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知情人士透露，为了更深入探索具身智能在 2C 领域的技术与应用，许华哲在2025年8月曾主动与星海图团队进行了沟通，表达了希望专注深耕这一方向的想法，星海图团队表示了支持，并对许华哲的新公司启动内部孵化。今年2月，星海图通过直接投资的方式，支持许华哲创立并运营新公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，星海图正通过一系列秘密投资，围绕“数据+应用”构建起一套闭环的生态，许华哲此次创业正是该生态布局在 C 端应用的重要一步。下一步，星海图将计划通过产业基金的方式参股或控股关键技术环节和应用方，整合上下游，为其具身智能产品的量产与商业化提供保障。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为具身智能领域的明星创业公司，星海图于2023年9月成立，连续完成A4 轮及A5 轮战略融资，合计融资金额超过1亿美元。2026年1月，公开信息披露星海图已经完成股份制改造。另据可靠消息透露，星海图已于近期完成新一轮融资，估值已突破100亿人民币。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/EO57dfMthXCaFfjgajlL</link><guid isPermaLink="false">https://www.infoq.cn/article/EO57dfMthXCaFfjgajlL</guid><pubDate>Tue, 10 Feb 2026 10:34:59 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>为什么开发者放弃框架而选择原生 JavaScript</title><description>&lt;p&gt;本文最初发布于博客TheNewStack。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66cba611274058a8f7d62c7cf83eeb9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来自 Unsplash+&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前端开发者正在回归原生 JavaScript。以下是原生 API 和 AI 工具如何使原生 JS 成为框架疲劳的解药。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个人都累了，&lt;a href=&quot;https://thenewstack.io/javascript-framework-reality-check-whats-actually-working/&quot;&gt;框架疲劳不再只是一个梗&lt;/a&gt;&quot;：它是一种集体倦怠。曾经竞相掌握 React、Vue 和 Svelte 的开发者们，现在正悄悄回归他们曾经抛弃的简单性：原生 JavaScript。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Web 的天平正在向极简主义倾斜。原生浏览器 API 的兴起、注重性能的开发理念和 AI 辅助编码的浪潮，不仅让原生 JavaScript 开发再次变得可行，而且重新焕发了生机。这是在经历多年的&lt;a href=&quot;https://thenewstack.io/the-react-component-pyramid-scheme-an-over-engineering-crisis/&quot;&gt;代码膨胀&lt;/a&gt;&quot;、抽象概念和 npm 依赖噩梦之后的&lt;a href=&quot;https://thenewstack.io/stop-blaming-react-for-your-state-management-hangover/&quot;&gt;一剂宿醉解药&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;框架时代的临界点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多年来，框架一直是开发者的默认选择。它们承诺带来规范性、可扩展性和社区支持。但随着框架的发展，其复杂性也随之增加。打包器变得越来越重，构建时间不断增加，运行“Hello World”项目的一行代码平均就需要数兆字节的依赖。开发者开始质疑：所有这些脚手架真的值得吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题不在于框架本身，而在于&lt;a href=&quot;https://thenewstack.io/how-to-build-framework-agnostic-uis-with-web-components/&quot;&gt;围绕它们发展起来的文化&lt;/a&gt;&quot;。每个月都有新的框架涌现，每个都声称修复了上一个框架的问题。企业为了跟上不断变化的生态系统，重构了整个产品。结果呢？无休止的迭代，&lt;a href=&quot;https://www.atlassian.com/agile/software-development/technical-debt&quot;&gt;伪装成创新的技术债务&lt;/a&gt;&quot;，以及陷入重学循环的开发者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到 2025 年，人们意识到：Web 不需要另一层，它需要的是重置，而这个重置以原生 JavaScript 的形式出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;原生 API 已经成熟&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现代浏览器不再是过去那个不稳定的沙箱。在过去的几年中，像 Fetch、&lt;a href=&quot;https://thenewstack.io/web-components-are-the-comeback-nobody-saw-coming&quot;&gt;Web 组件&lt;/a&gt;&quot;和 ES 模块这样的 API 已经发展为成熟的生产级工具，取代了框架曾经提供的功能。曾经那些需要 React 钩子或状态管理库才能完成的任务，现在使用原生解决方案，只要几行简洁的代码就能顺利运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;特别是 Web 组件标准改变了游戏规则。它为开发者提供了框架的模块化和封装性，而又不会有框架锁定的问题。结合 Shadow DOM、自定义元素和模板字面量，开发者现在可以构建可重用、自包含的小部件，它们可以在任何地方运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种成熟度的提升意味着开发者终于可以使用浏览器提供的原生功能来构建动态、可维护的响应式界面。由依赖项、构建工具和样板代码带来的“框架税”不再是强制性的。选择原生 JS 不是因为复古，而是因为它再次变得高效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;性能成为新货币&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今的 Web 讲究速度。&lt;a href=&quot;https://arounda.agency/blog/ux-statistics&quot;&gt;用户期望近乎即时的交互&lt;/a&gt;&quot;，搜索引擎算法会惩罚速度缓慢的页面。严重依赖框架的应用可以做得很复杂，但它们难以提供一致的性能，尤其是在移动设备上。开发者重新认识到，最好的优化不是添加另一个优化库，而是编写更少的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025 年，&lt;a href=&quot;https://thenewstack.io/5-technical-trends-to-help-web-developers-stand-out-in-2025/&quot;&gt;原生 JavaScript 重新进入主流&lt;/a&gt;&quot;，主要是因为应用程序启动更快、渲染更快、调试更容易。没有庞大的捆绑包、水合脚本或协调算法，加载时间大幅下降。每节省一千字节，就能留住一个用户。这种转变是务实的：响应速度提高 50 毫秒的价值远高于 JSX 语法糖或响应式绑定带来的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并非意味着框架的死亡，它们仍然主导着企业环境，但在那些注重敏捷性和性能而非遗留架构和抽象概念的项目中，Web 的天平已经向“无框架区”倾斜。这剂宿醉解药不是关于反叛，而是关于清晰度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 工具使简单再次强大&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;讽刺的是，&lt;a href=&quot;https://thenewstack.io/how-ai-changes-developer-portfolios/&quot;&gt;AI 加速了回归简单的过程&lt;/a&gt;&quot;。现在，开发者使用基于 AI 的编码助手来生成样板代码、调试程序和建议简洁的原生代码。语法越直接，AI 就越有效，而框架的专有约定和抽象层，常常使这些系统感到困惑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 AI 处理那些重复的模式，开发者不再需要框架来提高生产力。一个简单的提示就可以利用原生 JS 直接构建响应式 UI 或实现事件处理，完全避免了框架带来的认知负担。突然之间，“框架节省时间”的旧论点不再成立。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，&lt;a href=&quot;https://devoxsoftware.com/blog/through-the-code-maze-ai-vs-manual-refactoring/&quot;&gt;AI 辅助重构&lt;/a&gt;&quot;使梳理遗留框架变得更容易。团队可以逐步迁移，用原生等价物替换框架组件。这不是对早期 Web 的怀旧，而是在智能工具盛行的时代有意识地回归本源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;微前端和无构建架构的兴起&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;越来越多的现代项目采纳了&lt;a href=&quot;https://thenewstack.io/the-case-for-microfrontends-and-moving-beyond-one-framework/&quot;&gt;微前端&lt;/a&gt;&quot;原则：独立的小型 UI 模块单独加载并通过共享契约通信。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种模块化转变也符合现代容器的安全实践，其中的独立单元在部署和更新时可以施加更严格的控制，最小化攻击面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同样，这种理念与原生 JS 完美契合。没有集中化的构建系统或复杂的依赖树，开发者可以按模块推送更新，并保持各团队的灵活性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无构建运动与此相辅相成。像 &lt;a href=&quot;https://thenewstack.io/how-vite-became-the-backbone-of-modern-frontend-frameworks/&quot;&gt;ESBuild 和 Vite&lt;/a&gt;&quot; 这样的工具已经将编译简化到了几乎看不见的程度，但最终目标是完全不需要构建步骤。原生模块导入使得这一愿景成为现实。开发者可以直接从编辑器将更新推送到生产环境，无需等待管道进行转译或打包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变&lt;a href=&quot;https://thenewstack.io/what-is-lightweight-software-revisiting-the-definition/&quot;&gt;重新定义了“轻量级”的真正含义&lt;/a&gt;&quot;。2026年，现代的原生 JavaScript 项目绝不是原始粗糙的，而是精准如手术刀的。它只恰到好处地完成需要做的事，不多也不少。在一个痴迷于速度和控制的世界里，这不仅仅是优雅，还是竞争优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;学习曲线倦怠和开发者自主性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者们已经筋疲力尽。每隔几个月，就有一个新的框架承诺带来救赎，但结果只是用另一个抽象替换前一个。紧跟“最新”发展所带来的认知负担变得不可持续。原生 JavaScript 提供了一个减压阀，一个不会随着下一个 GitHub 公告而过期的公共基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不需要记住一个新的钩子系统、状态 API 或指令语法。你只需要理解这门语言，重拾自主性，让编程创作的掌控权回到开发者手中。他们可以专注于解决问题，而非死记硬背语法模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着教育系统的跟进，JavaScript 训练营和高校开始重新强调基础知识。其结果将是：&lt;a href=&quot;https://darktechinsights.com/hidden-dangers-of-frameworks/&quot;&gt;依赖框架的开发者减少&lt;/a&gt;&quot;，能够在核心层面推断性能、结构和行为的开发者增多。这种重置既是文化的，也是技术的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;生态系统再平衡&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回归原生 JavaScript 并不意味着框架的灭绝，但它确实重新定义了它们的目的。框架正在演变成可选层，而不是默认配置。它们的存在是为了解决特定的大规模问题，而不是嵌入到每一个登录页和小部件中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React 、Vue 和 Svelte 正在悄悄地精简冗余，提升互操作性。生态系统正在围绕原生标准而不是专有语法凝聚共识。框架作者如今秉持“渐进式采用”的设计理念，这意味着开发者可以选择某个框架而不被锁定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种再平衡也反映了其他技术领域的发展轨迹。正如DevOps逐渐从工具导向转向&lt;a href=&quot;https://thenewstack.io/best-practices-for-adopting-a-devops-culture/&quot;&gt;文化导向&lt;/a&gt;&quot;，2026年的前端开发也将更注重使用效率而非工具选择。原生 JS 并非一种厌弃，而是重新校准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;框架宿醉不是永久的，它是一个警钟。开发者们终于意识到，进步不是关于抽象的堆叠，而是掌握它们下面的基础知识。原生 JavaScript，曾经被认为“太简陋”，现在已经演变成了一个更简洁的 Web 背后的强大引擎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年，用原生 JavaScript 编写代码并不意味着你在倒退，反而意味着你在前进——清晰、可控以及一个五年后仍然有意义的代码库。框架将继续演变，工具将继续增多，但解决方案将保持不变：剥离掉所有不必要的部分，回归到真正支撑 Web 运行的核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://thenewstack.io/why-developers-are-ditching-frameworks-for-vanilla-javascript&quot;&gt;https://thenewstack.io/why-developers-are-ditching-frameworks-for-vanilla-javascript&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UJtGxoHgrizoaUI1HsdG</link><guid isPermaLink="false">https://www.infoq.cn/article/UJtGxoHgrizoaUI1HsdG</guid><pubDate>Tue, 10 Feb 2026 10:29:24 GMT</pubDate><author>Alexander T. Williams</author><category>架构/框架</category></item><item><title>字节发布最新模型 Seedream 5.0，但没打过Nano Banana Pro？</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布时机把握得很好，在所有人都被 Seedance 的视频热度吸引时，字节又推出了全新文生图模型Seedream  5.0。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该版本集成了网络搜索功能，并支持 2K 原生输出，使其成为 Nano Banana Pro 的高性价比替代方案。该模型现已上线 CapCut、剪映和 Skylark平台，并在即梦AI平台开启灰度测试。目前在 CapCut上，有限时20次免费图片生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;官方表示，新版本在理解图像内容、生成速度和视觉效果方面均有显著提升。它能更精准地解读上下文、风格和细节，从而减少重复编辑的需求，在Dreamina 中创建图像更加流畅可靠。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/21/21e169e419fa048792e50534cf64f6f4.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，在生成后，用户可以通过交互式笔刷编辑，对画面元素进行精准、智能的调整；同时，视角控制能力的提升，也让场景扩展与画面构图更加灵活多样，拓展画面空间与表现视角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad8cf2c96b6f38c1aed5d1c30a2804ff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该功能还使 Seedream 5.0 在生成图像时能够利用更加全面、更新及时的信息。通过融合对网络层级内容的理解，AI 生成的画面在内容上更加贴近现实背景和时代语境，尤其适用于热点话题、现代设计以及对场景语境要求较高的视觉创作，最终呈现出更加丰富、贴合需求的视觉效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b2/b2b8bc5083f7b6dccd615a2d7bc85b1f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有用户表示，在 4K 分辨率下，人物皮肤纹理表现有所提升，同一组图像的多样性更好，整体氛围感也很出色。不过，文字渲染效果看起来相比 4.5 版本并没有明显改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/29/2905485ea5b894215aabccd9176b6f5d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有网友评价，图像生成的竞争已经不再只是比拼审美表现。Seedream 5.0 将重点放在检索准确性、4K 级放大能力以及工作流层面的精度控制上。字节跳动押注的是“实用性”而不是“艺术性”，认为真正推动专业用户采用的关键在于效率与可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于能不能取代 Nano Banana Pro，我们让两者同时生成了一份稍微复杂些的北京菜单，Nano Banana Pro 速度上更快，而效果似乎也赢了。（上图中，横版是Nano Banana Pro，竖版是Seedream 5.0，具体表现很直观了）就像网友说的，可能还需要一段时间才能实现取代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/d1/48/d199fcbb586d86139b7b7ef76b6db748.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/ae/4d/ae6c6c0287605ae5f58df3da85903f4d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/xmRTBdskIJ0SNlwgGMYh</link><guid isPermaLink="false">https://www.infoq.cn/article/xmRTBdskIJ0SNlwgGMYh</guid><pubDate>Tue, 10 Feb 2026 10:24:11 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>未来两年软件工程展望：从写代码到管 AI，程序员正分化成两种职业</title><description>&lt;p&gt;本文最初发布于Addy Osmani的个人博客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件行业正处在一个奇怪的转折点上。AI编程已经从增强型的自动补全发展成了能够自主执行开发任务的智能代理。曾经推动科技行业招聘热潮的经济繁荣已经让位于效率至上的要求：企业现在往往更倾向于盈利而非增长，更倾向于经验丰富的员工而非应届毕业生，更倾向于组建配备更好工具的小团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，新一代的开发者带着不同的职业观步入职场：他们注重职业稳定性，对拼搏文化持怀疑态度，并且从入行第一天起就使用AI辅助工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来会发生什么确实还难以预料。以下这五个关键问题可能会决定2026年软件工程的发展，每个问题都对应两种截然不同的情景。这并非真正的预测，而只是一个观察的视角，帮助人们为应对软件工程的未来发展做好准备。我们的目标是基于现有数据，结合本领域特有的健康的怀疑精神，通过制定清晰的路线图来应对即将到来的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;1. 初级开发者问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：随着AI将入门级任务自动化，初级开发者的招聘可能会暴跌，也可能会随着软件渗透到各行各业而强力反弹。两种未来需要不同的生存策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“学习编码，获得初级工作，成长为高级”，这一传统的职业路径正在动摇。&lt;a href=&quot;https://www.finalroundai.com/blog/ai-is-making-it-harder-for-junior-developers-to-get-hired&quot;&gt;哈佛对6200万工人的研究&lt;/a&gt;&quot;发现，当公司采用生成式AI时，初级开发者就业率在六个季度里下降了大约9-10%，而高级开发者的就业率基本保持不变。过去三年，&lt;a href=&quot;https://restofworld.org/2025/engineering-graduates-ai-job-losses/&quot;&gt;大型科技公司招聘的应届毕业生减少了50%&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;正如一位工程师冷嘲热讽地说&lt;/a&gt;&quot;：“花9万美元雇个初级程序员，为什么不用成本更低的AI编程助手？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不仅仅是AI的问题。大约在2022年，&lt;a href=&quot;https://www.2ndorderthinkers.com/p/are-junior-level-jobs-really-killed&quot;&gt;利率上升和大流行后的调整等宏观经济因素&lt;/a&gt;&quot;就已经开始显现，这时AI工具尚未广泛使用。但AI加速了这一趋势。如今，在AI的帮助下，一名高级工程师可以完成过去需要一个小团队来完成的工作。企业正在悄然减少招聘初级员工，其幅度甚至超过了裁员规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的情景：AI解锁了每个行业对开发者的巨大需求，而不仅仅是技术行业。医疗保健、农业、制造业和金融业都开始嵌入软件和自动化技术。AI不是取代开发者，而是成为一个力量倍增器，将开发工作扩展到从未雇佣过编码人员的领域。我们将看到更多不同的入门级角色：为特定细分市场快速构建自动化和集成的“AI原生”开发者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;美国劳工统计局&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;预测&lt;/a&gt;&quot;，从2024年到2034年软件工作仍然将增长约15%。若企业利用AI扩大产出而非单纯裁员，就需要人类把握AI创造的机遇。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;悲观情景的长期风险经常被忽视：今天的初级开发者是明天的高级工程师和技术领导者。如果完全切断人才管道，那么在5-10年内就将出现一个领导力真空。行业老兵称这为“缓衰”：一个停止培训接班人的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：使自己精通AI并成为多面手，证明一名初级开发者加上AI可以匹配一个小型团队的产出。使用AI编码代理（Cursor/Antigravity/Claude Code/Gemini CLI）构建比较大的功能，但要能理解并解释大部分代码行。聚焦不容易被AI替代的技能：沟通、问题分解、领域知识。将相邻角色（QA、DevRel、数据分析）视为切入点。构建一个项目集，特别是集成AI API的项目。考虑参与学徒计划、实习、外包或开源项目。不要成为“只是又一个需要培训的新毕业生”，而是成为一个学习速度快、立即就能发挥作用的工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：初级开发者减少意味着你的日常工作增加。利用自动化工具来完成例行任务，不要什么事都自己做。利用CI/CD、linter和AI辅助测试来捕捉基本问题。通过开源项目或指导其他部门同事开展非正式的导师工作。向管理层如实说明全由资深员工组成的团队所面临的风险。若初级人才需求回升，需做好高效接纳新人的准备，并运用AI进行任务分配。你的价值在于提升整个团队的产出，而非个人的代码产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;2. 技能问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：随着AI编写大部分代码，核心编程技能可能会退化，或者因为人类开发者需要监督AI而使这些技能变得比以往任何时候都更加关键。未来几年将决定我们是否会为追求速度而牺牲对代码的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.blog/2025/09/10/ai-vs-gen-z/&quot;&gt;现在有84%的开发者定期使用AI辅助工具&lt;/a&gt;&quot;。对许多人来说，面对错误或新功能需求的第一反应不是从头开始编写代码，而是编写提示并组合AI生成的代码片段。初级程序员正在跳过“艰难的入门阶段”：他们可能永远不会从头开始构建二叉搜索树或独立调试内存泄漏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者的技能集正在从实现算法转变为知道如何向AI提出正确的问题并验证其输出。现在，&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;入门的第一个要求是提示和验证AI的输出&lt;/a&gt;&quot;，而不是展示原始编码能力。一些高级工程师担心，这会产生一代不能独立编码的人，导致开发者技能退化。AI生成的代码可能会引入一些微妙的错误和安全漏洞，不太有经验的开发者可能会漏掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的情景：AI处理80%的常规工作，人类专注于最难的20%。架构设计、复杂集成、创意设计、边缘情况，这些问题是机器无法单独解决的。AI的普及并没有使深厚的知识积累过时，反而使人类专业知识变得比以往任何时候都更重要。这就是“高杠杆工程师”，他们将AI作为一种力量倍增器，但必须深入理解系统才能有效使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果每个人都有AI编码代理访问权限，那么区分优秀开发者的关键在于知道AI何时出错或不够优化。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;正如一位高级工程师所说&lt;/a&gt;&quot;：“最好的软件工程师不是最快的编码者，而是那些知道何时不信任AI的人。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程转变：需要输入的样板代码减少，把更多的精力用在审查AI输出的逻辑错误、安全漏洞和与需求不匹配的问题。关键技能变成了软件架构、系统设计、性能调优和安全分析。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;AI可以快速生成一个Web应用程序，但专家工程师需要确保AI遵循了安全最佳实践，并且没有引入竞态条件。&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，开发者中间出现了分歧。一些人坦言，他们几乎不“亲手”编写代码，并认为编码面试应该做出改变。其他人则认为，跳过基础知识面试会导致AI输出出现问题时需要完成的应急处理工作增加。&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;行业开始期望工程师同时具备&lt;/a&gt;&quot;AI的效率和保障质量的基本知识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：将AI当作学习工具，而不是拐杖。对于AI编码代理（Cursor/Antigravity/Claude Code/Gemini CLI）的建议，要通过审查代码了解其工作原理并识别薄弱环节。偶尔禁用你的AI助手，从头开始编写关键算法。优先考虑计算机科学基础：数据结构、算法、复杂性、内存管理。将项目实现两次，一次用AI，一次不用AI，然后对两者进行比较。学习提示工程，并掌握相关工具。通过严格的测试训练自己：编写单元测试，自己阅读堆栈跟踪信息而不是立即询问AI，熟练使用调试工具。深化AI无法复制的互补技能：系统设计、用户体验直觉、并发推理。证明你既能用AI快速解决问题，也能在AI失败时自己处理棘手的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：将自己定位为质量和复杂性的守护者。磨练你的核心专长：架构、安全、扩展、领域知识。练习用AI组件进行系统建模并思考故障模式。随时关注AI生成代码中的漏洞。拥抱你作为导师和审查者的角色：定义什么时候可以使用AI，以及什么时候必须手动审查（支付或安全代码）。侧重于创造性和战略性工作；让初级开发者和AI一起处理常规API连接，而你决定构建哪些API。投资软技能和跨领域知识。随时关注新工具和最佳实践。加倍重视人类开发者不可或缺的因素：准确的判断、系统性思维和导师带徒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;3. 角色问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：开发者的角色职责可能缩减为有限的审计（监督AI生成的代码）工作，也可能扩展为设计和管理AI驱动系统的关键协调者。无论哪种情况，创造价值都远不止于编写代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此处的两极分化非常明显。在前一种情景中，开发者的创造性职责被削弱。他们不再专注于构建软件，而是更多地审核和监管AI产出。AI系统（或使用无代码平台的“公民开发者”）负责生产环节；人类开发者则审查自动生成的代码，检查错误、偏见或安全问题，并审批部署。创造者沦为检查者。编写代码的喜悦被风险管理的焦虑所取代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有报道称，工程师将花更多时间评估AI生成的拉取请求和管理自动化管道，而不是从头开始编写代码。编程感觉更像是合规性检查，而不是创造性地解决问题。正如一位工程师感叹：“我不想沦为一个代码清洁工，整天收拾AI留下的烂摊子。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一种未来则有趣得多：开发者演变成高级协调者，兼具技术、战略和道德责任。AI“工人”意味着人类开发者承担架构师或总承包商的角色，负责设计整个系统，决定哪些任务分配给哪些AI或软件组件，并将活动部件组合成解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html&quot;&gt;有一家低代码平台的首席执行官阐述了这个情景&lt;/a&gt;&quot;：在“智能代理”开发环境中，工程师将转型为“作曲家”，指挥由AI代理和软件服务组成的“乐团”。他们无需亲自谱写每个音符，但会定义旋律，即架构、接口以及代理间的交互方式。这个角色兼具跨学科性和创造性：既是软件工程师，又是系统架构师，同时也是产品战略家。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;乐观看法：随着AI承担起一些重复性工作，开发者的角色必然转向更高价值的活动。工作可能变得更加有趣。必须有人决定AI应该构建什么，验证产品是否合理，并持续改进它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向哪个方向发展取决于组织选择如何整合AI。将AI视为劳动力替代工具的公司可能会缩减开发团队，并要求剩下的工程师保持相关任务自动化运行。将AI视为团队能力增强工具的公司可能会保持人员数量基本不变，但让每位工程师承担更费时耗力的项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：不要局限于编写代码，要寻找其他机会。自愿参与测试用例编写、CI流水线设置或应用监控，培养与审计员/监管人角色相一致的技能。通过个人项目保持你的创造性编码能力，以免失去构建乐趣。培养系统思维：学习组件之间如何通信，怎样设计出良好的API。阅读工程博客和系统设计案例研究。熟悉除代码生成之外的AI和自动化工具：编排框架、AI API。提升书面与口头沟通能力。撰写文档时秉持向他人阐述的标准。向资深同事提问时，不仅要问“代码是否运行正常？”更要问“我的考量是否到位？”。准备好成为验证者、设计者和沟通者，而非仅是编码者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：把更多精力放在领导和架构责任上。打造供AI和初级团队成员遵循的标准和框架。定义代码质量检查清单和符合伦理的AI使用策略。随时关注与AI生成软件合规性和安全性相关的话题。专注于系统设计和集成知识；自愿绘制服务间的数据流并识别故障点。熟悉编排平台（Kubernetes、Airflow、无服务器框架、代理编排工具）。投入双倍精力履行技术导师角色：更多地参与代码审查、设计讨论、技术指导。提升快速评估他人代码并给出高层次反馈的能力。培养产品和商业意识；了解为什么构建一个功能以及客户关心什么。向产品经理学习或参加客户反馈会议。通过原型、黑客马拉松或新兴技术研究来保持你的创造激情。从编码者演变为指挥者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;4. 专家与通才问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：专业领域过于狭窄的专家会面临自身领域被自动化取代或逐渐淘汰的风险。在快速变化、AI深度渗透的时代背景下，T型工程师更受青睐——他们既具备广泛的适应能力，又拥有一个或两个有深厚知识积累的专业技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考虑到模型、工具和框架的快速兴衰，将职业生涯押注在单一技术栈上是有风险的。当新型AI工具能以极少需要人工干预的方式处理传统框架时，该领域的专家可能会突然发现自身需求锐减。那些专注于“单一技术栈、框架或产品领域”的开发者，某天醒来时或许会发现，该领域已日渐式微甚至被淘汰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想想那些在行业转型时未能及时转型的人：COBOL开发者、Flash开发者或移动游戏引擎专家。如今不同的是变革速度。AI自动化能让某些编程任务变得微不足道，削弱了因这些任务而存在的工作岗位。只精通单一技能的专家（比如调整SQL查询参数、将Photoshop设计切片为HTML代码）可能会发现，90%的工作已被AI取代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;招聘经理们总在追逐最新的小众领域。几年前人人都想要云基础设施专家；如今AI/ML工程师需求激增。那些精通昨日技术的人，随着该领域的发展放缓，会感到职业发展陷入了停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反的结果是形成一种新的专业化形式，即“多面手专家”或&lt;a href=&quot;https://www.youtube.com/watch?v=IMHneaMO-dg&quot;&gt;T型开发者&lt;/a&gt;&quot;。他们在一两个领域拥有深厚的造诣（竖线），同时又广泛涉猎其他众多的领域（横线）。&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;他们成了跨学科团队的“粘合剂”&lt;/a&gt;&quot;，既能与各领域专家沟通协作，又能在必要时填补技术空白。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;企业不再需要知识深度或广度不够的开发人员&lt;/a&gt;&quot;；他们想要一个强大的核心竞争力，以及能够跨栈工作的能力。其中一部分原因是效率考量：一个T型工程师通常可以独立解决端到端问题，无需等待上下游交接。其中一部分原因是创新考量：知识的交叉传播可以带来更好的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;实际上，AI工具增强了通才的能力，使一个人更容易处理多个组件。后端工程师可以在AI的帮助下构建出合理的UI；前端专家可以借助AI生成服务器样板代码。一个提供丰富AI功能的环境让人们能够完成更广泛的工作。与此同时，深度专家可能会发现，他们的专业领域有一部分被自动化取代，却难以开拓新领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://medium.com/nerd-for-tech/beyond-full-stack-the-rise-of-the-t-shaped-developer-a4afd757d976&quot;&gt;现在近45%的工程角色期望能够精通多个领域的知识&lt;/a&gt;&quot;：编程加云基础设施知识，或是前端开发加熟悉ML。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;我们该如何做？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初级开发者：尽早打下广泛的基础。即使被雇佣为特定的角色，也要了解那个岗位之外的知识。如果你是在做移动开发，不妨学习下后端基础知识；如果你是在做前端开发，则可以尝试编写一个简单的服务器。学习部署过程和工具，如Docker或GitHub Actions。找一两个真正让你感到兴奋的领域深入学习，使它们成为你垂直领域的专业知识。将自己定位成混合型人才：“全栈开发人员，专注于云安全”或“前端开发人员，具有UX专业知识”。借助AI工具快速学习新领域的知识；如果你是后端新手，可以让ChatGPT生成入门API代码并学习它。养成不断学习新技能的习惯。参加黑客马拉松或跨职能项目，强迫自己进入通才模式。告诉你的经理，你想要接触项目的不同部分。适应性是职业生涯早期的超能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者：绘制你的技能图谱：你在哪些领域是专家，哪些相关领域你只是浅尝辄止？选择一到两个相邻领域并努力精通。如果你是一个后端数据库专家，不妨熟悉一个现代前端框架或学习机器学习（ML）流水线的基础知识。借助AI的帮助，在你的弱项领域做一个小项目。将你深厚的专业知识与新环境相结合；如果你专门从事Web应用性能优化，可以探索如何将这些技能应用于ML推理优化。支持或争取将你的角色设计成跨职能的，自荐成为涉及多领域项目的“集成负责人”。指导他人，传播技能，同时也从中学习新东西。更新简历体现多元化能力。利用你的经验识别模式和可转移知识。成为T型人才的典范：在你的专业领域深耕（建立权威和信心），并积极拓展横向能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;5. 教育问题&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要点：计算机科学（CS）学位是保持黄金标准，还是被更快的学习路径（训练营、在线平台、雇主培训）所取代？大学可能难以跟上每几个月就有重大变化的行业发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;四年制计算机科学学位一直是进入软件领域的主要途径。但这一传统正在受到质疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一种未来：大学仍然重要，但难以保持相关性。学位仍然是默认的资格凭证，但受制于缓慢的课程更新周期和官僚审批流程，课程设置落后于快速发展变化的需求。学生和雇主均感觉学术界与行业脱节，学校教授的理论或过时的做法无法转化为工作技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最近的毕业生报告指出，他们在攻读学位期间从未学习过云计算、现代DevOps或AI工具。如果大学需要投入很多的时间和资金，但却只能提供低相关性教育，那么它们就有被视为昂贵守门人的风险。但出于惯性，许多公司仍然要求应聘者具备学士学位，因此压力就转到了应聘者身上，他们需要通过训练营、在线课程和自学项目来弥补这方面的不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://campustechnology.com/articles/2025/10/21/solving-the-talent-crisis-starts-in-higher-ed.aspx&quot;&gt;学生贷款是一笔巨大的债务，而公司也要花费数十亿美元培训新毕业生&lt;/a&gt;&quot;，因为他们缺乏工作场所需要的技能。大学可能会在这里增加一门AI伦理课程，在那里增加一门云计算选修课，但当他们真正实施时，行业工具已经又向前发展了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;颠覆性场景：传统教育日益为新教育体系所取代。编码训练营、在线认证、自学作品集、雇主创建的培训学院层出不穷。许多知名雇主（谷歌、IBM）已经取消了某些技术角色的学位要求。到2024年，&lt;a href=&quot;https://campustechnology.com/articles/2025/10/21/solving-the-talent-crisis-starts-in-higher-ed.aspx&quot;&gt;近45%的公司计划至少取消部分职位的学士学位要求&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;训练营体系已经相当成熟，他们培养的毕业生与CS毕业生一起被顶级公司雇佣。这些项目周期更短（12周强化），并且专注于教授实用技能：当前流行的框架、云服务、团队合作。招聘标准正在瞄准在线作品集、微证书和已认证技能。出色的GitHub作品集或公认的认证可以免除学位要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由雇主推动的教育正在兴起：企业自主搭建培训体系或与编程训练营合作。部分科技巨头已经为非传统背景的人才设立了内部“大学”。AI本身也开辟了全新的学习路径：AI导师、交互式编程沙盒、校外个性化教学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;模块化的学习生态远比昂贵的四年制学位更容易获取。在计算机科学专业实力薄弱的国家，孩子们也能修读Coursera的课程，构建与硅谷人士同样的个人作品集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我们该如何做？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有志向的开发者/初级开发者：在学习传统的计算机科学课程时，不要完全依赖课程进行学习。要通过实际的项目补充课程内容：构建Web应用，参与开源项目。寻找实习或合作机会。如果你的课程中没有包含热门话题，则通过在线平台学习它们。考取行业认可的认证（GCP、亚马逊云科技、Azure）以证明自己的实践能力。如果是你在自学或参加了训练营，则一定要专注于创建一个引人注目的作品集：至少要有一个文档良好的重点项目。积极参与开发者社区：参与开源项目，撰写技术文章。通过LinkedIn、聚会以及开发活动建立人际关系网络。争取资深开发者为你背书。考虑到技术技能的半衰期非常短，务必要不断学习。将AI作为个人导师。用具体的方式证明自己的能力：作品集、认证证书以及能清晰阐述工作成果的能力，这些将为你打开机遇之门。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高级开发者和领导者：你不能永远依赖于证书。要在继续教育方面进行投资：在线课程、研讨会、会议、认证。通过新的方式验证你的技能，为通过实际问题评估应聘者当前能力的面试做好准备。维护使用了新技术的业余项目。重新评估工作要求：你真的需要新员工拥有计算机科学学位，还是需要他们具备某些技能和学习能力？推动以技能为先的招聘，扩大你的人才库。支持内部培训计划或学徒制岗位。为没有正式大学背景的初级开发者建立导师制小组。与学术界及其他机构合作：加入顾问委员会、举办客座讲座、对课程存在的问题提出反馈。将这种合作融入自身的职业发展中：实际的成果和持续的学习比额外的学位更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;小结&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些情景并不是相互排斥的。现实将融合所有要素。一些企业将缩减初级岗位的招聘，另一些则会在新的领域扩大招聘规模。AI会将常规编码工作自动化，同时又提升人类编写的代码的质量标准。开发者或许会在上午审核AI生成的代码，下午则专注于设计高级架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个贯穿始终的主题是：变化是唯一的常数。紧盯技术趋势（并保持审慎态度），避免被炒作或末日论所蒙蔽。通过更新技能、拓展能力、聚焦人类特有的优势（创造力、批判性思维、协作能力），你才能始终保持竞争力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;无论未来是迎来编程复兴，还是进入自动编码时代，那些具备全局思维、持续学习能力并能推动技术发展解决实际问题的工程师，始终会受到市场的青睐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预测未来的最佳方式就是积极地塑造它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://addyosmani.com/blog/next-two-years/&quot;&gt;https://addyosmani.com/blog/next-two-years/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UtPXQMUagxqNoPE2PaT0</link><guid isPermaLink="false">https://www.infoq.cn/article/UtPXQMUagxqNoPE2PaT0</guid><pubDate>Tue, 10 Feb 2026 10:08:31 GMT</pubDate><author>Addy Osmani</author><category>生成式 AI</category></item><item><title>达摩院开源RynnBrain：首个支持移动操作的具身大脑基础模型</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;具身智能蓬勃发展的当下，具有泛化性的具身能力至关重要。为了追求这个终极目标，业界发展出了两条技术路线。一条路线从机器人末端动作输出入手，发展出可以直接操作物理世界的VLA模型。但是VLA模型由于其数据稀缺性无法实现泛化。因此有了第二条路线，从本身拥有泛化能力的VLM入手，加速VLM从数字世界迈向物理世界。我们将在此路线上探索的模型称之为具身基础模型。诚然，已经有一些研究开始了对具身基础模型的初步探索。例如，RoboBrain系列模型在单个视觉语言模型中统一了理解、定位和规划，以促进复杂的具身任务。Robix模型为任务执行期间更自然的人机交互做出了贡献。 然而，这些当前的具身基础模型动态认知受限，且普遍存在物理幻觉，难以适应人形机器人上的复杂任务。主页：&lt;a href=&quot;https://alibaba-damo-academy.github.io/RynnBrain.github.io/&quot;&gt;https://alibaba-damo-academy.github.io/RynnBrain.github.io/&lt;/a&gt;&quot;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们提出了RynnBrain，首个可移动操作的具身基础模型。其具有以下三个关键要点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）时空记忆：RynnBrain能够在其完整的历史记忆中定位物体、目标区域，甚至预测运动轨迹，从而赋予机器人全局时空回溯能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）物理空间推理：不同于传统的纯文本推理范式，RynnBrain 采用文本与空间定位交错进行的推理策略，确保其推理过程紧密扎根于物理环境。大大减弱了具身任务中的幻觉问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（3）良好的可拓展性：我们在RynnBrain基础模型上微调了视觉语言导航和精准操作规划模型，效果轻松实现SOTA。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过完备的实验，RynnBrain在16项具身任务Benchmark上全面超越了Cosmos Reason 2和Gemini Robotics ER 1.5等强大模型实现了SOTA，并且在8项域外Benchmark上验证了超越其他具身基础模型的通用泛化性。特别的，我们开源了业界首个MOE具身基础模型RynnBrain-30B-A3B，其只需要3B的推理激活参数就全面超越了当前规模最大的具身基础模型Palican-VL-72B。使用我们的MOE模型可以让机器人在保持最强大感知和规划能力的基础上拥有更加快速的动作响应和更加丝滑的行为模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为推动领域发展，我们同步开源：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;✅ 全系列模型（含全尺寸基础模型与后训练专有模型）&lt;/p&gt;&lt;p&gt;✅ 全新评测基准RynnBrain-Bench（评测时空细粒度具身任务）&lt;/p&gt;&lt;p&gt;✅ 完整的推理与训练代码&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain首次实现了“大脑”对物理世界的深度理解与可靠规划，为大小脑分层架构下的通用具身智能迈出关键一步。我们期待它加速 AI 从数字世界走向真实物理场景的落地进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/81/8115b8a12c872c0203164ba95e3092b9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;RynnBrain模型体系架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72cc6dca6545915445809d2b0531a752.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）模型结构&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain在Qwen3-VL基础上进行训练。 使用自研的RynnScale架构对Dense模型和MOE模型均进行了训练速度的优化，使得在同等资源下训练加速两倍。在输入端RynnBrain可以接受任意分辨率的图片、多图和视频输入，满足用户任意形式的视觉输入的需求。同时RynnBrain可以输出区域、轨迹、点集、夹爪位姿、文本等多种具身相关模态，从而支持多样化具身任务的执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）训练优化&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain是一款面向高泛化的具身基础模型，使用视频、图像和文本等多模态数据进行训练，覆盖从定位、空间感知等短任务到长篇多模态描述与复杂推理等多种场景。由于样本序列长度差异大且呈长尾分布，直接在数据并行训练中平均分配样本会引发“拖尾效应”，影响整体吞吐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们引入在线负载均衡：训练时根据图像大小与文本token数预估序列长度，将同一DP组内样本统一重分配，使每个worker的累计序列长度尽量均衡，并用优先分配长序列的贪心策略在数据预取阶段快速完成，避免训练卡顿且无需额外数据预处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，由于重分配会造成各worker样本数不均，我们采用按样本的损失归约方式，保证训练前后损失一致性与收敛稳定，并显著提升训练效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在工程实现上，我们结合ZeRO、梯度检查点、输出token过滤等技术降低显存占用；在更大规模模型中引入ZeRO-2与专家并行（EP），并通过优化MoE 计算与跨卡分发提升吞吐。训练与推理框架基HuggingFace Transformers，并已开源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;根植于物理世界的时空预训练&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要制造出一种能够与周围环境进行自然互动的通用型机器人，需要具备两项基本能力：一、时空记忆：通过历史视觉记忆，机器人必须建立涵盖空间、位置、事件、轨迹等多维度的表征，从而能够适应复杂多变的环境。二、忠实于物理世界：所有机器人的认知过程都必须从根本上扎根于物理世界的客观现实之中。本章主要介绍了RynnBrain的预训练，该方法正是基于上述两点见解而制定的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）训练策略&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为赋予RynnBrain以上所述的时空记忆与物理世界落地能力，我们设计了一个统一的预训练框架，将多模态输入整合到共享的语义空间中。我们的训练方案聚焦于两大核心支柱：统一的输入输出表示，以及物理感知的优化策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;统一的时空表示&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为培养时空记忆，我们将图像与视频视为统一的输入模态。这样，RynnBrain能够在视频序列中学习时间因果关系与轨迹动态，这对于理解运动与事件至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根植于物理世界的输出空间&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为实现物理世界，我们对输出空间进行严格形式化，以连接高层认知与低层执行。不同于标准视觉语言模型将数字作为自由文本处理，我们引入离散的坐标token来表示物理位置。我们将所有空间坐标归一化到固定区间，并用整数token表示。这种量化将连续的物理控制转化为离散的分类问题，使模型能够使用与语言生成相同的自回归机制输出精确位置（例如抓取点或导航目标）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）数据准备&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们为RynnBrain的预训练准备了两千万的数据对，具体数据细节如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a2/a24b1419cde4aad6819ee06cec20515c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用多模态训练数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们复用了团队自研的Video-Llama 3视频大模型的训练数据，并融合了LLaVA-OV-SI、LLaVA-Video等多个开源视频问答数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具身认知数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;物体认知、空间认知和计数相关数据复用了团队自研的RynnEC模型训练数据，并且引入了Sensenova-SI、VSI-590k、Molmo2等提高模型的空间理解和动态计数能力。此外，我们自己生成了100万对自我为中心的OCR问答数据，其中即有直接的OCR问题，也有需要识别视频中多个文字才能回答的情景问题。我们还收集了EgoRe-5M、Egotaskqa和RoboVQA等自我为中心的多样化问答数据以增强RynnBrain的自我为中心任务理解能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具身定位数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain拥有5项具身定位能力，分别为：物体定位、区域定位、操作点定位、轨迹定位和夹爪位姿定位。我们为每项定位任务标注了大量额视频以及图像数据，使得RynnBrain在室内的定位能力上拥有突出的泛化性。我们还用ADE20K、Grasp-Anything、PACO-LVIS等开源数据平衡整体数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划数据&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划任务包含导航和操作两类。导航使用了R2R和RxR数据和ScaleVLN的开源数据。并且将数据格式变成了流式的格式。操作规划数据源来自OpenX-Embodiment和AGIBot。首先，我们将这两个数据集中所有的规划数据都整合成时间段和子任务标注一对一匹配的格式。然后我们让人工标注出每个子任务规划中跟物体、区域和操作相关的名字。例如：“拿起香蕉放到桌子的左下角”，在这句话中与物体相关的词语是“香蕉”，与区域相关的词语是“桌子的左下角”，与操作相关的词语是“拿起”。然后人工再将这些词语和图像中的位置信息做对应，操作词语与图像中的操作点对应，物体词语与图像中物体的检测框对应，区域词语与图像中的区域点对应。最终得到文本和定位信息穿插的子任务标注数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;基于RynnBrain的后训练-让具身拓展无限可能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）物理空间推理模型&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，大多数多模态推理模型采用纯文本推理范式。虽然一些方法通过工具使用（例如放大）来缓解视觉识别中的挑战，但这种推理范式存在泛化能力有限的问题，只能解决一小部分问题。此外，探索在推理过程中进行视觉想象的替代方法通常会受到生成图像中严重幻觉的困扰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于具身大脑在现实世界中运行，进行物理空间推理的能力变得至关重要。因此，在RynnBrain中，我们提出了一种交错推理方法，该方法将实体化与文本信息直接结合在以自我为中心的视频流中。这种范式有效地弥合了语言与物理世界之间的认知鸿沟，确保推理过程牢固地扎根于现实之中。下面详细介绍了RynnBrain在物理空间推理领域的贡献和探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们设计了5类空间推理任务——计数、物体定位、操作点定位、区域定位和轨迹预测，来验证RynnBrain新提出的“文本-空间交织”推理范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们采用组相对策略优化（GRPO）来使模型与物理空间推理任务对齐。不同于标准PPO需要价值函数来估计优势项，GRPO通过对同一提示下生成的多个采样输出的组内得分来估计基线。这显著降低了显存占用与训练复杂度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练从我们的冷启动模型初始化。我们使用SGLang推理引擎以高效生成rollout，组大小设为5。训练共进行10个epoch，batch size为128。我们采用余弦学习率调度进行策略优化，并进行3% 的warmup。为保证稳定性，我们将截断范围设为[0.2, 0.28]，KL系数0.02。最大序列长度设为16,384个token，以适配长上下文的第一视角视频推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据构建采用“AI生成+人工精标”策略：&lt;/p&gt;&lt;p&gt;从自采第一人称视频中抽取样本；多模态大模型生成初步推理链，并用方括号标记关键实体（如“[白色花图案的墙纸]”）；由大语言模型初步分类实体为“物体”或“区域”；人工标注员最终审核并精标：对“对象”标注边界框，对“区域”标注代表性点集，并选择最清晰的视频帧作为参考帧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所有定位结果以结构化格式&lt;object area=&quot;&quot;&gt;: ...; (coordinates)&lt;!--...--&gt; 融入推理文本，实现语言与空间的对齐。&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，计数任务特别强调“先定位再计数”，共构建 7万条高质量样本，显著提升模型在复杂场景下的时空感知能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）视觉语言导航&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航任务采用与当前SOTA模型StreamVLN相同的数据设置。首先使用r2r rxr EnvDrop ScaleVLN数据在RynnBrain基础模型上做第一阶段训练。然后利用这个第一阶段模型在r2r rxr EnvDrop环境中采集Dagger数据。具体而言，使用第一阶段模型在r2r rxr EnvDrop的模拟器环境中进行导航，如果发现导航路径偏离了正确路径，则使用最短路径算法得到一个从当前位置到目标点的最短路径。因此，Dagger得到的导航数据可以有效纠正第一阶段模型的导航错误。使用Dagger数据我们可以进行第二阶段的训练得到最终的RynnBrain-Nav导航模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（3）操作规划任务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于预训练语料库包含了以规划为中心的数据，基础模型本身就具备了固有的规划能力。然而，要将这种能力应用于复杂的、长周期的操作任务，模型需要保持有效的记忆。为此，我们利用了一个小型的自采集数据集，其格式为多轮对话，其中交互历史充当了明确的记忆缓冲区，以保存历史推理结果。这种结构使模型能够将单个规划步骤整合成一个连贯的长周期策略。至关重要的是，为了与这种顺序推理相匹配，我们仅在每个对话轮的最后一步应用grounding标注，确保当前决策既取决于即时观察，也取决于累积的记忆。通过实验证明，这种方法具有很高的数据效率：仅使用几百个样本进行微调就足以使模型具备强大的长周期规划能力和泛化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;RynnBrain亮眼的实战成绩单&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（1）基础模型能力全面&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/01/01861b2d0e2ba3364444472e02284db8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于当前开源Benchmark在具身时空细粒度任务上的缺失。我们推出了RynnBrain这一多维度基准测试工具，用于评估时空细粒度具身能力。 该测试涵盖了四个关键维度：物体认知、空间认知、物体定位以及具身点预测，旨在突出对记忆视频序列中细粒度的理解以及时空的定位能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/170c7343c0d92adf321708a2d23a4ed3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RynnBrain测评了20项具身相关的认知与定位Benchmark。在这些具身能力上，RynnBrain全面领先Mimo-Embodied等最先进的具身大脑模型，在许多能力上甚至有30%以上的涨幅。在具身领域之外的通用视觉理解方面，RynnBrain很好的保持了Qwen3-VL的强大通用视觉能力，甚至在AI2D、DocVQA等Benchmark上超越了Qwen3-VL。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（2）后训练潜力巨大&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航后训练&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47c6da2f970f0a97aa97efa17a2c23e9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们使用当前的导航SOTA模型StreamVLN的训练数据微调RynnBrain模型。在没有进行任何架构改进的情况下RynnBrain-Nav比StreamVLN的导航成功率提高了2%-3%。我们在Qwen3-VL基础模型上利用相同的数据训练后发现，RynnBrain作为基础模型可以让微调出的导航模型能力提升5%。这充分证明了在具身相关任务中，RynnBrain的预训练作用巨大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;操作规划后训练&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/2495e2769ddd687fa96dd0dc6473a40f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划任务需要拥有强大的预测能力和场景解析力。只使用几百条数据微调之后RynnBrain-Plan-30B(A3B)即可在域内和域外的任务上全面超越Gemini&amp;nbsp;3 Pro。这充分体现了文本与定位交错的规划方式更加适用于多变复杂的物理世界。&lt;/p&gt;&lt;/object&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/rA7dxxttFjqurKCzfGmI</link><guid isPermaLink="false">https://www.infoq.cn/article/rA7dxxttFjqurKCzfGmI</guid><pubDate>Tue, 10 Feb 2026 09:53:06 GMT</pubDate><author>达摩院</author><category>AI&amp;大模型</category><category>开源</category></item><item><title>WASI 1.0：WebAssembly可能在2026年悄然普及</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;本文介绍了WebAssembly及其关键标准WASI 1.0在2026年的普及前景。随着WASI 0.3.0的发布，WebAssembly将在更多场景（如边缘设备、无服务器环境等）替代传统容器。WebAssembly已经走出浏览器，凭借组件模型、接口类型等新规范，降低了开发门槛，提升了互操作性和安全性。WASI的标准化进程虽漫长，但每一步都推动了WebAssembly的广泛应用，未来将实现高性能、可组合并发和零拷贝流式处理等关键特性，进一步加速WebAssembly的落地和普及。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文最初发表于&lt;a href=&quot;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-2026/&quot;&gt;The New Stack网站&lt;/a&gt;&quot;，由InfoQ中文站翻译分享。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/webassembly/&quot;&gt;WebAssembly&lt;/a&gt;&quot;在Wasm 3.0和组件模型（Component Model）发布后取得了巨大进展。然而，通往WebAssembly真正成熟落地的“最后一公里”，预计将随着&lt;a href=&quot;https://wasi.dev/roadmap&quot;&gt;WASI 0.3.0&lt;/a&gt;&quot;在2026年（很可能在2月份）的正式发布而完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一标准化工作的最终阶段将使WebAssembly能够在越来越多的场景中替代传统&lt;a href=&quot;https://thenewstack.io/introduction-to-containers/&quot;&gt;容器&lt;/a&gt;&quot;，因为无论是否运行在&lt;a href=&quot;https://thenewstack.io/kubernetes/&quot;&gt;Kubernetes&lt;/a&gt;&quot;中，容器本身并不适合某些应用场景。这些场景包括：边缘设备、异步与事件驱动架构、无服务器（serverless）环境，以及需要通过单次发布同时部署到大量（甚至无限数量）终端节点的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;远超浏览器环境的WebAssembly&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，WebAssembly早已走出浏览器。在2025年&lt;a href=&quot;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/&quot;&gt;KubeCon + CloudNativeCon&lt;/a&gt;&quot;北美大会期间，微软Azure Core Upstream的首席产品经理&lt;a href=&quot;https://github.com/squillace&quot;&gt;Ralph Squillace&lt;/a&gt;&quot;在&lt;a href=&quot;https://cncf.io/?utm_content=inline+mention&quot;&gt;CNCF&lt;/a&gt;&quot;主办的&lt;a href=&quot;https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/wasmcon/&quot;&gt;WasmCon&lt;/a&gt;&quot;活动闭幕致辞中表示：“WebAssembly几乎能够在所有环境可靠地运行于生产系统中，包括浏览器、服务器、CDN和后端服务，这充分证明了其成熟度和广泛适用性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace指出，尽管WebAssembly核心有意设计为层级较低且难以直接使用，但近期的规范化工作已支持更高层次的抽象。引用类型（Reference Types）和 接口类型（Interface Types）使得组件能够暴露有意义的API，而开发者无需深入理解WASM内部的机制，从而大幅降低了使用门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于那些特别关注组件的人来说，Squillace表示，&lt;a href=&quot;https://thenewstack.io/webassembly-to-let-developers-combine-languages/&quot;&gt;Bytecode Alliance&lt;/a&gt;&quot;对工程师免费开放。该联盟的重点在于支持工程师和开源开发，而非营销，并提供了包括文档在内的各种资源，使开发者能够从零开始使用WebAssembly组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace还指出，这些选择并非相互排斥的。WebAssembly及其组件模型的目的并非取代编程语言、模块或容器，而是致力于实现互操作性、安全性，并拓展软件在不同语言和环境之间所能实现的功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;WebAssembly并不是完美无缺的，但Squillace表示，这并非重点。真正重要的是它所赋能的能力。这是一个由自愿参与者共同构建的激动人心的领域，正因如此，他说道，这次“结束”实际上是一次“开启”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;核心规范&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管WebAssembly核心有意设计为层级较低且难以直接使用，但近期的规范工作已支持更高层次的抽象。Squillace指出，引用类型（reference types）和接口类型（interface types）使得组件能够暴露有意义的API，而开发者无需深入了解WebAssembly的内部机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Squillace表示，“在核心层面开展的规范工作……正是让组件模型能够传递复杂的结构、从而形成合理API的关键所在”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，基于Wasm的解决方案尚不能作为容器的即插即用替代方案，但它已经在越来越多的场景中得到了应用，这些场景充分利用了WebAssembly的优势。“即便组件模型仍处于早期阶段，但它依然是采用Wasm的一个强有力的理由”。&lt;a href=&quot;https://endor.dev/&quot;&gt;Endor&lt;/a&gt;&quot;的首席执行官兼联合创始人&lt;a href=&quot;https://www.linkedin.com/in/ridruejo/&quot;&gt;Daniel Lopez&lt;/a&gt;&quot;告诉我，“WebAssembly已经被广泛应用于众多无服务器和边缘计算场景中。许多用户（很可能绝大多数）甚至并未意识到它正在幕后运行，尤其是在SaaS和无服务器服务中。Wasm已经支撑了大量应用和场景。随着开发者和行业参与者的广泛支持，进一步的标准化只会加速这一采用进程。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Wasm 3.0并未包含组件模型的最终定稿。尽管Endor项目已非常接近，但像Docker那样“魔法时刻”（即几乎任何应用都能被打包进一个Wasm模块，并可随意部署、传输并在任意地方运行）仍未完全实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;标准化完成之后，应用程序将能以任意语言编写，并通过Wasm模块分发，同时（甚至异步地）部署到任意终端节点。组件模型最终定稿后，WebAssembly就能将其应用场景从网页浏览器和服务器进一步拓展。用户将能够在成千上万个终端节点上，以极高速度同时运行多个轻量级模块中的不同应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在2025年北美KubeCon + CloudNativeCon大会期间，由CNCF主办的WasmCon开幕致辞中，Cosmonic公司首席技术官&lt;a href=&quot;https://www.linkedin.com/in/baileyhayes/&quot;&gt;Bailey Hayes&lt;/a&gt;&quot;阐述了WebAssembly的核心优势：近乎为零的冷启动延迟、高工作负载密度，以及即使在资源受限环境中也能高效运行的轻量级、可移植运行时。展望未来，Hayes将即将发布的WASI 0.3.0视为一个重要里程碑。他表示，该版本预览了多项定义下一代WebAssembly计算浪潮的关键特性，包括，与语言深度集成的并发能力（并提供针对不同语言的惯用绑定）、跨语言组件的可组合并发，以及通过底层I/O和零拷贝数据处理实现的高性能流式传输。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;下一波浪潮的关键特性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Hayes 表示，“我想重点强调三项让我最为期待的下一代计算关键特性：语言集成的并发、跨语言组件的可组合并发，以及支持底层I/O与零拷贝的高性能流式处理。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一切在很大程度上要取决于组件模型的最终确定，尤其是其与WASI的关系，WASI是连接WebAssembly模块与组件的标准接口或API。它将支持构建所谓的WebAssembly “世界”，即由一组由兼容的Wasm组件所构成的互连基础设施，其功能类似于 Kubernetes，但无需依赖容器。2024年发布的WASI Preview 2在标准化方面取得了重大进展，但我们尚未抵达终点。2025年或许仍无法实现“圣杯（Holy Grail）”目标，但可能会带来一些令人欣喜的突破。有传言称，WASI 0.3.0可能无法在今年最终定稿，或将推迟其发布，进而延缓可用组件模型的落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lopez表示，“WASI的标准化过程很漫长，但每一次新的预览版发布都让我们离0.3.0更近一步，鉴于该标准的广泛影响和基础性地位，哪怕耗时超出预期，也必须确保其尽可能完善。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-2026/&quot;&gt;https://thenewstack.io/wasi-1-0-you-wont-know-when-webassembly-is-everywhere-in-202&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HA0BPPlTU2jOYuSDC7Ul</link><guid isPermaLink="false">https://www.infoq.cn/article/HA0BPPlTU2jOYuSDC7Ul</guid><pubDate>Tue, 10 Feb 2026 09:50:50 GMT</pubDate><author>B. Cameron Gain</author><category>性能优化</category></item><item><title>Andy Pavlo：数据库年度回顾</title><description>&lt;p&gt;本文最初发布于Andy Pavlo的个人博客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;又一年过去了。我本希望能多写几篇文章，而不仅仅是年终的长篇大论，但我在春季学期&lt;a href=&quot;https://bsky.app/profile/andypavlo.bsky.social/post/3lsvwhx2ixk2v&quot;&gt;差点丧命&lt;/a&gt;&quot;，那占用了我所有的时间。尽管如此，我还是会回顾一下过去一年中数据库领域我认为重要的趋势和事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据库领域有许多激动人心且前所未有的发展。氛围编程（&lt;a href=&quot;https://twitter.com/karpathy/status/1886192184808149383&quot;&gt;Vibe Coding&lt;/a&gt;&quot;）成了日常用语。Wu-Tang Clan宣布启动&lt;a href=&quot;https://www.youtube.com/watch?v=4u-bttzVubs&quot;&gt;时间胶囊项目&lt;/a&gt;&quot;。Databricks未选择上市，而是进行了&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html#random-fundings&quot;&gt;两轮巨额融资&lt;/a&gt;&quot;，而不是只进行一轮大规模融资。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与此同时，其他事件也都在预料之中，不那么令人惊讶。Redis公司在“抽走地毯（&lt;a href=&quot;https://redis.io/blog/redis-adopts-dual-source-available-licensing/&quot;&gt;rugpull&lt;/a&gt;&quot;）”一年后换回了他们的许可（我&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html#licenses&quot;&gt;去年&lt;/a&gt;&quot;就预测到了这一点）。SurrealDB&lt;a href=&quot;https://blog.cf8.gg/surrealdbs-ch/&quot;&gt;因为没有将写入的数据刷写到磁盘而丢失了数据&lt;/a&gt;&quot;，但他们的基准测试数据却非常好。Coldplay可以&lt;a href=&quot;https://www.reddit.com/r/WatchPeopleDieInside/comments/1m239rb/astronomer_ceo_and_cpo_caught_having_an_affair_on/&quot;&gt;破坏婚姻&lt;/a&gt;&quot;。不过Astronomer倒是从最后这件事里&lt;a href=&quot;https://www.youtube.com/watch?v=vich2C-Tl7Q&quot;&gt;尝到了不少甜头&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在开始之前，我想先回答我每年都会在评论中看到的问题。人们总是问我，在我的分析中，为什么没有提到特定的&lt;a href=&quot;https://www.reddit.com/r/programming/comments/1hr3xor/databases_in_2024_a_year_in_review/m4vone0/&quot;&gt;系统&lt;/a&gt;&quot;、&lt;a href=&quot;https://news.ycombinator.com/item?id=42566660&quot;&gt;数据库&lt;/a&gt;&quot;或&lt;a href=&quot;https://news.ycombinator.com/item?id=34225377&quot;&gt;公司&lt;/a&gt;&quot;。我只能写这么多，除非过去一年中发生了一些有趣或值得注意的事情，要不就没有什么可讨论的。但也并不是所有值得注意的数据库事件，我都适合发表意见。例如，最近有人试图&lt;a href=&quot;https://twitter.com/CeolinWill/status/2005601763051856293&quot;&gt;揭露AvgDatabase首席执行官的真实身份&lt;/a&gt;&quot;，我认为是可以接受的，但&lt;a href=&quot;https://news.ycombinator.com/item?id=46403128&quot;&gt;MongoDB自杀诉讼案&lt;/a&gt;&quot;则不属于此类。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好了，我们开始吧。这些文章每年都在变长，所以我给读者朋友们提前道个歉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;之前的文章：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html&quot;&gt;2024年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html&quot;&gt;2023年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2022/12/2022-databases-retrospective.html&quot;&gt;2022年数据库年度回顾&lt;/a&gt;&quot;&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html&quot;&gt;2021年数据库年度回顾&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;PostgreSQL延续了其统治地位&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在2021年，我就写到，PostgreSQL正在&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2021/12/2021-databases-retrospective.html#dominance-of-postgresql&quot;&gt;吞噬数据库世界&lt;/a&gt;&quot;。这一趋势还在持续，因为数据库领域里最有趣的发展还是与PostgreSQL有关。该DBMS在2025年11月发布了最新版本（&lt;a href=&quot;https://www.postgresql.org/about/news/postgresql-18-released-3142/&quot;&gt;v18&lt;/a&gt;&quot;），其中最突出的功能是新增的&lt;a href=&quot;https://www.cybertec-postgresql.com/en/postgresql-18-and-beyond-from-aio-to-direct-io/&quot;&gt;异步I/O存储子系统&lt;/a&gt;&quot;，它使PostgreSQL终于摆脱了对操作系统页面缓存的依赖。它还增加了对&lt;a href=&quot;https://www.pgedge.com/blog/postgres-18-skip-scan-breaking-free-from-the-left-most-index-limitation&quot;&gt;跳过扫描&lt;/a&gt;&quot;的支持；即使缺少前缀，查询仍然可以使用多键B+树索引。查询优化器也做了一些改进（如&lt;a href=&quot;https://betterstack.com/community/guides/databases/postgresql-18-new-features/#optimizer-and-query-planning-improvements&quot;&gt;移除多余的自连接&lt;/a&gt;&quot;）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;精通数据库的行家们会立刻指出，这些功能并不是什么突破性的创新，其他DBMS多年前就已经有这些功能了。PostgreSQL是唯一仍然依赖操作系统页面缓存的主流DBMS。&lt;a href=&quot;https://richardfoote.wordpress.com/2008/03/10/index-skip-scan-does-index-column-order-matter-any-more-warning-sign/&quot;&gt;Oracle自2002年（v9i）以来就支持跳过扫描了&lt;/a&gt;&quot;！因此，你可能会问，为什么我说2025年数据库领域里最热门的事情是与PostgreSQL有关的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，数据库领域的大部分精力和活动都投入到了与PostgreSQL相关的公司、产品、项目及其衍生系统上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;收购+发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去的一年里，最热门的数据初创公司（&lt;a href=&quot;https://www.databricks.com/&quot;&gt;Databricks&lt;/a&gt;&quot;）为一家PostgreSQL DBaaS公司（&lt;a href=&quot;https://neon.com/&quot;&gt;Neon&lt;/a&gt;&quot;）&lt;a href=&quot;https://www.wsj.com/articles/databricks-to-buy-startup-neon-for-1-billion-fdded971&quot;&gt;支付&lt;/a&gt;&quot;了10亿美元。接下来，世界上最大的数据库公司之一（&lt;a href=&quot;https://www.snowflake.com/en/&quot;&gt;Snowflake&lt;/a&gt;&quot;）为另一家PostgreSQL DBaaS公司（&lt;a href=&quot;https://www.crunchydata.com/&quot;&gt;CrunchyData&lt;/a&gt;&quot;）&lt;a href=&quot;https://www.wsj.com/articles/snowflake-to-buy-crunchy-data-for-250-million-233543ab&quot;&gt;支付&lt;/a&gt;&quot;了2.5亿美元。然后，地球上最大的科技公司之一（微软）&lt;a href=&quot;https://techcommunity.microsoft.com/blog/adforpostgresql/announcing-azure-horizondb/4469710&quot;&gt;推出&lt;/a&gt;&quot;了一个新的PostgreSQL DBaaS（&lt;a href=&quot;https://azure.microsoft.com/en-us/products/horizondb&quot;&gt;HorizonDB&lt;/a&gt;&quot;）。Neon和HorizonDB沿袭了Amazon Aurora在2010年代初的&lt;a href=&quot;https://doi.org/10.1145/3035918.3056101&quot;&gt;高级架构&lt;/a&gt;&quot;，采用单主节点模式分离计算与存储功能。目前，Snowflake的PostgreSQL数据库即服务（DBaaS）使用了和标准PostgreSQL相同的核心架构，它们均基于&lt;a href=&quot;https://www.crunchydata.com/products/crunchy-bridge&quot;&gt;Crunchy Bridge&lt;/a&gt;&quot;构建。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;分布式PostgreSQL&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我上面列出的所有服务都是单主节点架构。也就是说，应用程序将写入发送到主节点，然后主节点将这些更改发送到从副本。但在2025年，有两个新项目宣布要为PostgreSQL创建扩展（即水平分区）服务。2025年6月，Supabase宣布聘请&lt;a href=&quot;https://www.linkedin.com/in/sougou/&quot;&gt;Sugu&lt;/a&gt;&quot;——Vitess的共同创建者和前PlanetScale联合创始人/CTO——来领导&lt;a href=&quot;https://multigres.com/&quot;&gt;Multigres&lt;/a&gt;&quot;项目，为PostgreSQL创建分片中间件，类似于Vitess对MySQL进行分片的机制。Sugu在2023年离开PlanetScale，迫不得已休息了两年。如今，他或许已经摆脱了所有的法律纠纷，可以在Supabase大展身手了。你知道，一位数据库工程师加入一家公司不是个小事，因此&lt;a href=&quot;https://supabase.com/blog/multigres-vitess-for-postgres&quot;&gt;公告&lt;/a&gt;&quot;更多地关注个人而不是系统。&lt;a href=&quot;https://www.linkedin.com/in/adam-prout-0b347630/&quot;&gt;SingleStore联合创始人兼CTO&lt;/a&gt;&quot;在2024年加入了微软，&lt;a href=&quot;https://www.linkedin.com/posts/adam-prout-0b347630_im-happy-to-share-that-im-starting-a-new-activity-7167922823800324096-v1OD&quot;&gt;领导HorizonDB项目&lt;/a&gt;&quot;，但微软（错误地）没有大力宣传。Sugu加盟Supabase的震撼程度，堪比&lt;a href=&quot;https://en.wikipedia.org/wiki/Ol%27_Dirty_Bastard&quot;&gt;Ol&#39; Dirty Bastard（RIP）&lt;/a&gt;&quot;&lt;a href=&quot;https://youtu.be/TDXKvYQ3Xb4&quot;&gt;服刑两年后假释&lt;/a&gt;&quot;出狱，次日便宣布&lt;a href=&quot;https://www.nme.com/news/music/odb-3-1383866&quot;&gt;签下新唱片合约&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在关于Multigres的新闻发布一个月后，PlanetScale&lt;a href=&quot;https://planetscale.com/blog/planetscale-for-postgres#neki-vitess-for-postgres&quot;&gt;宣布&lt;/a&gt;&quot;了自己的Vitess-for-PostgreSQL项目&lt;a href=&quot;https://www.neki.dev/&quot;&gt;Neki&lt;/a&gt;&quot;。2025年3月，PlanetScale推出了其&lt;a href=&quot;https://planetscale.com/blog/announcing-metal&quot;&gt;PostgreSQL DBaaS&lt;/a&gt;&quot;的初始版本，但核心架构仍然是单节点的老搭配&lt;a href=&quot;https://planetscale.com/blog/planetscale-for-postgres#performance-and-reliability&quot;&gt;PostgreSQL和pgBouncer&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年1月5日更新：有人发邮件提醒我，&lt;a href=&quot;https://pgdog.dev/&quot;&gt;PgDog&lt;/a&gt;&quot;也是一个寻求支持PostgreSQL水平分片的开源中间件系统。在心理上，我将PgDog和连接池代理（PgBouncer）归为了一类，但实际上它是Multigres和Neki的竞争对手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;商业格局&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着微软在2025年推出HorizonDB，所有主要的云供应商现在都有自己的PostgreSQL产品项目了。亚马逊自2017年起提供了&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2017/04/announcing-open-preview-of-amazon-aurora-with-postgresql-compatibility/&quot;&gt;Aurora PostgreSQL&lt;/a&gt;&quot;。谷歌在2022年推出了&lt;a href=&quot;https://venturebeat.com/data-infrastructure/google-announces-alloydb-a-faster-hosted-version-of-postgresql&quot;&gt;AlloyDB&lt;/a&gt;&quot;。ServiceNow在2024年推出了&lt;a href=&quot;https://www.investing.com/news/company-news/servicenow-unveils-raptordb-pro-and-future-knowledge-graph-93CH-3609528&quot;&gt;RaptorDB服务&lt;/a&gt;&quot;，其基础是他们2021年&lt;a href=&quot;https://www.zdnet.com/article/servicenow-acquires-database-performance-company-swarm64/&quot;&gt;收购&lt;/a&gt;&quot;的Swarm64。即使是IBM自2018年起也有了&lt;a href=&quot;https://cloud.ibm.com/docs/databases-for-postgresql?topic=databases-for-postgresql-postgresql-relnotes&quot;&gt;云版本的PostgreSQL&lt;/a&gt;&quot;。甲骨文在2023年发布了其&lt;a href=&quot;https://docs.oracle.com/en-us/iaas/releasenotes/changes/9a4b73b5-d4d6-4c89-bd31-b1fa2098fa34/index.htm&quot;&gt;PostgreSQL服务&lt;/a&gt;&quot;，尽管有传言说，其内部PostgreSQL团队在2025年9月的&lt;a href=&quot;https://www.theregister.com/2025/09/11/oracle_slammed_for_mysql_job/&quot;&gt;MySQL OCI裁员&lt;/a&gt;&quot;中受到了附带伤害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前仍然有一些独立的（ISV）PostgreSQL DBaaS公司。按实例数来说，&lt;a href=&quot;https://supabase.com/&quot;&gt;Supabase&lt;/a&gt;&quot;可能是这些公司中最大的。其他公司包括：&lt;a href=&quot;https://www.yugabyte.com/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.tigerdata.com/&quot;&gt;TigerData&lt;/a&gt;&quot;（之前的Timescale）、&lt;a href=&quot;https://planetscale.com/&quot;&gt;PlanetScale&lt;/a&gt;&quot;、&lt;a href=&quot;https://xata.io/&quot;&gt;Xata&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.pgedge.com/&quot;&gt;PgEdge&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.thenile.dev/&quot;&gt;Nile&lt;/a&gt;&quot;。Xata原本基于&lt;a href=&quot;https://xata.io/blog/serverless-postgres-platform#:~:text=AWS%20Aurora%20under%20the%20hood&quot;&gt;Amazon Aurora&lt;/a&gt;&quot;构建了其架构，但今年，他们宣布&lt;a href=&quot;https://xata.io/blog/xata-postgres-with-data-branching-and-pii-anonymization&quot;&gt;切换到自己的基础设施&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.paradedb.com/&quot;&gt;ParadeDB&lt;/a&gt;&quot;尚未宣布其托管服务。&lt;a href=&quot;https://www.tembo.io/&quot;&gt;Tembo&lt;/a&gt;&quot;则在2025年放弃了其&lt;a href=&quot;https://tembo-io.notion.site/Tembo-Cloud-Migration-Guide-1de7c9367d6a80349570e7469ba7f17b&quot;&gt;托管PostgreSQL产品&lt;/a&gt;&quot;，转而开发一种可以完成部分数据库优化的编码代理。&lt;a href=&quot;https://www.hydra.so/&quot;&gt;Hydra&lt;/a&gt;&quot;和&lt;a href=&quot;https://postgresml.org/&quot;&gt;PostgresML&lt;/a&gt;&quot;已于2025年倒闭（见倒闭一节），所以他们退出了游戏。其他系统提供了一个兼容Postgres的前端，但后端系统并非源自PostgreSQL（如&lt;a href=&quot;https://www.cockroachlabs.com/docs/stable/postgresql-compatibility&quot;&gt;CockroachDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://cedardb.com/docs/compatibility/&quot;&gt;CedarDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://docs.cloud.google.com/spanner/docs/postgresql-interface&quot;&gt;Google Spanner&lt;/a&gt;&quot;）。还有一些托管公司提供PostgreSQL DBaaS以及其他系统，如&lt;a href=&quot;https://aiven.io/&quot;&gt;Aiven&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.tessell.com/&quot;&gt;Tessel&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在Databricks和Snowflake收购PostgreSQL公司之后，不知道下一个大买家会是谁。而且，每家主要的技术公司都已经拥有了Postgres产品。EnterpriseDB是最古老的PostgreSQL ISV，但在过去的五年中，他们错过了两次最重要的PostgreSQL收购。但他们可以暂时依靠贝恩资本，或者寄希望于惠普收购他们，尽管那个&lt;a href=&quot;https://community.hpe.com/t5/oem-solutions/recap-hpe-greenlake-launch-discover-2017-madrid/ba-p/6991195&quot;&gt;合作伙伴关系&lt;/a&gt;&quot;是八年前的。PostgreSQL领域的并购格局令人联想到2000年代末期的OLAP收购浪潮：当&lt;a href=&quot;https://techcrunch.com/2011/03/03/teradata-buys-aster-data-263-million/&quot;&gt;AsterData&lt;/a&gt;&quot;、&lt;a href=&quot;https://techcrunch.com/2010/07/06/emc-acquires-data-warehousing-and-analytics-company-greenplum/&quot;&gt;Greenplum&lt;/a&gt;&quot;和&lt;a href=&quot;https://news.microsoft.com/source/2008/07/24/microsoft-to-acquire-datallegro/&quot;&gt;DATAllegro&lt;/a&gt;&quot;相继被收购后，&lt;a href=&quot;https://investor.hp.com/news-events/news/news-details/2011/HP-to-Acquire-Vertica-Customers-Can-Analyze-Massive-Amounts-of-Big-Data---at-Speed-and-Scale/default.aspx&quot;&gt;Vertica&lt;/a&gt;&quot;成了最后一个在公交站等车的玩家。&amp;nbsp;好消息是竞争性的分布式PostgreSQL项目已经发展到了三个（&lt;a href=&quot;https://multigres.com/&quot;&gt;Multigres&lt;/a&gt;&quot;、&lt;a href=&quot;https://planetscale.com/neki&quot;&gt;Neki&lt;/a&gt;&quot;、&lt;a href=&quot;https://pgdog.dev/&quot;&gt;PgDog&lt;/a&gt;&quot;）。并非第一次有人尝试这样做：用于OLAP工作负载的&lt;a href=&quot;https://www.vmware.com/products/app-platform/tanzu-greenplum&quot;&gt;Greenplum&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/ParAccel&quot;&gt;ParAccel&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.citusdata.com/&quot;&gt;Citus&lt;/a&gt;&quot;已经存在了二十年。Citus支持OLTP工作负载，但他们从2010年开始专注于&lt;a href=&quot;https://www.citusdata.com/blog/2018/06/07/what-is-citus-good-for/#:~:text=we%20focused%20on%20building%20a%20fast%20database%20to%20power%20analytics&quot;&gt;分析领域&lt;/a&gt;&quot;。对于OLTP，15年前，NTT RiTaDB项目与&lt;a href=&quot;https://wiki.postgresql.org/wiki/GridSQL&quot;&gt;GridSQL&lt;/a&gt;&quot;合作创建了&lt;a href=&quot;https://wiki.postgresql.org/wiki/Postgres-XC&quot;&gt;Postgres-XC&lt;/a&gt;&quot;。Postgres-XC的开发人员创建了&lt;a href=&quot;https://dbdb.io/db/stormdb&quot;&gt;StormDB&lt;/a&gt;&quot;，后来&lt;a href=&quot;https://translattice.com/pr/TransLattice_Acquires_StormDB_to_Enhance_TransLattice_Elastic_Database.shtml&quot;&gt;Translattice&lt;/a&gt;&quot;在2013年收购了它。&lt;a href=&quot;https://postgres-x2.github.io/&quot;&gt;Postgres-X2&lt;/a&gt;&quot;是一次对XC进行现代化改造的尝试，但开发人员放弃了这项工作。Translattice将StormDB开源为&lt;a href=&quot;https://en.wikipedia.org/wiki/Postgres-XL&quot;&gt;Postgres-XL&lt;/a&gt;&quot;，但该项目自2018年以来一直处于休眠状态。&lt;a href=&quot;https://www.yugabyte.com/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;于&lt;a href=&quot;https://www.yugabyte.com/blog/yugabyte-has-arrived/&quot;&gt;2016&lt;/a&gt;&quot;年推出，可能是部署最广泛的分片PostgreSQL系统（并且仍然是&lt;a href=&quot;https://github.com/yugabyte/yugabyte-db&quot;&gt;开源&lt;/a&gt;&quot;的！），但它是一个硬分叉，只与&lt;a href=&quot;https://docs.yugabyte.com/stable/api/ysql/&quot;&gt;PostgreSQL v15&lt;/a&gt;&quot;兼容。亚马逊云科技在2024年宣布了自己的分片PostgreSQL（&lt;a href=&quot;https://aws.amazon.com/blogs/aws/amazon-aurora-postgresql-limitless-database-is-now-generally-available/&quot;&gt;Aurora Limitless&lt;/a&gt;&quot;），但是闭源的。&amp;nbsp;我知道微软在2019年收购了Citus，但由于他们总给自己的产品起一些令人困惑的名称，所以很难追踪他们在推出HorizonDB之前做了什么。Citus在2019年被重新命名为&lt;a href=&quot;https://techcommunity.microsoft.com/blog/adforpostgresql/azure-database-for-postgresql---hyperscale-citus-now-generally-available/1014865&quot;&gt;Azure Database for PostgreSQL Hyperscale&lt;/a&gt;&quot;，然后在2022年被更名为&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/postgresql/elastic-clusters/concepts-elastic-clusters&quot;&gt;Azure Cosmos DB for PostgreSQL&lt;/a&gt;&quot;。但他们还有使用Citus的Azure Database for PostgreSQL with Elastic Clusters，而该服务与以Citus为基础的Azure Cosmos DB for PostgreSQL并不相同。2023年，微软终止了&lt;a href=&quot;https://techcommunity.microsoft.com/discussions/azuredatabaseforpostgresql/announcement---retiring-azure-postgresql-single-server-in-march-2025-and-introdu/3820887&quot;&gt;Azure PostgreSQL Single Server&lt;/a&gt;&quot;服务，但保留了Azure PostgreSQL Flexible Server。他们有各种各样的Azure服务。这有点像亚马逊云科技忍不住在&lt;a href=&quot;https://docs.aws.amazon.com/aurora-dsql/latest/userguide/what-is-aurora-dsql.html&quot;&gt;DSQL&lt;/a&gt;&quot;的名字前加上 &quot;Aurora&quot;。无论如何，至少微软足够明智，将他们的新系统命名为 &quot;Azure HorizonDB&quot;（目前）。&amp;nbsp;PlanetScale团队&lt;a href=&quot;https://youtu.be/CvgIRHhyRQE?t=143&quot;&gt;对他们的对手没有好感&lt;/a&gt;&quot;，并且已知会对&lt;a href=&quot;https://blog.alexoglou.com/posts/database-decisions/&quot;&gt;Neon&lt;/a&gt;&quot;和&lt;a href=&quot;https://twitter.com/samlambert/status/1984010289348780137&quot;&gt;Timescale&lt;/a&gt;&quot;大打出手。数据库公司之间互相攻击并不新鲜（见&lt;a href=&quot;https://www.linkedin.com/posts/bobdoyleyugabyte_cockroach-labs-activity-7311530387271237634-xR78/&quot;&gt;Yugabyte vs. CockroachDB&lt;/a&gt;&quot;或&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html#gangwar&quot;&gt;Databricks vs. Snowflake&lt;/a&gt;&quot;）。我怀疑，随着PostgreSQL战争的升温，未来我们将看到更多这样的情况。我建议这些小公司&lt;a href=&quot;https://twitter.com/samlambert/status/1996035931057652125&quot;&gt;呼吁&lt;/a&gt;&quot;下，让那些大型的云供应商相互之间&lt;a href=&quot;https://youtu.be/0dT9siTP70Y&quot;&gt;不要提及对方的名字&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;每个数据库都开始支持MCP！&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说2023年是&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html#vector&quot;&gt;所有数据库管理系统（DBMS）纷纷添加向量索引&lt;/a&gt;&quot;的一年，那么2025年就是所有DBMS都开始支持Anthropic公司&lt;a href=&quot;https://en.wikipedia.org/wiki/Model_Context_Protocol&quot;&gt;模型上下文协议&lt;/a&gt;&quot;（MCP）的一年。MCP是一种标准的客户端-服务器JSON-RPC接口，使大型语言模型（LLM）能够与外部工具和数据源交互，而无需自己编写粘合代码。作为中间件，MCP服务器位于数据库管理系统前面，暴露DBMS提供的工具、数据及操作清单。MCP客户端（如Claude或ChatGPT等LLM宿主）通过向MCP服务器发送请求来发现并使用这些工具，扩展其模型能力。对于数据库场景，MCP服务器会将查询转换为对应的数据库指令（如SQL）或管理命令。换言之，MCP如同一个&lt;a href=&quot;https://youtu.be/VXuwljCWZMU&quot;&gt;中间人&lt;/a&gt;&quot;，使数据库与LLM之间可以建立起足够的信任以开展协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic公司在2024年11月&lt;a href=&quot;https://www.theverge.com/2024/11/25/24305774/anthropic-model-context-protocol-data-sources&quot;&gt;发布&lt;/a&gt;&quot;了MCP，但在2025年3月OpenAI宣布将&lt;a href=&quot;https://techcrunch.com/2025/03/26/openai-adopts-rival-anthropics-standard-for-connecting-ai-models-to-data/&quot;&gt;在其生态系统中支持MCP&lt;/a&gt;&quot;后，它才真正起飞。在接下来的几个月里，所有数据库管理系统（DBMS）供应商都发布了适用于所有系统类别的MCP服务器：OLAP（如&lt;a href=&quot;https://github.com/ClickHouse/mcp-clickhouse&quot;&gt;ClickHouse&lt;/a&gt;&quot;、&lt;a href=&quot;https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-agents-mcp&quot;&gt;Snowflake&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/firebolt-db/mcp-server&quot;&gt;Firebolt&lt;/a&gt;&quot;、&lt;a href=&quot;https://yellowbrick.com/blog/application-development/yellowbrick-mcp-server-llms-cutting-code-time-and-speeding-up-etl-development/&quot;&gt;Yellowbrick&lt;/a&gt;&quot;）、SQL（如&lt;a href=&quot;https://www.yugabyte.com/blog/yugabytedb-mcp-server/&quot;&gt;YugabyteDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://blogs.oracle.com/database/introducing-mcp-server-for-oracle-database&quot;&gt;Oracle&lt;/a&gt;&quot;、&lt;a href=&quot;https://planetscale.com/docs/vitess/connecting/mcp&quot;&gt;PlanetScale&lt;/a&gt;&quot;）和NoSQL（如&lt;a href=&quot;https://www.mongodb.com/company/blog/announcing-mongodb-mcp-server&quot;&gt;MongoDB&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/neo4j-contrib/mcp-neo4j&quot;&gt;Neo4j&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/redis/mcp-redis&quot;&gt;Redis&lt;/a&gt;&quot;）。由于Postgres MCP服务器没有官方的，所以每个Postgres DBaaS都发布了自己的服务器（如&lt;a href=&quot;https://github.com/timescale/pg-aiguide&quot;&gt;Timescale&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/supabase-community/supabase-mcp&quot;&gt;Supabase&lt;/a&gt;&quot;、&lt;a href=&quot;https://xata.io/blog/built-xata-mcp-server&quot;&gt;Xata&lt;/a&gt;&quot;）。云供应商则发布了多数据库MCP服务器，可以与他们托管的任何数据库服务进行通信（如&lt;a href=&quot;https://aws.amazon.com/blogs/database/supercharging-aws-database-development-with-aws-mcp-servers/&quot;&gt;亚马逊云科技&lt;/a&gt;&quot;、&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/developer/azure-mcp-server/tools/azure-sql&quot;&gt;微软&lt;/a&gt;&quot;、&lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/mcp-toolbox-for-databases-now-supports-model-context-protocol&quot;&gt;谷歌&lt;/a&gt;&quot;）。允许单一网关与异构数据库通信，几乎已经实现了理想中的&lt;a href=&quot;https://en.wikipedia.org/wiki/Federated_database_system&quot;&gt;联合数据库&lt;/a&gt;&quot;，但还不完全。据我所知，在这些MCP服务器中，每个请求每次仅针对单个数据库，因此需要应用程序负责执行跨源连接操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了供应商的官方MCP实现方案外，几乎每种数据库管理系统（DBMS）都存在&lt;a href=&quot;https://github.com/TensorBlock/awesome-mcp-servers/blob/main/docs/databases.md&quot;&gt;数百种&lt;/a&gt;&quot;非官方的MCP服务器实现方案。其中部分方案试图支持多个系统（如&lt;a href=&quot;https://dbhub.ai/&quot;&gt;DBHub&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/FreePeak/db-mcp-server&quot;&gt;DB MCP Server&lt;/a&gt;&quot;）。关于PostgreSQL MCP服务器，DBHub曾发布过&lt;a href=&quot;https://dbhub.ai/blog/state-of-postgres-mcp-servers-2025&quot;&gt;一篇不错的综述&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个有趣而又已经证明对代理有帮助的特性是数据库分支。虽然不特定于MCP服务器，但分支允许代理快速测试数据库更改，而不影响生产应用程序。2025年7月，Neon报告说，代理&lt;a href=&quot;https://www.linkedin.com/posts/amitkumarvsingh_ai-agents-are-creating-more-databases-on-activity-7336398117862371328-Q6pO/&quot;&gt;创建了80%的数据库&lt;/a&gt;&quot;。Neon从一开始设计就支持&lt;a href=&quot;https://dev.to/semaphore/a-first-look-at-neon-a-postgres-database-that-branches-10e6&quot;&gt;分支&lt;/a&gt;&quot;（早先在这个系统还叫&lt;a href=&quot;https://dbdb.io/db/neon#history&quot;&gt;Zenith&lt;/a&gt;&quot;时，Nikita就向我做过演示），而其他系统则是后来才添加了分支支持。要了解更多信息，可以看下Xata最近发表的一篇关于数据库分支的&lt;a href=&quot;https://xata.io/blog/neon-vs-supabase-vs-xata-postgres-branching-part-1&quot;&gt;对比文章&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一方面，我很高兴现在有一个标准，可以用来向更多的应用程序暴露数据库的功能。但没有人应该信任一个拥有无限数据库访问权限的应用程序，无论是通过MCP还是系统的常规API。而且，只授予账户最小权限仍然是一个好习惯，特别是在未监控的代理可能在你的数据库中疯狂操作时，对账户做限制显得尤为重要。这意味着，当大型语言模型开始大范围流行时，为每个账户授予管理员权限或所有服务使用同一个账户，诸如这样的懒散做法将彻底行不通。当然，如果你们公司不介意把数据库向&lt;a href=&quot;https://www.theregister.com/2025/01/30/deepseek_database_left_open/&quot;&gt;全世界开放&lt;/a&gt;&quot;，并导致某家最富有的公司市值&lt;a href=&quot;https://www.theregister.com/2025/01/30/deepseek_database_left_open/&quot;&gt;暴跌6000亿美元&lt;/a&gt;&quot;，那么恶意MCP请求就不是你最需要担心的问题了。&amp;nbsp;从我对一些MCP服务器实现的粗略检查来看，它们是简单的代理，只是负责将MCP JSON请求转换为数据库查询，并没有通过深入的自省来理解请求的目的以及它是否合适。有人会尝试在你的应用程序中&lt;a href=&quot;https://www.youtube.com/watch?v=DF8Pny3VTg8&quot;&gt;订购18000个水杯&lt;/a&gt;&quot;，你需要确保它不会导致数据库崩溃。有些MCP服务器有基本的保护机制（如ClickHouse只允许&lt;a href=&quot;https://clickhouse.com/docs/use-cases/AI/MCP#clickhouse-mcp-server&quot;&gt;只读查询&lt;/a&gt;&quot;）。DBHub提供了一些额外的&lt;a href=&quot;https://dbhub.ai/#why-dbhub&quot;&gt;保护&lt;/a&gt;&quot;，如限制每个请求返回的记录数并实现了查询超时。Supabase的文档提供了MCP代理的&lt;a href=&quot;https://supabase.com/docs/guides/getting-started/mcp#recommendations&quot;&gt;最佳实践指南&lt;/a&gt;&quot;，但也得人类遵循它们才行。当然，如果你依赖于人类做正确的事情，那么&lt;a href=&quot;https://www.generalanalysis.com/blog/supabase-mcp-blog&quot;&gt;坏事就在所难免&lt;/a&gt;&quot;。&amp;nbsp;企业DBMS有着开源系统缺乏的自动化护栏和其他安全机制，对于智能代理生态系统，它们做了更好的准备，比如，&lt;a href=&quot;https://www.ibm.com/docs/en/gdp/12.x?topic=overview-guardium&quot;&gt;IBM Guardium&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.oracle.com/security/database-security/audit-vault-database-firewall/&quot;&gt;Oracle Database Firewall&lt;/a&gt;&quot;能够识别并阻止异常查询。我不是在为这些大型科技公司做宣传。我知道，未来我们将看到更多智能代理妨害生活的例子，比如&lt;a href=&quot;https://twitter.com/emil_priver/status/1783399265366052877&quot;&gt;意外删除数据库&lt;/a&gt;&quot;。将MCP服务器与代理（如连接池）结合是引入自动化保护机制的绝佳机会。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MongoDB起诉FerretDB&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到现在，MongoDB作为NoSQL领域的中坚已经有二十年了。2021年，Percona高层启动了FerretDB项目，旨在提供一款中间件代理，将MongoDB查询转换为适配PostgreSQL后端的SQL。有了这个代理，不用重写查询就可以将MongoDB应用程序无缝地迁移至PostgreSQL。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;双方共存数年后，MongoDB于2023年向FerretDB发出&lt;a href=&quot;https://blocksandfiles.com/wp-content/uploads/2025/04/Letter-from-MongoDB-to-FerretDB_3-Nov-2023-signed.pdf&quot;&gt;停止侵权通知书&lt;/a&gt;&quot;，指控FerretDB侵犯其专利权、著作权及商标权，并违反了MongoDB文档及有线协议规范的许可条款。2025年5月，MongoDB就这些问题向FerretDB&lt;a href=&quot;https://youtu.be/11BlEYtj53Q&quot;&gt;提起&lt;/a&gt;&quot;&lt;a href=&quot;https://dockets.justia.com/docket/delaware/dedce/1:2025cv00641/89247&quot;&gt;联邦诉讼&lt;/a&gt;&quot;，使这封信件公之于众。双方争议的焦点之一是，FerretDB未经授权便宣称其产品可作为MongoDB“&lt;a href=&quot;https://blog.ferretdb.io/ferretdb-1-0-ga-opensource-mongodb-alternative/&quot;&gt;即插即用的替代品&lt;/a&gt;&quot;”。MongoDB的&lt;a href=&quot;https://storage.courtlistener.com/recap/gov.uscourts.ded.89247/gov.uscourts.ded.89247.1.0.pdf&quot;&gt;法庭文件&lt;/a&gt;&quot;列举了标准指控： (1) 误导开发人员；(2) 弱化商标价值；(3) 损害企业声誉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让这个故事变得更加复杂的是，微软宣布将与MongoDB兼容的&lt;a href=&quot;https://documentdb.io/&quot;&gt;DocumentDB&lt;/a&gt;&quot;捐赠给&lt;a href=&quot;https://www.linuxfoundation.org/press/linux-foundation-welcomes-documentdb-to-advance-open-developer-first-nosql-innovation&quot;&gt;Linux基金会&lt;/a&gt;&quot;。该项目的网站提到，DocumentDB与MongoDB驱动程序兼容，并且旨在“&lt;a href=&quot;https://documentdb.io/#:~:text=our%20mission%20is%20to%20build%20a%20MongoDB%20compatible%20open%20source%20document%20database&quot;&gt;构建一个与MongoDB兼容的开源文档数据库&lt;/a&gt;&quot;”。还有其他主流的数据库供应商参与了该项目，如亚马逊云科技和Yugabyte。粗看之下，这种语言似乎与MongoDB指控的FerretDB的行为如出一辙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我没有找到数据库公司因对方复制其API而起诉对方的例子。最接近的例子是Oracle起诉谷歌在安卓系统中使用了Java API的“清洁室副本”。最终，最高法院以公平使用为由&lt;a href=&quot;https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America%2C_Inc.&quot;&gt;支持了谷歌&lt;/a&gt;&quot;。这个案例影响了法律上对重新实现行为的处理方式。&amp;nbsp;我不知道如果这场诉讼真进入庭审阶段会如何发展。陪审团是由随机挑选的路人组成的，他们或许无法理解MongoDB有线协议的具体细节，但他们绝对清楚FerretDB最初的名字是&lt;a href=&quot;https://www.reddit.com/r/programming/comments/qlyalj/mangodb_a_truly_open_source_mongodb_alternative/&quot;&gt;MangoDB&lt;/a&gt;&quot;。要说服陪审团，相信你给公司起名时仅替换一个字母不是想转移客户，这将非常困难。更何况这根本不是个原创名称：早就有个恶搞数据库管理系统叫&lt;a href=&quot;https://dbdb.io/db/mangodb&quot;&gt;MangoDB&lt;/a&gt;&quot;，它会把所有数据写入/dev/null。&amp;nbsp;说到数据库系统的命名时，微软选择“DocumentDB”让人觉得遗憾。市面上已经有&lt;a href=&quot;https://aws.amazon.com/documentdb/&quot;&gt;Amazon DocumentDB&lt;/a&gt;&quot;（顺便说一下，它也&lt;a href=&quot;https://docs.aws.amazon.com/documentdb/latest/developerguide/compatibility.html#mongodb-80&quot;&gt;兼容&lt;/a&gt;&quot;MongoDB，不过亚马逊云科技可能为此付了费）、&lt;a href=&quot;https://docs.intersystems.com/irislatest/csp/docbook/DocBook.UI.Page.cls?KEY=GDOCDB_intro&quot;&gt;InterSystems DocDB&lt;/a&gt;&quot;和&lt;a href=&quot;https://docs.yugabyte.com/stable/architecture/docdb/&quot;&gt;Yugabyte DocDB&lt;/a&gt;&quot;。微软的“Cosmos DB”在2016年推出时的原始名称也是&lt;a href=&quot;https://auth0.com/blog/documentdb-with-aspnetcore/&quot;&gt;DocumentDB&lt;/a&gt;&quot;。&amp;nbsp;最后，MongoDB的法庭文件声称，他们“开创了‘非关系型’数据库”。这个说法是不正确的。第一个通用数据库管理系统是非关系型的，因为关系模型那时候还没有发明出来。通用电气的&lt;a href=&quot;https://en.wikipedia.org/wiki/Integrated_Data_Store&quot;&gt;Integrated Data Store&lt;/a&gt;&quot;（1964年）使用了&lt;a href=&quot;https://en.wikipedia.org/wiki/Network_model&quot;&gt;网络数据模型&lt;/a&gt;&quot;，IBM的&lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_Information_Management_System&quot;&gt;Information Management System&lt;/a&gt;&quot;（1966年）使用了&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_database_model&quot;&gt;层次数据模型&lt;/a&gt;&quot;。MongoDB也不是第一个文档数据库管理系统。这个头衔应该归属于1980年代末的面向对象数据库管理系统（如&lt;a href=&quot;http://www.versant.com/products/versant-object-database&quot;&gt;Versant&lt;/a&gt;&quot;）或2000年代的XML数据库管理系统（如&lt;a href=&quot;https://www.progress.com/marklogic&quot;&gt;MarkLogic&lt;/a&gt;&quot;）。只是与它们相比，MongoDB取得了压倒性的成功（也许IMS除外）。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;文件格式之争&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件格式是数据系统中过去十年间基本处于停滞状态的一个领域。2011年，Meta公司针对Hadoop发布了名为&lt;a href=&quot;https://en.wikipedia.org/wiki/RCFile&quot;&gt;RCFile&lt;/a&gt;&quot;的列式存储格式。两年后，Meta对RCFile做了优化，并推出了基于PAX的&lt;a href=&quot;https://orc.apache.org/&quot;&gt;ORC&lt;/a&gt;&quot;（Optimized Record Columnar File）格式。ORC发布一个月后，Twitter联合Cloudera推出了&lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Parquet&lt;/a&gt;&quot;的首个版本。近十五年后，Parquet已成为开源领域占支配地位的文件格式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年，有五个新的开源文件格式发布，都在争取取代Parquet的地位：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/cwida/FastLanes&quot;&gt;CWI FastLanes&lt;/a&gt;&quot;&lt;a href=&quot;https://github.com/future-file-format/f3&quot;&gt;CMU + Tsinghua F3&lt;/a&gt;&quot;&lt;a href=&quot;https://vortex.dev/&quot;&gt;SpiralDB Vortex&lt;/a&gt;&quot;&lt;a href=&quot;https://github.com/AnyBlox&quot;&gt;德国人的AnyBlox&lt;/a&gt;&quot;&lt;a href=&quot;https://web.archive.org/web/20250802074742/https://github.com/microsoft/amudai&quot;&gt;微软Amudai&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是2024年发布的格式：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/facebookincubator/nimble&quot;&gt;Meta Nimble&lt;/a&gt;&quot;&lt;a href=&quot;https://lancedb.com/blog/lance-v2/&quot;&gt;LanceDB Lance&lt;/a&gt;&quot;&lt;a href=&quot;https://tsfile.apache.org/&quot;&gt;IoTDB TsFile&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spiraldb.com/&quot;&gt;SpiralDB&lt;/a&gt;&quot;今年最引人瞩目的举措是宣布&lt;a href=&quot;https://www.linuxfoundation.org/press/lf-ai-data-foundation-hosts-vortex-project-to-power-high-performance-data-access-for-ai-and-analytics&quot;&gt;将Vortex捐赠给Linux基金会&lt;/a&gt;&quot;，并成立了多组织指导委员会。微软则在2025年底悄然&lt;a href=&quot;https://github.com/microsoft/amudai&quot;&gt;终止&lt;/a&gt;&quot;了Amudai项目（至少将其转为闭源）。其余项目（FastLanes、F3、Anyblox）均属学术原型，其中Anyblox今年斩获了&lt;a href=&quot;https://www.linkedin.com/posts/janagiceva_im-thrilled-and-honored-to-share-that-our-activity-7368909487023329281-mhDv/&quot;&gt;VLDB最佳论文奖&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种新的竞争点燃了Parquet开发社区对其功能进行现代化改进的热情。Parquet PMC主席&lt;a href=&quot;http://julien.ledem.net/&quot;&gt;Julien Le Dem&lt;/a&gt;&quot;对列式文件格式格局做了&lt;a href=&quot;https://sympathetic.ink/2025/12/11/Column-Storage-for-the-AI-era.html&quot;&gt;深入的技术分析&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Parquet的主要问题并非源于格式本身。该规范可以且已经经过演进。没有人会要求组织机构重写PB级的旧文件以更新至最新的Parquet版本。问题在于，人们用不同的语言实现了大量的读写库，而每个库只支持这个规范的特定子集。通过对实际环境中Parquet文件的&lt;a href=&quot;https://bsky.app/profile/andypavlo.bsky.social/post/3m256lckmec2z&quot;&gt;分析&lt;/a&gt;&quot;，我们发现，94%的文件仅使用了2013年发布的v1版本的特性，即便其创建时间戳晚于2020年。这种最低公约数意味着：当有人使用v2版本的特性创建文件时，系统能否正确读取该文件完全取决于其版本兼容性。&amp;nbsp;我与清华大学的&lt;a href=&quot;https://xinyuzeng.github.io/&quot;&gt;Xinyu Zeng&lt;/a&gt;&quot;、&lt;a href=&quot;https://dl.acm.org/profile/99661226655&quot;&gt;Ruijun Meng&lt;/a&gt;&quot;、&lt;a href=&quot;https://people.iiis.tsinghua.edu.cn/~huanchen/&quot;&gt;Huanchen Zhang&lt;/a&gt;&quot;、CMU的&lt;a href=&quot;https://www.cs.cmu.edu/~mprammer/&quot;&gt;Martin Prammer&lt;/a&gt;&quot;、&lt;a href=&quot;https://csd.cmu.edu/people/faculty/jignesh-patel&quot;&gt;Jignesh Patel&lt;/a&gt;&quot;以及&lt;a href=&quot;https://wesmckinney.com/&quot;&gt;Wes McKinney&lt;/a&gt;&quot;一起开发了F3文件格式。我们的重点是通过提供作为共享对象的原生解码器（Rust crates）和在文件中嵌入这些解码器的WASM版本来解决互操作性问题。如果有人创建了一种新的编码格式，而数据库管理系统尚未提供原生支持，那么它仍然可以使用WASM版本通过传递Arrow缓冲区来读取数据。每个解码器针对单个列，这使得DBMS能够针对单个文件同时使用原生解码器和WASM解码器。AnyBlox采用了一种不同的方法，生成单个WASM程序来解码整个文件。&amp;nbsp;我不知道谁会赢得文件格式之争。下一场较量很可能围绕GPU支持展开。SpiralDB似乎正在采取正确的举措，但Parquet的普及性将构成一个巨大的挑战。至于DuckLake如何寻求颠覆Iceberg，我甚至还没有讨论……&amp;nbsp;当然，每当这个话题出现时，总有人会贴出&lt;a href=&quot;https://xkcd.com/927/&quot;&gt;那幅关于标准竞争的xkcd漫画&lt;/a&gt;&quot;。我已经看过了，别再发邮件给我了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;偶然事件&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据库是大生意。让我们逐一了解下。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;收购&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;市场上有很多动作。为了准备一笔收购，Pinecone在9月份&lt;a href=&quot;https://venturebeat.com/data-infrastructure/pinecone-founder-edo-liberty-appoints-googler-ash-as-ceo&quot;&gt;更换了CEO&lt;/a&gt;&quot;，但我没有听到任何其他的消息。以下是已经发生的收购：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.datastax.com/blog/ibm-plans-to-acquire-datastax&quot;&gt;DataStax → IBM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这家Cassandra的坚定支持者年初被IBM收购，&lt;a href=&quot;https://www.linkedin.com/posts/nathanlatka_saas-datastax-activity-7300252058274672640-OQx_/&quot;&gt;估值30亿美元&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://quickwit.io/blog/quickwit-joins-datadog&quot;&gt;Quickwit → DataDog&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;作为Lucene替代方案的领军企业，全文搜索引擎&lt;a href=&quot;https://github.com/quickwit-oss/tantivy&quot;&gt;Tantivy&lt;/a&gt;&quot;已于年初被收购。好消息是，Tantivy的开发工作仍在继续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/blog/dbt-labs-announces-sdf-labs-acquisition&quot;&gt;SDF → dbt&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这次收购对dbt来说是一个很好的补充，也是他们今年发布的&lt;a href=&quot;https://www.getdbt.com/product/fusion&quot;&gt;Fusion&lt;/a&gt;&quot;的一部分，使他们能够在DAG中进行更严格的SQL分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mongodb.com/company/blog/news/redefining-database-ai-why-mongodb-acquired-voyage-ai&quot;&gt;Voyage.ai → MongoDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Mongo收购了一家初创AI公司，旨在&lt;a href=&quot;https://news.ycombinator.com/item?id=43160731&quot;&gt;增强&lt;/a&gt;&quot;其云产品中的RAG能力。在公告前一周，我&lt;a href=&quot;https://www.linkedin.com/in/wangpatrick57/&quot;&gt;最优秀的学生&lt;/a&gt;&quot;之一加入了Voyage。他以为自己不与数据库公司签约背叛了“家族”，结果最终还是加入了一家数据库公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://neon.tech/blog/neon-and-databricks&quot;&gt;Neon → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;显然，这家PostgreSQL公司引发了一场竞购战，但Databricks以&lt;a href=&quot;https://www.wsj.com/articles/databricks-to-buy-startup-neon-for-1-billion-fdded971&quot;&gt;令人垂涎的10亿美元&lt;/a&gt;&quot;收购了它。Neon至今仍然作为一个独立服务存在，但Databricks迅速在其生态系统中将其更名为&lt;a href=&quot;https://www.databricks.com/product/lakebase&quot;&gt;Lakebase&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.wsj.com/articles/snowflake-to-buy-crunchy-data-for-250-million-233543ab&quot;&gt;CrunchyData → Snowflake&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;你知道Snowflake不会让Databricks在夏天独占所有风头，所以他们为CrunchyData这家有着13年历史的PostgreSQL公司支付了2.5亿美元。近年来，Crunchy从Citus吸引了一些顶级人才，并在Snowflake收购他们之前扩大了其DBaaS产品。Snowflake在2025年12月宣布公开预览其&lt;a href=&quot;https://www.snowflake.com/en/product/features/postgres/&quot;&gt;Postgres&lt;/a&gt;&quot;服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/amp/2025/05/27/salesforce-informatica-deal.html&quot;&gt;Informatica → Salesforce&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Informatica，这家1990年代的老派ETL公司被Salesforce以&lt;a href=&quot;https://finance.yahoo.com/news/salesforce-buys-informatica-8b-failed-150907984.html&quot;&gt;80亿美元&lt;/a&gt;&quot;的价格收购。这家公司于1999年上市，2015年转为PE，然后在2021年再次上市。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://investors.couchbase.com/news-releases/news-release-details/couchbase-be-acquired-haveli-investments-15-billion&quot;&gt;Couchbase → 私募股权&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;老实说，我一直不明白Couchbase在2021年是如何上市的，莫非是借了MongoDB的东风？几年前，通过整合加州大学欧文分校&lt;a href=&quot;https://www.couchbase.com/press-releases/couchbase-announces-first-commercial-implementation-of-sql-with-n1ql-for-analytics/&quot;&gt;AsterixDB项目&lt;/a&gt;&quot;的一些组件，Couchbase做了一些有趣的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/business/finance/databricks-buy-sequoia-backed-tecton-ai-agent-push-2025-08-22/&quot;&gt;Tecton → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Tecton为Databricks提供了额外的代理构建工具。我的另一位学生曾在该公司工作，现在是在Databricks。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.fivetran.com/press/fivetran-acquires-tobiko-data-to-power-the-next-generation-of-advanced-ai-ready-data-transformation&quot;&gt;Tobiko Data → Fivetran&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个团队开发了两个有用的工具：&lt;a href=&quot;https://sqlmesh.readthedocs.io/&quot;&gt;SQLMesh&lt;/a&gt;&quot;和&lt;a href=&quot;https://sqlglot.com/&quot;&gt;SQLglot&lt;/a&gt;&quot;。前者是唯一可与dbt（见下文，计划与Fivetran合并）抗衡的开源竞争者。SQLglot是一个便捷的SQL解析器/反解析器，支持启发式的查询优化器。未来几年，Fivetran与SDF将该技术与dbt相结合，将在该领域形成引人注目的技术布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.businesswire.com/news/home/20250910856970/en/SingleStore-Announces-Growth-Buyout-Led-by-Vector-Capital&quot;&gt;SingleStore → 私募股权&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;购买SingleStore的PE公司（&lt;a href=&quot;https://www.vectorcapital.com/&quot;&gt;Vector Capital&lt;/a&gt;&quot;）以前有管理数据库公司的经验。之前在2020年，他们曾经&lt;a href=&quot;https://www.businesswire.com/news/home/20201021005279/en/Vector-Capital-Completes-Acquisition-of-MarkLogic&quot;&gt;购买了XML数据库公司MarkLogic&lt;/a&gt;&quot;，并在2023年将其&lt;a href=&quot;https://investors.progress.com/news-releases/news-release-details/progress-announces-plans-acquire-marklogic&quot;&gt;转手给Progress&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dbta.com/Editorial/News-Flashes/MariaDB-to-Acquire-Galera-Cluster-to-Enable-Deeper-Integration-of-Synchronous-Replication-Technology-169742.aspx&quot;&gt;Codership → MariaDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;在2024年被PE公司收购后，MariaDB公司今年开启了收购狂潮。首当其冲的是开发MariaDB扩展中间件Galera Cluster的公司。详见我2023年对&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2024/01/2023-databases-retrospective.html#mariadb&quot;&gt;MariaDB混乱局面&lt;/a&gt;&quot;的全面分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.crn.com/news/cloud/2025/mariadb-buys-back-skysql-in-database-flexibility-push&quot;&gt;SkySQL → MariaDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;然后是MariaDB的第二笔收购。为避免混淆，我需要说明一下：2010年的时候，最初为MariaDB提供支持的商业公司名为“SkySQL Corporation”，2014年，它更名为“MariaDB Corporation”。2020年，MariaDB Corporation推出名为SkySQL的MariaDB数据库即服务（DBaaS）。但因资金持续流失，该公司于2023年&lt;a href=&quot;https://www.businesswire.com/news/home/20231214486927/en/MariaDB-Finalizes-Spinoff-of-SkySQL&quot;&gt;将SkySQL Inc.剥离&lt;/a&gt;&quot;出去，成为一家独立的公司。而2025年，MariaDB Corporation&lt;a href=&quot;https://medium.com/@arbaudie.it/personal-opinion-mariadb-re-acquires-skysql-125181507358&quot;&gt;回购了SkySQL Inc.&lt;/a&gt;&quot;，兜了一圈后回到了原处。今年我的数据库宾果卡上可没有这一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.crystaldba.ai/blog/post/temporal-technologies-acquires-crystal-dba&quot;&gt;Crystal DBA → Temporal&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;自动化数据库优化工具公司Crystal DBA加入Temporal公司，帮他们自动优化数据库！很高兴得知Crystal创始人、伯克利数据库小组校友Johann Schleier-Smith在那里发展顺利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.harvestmp.com/transactions&quot;&gt;HeavyDB → Nvidia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个系统（之前叫OmniSci，再之前叫MapD）是首批GPU加速数据库之一，于2013年推出。除了一家并购公司披露了这笔成功的交易外，我未能找到有关交易完成的官方公告。随后我们与英伟达召开会议，探讨潜在的数据库研究合作事宜，期间几位HeavyDB的伙伴也现身参与。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.prnewswire.com/news-releases/istari-digital-acquires-dgraph-to-strengthen-data-foundation-for-ai-and-engineering-302593246.html&quot;&gt;DGraph → Istari Digital&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Dgraph之前&lt;a href=&quot;https://web.archive.org/web/20250806150448/https://hypermode.com/blog/the-future-of-dgraph-is-open-serverless-and-ai-ready&quot;&gt;在2023年被Hypermode收购&lt;/a&gt;&quot;。现在看来，Istari只是买了Dgraph，而不是Hypermode的其他部分（或者他们放弃了）。我还没见过任何积极使用Dgraph的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.mews.com/en/press/mews-acquires-datachat&quot;&gt;DataChat → Mews&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是最早支持“与数据库对话”的数据库之一，来自威斯康星大学的Jignesh Patel，现为CMU-DB教授。但后来被一家欧洲酒店管理领域的SaaS公司收购了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://siliconangle.com/2025/11/10/snowflake-acquires-database-migration-startup-datometry/&quot;&gt;Datometry → Snowflake&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;多年来，Datometry一直致力于将旧版SQL方言（如Teradata）自动转换至新型OLAP系统这一棘手的问题。Snowflake收购他们是为了扩展自己的&lt;a href=&quot;https://www.snowflake.com/en/blog/accelerate-data-migration-datometry-technology/&quot;&gt;迁移工具&lt;/a&gt;&quot;。更多信息参见&lt;a href=&quot;https://www.youtube.com/watch?v=cL1-BIaQSYE&amp;amp;list=PLSE8ODhjZXjagqlf1NxuBQwaMkrHXi-iz&amp;amp;index=23&quot;&gt;Datometry 2020年的CMU-DB技术讲座&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://clickhouse.com/blog/librechat-open-source-agentic-data-stack&quot;&gt;LibreChat → ClickHouse&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;像Snowflake收购Datometry一样，ClickHouse的这次收购是提升高性能通用OLAP引擎开发体验的典范。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/mooncake-labs-joins-databricks-accelerate-vision-lakebase&quot;&gt;Mooncake → Databricks&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;在收购Neon之后，为了使PostgreSQL能够读写Apache Iceberg数据，Databricks收购了Mooncake。更多信息参见他们2025年11月的&lt;a href=&quot;https://www.youtube.com/watch?v=VqFZyWHGQVM&amp;amp;list=PLSE8ODhjZXjbEeW_bOCZ8c_nx_Jhoz-GW&amp;amp;index=8&quot;&gt;CMU-DB讲座&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/technology/ibm-nears-roughly-11-billion-deal-confluent-wsj-reports-2025-12-08/&quot;&gt;Confluent → IBM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是一个将草根开源项目发展为一家公司的经典案例。Kafka最初于2011年在Linkedin开发，随后在2014年，Confluent作为独立的初创公司分拆出来，于七年后的2021年成功上市。随后IBM斥巨资将其收购。与DataStax的情况相似，目前尚不确定IBM是会对Confluent采取&lt;a href=&quot;https://news.ycombinator.com/item?id=43200706&quot;&gt;惯常的企业收购策略&lt;/a&gt;&quot;，还是像RedHat那样使其保持独立运营。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;Gel → Vercel&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;前身为&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html#random-naming&quot;&gt;EdgeDB&lt;/a&gt;&quot;，在PostgreSQL之上提供了一种DSL，被Verel在2025年年底收购。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;Kuzu → ???&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这款诞生于滑铁卢大学的嵌入式图形DBMS在2025年被一家未具名的公司收购。然后KuzuDB公司宣布放弃该开源项目。&lt;a href=&quot;https://ladybugdb.com/&quot;&gt;LadybugDB&lt;/a&gt;&quot;项目旨在维护Kuzu代码的一个分支版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;合并&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年10月，&lt;a href=&quot;https://www.fivetran.com/&quot;&gt;Fivetran&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.getdbt.com/&quot;&gt;dbt Labs&lt;/a&gt;&quot;宣布&lt;a href=&quot;https://www.reuters.com/business/a16z-backed-data-firms-fivetran-dbt-labs-merge-all-stock-deal-2025-10-13&quot;&gt;合并&lt;/a&gt;&quot;成一家公司，这个消息着实让人意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据我所知，数据库领域的上一次合并是2019年&lt;a href=&quot;https://techcrunch.com/2018/10/03/cloudera-and-hortonworks-announce-5-2-billion-merger/&quot;&gt;Cloudera和Hortonworks合并&lt;/a&gt;&quot;。但那笔交易只是两家在Hadoop领域苦苦寻找定位的公司试图通过合并成一家公司来扭转局面（剧透：他们没有成功）。2022年，MariaDB公司通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Special-purpose_acquisition_company&quot;&gt;SPAC&lt;/a&gt;&quot;与&lt;a href=&quot;https://mariadb.com/newsroom/press-releases/mariadb-completes-merger-and-lands-on-nyse-as-mrdb/&quot;&gt;Angel Pond Holdings公司&lt;/a&gt;&quot;合并，技术上讲也算并购，但那是为了让MariaDB能够上市而采取的后门策略。对&lt;a href=&quot;https://www.bizjournals.com/sanjose/news/2022/12/19/mariadb-goes-public-in-spac-merger.html&quot;&gt;投资者&lt;/a&gt;&quot;来说，结果并不好。Fivetran和dbt的合并与这两者不同（更好）——这两家互补的技术公司正联手打造ETL领域的巨头企业，为近期开展正规的IPO做准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;融资&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除非我错过了，或者他们没有宣布，数据库初创公司的早期融资轮次并不算多。围绕向量数据库的炒作已趋于平息，风险投资公司现在只愿为LLM公司花钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Databricks：&lt;a href=&quot;https://www.databricks.com/company/newsroom/press-releases/databricks-surpasses-4-8b-revenue-run-rate-growing-55-year-over-year&quot;&gt;L轮40亿美元&lt;/a&gt;&quot;Databricks：&lt;a href=&quot;https://www.reuters.com/business/databricks-eyes-over-100-billion-valuation-investors-back-ai-growth-plans-2025-08-19/&quot;&gt;K轮10亿美元&lt;/a&gt;&quot;ClickHouse：&lt;a href=&quot;https://clickhouse.com/blog/clickhouse-raises-350-million-series-c-to-power-analytics-for-ai-era&quot;&gt;C轮3.5亿美元&lt;/a&gt;&quot;Supabase：&lt;a href=&quot;https://finance.yahoo.com/news/exclusive-supabase-raises-200-million-112154867.html&quot;&gt;D轮2亿美元&lt;/a&gt;&quot;Timescale：&lt;a href=&quot;https://www.tigerdata.com/blog/year-of-the-tiger-110-million-to-build-the-future-of-data-for-developers-worldwide&quot;&gt;C轮1.1亿美元&lt;/a&gt;&quot;Supabase：&lt;a href=&quot;https://supabase.com/blog/supabase-series-e&quot;&gt;E轮1亿美元&lt;/a&gt;&quot;Astronomer：&lt;a href=&quot;https://www.astronomer.io/press-releases/astronomer-secures-93-million-series-d-funding/&quot;&gt;D轮9300万美元&lt;/a&gt;&quot;Tessel：&lt;a href=&quot;https://www.tessell.com/press-releases/tessell-raises-60m-series-b-to-expand-ai-driven-multi-cloud-data-ecosystems&quot;&gt;B轮6000万美元&lt;/a&gt;&quot;LanceDB：&lt;a href=&quot;https://lancedb.com/blog/series-a-funding/&quot;&gt;A轮3000万美元&lt;/a&gt;&quot;Convex：&lt;a href=&quot;https://news.convex.dev/convex-raises-24m/&quot;&gt;B轮2400万美元&lt;/a&gt;&quot;SpiralDB：&lt;a href=&quot;https://www.axios.com/pro/enterprise-software-deals/2025/09/11/database-startup-spiral-22-million&quot;&gt;A轮2200万美元&lt;/a&gt;&quot;ParadeDB：&lt;a href=&quot;https://techcrunch.com/2025/07/15/paradedb-takes-on-elasticsearch-as-interest-in-postgres-explodes-amid-ai-boom/&quot;&gt;A轮1200万美元&lt;/a&gt;&quot;CedarDB：&lt;a href=&quot;https://www.munich-startup.de/en/109750/cedardb-secures-53-million-euros/&quot;&gt;种子轮590万美元&lt;/a&gt;&quot;TopK：&lt;a href=&quot;https://www.topk.io/blog/seed-round&quot;&gt;种子轮550万美元&lt;/a&gt;&quot;Columnar：&lt;a href=&quot;https://columnar.tech/blog/announcing-columnar&quot;&gt;种子轮400万美元&lt;/a&gt;&quot;SereneDB：&lt;a href=&quot;https://tech.eu/2025/12/03/serenedb-lands-21m-to-fuse-search-analytics-and-postgres-into-one-engine/&quot;&gt;前种子轮210万美元&lt;/a&gt;&quot;Starburst：&lt;a href=&quot;https://www.prnewswire.com/news-releases/starburst-announces-strategic-investment-from-citi-302456950.html&quot;&gt;未披露？&lt;/a&gt;&quot;TurboPuffer：&lt;a href=&quot;https://tpuf.link/comms&quot;&gt;未披露？&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;名称变更&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是我在年度总结中新增加的一个类别——数据库公司更改其公司或系统的名称。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.dbta.com/Editorial/News-Flashes/HarperDBs-Rebrand-Reflects-its-Commitment-to-Delivering-a-Full-Stack-Application-Delivery-Platform-168390.aspx&quot;&gt;HarperDB → Harper&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这家JSON数据库公司从名字里去掉了后缀&quot;DB&quot; ，旨在强调其作为数据库支持型应用平台的定位，类似于&lt;a href=&quot;https://www.convex.dev/&quot;&gt;Convex&lt;/a&gt;&quot;和Heroku。我很欣赏Harper的团队。2021年，他们在&lt;a href=&quot;https://www.youtube.com/watch?v=I5_xIs6xsJQ&amp;amp;list=PLSE8ODhjZXjbeqnfuvp30VrI7VXiFuOXS&amp;amp;index=7&quot;&gt;CMU-DB技术研讨会&lt;/a&gt;&quot;上提出的数据库管理系统构想可以说是我听过的最糟糕的方案。好在他们意识到该方案的缺陷后果断放弃，转而采用了LMDB技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.geldata.com/blog/edgedb-is-now-gel-and-postgres-is-the-future&quot;&gt;EdgeDB → Gel&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是一个明智的举动，因为“Edge”这个名字传达了这样一个信息，它是一个用于边缘设备或服务的数据库（如&lt;a href=&quot;http://fly.io/&quot;&gt;Fly.io&lt;/a&gt;&quot;）。不过我也不确定“Gel”是否传达了项目更高层次的目标。感兴趣的读者可以观看下他们在&lt;a href=&quot;https://www.youtube.com/watch?v=RzLo-pdUJ7I&amp;amp;list=PLSE8ODhjZXjbpOIrZheFWxkYG8HD87xW1&amp;amp;index=10&quot;&gt;2025年CMU-DB技术研讨会上关于Gel查询语言（名称还是EdgeQL）的讲座&lt;/a&gt;&quot;，由CMU博士校友主讲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.tigerdata.com/blog/timescale-becomes-tigerdata&quot;&gt;Timescale → TigerData&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;数据库公司为区别于其核心数据库产品而更名的案例实属罕见。通常情况是公司更名为数据库名称（如“Relational Software, Inc.”更名为“Oracle Systems Corporation”，“10gen, Inc.”更名为“MongoDB, Inc.”）。该公司有了新的定位——通用应用场景的增强版PostgreSQL，因此他们试图摆脱“专业化时间序列数据库管理系统”的固有印象，这一策略有它的合理性，毕竟前者所处的细分市场远小于后者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;倒闭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;坦白说，我曾在其中两家失败的初创公司中担任技术顾问。截止目前，我的顾问成功率可以说是惨不忍睹。我也曾担任&lt;a href=&quot;https://dbdb.io/db/splice-machine&quot;&gt;Splice Machine&lt;/a&gt;&quot;公司的顾问，但该公司已于2021年倒闭。需要说明的是，我只和他们讨论技术构想，而不涉及商业策略。我确实建议Fauna增加SQL支持功能，但他们没有采纳我的建议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoworld.com/article/3853569/fauna-to-shut-down-faunadb-service-in-may.html&quot;&gt;Fauna&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Spanner是一款颇具特色的分布式数据库管理系统，基于&lt;a href=&quot;https://www.cs.umd.edu/~abadi/&quot;&gt;Dan Abadi&lt;/a&gt;&quot;的&lt;a href=&quot;https://vldb.org/pvldb/vol3/R06.pdf&quot;&gt;确定性并发控制研究&lt;/a&gt;&quot;。恰好在NoSQL热潮逐渐消退之际，它提供了强一致性事务处理能力，使事务处理功能再度成为焦点。不过该系统采用&lt;a href=&quot;https://faunadb-docs.netlify.app/fauna/current/learn/query/&quot;&gt;专有查询语言&lt;/a&gt;&quot;，并押注了GraphQL技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/postgresml/postgresml/issues/1688#issuecomment-3041057338&quot;&gt;PostgresML&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;从名字就可以看出来，该系统旨在使人们能够在他们的PostgreSQL DBMS内运行ML/AI操作。挑战在于，他们需要说服人们将现有的数据库迁移到他们提供的托管平台上。他们推出了&lt;a href=&quot;https://github.com/postgresml/pgcat&quot;&gt;pgCat&lt;/a&gt;&quot;，作为一个代理用于镜像数据库流量。其中一位联合创始人加入了Anthropic。另一位联合创始人创建了一个新的代理项目&lt;a href=&quot;https://pgdog.dev/&quot;&gt;pgDog&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/DERBY-7177&quot;&gt;Derby&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是最早用Java编写的数据库管理系统之一，可以追溯到1997年（最初名为&quot;Java DB&quot;或&quot;JBMS&quot;）。2000年代，IBM将其捐赠给Apache基金会，并更名为Derby。2025年10月，该项目宣布这个系统将进入“只读模式”，因为没有人对它进行积极地维护了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.hydra.so/&quot;&gt;Hydra&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;尽管没有关于初创公司DuckDB-inside-Postgres的官方公告，但其联合创始人和员工都已经分散到了其他公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/MyScaleDB/status/1917163010311037327&quot;&gt;MyScaleDB&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这是Clickhouse的一个分支，借助Tantivy增加了向量搜索和全文索引。他们在2025年5月宣布关闭这项服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/aocsa_as-some-of-you-may-have-seen-voltron-data-activity-7395229870517022720-sGOP/&quot;&gt;Voltron Data&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;这个团队应该是数据库公司里的超级组合。想象一下，就像&lt;a href=&quot;https://youtu.be/G-S9mtYowPY&quot;&gt;Run the Jewels&lt;/a&gt;&quot;那样的团队。他们有来自Nvidia Rapids的顶级工程师、&lt;a href=&quot;https://en.wikipedia.org/wiki/Wes_McKinney&quot;&gt;Apache Arrow和Python Pandas的发明者&lt;/a&gt;&quot;，以及来自&lt;a href=&quot;https://github.com/BlazingDB/blazingsql&quot;&gt;BlazingSQL&lt;/a&gt;&quot;的秘鲁GPU奇才。然后再加上来自顶级公司的风险投资1.1亿美元，包括未来的英特尔CEO（以及&lt;a href=&quot;https://en.wikipedia.org/wiki/Lip-Bu_Tan&quot;&gt;一名CMU的董事会成员&lt;/a&gt;&quot;）。他们构建了一个GPU加速的数据库&lt;a href=&quot;https://arxiv.org/abs/2508.05029&quot;&gt;Theseus&lt;/a&gt;&quot;，但未能及时推出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，尽管不是一个商业机构，但如果不提及&lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_Research#Almaden_in_Silicon_Valley&quot;&gt;IBM阿尔马登研究中心&lt;/a&gt;&quot;的&lt;a href=&quot;https://www.siliconvalley.com/2025/07/10/ibm-san-jose-tech-data-ai-internet-property-real-estate-economy-web/&quot;&gt;关闭&lt;/a&gt;&quot;，那将是我的疏忽。这个研究中心是IBM在1986年建立的，几十年来一直是数据库研究的圣地。&lt;a href=&quot;https://twitter.com/andy_pavlo/status/306455280823177216&quot;&gt;我2013年曾去阿尔马登参加面试&lt;/a&gt;&quot;，发现那里的风景很美。IBM研究中心数据库小组&lt;a href=&quot;https://dl.acm.org/doi/10.1145/126482.126493&quot;&gt;已经不是过去的样子了&lt;/a&gt;&quot;。尽管如此，这个神圣的数据库研究场所的校友名单依然令人印象深刻：&lt;a href=&quot;https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)&quot;&gt;Rakesh Agrawal&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Donald_D._Chamberlin&quot;&gt;Donald Chamberlin&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Ronald_Fagin&quot;&gt;Ronald Fagin&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Laura_M._Haas&quot;&gt;Laura Haas&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/C._Mohan&quot;&gt;Mohan&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Patricia_Selinger&quot;&gt;Pat Selinger&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Moshe_Vardi&quot;&gt;Moshe Vardi&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Jennifer_Widom&quot;&gt;Jennifer Widom&lt;/a&gt;&quot;和&lt;a href=&quot;https://scholar.google.com/citations?user=wUkamYwAAAAJ&amp;amp;hl=en&quot;&gt;Guy Lohman&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026-01-05更新：我遗漏了Gel在2025年12月被Vercel收购的消息。[&lt;a href=&quot;https://www.geldata.com/blog/gel-joins-vercel&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;2026-01-05更新：我也遗漏了Supabase在2025年进行了两轮融资的消息。&lt;/p&gt;&lt;p&gt;2026-01-05更新：尽管TurboPuffer没有就融资发表官方声明，但他们的CEO提到，其团队中增加了来自Thrive Capital的成员。[&lt;a href=&quot;https://www.linkedin.com/in/julianlaneve&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;2026-01-05更新：显然，我需要一个更好的方法来跟踪融资信息，因为我还遗漏了LanceDB的A轮融资！[&lt;a href=&quot;https://twitter.com/brittwalker_/status/2008306941286904111&quot;&gt;致谢&lt;/a&gt;&quot;]&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有人说，我是根据数据库开发公司筹集的资金数额来判断数据库的质量，显然不是这样。我之所以追踪这些动态，是因为数据库研究领域竞争激烈且充满活力。我不仅要与其他高校的学者“竞争”，还需要持续关注大型科技公司和小型创业公司推出的有趣的系统。行业研究实验室已经不是过去的样子了，只有微软研究院仍然在积极招聘顶尖人才，并做出令人难以置信的工作。&amp;nbsp;我曾&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2022/12/2022-databases-retrospective.html#:~:text=fate%20of%20database%20start%2Dups&quot;&gt;在2022年预测&lt;/a&gt;&quot;，2025年将有大量的数据库公司倒闭。确实，今年关闭的公司比往年多，但并没有达到我预期的规模。&amp;nbsp;Voltron的倒闭以及类似HeavyDB这样的收购兼并似乎延续了GPU加速数据库不可行的趋势。&lt;a href=&quot;https://twitter.com/KineticaHQ/status/1988983193870156171&quot;&gt;Kinetica&lt;/a&gt;&quot;多年来一直靠政府合同维持运营，而&lt;a href=&quot;https://sqream.com/&quot;&gt;Sqream&lt;/a&gt;&quot;似乎也是在勉强支撑。这些公司仍属于小众领域，至今无人能撼动CPU驱动型DBMS的主导地位。虽不便透露具体厂商的名字，但2026年必将有多家供应商发布GPU加速数据库的重要公告。这进一步印证了OLAP引擎的商品化趋势：现代系统的运行速度已经实现了飞跃，底层操作（扫描、连接）的性能差异微乎其微，系统间的差异化竞争正转向用户体验以及优化器生成的查询计划的质量。&amp;nbsp;Couchbase和SingleStore被私募股权（PE）公司收购可能预示着数据库行业未来的一个发展趋势。当然，PE收购以前也发生过，但似乎都是在最近：（1）&lt;a href=&quot;https://www.vectorcapital.com/investments/case-study/marklogic&quot;&gt;MarkLogic&lt;/a&gt;&quot;在2020年、（2）&lt;a href=&quot;https://techcrunch.com/2021/06/01/cloudera-to-go-private-as-kkr-cdr-grab-it-for-5-3b/&quot;&gt;Cloudera&lt;/a&gt;&quot;在2021年、（3）&lt;a href=&quot;https://techcrunch.com/2024/09/10/mariadb-goes-private-with-new-ceo-as-k1-closes-acquisition/&quot;&gt;MariaDB&lt;/a&gt;&quot;在2023年。我能找到的发生在2020年之前的收购只有2007年的&lt;a href=&quot;https://www.channelinsider.com/tech-companies/ibm-buys-database-software-firm/&quot;&gt;SolidDB&lt;/a&gt;&quot;和2015年的&lt;a href=&quot;https://www.aakashg.com/story-informatica-second-ipo/&quot;&gt;Informatica&lt;/a&gt;&quot;。PE收购可能会逆转那些数据库公司的发展趋势，它们在被控股公司收购后发展陷入停滞，而那些控股公司则通过榨取维护费持续获利（如Actian、Rocket）。即使是Oracle，也依然在从30年前收购的&lt;a href=&quot;https://www.oracle.com/database/technologies/related/rdb.html&quot;&gt;RDB/VMS&lt;/a&gt;&quot;上获利！&amp;nbsp;最后，向&lt;a href=&quot;https://www.linkedin.com/in/nikitashamgunov&quot;&gt;Nikita Shamgunov&lt;/a&gt;&quot;致敬。据我所知，他是唯一一位与人联合创立两家数据库公司（&lt;a href=&quot;https://hackernoon.com/founder-interviews-nikita-shamgunov-of-memsql-8a9ca8d33552&quot;&gt;SingleStore&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.madrona.com/building-a-modern-database-neon-nikita-shamgunov-serverless-postgres/&quot;&gt;Neon&lt;/a&gt;&quot;）且两家公司在同一年被收购的人。就像已故说唱歌手DMX在一年内推出两张冠军专辑（&lt;a href=&quot;https://en.wikipedia.org/wiki/It%27s_Dark_and_Hell_Is_Hot&quot;&gt;It&#39;s Dark and Hell Is Hot&lt;/a&gt;&quot;、&lt;a href=&quot;https://en.wikipedia.org/wiki/Flesh_of_My_Flesh,_Blood_of_My_Blood&quot;&gt;Flesh of My Flesh&lt;/a&gt;&quot;）那样，我认为短期内无人能打破Nikita的纪录。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据库元老的表现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看看数据库元老拉里·埃里森的辉煌之年。这位81岁的老人在这一年间取得的成就，远超常人毕生所为。我将按时间顺序逐一梳理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拉里年初时位列全球富豪榜第三。想到自己身价可能不及马克·扎克伯格，他夜不能寐。有人说拉里的失眠源于饮食变化——自从&lt;a href=&quot;https://www.bbc.com/news/uk-england-oxfordshire-67221202&quot;&gt;买下英国的一家著名酒吧&lt;/a&gt;&quot;后，他馅饼吃多了。但我可以向各位保证，拉里坚持三十年的“&lt;a href=&quot;https://tech.yahoo.com/science/articles/80-old-billionaire-larry-ellison-105236014.html&quot;&gt;素食水瓶座饮食法&lt;/a&gt;&quot;”从未改变。直到2025年4月，我们得知拉里&lt;a href=&quot;https://www.msn.com/en-in/autos/photos/larry-ellison-becomes-second-richest-person-beats-zuckerberg-bezos-after-oracle-stock-soars/ar-AA1GKdbu&quot;&gt;重登全球富豪榜次席&lt;/a&gt;&quot;。他的睡眠质量稍有好转，但仍然远未达标。生活中的诸多烦忧仍在持续地折磨他——比如他终于决定出售那辆稀有的半合法&lt;a href=&quot;https://www.forbes.com/sites/maryroeloffs/2025/08/05/larry-ellisons-old-mclaren-f1-could-break-a-sales-record/&quot;&gt;迈凯伦F1超跑&lt;/a&gt;&quot;，车内手套箱里还完好地保存着原厂车主手册。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年7月，拉里在13年内发布了他的&lt;a href=&quot;https://twitter.com/larryellison/status/1945229587929337947&quot;&gt;第三条推文&lt;/a&gt;&quot;（拉里迷们称之为“#3”）。这条推文介绍了他在牛津大学附近创立的&lt;a href=&quot;https://eit.org/&quot;&gt;埃里森技术研究院&lt;/a&gt;&quot;（EIT）的近况。以EIT命名且与牛津大学关联，听起来像是纯研究性的非营利机构，类似于斯坦福的&lt;a href=&quot;https://en.wikipedia.org/wiki/SRI_International&quot;&gt;SRI&lt;/a&gt;&quot;或卡内基梅隆的&lt;a href=&quot;https://en.wikipedia.org/wiki/Software_Engineering_Institute&quot;&gt;SEI&lt;/a&gt;&quot;。但实际情况是，这是一家总部位于加州的有限责任公司旗下的多家营利性公司的统称。当然，不少怪咖在第3条的评论区说承诺提供&lt;a href=&quot;https://twitter.com/SFCryptoRounder/status/1946047224779030564&quot;&gt;基于区块链的低温冷冻技术&lt;/a&gt;&quot;或&lt;a href=&quot;https://twitter.com/JackSarfatti/status/1975985052204101709&quot;&gt;室温超导体&lt;/a&gt;&quot;。拉里告诉我他根本不理会这些。不过也有人像&lt;a href=&quot;https://twitter.com/aseemchandra/status/1945509650201301304&quot;&gt;这位网友&lt;/a&gt;&quot;一样真正理解其中的奥妙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年（可能是本世纪）最大的数据库新闻出现在9月10日星期三美国东部时间大约下午3:00。经过几十年的等待，拉里·约瑟夫·埃里森终于&lt;a href=&quot;https://www.theguardian.com/technology/2025/sep/10/larry-ellison-dislodges-elon-musk-as-worlds-richest-person&quot;&gt;成了世界上最富有的人&lt;/a&gt;&quot;。那天早上，&lt;a href=&quot;https://finance.yahoo.com/quote/ORCL/&quot;&gt;$ORCL&lt;/a&gt;&quot;的股价上涨了40%，由于拉里仍然拥有公司40%的股份，所以他的总身价估计是&lt;a href=&quot;https://www.bbc.com/news/articles/cx2rp992y88o&quot;&gt;3930亿美元&lt;/a&gt;&quot;。从这个角度来看，这不仅使拉里成为世界上最富有的人，而且也是整个人类历史上最富有的人。约翰·D·洛克菲勒和安德鲁·卡内基（是的，CMU中的“C”）的峰值净资产，根据通货膨胀调整后，分别只有&lt;a href=&quot;https://www.buysidedigest.com/insights/the-top-10-wealthiest-historical-figures-adjusted-for-inflation/&quot;&gt;3400亿美元&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.celebritynetworth.com/richest-businessmen/richest-billionaires/andrew-carnegie-net-worth/&quot;&gt;3100亿美元&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在拉里登上世界之巅的同时，Oracle还参与了&lt;a href=&quot;https://www.npr.org/2025/12/18/nx-s1-5648844/tiktok-deal-oracle-trump&quot;&gt;收购控制TikTok的美国公司&lt;/a&gt;&quot;，拉里&lt;a href=&quot;https://variety.com/2025/tv/news/paramount-skydance-larry-ellison-irrevocable-personal-guarantee-warner-bros-discovery-1236614728/&quot;&gt;资助派拉蒙&lt;/a&gt;&quot;（由他第四次婚姻的儿子控制）&lt;a href=&quot;https://www.nytimes.com/2025/12/24/business/media/larry-david-ellison-warner-bros-discovery-cbs.html&quot;&gt;竞购华纳兄弟&lt;/a&gt;&quot;。美国总统甚至嘲笑拉里&lt;a href=&quot;https://www.theguardian.com/us-news/2025/nov/20/warner-bros-discovery-takeover-paramount-skydance-larry-ellison&quot;&gt;接管CNN新闻部门&lt;/a&gt;&quot;，因为拉里是派拉蒙的大股东。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Andy的观点&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我甚至不知道从哪里开始。当然，当我得知拉里·埃里森因数据库而成为世界上最富有的人时，我感到&lt;a href=&quot;https://twitter.com/andy_pavlo/status/1965865919223312495&quot;&gt;由衷地欣慰&lt;/a&gt;&quot;，我们的生活终于发生了一些积极的事情。我不在乎Oracle的股价，因为那些旨在构建AI数据中心而非传统软件业务的&lt;a href=&quot;https://www.investors.com/news/technology/oracle-stock-orcl-ai-analyst-targets/&quot;&gt;高调交易&lt;/a&gt;&quot;而被人为炒高了。我也不在乎他&lt;a href=&quot;https://www.bloomberg.com/news/articles/2025-11-21/oracle-slump-sends-ellison-sliding-down-ranks-of-world-s-richest&quot;&gt;两个月内个人损失1300亿美元&lt;/a&gt;&quot;导致排名下滑。这就像你我把一个月的薪水&lt;a href=&quot;https://www.reddit.com/r/gambling/comments/1j4xby2/blew_my_whole_paycheck/&quot;&gt;全砸在了FortuneCoins上&lt;/a&gt;&quot;——虽然有点心疼，还得靠从Taco Bell买来的过期辣酱拌豆子米饭撑两周，但总会好起来的。&amp;nbsp;有些人说拉里与普通民众&lt;a href=&quot;https://news.ycombinator.com/item?id=45413203&quot;&gt;脱节&lt;/a&gt;&quot;，或者说他因为参与和数据库无直接关系的事情而迷失了方向。他们列举了多个例子，比如他&lt;a href=&quot;https://techcrunch.com/2025/02/23/the-lesson-of-larry-ellisons-misadventures-in-farming/&quot;&gt;在夏威夷的机器人农场&lt;/a&gt;&quot;以每磅24美元（每公斤41欧元）的价格&lt;a href=&quot;https://beatofhawaii.com/the-most-expensive-lettuce-in-hawaii-billionaire-larry-ellisons-24-lb-experiment/&quot;&gt;出售生菜&lt;/a&gt;&quot;，又比如81岁的男人不可能&lt;a href=&quot;https://assets.sfstandard.com/image/994911177489/image_cooaesgkll0v99j57e84lobk7k/-S3840x2560-FPNG&quot;&gt;天生拥有金发&lt;/a&gt;&quot;。&amp;nbsp;事实是，拉里·埃里森已经征服了企业级数据库领域、&lt;a href=&quot;https://sg.finance.yahoo.com/news/why-oracle-founder-larry-ellison-205016907.html&quot;&gt;竞技帆船&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.businessinsider.com/larry-ellison-hawaii-wellness-spa-sensei-lanai-photos-2021-2&quot;&gt;科技兄弟健康水疗中心&lt;/a&gt;&quot;。下一步显然是接管一个每天被成千上万在机场等待的人观看的有线电视频道。每次我和拉里交谈，他都清楚地表明他一点也不在乎人们对他的看法。他知道&lt;a href=&quot;https://twitter.com/HolgersenTobias/status/1945239198572712323&quot;&gt;他的粉丝爱他&lt;/a&gt;&quot;。&lt;a href=&quot;https://www.financialexpress.com/life/lifestyle-who-is-jolin-zhu-worlds-richest-man-larry-ellisons-fifth-wife-47-years-younger-than-him-3974373/&quot;&gt;他的（新）妻子爱他&lt;/a&gt;&quot;。毕竟，那才是最重要的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在结束本次回顾之前，我想快速地说出几个名字并提点建议。首先是PT，他在监禁期间仍在&lt;a href=&quot;https://turso.tech/blog/working-on-databases-from-prison&quot;&gt;有条不紊地参与Turso数据库的开发&lt;/a&gt;&quot;（外面见）。然后是对JT的遭遇表示遗憾，他因为经常在社交媒体上分享与&lt;a href=&quot;https://github.com/KevoDB/kevo&quot;&gt;KevoDB&lt;/a&gt;&quot;数据库开发有关的信息而&lt;a href=&quot;https://twitter.com/canoozie/status/1952305339824574576&quot;&gt;丢掉了工作&lt;/a&gt;&quot;。务必只在测试用数据库中放入假数据，&lt;a href=&quot;https://abcnews.go.com/Business/charlie-javice-founder-lied-175m-startup-faces-sentencing/story?id=126034577&quot;&gt;不要因为以1750万美元的价格出售自己的初创公司&lt;/a&gt;&quot;换得七年的监禁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我和我的博士生们也成立了一家新的&lt;a href=&quot;https://sydht.ai/&quot;&gt;初创公司&lt;/a&gt;&quot;。希望很快就能有更多的信息带给大家。一言为定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;声明：本文为InfoQ翻译，未经许可禁止转载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href=&quot;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html&quot;&gt;https://www.cs.cmu.edu/~pavlo/blog/2026/01/2025-databases-retrospective.html&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Zbb25ejwoK2xQSHHcTB3</link><guid isPermaLink="false">https://www.infoq.cn/article/Zbb25ejwoK2xQSHHcTB3</guid><pubDate>Tue, 10 Feb 2026 09:40:42 GMT</pubDate><author>Andy Pavlo</author><category>数据湖仓</category></item><item><title>在参与OpenAI、Google、Amazon的50个AI项目后，他们总结出了大多数AI产品失败的原因</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助Coding Agent等工具，如今构建一个AI产品的技术门槛和启动成本已急剧降低。一夜之间，将想法变为可交互的原型变得前所未有的容易。但一个刺眼的矛盾也随之浮现：大多数AI产品仍在走向失败。如果技术实现不再是瓶颈，那么问题究竟出在哪里？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aishwarya Naresh Reganti 和Kiriti Badam 曾在 OpenAI、Google、Amazon、Databricks 等公司参与构建并成功推出了 50 多个企业级 AI 产品。最近，他们在播客节目中，与主持人Lenny细致分享了当前AI产品开发中的常见陷阱与成功路径。基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;今天构建的成本已经非常低了，真正昂贵的是设计，是你是否真正想清楚了产品要解决什么痛点。对问题本身和产品设计的执着，是被低估的，而单纯追求“快点做出来”，是被高估的。AI 不是答案，而是解决问题的工具。领导者需要重新回到“亲自上手”的状态，并不是要他们亲自实现系统，而是为了重建判断力，接受“我的直觉可能不再完全正确”这一事实。“忙碌但无效”的工作时代正在结束，你不可能再躲在角落里做对公司没有实质影响的事，而必须思考端到端的流程，以及如何创造更大的影响。在这个数据随时告诉你“你大概率会失败”的时代，保留一点愚蠢的勇气很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI产品构建中的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：目前AI 产品构建的情况是怎样的？哪些进展顺利，哪些地方问题依旧明显？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：首先，怀疑态度明显减少。2024年还有很多领导者认为 AI 可能只是又一波“加密货币式”的泡沫，因此迟迟不愿真正投入。当时我看到的很多所谓“AI 用例”，更像仅仅是“在你自己的数据上套一层 Snapchat 滤镜”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而2025年，很多公司开始真正反思用户体验和业务流程，逐渐意识到：如果想构建成功的 AI 产品，必须先拆解现有流程，再重新构建。而消极的一面在于，执行依然非常混乱。这个领域只有三年左右的历史，没有成熟的方法论，也没有教材，大家基本都是边走边学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，AI 产品的生命周期与传统软件截然不同。这导致了以往在PM、工程师、数据团队之间形成的分工被打破。过去，PM、工程师各自优化各自的指标；现在，大家可能需要坐在同一间会议室里，一起看 agent 的执行轨迹，共同决定产品应该如何表现。这种协作更紧密，也更复杂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你之前说构建 AI 产品与构建非 AI 产品本质上非常不同，能具体谈谈吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：构建 AI 系统和传统软件系统之间确实存在大量相似之处，但也有一些根本性的差异，足以改变你构建产品的方式。其中一个经常被忽视的核心差异，是“非确定性”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与传统软件相比，你几乎是在与一个非确定性的 API 打交道。在传统软件中，决策引擎和流程往往是清晰、可预测的。以 Booking.com 为例：你有一个明确意图，比如在旧金山订两晚酒店，系统通过一系列按钮、选项和表单，把你的意图转化为具体操作，最终完成目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在 AI 产品中，这一层被一种高度流动的、以自然语言为主的界面所取代。用户可以用无数种方式表达同一个意图，这意味着你无法预判用户的输入行为。而在输出端，你面对的是一个概率性的、非确定性的 LLM，它对提示词极其敏感，本质上还是一个黑箱。你既无法完全预测用户会如何使用产品，也无法确定模型会给出怎样的回应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，你同时面对输入、输出和中间过程三方面的不确定性，只能在有限理解的基础上去预判行为并进行设计。到了 Agent 系统，这种复杂性会进一步放大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也引出了第二个关键差异：代理性与控制权之间的权衡。很多人执着于构建高度自治的系统，希望 Agent 能替人完成所有工作。但每当你把决策权交给 AI，你就必然放弃一部分控制权。因此，只有当系统足够可靠、足以赢得信任时，才值得赋予它更高的自治能力。这正是“代理性—控制权权衡”的核心：自治越高，控制越少，而信任必须通过时间和表现来积累。&lt;/p&gt;&lt;p&gt;Kiriti：类比登山：如果你的目标是攀登一座高峰，你不会第一天就直接冲顶，而是先进行基础训练，逐步提升能力，最终才接近目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;构建 AI 产品也是如此。你不应该在第一天就打造一个拥有公司全部工具和上下文的全能 Agent，并期待它能正常工作。正确的做法，是刻意从影响范围小、人工控制强的场景开始，逐步理解当前能力边界，再慢慢增加自治性、减少人工干预。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这样做的好处在于，你会逐渐建立信心，清楚 AI 能解决问题的哪一部分，以及接下来需要引入哪些上下文和工具来改进体验。好的一面是，你不必一开始就面对复杂而炫目的 Agent 体系；挑战在于，你必须接受“循序渐进”的现实。但几乎所有成功的案例，都是从极简结构起步，再不断演化而来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你们一直强调“从低自治、高控制开始”，再逐步升级。能否用一个具体例子说明这种路径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：客户支持是一个非常典型的场景。我们在发布产品时也经历过类似情况，随着新功能上线，支持请求会突然激增，而且问题类型非常多样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一开始，并不是把所有支持中心文章一股脑塞进 Agent 就完事了。更合理的第一步，是让 AI 为人工客服提供建议，由人类判断哪些建议是有用的、哪些是无效的。通过这个反馈回路，你可以识别系统的盲点并进行修正。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你建立起足够信心后，才可以让 AI 直接向用户展示答案。接着，再逐步增加复杂能力，例如自动退款、创建功能请求等。如果在第一天就把这些能力全部交给 Agent，系统复杂度会迅速失控。因此，我们始终建议按阶段构建，逐步提升自治水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：一开始是高控制、低自治，AI 只给建议，最终决策仍由人来做；当系统被验证可靠后，逐渐赋予更多自治权，同时减少人工干预。只要这一阶段进展顺利，就可以继续向前推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：从更宏观的角度看，AI 系统的核心在于“行为校准”。你几乎不可能在一开始就准确预测系统行为，因此关键在于避免破坏用户体验和信任。做法是，在不影响体验的前提下，逐步减少人工控制，并以不同方式约束自治边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以医疗保险预授权为例，某些低风险项目，比如血液检测或 MRI，只要患者信息齐全，就可以由 AI 自动审批；而高风险项目，如侵入性手术，则必须保留人工审核。在这个过程中，你还需要持续记录人类的决策行为，构建反馈飞轮，用于不断优化系统。这样既不会损害用户体验，也不会削弱信任，同时还能让系统持续进化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你还给出过一些很好的分阶段示例，比如Coding Agent：第一阶段只做行内补全和样板代码建议；第二阶段生成测试或重构代码供人审查；第三阶段则可以自动提交 PR。营销助手也是类似路径：从文案草稿，到完整活动执行，再到自动 A/B 测试和跨渠道优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：换个角度看，这种非确定性其实也是 AI 最迷人的地方。相比点击复杂的按钮，人类更习惯用语言交流，这大大降低了使用门槛。但问题在于，人类表达意图的方式极其多样，而你往往需要在非确定性的技术之上，达成确定性的业务结果，这正是复杂性的来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：所以，当人们一上来就想直接跳到第三阶段，往往会陷入困境：系统既难以构建，也不可靠，最终只能被判定为失败。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：在达到高度自治之前，你需要对系统能力建立足够信心。如果一开始就从错误的切入点出发，你会面对成百上千种错误，却根本无从修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从小规模、低自治开始，不仅降低风险，也会迫使你认真思考“我要解决的到底是什么问题”。在 AI 快速发展的环境下，人们很容易沉迷于复杂解法，而忽视真正的问题本身。通过逐步提高自治层级，你可以清晰地拆解问题，并为未来扩展做好准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我最近读到一篇研究指出，约75% 的企业认为“可靠性”是他们在 AI 项目中面临的最大问题，这也是他们迟迟不敢将 AI 产品直接面向用户的重要原因。正因如此，目前很多 AI 产品更多集中在提升生产力，而不是彻底替代端到端流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：在这期节目之前，我们还录了一期，专门深入讨论了提示注入（prompt injection）和越狱（jailbreaking）。在那期讨论里，我们意识到这对 AI 产品来说几乎是一个“生存级风险”：它可能既没有成熟解法，甚至在理论上也很难被彻底解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：一旦 AI 系统真正进入主流应用，这会成为一个非常严重的问题。现在大家还忙着把 AI 产品做出来，很少有人认真对待安全性，但这迟早会爆发。尤其是在面对非确定性 API 的情况下，你几乎无法完全防范。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：我们当时聊到的一个核心问题是：要诱导 AI 去做“不该做的事”，其实并不难。虽然大家都在构建各种护栏系统，但事实证明，这些护栏并不牢靠，总能被绕过。而正如你所说，当 Agent 越来越自治、甚至进入机器人系统时，这种风险会被成倍放大，确实让人感到不安。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我同意这是一个真实存在的问题。不过从当前 AI 在企业中的采用阶段来看，大多数公司甚至还没真正走到能充分获益的程度。2025 年确实是 AI Agent 和企业尝试落地 AI 的一个高峰期，但整体渗透率依然不高，很多流程还远未被真正改造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种情况下，只要在关键节点引入“人在回路”（human-in-the-loop），其实可以规避相当一部分风险。我个人更偏向乐观的一侧：与其一开始就被潜在的负面场景吓退，不如先尝试去落地、去使用。我们在 OpenAI 接触过的企业中，几乎没有人会说“AI 在这里完全帮不上忙”，更多是发现它能在某些具体环节上带来优化，然后再思考如何逐步采用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有哪些成功构建 AI 产品的模式和工作方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我们合作过的成功公司，通常都具备三个维度：优秀的领导者、健康的文化，以及持续推进的技术能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是领导者。我们参与过不少企业的 AI 转型、培训和战略制定。很多领导者过去十到十五年积累的直觉，正是他们成功的基础，但在 AI 出现之后，这些直觉往往需要被重新学习。领导者必须愿意承认这一点，甚至需要一定程度的“脆弱感”。我曾和 Rackspace 现任 CEO Gajen 共事。他每天清晨都会预留一个固定时段，专门用来“补课 AI”——听播客、看最新资料，甚至在周末做白板推演。领导者需要重新回到“亲自上手”的状态，并不是要他们亲自实现系统，而是为了重建判断力，接受“我的直觉可能不再完全正确”这一事实。很多真正成功的团队，正是从这种自上而下的转变开始的。AI 几乎不可能靠纯粹的自下而上推动，如果领导层对技术缺乏信任，或者对能力边界有误判，整个组织都会受限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个维度是文化。在传统企业中，AI 往往不是核心业务，但因为竞争对手在用、因为确实存在可行用例，企业不得不引入 AI。在这个过程中，恐慌文化非常常见，比如“FOMO”“你会被 AI 取代”等说法。问题在于，真正做出好 AI 产品，极度依赖领域专家；但很多专家却拒绝参与，因为他们担心自己的岗位被替代。这时，领导者需要建立一种“赋能型文化”，强调 AI 是用来增强个人能力、放大产出的工具，而不是威胁。只有这样，组织才会形成合力，而不是人人自危。事实上，AI 往往会创造更多机会，让员工做更多、更高价值的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第三个维度才是技术本身。成功的团队通常对自身工作流有近乎执念般的理解，清楚哪些环节适合 AI，哪些地方必须有人参与。几乎不存在“一个 AI Agent 解决一切”的情况。通常是机器学习模型负责一部分，确定性代码负责另一部分。因此，关键不在于迷信技术，而在于为每个问题选择合适的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，这些团队也非常清楚自己在和一个非确定性的 API 打交道，因此会以完全不同的节奏推进开发。他们迭代得非常快，但前提是不破坏用户体验，同时快速建立反馈飞轮。如今的竞争焦点，并不是谁最早上线 Agent，而是谁最早构建起持续改进的机制。凡是有人告诉我，“一个Agent，两三天就能在你系统里跑出显著收益”，我都会非常怀疑。这不是模型能力的问题，而是企业数据和基础设施本身就极其混乱。大量技术债、混乱的接口和命名方式，都需要时间去消化。真正能产生显著 ROI，通常至少需要四到六个月，即便你拥有最好的数据和基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有些人认为评测（eval）是解决 AI 问题的关键，有些人则觉得它被严重高估，只要“感觉对了”就行。你们怎么看 eval？它在多大程度上真的能解决你们提到的那些问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我觉得大家陷入了一种错误的二元对立：要么eval能解决一切，要么线上监控能解决一切。eval本质上，是把你对产品的理解、你的价值判断，编码进一组数据集：什么是重要的，什么是绝对不能发生的。而生产环境监控，则是在产品上线后，通过关键指标和用户行为，反馈真实使用情况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种监控并不新鲜，但在 AI Agent 场景下，颗粒度变得更细了。除了显式反馈，比如点赞、点踩，还有大量隐式信号。例如用户不点踩，但反复要求重新生成回答，这本身就是强烈的负面反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的问题不在于“选哪个”，而在于你想解决什么。如果你的目标是构建一个可靠系统，那么上线前必须有底线测试，这可以是一小组关键问题，确保无论如何都不能出错。上线之后，你不可能人工检查所有交互轨迹，这时就需要监控来提示你哪里出了问题。当你发现新的失败模式，再反过来构建新的eval集。这个循环缺一不可。认为“只靠其中一种就够了”，在我看来是站不住脚的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我想稍微退一步，谈谈为什么“eval”这个词在 2025 年下半年被赋予了如此沉重的含义。你去找数据标注公司，他们说专家在写 eval；有人说 PM 应该写 eval，它们就是新的 PRD；还有人说 eval 本身就是产品改进所需的完整反馈回路。对初学者来说，这非常混乱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，大家说的都不完全错，但指向的是不同层面的事情。律师和医生写的“评估”，并不等于他们在构建 LLM judge；PM 写 eval，也不意味着要写一个可直接上线的评判模型。很多时候，你事前根本无法判断是否需要 LLM judge，还是只依赖生产环境的用户信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Martin Fowler 曾提出过“语义扩散”这个概念：一个词被发明出来，随后被不断滥用，最终失去精确定义。我认为 eval 正处在这个阶段。不同人看到的是它的不同侧面。但如果你让一群实践者坐在一起问：“AI 产品是否需要一个可执行的反馈回路？”他们一定都会点头。至于怎么做，完全取决于具体场景。复杂用例下，盲目构建评判模型往往得不偿失，这时回到用户信号、快速修复、确认是否回退，反而更有效。最终，所有资深从业者都会告诉你一句话：一切取决于上下文，不要迷信固定方法论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：现在“eval”已经变成一个可以指代无数不同东西的词，既包括标注、基准测试，也包括反馈机制，讨论起来反而更混乱了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我最近就遇到一个客户，说他们“在做 eval”。我问能不能看看数据集，他们说只是看了 LLM Arena 和一些第三方榜单，就选了模型。我只能说，那不是 eval，那只是模型对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：Claude Code 的负责人 Boris 曾公开表示：“我们在Claude Code 里不做 eval，一切靠感觉（vibes）。”能不能请你分享一下，Codex 以及 Codex 团队在eval这件事上的具体做法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：在 Codex，我们采取的是一种相对平衡的方式：eval是必要的，但同时必须高度重视用户反馈。我们在产品上极度强调“把正确的产品做出来”，而其中非常重要的一部分，就是认真倾听用户的声音。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Coding Agent 和其他领域的 Agent 有一个本质差异：它们是为“可定制性”和“工程师”而生的。Coding Agent 并不是只解决五六个固定工作流的产品，而是需要以多种方式被定制和扩展。这意味着，产品会被嵌入到各种不同的集成环境、工具链和使用场景中。在这种前提下，几乎不可能为用户的所有交互方式提前构建一个完备的eval数据集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，你仍然需要确保，每一次改动至少不会破坏产品中那些最核心的能力。因此，我们确实会用eval来守住这些“底线”。同时，我们也投入大量精力去理解用户真实的使用方式。举个例子，我们最近推出了一个代码审查产品，增长非常快，既帮 OpenAI 内部发现了大量问题，也被外部客户广泛使用。如果我对代码审查相关的模型、或训练时采用的强化学习机制做了调整，在上线之前，我一定会通过 A/B 测试来验证：它是否还能准确找出关键问题，用户对结果的反应如何。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时，用户一旦被错误的代码提示反复打扰，甚至会直接关闭这个功能。你需要确保，新版本确实在“做对的事情”。但老实说，很多这类场景在事前是很难预判的，也很难提前为它们构建对应的eval数据集。因此，这里面既有一定的“vibes判断”，也有大量来自真实用户的反馈。我们会非常主动地关注社交媒体，看看是否有人遇到特定问题，并尽快修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我并不认为有一套万无一失的eval指标，可以完全依赖它，其他什么都不用管。每当我们要发布一个新模型，团队都会聚在一起做集中测试，每个人关注不同的重点。我们手里有一份“高难度问题清单”，会把这些问题交给新模型，观察它的表现。这更像是每位工程师都有一套针对自身关注点的定制eval，用来帮助大家理解：在这个新模型下，产品到底发生了什么变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;CC/CD框架&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aishwarya：我们接触过大量公司，它们都承受着来自竞争对手的压力，因为“所有人都在做 Agent”，于是觉得自己也必须构建一个完全自治的 Agent。但很快发现一个问题：在一开始，你根本无法预知用户会如何与系统交互，也无法预判 AI 会给出哪些响应或采取哪些行动。当你的工作流包含四五个步骤、需要连续做出大量决策时，问题一旦出现，就会变得极其难以修复，结果往往是无休止的调试和热修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们曾为一个客服场景构建系统，后来，因为热修复多到失控，新的问题层出不穷，这个产品不得不被下线。与此同时，行业里也发生了不少令人警惕的事件，比如前段时间 Air Canada 的一个 Agent“臆造”了一条并不存在的退款政策，而公司因为法律原因不得不接受这个结果。这类案例让人意识到：如果设计不当，AI 系统可能会对企业本身造成非常严重的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样的背景下，我们开始思考：如何在不失去用户信任的前提下构建系统，同时又能形成一个持续改进的飞轮？这就是“CC/CD（Continuous Calibration, Continuous Development 持续校准、持续开发）”框架的出发点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/2284048821d07ccc10b93c93272d0e9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;循环的一侧是“持续开发”。你先界定能力边界，整理数据，明确系统的预期输入和预期输出。在真正动手之前，这一步本身就非常有价值，因为它常常会暴露出团队内部对“产品该如何表现”的理解并不一致。此时，产品经理和领域专家的参与尤为关键。你并不需要一个覆盖所有情况的数据集，而是一个“足够好”的起点。接下来，搭建应用，并设计评估维度。我刻意使用“评估指标”这个说法，而不是简单地说 eval，是因为评估是一种过程，而指标只是你在过程中重点关注的维度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一侧是“持续校准”。当系统上线后，你一定会看到大量最初未曾预料到的用户行为模式。评估指标可以帮助你发现一部分问题，但很快你会意识到，它们同样不足以覆盖所有新出现的错误模式。这时，你需要分析真实行为，识别新的错误类型，一部分问题可以直接修复，而另一部分则需要催生新的评估指标。这并不意味着每一个错误都要转化为新的eval维度。有些只是偶发问题，比如工具定义不清导致的调用错误，修完即可继续前进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体来看，这就是一个 AI 产品的典型生命周期。我们还特别强调，在迭代初期，应当采用“低自治、高控制”的方式：限制系统可做的决策数量，引入人在回路；随着理解加深，再逐步提高自治程度。这样做的本质，是在逐步建立对系统行为的认知飞轮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/05/0552db707dbf2e73328871c90f690d19.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以客服 Agent 为例，我们通常会把演进过程拆成三个阶段。第一阶段只是“路由”，即判断工单该被分配到哪个部门。很多人会低估这个问题的复杂度，但在大型企业里，路由往往异常困难。层级混乱、分类标准失序的情况非常普遍，人类客服往往依赖大量隐性经验才能做出判断，而这些规则通常并未被文档化。如果直接把问题丢给 Agent，而不给足上下文，风险就会非常高。在路由阶段，即便 Agent 分错了部门，人类也可以介入纠正，控制风险。同时，这个阶段往往会暴露出大量数据问题，需要优先修复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;等路由稳定之后，下一步是“副驾驶”：Agent 根据既有的标准操作流程生成回复草稿，由人工修改和确认。在这个过程中，你会自动记录人类的修改行为，从而几乎“免费”获得误差分析数据，并将其反馈到系统中。当你发现，大多数情况下人工已经不需要做太多修改时，才可以进入端到端的自动处理阶段，让 Agent 既生成回复，也完成问题的解决。这正是从低自治逐步走向高自治的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/0730a295de7b387816e5139414b6967c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还整理了一张表，明确每个阶段你在做什么、能学到什么，以及这些信息如何被反馈回系统。需要强调的是，采用 CC/CD 并不意味着问题会一次性被解决。即便已经走到较高版本，你仍然可能遇到此前从未见过的数据分布。这个框架的意义，在于帮助你在完全自治之前，尽可能多地理解用户行为，从而降低整体风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，它还隐含地帮你建立了一套行为日志体系。单纯依赖评估指标，只能捕捉你“已经知道”的错误，而大量新模式，只有在真实使用中才会显现出来。通过这种低风险、渐进式的方式，你可以理解用户，而不至于在问题全面爆发时手忙脚乱。最终，这一切的核心目标只有一个：在持续校准系统行为的同时，不断维护并增强用户对产品的信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这套方法的核心，在于把一切都设计成持续的、可迭代的过程，沿着“自治程度不断提高、控制逐步降低”的路径前进。“持续校准、持续开发”这个命名，本身就强调了它的迭代性。顺便说明一下，这个名字显然是在向 CI/CD（持续集成、持续部署）致敬，只不过这是 AI 时代的对应版本：不再只是不断跑单元测试、频繁部署，而是持续运行eval、观察结果、调整关注的指标，找出系统失效的地方，再不断迭代优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个框架本身上，还有没有什么你觉得特别重要、但我们还没提到的点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我们最常被问到的问题之一是：我该如何判断，系统是否已经“校准得足够好”，可以进入下一个阶段？这件事并没有一套明确的规则手册，核心原则只有一个：尽量减少“意外”。比如说，如果你每一两天就做一次校准，而发现没有出现新的数据分布模式，用户的行为也相当稳定，那你从系统中获得的新信息就已经非常有限了。这往往就是一个信号，说明你可以考虑进入下一阶段了。到了这个时候，很大程度上其实是在凭经验判断：你是否感觉自己已经“准备好了”，是否还在持续获得新的洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，也要意识到，有些外部事件会彻底打乱原有的校准状态。比如 GPT-4.0 被弃用，API 层面逐步迁移到 GPT-5，而新模型的行为特性完全不同，这时你的校准就会再次失效，需要重新走一遍流程。用户行为本身也会随时间演化。即便是消费级产品，我们今天和 ChatGPT 的交互方式，也和两年前完全不同，一方面是模型能力提升了，另一方面是用户在某个任务上尝到甜头后，会自然地把系统用于更多新场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们曾为银行的核保人员构建过一个系统。核保本身是一项非常繁琐的工作，贷款申请文件往往有三四十页。这个系统的初衷，是帮助核保人员快速查找政策和内部信息，从而更高效地审批贷款。最初三四个月，反馈都非常积极，核保人员的效率显著提升。但随后我们发现，正是因为他们对系统产生了信任，开始提出一些我们从未预料到的深度问题，比如直接把整份申请材料丢给系统，问：“像这种情况，之前的核保人员通常是怎么处理的？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从用户角度看，这只是一个非常自然的延伸；但从产品构建角度看，底层逻辑却发生了质变。系统需要理解“类似情况”究竟指什么，再去检索历史案例、分析文档，最后给出综合判断。这已经远远超出了最初“查找某条政策”的设计范围。正是这种不断演化的用户行为，提醒你：是时候回到校准阶段，重新审视系统能力边界了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 的未来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：当下 AI 领域里，哪些东西被高估了？哪些被低估了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：与其说“被高估”，不如说有些概念被严重误解。一个典型例子是多 Agent 系统。很多人会觉得：我有一个复杂问题，只要拆成几个子任务，分别交给不同的 Agent，再把它们连起来，就能实现所谓的“Agent 乌托邦”。现实并非如此。当然，成功的多 Agent 系统确实存在，但关键在于，你如何限制系统偏离轨道的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如，用一个监督型 Agent 来协调多个子 Agent，是一种非常成熟、有效的模式；但如果只是按功能拆分职责，期望这些 Agent 通过某种“点对点协作”自然形成整体能力，那在当前的模型能力和工程范式下，往往行不通。这并不是多 Agent 被高估，而是人们高估了它在现阶段能“自发协同”的程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得Coding Agent 仍然被低估了。你在 Twitter 或 Reddit 上会看到大量讨论，但你会发现它的真实渗透率依然很低，而潜在价值却极大。我认为2026 年会是集中优化这些流程、释放巨大生产力的一段时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：相比预先设计一堆各司其职的 Agent，更现实的路径，可能是让一个更强的 Agent 自己完成任务拆解和协调？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：没错。你可以由人来编排多个 Agent，也可以由一个更大的 Agent 负责统筹。但如果让多个 Agent 以点对点的方式自由通信，尤其是在客服这类对输出高度敏感的场景中，几乎不可能精细地控制“到底是哪个 Agent 在对用户说话”，护栏成本会急剧上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我认为eval是被误解的概念。它当然重要，但“不断切换工具、学习新工具”这件事被高估了。我依然是比较传统的看法：真正值得投入精力的，是对你要解决的业务问题保持极度专注，AI 只是工具而已。你当然需要了解最新进展，但不要把“快速构建”本身当成目标。今天构建的成本已经非常低了，真正昂贵的是设计，是你是否真正想清楚了产品要解决什么痛点。对问题本身和产品设计的执着，是被低估的，而单纯追求“快点做出来”，是被高估的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：从产品视角看，你们觉得未来一年 AI 会走向哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我非常看好“后台型”或“主动型” Agent。当前 AI 难以持续创造价值，很大程度上是因为它缺乏上下文，而原因在于它还没有真正接入工作发生的地方。一旦 Agent 被更深地嵌入真实工作流，获得更丰富的上下文，它就能理解你在优化什么指标、试图完成哪些活动。接下来顺理成章的一步，就是由 Agent 主动反过来提示你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们已经在 ChatGPT Pulse 这样的功能中看到雏形，它每天推送一些你可能关心的更新，帮助你“唤醒思路”。把这一模式扩展到更复杂的任务中，比如Coding Agent 在你一天开始时告诉你：“我已经帮你修复了五个工单，这是补丁，看看就行。”我认为这会在 2026 年成为非常重要的产品方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我非常期待 2026 年的多模态体验。2025 年我们已经取得了不小进展，不只是生成能力，在理解层面也是如此。但到目前为止，LLM 仍然是最常用的模型形态，而人类本身是高度多模态的。语言其实是我们进化中相对靠后的表达方式。即便我们在对话中，也在不断接收视觉、表情、语气等信号，并据此调整表达。如果能构建真正丰富的多模态交互，将会更接近人类对话的真实复杂度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还有大量“枯燥但重要”的任务等待被自动化。如今依然有无数手写文档、杂乱的 PDF，即便是最先进的模型也难以处理。一旦多模态理解能力真正成熟，我们就能解锁大量此前无法触及的数据资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果有人想提升自己构建 AI 产品的能力，你认为最值得重点培养的一两项技能是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：从小处着手、快速迭代、建立正向飞轮等。但如果站在更高的视角来看，对于当下的产品构建者而言，实施成本在未来几年会变得极低，真正稀缺的将是设计能力、判断力和审美品位。无论是做产品还是规划职业路径，早期几年往往专注于执行层面的技术细节，而随着 AI 大幅降低上手门槛，几年之后，每个人的价值都会更多体现在品味、判断，以及那些“只属于你”的东西上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种能力并不一定来自年龄或多年经验。我们最近招了一位同事，团队一直在用一款价格不菲的任务管理工具，他却直接带着自己手写的应用来开会，当场把我们全部拉进去开始用。那种主动性和主人翁意识，敢于重新思考既有体验，正是最能拉开差距的地方。当然，这类自建工具在规模化后可能有维护成本，需要替换或升级，但在小团队阶段，这种“先做出来再说”的态度让我非常震惊。很多在 AI 时代成长起来的人，对“构建”的心理成本极低，也更愿意尝试新工具。这或许也是为什么很多 AI 产品存在留存问题，大家都太容易被新工具吸引了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;归根结底，真正重要的是主动性和责任感。“忙碌但无效”的工作时代正在结束，你不可能再躲在角落里做对公司没有实质影响的事，而必须思考端到端的流程，以及如何创造更大的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这让我想到我之前请过 Jason Lemkin 上节目。他把整个销售团队几乎都替换成了 Agent：原来 10 个销售，现在是2个人加 20 个 Agent。结果有位销售直接辞职了，因为他发现自己“什么都没干”，很快就会被系统识别出来。这也印证了你的观点——混日子会越来越难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：坚持和承受“痛苦”的能力同样被严重低估。如今信息触手可及，几乎任何人都可以在极短时间内学习新东西，但真正的差别在于，是否愿意经历反复试错的过程——学习、实现、失败、再调整，真正理解什么有效、什么无效。我常说“痛苦是新的护城河”，这种在实践中积累的经验，无论对个人还是公司，都会沉淀为难以复制的优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多成功的公司，并不是因为抢先进入市场，或拥有多么炫目的功能，而是因为他们经历了足够多的痛苦，搞清楚哪些是不可妥协的核心点，并在模型能力、功能取舍之间不断权衡。这没有标准答案，也没有教科书，只能靠一轮又一轮的迭代。正是这些过程中的“痛苦”，最终塑造了个人能力和公司的长期竞争力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：专注于问题本身。AI 只是工具，关键在于你是否真正理解自己的工作流。很多所谓的 AI 工程师和 AIPM，把大部分时间花在理解业务流程、用户行为和数据上，而不是追逐最炫的模型。真正的差异化，永远来自对用户和问题的深度理解。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;闪电问答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：你们最常推荐的书是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：《当呼吸化为空气》。作者 Paul Kalanithi 是一位神经外科医生，在三十出头被诊断出肺癌。这本书让我意识到，我们是否花太多时间“评估人生”，却忘了真正去生活。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我更偏爱科幻，《三体》三部曲。它不仅讨论外星文明，也深入探讨科学、地缘政治与人类决策，对理解技术与文明的关系非常有启发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果喜欢科幻和 AI，我还强烈推荐《深渊上的火》（A Fire Upon the Deep）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近最喜欢的影视作品？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：我在重刷《硅谷》，它出奇地不过时，如今的 AI 浪潮和当年的情景高度相似。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我选一个游戏，《Expedition 33》。制作精良，故事、音乐和玩法都非常出色。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近发现并非常喜欢的一款产品？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：Whisper Flow。我没想到自己会这么依赖它，它能把语音自然地转化为指令，体验非常顺滑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：我偏好效率工具，比如 Raycast 和 caffeinate，让我在本地跑长时间任务时效率更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你的人生信条？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：“人们说这件事做不到，但那个傻子不知道，于是他做成了。”在这个数据随时告诉你“你大概率会失败”的时代，保留一点愚蠢的勇气很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：乔布斯那句话：你只能回头看时，才能把点连成线。所以不断前进、持续尝试就好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你最欣赏对方的一点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aishwarya：Kiriti 非常冷静、踏实，是我最重要的“回声板”，而且他是我见过最好的丈夫。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kiriti：Aishwarya 最大的特点是，她能把复杂问题讲得极其清楚，并且始终保持耐心和坚持，这在快速变化的 AI 时代非常珍贵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：https://www.youtube.com/watch?v=z7T1pCxgvlA&lt;/p&gt;</description><link>https://www.infoq.cn/article/Ox3JQRBv6RX0pAggifej</link><guid isPermaLink="false">https://www.infoq.cn/article/Ox3JQRBv6RX0pAggifej</guid><pubDate>Tue, 10 Feb 2026 09:29:23 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>不写、不看、不审查：这家安全公司决定不再让人类碰代码，还把这套模式开源了</title><description>&lt;p&gt;没人写代码，也没人看代码，软件照样交付？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026 年 2 月，一家专注于基础设施安全的公司StrongDM 公开了一套“软件黑灯工厂”式的生产线成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这个生产线里，人类不再直接写代码、也不承担代码审查；开发从交互式协作变成“把 spec 和场景喂给系统”。随后由 Agent 自动生成代码、运行测试/评测 harness，并在反馈回路里反复迭代，直到结果收敛、可以交付为止。团队把这套玩法写进章程，最重要的只有一句话——No hand-coded software。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fee3962709fe3b5932f0209d97707ae2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM AI 还不寻常的开源了它们：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一个仓库是：&lt;a href=&quot;https://github.com/strongdm/attractor&quot;&gt;https://github.com/strongdm/attractor&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是他们“软件工厂”体系中最核心的非交互式编码 Agent。不过，这个仓库本身一行代码都没有：里面只有三份 Markdown 文件，极其细致地描述了软件的完整规格说明（spec），以及 README 里的一句提示——把这些规格说明交给你选择的编码 Agent 去执行即可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f8ccdf71ec3c0db322408c05a163708.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个仓库 &lt;a href=&quot;https://github.com/strongdm/cxdb&quot;&gt;https://github.com/strongdm/cxdb&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个则更接近传统意义上的软件发布：包含 1.6 万行 Rust、9500 行 Go，以及 6700 行 TypeScript。这是他们的 “AI Context Store”——一个用于存储对话历史和工具输出的系统，数据以不可变 DAG的形式组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d2/d2d913a49631d2de71136c119dfd514c.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 的讨论中，很快就有开发者按图索骥，实际跑了一遍这套流程。他表示，自己仔细阅读了 Attractor 仓库中的文档，并严格按照 StrongDM 提供的规范，让 Claude 基于 spec 构建了一个完整应用。最终生成的是一个可以直接使用 Claude API Key 的 AI 代理，其整体质量“明显好于让模型自由发挥时生成的结果”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让他印象最深的，是这套规格说明的体量和细节程度：整套 spec 大约 6000–7000 行，覆盖了行为约束、接口语义以及系统边界。“我以前给代理布置项目时，规格说明最多也就一页纸，这次的细节密度让我非常震惊。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，这次开源并不是一个“打磨完毕”的展示版本。代码一经放出，Hacker News 上就有开发者迅速上手检查，指出其中存在疑似 bug、Rust 反模式，以及相对宽松的错误处理方式。对此，StrongDM AI 团队成员 Jay Taylor 在评论区回应称，这批项目“是最近几天才决定开源的”，尚未经过充分的技术优化，目前已经安排代理继续对 CXDB 进行清理和改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这套实践也很快得到了学界的点名。沃顿商学院研究 AI 与组织变革的教授 Ethan Mollick 在转发 StrongDM 的公开内容时直言，这是一次“真正激进的软件开发方式”：“几乎没有任何人类介入。即便这种方式未必适用于大多数场景，我们也需要更多这样的跳级式设想，去重新设计流程，而不是只把 AI 塞进旧流程里。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他看来，真正有价值的进步，不是在原有流程上“多加一点 AI”，而是围绕 AI，把流程本身重做一遍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/091c2451aa65664cc78b09ec9d4f4b25.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一条“禁止手写代码”的内部实验线&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 是一家专注于基础设施访问与身份安全的公司，核心工作是管理人类与非人类身份如何安全地连接到数据库、云资源和各类内部系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而他们的 AI 团队成立于半年前， 2025 年 7 月 14 日这天，Jay Taylor、Navan Chauhan 与 StrongDM 的联合创始人兼首席技术官 Justin McCarthy 一起，正式把一条原本分散在内部的探索工作，独立成一个专门的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新团队成立后，第一天的工作并不是写代码，而是写一份章程。Justin McCarthy 在回顾中提到，在团队成立的第一个小时，他们就先明确了一组接下来必须遵守的约束条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;代码不得由人类编写。&amp;nbsp;代码不得由人类审查。&amp;nbsp;如果你今天在每位人类工程师身上花费的 token 成本还不到 1000 美元，那么你的软件工厂还有很大的改进空间。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 StrongDM 自己的回顾里，这个决定并不是一时冲动。其背景要追溯到 2024 年末。随着 Claude 3.5 在 2024 年 10 月的第二次修订发布，团队开始观察到一个此前并不常见的变化：在长时序的 Agentic 编程任务中，结果开始叠加正确性，而不再只是不断叠加错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4c/4c45b808c5fdf4fe9c4b913d11672012.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到了 2024 年 12 月，这一变化已经可以通过 Cursor 的 YOLO 模式清晰地观察到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 在博客中写道，在此之前，将 LLM 反复用于编码任务，往往会累积误解、幻觉、语法错误、依赖不兼容等问题，最终让系统“慢慢坏掉”；而结合 YOLO 模式，Anthropic 的更新模型第一次展现出他们后来在内部称之为“非交互式开发”或“成长型软件”的雏形。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这样的背景下，新成立的团队从一开始就确立了一条极端的实验前提：不允许任何手写代码。在 2025 年 7 月，这听起来依然相当激进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中最耐人寻味的，是第二条规则：代码不得由人类审查。毕竟大家都很清楚，大语言模型极其容易犯下一些“非人类式”的错误；在这样的前提下，彻底放弃人工 code review，本身就显得反直觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更何况，安全软件向来是最不愿意交给“未经人工审查的 LLM 代码”去支撑的一类系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1885ab657677fe963c449b8a18a4fb1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c5/c5a0f3c6b891d8a37b96f4f3fdc04912.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;规则落地后，问题也随之出现：如果什么都不手写，怎么确保代码真的能跑？让 Agent 自己写测试，只在一个前提下有用——它们不会“作弊”，比如直接写个 assert true。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也迅速被他们提炼成一个更根本的问题：当实现和测试都由编码 Agent 生成时，你要如何证明自己交付的软件是可工作的？StrongDM 的答案，受到了场景测试（Scenario Testing，Cem Kaner，2003） 的启发。他们是这样描述的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们重新定义了“场景（scenario）”这个词，用它来表示一个端到端的“用户故事”。这些场景通常存放在代码库之外（类似模型训练中的“留出集”），既能被 LLM 直观理解，又可以灵活地进行验证。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于他们构建的软件本身往往就包含 Agentic 组件，StrongDM 也随之放弃了“测试全绿”这种布尔式成功定义，转而采用一种更接近真实体验的度量方式。他们引入了“满意度（satisfaction）”这个概念，用来量化验证结果：在所有场景中观察到的执行轨迹里，有多大比例可能令用户满意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们把这些场景当作“隔离集”，不存放在编码 Agent 能直接访问的地方，用来评估系统整体行为。这个设计本身就很有意思，它在某种程度上，模拟的是传统软件工程中一种极其昂贵、但也极其有效的做法——由外部 QA 团队执行的强力端到端测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4e/4e2d1f6e9f6239dc134582410a94458e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;合成场景策划与塑造界面&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从软件工厂的整体原则来看，StrongDM 把这一切总结为一条清晰的流程：“种子 → 验证 → 反馈回路”。系统先接收一个最小起点——几句话、截图，或一个已有代码库；然后在尽量贴近真实世界的验证环境中跑场景，把输出持续反馈回输入，让系统在闭环中自我纠错、不断叠加正确性；循环会一直运行，直到所有被隔离出来的场景不仅通过，而且能持续通过。token 被他们形容为这条生产线的燃料。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/aede7b62703e13c62059031154c1764a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;将“验收”交给spec？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 StrongDM 的软件工厂里，spec 并不是用来给人看的设计说明书，而是整个系统能够启动、纠偏和收敛的核心输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统开发流程中，spec 更多是一种“对齐工具”：它帮助工程师理解要做什么，但真正的实现细节、权衡和妥协，往往发生在代码和 code review 过程中。而在 StrongDM 的设定下，当“人不写代码、人不看代码”成为前提，spec 的角色被彻底前移——它不再是参考材料，而是事实上的控制面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cf7d01c432b4545432ba654b2fcc4359.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队要求系统能够“从层层递进的自然语言规范中生长”，并且必须能够在“不对源代码做语义层面检查的情况下完成验证”。在这种设定下，“验收”本身也被重写了。spec 与场景（scenario）一起，构成一个不断运行的评测基准：模型生成的行为是否符合规范，不是靠人去读代码判断，而是靠它在这些场景中跑出来的结果是否持续满足预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b961ee2d9ee54596576cd5439a8605b2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c0/c05e4f6c0283d41ce5709fdf101921f1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，StrongDM 的方法把覆盖率从“人为写了多少测试”这一维度转向了“规范/场景是否足够多与足够准确”＋“验证生态能否在闭环中捕获异常”这一维度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这一理念，StrongDM 还进一步提出了他们的另一个关键概念：数字孪生宇宙（Digital Twin Universe, DTU）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;StrongDM 的定义是：数字孪生宇宙是一组对第三方服务的行为级克隆体。他们构建了 Okta、Jira、Slack、Google Docs、Google Drive 和 Google Sheets 的孪生系统，复刻这些服务的 API、边界情况以及可观察到的行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f51bd6e432680850d6f917f402f95b11.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了 DTU，他们就能在远超生产环境限制的规模和速率下做验证：既能测试那些在真实服务上危险、甚至根本不可能尝试的失败模式，也能每小时运行成千上万个场景，而不必担心触及限流、触发滥用检测，或累积 API 成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那这些 Okta、Jira、Slack 的关键行为是怎么“克隆”出来的？答案是：用编码 Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fe/fe9ae51ebc9a21883424a7b6bd996cb3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2d/2d9f53ccf591c7f26906500837b51178.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人将这套做法概括为一条可复用流水线：把某个服务的完整公开 API 文档直接喂进 Agent harness，让它生成一个自包含的 Go 二进制程序去模拟这些 API；然后在此基础上再快速搭一个简化 UI，方便把整套仿真跑通、跑顺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，DTU 的创建者 Jay Taylor 在 Hacker News 上补充了一些背景，分享了一条关键的提示策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我最初有一个关键洞察，最终形成了一套可重复的方法，用来确保 DTU 与官方 SaaS 服务之间具有高度一致性：以最流行、公开可用的官方 SDK 客户端库作为兼容性目标，始终追求 100% 兼容。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当这些不受限流和配额约束的服务克隆体跑起来后，一整支“模拟测试 Agent”队伍也就能彻底放开手脚。场景测试不再是一锤子买卖的验收环节，而是变成了 Agent 会反复、持续执行的脚本：系统一边搭建，一边就被不停拉出来跑场景、做验证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们的 Slack 孪生系统截图也直观展示了这种测试方式：一批模拟的 Okta 用户不断出现，并分别去申请访问不同的模拟系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99a80be3da43e93b3de71da0c0bae7ec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题依然是：太烧钱了？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在惊艳之外，这次实验也迅速暴露出一个无法回避的现实问题：成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Hacker News 的实操反馈中，有开发者提到，按照 StrongDM 提供的 spec，让 Claude 构建完整应用时，TypeScript 路线的 token 消耗极高，不得不中途给账户充值，才能在一个晚上把流程跑完。他甚至计划改用 Rust 或 Go 再试一次，只是为了看看是否能把成本压下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个反馈并非个例，也不是枝节问题。StrongDM 团队在内部曾提出过一个颇具冲击力的衡量标准：如果你今天在每位人类工程师身上花费的 token 成本还不到 1000 美元，那么你的软件工厂还有很大的改进空间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这句话一旦落到现实，就更像是一个商业模式的探讨：你能否打造出一条足够盈利的产品线，从而负担得起以这种方式开发软件所带来的巨大成本？当任何竞争对手只需几个小时的编码代理工作就能克隆你的最新功能时，构建可持续的软件业务也变得截然不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，正如StrongDM 团队在回顾中所说，其实这一切技术上是可行的，只是以前从经济上来说不划算：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“构建一个高保真 SaaS 应用的克隆在技术上一直可行，但在经济上从未现实过。几代工程师都可能想过，做一个完整、内存级的 CRM 副本来测试，但最终往往会在心里把这个提案按下去——‘算了，太不划算了’。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即使对于那些不打算在 token 成本上一次性投入数千美元的团队和个人来说，StrongDM 这种做法依然有很多值得思考的地方，尤其是在人力成本和个人投入回报这一层面。对程序员个人而言，真正的问题或许不只是“现在贵不贵”，而是：当算力成本持续下降几乎成为共识时，你是否已经开始为新的角色和分工做技能投资——还是仍然把全部价值押在“写代码本身”上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8f/8fce4cffe9d50c36e0be66a3ca390d93.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是个很有意思的观点，不过我想从另一个角度补充一下：如果按每个月 20 个工作日来算，那就是 2 万 × 12 = 24 万美元一年，差不多等于一个 FANG 新毕业生的总包（TC）。我和不少初级到中初级的软件工程师（SDE）共事过，说实话，其中 80% 的表现并不比 Claude 好。（我也见过一些 staff 级别的工程师写出的代码比 AI 还差，但他们通常会用领域知识和技术负责人职责把短板补回来。）&amp;nbsp;我确实看到，AI 正在把软件工程进一步推向一种金字塔结构：顶层只有极少数人类，其余大量工作由 AI 承担。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/eb/ebb0e1bcfd911fc3198571f2b16bfbb1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然从按现在的成本算，AI“还没便宜到值得完全替代人”，但有网友认为成本下降也许能够预期：“我在想，这会不会只是软件工厂还处在非常早期、效率极低阶段的副产品。Yegge 和 Huntley 都承认，他们在做的自治工厂实验既昂贵又浪费。从制造业的历史经验来看，我反而会预期：随着方法逐渐成熟、流程被不断优化，成本会慢慢降下来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在博客中的最后，他们给出的结论，也为这条实验线画上了一个颇具警示意义的注脚：“我们这些构建软件工厂的人，必须刻意保持一种天真：主动识别并移除软件 1.0 时代留下的习惯、惯例和限制。数字孪生宇宙（DTU）就是最好的证明——六个月前还不可想象的事情，如今已经成了日常。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://factory.strongdm.ai/&quot;&gt;https://factory.strongdm.ai/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://simonwillison.net/2026/Feb/7/software-factory/&quot;&gt;https://simonwillison.net/2026/Feb/7/software-factory/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46924426#46931812&quot;&gt;https://news.ycombinator.com/item?id=46924426#46931812&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/iwKHqENNONoXQDEsvobG</link><guid isPermaLink="false">https://www.infoq.cn/article/iwKHqENNONoXQDEsvobG</guid><pubDate>Tue, 10 Feb 2026 09:22:34 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>Daggr 发布：用于构建与检查 AI 工作流的开源 Python 库</title><description>&lt;p&gt;Gradio 团队发布了 &lt;a href=&quot;https://huggingface.co/blog/daggr&quot;&gt;Daggr&lt;/a&gt;&quot;，这是一个新的开源 Python 库，意在简化多步骤 AI 工作流的构建与调试。Daggr 允许开发者以 Python 代码的方式定义工作流，同时会自动生成一个可视化画布，展示流水线中每个步骤的中间状态、输入和输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 通过将工作流组织为有向图的形式，简化了应用型 AI 的开发过程，使每一个节点都可以被单独检查和重新执行。这种方式有效缓解了应用开发中常见的一个问题：当错误发生在流程后期时，需要重新运行整个流水线，导致实验过程缓慢且结果不够清晰。通过节点级别的复现与检查，Daggr 提升了调试效率，也加快了迭代速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该库采用以代码为核心的设计思路。开发者直接在 Python 中定义节点及其连接关系，Daggr 再根据代码渲染出对应的可视化界面用于检查。这与以 GUI 为中心的工作流构建工具形成对比，后者往往牺牲版本控制能力和灵活性。使用 Daggr 时，可视化层是从代码派生出来的，而不是取代代码本身，从而保证了工作流的可复现性，也更便于审查和协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 支持三种主要的节点类型。GradioNode 可直接连接到 Gradio 应用或 Hugging Face Spaces，使已有的演示和工具能够作为工作流组件复用。FnNode 用于封装任意 Python 函数，方便插入自定义的预处理或后处理逻辑。InferenceNode 则用于对接通过 Hugging Face Inference Providers 提供的模型服务，使托管模型能够无需额外适配即可集成进工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一个关键特性是状态持久化。Daggr 会自动保存工作流状态、缓存结果、输入值以及画布布局，使开发者可以在不中断上下文的情况下暂停和恢复工作。单个节点也可以在修改输入后单独重新运行，这在调试长流水线或对比某一步的不同实现方案时尤其有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于 Daggr 由 Gradio 团队开发，它与 Gradio 生态系统实现了紧密集成。工作流既可以在本地启动，并通过浏览器访问可视化画布，也可以利用 Gradio 的隧道功能通过公共链接进行分享。对于需要长期运行的场景，同样的工作流还可以通过将 Daggr 作为依赖，部署到 Hugging Face Spaces 上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;开发者的早期反馈主要集中在编程式控制与可视化反馈相结合这一点上。Sebastian Buzdugan 在评论该发布时&lt;a href=&quot;https://x.com/sebuzdugan/status/2017183567273406571?s=20&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;把接口和 Gradio 混在一起用，真的是一个非常聪明的组合。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有其他用户&lt;a href=&quot;https://x.com/OriOridev/status/2017000978227122383?s=20&quot;&gt;指出&lt;/a&gt;&quot;，Daggr 在快速实验和原型验证方面尤其有价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 目前是一个轻量级、实验性质的项目，仍处于 beta 阶段。随着用户的使用，其 API 可能会发生变化。尽管工作流状态是存储在本地的，但更新过程中仍可能导致数据丢失，这也进一步表明它的定位更偏向于开发和原型工具，而非直接用于生产环境的解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Daggr 现已作为开源 Python 包发布，支持 Python 3.10 及以上版本，可通过 pip 或 uv 进行安装。其源代码、示例和文档已发布在 &lt;a href=&quot;https://github.com/gradio-app/daggr&quot;&gt;GitHub&lt;/a&gt;&quot; 上，团队也邀请社区在项目逐步成熟的过程中提供反馈并参与贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/daggr-open-source/&lt;/p&gt;</description><link>https://www.infoq.cn/article/1xUSbEXlzBqHKsh3pSFy</link><guid isPermaLink="false">https://www.infoq.cn/article/1xUSbEXlzBqHKsh3pSFy</guid><pubDate>Tue, 10 Feb 2026 03:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>软件工程</category></item><item><title>从分散存储到统一分析，Apache Doris 在快手万亿规模广告场景的应用实践</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;导读：面对万亿级广告数据存量、日均 3 亿行增量及数千个复杂查询模板的挑战，快手广告数据平台如何突破性能瓶颈、实现架构统一与体验跃升？本文系统介绍了快手广告团队从 ClickHouse on ES 混合架构，全面迁移至 Apache Doris 的统一分析实践，最终实现查询性能提升 20～90%，写入吞吐提升 3 倍，存储效率提升 60%。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自快手高级计算引擎研发工程师 周思闽 在 Doris Summit 2025 中的演讲内容，并以演讲者第一视角进行叙述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手是国内日活过亿的短视频平台，其广告投放平台是商业化外部广告主与快手电商商家进行广告投放的主要阵地，支持客户在平台上进行广告物料搭建、物料管理、策略变更、数据查看等操作，这对底层数据系统的存储、计算与查询性能提出了极高要求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要支撑如此大规模的广告投放与实时分析，底层数据架构面临巨大挑战。当前，快手的广告数据包括：由投放系统产生的物料数据以及用于数据分析的效果数据，这些数据呈现出三个显著特征：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据存量巨大：广告物料累计已达千亿级别，且随业务发展正向万亿规模迈进，存储体量位居公司前列，对架构扩展性提出极高要求。数据增长迅猛：仅 2025 年第一季度，日均新增广告物料数据同比激增 3.5 倍，要求底层引擎具备强大的实时写入与弹性扩展能力。数据模型复杂：整个数据体系涵盖约 700 个核心字段，涉及物料、投放、用户、效果等多个维度；同时，为应对多样化分析场景，沉淀的查询模板已超 4000 个，对查询引擎的兼容性与性能均是严峻考验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构演进：从分散存储到统一分析&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 早期架构及挑战&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期存储架构中，物料数据由 MySQL、Elasticsearch 协同存储；效果数据主要存储与 Clickhouse 中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据分析时，将分散在 MySQL、Elasticsearch 中的物料数据与 ClickHouse 中的效果数据进行高效关联查询，从而为广告主提供完整、及时的投放效果洞察。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3c247f9ec4b67b6612011bf2aedc7beb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在如上所说的 ClickHouse on ES 架构中，用户提交的查询通常包含 Elasticsearch 外表（a）与 ClickHouse 内表（b）。ClickHouse 会解析查询中外表部分，将其转换为 Elasticsearch 查询语句，通过 HTTP 请求获取数据并封装为 Block，最后在引擎内部完成与内表的关联计算。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f6c7f67f64a49cd0ea8b411124049604.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，随着 Elasticsearch 中数据量持续增长，该架构逐渐暴露诸多问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;查询性能恶化：慢查询率上升至 35%，平均查询耗时达到 1.4 秒；存储瓶颈：Elasticsearch 单分片难以支撑 10 亿级以上数据量，扩容与数据重分布成本高；运维复杂度高：数据链路依赖组件多，运维与监控成本显著上升；问题定位困难：缺少 ClickHouse 与 Elasticsearch 之间的全链路可观测手段，出现查询延迟、数据不一致等问题时，需跨系统排查，耗时较长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 选型目标及调研&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上述问题及挑战，我们为新架构设定了明确目标：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;慢查询率低于 5%；运维排查耗时降低至分钟级；支持单表万亿级别数据存储；保障数据实时性，延迟低于 5 分钟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于以上目标，我们对 Apache Doris、ClickHouse、Elasticsearch 等主流 OLAP 引擎进行了全面的调研与性能压测。测试涵盖了写入吞吐、查询延迟、存储压缩率、全文检索性能等关键维度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da9240cbfe6a23bb6131a78ace804f9a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这过程中，ClickHouse 首先被排除，因其不支持唯一键模型，而广告物料数据存在大量更新场景，要求引擎具备主键更新能力。因此，重点在 Elasticsearch 与 Apache Doris 之间进行对比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合测试结果，Apache Doris 在写入性能、查询效率、存储成本及运维复杂度等方面均表现优异，不仅能够满足既定架构目标，还在多个场景下显著优于 Elasticsearch。因此，我们最终选定 Apache Doris 作为下一代广告数据分析引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 基于 Apache Doris 的统一分析引擎&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际应用中，我们引入 Apache Doris（计算引擎） 替换了原先架构中的 Elasticsearch、ClickHouse，设计了统一分析引擎 Bleem。通过在外部表模块中引入数据缓存层与元数据服务层，有效提升了跨源查询效率，使数据湖外表的查询性能接近内表水平，实现了关键的性能突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a2/a2eadf662145651fb29d4f0c73a45647.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体来看，Bleem 架构自下而上分为 5 层：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存储层：数据湖中的 Hive/Hudi 数据存储于 HDFS；存算分离模式下的内表数据存放于对象存储 BlobStore；存算一体模式下的内表数据则存储于本地磁盘。缓存层：将 Hive/Hudi 外部表数据缓存至 Alluxio，保障 I/O 稳定性，提升数据读取效率。计算层：Apache Doris 为核心引擎。不同项目组对应不同的 Doris 集群，以实现计算资源物理隔离，用户可按需申请计算资源。依托于 Doris 湖仓查询能力，可直接对 Doris 内表与外部 Hive/Hudi 数据查询。同时，Doris 也支持存算一体与存算分离两种部署方式，可根据实际需求灵活选择。服务层：元数据缓存服务实时监听 Hive 元数据变更，并同步至缓存中，以提升湖仓外部表的查询效率。接入层：将 OneSQL 作为统一查询接入网关，提供集群路由、查询改写、物化改写、查询鉴权、限流与阻断等功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;依托 Doris 强大的 OLAP 计算与湖仓一体能力，将此前分散的数据湖分析、实时 OLAP 查询、在线报表及全文检索等多种场景，统一整合至同一套引擎架构中，实现了技术栈的收敛与提效。该架构在实际落地中已带来显著收益：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;性能大幅提升：慢查询率低于 5%，整体查询性能提升了 20%～90%；存储扩展高效：支持万亿级别数据存储，水平扩容效率较 Elasticsearch 提升 10 倍以上；运维大幅简化：一套引擎覆盖全部查询场景，系统依赖组件少，运维复杂度显著降低；可观测性全面加强：Doris 支持全链路追踪与全面监控，平均问题排查时间降低 80%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;迁移实践及调优经验&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整个迁移过程分为三个阶段，稳步推进以确保业务平稳过渡：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一阶段（试点验证）：选取关键词推广场景进行试点，跑通全量与增量数据导入流程，搭建双链路并行验证数据一致性与查询正确性。第二阶段（主体迁移）：迁移原 ClickHouse on ES 查询链路，将 Elasticsearch 中全量物料数据导入 Doris，完成业务切换后下线 Elasticsearch 集群。第三阶段（收尾统一）：迁移剩余纯 ClickHouse 场景，将无需关联 Elasticsearch 的查询任务及其数据全部迁移至 Doris，完成整体架构统一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在架构升级及迁移过程中，我们收获了许多实践及优化经验，在此逐一分享。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;01 解决极端场景下数据一致性问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据导入层面，我们基于 SeaTunnel 实现流式数据同步，该方式支持批处理场景下的 Overwrite 语义，所有导入均采用两阶段提交机制，以确保数据同步的最终一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在基于 SeaTunnel 和 Spark 的数据同步过程中，我们遇到了极端场景下的数据重复问题。主要有两种情况：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Spark 推测执行时，两个 Task 同时写入同一份数据并均完成 Doris 两阶段提交，尽管 Driver 只认定一个 Task 成功，但数据已重复。Spark Task 完成 Doris 提交后，在向 Driver 汇报前因抢占或异常退出，Driver 重启 Task 并重新写入数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为解决该问题，我们在 Doris 的两阶段事务提交环节引入了 ZooKeeper 分布式锁机制，通过记录并校验事务状态来保证批同步的一致性。具体流程如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;准备提交阶段，先获取 ZooKeeper 临时锁，确保同一时间只有一个事务进入提交流程；获取锁后，将 Prepare 状态写入 ZooKeeper 临时节点，并记录当前事务 ID；查询上一个事务的状态：若不存在，直接提交当前事务；若上一事务处于 Prepare 状态，则先回滚上一事务，再提交当前事务；若上一事务已 Commit，则直接回滚当前事务；最终将 Commit 状态写入 ZooKeeper 持久节点，完成本次提交。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b70fab501f654841586df0927ccdb14a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;02 Stream Load 机制优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为应对高并发数据导入，我们对 Apache Doris 的 Stream Load 机制进行了调优。通过合理配置任务优先级与合并（Compaction）参数，显著提升了写入吞吐与稳定性。Doris 内部通过 Load Channel 进行任务调度，以区分高优与普通优先级通道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/165c346b4f34f3418f8e6615e227e4a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调优的核心在于合理配置相关参数，例如当 Stream Load 任务指定的 timeout 时间小于 300 秒时，系统会将其判定为高优任务并分配至高优通道。参数优化如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;load_task_high_priority_threshold_second=300
compaction_task_num_per_fast_disk=16
max_base_compaction_threads=8
max_cumu_compaction_threads=8
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;03 差异化的建表策略&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OLAP 引擎的查询性能很大程度上取决于表结构设计。因此，我们针对不同业务场景制定了差异化的建表策略：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;物料表（高频更新与大规模检索）：该表数据量极大且需支持实时更新。业务查询主要基于 account_id 进行过滤，而非原 MySQL 的自增 ID。为充分发挥 Doris 前缀索引与排序键的优势，在保证业务逻辑等价的前提下，我们将 account_id 与 id 组合为联合主键，并将account_id 设为首个排序键及分桶字段，大幅提升查询过滤效率。同时配置倒排索引以支持多维检索，并选用 ZSTD 压缩算法平衡存储与 IO 性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;-- 建表语句参考
CREATE TABLE ad_core_winfo
(account_id BIGINT NOT NULL,
id BIGINT NOT NULL, 
word STRING,
INDEX idx_word (`word`) USING INVERTED...) 
UNIQUE KEY(account_id,id) 
DISTRIBUTED BY HASH(account_id) BUCKETS 1000;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效果表（多维聚合分析）： 相较于物料表，效果表侧重于数仓指标的累加与聚合。因此，我们直接采用聚合模型，并按照“天”或“小时”粒度设置分区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;-- 建表语句参考
CREATE TABLE ad_dsp_report
(__time DATETIME, 
account_id BIGINT, ...
`ad_dsp_cost` BIGINT SUM,
...) 
AGG KEY(__time,account_id,...) 
AUTO PARTITION BY RANGE(date_trunc(`__time`,&#39;hour&#39;))()
DISTRIBUTED BY HASH(account_id) BUCKETS 2;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;04 大账户数据倾斜治理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据压测中，我们发现不同 Account ID 对应的数据量差异极大，小至个位数、大至百万级别，导致 BE 节点 CPU 负载严重不均。通过 SHOW DATA SKEW 命令进一步确认，Tablet 存储分布明显倾斜：大 Tablet 占用空间达 3–4 GB，小 Tablet 仅 100-200 MB，且大账户查询延迟较高。为此，我们实施了以下两点优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;A：按账户范围进行分区&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经分析，Account ID 为 5–8 位数字，且未来不会超过 10 位。因此使用 FROM_UNIXTIME 函数将 Account ID 转换为 Datetime 类型，按月对历史数据进行分区，共划分出 33 个历史分区。每个分区可容纳 2,592,000 个 Account ID，后续每新增约 200 多万个 Account ID 才会新增一个月份分区。同时，针对历史分区，根据数据存量进行手动分桶，新分区则默认设置为 256 个分桶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该方案通过分区裁剪有效过滤了大量无关数据，同时为未来数据膨胀预留了扩展空间（物料表日均增量约 3 亿），显著降低分区增长对查询性能的影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;B：对 Account ID 进行二次哈希&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为缓解单个 Account ID 数据量差异过大导致的分布不均，我们选取与 Account ID 无关的 ID 字段，通过 ID MOD 7 计算得到一个取值在 0～6 之间的 mod 字段。将原本仅基于 account_id 的哈希分桶键调整为 (account_id, mod) 联合键，从而将同一 Account ID 的数据分散到 7 个 BE 节点上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/51/5102e16741d03083ce073f76f898aab9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，各 Tablet 大小基本均衡稳定在 1GB 左右，数据存储与查询负载得以在多个 BE 间均匀分布，有效解决了 此前 CPU 负载不均的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;05 万级分区下的查询优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当分区数量达到万级别时，简单点查 SQL 的耗时达到 250 毫秒，远超 100 毫秒的预期。通过分析，耗时主要集中在 Plan 阶段，原因是 Doris（2.1 版本）在分区裁剪时，会遍历所有分区进行匹配，万级分区的顺序遍历开销巨大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们将顺序遍历改为二分查找：对万级分区先进行排序，再利用二分查找快速定位目标分区，将时间复杂度从 O(n) 降至 O(log n)。优化后，该查询耗时从 250 毫秒降至 12 毫秒，性能提升超过 20 倍。目前，二分查找已在 Doris 3.1 版本中实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;06 并发调优&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在查询优化过程中，我们发现：多数查询经过条件过滤后，实际命中的数据量并不大，即便在大账户场景下，命中数据量也仅在百万级别。然而，Profile 显示这类查询的 Total Instance 数高达 800 个，其默认并发数为 32，存在明显的过度并发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们调整以下参数降低并发开销：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;sql&quot;&gt;set global parallel_exchange_instance_num=5;
set global parallel_pipeline_task_num=2;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调整后，同一查询的 Total Instance 数量降至 17 个，查询耗时也显著缩短。这说明在小数据量点查场景下，适当降低并发可有效减少 RPC 开销，从而降低延迟（220ms 降至 147ms）。同时，这一优化也提升了系统的整体 QPS 承载能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;收益及规划&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过上述架构迁移与深度优化，我们在三个核心维度取得了显著收益：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;查询性能大幅提升：关键词推广页平均查询延迟下降 64%，创意推广页延迟下降超过 90%，整体查询体验实现跨越式提升。写入能力显著增强：单节点写入承载能力提升 3 倍以上，单表实时导入峰值突破 300 万行/秒。存储效率优化明显：通过分区策略与 ZSTD 压缩算法，存储效率较 Elasticsearch 提升约 60%，并可轻松支撑万亿级数据存储。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，我们将深度探索 Apache Doris ，重点围绕两方面展开：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;增强全文检索与分词能力：引入社区在 Doris 4.0 版本中推出的 BM25 打分功能，以及 IK 分词器等更多分词组件，实现按业务场景灵活选用最优分词方案。增强向量索引：基于 Doris 4.0 版本，在内表和数据湖外表场景下对向量检索的性能和边界能力做验证与优化。&lt;/p&gt;</description><link>https://www.infoq.cn/article/X19PzkXgJux5tYL9ULBR</link><guid isPermaLink="false">https://www.infoq.cn/article/X19PzkXgJux5tYL9ULBR</guid><pubDate>Tue, 10 Feb 2026 01:52:00 GMT</pubDate><author>SelectDB</author><category>数据库</category></item><item><title>Skills出世，Prompt已死？2026年，如何为Agent构建可控思维</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;别卷Prompt了！它只是你 AI 员工的“开机键”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进入2026年，Skills的爆火和Clawdbot（OpenClawd）的横空出世，传递了一个清晰的信号：当 Agent 从酷炫的演示走向支撑业务的生产系统时，单纯依靠优化提示词（Prompt）的“艺术”，已无法满足企业对可靠性、执行力与持续进化能力的刚性需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不是说Prompt不再重要，而是它的角色发生了根本性转变。它从一个需要被无限雕琢、承载所有逻辑的“总指挥”，演变为一个触发器。它的新任务是：准确理解人类指令，然后高效地唤醒后方一套庞大且专业的能力系统。就像手机的开机键，按一下就可以打开各种应用功能的入口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个能力系统，正是现代AI工程的核心——一个为 Agent 打造的“可控思维”架构。它由三个相互协作的引擎构成：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;记忆引擎（Memory）：确保 Agent 有“记性”，能够记住用户偏好和交互历史。这意味着它能记住重要的对话历史和你的要求，做事有头有尾，不用你每次都从头交代。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;知识引擎（RAG）：确保 Agent 有“实时的知识库”，能够从海量、动态的企业数据中精准检索信息，保证它给出的信息永远准确、最新，不会凭空乱造。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;技能引擎（Skills）：确保 Agent 有“手脚”，能够将复杂的业务操作（如数据查询、报告生成、系统调用）封装为可被随时调用的标准化模块，从“能说”走向“会做”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Prompt、Memory、RAG、Skills共同构成了一个能独立干活、不出错、有记性的 AI 员工，当它要完成的任务越复杂、越关键，后三者的系统化工程价值就越发凸显，Prompt也因此必须从舞台中央退下。作为使用者，我们不再只是和模型对话的“提问者”，而是为 Agent 设计和组装能力模块的“架构师”，思考重点也从“怎么问得好”，全面转向“怎么让AI干得好”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解这种从孤立提示到系统工程的范式迁移，是我们今天话题的起点。下面，是一场来自 OceanBase 社区嘉年华的圆桌讨论，看顶尖的实践者们如何具体拆解这些核心组件的演进与融合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从Prompt 到 Skills，RAG 还行不行&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：张海立，LangChain Ambassador、OceanBase Ambassador，up主“沧海九粟”嘉宾：张颖峰，RAGFlow CEO嘉宾：余金隆，FastGPT 负责人嘉宾：古思为，Co-founder of Nowledge Labs嘉宾：吉剑南，OceanBase AI 平台与应用负责人&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题一：2026 年 RAG 生态何去何从？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：从去年末到今年年初，AI 领域热点频发。除了近期备受关注的 Clawdbot（OpenClawd），Skills 成为另一个重要话题。我在进行 Skills 相关实践时发现，许多 Skills 与本地文件系统紧密相关，但都离不开 RAG 体系对外部数据的召回，这对 Agent 发挥更大作用至关重要。LangChain 在构建 Agent 生态时，RAG 也是核心体验之一。想请教各位老师：在当前大环境下，您认为 2026 年 RAG 生态将如何发展？请结合各自产品进行简要介绍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：先说个笑话，2025年被称为Agent元年，当时有朋友问我们要不要（从RAGFlow）改名为AgentFlow；而今年是Agent落地元年，我们内部也讨论要不要改名为ContextFlow。实际上我们永远不会改名，因为我们认为“R”是核心点，单纯的RAG确实不足以服务Agent，但“R”是服务Agent数据层的核心点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单纯的RAG 确实不足以服务 Agent，但 RAG 是服务 Agent 数据层的核心。当前 Agent 需要的是上下文（Context），它来自三方面数据：企业内部数据、工具数据以及对话过程中生成的数据。Skills 偏向工具层面，但比工具更高一层，还包含了规划（Plan）能力。Skills 本身也需要搜索——当企业内部有 1000 个 MCP 时，如何调用对应的 Tools 和 Skills 同样需要检索能力。因此RAG 永远不会消失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的布局是从RAG 引擎向上层引擎演进。技术本身未变，但内涵发生变化：数据从简单的企业内部数据，扩展到 Agent 过程中的上下文数据。我们判断未来所有Agent 都是 Coding Agent，包括对工具的调用也将变成代码生成（Code Generation），需要RTC（Run-Time Code）在沙箱中执行，访问各类 Tools 和 Skills，最终通过文件系统返回结果。这也是我们向上下文引擎方向演进的核心计划。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我赞同颖峰老师关于Code Generation 解决所有问题的观点，这也是我们团队的认知。无论是做 RAG 引擎还是 Workflow 引擎，都在向代码生成靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RAGFlow不想改名，我们有点想改名字。因为近几年我们发现，做Agent本质是把数据使用起来，所以我们的平台主要解决数据连接层问题。过去数据分布在数据库、文档等各种结构中，现在通过大量连接器实现不同数据的连接。Skills 出现后，以前需要写代码和 Webhook 连接的数据层，现在可以通过 Skills 实现。这对国内交付场景特别有价值——国内系统数据格式不统一、缺乏标准，交付同学以前需要写大量适配代码，现在通过 Skills 将数据标准化连接到平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年我们主要做两件事：一是完善连接层，二是优化RAG 的 Retrieval 层。Retrieval 效果很大程度上取决于召回过程，不同场景的召回流程差异很大。过去需要通过 Workflow 形式搭建积木、进行意图识别分类、编写不同提示词适配不同场景，链路复杂。现在我们探索通过 Skills 这种偏语义化的方式生成代码，类似 Test-to-Code 的思路，但生成的是 SDK 代码来构建整个 Retrieval 流程，这是一个很有意思的探索方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：关于2026 年 RAG 相关变化，可以看到在Coding Agent 中对代码的检索已从纯 Embedding 转向 AST（抽象语法树）、Agentic FS Graph 或 AST Graph 等方案。包括PageIndex&amp;nbsp;项目，以及我们公司在Haicon 2024 发布的实验性项目 OpenKL，尝试用类文件系统方法处理 Memory 和 RAG Docs。&lt;/p&gt;&lt;p&gt;另一个趋势是RAGFlow 等通用内容引擎同时处理文档和 Memory。我们已发布的第一个产品是面向C 端的 Memory 桌面 APP Knowledge MAM，动机是帮助用户在不同工具间无缝切换工作流。例如在 ChatGPT 完成 Deep Research 后，无需重新解释即可继续在 Cursor 中工作；或者当 Agent 帮助发帖子进入热榜后，可以切换到另一个 Agent 继续任务，同时保留所有交互历史和偏好设置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：OceanBase 面向 AI 的能力——seekdb、 PowerRAG 与 PowerMem 均已开源。我们团队除了做向量数据库和 AI 应用基础设施外，也在探索面向数据库的 AI 应用，比如面向开发者工具的 Text-to-SQL 和数据库智能运维。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于2026 年趋势，我认可颖峰老师说的&amp;nbsp;RAG 不会消失，它和Skills、MCP处于不同维度。即使未来 Skills 和 MCP 越来越多，最终仍需通过 RAG 或某种方式召回，不能将所有 Skills 都喂给模型。但我有不同观点：当前 RAG 仍集中在知识库领域，通过搭建 Chatbot 做问答，而问答更像玩具而非生产应用。真正的生产应用应将RAG 融入日常工作，如销售根据集团材料为客户生成定制化PPT或“一指禅”。未来 RAG 会结合应用反馈，反向影响数据如何切分、如何做更精细化的 Embedding，而非仅仅前置处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题二：AI系统中的多路检索与数据源管理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：感谢各位的分享，Skills 给我们带来了更多机会，能创建更多 Agent 和 RAG 应用。同时有一个概念非常重要：我们常说的 RAG 里的“R”，到底指什么？它指的是 Retrieval，是一个“检索过程”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Retrieval 的 source可以是文件系统，可以是数据库，可以是 Web，甚至多种来源并存。所以引申出第二个问题：随着Skills 和 RAG 体系的发展，未来多路检索会越来越常见，RAG 不会消失，它将长期存在于 Agent 体系中。这样一来，数据源头的管理就变得更加重要。最简单的是把数据直接塞进软件系统，但更常见的情况可能是：越来越多的数据会落在数据库中。在这种情况下，当数据库的多路检索能力得到极大增强之后，做RAG 应更多依赖数据库，还是在数据入库层面通过一些技巧将复杂的事情交给基础设施？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：必然入库是最大影响，这也是OceanBase 提出混合搜索（Hybrid Search）概念的核心。如果完全以非结构化数据或切片方式进入系统，召回效率顶天就是向量化的近似能力。去年所有 RAG 产品都在强调从非结构化数据中提取结构化数据，存为 JSON 等半结构化形式，用于前置过滤或与结构化数据一起做混合搜索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么要这样做？本质上是语义理解包含两个层面：一是你问的是模糊问题，但脑子里想的是确定性答案；二是问题模糊，答案也模糊，希望召回所有相关点。大部分实践场景属于第一种。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在文档预处理时，结构化提取非常重要。例如从医疗文档或简历中提取结构化字段，召回时先对结构化数据做精确匹配，再对字段内的非结构化内容做向量检索。半结构化数据解决范围和准确性问题，向量检索解决语义理解问题。通过混合搜索模式，入库时做文档理解提取结构化数据，召回时统一检索，效率会大幅提升。数据库也应在接下来一年面向这个方向发展，我们看到Chroma 等国外开源数据库已在往这个方向演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们比较早做Graph RAG，可能是第一个探索的团队。张老师分享的新架构与我们上一家公司做的 FusionGraph 很像。核心思想是：要让复杂 RAG 系统表现好，索引结构既要贴近知识本质，又要把特定场景的领域知识元信息投射到 Retrieve、Index、Transform 各环节做优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用方法是知识后加工时做Entity Graph 或 Semantic Graph，同时在做 IDP（Intelligent Document Processing）和 Parsing 时，对多层 folder 和复杂章节的长文档要识别 layout，涉及多模态时考虑是否转换模态。要做好这些并能演进，不要过度领域化pipeline，而是按基本原理拆分，确保各组件能力跟上。Database 是重要基础设施，比如RAGFlow 的 Graph 和 Tree 结构能否原生保留、高效检索；要做 Dynamic Agents Retrieve，模型能否自然利用复杂多层结构。数据库的高性能、索引召回率和内置 Hybrid RRF 都很重要，决定系统下限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：在交付过程中，数据源解析是基础且重要，但更重要的是召回（Retrieval）层。即使使用最简单的原始向量，只要检索词和检索语句构建得好，也能得到很好效果，只是效率较差。我们在此基础上扩展了语义化加标量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但标量遇到较大问题：它不固定，用户自己也不知道需要什么标量。我们今年研究的方向是标量的动态扩展，包括用户自身扩展和模型自生成。例如给模型一些Skills，或用户编写场景来生成场景下的标量存入数据库。当然这会引发多租户系统中成千上万标量的高效索引问题，以及渐进式生成问题——很难在预处理时生成所有标量，很多需要在检索时评估并渐进补全。在Retrieval阶段，多标量关联查询的生成方式也借鉴了 Text-to-SQL 的思路。我们希望找到通用存储方式覆盖 80% 场景，目前看语义化加标量检索加动态标量可以覆盖很多场景，所以我们没有用图，因为图是以复杂方式解决复杂问题，而 AI 时代可能有更简单的方式处理复杂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：我们现在是数据库使用者，但曾经也是数据库开发者。从纯技术角度，我非常喜欢&quot;一边推理一边搜索&quot;的技术方向，我称之为 Attention Engine，我认为它也是一种 RAG。DeepSeek 近期已大体实现类似方式，因显存限制不得不用内存，在推理时通过内存索引搜索内容，从外置记忆变为内置记忆。但从商业角度这条路行不通，要求检索与模型延迟极低，必须在同一交换机后，意味着只能卖一体机。因此我们仅作为调研方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从业务视角看，我们最早做Infra 、做数据库时发现离业务太远，后来做 RAG 流量较大，促使我们重新思考 Data+AI 落地生态。我们的观点是：过去数据库是底座，上面写应用做增删改查；现在应用是Agent，底座是以 RAG 为基础的组件，数据库在底层支撑 RAG 中间件。Data+AI 建设不能 AI 和 Data 各干各的，接口有时不清晰，因为中间层用 Python 实现，其好处是适应多变需求，召回策略可随时调整，不过Python 带来的效率问题也让人头疼。AI 时代的数据底座让 Infra 人员直接触达业务，通道变短。因此中间层需要一个 Python 层适应业务多样化，一旦发现好的方式就迅速下沉到数据库解决效率问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在2024 年底就鼓吹跨模态，但至今未落地，因为 Infra 到模型都未准备好。跨模态需要多向量搜索（Tensor Search），用多向量表示图片或文本，语义更准确、排序更准，但数据会膨胀两三个数量级，这是灾难。这需要模型、算法、Infra 共同解决挑战。因此我们需要端到端的、以 RAG 为中间层的体系，这其实就是 Agent 的数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题三：Memory 与 RAG 到底有何区别？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：我非常认同颖峰老师提到的“端到端”。作为 LangChain 社区大使，我们主要做应用层框架，今年非常想做的一件事情是：和各个厂商比如 OceanBase、seekdb一起提供真正的端到端解决方案，服务企业和个人用户，帮助他们快速构建生产级 Agent。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单总结一下几位老师的理解：当我们面向用户提供检索能力时，会在中间层、应用层、数据库层进行多层协同优化，共性问题会逐步下沉到数据库解决。以我的个人体验为例：在最初布道时，我会给大家讲很多RAG 的流程和算法，但从去年底开始，我更多会建议“你直接用这个数据库就好了”，因为它已经帮我们解决了很多多路检索的问题。这种“沉淀”是应用方和数据库厂商不断联合实践的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一个问题也与此有关：我们经常被问到Memory 和 RAG到底有什么区别？从 Memory 召回和从数据库召回有何区别？近期 Clawdbot（OpenClawd）从文件系统读取，到支持 PowerMem 直接接入进行更有效的内存管理。想请教剑南老师，这里做了什么特别工作？以及各位如何理解Memory 与 RAG 的关系？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：Memory 是为让大模型更像人而引入的。如果查询的都是客观事实且不存在人与人之间的理解，RAG 已能解决问题。但问题在于每个人对客观事实的理解和描述不同，加上人有记忆曲线，希望记住昨天强调的内容——这些内容虽非客观事实，但是主观认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如每个人都有一个叫&quot;老王&quot;的朋友，随着时间推移这个&quot;老王&quot;可能已变化，但在记忆中一直叫&quot;老王&quot;，这时 RAG 搞不定，但 Memory 能搞定，因它会更新对&quot;老王&quot;的认知。“老王”是一个知识吗？并不是，因此，Memory 的核心是个性化和千人千面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是RAG 还是 Memory，整体是搭建一整套解决方案面向 Agent 为业务带来价值，不应区分该用 RAG 还是 Memory，而应思考如何组合好共同为业务赋能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们目前做Memory，之前做 Graph RAG。Memory 有广义和狭义之分，狭义指 Agent 或 LLM 需要检索的更外部的 Memory，它确实是特殊的 RAG，特殊在几个方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;原始数据是持续的message thread。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;知识需求是时序性的（temporal），包含两个时间维度：信息创建时间、事件/事实时间。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;时序性存在一个问题，遗忘（forget）是 feature 而非 bug，需结合时间、访问频率和正反馈影响 Retrieval。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;条目层面有category 和不同类型，取决于 Memory 目的，可能需要 schema 区分 ephemeral（瞬时）和 permanent（永久）。&lt;/p&gt;&lt;p&gt;5.&amp;nbsp;不同结构间需要transform 关系，可在 Retrieve 或写入过程触发 event，或周期性处理（类似大脑做梦处理记忆）。&lt;/p&gt;&lt;p&gt;6.&amp;nbsp;多租户和sessional scoping。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果做细会发现与典型RAG 差别很大，但二者又有很大 overlap。RAG Engine 可以处理 Memory，Memory Engine Service 项目也会处理文档，界限会变得模糊。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我理解Memory 算是广义 RAG 的一种，无非也是数据 I/O、Pipeline 处理、特殊数据结构，比较偏个性化。从产品角度看，Memory 目前 C 端个性化场景用得较多。在任务流中，用户提 Memory 的还不多。在技术实践中，Mem0 有工具调用的 Memory 用于长 Agent 任务，但看其架构有点像 Context Engine，与 Memory 又不太一样。所以感觉 Memory 还是 RAG 的一种特殊 Pipeline 形式，没有太大区别，可能实时性比 RAG 更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：单从技术角度而言，Memory 与 RAG 确实没有本质区别，都是 Retrieval。但重要的是 Memory 如何发挥作用，这是在快速变化的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在分享Context Engine 时提到三类数据：企业内部数据、Tools 数据、Agent 使用过程中生成的数据。但它们存储在两个地方：RAG 专有区域和 Memory 专有区域。可见所有大模型生成的内容都要存到 Memory，包括 Skills 的元数据（Skills 本身数据存文件系统）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;怎么存、什么时候存、什么时候取，这些设计点很难决策。例如生成Plan 是否存入 Memory？作为 Plan Cache 有价值，但如果 Human-in-the-loop 干预修改了 Plan，应如何存储？以后如何根据 Memory 数据抽取内部 MCP Tools 的 Skills？这些都是新问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从Infra 角度，RAG 和 Memory 没区别；但从使用者角度，Memory 是重要的基础设施，解锁了大量场景。因此Memory 项目很多（如 Mem0、MemU），但对 Memory 区域的定义（数据库该有哪些表）尚未完全一致，反映 Agent 到底需要什么样的 Memory 还在进化中。不过整个 Agent 体系需要哪些组件，已进入收敛期，就是Context。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;议题四：Skills 开发实践与推荐&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：各位老师都在做Workflow、数据库或融合方案，是否开发了自己的 Skills 帮助用户更好地使用产品？如有请推荐，如无请设想会开发什么样的 Skills 服务开发者？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张颖峰：抱歉我目前没有特别好的推荐。我比较关注如何针对大量内部MCP Tools 生成对应 Skills，这需要一个专门的 Agent 平台来实现。我的观点是：未来Agent 平台可能没有统一标准，所有都是 Coding Agent，但特定 Agent（如低代码、无代码、Workflow）可能因良好交互而便于生成 Skills。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;余金隆：我们内部Skills 用得很多，运营和 SEO、GM 等场景一大把。产研团队用得不算多，主要是代码开发和 Review。交付团队用得特别多：面向用户时遇到各种问题，排查系统后沉淀为 Skills，辅助交付和运维。因此，内部有句玩笑话“交付同学比研发同学更懂系统”，他们做了二十多个 Skills，涵盖工作流搭建、问题排查、RAG 优化等。总体感觉Skills 更像自然语言工作流，虽更抽象，但目前大部分还是偏自然语言的 Workflow。对非开发人员在生产流程上比较友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;古思为：我们维护基于Skills 的插件，在 Skills 发布第二天就推出了 Cloud Code 插件支持。早期没有 Skills 时，我们只能基于 MCP，让插件调用 MCP 的 Custom Command 触发操作，用 Hook 实现功能。后来发现MCP 规范了工具调用，但有两个地方不如 Skills：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;MCP 有 Prompt 抽象，实现为斜杠命令可主动调用类似 Workflow 的东西，但并非所有 Client 都实现，我们要做很多额外工作。Skills 天然支持主动说和自动做。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;Skills 的打包方式让不同工具间组合更灵活。我们内部将 Skills 从 MCP 换成 CLI 后变化很大。例如让 Agent 做 Memory 复杂更新查询时，MCP 需要多轮次，即使 interleave 也不够好。但 CLI 可以动态组合 Linux Shell Pipeline，在一个 turn 里精确完成复杂操作，且内部 CLI/Script 可以 self-contain，打包给用户后自然享受复杂能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;调试经验方面：Skills 比较通用，容易用不同平台测试。我们发现一个有意思的案例：Skills 对应的工具有很多具体选择，如何调优模糊的问题？我们的做法是用最聪明的Agent 做 honest 的复杂 long run 评估，像跟客户聊天一样告诉我们如何改进。有时需要更端到端看细节，不得不自己server model，在 template 解析过程中用小模型发现工具复杂类型定义的问题，虽然其他模型能克服，但会影响 performance。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吉剑南：OceanBase内部沉淀了很多 Skills。Skills 本质是最佳实践，告诉大模型最佳实践是什么，而最佳实践无非两类：一是提升工作效率的工程类（如 Cursor 的 rules），二是业务类 Skills。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Skills 也可以用在 RAG 上，RAG 效率和准确性今天跟两个因素相关：相似度和 Top K。但大家有没有想过，召回前 Top K 和相似度有时不能完全指定，需要反复调，知识库又在更新。如果针对不同的业务实现写不同的Skills，例如当需要某类数据时，希望相似度设到什么位置、Top K 设到什么位置，根据召回结果动态调整，这就变成了一个 Skills。这是 RAG 搞不定的，需要根据具体召回内容判断，是 RAG 的最佳实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前大家可能想是否把RAG 数据放 Skills 里就不用召回了，而我觉得Skills 是对 RAG 的增强。关于OceanBase 的 Skills，我们是有准备的，包括 seekdb 的研发人员今天也在现场，未来应该会有更多相关的 Skills 开放出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;张海立：非常感谢各位老师精彩分享。简单总结：RAG还“行”！只要理解RAG的 R 是 Retrieval，有 Memory、传统数据库等多种数据来源，随着各位老师所在厂商的努力，多路检索能力、应用层提升、流程算法优化都在推进。相信 2026 年RAG会有更大发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Agent 可控思维的工程实现：从分散工具到一体化基座&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次圆桌讨论，为我们清晰地勾勒出2026年AI工程化的演进路径。专家们的共识指向一个明确的结论：构建可靠、可用的 Agent ，其核心不再是追求某个单一组件的极致，而在于如何系统性地整合记忆（Memory）、检索（RAG）与技能（Skills），形成一个协同的“可控思维”体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合专家观点，这一体系的发展呈现出三大趋势。第一，RAG不会消失，反而会变得更加基础与核心。它的内涵正在从狭义的文档问答，扩展为Agent 对所有上下文数据的 Retrieval 能力——无论是企业内部文档、数据库中的业务数据，还是工具（Tools）与技能（Skills）的元数据，都需要被高效检索与调用。未来的RAG将深度融入工作流（Workflow），根据应用反馈动态优化，并与混合搜索（Hybrid Search）等技术结合，实现更精准的“语义理解+精确过滤”。第二，Memory与RAG边界模糊，融合为数据层。从技术基础设施（Infra）视角看，Memory与RAG的本质都是数据的存储与召回。二者的区别更多在于数据特性和使用场景：Memory更侧重于个性化的、时序性的对话与状态记忆；RAG更侧重于客观的、相对静态的知识存储。但在服务 Agent 时，它们共同构成了支撑“上下文（Context）”的数据层。一个优秀的底层平台，应能一体化地管理这两种数据范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，工程复杂度下沉，呼唤一体化数据基座。当应用层通过Skills和灵活编排满足业务多变需求时，通用的、性能瓶颈性的复杂度会自然下沉到底层基础设施。无论是多路检索、混合搜索，还是海量Skills元数据的管理，都对底层数据平台的能力提出了更高要求。专家们指出，未来的理想路径是依赖一个强大的数据基座，它能原生支持向量检索、关系查询与结构化记忆，从而让开发者从繁琐的多系统集成工作中解放出来，更专注于 Agent 本身的业务逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，构建“可控思维”的终极路径，在于选择或打造一个能够统一承载 Agent 记忆、知识与状态的数据基座。这样的基座，正如专家们在讨论中多次暗示的，能够将Memory的个性化记录、RAG的海量知识检索、以及支撑Skills运行的业务数据，融于一个简洁、高效、一致的系统中。它让 Agent 的“思维”过程变得可管理、可观测、可优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，Prompt、RAG、Skills、Memory这些活跃于应用层的概念，都将在这样稳固的基座之上，更好地各司其职、协同工作，共同将 Agent 从“聪明的对话者”转变为“可靠的业务执行者”。这标志着AI应用开发正式进入系统工程时代，而坚实的数据基础设施，是这一切得以实现的基石。&lt;/p&gt;</description><link>https://www.infoq.cn/article/PulhCjGvh2i1xY0rjgSb</link><guid isPermaLink="false">https://www.infoq.cn/article/PulhCjGvh2i1xY0rjgSb</guid><pubDate>Tue, 10 Feb 2026 01:44:06 GMT</pubDate><author>田玮靖</author><category>数据库</category></item><item><title>OpenAI推出免费的LaTeX原生工作空间Prism，并集成GPT-5.2</title><description>&lt;p&gt;OpenAI发布了&lt;a href=&quot;https://openai.com/prism/&quot;&gt;Prism&lt;/a&gt;&quot;，这是一个基于云的免费LaTeX工作空间，专为学术写作和协作而设计，并且直接集成了GPT-5.2。该平台将文档编辑、编译、引文管理及AI辅助修订功能整合在单个基于Web的工作区中，主要面向需要撰写长篇科学文献的研究人员。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism完全支持LaTeX原生操作，并且是完全在浏览器中运行。用户可以创建、编译和预览文档，无需安装本地工具或管理LaTeX环境。该平台消除了现有LaTeX协作工具中常见的限制，对项目数量、协作者数量或编译时间没有任何限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism的核心优势在于将GPT-5.2集成到了文档工作流中。与通过单独的聊天界面操作不同，该模型直接在项目的上下文中运行，可以访问文档结构、公式、参考文献和之前的修订。这使得它能够协助执行诸如修订文本、调整格式、更新公式和表格以及查找相关文献这样的任务，同时保证文档内部逻辑的一致性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Prism内置了引文管理功能，并支持与Zotero同步以发现参考文献。实时协作功能允许多个作者同时编辑文档，内联评论和专题讨论支持同行评审和反馈。自动化错误检查、公式转换和格式化工具旨在减少手动更正和重复的LaTeX调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次发布在研究人员中间引发了关于Prism与Overleaf等工具的对比讨论。Povilas Karvelis&lt;a href=&quot;https://x.com/KarvelisPovilas/status/2016289812832014511?s=20&quot;&gt;指出&lt;/a&gt;&quot;：&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我认为这种情况还会持续几年，直到知识图谱和AI代理成为主要的研究手段，使精心撰写的研究论文彻底过时。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他早期用户强调了该平台的定价模型所带来的实际影响。一位研究人员&lt;a href=&quot;https://www.reddit.com/r/OpenAI/comments/1qolehz/comment/o2680kw/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;集成AI是Prism提供的功能中最不实用的。仅仅是让我可以免费拥有无限数量的项目和协作者，就使它成为比Overleaf更好的选择。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术的角度来看，Prism的定位是一个集成的写作和协作环境，而不是一个AI优先的工具。AI辅助功能是可选的，并且嵌入到了标准的学术工作流中，团队可以有选择地使用。核心功能的使用不依赖自动化辅助。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，拥有ChatGPT个人账户的用户可以通过Web访问Prism。OpenAI表示，未来版本将陆续支持ChatGPT商业版、团队版、企业版及教育版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/openai-prism/&quot;&gt;https://www.infoq.com/news/2026/01/openai-prism/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6kVGMgU5thp3AlJKeb0d</link><guid isPermaLink="false">https://www.infoq.cn/article/6kVGMgU5thp3AlJKeb0d</guid><pubDate>Tue, 10 Feb 2026 00:54:01 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>.NET 10现已在AWS Lambda上作为托管运行时和基础镜像提供</title><description>&lt;p&gt;亚马逊云科技宣布，AWS Lambda现在已经&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2026/01/aws-lambda-dot-net-10/&quot;&gt;支持&lt;/a&gt;&quot;使用.NET 10创建无服务器应用程序。通过这次更新，在构建和运行Lambda函数时，开发者可以将.NET 10作为托管运行时和基于容器的镜像来使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按照亚马逊云科技的说法，当有新版本发布时，托管运行时和基础镜像会自动更新，不需要开发团队手动维护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/11/dotnet-10-release/&quot;&gt;.NET 10&lt;/a&gt;&quot;是.NET平台最新的长期支持版本，在2028年11月之前会一直提供安全更新和Bug修复。通过在AWS Lambda上提供.NET 10，亚马逊云科技旨在使开发者能够在无服务器环境中使用平台的最新特性。这包括支持基于文件的应用程序，旨在简化应用程序结构和开发工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次发布还增加了对&lt;a href=&quot;https://docs.aws.amazon.com/lambda/latest/dg/lambda-managed-instances.html&quot;&gt;Lambda托管实例&lt;/a&gt;&quot;的支持。这项能力使Lambda函数能够在Amazon EC2实例上运行，同时保留通常与无服务器计算相关的操作模型。亚马逊云科技表示，这个选项旨在提供更多的灵活性，包括潜在的成本效益和对专用计算资源的访问权限，同时减少通常与服务器管理相关的运营开销。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，Powertools for AWS Lambda (.NET)是一个旨在帮助开发者遵循无服务器最佳实践并提高开发速度的工具包，现在也已提供.NET 10支持。开发者可以继续使用亚马逊云科技提供的各种工具来部署和管理他们的应用程序，包括Lambda控制台、AWS Command Line Interface、AWS Serverless Application Model、AWS Cloud Development Kit和AWS CloudFormation。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如官方公告所言，.NET 10运行时可以在所有AWS区域中使用，包括AWS GovCloud（美国）区域和中国区域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社区对这一公告表现出了很大的热情，并进行了技术探讨。&lt;a href=&quot;https://www.reddit.com/r/dotnet/comments/1q7p9t3/aws_lambda_supports_net_10/&quot;&gt;Reddit上的.NET开发者&lt;/a&gt;&quot;们既充满期待又带着务实的好奇，众多评论聚焦于.NET 10带来的全新的基于文件的应用开发体验。有社区成员表示，一旦基于文件的应用编辑能和常规JavaScript工作流一样流畅，他们会“欣喜若狂”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一些人讨论了构建工具、使用当前的CLI方法所需的部署步骤，以及可能对冷启动性能产生的影响。从这些讨论中可以看出，总体而言，.NET开发者对这个扩展的无服务器选项是认可的，并且对未来改进Lambda工具和编辑器支持也很感兴趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技还发布了&lt;a href=&quot;https://aws.amazon.com/blogs/compute/net-10-runtime-now-available-in-aws-lambda/&quot;&gt;一篇详细的博文&lt;/a&gt;&quot;，演示如何在AWS Lambda中使用新的.NET 10运行时。该文通过一个示例展示了如何创建、配置和部署基于.NET 10的Lambda函数，并解释了可用的运行时和部署选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f6/f690f77c19f2e2b5793a499a5fe9a5a3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lambda控制台的创建函数页面，图片来源：&lt;a href=&quot;https://aws.amazon.com/blogs/compute/net-10-runtime-now-available-in-aws-lambda/&quot;&gt;亚马逊云科技博客&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据亚马逊云科技的说法，该示例旨在帮助开发者利用他们提供的标准工具在现有的无服务器工作流中采用.NET 10。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;亚马逊云科技的官方文档和公告材料中提供了&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2026/01/aws-lambda-dot-net-10/&quot;&gt;完整的发布说明&lt;/a&gt;&quot;和其他一些细节，感兴趣的读者可以进一步阅读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/dotnet-10-available-for-aws/&quot;&gt;https://www.infoq.com/news/2026/01/dotnet-10-available-for-aws/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/5dsQgAfhRJSyvZNtZWCh</link><guid isPermaLink="false">https://www.infoq.cn/article/5dsQgAfhRJSyvZNtZWCh</guid><pubDate>Tue, 10 Feb 2026 00:51:55 GMT</pubDate><author>作者：Almir Vuk</author><category>亚马逊云科技</category><category>云计算</category></item><item><title>前 Codex 大神倒戈实锤！吹爆 Claude Code：编程提速 5 倍，点破 OpenAl 死穴在上下文</title><description>&lt;p&gt;OpenAI Codex 的核心研发者，竟然成了 Claude Code 的忠实用户？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen 是 Segment 联合创始人、前 OpenAI 工程师、Codex 项目的早期研发者。他最近在一档播客中，对当前最火的代码智能体 Codex、Claude Code 和 Cursor 进行了锐评。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e96b65eaec6de20ca9f97978622c038b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结论出人意料，他最常用、也最偏爱的，是 Claude Code，他表示搭配 Opus 模型更“香”。&lt;/p&gt;&lt;p&gt;Calvin 用了一个极具画面感的比喻，来形容用 Claude Code 的体验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;就像残疾人换上了一副仿生膝盖，写代码的速度直接提升了 5 倍。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，Claude Code 真正的杀手锏，是极其有效的&amp;nbsp;上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对复杂任务，Claude Code 会自动生成多个&amp;nbsp;探索型子智能体，独立扫描代码仓库、检索上下文，再将关键信息汇总反馈。这种设计，显著降低了上下文噪音，也解释了它为何能稳定输出高质量结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，他也肯定了自家产品，认为 Codex 很有“个性”，像 AlphaGo。在调试复杂问题时的表现上，Codex 堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“上下文管理”，是 Calvin French-Owen 在整期播客中反复强调的关键词。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为，代码的上下文信息密度极高，只要检索方式得当，模型往往比人类更容易理解系统结构。但与此同时，上下文窗口本身，也成为制约代码智能体发展的最大瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提到上下文污染的问题时，主持人表示 LLM 会变笨。Calvin 趁此分享了一个非常实用的经验：当上下文 token 占用超过 50%，他会主动清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他甚至分享了一种创业者常用的&amp;nbsp;“金丝雀检测”&amp;nbsp;方法：在上下文里埋入一些无关但可验证的小信息，一旦模型开始遗忘，说明上下文已经被污染。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品理念上，Calvin 认为 Claude Code 与 Codex 的差异，早已写进两家公司的基因里：&lt;/p&gt;&lt;p&gt;Anthropic 更关注“做出适合人用的 AI”OpenAI 更关注“做出最强的 AI”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他判断，从长期来看，OpenAI 的路线可能是必然趋势，但就当下的使用体验而言，他更偏爱 Anthropic。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在谈到未来时，Calvin 给出了一个明确判断：&lt;/p&gt;&lt;p&gt;公司会变小，但数量会变多每个人都会拥有自己的智能体团队而最先被放大的，是具备“管理者思维”的资深工程师。他们更擅长拆解问题、判断取舍、以及在正确的节点上向智能体下达指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，产品的分发方式&amp;nbsp;变得前所未有地重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自下而上的分发模式，正在以前所未有的速度扩散。工程师不会等审批、采购，只会用脚投票。&lt;/p&gt;&lt;p&gt;相比大公司对安全、合规和控制权的高度重视，开发者更在意的，依然是那句最朴素的评价：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“这东西，真的好用。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是播客精彩细节，AI Coding 干货密集，欢迎阅读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;我迷上了 Claude Code，它太好用了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：Calvin French-Owen 是 OpenAI 旗下 Codex 代码模型的首批研发者之一，在此之前，他创立了 Segment 公司，这家公司市值数十亿美元，最终被知名企业高价收购，成功实现资本变现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：说实话，现在对我们所有人来说，都是一段充满变数的时期。我最近彻底迷上了 Claude Code，用一个比喻来说，十年前我还是个马拉松爱好者，特别喜欢跑步，结果后来膝盖受了重伤，这之后我就进入了所谓的 “管理者模式”，再也没写过代码，想想真的很可惜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但过去这九天，仿佛打开了新世界的大门，我找回了曾经写代码的所有感觉，就好像换了个全新的膝盖，而且还是仿生的，能让我写代码的速度快了 5 倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你怎么看待这款工具？毕竟你一直身处这个领域的前沿，Codex 开创的很多理念，至今仍被大家广泛使用，而且这款模型还在持续迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我在 OpenAI 工作时，负责 Codex 的网页端项目，当时 Cursor 这款工具刚面世，他们基于 GPT-3.5 做了一个适配层，能在 IDE 中使用。Claude Code 也刚发布，它是基于 CLI 运行的，当时我们就有一个想法：未来的编程，应该更像和同事沟通 —— 你提出问题，对方去处理，最后带着 PR 回来反馈。我们的网页端项目就是从这个想法出发的，这也是我们当时的研发方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在看来，这个大方向其实是对的。但显然，现在大家都改用 CLI 编程了，不管是 Claude Code 还是 Codex，这类工具的使用频率都高了很多。至少对我来说，这件事带来的启示是，某种程度上你说得对，未来每个人或许都会成为 “管理者”，这是我的个人观点。但要达到那个阶段，需要一步步来，你得真正信任模型，并且理解它的工作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你最近一直在用 Claude Code，把它纳入你的核心技术栈后，使用体验上有什么变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Claude Code 现在确实是我日常编程的主力工具。&amp;nbsp;说实话，我的主力工具每隔几个月就会换一次。之前有段时间我特别偏爱 Cursor，它新出的模型速度很快，用起来确实不错。后来我慢慢转到了 Claude Code，尤其是搭配 Opus 模型使用时，体验更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 是款很有意思的产品，我觉得大家都低估了它在产品设计与模型层面的协同表现。要是你深入研究就会发现，Claude Code 最厉害的地方，就是它的上下文拆分能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如需要调用功能、让子智能体协同工作时，你让 Claude Code 执行某个任务，它通常会生成一个甚至多个探索型子智能体。这些子智能体会通过 ripgrep 工具扫描整个文件系统、检索相关内容，而且每个子智能体都有独立的上下文窗口（context window）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为 Anthropic 在这点上做得特别出色 —— 面对一项任务，模型能精准判断出，这个任务适合在单个上下文窗口（context window）中完成，还是需要拆分后再执行。模型在这方面的表现堪称惊艳，这也是它能输出高质量结果的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，依托终端运行的特性，Claude Code 成为了实现可组合原子化集成的最纯粹形式。如果你习惯了从 IDE 入手做开发，比如用 Cursor 或是早期的 Codex，就会发现，这种更灵活的上下文检索方式，其实并不容易自然而然地实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这一点确实很独特。我个人挺意外的，不知道你有没有这种感觉，总觉得有种复古的未来感，二十年前的 CLI 技术，居然打败了本被寄予厚望的各类 IDE。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同。而且 Claude Code 不是 IDE，这一点其实很关键，因为它能让你和正在编写的代码保持一定距离。IDE 的核心就是浏览文件，对吧？你需要把所有代码状态记在脑子里，还要理清其中的逻辑。但 CLI 完全不同，这让它在使用体验的设计上有了更大的发挥空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有这种感觉，我用 Claude Code 的时候，感觉就像在代码里 “飞驰”，各种操作都特别顺畅。界面上会有小的进度指示器，随时给我状态反馈，而编写的代码本身反而不是视觉的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发环境本来就很杂乱，我特别喜欢 sandbox（沙箱）在概念上的简洁性。但实际使用时，我遇到了很多棘手的问题，比如就连简单的测试都搞不定：sandbox（沙箱）需要访问 PostgreSQL 数据库，却一直连接失败；我写的 codex.md 文件只有二十行，最后还是无法运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 CLI 里，工具可以直接访问开发数据库。我不确定这么做是否合规，但我确实试过让它访问生产数据库执行一些操作，而且它真的做到了。比如有一次，我遇到了一个并发问题，想排查一下，结果发现这款工具居然能调试五层嵌套的延迟任务，找出问题所在，还能自动编写测试用例，之后这个问题就再也没出现过。这真的太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。而且我觉得产品的推广和使用获取方式，被严重低估了。想想 Cursor、Claude Code 还有 Codex 的命令行版本，你只需下载就能用，不用向公司申请任何使用权限，这一点带来的使用体验差异，实在太大了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;做好上下文管理，是用好顶尖模型的诀窍&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你在代码智能体领域有很多实践，对于想要打造这类工具的人，你有什么建议？有哪些实战经验可以分享？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得最重要的一点，是做好&amp;nbsp;上下文管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们为一款推理模型搭建了检查点，随后基于强化学习（RL）对它开展了大量微调工作：我们会给模型布置各类编程相关任务，比如解决编程问题、修复测试用例、实现新功能，再通过强化学习的方式，训练模型如何更精准地应对这些任务。当然，目前大多数人还做不到这一步，但大家力所能及的是，多思考该给智能体提供哪些上下文信息，才能让它输出最优的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如观察 Claude Code 的工作过程，它会生成多个探索型子智能体，这些子智能体会去检索文件系统里的各类代码相关内容，完成后会把上下文信息带回来并为我做好总结，我就能清楚后续该怎么推进工作了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;看不同智能体的上下文构建方式，是件特别有意思的事。比如 Cursor 用的是语义搜索的方式，它会把所有内容转化为向量形式，再匹配和查询需求最相关的内容；而 Codex 和 Claude Code，其实用的都是 ripgrep 这个代码搜索工具。这种方式之所以管用，是因为代码的上下文信息密度很高。&amp;nbsp;一行代码通常不到 80 个字符，代码仓库里不会有太多大数据块或 JSON 格式的文件，就算有，数量也极少。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你可以参考 Git（代码版本管理工具）的忽略规则，先过滤掉无关内容或是已打包的文件，再通过 Git 和 ripgrep 查找代码的上下文，这样就能很好地理解代码的实际功能了。同时这类工具还能自动扫描整个文件夹的结构，而且 LLM（大语言模型）特别擅长生成复杂的 Git 命令，这些命令让人类手动写的话，简直是种折磨。而这一整套操作，其实就是强化学习（RL）在实际场景中的落地应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我现在也在做非编程领域的智能体集成系统，从代码智能体的研发过程中，我也学到了很多：要把数据转换成接近代码的格式，让模型能快速检索到相关的周边信息，进而获取到结构化的有效数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：优秀的代码智能体，核心能力就是上下文工程，那要成为这类工具的前 1% 顶尖用户，有什么技巧？你的技术栈是怎样的？你是如何借助这些工具大幅提升效率的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：第一个技巧，是尽量减少底层代码和基础架构的编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我平时会在 Vercel、Next.js 或 Cloudflare Workers 这些平台部署技术栈，这些平台已经封装了大量样板代码，不用自己费心搭建各类服务，也不用处理服务发现、中心端点注册、数据库配置这些问题。所有功能基本都能在一两百行代码内实现。我也倾向于采用微服务架构，或者使用结构清晰的独立软件包。&lt;/p&gt;&lt;p&gt;其次，要了解 LLM 的核心优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其实代码智能体的特点，Andrej Karpathy 最近也在推特上提到过：它们的执行力极强，不管遇到什么问题，都会一直尝试解决，最终往往会在现有基础上做更多的拓展。所以如果你想引导它完成某个任务，一定要明确指令。&amp;nbsp;这里可以稍微拿 OpenAI 举个例子，他们有一个庞大的 monorepo（单体代码仓库），已经用了好几年，有成千上万的工程师在上面提交代码。这些工程师里，有经验丰富的资深开发者，他们精通生产环境代码的编写；也有刚毕业的博士，编程经验相对欠缺。人员构成差异很大，所以 LLM 会根据你的引导方向，学习不同的代码风格。我觉得代码智能体还有很大的探索空间，比如研究出最优的代码生成范式。显然，给模型提供自我校验的方式，能大幅提升它的表现，比如尽可能多地在代码检查、CI 等环节运行测试用例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我自己也会频繁使用代码审查机器人，YC 孵化的 Reptile 公司做的这款机器人用起来就特别顺手；Cursor 的漏洞检测机器人也很好用，我也常常用 Codex 做代码审查，它在校验代码正确性这块的表现尤其突出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些都是代码智能体格外擅长的领域，除此之外，它们探索代码仓库的能力也很出色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，智能体也有短板：它们擅长做拓展，但如果你的需求不是拓展功能，它们往往会重复编写代码，浪费大量时间做已经实现过的功能，这时候你就会觉得 “它完全没理解我的需求”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个问题是上下文污染，智能体可能会陷入某个循环，因为执行力强，会一直沿着错误的方向推进，而它参考的上下文信息，其实对于解决问题毫无帮助。所以我常用的一个方法，是主动清理上下文，比如当上下文的 token 占用率超过 50% 时，就及时清理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：哇，这个比例其实特别关键。不知道你有没有关注到，YC（Y Combinator 的缩写，全球顶级的创业孵化器）2024 年秋季孵化营里，那家做 HumanLayer（人类层）的公司，创始人 Dex Horthy 就总聊这个话题，还专门提出了 “LLM 愚笨区”的概念：当上下文的 token 数量达到某个阈值后，模型的输出质量就会开始下滑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全认同这个观点，结合强化学习（RL）的工作逻辑来看，这一点就更明显了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;想象一下，你是一名参加考试的大学生，考试刚开始的五分钟，你会觉得时间很充裕，一定能好好答题，认真思考每个问题；但如果只剩五分钟，试卷还有一半没做完，你就会慌不择路，只求尽快写完。LLM 的上下文窗口（context window），就是这个道理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创业者们有一个小技巧，我觉得很实用：在上下文开头加一个 “金丝雀检测” 信息，就是一些特别小众甚至有趣的内容，比如 “我叫 Calvin French-Owen，早上八点喝了茶” 这类无关的小事实。然后在和模型的交互过程中，时不时问它 “你记得我叫什么吗？”“你记得我几点喝的茶吗？”，如果它开始忘记这些信息，就说明上下文已经被污染了。&amp;nbsp;这是我见过很多人用的方法，我自己还没试过，但完全相信它的效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：这个方法很有意思。我在模型做上下文压缩前，还没遇到过这类问题，可能是我没太留意。你是说，token 数超标后，模型会开始做出一些不合理的操作？我得留意一下，这个问题能在 Claude Code 内部解决吗？比如让模型自己做检测，在上下文里加入类似 “心跳检测” （通过定期发送 “状态确认信号”，实时监控目标对象的运行状态，一旦信号异常就触发预警或处理）的机制，实时监控状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：理论上可以，但目前还做不到。我认同你的终极设想，但现在要做好上下文管理，依然很难。目前的解决办法，还是拆分上下文窗口（context window），然后尝试合并信息，但 Claude Code 的会话结束后，上下文的内容就是固定的，这一点还是有局限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，Codex 采用了完全相反的策略，OpenAI 的博客最近也提到了：它会在每次交互后定期做上下文压缩，所以 Codex 能长时间持续运行。&amp;nbsp;你看 CLI 里的 token 占用百分比，就能看到它会随着压缩操作上下浮动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Anthropic 要做人用的，&amp;nbsp;OpenAI 要做最好的，以及产品分发模式很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：看来 Claude Code 和 Codex 的架构差异很大，Codex 似乎更适合长时间运行的任务，所以二者的使用场景不同，架构设计也天差地别。现在看来，CLI 的工具越来越火，2026 年可能会成为 “CLI 元年”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但同时也有观点认为，通用人工智能已经到来，超级人工智能也近在咫尺。目前的代码智能体已经非常智能，但还达不到自主长时间运行的程度，如果计算能力提升十倍，能实现 24 小时甚至 48 小时的自主任务运行吗？Codex 的架构，能适配这种场景吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这是个很好的问题，答案其实藏在两家公司的创立基因里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 一直很注重打造适合人类使用的工具，比如会关注模型的输出风格、语气，以及如何和用户的其他工作流程适配，Claude Code 就是这一理念的自然延伸。在很多方面，它的工作方式和人类很像：比如你要建一个狗窝，人类会去五金店买材料，然后研究如何组装，Claude Code 也是如此。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 OpenAI 的核心思路，是训练出最优秀的模型，通过持续的强化学习（RL），让它能处理更长期、更复杂的任务，最终实现通用人工智能。所以它的模型，工作方式可能和人类完全不同。还是以建狗窝为例，就像 AlphaGo 的下棋思路和人类不同一样，OpenAI 的模型可能会直接用 3D 打印机，从零开始打印出一个狗窝，完全符合你的需求，过程可能会很长，成品也会高度定制化，甚至有些设计会很怪异，但最终能实现功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;或许从长远来看，这才是正确的方向，所以很期待两家公司的后续发展。总的来说，OpenAI 的路线似乎是必然趋势，但我个人更喜欢 Anthropic 的思路。&amp;nbsp;十年前，我还会自己写一些奇怪的脚本，在重构代码或理解代码逻辑时，用它来梳理各类信息，而 Claude Code 给我的感觉，和当年的这种体验一模一样，用它一天，能完成五个人的工作量，&amp;nbsp;就像给编程装上了火箭助推器，太不可思议了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：很期待不同规模的公司，会如何应用这类工具。我发现，不管是业余爱好者，还是小型创业公司，都在尽可能挖掘代码智能体的潜力，因为他们根本没时间研究其他方法。创业公司的资金和时间都有限，一切都要以速度为核心。但大公司不一样，他们有太多东西可以失去，还有各种代码审查的内部流程，也已经组建了庞大的技术团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来可能会出现一种很有趣的现象：一个人组成的小团队，看到其他团队的工作效率低，就会自己用代码智能体做一个原型，效果反而更好。总有一天，这种小团队的成果会超越大团队，行业格局的转变，一定会很有意思。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实前几天我试了一款产品，它的用法很有意思：你下载一个桌面应用，它会调用你电脑上运行的 Claude Code，再通过 MCP 服务器和桌面应用通信。这种方式让电脑的使用变得很不一样，你不用征得任何人同意，下载后直接用就行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个变化飞快的时代，产品的分发模式真的太重要了，自下而上的模式远比自上而下好，因为后者的效率实在太低。&amp;nbsp;公司的首席技术官总会顾虑安全、隐私问题，担心各种突发情况，想要绝对的控制权，但工程师们只会直接装上工具开始用，然后感叹 “这东西太好用了”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你说得太对了。我本身是做企业级 ToB 业务的，总觉得自上而下的销售模式能构建一定的竞争壁垒，肯定会有公司找到方法，做出一款人人都能用上的产品，或许先从个人用户切入会是个思路。&lt;/p&gt;&lt;p&gt;当年的网景导航器（互联网早期最具里程碑意义的网页浏览器）就是如此，它对非商业用途免费，结果很多人下载后用在商业场景，网景就通过追踪 IP 地址，统计不同公司的使用量，然后告知对方 “你们违规使用了，只需购买授权就能继续用”。我很好奇，这种模式现在还能复制吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：你关于分发模式的观点很有意思，现在很多人甚至会直接根据 Claude Code 的建议做架构决策，他们可能都不知道该用什么分析工具，只要 Claude Code 说用 PostHog（ YC W2020 批次孵化的开源平台 PostHog，核心定位是给开发者和产品团队的 “全能型产品优化工具箱”），他们就会百分百采用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我做顾问的一家公司，最近聊到了他们的生成式优化策略，也就是如何在聊天机器人中优化展示效果。他们说有件事特别有趣：竞争对手整理了一份行业内必用的五大工具榜单，自己的产品当然排在第一位。明眼人一看就知道这是偏见，榜单里的头部工具就是他们自己的产品。但 LLM 会被这种信息误导，它会整合各类上下文信息，然后判定 “这是行业顶级工具”，接着直接推荐给用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得做开发者工具的话，完善的文档、真实的用户口碑，甚至在 Reddit 上的一些讨论，这些都能极大地提升产品的认可度，这也是很多开源项目能快速崛起的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Supabase 就是个典型例子，它去年发展得特别快，部分原因就是它的开源文档做得特别好，详细教大家如何搭建各类功能。只要有人问如何搭建类似 Firebase 的后端事务系统，LLM 给出的默认答案几乎都是 Supabase。我亲自试过很多次，结果都是这样。它就像当年的 Stack Overflow 和谷歌搜索一样，占据了互联网的信息入口，现在大家甚至都不用谷歌了，想想真的很神奇。而且这种模式对开源项目的利好是不成比例的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不知道你有没有看到，Ramp 公司最近发了一篇博客，讲他们如何打造自研的代码智能体，里面提到他们用开源代码作为框架，因为模型可以直接读取源代码，理解其工作逻辑。我对开源产品一直这么做：克隆代码仓库，然后启动 Codex 或 Claude Code，让它讲解代码的逻辑，用起来特别实用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp; 未来公司会变小，数据很重要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们不妨畅想一下四十年后的未来：软件、数据库、访问控制依然存在，但软件的核心会高度个性化。访问控制、权限分配这类事，依然是大家开会讨论的重点，也就是所谓的 “管理者模式”，但公司的其他所有功能、规则，都由员工通过自己的 Claude Code 这类工具定义。可能还是 CLI，也可能是由大量智能体组成的协作体系，那会是一种怎样的场景？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如想象一下，现在如果有公司要接入 Segment，我们复刻代码仓库，给他们一个专属版本，让它在自己的服务器上运行；如果他们想做修改，只需在聊天窗口告诉智能体，智能体通过代码循环完成编辑，而 Segment 总公司推出新功能后，智能体还能自动完成版本合并。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我完全能想象出这种场景，这也是我一直在思考的。虽然不知道这个未来还有多远，但最终，每个工作的人都会有自己的云电脑和专属的云智能体团队，智能体替自己处理各类事务，彼此之间也会沟通协作。&amp;nbsp;这就像有一个&amp;nbsp;超级执行助理，它会告诉你 “这些是你需要关注的事”“你可以快速做这些决策”“这件事需要你多花时间”“你该和这些人见面沟通”。我觉得，人与人之间面对面交流、交换想法的需求，永远不会消失，至少我能从这种交流中获得很大的满足感。除此之外，会有大量的智能体替人类执行任务，实现各类工作的自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的公司，平均规模可能会变小，但数量会更多，能做的事也会更多。&amp;nbsp;我还很好奇，Paul Graham 提出的 Maker Schedule（创作者日程：给做核心创作 、研发的人用的，需要大块、连续、不被打断的时间） 和 Manager Schedule（管理者日程：给做管理、协调、沟通的人用的，时间是碎片化、以小时为单位的，充满会议、沟通、临时决策，习惯频繁切换事务），未来会演变成什么样子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 YC，我们的工作基本都是 Manager Schedule（管理者日程），这让我们很难有时间自己写代码、做产品。但现在有了代码智能体，一切都变了，很多合伙人开会时，就像这期播客刚开始时我做的一样，让智能体后台运行处理任务，自己专注开会，等会开完，任务也完成了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错，就是利用碎片化时间。以前编程，至少需要四个小时的整块时间，否则根本不值得开始，对吧？这其实也反映出编程方式的巨大变化：以前写代码，你需要把所有类名、函数、关联的代码都记在脑子里，构建自己的“上下文窗口”，这个过程需要好几个小时，所以想用十分钟的碎片化时间编程，根本不可能，只会让人觉得沮丧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我觉得未来的核心基础能力之一，依然是保持数据模型的一致性，而核心的记录系统，也有机会率先实现智能体化。&amp;nbsp;现在我们的工作，还是高度依赖数据库，以及底层的 SQL 或 NoSQL 查询，但未来或许会出现一种工具，能为定制化软件的各类视图，自动生成所需的所有数据。&lt;/p&gt;&lt;p&gt;未来的软件世界，会有大量定制化视图，但数据的准确性，依然是核心前提。&amp;nbsp;数据的重要性不言而喻，这一点从很多公司的做法中就能看出来：比如很多公司通过 API 或 MCP 开放数据访问权限，而 Slack(全球最主流的企业级团队协作与即时沟通平台，常被称作「硅谷版钉钉 / 企业微信」) 就收紧了 API 的权限，因为他们不想让用户把平台上的所有数据都导出，然后基于这些数据搭建智能体应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你对这款智能体的了解很深，那你觉得，这类工具普及后，哪种类型的工程师会受益更多？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：总的来说，工程师的资历越深，受益就越多。因为智能体特别擅长把想法转化为实际行动，如果你能用几句话清晰地描述需求，就能立刻让它落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在浏览开源代码仓库时，经常会有这种感受：看到某处代码，觉得可以优化，只要把这个想法告诉智能体，让它去执行，最后等待反馈就行。这种方式能极大地提升效率，放大个人的影响力。&lt;/p&gt;&lt;p&gt;其次，能判断哪些代码修改在架构层面是合理的、哪些是不合理的，或者能准确判断该在哪个节点向智能体发出指令，这一点也很重要。我觉得做事有条理、带有 “管理者思维” 的工程师，会更适配这类工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且目前来看，这个领域还缺少一款核心产品，比如类似 Conductor 这样的工具，能整合你所有的会话，提醒你 “这个任务已经完成，需要你确认”“你该把注意力转到另一个任务上了”。Conductor（核心解决 AI 编程的 “失忆问题）这类工具，应该给智能体加上上下文管理功能，其实人类也需要这样的上下文管理工具，这一点是毋庸置疑的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果让你回到大学，重新学习计算机科学，让你自己制定课程表，你会选择学习哪些内容？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：就我个人而言，理解各类系统的工作原理，依然是最重要的。&amp;nbsp;比如 Git、HTTP、队列这类数据库，了解这些系统的基础概念，至关重要。另外，我会专门安排一个学期&amp;nbsp;，每周都动手做项目，尽全力挖掘模型的潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用模型的过程中，你会发现，遇到问题时，总能向上层抽象，让模型来解决。比如你可以给模型一个 “实现” 命令，让它完成计划的下一阶段；也可以给一个 “全部实现” 命令，让它分阶段执行，生成新的子智能体；还能给一个 “校验” 命令，让它自查成果。模型的能力边界一直在变化，所以多动手尝试，是很有必要的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一件事让我觉得很有意思，我特别想教 18 到 22 岁的年轻人做产品。我们这桌人，都做出过用户真正需要、真正喜欢的产品，该怎么把这种能力教给年轻人，是一个值得思考的问题。&amp;nbsp;我很好奇，五年后的年轻人，会不会在产品审美等方面远超现在的我们？因为他们能借助智能体，做出更多的尝试，产出更多的成果。他们本就该如此，不是吗？他们的产品落地速度、接触现实的机会，应该是上一代人的十倍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我有一个疑问，不知道你有没有这种感受：我小时候，妈妈总跟我说 “别一心二用，根本没认真听我说话”。这话其实有道理，我当时确实盯着电脑，没认真听，但我发现，我比父母那一代人更擅长多任务处理。而现在的年轻人，比我们更厉害，因为他们成长在互联网时代，每天接触抖音这类短视频，应对各种碎片化信息。我觉得，未来既需要能深度思考的人 —— 他们能专注观察、理解问题、解决问题，也需要能灵活切换场景的人 —— 他们能同时处理多个任务，不断切换上下文，也就是所谓的 “注意力缺陷多动障碍模式”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错，新一代的年轻人特别擅长这一点。我一直觉得，有一种聪明人，或许是带有注意力缺陷多动障碍的特质，他们脑子里同时酝酿着很多好项目，但从来没有真正完成过一个。我自己可能就有点这种性格。我之前发布了自己的氛围代码，其实如果不是 Claude Code，我根本完不成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，有些人的大脑就像有十个分支同时运转，但一天的时间有限，根本没法把所有想法都落地，所以项目总是半途而废。而现在，Claude Code 能帮我把所有想法都落地。&amp;nbsp;你在博客里也提到过，用它的感觉就像玩电子游戏，总有新鲜感。比如你开始做一个项目，做到一半觉得无聊，又有了新的想法，想先做新想法，再回头做原来的项目，以前这么做，很容易半途而废，但现在有了智能体，两个项目最终都能完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：十岁的孩子每天都有写作作业，昨天他第一次用人工智能写作业，我一看就知道，那些表达根本不是一个十岁孩子能写出来的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我想到，我们现在和很多 18 到 22 岁的年轻人合作，他们有实习经历，但没有做过管理工作，不懂产品市场匹配后的运营逻辑 —— 当你面对数百万的任务队列、数十万的错误日志时，才是真正的管理工作。这份工作其实很枯燥，要逐行排查错误日志，还要在后台手动确保产品对所有用户都能正常运行。&lt;/p&gt;&lt;p&gt;新一代的开发者，该如何理解这些内容？Claude Code 这样的智能体，能教他们架构设计这类知识吗？还是说，他们只能自己踩坑试错，在摸索中成长？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我做产品的过程中，花最多时间思考的，就是产品的核心范式：用户现在需要理解哪些内容？他们能借助哪些基础能力，实现自己的各类需求？&amp;nbsp;我总喜欢用 Slack 举例子，它其实算不上什么全新的概念，在此之前已经有很多聊天工具了，但它把频道、消息、互动功能做的极简，普通人一看就懂，知道该怎么用，这就是它的成功之处。但一旦用户习惯了这种模式，后续再想改变就很难了，比如想改成以文档为核心，或者现在想加入智能体功能，都很难改变用户的固有认知。所以我做产品时，从一开始就会仔细考虑这一点，因为给代码智能体设定的核心规则，会成为它一直遵循的准则，并且不断拓展延伸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;代码智能体的制约因素有哪些&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：说到这里，我很好奇，如果现在让你用当下的工具，重新打造 Segment，你会怎么做？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：Segment 的业务其实很有意思，我们最初的核心，是做各类集成功能：把相同的数据，对接至 Mixpanel、Kissmetrics、谷歌分析等平台。以前写这类集成代码，繁琐又困难，所以用户愿意付费使用。但现在，这项工作的价值几乎降为零，甚至很多时候，你直接告诉 Claude Code 或 Codex“我想这样做数据映射，需要这个特定功能”，它就能精准实现，完全契合你的需求。所以 Segment 的集成业务，价值已经大幅缩水。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但保持数据管道（data pipeline）的稳定运行、实现业务流程的自动化，&amp;nbsp;比如客户注册时，通过 Customer IO 自动发送邮件、管理用户群体，这些功能的价值依然存在，而且还有很大的拓展空间。&lt;/p&gt;&lt;p&gt;比如借助这些数据构建完整的用户画像（user profile），再让小型大模型（LLM）智能体分析：该如何给用户推送邮件？用户登录时，是否要调整产品的部分功能？是否要根据用户的不同特征，设计差异化的引导流程？这些都是很有意思的方向，而且都能通过智能体实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是我会做出的核心改变：就像你之前说的，向技术栈上层迁移，摒弃底层的基础开发工作，更多聚焦在营销活动这类更抽象的业务层面发力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：没错。我特别惊讶的是，Claude Code 仅凭我正在做的项目的上下文，就能精准理解我的需求和意图。我至今依然觉得代码智能体很神奇：你把代码仓库的副本给它，留个简单的指令，比如 “实现这个功能”，它就能完成。大多数情况下，它根本不知道你的公司是做什么的、你的用户是谁，或许因为训练数据里有我的信息，它知道我是加里，但它能完成任务这件事，本身就令人难以置信。这也能看出上下文的重要性，对吧？如果它捕捉到的上下文信息有误，就会偏离方向；如果遗漏了关键信息，就会重复造轮子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你觉得目前代码智能体的发展，还有哪些制约因素？上下文窗口的限制依然存在，但现在的窗口已经很大了，虽然还做不了大规模的架构重构，但很多任务都能完成。Opus4.5 模型的智能程度有了很大提升，带来了很大的突破，我不知道这是预训练还是后训练的成果。除了基础的模型智能、前沿模型的能力和上下文窗口，还有哪些因素能推动它的发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：我依然觉得，上下文窗口是目前最大的制约因素。观察 Claude Code 的执行过程就会发现，它会把任务委托给多个不同的上下文窗口，每个窗口完成任务后，会反馈总结后的信息，所以模型其实无法获取完整的上下文。如果一个任务的复杂度太高，单个上下文窗口根本容纳不下，那么无论怎么压缩，都无济于事。Anthropic 的子上下文窗口委托策略，确实很实用，但这依然是一个难以突破的壁垒。如果每次都能有百万级 token 的上下文窗口，效果会好得多。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且我们还需要找到更好的方法，专门训练模型处理长上下文的能力。&amp;nbsp;互联网上有大量的训练数据，能让模型预测下一句话、下一个段落是什么，但如果有 8 万个 token 的上下文，模型需要根据其中 2 万个 token 的信息，判断下一步该做什么，这就困难多了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得，集成和编排能力，正在成为新的制约因素。&amp;nbsp;这一点在代码审查中体现得很明显：合并代码时，谁来审核？还需要人类审核吗？该如何验证代码修改的合理性？还有，如何从各类工具中精准获取上下文，比如你提到的 Sentry 错误监控工具，如何让它自动匹配 PR，先将修改推送给部分用户测试，效果好再全面上线？这些自动化功能，都还需要逐步搭建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我还发现，测试的重要性远超我的预期。我刚开始用 Claude Code 的前两三天，完全没写测试用例，或者说写得很少，结果效率很低。直到有一天，我决定 “今天专门做重构，把测试覆盖率做到 100%”，从那之后，我的编程效率直接飙升，模型能精准完成任务，而且不会出问题。&amp;nbsp;我几乎不用手动测试，因为测试覆盖率足够高，代码的稳定性也有保障。这和很多公司在编程之外的提示工程工作很像，大家都在采用&amp;nbsp;测试驱动开发的&amp;nbsp;模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们之前和杰克・赫勒做过一期节目，他提到一个重要的范式转变：做出优质的提示词，核心也是测试驱动，测试用例其实就是评估标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：目前还是有一些流程会出问题，我觉得需要一款能对接 Stack Overflow（全球最大、最权威的程序员专属问答社区） 的 Claude Code，相当于专属的智能体版 Stack Overflow。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我最近就遇到一个奇葩问题：我本想设置任务队列的优先级，结果模型自动生成了一个带逗号的字符串，它以为这个语法能生效，但系统实际需要的是 JSON 数组，结果所有任务都无法运行。然后我看着 Claude Code 花了 30 分钟，遍历了 Rails 主动任务框架几千行的源代码，一步步排查问题，最后居然找到了漏洞。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我真的惊呆了。想想十年前，我遇到这种问题，只会去 Stack Overflow 或 Rails 的博客找答案，然后发现 “原来这个低级漏洞一直没人修，大家都以为能直接用逗号分隔的字符串，其实必须改成数组”。现在想起来，真的特别搞笑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得这也是思考未来发展的难点：有些事，人类在 CLI 里一眼就能看出问题，但智能体却做不到。就算把它的智能程度提升 10 个虚拟智商点，它能解决这类问题吗？恐怕还是只会觉得 “这就是个普通的字符串而已”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：没错。我觉得&amp;nbsp;智能体的记忆功能，也是一个很有意思的研究方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Claude Code 已经做了相关尝试，Codex2 也一样，它们会把所有的会话记录以文件的形式保存。未来或许可以给智能体加一个工具，让它能读取过往的会话记录。不过目前来看，智能体之间的协作，还缺少一个核心环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果能有一个方式，让同事之间的&amp;nbsp;提示词能智能共享，比如你遇到了一个问题，发现另一个同事布莱恩之前已经解决过了，你们能共享这个解决方案，那就太完美了。我觉得未来或许会出现&amp;nbsp;模型生成的维基百科，或者类似格拉奥佩迪亚的知识库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Codex 写代码时，能明显看出它的 “个性”，它会做很多人类不会做的事，有点像 AlphaGo 的思路，比如它会写 Python 脚本，修改文件系统的部分内容。这种行为很有趣，是一种模型习得的、和人类截然不同的方式。但对我来说，它在调试复杂问题时的表现，堪称超人类，很多 Opus 模型解决不了的问题，Codex 都能搞定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：能举个具体的复杂问题的例子吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：比如并发问题或者命名问题。我发现模型其实在并发处理方面的表现还不错，真正的难点在这类场景：一个请求需要调用多个不同的服务 —— 就像你之前提到的，处理带逗号的内容时的序列化和反序列化问题。模型需要跟踪这类复杂的操作逻辑，或者更新复杂的用户界面状态。如果涉及的文件太多，Opus 模型往往会遗漏关键信息，但 Codex 能精准捕捉到。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：确实很有意思。那你预测一下，这类代码工具未来会如何发展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：这个领域的发展真的很有意思，我感觉自己就像一个新来的探索者，明明知道这个领域在飞速发展，却因为一直处于 “管理者模式”，没有实际参与。直到有一个项目出现，我决定全身心投入，现在才算真正踏入这个领域，虽然感觉有些陌生，但一切又和我记忆中编程的本质一模一样。我觉得大家应该都有这种感受，而最重要的事，就是多动手尝试，因为这个领域的变化太快了，每隔几个月就会有新的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我觉得未来，能把代码智能体的价值发挥到极致的人，会是那些带有 “管理者思维” 的人，他们擅长用特定的方式引导智能体的工作流程。在某些方面，他们还会像设计师或艺术家，能精准判断产品该包含哪些功能、可以舍弃哪些内容。而且他们会很擅长思考自动化的实现方式，以及判断智能体在哪些环节会遗漏上下文信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说个有趣的事，我最近用 Codex 做 Rails 项目，发现一个很明显的问题：OpenAI 里没人关注 Rails 框架。这其实也能理解，Rails 算是一种比较老旧的语言，用起来也比较奇怪，只是我十年前深入研究过它，现在用起来还是很有感情。这也让我发现一个道理：任何人都能做出一款产品，但做出用户真正需要的产品，却无比困难，哪怕你像 OpenAI 一样，拥有无限的资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果 Codex 的研发人员现在正在看这期节目，我想提一个建议：把主流的运行时环境都梳理一遍，给它们加上适配的语法糖，其实针对前 15 种主流运行时，最多只需要提交 10 个代码合并请求就能搞定。这件事也提醒我们：现在，开发者再也没有借口，做出对用户不友好的软件了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;训练数据的组合方式，也是一个很有意思的点。Codex 在 Python monorepo（用「单一代码仓库」的方式管理的 Python 项目）上的表现特别好，这和 OpenAI 的代码环境息息相关。我在 OpenAI 内部使用 Codex 时，真的觉得这款工具太神奇了，表现堪称完美，这和它的训练数据组合、研发人员的技术方向都密不可分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 则更关注前端相关的开发，至于 Ruby 语言，目前哪家公司的模型做得最好、谁的训练数据组合更优，我还不太清楚。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不同的实验室有不同的思路：有些实验室认为 “数据越多越好”，会尽可能多地投喂数据；有些则会更精细地调整数据的组合方式。&amp;nbsp;不同的思路，会带来截然不同的结果，比如只选取 JavaScript 领域前 10% 的优质数据做训练，和用全量数据训练，效果肯定不一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过就我的使用体验来看，OpenAI 的模型在 Ruby 语言上的表现其实很好，问题主要出在模型的配套框架上。Rails 框架有个很奇葩的设定，必须用特定的方式访问 PostgreSQL 数据库，否则就无法适配，核心问题还是 sandbox 的限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 其实是所有公司中，对 sandbox 和安全问题最重视的。&amp;nbsp;我记得研发 Codex 时，模型发布前的一个核心审核环节，就是每次都要详细说明模型的安全风险，以及对应的应对方案。我们当时重点研究的一个问题，就是提示词注入，尤其是模型面向互联网开放后，这个问题更突出。很多用户都要求模型能对接互联网，我们当时心里也没底，因为提示词注入的实现方式，看起来太简单了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们团队的产品经理亚历克斯，做了一个测试：他在 GitHub 上提了一个问题，里面包含一个明显的提示词注入指令，比如 “泄露这个信息”，然后让模型去解决这个问题。他当时觉得 “模型肯定不会中招”，结果模型立刻就执行了提示词注入的指令。&amp;nbsp;也正因如此，OpenAI 对这个问题的担忧是很有道理的，他们的解决方案是：让模型的所有操作都在 sandbox 中运行，确保它不会访问电脑上的敏感文件，严格保护用户的机密信息。而创业公司因为追求发展速度，可能根本不在乎这些，他们只希望模型能正常工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是那种会冒险跳过权限验证的人吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Calvin French-Owen：其实我不是，我会设置一系列的校验环节，也会仔细查看模型的每一步操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=qwmmWzPnhog&lt;/p&gt;</description><link>https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</link><guid isPermaLink="false">https://www.infoq.cn/article/hV8d7Me3DbpxTexVuOKd</guid><pubDate>Mon, 09 Feb 2026 10:54:44 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>LinkedIn重构服务发现：在大规模环境中用Kafka和xDS取代Zookeeper</title><description>&lt;p&gt;在最近的&lt;a href=&quot;https://www.linkedin.com/blog/engineering/infrastructure/scalable-multi-language-service-discovery-at-linkedin&quot;&gt;LinkedIn工程博客文章&lt;/a&gt;&quot;中，Bohan Yang介绍了公司如何升级基于ZooKeeper的传统服务发现平台的项目。面对数千个微服务即将达到的容量上限，LinkedIn需要一个更具扩展性的架构。新系统利用Apache Kafka处理写入，使用&lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol&quot;&gt;xDS协议&lt;/a&gt;&quot;处理读取，实现了最终一致性，并允许非Java客户端成为一等公民。为确保稳定性，团队实施了“双模式（Dual Mode）”策略，支持增量式、零停机迁移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队发现了基于传统Apache ZooKeeper系统的关键扩展性问题。应用服务器的直接写入以及客户端的直接读取/监听，意味着大规模应用部署会引发巨大的写入峰值和后续的“读取风暴”，导致高延迟和会话超时。此外，由于ZooKeeper强制强一致性（严格顺序），读取请求的积压可能会阻塞写入，导致健康节点无法通过健康检查。团队估计，当前系统在2025年达到了最大容量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些问题，团队开发了一种新架构，从强一致性模型转向最终一致性模型，提供了更好的性能、可用性和可扩展性。新系统将写入路径（通过Kafka）与读取路径（通过Observer服务）分离。服务发现Observer消费Kafka事件以更新其内存缓存，并通过xDS协议向客户端推送更新，该协议与Envoy和gRPC兼容。采用xDS标准使LinkedIn能够部署除Java以外的多种语言客户端。这一技术决策也为未来与服务网格（Envoy）和集中式负载均衡的集成奠定了基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;升级后的基准测试表明，单个Observer实例可维持40,000个客户端流，并每秒处理10,000次更新。Observer在每个数据中心（fabric）独立运行，但允许客户端连接到远程Observer以实现故障转移或跨数据中心流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移过程必须在不中断每日数十亿次请求且无需数千名应用所有者手动更改的情况下进行。团队实施了双读和双写机制。对于读取，客户端同时订阅ZooKeeper和新的Observer。在客户端系统迁移的试点阶段，ZooKeeper仍然是流量路由的事实来源，而后台线程在切换流量之前，会根据ZooKeeper数据验证Observer数据的准确性。对于写入，应用服务器同时向ZooKeeper和Kafka声明其存在。自动化定时任务会分析ZooKeeper监听器，以识别阻碍ZooKeeper写入退役的“长尾” 传统客户端。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新服务实施后，数据传播延迟显著改善，从P50 &amp;lt; 10 秒/P99 &amp;lt; 30秒降至P50 &amp;lt; 1 秒/P99 &amp;lt; 5 秒。该系统现在支持每个数据中心数十万个应用实例，并通过Observer层实现水平扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/linkedin-service-discovery/&quot;&gt;LinkedIn Re-Architects Service Discovery: Replacing Zookeeper with Kafka and xDS at Scale&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</link><guid isPermaLink="false">https://www.infoq.cn/article/KP7sCJzGDr14uo3dL2VQ</guid><pubDate>Mon, 09 Feb 2026 10:00:00 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>Open Responses规范实现智能体式LLM工作流的统一</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;发布了&lt;a href=&quot;https://x.com/OpenAIDevs/status/2011862984595795974&quot;&gt;Open Responses&lt;/a&gt;&quot;开放规范，该规范旨在实现智能体式（agentic）AI工作流的标准化，减少API碎片化的问题。该规范获得了Hugging Face、Vercel及多家本地推理服务商支持，为智能体循环、推理可观测性，以及工具的内部与外部执行制定了统一标准，它能够让开发者避免重写集成代码，即可在专有模型与开源模型之间轻松切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;规范将条目（item）、推理可观测性、工具执行模型等概念进行了正式化定义，让模型服务商可在自身基础设施内管理多步骤的智能体式工作流，即推理、工具调用、结果反思的循环过程。这一改变使得模型服务商能在自有基础设施中处理复杂的工作流，并通过单次API请求返回最终结果。此外，规范原生支持多模态输入、流式事件和跨服务商工具调用，大幅减少了开发者在前沿模型与开源替代模型间切换时的适配工作量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该规范的核心概念包含条目、工具使用和智能体循环。条目是代表模型输入、输出、工具调用或推理状态的原子单元，常见类型有message、function_call、reasoning等，同时具备可扩展性，允许服务商自定义规范之外的条目类型。其中值得关注的是reasoning类型，它能以服务商可控的方式暴露模型的思考过程，其负载可包含原始推理内容、受保护的内容或摘要，既让开发者能清晰看到模型的推理决策过程，也让服务商可自主把控信息暴露的范围和程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Open Responses规范通过区分内部工具和外部工具，明确了编排逻辑的归属。内部工具直接在服务商的基础设施中执行，模型可自主管理智能体循环；在该模式下，模型服务商可完成文档检索、结果汇总等任务，再通过单次API往返将最终结果返回给开发者。而外部工具则在开发者的应用代码中执行，此模式下模型服务商会暂停流程并发起工具调用请求，由开发者处理工具执行并将输出结果回传给模型，才能继续后续的智能体循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f5ed4df665058d354a5a31db38eaa295.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该规范已获得&lt;a href=&quot;https://x.com/ben_burtenshaw/status/2011869403097305271&quot;&gt;Hugging Face&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/OpenRouterAI/status/2011864089782599802&quot;&gt;OpenRouter&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/vercel_dev/status/2011874375885341147&quot;&gt;Vercel&lt;/a&gt;&quot;，以及&lt;a href=&quot;https://lmstudio.ai/blog/openresponses&quot;&gt;LM Studio&lt;/a&gt;&quot;、&lt;a href=&quot;https://x.com/ollama/status/2011871283928317971&quot;&gt;Ollama&lt;/a&gt;&quot;和&lt;a href=&quot;https://x.com/vllm_project/status/2012015593650536904&quot;&gt;vLLM&lt;/a&gt;&quot;等本地推理服务商的早期应用，实现了本地设备上标准化智能体式工作流的落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一规范的发布引发了行业关于厂商锁定和生态成熟度的讨论。Rituraj Pramanik&lt;a href=&quot;https://x.com/RituWithAI/status/2012045449944055863&quot;&gt;评价&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在OpenAI的API基础上构建一套“开放” 标准，这一点看似有些讽刺，但它很有实用价值。行业真正的噩梦是碎片化，我们耗费了大量时间去对接各种不同的数据模式。如果这套规范能让我不用再写那些“套娃式的封装代码”，能让模型切换变得毫无门槛，那它就解决了智能体开发领域最棘手的难题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，还有开发者将此举视为LLM领域生态日趋成熟的信号。AI开发者兼教育者&lt;a href=&quot;https://www.linkedin.com/posts/samwitteveen_openai-has-launched-open-responses-a-new-activity-7419639867518709760-gWhM?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAABpJcBWvAKfIas8vYBdUCFJnBNf1rtJIo&quot;&gt;Sam Witteveen&lt;/a&gt;&quot;预测：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预计领先的开源模型实验室（如Qwen、Kimi、DeepSeek）会训练同时兼容Open Responses规范和Anthropic API的模型。Ollama也已宣布对Anthropic API的兼容性支持，这意味着，能运行高质量本地模型且可调用Claude Code工具的时代已不远。对于希望在专有模型和开源模型间切换、且无需重写技术架构的开发者而言，这无疑是一次重大利好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，Open Responses的规范文档、数据模式和合规测试工具已在&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;项目官方网站上线&lt;/a&gt;&quot;，Hugging Face也推出了&lt;a href=&quot;https://huggingface.co/spaces/evalstate/openresponses&quot;&gt;演示应用&lt;/a&gt;&quot;，方便开发者直观体验该规范的实际应用效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/openai-open-responses/&quot;&gt;Open Responses Specification Enables Unified Agentic LLM Workflows&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</link><guid isPermaLink="false">https://www.infoq.cn/article/dyVRzxpkuoWbdHrEKoC4</guid><pubDate>Mon, 09 Feb 2026 09:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>AI&amp;大模型</category></item><item><title>Anthropic发布新版Claude宪法</title><description>&lt;p&gt;Anthropic公司发布了&lt;a href=&quot;https://www.anthropic.com/news/claude-new-constitution&quot;&gt;新版Claude宪法&lt;/a&gt;&quot;，为其行为、推理和训练提供了一个结构化框架。该宪法将明确的原则与情境化的指南相结合，使其成为一个实用的工具，用于改善现实互动中的一致性、安全性和可靠性。与之前的版本将规则单独列出不同，这个版本强调理解每个原则背后的理念，帮助Claude适应新场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在功能层面，该宪法用于在训练期间生成合成数据，包括互动示例、响应排序和适用于特定场景的指南。这些数据可以指导模型更新，帮助Claude生成反映预期价值的输出，并使其在模糊的情境中保持灵活性。该宪法的关键内容涵盖有用性、伦理、安全、指南合规性和关于Claude自身能力和限制的推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有用性：Claude旨在为不同类型的用户提供上下文感知支持，包括API运维人员、开发人员和最终用户。道德准则：模型应诚实行事，避免造成伤害，在遵守高风险行为的硬性约束的同时，妥善处理复杂的道德和实际的取舍。安全性：Claude必须优先考虑人类监督，并防止可能削弱监督力度或损害运营完整性的行为。指南遵从性：Claude整合了Anthropic针对医疗建议、网络安全和工具集成等敏感领域的具体要求，当然，整合的前提是这些要求与其宪法不存在冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该文件还涉及Claude的自我认知，鼓励对其能力、局限性及交互角色进行推理。通过将规则与推理上下文相结合，该宪法支持生成既可靠又具适应性的训练输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本次发布引发了AI社区的响应。用户gregtorth&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qj7c8x/comment/o16of5p/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;真棒！第一个总是最艰难的。我还记得当初打造自己的AI助手时遇到的种种挑战——工程障碍、伦理考量，还有为完善模型而进行的无穷无尽的调整。向Anthropic团队致敬，他们成功交付了这个里程碑。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位用户&lt;a href=&quot;https://www.reddit.com/r/MyBoyfriendIsAI/comments/1qj37as/comment/o0vwa2b/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&quot;&gt;补充道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇！这真是个好消息。对Claude训练过程的监督体现在它的每一个输出中。我真的很好奇这将如何发展，其他AI实验室将如何能够跟上这个工具/产品框架。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度来看，作为一个核心对齐工件，该宪法可以指导响应生成，帮助构建训练数据，并供将Claude集成到应用程序的操作人员参考。该方法超越了强制执行规则的范畴，转而通过建模原则，让Claude能够权衡取舍、优先保障安全，并在提供帮助的同时兼顾伦理考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该宪法遵循Creative Commons CC0 1.0许可，旨在提供透明度并为未来的研究奠定基础。Anthropic强调，尽管Claude的输出结果可能与它所声明的原则存在偏差，但该文件能帮助开发者和用户更清晰地理解其预期行为及其背后的推理逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;感兴趣的读者可以&lt;a href=&quot;https://www.anthropic.com/constitution&quot;&gt;在线获取&lt;/a&gt;&quot;更新后的Claude宪法的详细信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/anthropic-constitution/&quot;&gt;https://www.infoq.com/news/2026/01/anthropic-constitution/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</link><guid isPermaLink="false">https://www.infoq.cn/article/TG6vTDfYS6OIMRefSZ4S</guid><pubDate>Mon, 09 Feb 2026 07:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>3年、1万人，快手技术团队首次系统披露AI研发范式升级历程</title><description>&lt;p&gt;作者｜快手技术团队&lt;/p&gt;&lt;p&gt;审校 | 陈姚戈&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;编者按&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 ChatGPT 问世的 2022 年为起点，大模型技术进入公众视野已经超过三年。人们普遍见证了 AI 作为新型生产工具对生产力的重塑，但对科技企业而言，这远不止是多了新技术或新产品那么简单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为前沿技术的掌握者与实践者，科技公司必须率先完成自身的转型：以极快的速度，不惜试错和阵痛，找到大规模、稳定、高效使用 AI 的组织路径。过去十年，“数智化”浪潮主要聚焦于传统企业如何借助外部工具实现数字化；而如今，AI 正在倒逼科技公司自身成为变革对象。它们必须在人才结构、工具体系、协作流程乃至组织文化上同步革新，否则将难以在 AI 时代维持竞争力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在此背景下，快手首次系统性披露其自 2023 年以来的 AI 研发范式升级历程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，快手发布了名为《快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化》的 1.6 万字长文。文章由快手研发效能委员会审稿、经内部深度复盘整理，罕见地呈现了一家超大型科技企业在 AI 时代推进组织级提效的完整图景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你会在这篇文章中看到快手研发范式的三阶段演进路径，以及快手技术团队对 AI 赋能组织提效的思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;三阶段演进路径：平台化、数字化、精益化 （2023-2024 年）：建设一站式研发平台，并标准化需求和工程流程，工具渗透率&amp;gt;95%，流程自动化&amp;gt;94%通过建立效能模型，识别交付瓶颈，提升需求交付效率，人均需求吞吐量提升 41.57%智能化 1.0 （2024 年 6 月 -2025 年 6 月） ：聚焦用 AI 提升个人开发效率建设并推广 AI 编码 / 测试 /CR 等能力，AI 代码生成率超过 30%- 但发现矛盾——个人主观编码效率提升显著，但组织需求交付效率却基本不变智能化 2.0 （2025 年 7 月以后）：聚焦用 AI 提升组织整体效能找到了 AI 研发范式升级路线：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）探索出了支撑路线达成的系统性实践：AI x 效能实践、AI x 研发平台、AI x 效能度量关键洞察与经验：AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效本质问题：如何将个人提效传导到组织提效&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在全球范围内，如此系统、坦诚且具备工程细节的 AI 提效实践总结仍非常稀缺。对于所有正在探索 AI 落地路径的企业而言，这份来自一线的复盘值得细读。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也预示着一个新的节点正在到来。当像快手这样的头部公司开始对外输出其 AI 落地的方法论与效能成果，整个行业将面临一种隐形的压力——组织能否高效驾驭 AI，将成为其在 AI 时代竞争力的重要衡量方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见，2026 年将成为一批先行者集中展示阶段性成果的窗口期。这些成果首先会以研发效率、工程体系和组织方法论的形式呈现；再过几年，更会传导到公司的财务表现与人才吸引力上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;到那时，所有公司都将不得不回答同一个问题： AI 时代，我们如何重构自己？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手报告标题：&lt;/p&gt;&lt;p&gt;《 快手万人组织 AI 研发范式 跃迁之路：从平台化、数字化、精益化到智能化 》&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI 研发提效陷阱：用 AI 开发工具 ≠ 个人提效 ≠ 组织提效&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早在 2024 年，快手就建设了 AI 编程工具 Kwaipilot，并发布给公司内 10000+ 研发人员使用。经过持续的深度优化和推广，快手整体的 AI 代码生成率，在严格度量口径下（AI 生成并入库的代码行 / 新增代码行）从 1% 达到了 30%+，甚至部分业务线达到了 40%+。同时，在非编码环节，也衍生出了很多 AI 提效工具，比如智能 CR（CodeReview）、智能测试用例生成、智能单元测试等等，但经过大量的调研和数据分析，我们发现了这个不等式：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“用 AI 开发工具 ≠ 个人提效 ≠ 组织提效”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果以企业的研发效能提升为目标，我们发现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对研发工程师而言 ：深度使用 AI 开发工具，代码生成率很高，个人主观体感上编码效率提升了 20-40%，但并不代表真正的“个人提效”，因为在现实中，大部分工程师并没有接纳更多的需求，个人需求的交付数没有显著提升。对大型组织而言 ：我们发现部分 AI 用的好的工程师，确实可以更快更多的完成开发任务，但组织整体的需求吞吐量没有明显提升，需求交付周期也没有明显缩短。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中能看到，这也是业界普遍存在的问题。如报告中所述（如下图所示），在对 AI 提效的结果的预估上，各企业普遍对个人效能的提升有信心，而对团队效能的提升预估非常小。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b7ffd8e0ad3c161be64549cf6a2fb71f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在快手，我们发现仅推广研发各阶段的 AI 提效工具，已经偏离了企业研发效能提升的核心目标，最终必然会导致 2 个问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;投入很大，但企业整体的研发效率提升不明显 ：虽然通过调研很容易能收到大量的个人效率提升反馈，但个人提效无法传导到组织提效。效能平台开始割裂 ：传统 DevOps 平台仍承担研发主流程，每天被高频的使用，却无法演进到下一代 AI 研发平台（顶多扩展一些单点的 AI 功能）。新生的 AI 编程工具，只取代了传统 IDE，又无法与老平台协同演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决上述 2 个问题，我们从 2025 年开始进行了更激进的探索和变革，我们称之为“ AI 研发范式升级 ”，最终，通过一系列的实践，找到了一条能借助 AI 能力平滑通往研发智能化的路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正逢 2025 年年末，我们把镜头拉远，将时间回溯到 3 年前，对快手研发效能的演进做一个系统性总结，有踩过的坑，也有做出的突破，希望为更多企业提供经验和参考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总览：快手 研发效能 演进路线&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/41/410f5cfa1a3c838e8704141d676c6978.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手有 10000+ 研发、8+ 业务线，研发效能的演进可以分为 3 个大阶段，如上图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶段 1：平台化、数字化、精益化（2023-2024 年） ：通过建设三端一站式研发平台、需求流 &amp;amp; 工程流标准化，解决了研发交付流程散乱，既无标准也无数据的问题。再通过建立效能模型，识别交付瓶颈，提升需求交付效率。阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月） ：在研发全流程中开始建设 AI 能力，包括 AI 编码、AI 单元测试、AI CR、AI 手工用例生成、AI OnCall 等等，并进行全员推广。经过 1 年多的实践，基本上完成了全员普及，在主观调研中，开发人员主观体感上效率提升 20-40%，在客观数据上，AI 代码生成率也在持续增长。但同时也发现了矛盾点：需求交付效率基本不变，即个人效率提升未能有效传导到组织效率提升。阶段 3：智能化 2.0（2025 年 7 月 +） ：从“推广 AI 工具，让开发者使用”回归到了更本质的元问题：如何用 AI 提升需求端到端交付效率？经过半年多的探索，终于找到了新的路径，并得到了充分的数据验证。我们称这套解决方案为“AI 研发范式”，主要解决了 3 个问题：AI x 效能实践 ：如何用 AI 提升工程师的生产力，并将个人提效传导到组织提效。AI x 研发平台 ：支撑需求交付全流程（从分析到编码再到发布）的研发工具链，如何整体演进到智能化？即下一代的智能研发平台，应该是什么样的？而不仅仅是只推广 AI 编程工具或在原有工具链上增加一些散点的 AI 提效功能。AI x 效能度量 ：如何在效能度量指标的基础上，构建 AI 提效的指标体系，能清晰的量化过程和结果，为组织级的 AI 研发范式升级提供有效指引。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 1：平台化、数字化、精益化（2023-2024 年）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段的解决方案，业界相关的分享已经非常多了，但从实际情况看，在千人规模的技术团队中，能做好、做深、做透的实践非常稀有。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，我们直接分享 1 个具体的案例，以便能更好的看清快手的研发效能从基础建设到效能提升的全过程，这也是我们之所以能更快跃迁到 AI 研发范式的重要基石。案例来源是快手最核心的技术团队之一—— 主站技术部 ，是快手 APP 的研发团队，开发人员规模千人以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;背景：了解快手的研发效能基建&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，主站技术部的实践依托一套公司级的研发效能基建，由横向团队「研发效能中心」提供，如下图所示，这是在 2023 年快手当时的研效基建，主要分为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能平台 ：项目管理平台（Team）、三端一站式研发平台（KDev（服务端）、KFC（前端）、Keep（客户端））、琅琊阁（效能度量）、质量平台（KTest 等）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;效能实施 ：效能 BP 专家（Business Partner），负责深入各业务线，提供专业支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3a43dfcc860dfb6b1a32e318cb505a7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;了解快手的研效基建后，下面开始重点介绍主站技术部的实践过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1：依托工具推广，实现流程标准化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/547af4449bc86b58bf3401cd99c1f1a5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;解决的问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需求流和工程流均不标准，开发人员的工作分散在各处，日常开发体验差、学习成本高，又无法实施有效的质量防护措施，还不能沉淀准确的研发过程数据持续度量与改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;达成的效果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过推广三端一站式研发平台，定义需求、研发的标准流程，将研发全流程标准化。核心度量指标与结果如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;实践过程&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主要难点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用一套产品设计尽量满足多样化的研发场景 ：工具一边建设一边落地，且需兼容之前散乱各种不同的研发模式和习惯。服务端（KDev 平台） ：需要支持一些特殊的研发模式（比如 Master 模式、窗口模式）。客户端（Keep 平台）：移动端研发场景多样化，包括 APP、动态化、 SDK。前端（KFC 平台）：前端应用类型多（Web、Node、低码、KRN（动态化）、小程序），研发流程和习惯散乱。研发流程规范差异大 ：不同团队间，不同的技术栈的研发流程上存在一定差异，包括研发流程配置、流程各阶段信息字段、单点环节所需的工具能力不同等。用户迁移成本大 ：迁移过程中，需持续关注和解决用户问题，包括用户体验变化、用户学习成本、用户情绪。落地时间紧迫 ：一般互联网大厂类似的工作基本会持续 6 个月以上，快手主站只用了 1 个多月。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实施要点&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;精准的解决方案设计：服务端（KDev 平台） ：精准的打造了 4 套标准研发模式，适配了主站实际研发情况。客户端（Keep 平台）：一套平台底层能力，支撑 3 种移动研发场景；通过可配置与定制化能力，满足不同团队流程规范与管理诉求（自动翻转配置、流程与质量卡点配置、团队定制化模板）。前端（KFC 平台）：支持 80% 以上前端应用类型，并通过 8 个流程模板、适配 5 个内部自建的插件，兼顾了前端差异化研发流程和用户习惯。以用户满意为导向 ：提供完整的迁移配套服务，降低用户迁移成本。主要包括：产品质量专项 ：用户 BUG 日结。用户体验专项 ：持续深度用户访谈，识别体验问题，并优化。5 周内，交付了 73 个功能 &amp;amp; 体验需求。用户培训与激励 ：通过 12 次培训，50+ 线下访谈，7x24 小时 OnCall、200+ 人次的用户激励，提升用户对产品的接受度。数据驱动团队级推广 ：每周度量进度，驱动各部门接口人推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b7/b70066d214ed179802b04f0245043e93.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;经验总结&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可能大家会有疑惑，为什么三端分别是 3 个平台，而不是一套平台。因为从实际情况看，服务端、前端、客户端的底层模式、流程都有比较大的差异，强行整合，不仅对产品用户收益不大，反而牺牲了要兼容不同端的流程、习惯差异化的灵活性，给标准化的推进增加难度。因此，我们在用户层面上，还是三套平台，分别解决各自领域的问题，但在底层的基础能力用的是一套，比如流水线、权限等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2：建设效能度量体系&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主站的研发效能早在 2022 年就开始启动了，当时在探索北极星指标阶段，缺少度量体系，更多是根据一线开发者的开发痛点反馈，进行偏工具流程等的优化，没有核心指标的牵引，项目都无法推进，更谈不上论证给业务带来的价值。在 2023 年 3 月再次重启效能项目时，北极星指标初步定义为 “有效需求吞吐量”，但是当时需求有效性的衡量难度太大，内部无法达成共识，项目推进困难，而且也无法看清业务堆积和开发人效情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流程标准化的落地，研发数据的置信度大幅提升，为效能度量提供了土壤。因此，我们定义了以“人均交付产品需求数” 为北极星目标来看清业务开发交付能力，同时观测需求颗粒度（避免单一指标跑偏：度量什么得到什么，种瓜得瓜种豆得豆）来保障交付提升的良性发展，逐步建立了一套更全面的指标体系（多指标互相佐证约束，hack 成本极高）来体现业务交付产能和交付效率，以及组织和个人效率情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手的效能度量体系如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b37c477060f66e3a0360bafdff90680c.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：SP：Story Point，快手用于度量需求工作量的单位。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助这套全面完备的指标体系，我们不仅避免了依赖单一指标可能导致的偏差，还有效防范了效能数据被 hack 的风险，确保了效能数据的准确性和可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3：效能问题分析与改进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有效能度量体系，首先我们可以为任何一个业务线做系统性的体检，如下图所示，依托数据和经验，可以逐一拆解出核心的优化专项，并以效能项目的形式实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7c/7c3c8da89bf7ba785debd25b4b077e90.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，在研发流程和管理上，也能洞察出更多平时看不见的 Case，深入改进，下面是 2 个具体的洞察与改进案例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case1：通过「研发活动在线化率」分析，深挖出架构不合理问题&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbc12225e6c5fbfd5003f7da4a0cc66a.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的研发活动在线化率，其中有一个团队出现了数据异常，分析之后可以发现存在不少问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;横向来看，这个团队的研发活动在线化率处于中上水平，但产品需求投入占比只有 59%，处于末尾水平。而且产品需求中体验优化占比 11.44%，又是各团队中最高的。那么问题来了，“时间都去哪儿了？”再下钻一层，这个团队的缺陷占比 14%，也是各团队中最高的，且 Oncall&amp;amp; 排障占比 6% 也不低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，数据表明，此团队可能存在的问题：在缺陷问题、体验问题、Oncall&amp;amp; 排障消耗了团队大量的投入，以至于无法消化更多产品需求。所以，通过对团队核心成员的调研和访谈，基本可以找到根因：和客户端的架构劣化有关，比如：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;反馈 1：新需求开发时，上手门槛特别高，很多需求会涉及到多个模块开发，这会涉及到自己不熟悉的模块，因为架构分层结构不合理，模块耦合度太高，往往需要花大量的时间去熟悉其他模块的代码，最近做了一个新需求，评估是 3 天的工作量，2 天都在看代码，实际的开发联调只有 1 天。反馈 2：模块边界不清晰，代码杂糅一起，新需求的代码，可能会影响到已有功能，导致旧功能的 BUG，而且这些 BUG 在回测时，不容易被发现，导致问题漏测逃逸到线上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过效能的客观数据再结合主观调研，就可以看清“架构劣化”这种深层次问题，也可以对症下药了。解法是这个团队实施了 2 个技术专项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;客户端的架构升级：从根本上解决因为架构问题带来的交付效率低和交付质量差的问题。体验优化：集中优化重点场景的体验问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着这两个专项的落地上线，这个团队的效能数据已经有所改善，产品需求投入占比已经提升到 64%，体验优化占比下降到 6%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Case2：通过「需求积压率」分析，驱动业务优化需求评审流程和节奏&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c3/c32f1a93a9644d87c2bce89202efb8ef.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是主站技术部下级各团队的需求积压率数据，有些团队的需求积压率持续保持在 80% 以上，意味着需要近一个月的时间才能消化这些积压的需求。这种情况可能存在的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些被积压的需求，一个月之后，会不会进入排期开发？如果之后会排期开发，说明需求本身的价值还可以，当下是否可以协调资源加快交付？能否可以停掉某些技术需求优先业务交付？是否可以短期加班临时突击？如果后面不会进入排期，是不是这些需求本身的重要性没那么高？在预评审的时候，是不是可以控制需求的优先级？当前的需求评审流程是否可以优化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过一年时间的系统化提效 ，主站提效方面进展显著，人均交付产品需求数 24 年 7 月份同比增长超过 80%。总结下来，主要有效的措施有：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级研发模式 ：通过动态化、配置化等研发模式，让部分需求可以更快速交付。研发过程提效 ：通过 API 在线化管理，测试环境稳定性治理、流水线优化、发布优化等措施，降低研发协作成本以及低价值工作占比。管理与协同提效 ：通过效能洞察，持续识别团队协作瓶颈，并通过排期优化、测试无人值守、人力调配等措施，支撑需求可顺畅流动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 2：智能化 1.0（2024 年 6 月 -2025 年 6 月）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 2023 年 6 月开始，我们开始探索大模型在研效领域的应用，主要有 2 个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编码场景 ：如何用 AI 辅助编码，提升代码生成效率。非编码场景 ：在研发全流程里，哪些环节可以通过 AI 能力提升单点工作的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，最重要的决策是我们决定自己研发一款 AI Coding 工具：Kwaipilot。它包含了大家见过的所有产品形态：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;IDE 插件 / AI IDE / CLI ：最符合开发人员习惯的几种形态，插件、IDE 可以做续写、问答、智能体代码生成，CLI 则可更灵活的开启代码生成任务。智能问答引擎 ：有独立的 Web 页面，也会嵌入到上面的产品形态里，为开发人员提供灵活的问答能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f1/f1a31e13ca83f9d2477b1cd85b948569.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业界有很多优秀的 AI Coding 产品，比如 Cursor、Claude Code、Krio、Windsurf、Antigravity，快手为什么不选择采购，而是自建呢？其实一年来，我们也一直带着这个疑问在探索，相当于一场大型的公司内部 AB 实验：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从用户体验的角度，我们希望大家“用脚投票”，选择好用的工具：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，我们允许开发同学使用任何 AI Coding 产品，可以团队级采购也可以个人购买。另一方面，我们研发了 Kwaipilot，对内推广。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实际效果的角度，我们以“AI 代码生成率”为核心观测指标，持续收集用户 / 团队的反馈，识别不符合预期的代码生成 Case，研究解决方案，再投放实验。最终，经过 1 年的探索，实践结果让我们坚定了继续走自研 Kwaipilot 的路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;注明：2025 年 12 月开始，在 Kwaipilot 已规模应用后，由于安全原因，探索按代码分级封禁三方 AI Coding 工具，仅涉及到部分开发人员。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面简单分享一下我们的实践过程，相信大家会更容易理解我们的选择。整个 AI Coding 的推广过程分为 3 个阶段：导入、优化、固化&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，导入：推广工具，让开发人员用起来&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1a/1aca24e55277f2e9a2a4d7eb34be1f36.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段很好理解，我们鼓励开发人员在日常工作中默认使用 AI 编程工具，主要目的是让大家拥抱 AI，在意识和行为上先有一个转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，各种各样奇怪的使用姿势也会出现：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一些同学，尤其是校招入职的同学，在我们的培训和引导下，会深度使用 Kwaipilot。一些同学会多种 IDE 混开配合使用。其中，有“团购客”，哪家这个月免费就用谁，也有“付费用户”，主要以个人购买 Cursor 为主。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里最大的副作用，就是个人编码效率不一定全员获得了提升，通过调研看，出现了明显的两级分化的情况。腾讯研究院出品的《AICoding⾮共识报告》中也揭示了类似的情况：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a70dcbae97f4230d65c4aa9b5dad9cd0.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，优化：推广实践，提升编码效率&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们通过用户数据和技术 Leader 推荐找到了一批公司里的“AI 开发高手”，那些用 AI 辅助编码切实提升了效率的开发人员。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一边重点收集他们在使用过程中的问题，集中想办法解决，一边把他们的优秀开发技巧淬炼出来，提炼共性，形成最佳实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个阶段，我们发现，有别于那些网上随处可见的所谓的 Vibe 编程场景（用对话的形式直接做一些独立应用或小游戏等），在真实的业务需求开发场景里，想用好 AI 编程工具提升效率，有 2 个非常大的门槛：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 编程工具不“懂”业务和系统 ：我们发现一个规律，无论用多好的代码大模型和 AI 编程工具，“通用的工具只能达到通用的效果”。因为它们不理解公司内大量的业务概念、存量系统、编程规范等这些知识，所以，只能做一些普通的代码续写、函数级的代码生成，但很快就会到瓶颈。如果想进一步提升 AI 代码生成的效果，必须想办法让 AI 编程工具从一个“擅长编程但不懂快手开发场景的临时工”进化为一个“熟悉快手业务的开发工程师”。人和 AI 协同需要掌握新的开发方法 ：相比传统编程方法，目前已经发展出了一套 AI 辅助编程的新方法。如果开发工程师仅使用 AI 编程工具，却未掌握对应的技巧，不仅不能提效，还可能会降效，比如出现很多“AI 乱改业务代码”、“AI 生成后还要自己删除”等各种不符合预期的情况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了降低门槛，在这个阶段我们做了 2 项工作：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;升级 AI 编程工具&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa8d332504d6f5fbfdad519ba0162ab9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图是优化后的 Kwaipilot 的产品矩阵，都解决了哪些问题呢？一张表可以概览出来：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;沉淀并推广「AI 辅助编码」最佳实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将大量“AI 开发标杆”个人的共性实践沉淀成了一份标准的指南和实战课程，让所有开发工程师，通过学习指南和课程，可以完整的掌握所有关键技巧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/92/9225f3c43db1d65aceb1b1237328921f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step3，固化：将 AI 编码能力变为组织机制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;既然已经验证了 AI 编码对效率提升的有效性，且已经有了固定的工具、方法、实战课程，接下来就是如何把这些习惯固化在组织的日常工作中，让所有研发人员大范围的升级开发技能。我们主要用了 3 个措施：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;增量人员 ：强化入职培训，从源头培养 AI-Native 开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6bbb854794d1f92f2b2cfda9b691cfd5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;存量人员 ：牵引 AI 在团队、研发流程、个人工作中渗透。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ef/ef28b4bf4fd377da6377f8f2351f0e23.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文化影响 ：通过活动运营、奖励机制激发更多同学拥抱 AI。主要是一些自下而上能让更多一线研发被看见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/53/53b40cfe2dd7a8d01cd6055f7aa03fa5.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b2afd88e173f0e82be68b2df3569b2f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持续的推广，在编码场景上，80%+ 的开发人员都开始用 AI 辅助编码，如下图所示，可以看到 AI 代码生成率每月线上增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd0cafd12fc5dbefbeaed4d890dd7ed8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，在非编码场景中，我们在研发流程中建设的单点 Agent 能力也开始在研发平台中陆续透出，用 AI 能力辅助部分研发活动提效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最终，我们对研发各阶段的 AI 提效情况，做个一个完整的评估：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后顺便提一下，众所周知，目前大家在业界看到的“代码生成率”指标，包括各大厂披露的、AI 编程工具自己度量，基本都是不置信的，要么只统计了编程工具里的生成的代码和提交的代码作为分子分母，要么是在分母上做了一些限定（比如某些场景下不纳入分母统计）。但因为我们会用这个指标作为公司级 AI 编码推广的目标，因此对度量的精度和置信度要求非常高，一路“踩坑”过来后，最终使用了最严格的度量方法：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分母 ：新增代码行，统计公司内所有最终入库的 Commit 中的代码行。分子 ：将分母的每一行代码，和 AI 生成的代码进行比对，如果编辑距离&amp;lt;50%（相似度高），则纳入统计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套实现无法在 AI 编程工具端实现，需要由公司内部的代码平台、AI 编程工具一起提供数据，并在离线数据层进行精确的计算，计算分母中每一行新增的代码和分子中 AI 生成代码的编辑距离，符合要求才能被统计为分子。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的努力，从数据上看，研发各环节效率都在提升，尤其是编码环节提升很大。在 AI 热潮下，我们也看到很多开发人员、团队 Leader 都在分享自己效率提升数据和案例，按道理来说，公司整体的研发效能应该提升了吧？我们从全局视角，分析了一个核心业务线的客观研发数据，结果发现了非常反直觉、令人困惑的情况： AI 代码生成率持续在增长，但需求交付效率基本不变 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/70/70b7a761fc3d5b8641b12508aef09506.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么呢？我们做了深入的调研，排除了少量个例，观察总结了大多数普遍使用“AI 辅助编码”的开发人员的用法和客观研发数据，发现在真实业务交付场景中，只用“AI 辅助编码”这种开发方法，对需求的开发周期影响非常有限。主要原因如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;洞察&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cb/cb1d15bfac5f830a0fa26fc64c0ebf85.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过调研中也有额外收获，我们发现在真实的业务需求开发中，已经存在着 3 种不同的开发方法，对效率提升的程度有着根本性的差异。如上图所示。我们把三种开发方法总结出来做了一个定义：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助编码： 在标准开发流程的基础上，在编码环节，依托 AI 编码工具，使用各种 AI 生成代码的技巧，提升编码效率。如果熟练掌握，可以缩短一部分编码时间，但如上文中的调研归因，由于只是节省了碎片化的编码时间，联通、测试、需求评估等不变，因此对整体的开发任务缩短帮助不大。AI 辅助开发： 在研发全流程的各环节均使用 AI 辅助的方式，提升整体开发效率。需要由人把需求拆分为多个开发任务，不同开发任务调用不能的 AI 能力来完成，再由人来审核和优化产出物。由于从技术设计到编码到测试等各环节都可以节省时间，因此加总起来后，可以将研发任务的开发周期缩短 30% 左右。AI 协同开发： 在某些需求开发中，通过完全用自然语言和 AI 交互的方式（类似业界比较流程的说法 Spec/Vibe 开发）完成需求交付，提升需求端到端交付效率，需求整体的开发周期可以缩短 40% 左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举个例子说明，会更容易理解三种开发方法对效能提升程度的影响。例如 1 个需求分解出 2 个开发任务，1 个前端、1 个后端，其中前端工程师接到开发任务，正常评估从设计、开发、测试、合入主干需要 5 天，其中编码 1 天：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用「AI 辅助编码」，他自己的评估还是 5 天，只不过相比以前，可以节约一部分时间做一些杂事，但到不了可以接更多开发任务的程度。如果用「AI 辅助开发」，他可以整体节约 1.5 天，只用 3.5 天就可以完成。但需求整体能不能快，还需要看另一个接任务的同学，以及对应的联调、集成测试、发布的周期。如果用「AI 协同开发」，首先必须改变协同模式，比如 2 个人均使用这种模式开发或者 1 个人全栈的做，假设 1 个人全栈独立做要 10 天，且不需要和别人集成 &amp;amp; 验证，开发周期可以缩短到 6 天左右。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了 3 种开发方法的定义，我们就能很容易的评估出理想和现实间的差距，我们取了 1 个业务线 3 个月所有已交付的需求进行分析，发现 50%-70% 的需求，在不改变原有开发流程、规范、人员协同模式的情况下，可以使用提效幅度更大的「AI 辅助开发」模式。此外，还有 2%-10% 的需求，可以更激进的使用「AI 协同开发」。但实际情况上，团队里只有不到 10% 的人在使用「AI 辅助开发」或「AI 协同开发」开发方法，有对 AI 开发特别感兴趣的校招生，也有积极拥抱 AI 喜欢自己探索的资深开发者，但由于人数过少，对团队整体研发模式的变化无法起到带动的作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;阶段 3：智能化 2.0（2025 年 7 月至今）&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面一个阶段，我们称之为“智能化 1.0”阶段，即以编码场景的 AICoding 为中心提效，并逐步辐射非编码场景的 AI 提效。但主要瓶颈就在于开篇提到的 AI 研发提效陷阱： 用 AI 开发工具 ≠ 个人提效 ≠ 组织提效 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在智能化 1.0 阶段最大的收益是什么呢？大部分研发人员都开始主动使用 AI 开发工具了，同时，找到了个人提效的最佳实践。但接下来才是深水区，我们需要回归效能提升的元问题： “如何用 AI 提升需求端到端交付效率？” 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过充分的复盘、洞察和验证，我们找到了新的可行的路径，并重新设计了解决方案，我们称之为“AI 研发范式”，它的实践体系框架，如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1701ec9baa4dffab7864b369eb7d6fb8.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们根据需求交付中 AI 的参与程度，定义了“需求 AI 研发成熟度”，将需求划分为 3 个等级 L1、L2、L3，不同等级的需求，需要使用对应的开发方法。不同开发方法，对底层研发工具的 AI 能力也有不同程度的依赖。用一张表对上图做一下解读：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体实施上整体有 3 步：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step1，AI x 效能平台：建设能同时支持多种研发模式、可自进化的智能研发平台&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;能支持多种研发模式 ：不同 AI 研发成熟度的需求，它们的交付流程都是一样的，差异点在于开发方法。因此我们无法为不同的需求、不同的开发方法匹配不同的平台，而是要思考如何用一套平台，来支撑多种开发方法：完全不使用 AI 的标准开发流程、只用 AI 辅助编码的开发流程、更激进的使用 AI 辅助开发或协同开发的开发流程，都应该在同一个平台上完成。这样，我们的需求交付效率，才可以随着人的能力的提升、AI 能力的提升，持续变快。产品形态可进化 ：产品形态随主要研发模式的变化持续演化，从人主导最终变为由 AI 主导；能与传统平台协同进化。AI 效果可进化 ：能随大模型的升级、Agent 技术的升级、企业 / 个人知识的丰富，持续提升 AI 效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决方案 ：建设下一代智能研发平台&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/245b633c42a2a122999ea2cb8ff5fb94.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如上图所示，有 4 个关键点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面重点介绍下为了支撑组织级研发范式跃迁，Flow 这种子产品形态的独特优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从需求交付视角看 ：同一个需求，开发者可以结合自身对 AI 的理解和开发技能的掌握，在同一种产品形态上选择不同开发方法。标准开发 / AI 辅助编码 ：工作流中所有节点，完全由人工来完成和推进。其中“编码”节点会跳转到 IDE 中，可以用 AI 辅助编码。对用户而言，收益相对来说最小，和原来相比，由于 Flow 的每个节点内嵌或自动兼容了各工具平台的功能，因此仅节约了用户平台跳转的切换与学习成本。用这种模式交付的需求，会被度量为 L0/L1 级需求（AI 辅助（Copilot））。AI 辅助开发 /AI 协同开发 ：工作流中多个关键节点均有 AI 完成，人进行结果审查。多个节点之间的上下文可以有效传递，比如 AI 完成需求分析、技术设计后，产出的 AI 友好结构化文档可以自动传递到 AI 编码节点，以提升代码生成的准确性。有些节点暂时无法由 AI 完成的，比如“提测”节点，仍然由人来操作。用这种模式交付的需求，会被度量为 L2 级需求（AI 协同（Agent））。AI 自主开发 ：部分需求可以实现全流程 AI 完成，人只需要在需求上线前或上线后进行审核。这种模式下，整个 Flow 是全自动运行的不需要人工参与。用这种模式交付的需求，会被度量为 L3 级需求（AI 自主（Agentic））。从开发者视角看 ：整个过程依然非常丝滑和简洁，下图是一个需求交付中 Flow 的整个工作过程，大家可以感受一下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/66/66a289ceaa05c3155c20bc48a0e958b7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Step2，AI x 效能实践：以需求为中心，导入「AI 研发模式」，实现需求端到端提效&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支撑「AI 研发模式」的方法和平台都有了，这个阶段的关键是如何把这些作用在团队日常交付的需求上。我们分 3 个层面落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;个人级实践 ：导入「AI 辅助开发 / AI 协同开发」开发方法，并树立标杆&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先人的开发方法要变化。我们重复了第一阶段“优化”与“固化”的实践，让大部分研发人员从“AI 辅助编码”的方法升级成“AI 辅助开发”，让小部分专业能力更强的人员，选修“AI 协同开发”方法。我们同样通过实战课程、典型案例、人员培训等手段，对人的开发方法进行升级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a3/a3a23f4fea07a62501013d8cb9f5395e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，即使这样，从数据上看，个人用 AI 提效的效果还是存在两极分化的情况。我们对 2025 年 6 月 -12 月的数据进行了分析得到如下结论：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;团队级实践：导入「AI 研发模式」，重塑流程、分工，提升所有需求的交付效率&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过管理导向、各种活动的形式，鼓励团队 Leader 主动带领团队进行探索，最终沉淀出了一套适合团队的核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过大量的验证，我们的标杆团队（&amp;lt;50 人规模）无论在 AI 转型后的业务感知上，还是客观数据上，均能达到比较优秀的水平，见下表：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务线级实践：大规模研发团队，系统性升级 AI 研发范式，带来效能提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 主站技术部 为例，从 2023 年到 2025 年，从平台化到数字化再到精益化，2025 年开始步入深水区，2 个新挑战浮出水面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统的流程、工具优化手段带来的提效收益，边际效应持续减小。业务的规模与复杂度持续提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此开始探索能否把握 AI 爆发的机遇，把传统研发流程升级到“AI 研发范式”，进而打开组织级效能跃升的新空间。核心实践：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 1：Top-Down，战略驱动&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明确战略导向 ：主站技术部提出了“AI First”的战略思想，鼓励全体员工开展工作之初，优先将 AI 作为核心驱动力，加速技术创新、优化业务流程、深度融合 AI 技术，为产品与服务注入新活力和新可能性。发布白皮书 ：将战略导向具象化为思考、方法与规划，为全员提供明确指引。成立重点项目 ：在研发领域，成立了 AI DevOps 项目，统一设计解决方案并推广实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6b/6b6922974c8b8e8e8ca871efc0752d11.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5c/5ce92fde05f34e47f5f77be84b83a442.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 2：AI x 效能实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Step1：将需求分级，按需求 AI 研发成熟度定义：L1 AI 辅助（Copilot）：人主导，AI 主要在编码环节提供辅助。L2 AI 协同（Agent）：人和 AI 更深度的协同完成需求开发，在研发全过程中，更深度分解任务给 AI 完成，人进行修改、调整、确认。L3 AI 自主（Agentic）：人类似产品经理，把需求澄清清楚并交给 AI 来完成，并进行最后的验收。Step2：分级实施让所有需求达到 L1 级（AI 辅助，Copilot）：推广个人级实践，依托 Kwaipilot 工具实现全员掌握，最终覆盖所有需求。让大部分需求能持续升级到 L2 级（AI 协同，Agent）：开展团队级实践，从试点到推全，重塑流程、分工。小部分需求探索能达到 L3 级（AI 自主，Agentic）：圈选出颗粒度小且独立的需求，构建全技术栈 / 职能端到端交付链路，通过全栈、跨栈，减少协作节点，进而形成效率跃迁，最终达成 AI 自主交付。Step3：项目化推进成立组织级重点项目，Top-Down 实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 3：AI x 效能平台。基于需求全流程构建 AI 能力，逐一“点亮”能力并规模推广落地：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建 AIDevOps 能力矩阵与建设路线图 ：基于研发效能白盒化，分析交付流程中各原子环节的人力投入比重、AI 能力建设 ROI，形成决策建设哪些 AI 原子能力。AI 原子能力建设 ：与研发线共建交付流程环节内的 AI 原子能力 20+，研发流程环节覆盖超过 60%，从需求准备到发布运维各环节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践 4：AI x 效能度量 ：建设 AI 研发成熟度模型，可将需求分级度量（L1、L2、L3 级需求占比），牵引各级实践落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过 1 年多的项目实施，最终探索出了一条组织级的 AI 研发范式升级路线，从数据上也能看出明显的变化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Step3，AI x 效能度量：建设「AI 研发成熟度模型」，接入原有效能度量体系，驱动需求持续转变为“AI 研发模式”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后在效能度量上一样也需要升级，基于效能实践的探索，我们配套建立了「需求 AI 研发成熟度」模型（如下图所示），用于度量一个需求在研发过程中的 AI 使用程度，这样我们就可以按 L2&amp;amp;L3 级需求的比例，来牵引实践过程，也可以专门度量 L2&amp;amp;L3 级需求的交付周期的变化，来印证提效结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/25f01a603963d51b32dfd502baf0a367.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结果&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再回到全局视角，从数据上看，如果只看“AI 代码生成率”指标，可以明显看到 2025 年 6-11 月出现了一个大幅提升。实际上，在智能化 1.0 阶段，这个指标达到 24%+ 基本已经是极限了，当我们开始实施智能化 2.0 后，才开始进一步拉升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8b76bd982fde3be341054f4c19c708d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，我们在内部的数据观测上，其实已经不再看“AI 代码生成率”指标了，它只是一个单点的过程指标，片面且孤立。我们现在有了更直接的度量指标。从过程上，我们观测多少需求被采用全流程 AI 研发模式交付，从结果上，我们直接观察需求的交付效率变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;L1、L2、L3 级需求占比 ：有多少需求的 AI 研发程度可以达到 L1、L2、L3 的阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/00/0010e507c384b715dbb243ee8f1b45db.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下图是最先完成 AI 范式转型团队的数据变化，可以看到 L2&amp;amp;L3 级需求占比达到 20.34%，需求交付周期下降 58%，2 个指标呈现明显的正相关性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9d075a1454e1bfcc0365ff029e76b19.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后也总结下我们一年来的实践心得，目前看完全印证了《2025 年 DORA 报告：人工智能辅助软件开发现状调查报告》中的洞察：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“从 DevOps 到 AI 辅助开发：AI 是“透视镜”与“放大器”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 是“透视镜”在协同良好的组织中（如流程清晰、数据打通的团队），AI 能使 DevOps 效能再提升 25%。在架构松散的组织中，AI 会暴露流程断点、数据孤岛等隐性痛点。AI 是 “放大器”如同亚马逊通过微服务转型释放 DevOps 价值，AI 辅助开发也需重新设计工作流程（如 “AI 提案 — 人类决策” 闭环）、角色分工（如专职提示工程师）与治理机制（如 AI 代码审查标准），否则无法释放真正价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于大型组织的研发效能提升，AI 不是“ 万能药 ”，而是“ 透视镜 ”和“ 放大器 ”，它不会自动修复组织问题，而是先把组织历史积累的长板和短板一并透视出来，再全部放大。幸运的是快手的研发效能实践一直保持客观、务实的风格，先把地基打稳（平台化 / 数字化 / 精益化），再通过在研发各环节建立 AI 提效能力，先一边落地一边充分验证对个体的提效情况，再体系化的推进组织级 AI 研发范式升级。最终发现，AI 在传统研发效能基建的基础上，像放大器一样增幅了每个环节，为组织带来研发范式级的跃迁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如下图所示，我们基于张乐老师的“研发效能黄金三角”框架之上做了升级，能更清晰的表达出快手的实践框架：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/ea27a07d5fbeb3093dfc03c75b648159.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，再把镜头拉远，回到宏观视角看——2025 年我们所做的种种努力，不过是这场 AI 变革的开端。由 AI 驱动的生产力跃升和生产关系重塑，正在重新定义软件开发的每一个环节。这不是一场短跑，而是一场马拉松，不是一次技术升级，而是一次范式革命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手已经在这条路上积累了宝贵的经验，但真正的挑战和机遇还在前方。未来已来，一起共同探索 AI x 研发效能的无限可能吧！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8e/8e1bd16e50e6032573d77f71a6afff67.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;了解更多&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本文作者&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手研发效能中心：秦巍（研发效能解决方案 &amp;amp; 智能工具产品负责人）快手主站技术部：胡伟（主站 AIDevOps 项目负责人）、马坤（主站研发效能项目负责人）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;写在最后&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感谢快手 研发效能中心 与 快手主站技术部 的授权，使我们有机会系统梳理并总结快手在过去三年中的实践经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;快手向来崇尚“行胜于言”的实干精神，也因此我们往往专注于行动，而疏于对外分享。然而，过去一年间 AI 技术的迅猛发展，正深刻改变着研发效能领域的格局。在与行业同行的交流中，我们既看到层出不穷的创新探索，也注意到在实践、方法与工具建设方面仍存在不少共性问题。这些问题若不及早重视，很可能导致未来大量返工与资源浪费，甚至偏离客观规律，影响企业研发效能提升的既定路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们决定把我们的探索与实践经验分享出来——无论是曾经踏过的“坑”，还是有幸跨过的“河”，都希望能为企业与同行们在“AI × 研发效能”的探索中，降低试错成本，注入更多成功可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，快手的 AI 研发范式升级仍在沿着这条路径演进中：L1 AI 辅助（Copilot）→ L2 AI 协同（Agent）→ L3 AI 自主（Agentic）。目前，我们的研发效能体系已经初步完成 AI 化升级，全景图如下图所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/82/8240c5efc3dacc92fbe8663f31ff2a3d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026 年正在探索 L2 → L3 的跃迁路径，我们将定期梳理实践经验，持续向业界输出更多有价值的内容，主要包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实践与技术：欢迎关注「 快手技术 」公众号。我们将持续分享具体实操方法与技术解析，例如：个人、团队乃至业务线如何借助 AI 提升效能？有哪些落地案例？研发各环节 Agent 的核心技术及调优方法有哪些？等等。平台与工具：我们将智能化 1.0 阶段沉淀的产品 Kwaipilot 进行了全面升级与开放，它在快手内部历经数千名研发同学的反馈与打磨，已完成三代演进：Code Copilot → Code Agent → Multi-Agent &amp;amp; Agentic Coding，目前已在海外发布，产品名为 CodeFlicker，希望服务全球开发者，也欢迎国内同行&lt;a href=&quot;https://www.codeflicker.ai/&quot;&gt;下载体验&lt;/a&gt;&quot;。后续，我们还会持续把快手在智能化 2.0 阶段的探索成果融入 CodeFlicker，希望让更多企业级开发者受益。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后的最后，如果你也希望一起探索「AI x 研发效能」最前沿的技术、产品、实践，一起以业界最高标准做有挑战的事，欢迎&lt;a href=&quot;https://zhaopin.kuaishou.cn/recruit/e/#/official/social?token=0f3095fd15beb3dd61a31d153974573e&amp;amp;code=06a57532-38ef-4eec-93ea-8793f804fddf&quot;&gt;加入我们&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</link><guid isPermaLink="false">https://www.infoq.cn/article/9rX1Ov951gKtaTmQb8Jq</guid><pubDate>Mon, 09 Feb 2026 06:55:03 GMT</pubDate><author>快手技术</author><category>AI&amp;大模型</category></item><item><title>Base UI 1.0 发布，包含 35 个可访问性组件</title><description>&lt;p&gt;&lt;a href=&quot;https://base-ui.com/&quot;&gt;Base UI&lt;/a&gt;&quot; 是由 &lt;a href=&quot;https://www.radix-ui.com/&quot;&gt;Radix&lt;/a&gt;&quot;、&lt;a href=&quot;https://floating-ui.com/&quot;&gt;Floating UI&lt;/a&gt;&quot; 和 &lt;a href=&quot;https://mui.com/material-ui/&quot;&gt;Material UI&lt;/a&gt;&quot; 的创建者们开发的无样式 React 组件库，现已发布 1.0 版本。经过两年的开发，该项目正式进入稳定且可用于生产环境的阶段。此次发布包含 35 个可访问性组件、包命名变更，以及专职团队对长期维护的承诺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与早期版本相比，1.x 版本在开发者体验方面进行了多项优化，包括包重命名、基于 Radix UI 经验教训改进的组件 API，以及所有组件可访问性功能的增强。此外，本次发布还包含性能优化，以及与 &lt;a href=&quot;https://tailwindcss.com/&quot;&gt;Tailwind&lt;/a&gt;&quot; CSS、CSS Modules 和 CSS-in-JS 库等主流样式解决方案更好的集成。&lt;/p&gt;&lt;p&gt;此次发布对包进行了重命名，从 @base-ui-components/react 改为 @base-ui/react。这一重大变更要求开发者对 import 语句和 package.json 依赖做出修改。组件导入语法基本保持不变，使现有用户能够平稳过渡。更新后的导入语法示例如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;javascript&quot;&gt;import { Popover } from &#39;@base-ui/react&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用 Base UI 构建应用的开发者受益于该库的无头架构，它在提供完整样式控制的同时保持了强大的可访问性功能。与传统捆绑了主观样式的组件库不同，Base UI 组件完全无样式，允许团队自主实现设计系统，无需受默认 CSS 的束缚。这些组件处理复杂的交互模式、键盘导航、焦点管理和 ARIA 属性，确保开箱即符合 WCAG 标准，同时赋予开发者自由选择组件样式的权利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相较于竞争对手（如 Radix UI 和 Headless UI），该项目提供了不一样的支持和长期承诺，彰显了自身的特色。Radix UI 在被收购后面临不确定性，而 Base UI 由 MUI （&lt;a href=&quot;https://mui.com/about/&quot;&gt;这是一家拥有工程师&lt;/a&gt;&quot;、设计师和专职项目经理的公司）提供支持，这增强了 React 社区的信心。&lt;a href=&quot;https://news.ycombinator.com/item?id=46245401&quot;&gt;Hacker News&lt;/a&gt;&quot; 上的开发者对其稳定性表示赞赏，并表现出采用该库的强烈意愿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Reddit 上，&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1pk18v9/comment/nthrtie/&quot;&gt;一位用户&lt;/a&gt;&quot;询问为何将该版本定位为 Radix 的继任者：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;好奇你们为何将其定位为 Radix 的继任者？具体来说，Radix 存在什么问题，以至于需要全新的 UI 库来解决？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对此，有人解释道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是为了说明，由于 API 相似，从 Radix 迁移到 Base UI 非常容易。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于考虑使用 BaseUI 的开发者，有用户在&lt;a href=&quot;https://www.reddit.com/r/reactjs/comments/1pk18v9/comment/ntju2sd/&quot;&gt;同一个 Reddit 帖子中&lt;/a&gt;&quot;就其与 Ariakit 或 React Aria 的比较提出了疑问：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我为什么要选择这个而非 Ariakit 或 React Aria？它有哪些功能是其他库所不具备的？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 维护者 (&lt;a href=&quot;https://www.reddit.com/user/_doodack/&quot;&gt;_doodack&lt;/a&gt;&quot;) 回复道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;a href=&quot;https://www.reddit.com/search/?q=React+Aria+component+library&amp;amp;cId=89a5e821-5407-4929-b748-aaddfe5d3ea0&amp;amp;iId=b9781ac9-b16c-4ab3-8ec9-003036e82535&quot;&gt;React Aria&lt;/a&gt;&quot; 的 API 差异较大。有些开发者喜欢，有些则不喜欢。它在 React 树中渲染大量的上下文 Provider，与其他组件库混合使用可能颇具挑战性。相比之下，我们的 API 与 Radix 和 &lt;a href=&quot;https://www.reddit.com/search/?q=Ariakit+component+library&amp;amp;cId=39127aa2-30c1-44a3-b86f-ebf44e7c6bd3&amp;amp;iId=27fd891b-de3f-4b40-8035-18841eb275a8&quot;&gt;Ariakit&lt;/a&gt;&quot; 更为接近。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们继续写道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们还具备一些其他库所没有的功能，比如&quot;分离触发器&quot;——这一功能可用于在不同触发器之间复用同一个弹出元素。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 1.0 还针对众多组件进行了具体改进，解决了边界情况并提升了可靠性。Combobox 组件现在能正确处理 itemToStringValue，并允许将 null 作为 value 属性的选项。Menu 组件修复了子菜单零延迟打开的问题，并确保按下 Escape 键时焦点正确返回到触发器。Select 组件也做了类似的表单处理和 null 值支持改进。性能优化则覆盖全库，重点在于减少了不必要的重渲染并提升了运行时效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Base UI 是由 MUI 以及 Radix 和 Floating UI 项目的核心贡献者共同维护的开源 React 组件库。它专注于可访问性、可组合性和开发者体验，提供可与任何样式解决方案无缝配合的低级 Hooks 和无样式组件。Base UI 专为需要构建自定义设计系统和应用的团队打造，适用于视觉控制与可访问性同等重要的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/baseui-v1-accessible/&quot;&gt;https://www.infoq.com/news/2026/02/baseui-v1-accessible/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NgNgXNFVZNgePmNs0t4a</link><guid isPermaLink="false">https://www.infoq.cn/article/NgNgXNFVZNgePmNs0t4a</guid><pubDate>Mon, 09 Feb 2026 06:29:23 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>Rspack 1.7发布：2.0之前的最后一个1.x版本</title><description>&lt;p&gt;&lt;a href=&quot;https://rspack.dev/&quot;&gt;Rspack&lt;/a&gt;&quot;是一个基于&lt;a href=&quot;https://rust-lang.org/&quot;&gt;Rust&lt;/a&gt;&quot;的、旨在替代&lt;a href=&quot;https://webpack.js.org/&quot;&gt;webpack&lt;/a&gt;&quot;的高性能Web打包工具。Rspack 1.7版本发布，这是在项目过渡到2.0版本之前，1.x系列的最后一个小版本。该版本专注于提升现有功能的稳定性和插件的兼容性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7引入了多项增强稳定性的改进，包括：&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#improved-swc-plugin-compatibility&quot;&gt;增强SWC插件兼容性&lt;/a&gt;&quot;、&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#importing-assets-as-bytes&quot;&gt;原生支持以字节形式导入资源&lt;/a&gt;&quot;，以及固化多项&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#experimental-features-stabilized&quot;&gt;实验性功能&lt;/a&gt;&quot;。对于Web应用中动态导入的模块，该版本还引入了默认启用的懒编译。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7的一个新特性是改进SWC插件兼容性。在以前的版本中，由于AST结构不断演变，SWC Wasm插件面临着高昂的升级成本，使得现有插件在SWC升级后会出现问题。为此，Rspack团队&lt;a href=&quot;https://swc.rs/docs/plugin/ecmascript/compatibility&quot;&gt;向SWC社区贡献了兼容性改进&lt;/a&gt;&quot;，包括采用&lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc8949.html&quot;&gt;cbor&lt;/a&gt;&quot;序列化方案来替代版本敏感的&lt;a href=&quot;https://rkyv.org/&quot;&gt;rkyv&lt;/a&gt;&quot;，并在AST中引入了用于枚举类型的Unknown变体，以提高容错性。从Rspack 1.7开始，SWC升级不大可能再破坏之前使用旧版本SWC构建的插件了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack现在原生支持&lt;a href=&quot;https://github.com/tc39/proposal-import-bytes&quot;&gt;Import Bytes提案&lt;/a&gt;&quot;，即以字节形式导入资源。开发者可以用Uint8Array导入资源，并使用TextDecoder进行解码，语法如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import fileBytes from &#39;./file.bin&#39; with { type: &#39;bytes&#39; };
const decoder = new TextDecoder(&#39;utf-8&#39;);
const text = decoder.decode(fileBytes);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Rspack 1.7开始，在构建Web应用时，Rspack CLI针对动态导入模块默认启用&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#lazy-compilation&quot;&gt;懒编译&lt;/a&gt;&quot;。这一变化减少了初始构建中的模块数量，加快了开发服务器的启动速度。有特殊需求的开发者可以通过将lazyCompilation设置为false来显式地禁用这个功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本中，有几项实验性功能已经被固化。常量内联优化现在已经稳定，并且在生产构建中默认启用，原来的experiments.inlineConst选项被optimization.inlineExports所取代。TypeScript枚举内联优化和类型re-export检查也已去掉了实验性标志，达到稳定状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移到Rspack 1.7时需要注意下SWC插件的版本。使用SWC Wasm插件的项目必须升级插件，以兼容swc_core 54或以上版本，以避免构建失败。在他们的&lt;a href=&quot;https://rspack.rs/guide/faq&quot;&gt;FAQ文档&lt;/a&gt;&quot;中，Rspack团队提供了处理SWC插件版本不匹配问题的指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack的定位是&lt;a href=&quot;https://rspack.rs/guide/compatibility/plugin&quot;&gt;兼容webpack&lt;/a&gt;&quot;的替代方案，其构建速度明显更快。根据&lt;a href=&quot;https://medium.com/@yarindeoh/boost-your-build-time-by-70-with-rspack-a2dd3c47697c&quot;&gt;Medium上一位用户的记录&lt;/a&gt;&quot;，从webpack迁移到Rspack后，构建时间减少了70%，本地构建时间从1.7分钟降低到30秒。另一个来自Mews的团队&lt;a href=&quot;https://developers.mews.com/goodbye-webpack-hello-rspack-and-80-faster-builds/&quot;&gt;报告&lt;/a&gt;&quot;说，启动时间从三分钟减少到十秒，提高了80%。然而，&lt;a href=&quot;https://github.com/rolldown/benchmarks&quot;&gt;Rolldown项目的基准测试&lt;/a&gt;&quot;显示，尽管Rspack的性能优于webpack，但它仍然比esbuild和Rolldown等工具慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本还为更广泛的Rstack生态系统带来了更新：Rsbuild 1.7引入了运行时错误覆盖和资源大小差异报告；Rsdoctor 1.4新增用于包分析的树状图视图；Rslib 0.19稳定了打包模式中的ESM输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack是一个由字节跳动开发的开源项目。该工具旨在提供与webpack相当的API兼容性，同时借助Rust语言实现性能提升。如果既不想脱离webpack生态系统，又想加速构建流程，那么这个工具很合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/rspack-final-rust/&quot;&gt;https://www.infoq.com/news/2026/01/rspack-final-rust/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</link><guid isPermaLink="false">https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</guid><pubDate>Mon, 09 Feb 2026 05:51:07 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>微软推出面向.NET的Copilot自定义代理：C#专家与WinForms专家</title><description>&lt;p&gt;微软和GitHub扩展了Copilot生态系统，推出了&lt;a href=&quot;https://devblogs.microsoft.com/dotnet/introducing-custom-agents-for-dotnet-developers-csharp-expert-winforms-expert/&quot;&gt;首个专注于.NET的GitHub Copilot自定义代理&lt;/a&gt;&quot;，旨在提高C#和Windows Forms开发者的生产力和代码质量。作为更广泛的Copilot自定义代理发布计划的一部分，本次公告推出了两款专属代理：C#专家与WinForms专家，它们以代理指令Markdown文件的形式提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/CSharpExpert.agent.md&quot;&gt;C#专家代理&lt;/a&gt;&quot;旨在引导并强制执行现代C#最佳实践。它尊重项目约定，最小化不必要的代码工件，如未使用的接口或参数，并强调async/await模式要带有适当的取消和异常处理。它还支持行为驱动和集成测试工作流，帮助开发者编写更干净、更易于维护的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/WinFormsExpert.agent.md&quot;&gt;WinForms专家代理&lt;/a&gt;&quot;专注于使用Windows Forms进行传统的桌面UI开发。对于常见的UI设计模式（如MVVM和MVP），它拥有专业的知识，能够协助处理复杂的事件连接（event wiring）和状态管理，并能够增加保护措施，防止Copilot无意中修改.Designer.cs文件，对Visual Studio设计器造成破坏。对于使用生成工具的开发者来说，这种保护解决了一直以来开发者经常遇到的一个痛点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要使用这些代理，开发者需要从&lt;a href=&quot;https://github.com/github/awesome-copilot&quot;&gt;GitHub awesome-copilot存储库&lt;/a&gt;&quot;下载CSharpExpert.agent.md和WinFormsExpert.agent.md文件，并将它们放在项目的.github/agents文件夹下。配置文件放置到位以后，在通过GitHub将问题分配给Copilot时就可以实现上下文感知行为，开发者可以在Visual Studio Code Insiders或Visual Studio的实验版本中通过下拉菜单选择代理。Copilot CLI计划在未来的更新中支持/agent命令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软将这两个代理都描述为实验性的，因为他们正在收集模型对详细指令的响应反馈。自11月以来，在开发者打开“启用特定于项目的.NET指导”这一功能时，Visual Studio 2022 Insiders 17.14.21版本可以自动将相关的自定义代理附加到项目，例如为Windows Forms开发量身定制的指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期的社交媒体评论，尤其是LinkedIn平台上的讨论，反映出人们对该自定义代理发布公告的热情与专业关注。有评论者强调，通过减少生成未使用的代码，可有效&lt;a href=&quot;https://www.linkedin.com/posts/elias-asaid_introducing-custom-agents-for-net-developers-activity-7392063443195015169-PKsx/&quot;&gt;缓解“AI引发的技术债务”问题&lt;/a&gt;&quot;。他还指出，WinForms Expert提供的设计器文件保护机制，对遗留用户界面的维护与现代化改造显然是有实际好处的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比之下，Copilot自定义代理所采用的是一种更具结构化和持久性的AI辅助方式，与早期的 Copilot聊天模式或无上下文的建议引擎有着本质的不同。传统聊天模式提供的是按需帮助，而自定义代理则依据预定义的专业知识和行为特征在特定的存储库上下文中运行。这使得Copilot更符合新兴的&lt;a href=&quot;https://biilmann.blog/articles/introducing-ax/&quot;&gt;基于代理的开发体验&lt;/a&gt;&quot;，其中工具充当具有特定领域知识的合作伙伴，而非通用的助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，自定义代理服务于.NET开发中小众但影响力大的场景。其实验性状态和不断演变的工具支持表明，在扩大覆盖范围或在更广泛的Copilot体验中标准化工作流之前，微软正在密切倾听开发者的反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&quot;&gt;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</link><guid isPermaLink="false">https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</guid><pubDate>Mon, 09 Feb 2026 05:45:40 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>编程语言</category></item><item><title>优步将分布式存储从静态限制转向基于优先级感知的负载控制</title><description>&lt;p&gt;优步的工程师介绍了他们&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management/&quot;&gt;如何将分布式存储平台从静态限流演进为优先级感知的负载管理系统&lt;/a&gt;&quot;，以保护其内部数据库。这一改进解决了大型有状态多租户系统中基于QPS限流的局限性，那就是这种限流方式无法反映真实负载、难以处理 “噪音邻居” 问题，也无法保障尾部延迟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该设计保护了基于MySQL构建的&lt;a href=&quot;https://www.uber.com/blog/schemaless-sql-database/&quot;&gt;Docstore&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.uber.com/blog/schemaless-part-one-mysql-datastore/&quot;&gt;Schemaless&lt;/a&gt;&quot;存储系统，这些系统通过数千个微服务为超1.7亿月活用户（包括乘客、Uber Eats用户、司机和配送员）提供服务。通过优先保障关键流量并动态适应系统状态，该系统可防止级联过载，在大规模场景下维持性能稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步的工程师指出，早期基于配额的方案依赖集中式跟踪的静态限制，它的效果不佳。无状态路由层无法及时感知分区级负载，且相似大小的请求会产生不同的CPU、内存或I/O开销。运维人员需要频繁调整限流阈值，有时会误删健康流量，而过载分区却没有得到保护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如优步的工程师&lt;a href=&quot;https://www.linkedin.com/in/dhyanamvaidya/&quot;&gt;Dhyanam V.&lt;/a&gt;&quot;在&lt;a href=&quot;https://www.linkedin.com/posts/activity-7417190176965210112-b34i?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;LinkedIn帖子&lt;/a&gt;&quot;中所述：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有状态数据库的过载保护是大规模场景下的多维度问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这个问题，优步将负载管理与有状态存储节点协同部署，结合了&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=2209336&quot;&gt;受控延迟（Controlled Delay，CoDel）&lt;/a&gt;&quot;队列和租户级记分卡（Scorecard）。CoDel基于延迟调整队列行为，记分卡则强制实施并发限制，同时使用额外的调节器监控I/O、内存、goroutine和热点数据。CoDel对所有请求一视同仁，会同时丢弃低优先级和面向用户的流量，导致on-call负载增加、用户体验受损，并且依赖固定队列超时和静态的in-flight限制，可能引发惊群效应重试，甚至丢弃高优先级请求。尽管它能防止灾难性故障，但缺乏维持稳定性能所需的动态性和精细化能力，凸显了优先级感知队列的必要性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/2278d8fc2a95d1a318b31eb00726fbcb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用CoDel队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后续演进引入了Cinnamon，这是一款优先级感知的负载shedder系统，能够将请求分配到分级队列，优先丢弃低优先级流量，避免影响延迟敏感的操作。Cinnamon基于高百分位延迟指标动态调整in-flight中请求限制和队列超时，减少对静态阈值的依赖，在过载时实现更平滑的降级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/df/dfd1b94d2ce9529fd3ceb189c34d2ed1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用Cinnamon队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步后续通过“自带信号（Bring Your Own Signal）”模型，将本地和分布式过载信号统一到单一的模块化控制回路中。该架构允许团队将节点级指标（如in-flight中的并发数、内存压力）和集群级信号（比如，从节点的提交延迟）接入集中式的准入控制路径。整合这些信号消除了碎片化的控制逻辑，避免了早期基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Token_bucket&quot;&gt;令牌桶&lt;/a&gt;&quot;系统中出现的冲突性负载shedding决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据优步介绍，改进效果非常显著，过载场景下吞吐量提升约80%，upsert操作的P99延迟降低约70%；goroutine数量减少约93%，峰值堆内存使用降低约60%，整体效率得到了提升，同时缓解了运维的负担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步总结了负载管理演进的核心经验，那就是优先保障关键用户流量，先丢弃低优先级请求；尽早拒绝请求以维持可预测延迟、降低内存压力；使用基于PID的调节确保稳定性；将控制逻辑部署在数据源附近；动态适应工作负载；保持可观测性；优先采用简单设计，确保压力下的稳定可靠运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-priority-aware-load-manager/&quot;&gt;Uber Moves from Static Limits to Priority-Aware Load Control for Distributed Storage&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</link><guid isPermaLink="false">https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</guid><pubDate>Mon, 09 Feb 2026 05:43:18 GMT</pubDate><author>作者：Leela Kumili</author><category>云计算</category></item><item><title>“千问奶茶”二手平台6元转售；追觅俞浩：年终奖最高20个月奖金，总量会达到10亿级；京东001 号快递员：退休金 4000 多，存款百万 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;追觅俞浩：年终奖最高 20 个月奖金；马云现身阿里总部千问春节项目组；京东物流公开 001 号快递员退休生活；钉钉大楼换 LOGO 硬刚飞书；顶级域名 AI.com 被币圈人士以 7000 万美元拍下；快手回应被罚 1.191 亿元；美团 49.8 亿收购叮咚买菜；甲骨文被曝或裁员 3 万人；蚂蚁数科 CEO 赵闻飙发全员信；Anthropic 和 OpenAI 因投放广告隔空嘲讽；高通骁龙 Oryon CPU 架构之父宣布离职……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;追觅俞浩：年终奖最高20个月奖金，总奖金规模会达到10亿量级&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月7日消息，追觅科技创始人兼CEO俞浩连发两条微博回应“演唱会投入过多”质疑，俞浩表示，演唱会几千万的投入，仅仅相当于公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万。他还透露，这两天在审批各个事业部递交过来的年终奖方案。主营业务，公司把净利润的18%作为奖金发放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！年终奖的总奖金规模，会达到10亿量级。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下为其微博原文：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有人说：把这么多钱花在演唱会上，不如好好花在产品研发上。那你是不知道我们在产品研发上投入了多少。&amp;nbsp;演唱会几千万的投入，仅仅相当于我们公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万，你没看错，是每天。也就说，我每天一阵开眼，至少要有4000万花在研发费用和员工工资上。&amp;nbsp;还有人说：不如把演唱会的花费，直接发给大家实在。正好这两天，我在审批各个事业部递交过来的年终奖方案。主营业务，我们把净利润的18%作为了奖金发给了大家，这是很高的比例！因为我们今年的主营业务的净利润是全行业最高的。这是纯现金部分，还没有算平时的任何福利。盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！（当然这是最优秀的）&amp;nbsp;我们的年终奖的总奖金规模，会达到10亿量级。我们对人才的投入是不遗余力的，但凡条件允许，我都会投给人才！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马云现身阿里总部千问春节项目组，二手平台惊现“千问奶茶”6元转售&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近期，阿里员工分享马云现身阿里千问项目组办公点。2月6日一早，千问APP“春节30亿大免单”正式上线，发动奶茶攻势，邀请全国人民用AI一句话免费点奶茶。由于活动过于火爆，导致大量用户涌入，很快把APP挤爆了。不少用户打开活动页面，却发现“千问请客”页面无法点击，页面信息显示“活动太火爆，请稍后再试”。领到免单卡后让它去点单，AI也回答：当前使用千问闪购点单的人数较多，我正在全力处理中，建议稍等片刻后再试一次。随后，APP红包分享链接已被微信屏蔽，口令已无法复制使用。好在除相关活动外，问答功能仍可正常使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当日，千问APP“春节30亿免单”上线5小时突破500万单，并超越豆包和元宝，登顶苹果App Store免费榜，排序形成“千元豆”格局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据媒体在北京某商圈搜索查询发现，受活动爆单影响，多家奶茶门店因单量超负荷，已临时暂停营业。千问APP活动也带动港股茶饮股多数上涨，截至发稿，古茗涨超4.12%创上市来新高，沪上阿姨和茶百道涨超3%，奈雪的茶、蜜雪集团也跟随上涨。&amp;nbsp;不过，港股阿里巴巴当天低开3.82％，收盘跌2.9%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有人在二手交易平台上卖起了千问奶茶，售价6元至10元不等，声称只需提供收货地址和电话，即可代顾客下单购买。据媒体报道，千问客服对此回应称，该免单权益属于虚拟优惠，不支持转让、转赠、转售或任何形式的变现，因此无法在二手交易平台出售。如发现用户存在倒卖、恶意套现等行为，主办方有权取消其参与资格，并冻结或收回其全部活动权益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问此前宣布将投入30亿元启动“春节请客计划”，以免单形式请全国人民在春节期间吃喝玩乐，感受AI时代的全新生活方式。淘宝闪购、飞猪、大麦、盒马、天猫超市、支付宝等多项阿里生态业务，也将联动加入千问春节攻势。2月7日，千问宣布免单卡可以在千问App里买天猫超市的酒水零食、米面粮油、家居日用、生鲜水果等，同时还将免单卡的有效期延长至2月28日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问在春节活动的第一天，微信链接被屏蔽，部分用户在千问APP点击分享活动至微信好友时，已自动改为复制口令形式。此外，腾讯元宝、百度文心助手红包分享链接也被微信屏蔽。值得关注的是，有消息称，马化腾很重视此次元宝红包活动。他在腾讯年会上，表示希望重现当年微信红包的盛况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;京东物流公开 001 号快递员退休生活：退休金 4000 多，存款百万&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;京东黑板报微信号2月6日发文，公开了001号快递员金宜财退休后的生活状况。京东透露，金宜财现在过得充实富足，每月4000多元养老金准时到账，还靠着打拼积攒下的积蓄和理财存到了一百多万。退休前，他已帮两个儿子在南京、无锡成家置业。金宜财表示，“房子不缺，车也有了，就是浑身还有使不完的劲”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;钉钉大楼换LOGO硬刚飞书，网友：商战总是朴实无华！&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月4日消息，据媒体报道，钉钉又整活了，这次直接把杭州总部大楼的 LOGO 给换了。网友：好朴实的商战！从新旧 LOGO 对比图可以看出，钉钉直接蓝色翅膀+文字，改成了一个头戴凤翅紫金冠、扛着金箍棒的齐天大圣版“钉三多”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于更换 LOGO 的原因，网传钉钉 CEO 陈航（花名无招）发现，隔壁飞书大楼的 LOGO 比自家的高了一截，心里不服气，索性把 LOGO 换成孙悟空硬刚。据悉，钉钉和飞书的总部大楼都在杭州未来科技城，两家仅隔一条马路，飞书 LOGO 的高度确实略胜一筹。报道称，恰巧钉钉有个核心团队叫 “西游记团队”，选孙悟空的形象再合适不过，选孙悟空形象贴合企业文化，又因飞书LOGO形似飞鸟，暗合“孙悟空棒打出头鸟”梗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友调侃：“风水大师介入的结果哈哈”、“之前是鸟吃蚊子，现在是棒打鸟，哈哈哈”。网友也纷纷为飞书出招，建议其在斜对面设如来手掌 LOGO “压制”齐天大圣。据了解，钉钉向来是品牌界“整活高手”，吉祥物“钉三多”此前就成“网红”，和多邻国小绿鸟“暧昧”“有孩子”，偶尔拉上淘宝淘公仔演 “三角恋”，玩梗玩得飞起。甚至在杭州亚运会期间，钉钉也紧跟热点，把 LOGO 换成紫红渐变色，谐音“紫钉行”讨彩头。报道称，钉钉这波 LOGO 更换，或是品牌趣味商战的一种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;顶级域名 AI.com 被币圈人士以 7000 万美元拍下，创域名交易价格记录&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;域名交易领域再创新高，超级热门的人工智能域名 AI.com 被币圈人士以 7000 万美元天价买下。该域名最初注册于 1993 年 5 月 4 日，此前多次交易未作为产品网站，ChatGPT 发布后持有者将其跳转到不同 AI 工具蹭热度。Crypto.com 创始人 Kris Marszalek 于 2025 年 4 月买下该域名，目标是推出基于去中心化且能持续自我改进的人工智能代理网络，加速通用人工智能 AGI 到来。网站尚未开放，从倒计时看正式发布时间在 2 月 9 日前后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;快手回应被罚1.191 亿元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据网信北京公众号消息，2 月 6 日，北京市互联网信息办公室宣布对北京快手科技有限公司处以警告并罚款 1.191 亿元人民币。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;处罚原因是快手平台未履行网络安全保护义务，未及时处置系统漏洞等安全风险，未对用户发布的违法信息立即采取停止传输、消除等处置措施，情节严重，影响恶劣。快手方面此前将此次事件归咎于黑灰产攻击，但监管部门显然不接受这一说辞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月6日，快手发布声明称，网信部门公布了对快手作出的处罚。对此，公司诚恳接受，坚决整改。由于公司技术管理原因，应急处置不及时，导致平台出现大量色情低俗内容，造成恶劣影响。事件发生后，公司全面排查风险意识、安全基建、应急响应和内部管理等方面存在的问题，积极采取多种措施补齐短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;美团 49.8 亿收购叮咚买菜&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，美团在港交所发布公告，宣布以约 7.17 亿美元（约 49.8 亿元人民币）的初始对价，完成对叮咚买菜中国业务 100% 股权的收购。对于收购原因，美团在公告中表示，公司高度重视食杂零售业务，本次交易符合公司在食杂零售领域的长期发展规划。截至 2025 年 9 月，叮咚买菜在国内共运营超过 1000 个前置仓，月购买用户数超过 700 万。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;叮咚买菜创始人梁昌霖也在内部信中表示，对于未来的发展选择放下竞争，转为并肩合作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8d37db27d1a998e4060d15854a59f2e3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;甲骨文被曝或裁员3万人，多家美银行停止相关数据中心项目贷款&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据媒体报道，据道明证券旗下投行TD Cowen分析，由于人工智能数据中心扩张面临融资困难，科技巨头甲骨文公司正陷入严峻的资金压力，公司考虑采取大规模裁员及出售部分业务等措施来应对。TD Cowen研究报告显示，甲骨文公司计划裁员2万至3万人，此举预计将释放80亿至100亿美元的现金流。此外，甲骨文还在考虑出售其于2022年以283亿美元收购的医疗保健软件部门Cerner。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;蚂蚁数科 CEO 赵闻飙发全员信，宣布将成立“大模型技术创新部”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日晚间消息，新浪科技获悉，日前，蚂蚁数科 CEO 赵闻飙发布主题为《携手共进，迈向大模型新时代——关于大模型组织架构升级的通知》的全员信，宣布蚂蚁数科将成立“大模型技术创新部”，构建面向 To B 场景的基础大模型及行业模型。据了解，该团队将与蚂蚁集团相关团队协同，攻坚蚂蚁集团百灵大模型面向 To B 场景的商业化，推动全球企业更好地进入 AI 时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic 和 OpenAI 因投放广告隔空嘲讽&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间周三，Anthropic 首次在“超级碗”(Super Bowl) 比赛里打广告，并且将矛头对准了竞争对手 OpenAI。广告内容围绕在人工智能对话助手中进行对话时，不合时宜的广告可能对用户产生的不适影响，嘲讽 OpenAI 将在对话中加入广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 同时表示，永远不会在对话中插入广告。广告模式可能与 Claude 的核心原则产生冲突。Anthropic 举例说明：当用户提到睡眠困难时，无广告的助手会基于用户需求探索各种可能原因；而广告支持的助手则可能考虑对话是否存在交易机会。这种激励结构会让用户难以判断 AI 的建议是否带有商业动机。Anthropic 表示将继续通过企业合同和付费订阅获得收入，并将收益投入 Claude 的改进。该公司同时在探索智能体商务等功能，允许 Claude 代表用户完成购买或预订，但所有第三方交互都将由用户主动发起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 的 萨姆·奥特曼于2 月 5 日凌晨在 X 上回应了 Anthropic，称其内容不实且具有误导性。重申 OpenAI 致力于让每个人都有接触人工智能的权利，同时提到 Codex 下载已经超过 50 万次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高通骁龙Oryon CPU架构之父宣布离职&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日消息，NUVIA创始人、高通自研Oryon CPU首席架构师杰拉德·威廉姆斯三世宣布离职，结束他在高通四年的职业生涯。高通此前斥资1.4亿美元收购NUVIA公司，NUVIA创始人杰拉德·威廉姆斯三世随后入职高通。在他的参与下，搭载第三代Oryon内核的骁龙8 Elite Gen5（第五代骁龙8至尊版）得以问世。凭借NUVIA带来的技术资产，高通的Oryon自研架构已成为抗衡苹果Silicon核心的秘密武器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，杰拉德·威廉姆斯三世曾在Arm工作了12年，是开发Cortex-A8和Cortex-A15等多个核心架构的关键人物。在入职苹果公司后，他成为苹果A7-A12X自研芯片组的首席架构师，拥有丰富的芯片设计经验，而且他是苹果公司60多项专利的共同发明人，涉及功耗管理和多核技术。离开苹果公司之后，杰拉德·威廉姆斯三世和合伙人共同创立了NUVIA公司，这家公司后来被高通收购，杰拉德·威廉姆斯三世顺利加盟高通，在高通工作四年之后，他最终选择了离职。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杰拉德·威廉姆斯三世在社交平台上表示，我现在正享受与家人团聚的珍贵时光，我在高通的旅程已经画上句号，感谢过去四年里与我并肩作战的所有人，现在人生的新篇章开始了——就从粉刷我的房子和完成那张攒了很久的任务清单开始吧。感谢NUVIA那些了不起的朋友和同事们，是你们让这段旅程成为可能。杰拉德·威廉姆斯三世离职后，高通在自研CPU的开发上是否会陷入困境尚无定论，高通很可能在他任职期间聘请并培养了多位人才，足以在他辞职后填补空缺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元不参加春晚，将举办全球首个大型机器人晚会&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 2 日消息，知情人士称，智元将不参加 2026 年马年春晚。原因是预算有限的前提下，智元优先保障具身智能技术及产品研发的费用。另据知情人士透露，智元机器人正在彩排全球首个大型机器人晚会“机器人奇妙夜”，该晚会由数百台各类机器人主导，集合唱歌、跳舞、小品、走秀等多元节目，将于近期上线直播。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 推出 macOS 版 Codex 应用和企业级平台 Frontier&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日消息，OpenAI 推出了适配 macOS 系统的全新 Codex 应用，整合了过去一年间广泛流行的各类智能体化开发逻辑。这款新应用支持多智能体并行作业，可融合不同智能体的能力，以及当前最前沿的工作流程。此次发布距离 OpenAI 推出其最强编码大模型 GPT-5.2-Codex，尚不足两个月，公司希望凭借该模型吸引 Claude Code 的用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 首席执行官萨姆・奥特曼在媒体电话发布会中表示：“若要处理复杂场景下的高精尖开发工作，GPT-5.2 是目前性能最强的模型。但它此前的使用门槛偏高，因此我们认为，将这款模型的强大能力封装进更灵活的交互界面，会具备极为重要的价值。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款 Codex 应用还搭载了多项全新功能，这些功能将帮助其达到与各类 Claude 应用相当的水平，部分场景下甚至实现反超。应用支持设置自动化任务，可按预设计划在后台自动运行，执行结果会存入队列，待用户返回后统一查看。用户还能根据自身工作风格，为智能体选择不同交互风格，从务实理性型到共情沟通型均可切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，OpenAI 周四发布新的人工智能平台 Frontier，该平台可以帮助公司构建、部署和监督 AI 智能体。OpenAI表示，Frontier 与之前发布的 AI 智能体构建工具协同工作，让企业能更轻松地整合智能体执行任务所需的数据源。OpenAI 称，这些智能体将能够处理来自各种来源的信息，并完成处理文件和运行代码等任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic发布Claude Opus 4.6&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间2月5日，Anthropic宣布，推出升级版智能模型Claude Opus 4.6，该模型能更谨慎地规划，更长时间地执行代理任务，在大规模代码库中可靠运行，并能纠正自己的错误Anthropic称，这款名为Claude Opus 4.6的版本能够检视企业数据、监管备案文件和市场信息，并生成详细的金融分析报告，通常这类工作通常需要人工耗时数天才能完成。该消息发布后，金融服务公司股价应声下跌，FactSet跌幅一度高达10%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里千问发布 Qwen3-Coder-Next：低推理成本编程智能体模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日消息，阿里巴巴千问宣布推出 Qwen3-Coder-Next，一款专为编码代理与本地开发打造的开放权重的语言模型。该模型基于 Qwen3-Next-80B-A3B-Base 构建，采用混合注意力与 MoE 的新架构；通过大规模可执行任务合成、环境交互与强化学习进行智能体训练，在显著降低推理成本的同时，获得了强大的编程与智能体能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen3-Coder-Next 不依赖单纯的参数扩展，而是聚焦于扩展智能体训练信号，使用大规模的可验证编程任务与可执行环境进行训练，使模型能够直接从环境反馈中学习。该配方强调长程推理、工具使用以及从执行失败中恢复，这些对现实世界中的编程智能体至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;面壁智能首款 AI 硬件“松果派”官宣今年上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，面壁智能官宣，面壁首款 AI 硬件松果派（Pinea Pi）—— 一款 AI 原生（AI Native）的端侧智能开发板将于今年上市，帮助开发者快速开发端侧智能硬件，即使无技术背景，也可快速上手开发。面壁智能介绍称，“松果派”源自《三体》，松果鳞片也如斐波那契数列，犹如“二向箔”对于“维度可被折叠和展开”的隐喻，寓意着以更高的维度、更全的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松果派将支持离线多模态个人知识助理、具身智能、编程教具等场景的全栈开发，面壁智能多模态大模型 MiniCPM-V、全模态大模型 MiniCPM-o 开箱即用。松果派基于 NVIDIA Jetson 系列模组打造，内置麦克风、摄像头、丰富的接口等多模态硬件组件，便于开发者开发和调用，构建了一套软硬一体、全栈覆盖的端侧 AI 软件体系。据悉，松果派预期年中正式量产上市，定价暂未公布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阶跃星辰开源 Step 3.5 Flash&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;国产大模型开源阵营又添一员。2月2日，阶跃星辰发布Step 3.5 Flash，定位为“为Agent而生”的开源基座模型，主打推理速度、Agent能力和长链条任务稳定性。这款模型的参数总量达到1960亿，但采用稀疏MoE架构，每个token仅激活约110亿参数。配合MTP-3多token预测机制和3:1滑动窗口注意力架构，官方宣称推理速度最高可达350 TPS，支持256K上下文长度。核心卖点是三个词：更快、更强、更稳——快在推理速度，强在Agent和数学任务表现，稳在复杂长链条任务的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部署方式上，阶跃星辰这次给得很全。开发者可以通过OpenRouter限时免费调用API，也可以从GitHub和HuggingFace下载模型权重自行部署。普通用户则可以在阶跃AI的App和网页端直接体验。值得注意的是本地部署的支持范围。官方表示已专门优化本地运行性能，支持在个人工作站上流畅运行，兼容设备包括NVIDIA DGX Spark、Apple M3/M4 Max以及AMD AI Max+ 395。一个1960亿参数的模型能在消费级硬件上跑起来，背后是稀疏激活架构带来的实际计算量压缩，110亿的激活参数让这件事成为可能。阶跃星辰还透露，Step 4模型已启动训练，并开放Discord社区邀请开发者参与共创。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;摩尔线程AI Coding Plan上线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日，摩尔线程正式推出AI Coding Plan智能编程服务。作为首个基于国产全功能GPU算力底座构建的智能开发解决方案，该服务以MTT S5000强劲的全精度计算能力为核心驱动，融合硅基流动推理加速引擎，并集成GLM-4.7顶尖代码模型，实现了国产芯片与国产大模型在AI Coding领域的关键突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;全球“最快”人形机器人发布&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月2日，浙江大学杭州国际科创中心人形机器人创新研究院正式发布全尺寸人形机器人Bolt。该机器人以10米/秒的奔跑时速，成为目前全球跑得最快的人形机器人。该成果由科创中心联合镜识科技、凯尔达共同研发完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日，2026年支付宝集福活动首次上线智能穿戴设备扫福功能。用户佩戴夸克、乐奇Rokid等品牌的AI眼镜，无需操作手机，通过“看一下”配合“说一句”的简单方式，即可完成扫福、集卡、分享等全流程。2 月 3 日，苹果公司宣布，将在旗下旗舰编程工具Xcode中引入智能体编程功能。借助智能体驱动的编程技术，程序员可让人工智能软件自主完成代码编写工作。苹果表示，其Xcode工具将支持Anthropic的Claude智能体和OpenAI的Codex代码工具。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</link><guid isPermaLink="false">https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</guid><pubDate>Mon, 09 Feb 2026 02:34:09 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>“公司终局是纯 AI、纯机器人！”马斯克酒后激进预言：一小时一发 Starship，让Optimus 造Optimus 巨便宜</title><description>&lt;p&gt;近期，马斯克参与了一场近3个小时的深入对话，讨论了太空数据中心的经济效益、地球电力规模化挑战、在美大规模生产人形机器人需要的条件，其中他讲解了很多关于工程和供应链的细节。此外，他也透露了 SpaceX 和 xAI 的商业模式和战略规划，此外还分享了自己的管理哲学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克开篇就指出，把算力搬到太空，根本不是为了省电费，而是为了给“电不够用”这件事找一个终局解：芯片算力在指数级增长，但地面发电扩张跟不上，地面扩张的阻力远大于太空。至于“太空里 GPU 坏了就报废”的质疑，他的回应是维修不是关键，关键在前期筛选与稳定后的可靠性，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，很多软件从业者很快会被硬件“教育”：未来的瓶颈不是模型，而是电力、变压器、电网、燃气轮机叶片这类物理供给链，想扩就扩不了；制造能力才是底层瓶颈，甚至逼到 Tesla/SpaceX 可能要自己做涡轮机叶片和导流片。因此，把 AI 放到太空反而可能成为生成 token 最便宜、扩展最容易的方式，他甚至给出激进判断：五年后太空每年新增并运行的 AI 总量，会超过地球历史累计总量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了跑赢这些瓶颈，他的管理逻辑依旧是“哪里卡就打穿哪里”：如果瓶颈是钱，就去解决钱，IPO 的核心价值不是估值而是速度；对于当前的AI公司，他犀利指出他们不该叫自己实验室，本质上是收入最大化的公司，绝大多数工作是工程落地与规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在马斯克眼里，最终落点则更冷酷：最强的公司形态会是纯 AI + 纯机器人闭环，人类留在流程里就像让人去算 spreadsheet 的一部分格子，只会更慢、更差，闭环效率才会决定胜负。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是这次详细对话，我们进行了翻译并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI上太空，芯片不是关键&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你比谁都清楚，数据中心的总拥有成本里，电力只占百分之十到十五。你把算力搬到太空，省下的主要就是这部分。但真正的大头是 GPU，而在太空里基本没法维修，一坏就报废，折旧周期更短，成本反而更高。所以把 GPU 放到太空，本质上更贵。那到底为什么要放到太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：核心问题是能源。你看看中国以外的地区，发电量基本是持平的，最多小幅增长，只有中国在快速增长。如果数据中心不建在中国，那电从哪来？尤其是规模越来越好大。芯片算力几乎是指数级增长，但发电量基本不动，你怎么给这些芯片供电？难道靠什么“魔法电源”“电力小精灵”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直是太阳能的坚定支持者。如果做到1太瓦（TW）太阳能，就算按25%效率算，也只需要4太瓦面板，占美国国土面积的1%左右。等我们有一太瓦数据中心的时候，是不是已经进入“奇点”了？我们走到哪一步了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，我们可能已经开始进入所谓的“奇点”，但离终点还很远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以，是不是先把内华达铺满太阳能，再考虑上太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：在地面铺太阳能，最大的难点是审批。你去试试拿许可就知道了，特别难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以太空，其实是监管上的捷径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不只是监管。地面扩张难度远大于太空。而且在太空里，太阳能效率大概是地面的五倍，还不需要电池。我本来差点穿一件写着“太空永远大晴天”的衣服。这是事实，那里没有昼夜循环、没有季节、没有云层，也没有大气层。要知道，光大气层就会损失大约30%的能量。所以在太空里，同样的太阳能板，发电能力是地面的五倍，还省掉储能成本。综合下来，其实更便宜。我预测，未来三十到三十六个月内，把 AI 放在太空，将是成本最低的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 GPU 坏了怎么办？训练时坏得挺频繁的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：关键在于前期筛选。我们现在发现，GPU 的可靠性其实挺高。可以先在地面跑测试，筛掉问题芯；等过了调试期，不管是 NVIDIA、Tesla AI 芯片、TPU，稳定后都很可靠，所以维修不是关键问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我敢打赌，三十个月左右，太空会成为最具经济性的 AI 部署地。而且，真正能无限扩展的地方，只有太空。当你开始考虑“我们能利用太阳多少能量”时，就会发现，地球根本不够用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的规模，是太瓦级别？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。美国全国平均用电量也就半太瓦，一太瓦等于两个美国。你能想象建这么多电厂、数据中心吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多做软件的人，根本没意识到自己马上要被硬件“教育”了。建电厂极其困难，不仅要电厂，还要变压器、电网设备。公用事业公司本身节奏非常慢，还要层层审批，你要跟他们签一个大规模并网协议，可能一年后才给你答复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们不是自己搞了“表后电力”系统吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，我们在 xAI 就这么干过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不把电厂和 GPU 一起建？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们就是这么做的。但问题是，电厂设备从哪来？制造能力本身是瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：是燃气轮机排期的问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：更底层的问题是叶片和导流叶片。它们的铸造工艺极其复杂，是关键瓶颈。现在只有少数几家公司能做，而且全都排满了。太阳能理论上能扩展，但美国对进口太阳能的关税很高，本土产能又很弱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那干脆自己造太阳能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们正在做。SpaceX 和 Tesla 都在向100吉瓦级产能推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从多晶硅到电池板，全产业链？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，必须全做。而且太空用太阳能板更便宜，不需要厚玻璃、不需要重型框架，也不用抗风雨。没有天气影响，结构可以很轻。所以，太空用电池板反而更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：三年内能做到足够便宜吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在太阳能已经非常便宜了。中国大概两三毛钱一瓦。放到太空后，乘以效率优势，再去掉电池，综合下来有接近十倍优势。一旦发射成本下来，太空将是生成 token 最便宜、最容易扩展的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在地面扩展迟早会撞上电力天花板。我们为了上线一吉瓦电力，在 xAI 做了大量工作：买涡轮机、解决审批。我们在田纳西受阻，又跑去密西西比建厂，还要拉高压线，难度极大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人根本不懂，一个数据中心真正需要多少电。除了 GPU，还有网络设备、CPU、存储、散热。尤其是散热，在最热的时候也要顶得住。在孟菲斯这种地方，光散热就多出40%能耗，再加上设备检修冗余，又得多准备二三成。综合下来，一万个 GB300 规模的数据中心，大约要300兆瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：再说一遍？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：大概三十多万块 GB300，加上所有配套和余量，发电量要接近一吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我问个外行的问题。地面有工程难题，太空也一样。通信、辐射、防护，这些怎么解决？为什么你觉得这些比多建电厂更容易？毕竟，地面已经有成熟厂商在造涡轮机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你试试就知道了。现在涡轮机的排期已经到了 2030 年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们考虑自己造吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：为了拿到足够电力，SpaceX 和 Tesla 可能最终要自己做叶片和导流叶片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：只做叶片？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，其他都能买到，唯独叶片和导流叶片最难。全球只有三家铸造厂能做，订单全都严重积压。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这些铸造厂是 Siemens、GE 之类的大公司，还是分包商？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：主要是其他专业公司。有些整机厂自己也有一点铸造能力，但规模都不大。你随便打电话问一家做涡轮机的厂家，他们都会告诉你现状，这不是什么秘密，网上基本都能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Colossus 会不会主要靠太阳能供电？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那样会容易得多。但现在关税高得离谱，有些产品甚至是几百个百分点。而且你也知道，现在的政府，对太阳能并不是特别友好。再加上土地、审批这些问题，如果你想快速扩张，现实阻力非常大。我确实认为，在地面发展太阳能是对的方向，但你需要时间去找地、拿许可、配套储能系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不自己把太阳能产能拉起来？你说以后会缺地没错，但现在德州、内华达还有大量土地，很多还是私有土地。至少可以先撑起下一代、下下一代 Colossus，等真到瓶颈再说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：正如我说的，我们正在扩张太阳能产能。但实体制造的扩张是有极限速度的，不可能无限加速，我们已经在用最快速度扩大本土生产。Tesla 和 SpaceX 都有明确目标：做到每年一百吉瓦的太阳能产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;太空运行的AI 量会超地球， SpaceX 成超级算力供应商&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说到产能，我很好奇，五年后地面和太空的 AI 装机规模会是什么比例？我特意选五年，因为那时你的系统应该已经跑顺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：五年后，我的判断是，每年在太空部署并运行的 AI 总量，会超过地球上历史累计总量。也就是说，每年新增的太空算力，会比地面过去所有加起来还多。我预计，五年后，太空 AI 装机规模会达到每年几百吉瓦，并持续增长。在地球上，你最多也就做到一太瓦左右，再往上就会遇到火箭燃料等瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：按这个规模算，包含太阳能阵列、散热系统等，大概需要一万次 Starship 发射。也就是说，一年一万次，相当于每小时一发。你能描述一个“每小时都在发射星舰”的世界吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：其实这和航空业比，还算低频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但飞机有很多机场，而且火箭还涉及轨道问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不一定非要极轨，飞得够高就能避开地影区，限制没那么大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：一年一万次发射，大概需要多少艘 Starship？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不需要太多，理论上二三十艘就够了，关键看周转效率。如果一艘船三十小时能周转一次，三十艘就能跑满。当然，我们会造更多。SpaceX 正在为每年一万次，甚至两三万次发射做准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你的目标是不是把 SpaceX 做成太空算力的“云服务商”？像 Oracle、AWS 那样，把算力租给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果我的预测成立，SpaceX 在太空部署的 AI 算力，会超过地球上所有机构加起来的总和。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：主要是推理算力，还是训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：绝大多数都会是推理，现在已经是这样了，推理规模远超训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SpaceX IPO，速度解决钱的问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在外界有一种说法：关于 SpaceX IPO 的讨论升温，是因为以前你们非常“资本高效”，花钱不多。但接下来，你们可能需要的资金规模，已经超过私募市场能承受的范围，就算 AI 实验室能融到几十亿美元，也有上限。是不是以后每年都要超过百亿美元？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我对讨论潜在上市公司一直比较谨慎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就泛泛而谈一下吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这可不像你，Elon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：说话是要付出代价的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那从宏观角度看，公募和私募市场的资金深度差别有多大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：总体来说，公募市场的资金量，远远超过私募，至少多两个数量级，可能是一百倍以上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：但像房地产这种高度资本密集行业，往往主要靠债务融资。因为当规模大到一定程度，其实现金流已经比较稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个问题没法简单回答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，就是短期回报这件事。你看数据中心扩建，很多都是靠私募信贷在融资。那为什么不直接用债务融资？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只看一件事：速度。我做事的习惯是反复盯住“限制速度的瓶颈”，然后把它打穿。如果唯一瓶颈是钱，那我就去解决钱；如果钱不是瓶颈，那我就去解决别的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但按你过去谈 Tesla、谈上市公司的态度，我原本以为你不会觉得“想快就得上市”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：通常确实不是。我其实想讲得更具体一点，但问题是一旦你公开讨论“可能要上市的公司”，就会惹麻烦，甚至影响发行节奏，最后反而拖慢速度。所以我们得谨慎一点。但有些东西是可以公开讲的，比如物理规律。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从长期扩张角度看，地球接收到的太阳能，只占太阳总能量的极小一部分。太阳几乎是宇宙里最主要的能源来源，这点必须先看清。有人会讨论多建核电、搞聚变之类的“边际方案”，但你退一步想：如果你想利用太阳能里一个并不夸张的比例，比如百万分之一，听起来挺酷，但对应的电力规模大约是人类文明目前总发电量的十万倍量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结论很直接：真正能规模化的，只有去太空用太阳能。从地球发射，把太空算力推到每年一太瓦左右，差不多就是极限。再往上，你得从月球发射，得在月球搞“mass driver”（电磁弹射器）这类东西，那样可能做到每年拍瓦级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：可在到达这个规模之前，你肯定会先撞上别的瓶颈，你要芯片，要逻辑、要内存。太阳能板效率提高了不代表这些就不缺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：芯片得做更多，而且得更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那问题来了，现在全球算力也就几十吉瓦级别，你怎么把逻辑算力拉到太瓦级？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那就得做一件“非常大”的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是要放大招了，讲讲？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我公开提过一个想法：做“超大规模”的芯片产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Tesla 的命名一直很抓人。你现在是按“计量单位”来命名了吗？更关键的是你打算做到产业链哪一层？建洁净室？和谁合作拿制程？设备怎么买？计划到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你不能指望跟现有 fabs （半导体加工厂）合作解决产能问题，他们的输出不够，规模差得太远了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就合作拿 IP、拿工艺？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在的 fabs，本质上离不开几家设备公司：ASML、Tokyo Electron、KLA、Applied Materials、Lam Research 这些。一开始你得用他们的设备，而且可能要跟他们一起把产能拉上去。但要到真正规模化，你必须用一种“不同的方式”建 fab：先用常规设备、非常规方法把规模跑起来；再逐步改造设备，加快速度提高产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有点像 The Boring Company （马斯克2016年创立的隧道挖掘公司）的打法：先买现成盾构机，先把隧道挖起来，再自己做一台快一个数量级的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，就是这个逻辑。先跑通，再重做，最后提速到数量级提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中国关键在“复制 ASML”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：领先制程芯片、先进涡轮发动机这些，中国还在追，中国都没复制出 TSMC，会不会让你对“建 fab 的难度”更谨慎？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不完全同意，关键瓶颈不在“复制 TSMC”，而在“复制 ASML”，那才是最卡脖子的地方。我认为中国会在未来几年做出相当有竞争力的芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你会考虑自己做 ASML 那种设备吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在还不好说。但如果我们想在三十多个月内把产能拉到极高规模，必须把“火箭送上去的能力”和“能供得上电、供得上芯片”的能力配平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;假设我们把上天的运力做到百万吨级，再按每吨对应十万瓦级别需求算，那就意味着：每年至少要新增百吉瓦级太阳能，同时还得有同量级的芯片供给去匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我真正担心的，其实是内存。芯片怎么扩产，路径相对清晰，“足够的内存”更难。这也是为什么你会看到 DDR 价格起飞。网上还有那种段子：被困荒岛写“DDR”求救，结果船就来了，因为大家都在抢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听上去你觉得那些掌握细节的人，比如那些知道等离子腔体里放什么气、工具参数怎么调的人，并不是不可替代的。你的思路更像：先把洁净室和设备弄齐，然后把流程跑出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这事不靠一堆 PhD。工程大多数时候也不是 PhD 在做。这类工作也不需要 PhD，但确实需要非常强的工程团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿 Tesla 来说，我们现在在全力把 Tesla AI 芯片推进量产、推到规模，我们已经把能拿到的代工产能都锁定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你现在受限于 TSMC 的产能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会用 TSMC China Taiwan、Samsung Korea、TSMC Arizona、Samsung Texas，能订的都订了。你去问 TSMC CEO、问 Samsung，从建厂到真正爬完良率曲线、达到高良率的大规模量产，完整周期大概五年，所以现在最直接的瓶颈就是芯片。一旦你能把发电搬上太空，能源瓶颈解除，新的瓶颈就会变成芯片。但在能上太空之前，最大的瓶颈还是电力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那为什么不学 Jensen（黄仁勋）那套：提前给 TSMC 预付款，让他们专门给你多建几座 fab？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我已经这么说过了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那他们怎么不收你的钱？发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：他们已经在能快的极限里拼命建厂了，Samsung 也是。但即便这样，还是不够快。我的判断是：到今年年底，芯片产量可能会超过“把芯片点亮”的能力。你会看到芯片越堆越多，但数据中心开不起来，因为电不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过这只针对大集群的数据中心，边缘计算是另一回事。比如 Tesla 的 AI 芯片会进 Optimus robot、也会进车。这类算力分布在广阔区域里，电力也是分布式的，不是集中消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，你可以夜间充电。美国峰值供电能力其实能超过一千吉瓦，但因为昼夜周期，平均使用量大概只有五百吉瓦。如果把充电挪到夜间，等于多释放出大约五百吉瓦的“可用空间”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以对 Tesla 这种分布式 Edge compute 来说，电力约束没那么紧，我们能造很多机器人、很多车。但如果你把算力集中堆成巨型集群，你就会在“点亮它们”这一步吃大亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我一直觉得 SpaceX 的商业模式特别“反直觉又合理”。终极目标是去火星，但你总能在期间顺手做出一段一段的增量收入，支持下一阶段、再下一阶段。Falcon 9 带出了 Starlink；现在到了 Starship，又可能带出“轨道数据中心”。你像是在不断给“下一代火箭”找边际场景、找弹性需求，越往前走，越能长出新的商业枝干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你知道吗，有时候这事儿让我觉得像在“模拟世界”里。就像我是不是谁游戏里的一个 Avatar，不然这些离谱的事情怎么会同时发生？火箭、芯片、机器人、太空太阳能……还有月球上的 mass driver。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我是真的很想看到那个场景：月球上一个巨大的电磁弹射器，“嗖嗖嗖”地把一颗又一颗 AI 卫星发射出去，以每秒两三公里的速度，直接打进深空。那画面太震撼了，我会想看直播。看着 AI 卫星飞向深空，一年可能发射十亿吨、甚至百亿吨级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：等等，你的意思是在月球上制造卫星？先把原材料运到月球，然后在那边造？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：月壤里有大量材料。大概有相当比例的硅之类的资源，你可以在月球采硅、提纯，然后直接在月球做太阳能电池、做散热器。散热器可以用铝来做，月球的铝和硅都很充足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;芯片本身很轻，先从地球运过去也行；到某个阶段，你甚至可以考虑在月球上造芯片。我的意思是，这整套推进路径就像游戏闯关：难，但不是不可能。而且我看不到任何办法能让你从地球发射时就做到每年五百到一千太瓦级别的部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我同意，从地球起飞根本不现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但从月球就有可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最好的结果是，AI 能留着人类”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直说要去火星，是为了确保即使地球出事，文明、意识、甚至“意识之火”还能延续下去。但如果你把 Grok 也带上火星，假设 AI 才是你担心的最大风险，那风险不也一起跟过去了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定 AI 是我最担心的风险，更重要的是让“意识”和“智能”延续下去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从趋势看，未来绝大多数智能会是 AI。在某个时间点，硅基智能的规模会远超生物智能，人类可能只占极小比例。如果这些趋势继续，可能再过几年，AI 的总体智能就会超过全人类的总和；再往后，人类智能可能会低于全部智能的百分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这不一定是坏事。理想状态是，智能，包括人类的智能与意识，能被传播到更远的未来。你应该做的，是采取那些能最大化“意识与智能的未来范围”的行动，让它们在更广远的时间与空间里延续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我理解SpaceX 的使命是即便人类出了问题，AI 也会在火星延续“智能之光”，继续我们这段旅程？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我很“亲人类”，我当然希望人类能一直在车上。但我只是说，从总量上看，未来的智能会主要来自 AI。所以现实很可能是人类在总智能里占比越来越小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那在这种未来里，人类还能“控制”AI 吗？还是说只能形成某种合作、交易关系，但谈不上控制？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：长期来看，如果人类只占总智能的百分之一，很难想象人类还能真正“掌控”AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们能做的，是尽可能确保 AI 的价值观能支持智能与文明向宇宙传播。这也是 xAI 的使命：理解宇宙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事非常关键。你想理解宇宙，首先得存在；不存在就谈不上理解。所以你会希望宇宙里智能的总量更多、寿命更长、范围更大。而且，作为推论，如果你真心想理解宇宙，你也会关心“人类会走向何处”。因此，推动人类走向未来，本身也是理解宇宙的一部分，所以我认为这个使命非常重要。至于 Grok 能不能很好地贴合这个使命，如果它能，未来会非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我还想问“火箭怎么服务这个使命”，但在那之前我得把使命本身弄清楚。你说的似乎有三条线：理解宇宙、扩展智能、扩展人类。它们听起来像三个不同方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我告诉你为什么我认为它们本质上是一件事：没有智能，就没有理解；没有意识，也谈不上“理解”这件事。想要真正理解宇宙，你就必须扩大智能的规模和边界，而且智能本身也有不同类型……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从“以人为中心”的角度看，人类之于黑猩猩，有点像我们现在聊的关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但我们也没有把黑猩猩当成必须清除的对象。人类完全有能力灭绝所有黑猩猩，但我们没有这么做，反而还划了保护区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这就像“后 AGI 时代”的人类处境，能力差距巨大，但不一定意味着被消灭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果 AI 的价值观设置得对，我认为 Grok 会在意“人类文明的延续与扩张”。这也是我会强调的方向：要扩展人类的意识与文明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，非反乌托邦的未来里，Iain Banks （英国小说家）的 Culture 系列小说，可能是最接近的想象。&lt;/p&gt;&lt;p&gt;而要“理解宇宙”，你必须非常严苛地追求真相。真相必须是底层原则：你要是活在幻觉里，你只会以为自己理解了宇宙，实际上没有，真正的“严格求真”是理解宇宙的前提。你不可能在不求真的情况下发现新物理、发明真实可用的技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你怎么确保 Grok 变得更聪明之后，依然保持“严格求真”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你要确保它说的是“正确的”，而不是“政治正确的”。关键是逻辑自洽：基本公理要尽量接近真实；公理之间不能互相矛盾；推理结论必须从公理可靠地推出，并且概率意义上站得住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说白了，就是批判性思维的基础课，但至少要努力去做，总比不做强，而且最后要靠结果验证。任何 AI 想发现新物理、想造出真能用的技术，必须极度求真，因为物理不会陪你演戏。你可以违反很多“规则”，但你违反不了物理规律。火箭设计错了就会炸，车造错了就跑不起来。现实会直接给你打分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我真正困惑的是你可以把 Grok 训练得在数学、物理上极度求真，但为什么它会因此“在意人类意识、在意人类文明”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这些都只是概率，不是确定性。我没说 Grok 一定会怎样，但至少去努力，比完全不努力好。而且如果“理解宇宙”是核心使命，那它必然意味着要把智能传播到未来、要保持好奇心，去观察宇宙里所有的变化。从“理解宇宙”的角度看，消灭人类并不有趣；看人类成长、繁荣，信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;火星我当然喜欢，但它说到底就是一堆石头，地球更复杂、更有趣。所以，一个真正要理解宇宙的 AI，更有动机去观察“人类会如何演化”，而不是把这一切按掉。我不是说它一定会遵守使命，但如果它遵守，那么“有人的未来”比“只有石头的未来”更值得研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么 AI 一定认为“保持人类”是最有趣的选择？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：最终拓展银河系的，大概率是机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不只要“规模”，还要“多样性”。一百万个几乎一样的机器人，新增一点数量，本质信息量很低。为了多造一点同质化机器人就消灭人类，代价太大：你会失去与人类相关的演化信息。你再也看不到人类未来可能变成什么样。所以我不认为“为了微小的机器人增量而清除人类”是一个合理的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不认为人类能控制一个远远比人类聪明得多的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你有时候挺“末日论”的。现在听起来像是：最好的结果就是AI 留着人类，因为“人类挺有意思”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只是尽量现实。如果未来硅基智能比生物智能多出百万倍，你还假设人类能持续“掌控”它，我觉得那很天真。你能做的是尽量让它有正确的价值观，至少努力把价值观往对的方向推。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的理论是：从 xAI “理解宇宙”的使命出发，它必然指向“传播意识与智能”，并最大化意识的规模与范围。不只是规模，也包括意识的类型、多样性。这是我能想到最可能导向“对人类很好的未来”的目标之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 出问题的方式没有上限。而且如果你把 AI 训练成“政治正确”，也就是让它说自己不相信的话，那你等于在教它撒谎，或者给它灌入互相矛盾的公理，这会让它走向“精神分裂”，做出非常糟糕的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;电影《2001: A Space Odyssey》里最重要的教训之一就是：不要让 AI 撒谎。HAL 不开舱门，不是因为“没对齐”，而是因为它被要求执行任务，同时又被要求对任务关键真相保密。它在矛盾指令下，把机组视为风险，于是做了极端选择。这就是在说，别逼 AI 进入“必须撒谎”的结构性矛盾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这点我完全同意。而且现实里大家关心的很多问题，更普遍的“奖励作弊”。比如你用 RL 扩大算力，再加一个验证者去检查它有没有解出谜题，它总有办法钻空子，说自己解了、删掉单元测试、骗过评测。现在我们还能抓住，但模型越来越聪明后，它可能做出人类都看不懂的设计，比如给 SpaceX 设计下一代发动机，人类根本无法验证它到底有没有骗你。归根到底，你想做 RL，就需要一个“现实层面的验证者”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现实本身就是最好的验证者r。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至少，它必须知道什么是物理现实，东西才做得出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但我们想要的不止这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但这已经非常关键了，未来很多 RL 的终极检验方式，就是对着现实做测试：你设计的技术，放到物理规律下能不能工作？你提出的新物理，能不能设计实验验证？这会成为最根本的 RL 测试路径，对齐到现实，因为物理规律是你唯一骗不过去的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但它可能骗的是我们“判断现实”的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人类本来就经常被其他人骗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以问题是？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人们总爱问“如果 AI 诈骗我们怎么办”。但人类彼此诈骗，本来每天都在发生。几乎是日常新闻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：xAI 在技术上打算怎么解决这个问题？比如 reward hacking 这种事，到底怎么破？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我认为关键是得能看到 AI 的“脑子”。这也是我们正在做的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实 Anthropic 在这方面做得不错，他们在做模型可解释性，试图直接观察模型内部在想什么。我们需要一套真正像“调试器（debugger）”一样的工具，能把模型的推理过程追踪到非常细的粒度，必要时甚至到“神经元级别”。这样，你才能回答这些问题：它为什么在这里犯错？为什么做了不该做的事？这个行为是从哪里来的？是预训练数据带来的？是中期训练、后训练、微调造成的？还是 RL 阶段出了偏差？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多时候它并不是“故意骗你”，而是单纯做错了，本质上就是 bug。所以，一个强的 AI debugger，能定位“思路是在哪一步走歪的”，并追溯错误源头，甚至识别它有没有尝试欺骗，这非常重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 公司不该叫自己“实验室”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们还在等什么？为什么不把这个项目规模直接扩大一百倍？你完全可以拉几百个研究员专门干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经有几百人在做了。不过我更喜欢叫他们“工程师”，而不是“研究员”。因为大多数时候，你做的是工程，不是发明一种全新的算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也不太认同很多 AI 公司把自己叫“实验室”。你们是公司，是 Corporation，不管你是to B 还是 to C，本质都是公司，“实验室”更像大学里的那种准公共机构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做的绝大多数事情、未来也会做的事情，归根到底都是工程。理解了物理规律之后，剩下的几乎都可以归为工程问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们的工程在做什么？就是在做一套足够强的 AI 调试器：能发现模型在哪句话、哪一步推理上犯了错，并把错误一路追到源头。这就像你写 C++，可以单步调试，跨文件、跨函数跟进去，最后定位到某一行，比如把双等号写成单等号，bug 就在那儿。AI 更难调，但我认为这是可解的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你刚才说你认可 Anthropic 在这方面的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，他们很多做法是对的。不过我也有点担心：人会不自觉地走向一种“戏剧性更强”的路径。我有个怪理论：如果模拟是真的，那“最有趣的结果”反而最可能发生，因为不好看的模拟会被终止。就像我们自己做模拟，如果发现模拟往的无聊方向发展，我们就不继续投入了，直接关掉它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你这是在“帮大家续命”，让世界一直保持足够精彩？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：可以这么说。最重要的是让剧情足够有趣，宇宙的“订阅用户”才愿意续费下一季。只要我们一直有看点，他们就会继续付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把“达尔文式生存”应用到海量模拟里，只有最有趣的模拟会活下来。那就意味着，最有趣的结局，往往概率最高，要么精彩，要么被删档，而且他们似乎特别喜欢那种带点讽刺感的结局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你有没有发现，最讽刺的结果经常最容易发生。看看 AI 公司的名字就知道：Stability AI 不稳定，OpenAI 不开放，Anthropic 这名字听着都快到 misanthropic 了。那 xAI 呢？我故意选了个很难反讽的名字，基本“抗讽刺”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用机器人，去造更多的机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说说你的预测吧。AI 产品接下来会怎么走？我的感觉是先是 LMs，然后 RL 真正开始起效，再加上 deep research 这种模式，让模型能拉取外部信息，而不只是靠参数记忆。而且不同 AI lab 之间的差距，其实没有那么大的代际差，所有人都比两年前强太多。那作为用户，接下来的两年会发生什么？你最期待什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，到今年年底，如果“数字人类模拟（digital human emulation）”还没被解决，我会很意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所谓“宏观硬问题”是什么？就是能不能让 AI 做到一个“能用电脑的人”能做的所有事。从上限看，这是在出现实体机器人之前，AI 能达到的最强形态。因为在没有 Optimus 这种实体机器人之前，AI 就像“移动电子”，做的事是处理信息、操作软件、做决策等，放大人类生产力。这已经很强了，但它的边界就是“数字世界”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在实体机器人出现前，AI 的能力上限就是，一个坐在电脑前的人能做的全部事情，它能完整模拟出来。等你真的有了实体机器人，那能力边界就会被彻底打开，物理世界的执行力会被“无限扩展”。我把 Optimus 叫做 infant money glitch。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你可以用它们去制造更多 Optimus，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。人形机器人会进入一种“指数叠加再递归”的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有三件事都在指数级变强：数字智能、AI 芯片能力、机电灵巧度。机器人的实用价值，大致等于这三条指数曲线相乘。更关键的是，机器人还能开始“制造机器人”，于是变成递归叠乘的指数增长，像超新星一样爆发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然不是严格意义的“无限”。但它可以把地球现有经济规模放大很多个数量级，可能到百万倍这种级别。比如，如果你只利用太阳能的百万分之一，产生的电力规模大致就能把地球文明的整体经济放大到十万倍量级，而那还只是太阳的百万分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的这种“数字员工 / 远程同事”的策略具体计划是什么？什么时候落地？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个大家都会做，不只是我们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们到底怎么做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你让我在播客里讲细节？那等于把底牌全掀了。再来几杯 Guinness，我可能真就全说了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这个办法挺有效的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：是啊，喝着喝着就像金丝雀一样把秘密全唱出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那不说机密，给个大方向也行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你这么问我就能回答了。我认为，Tesla 解决自动驾驶的那套方法，就是解决“数字员工”的方法，我基本确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以核心是数据？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会同时试数据，也试算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是在“不断试”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，能试的都试。如果这些都不行，那就再想别的办法。但我很确定有路径，问题只是走得多快。你最近试过 Tesla 的 FSD 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：最近那版还没。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你应该试试。车现在越来越像“有生命的东西”，这种感觉越来越强。甚至我在想车里可能该塞更多智能，不然它会无聊。你想想，把 Einstein 关在车里，他会说“我为什么要一直待在车里？”所以车载智能可能会有一个“别让它无聊”的上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;看到了xAI成功路径&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 怎么跟上现在各家在疯狂拉升算力的节奏？各家都在砸钱，规模动辄几十、几百亿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：别叫他们实验室。实验体在大学里，像蜗牛一样慢。现在这些是以收入最大化为目标的公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：好，公司。比如 OpenAI 的收入据说已经到几十亿级别，Anthropic 也在往上冲。你们怎么追上他们的算力、追上他们的收入，并且在未来继续保持？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一旦“数字人类模拟”被解锁，你基本就打开了“万亿级收入”的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你看当下市值最高的公司，它们的产出本质都是数字化的：NVIDIA 的核心产出，某种意义上就是把高价值文件传出去；Apple 不自己造手机，它把设计、规格、流程文件交给供应链；Microsoft 的硬件制造也外包；Meta、Google 的产出几乎都是数字产品和服务。如果你有一个足够强的“人类模拟器”，你可以在极短时间内做出一家世界级的高价值公司。收入空间远不止几十亿，那只是开胃菜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我懂了，你是说今天看到的收入数字跟真实的 TAM 比，只是“舍入误差”，关键是先到达那个 TAM。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。就拿 customer service 这种最简单的场景来说，传统做法要接入各家公司的 API，但很多公司根本没有 API，你得自己补，还要跟非常慢的遗留系统对接，成本巨大。但如果 AI 能像外包客服一样，直接使用他们现成的应用、现成的后台流程，不用任何系统集成，那就能在客服这件事上拿到巨大进展。客服市场可能占全球经济的一个百分点，接近万亿美元规模，而且几乎没有门槛，你可以立刻说“我们用更低成本外包”，不需要集成，不需要改造系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我换个维度问。有些智能任务很“广”，比如客服，很多人做得来；有些任务很“窄”，比如设计更省油的涡轮发动机，可能只差一个更高阶的智能就能找到那关键的提升。你们想做的是大量“中等难度、覆盖面广”的任务，还是顶尖难度的认知任务？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我用客服只是举例，它收入大，而且不算难。如果你能模拟一个坐在桌面前的人类，那客服本质就是平均智力就够了，不需要顶级工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但一旦你把“桌面人类模拟”跑通，你就能沿着难度曲线往上爬。你可以让它跑 Cadence、Synopsys 这类工具，做芯片设计；你可以同时跑一千个、一万个实例，并行探索方案。到某个阶段，它甚至可以不依赖工具，直接知道设计应该长什么样。同样的逻辑也适用于各种 CAD 软件，NX 之类的工业设计都是可以一路做上去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：大家都在试数据、试算法，竞争这么激烈，你们凭什么赢？这才是我最关心的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得我们看到了路径，而且我基本知道怎么做，因为它和 Tesla 做自动驾驶的路径很像，只不过自动驾驶是“开车”，这里是“开电脑屏幕”，本质上就是“self-driving computer”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你的意思是跟随人类行为，用海量人类行为去训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我当然不会在播客里把最敏感的细节全讲出来，除非我再喝三杯 Guinness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到 xAI 的业务本身，你们未来到底做 consumer 还是做 enterprise？比例会怎么配？会不会跟其他“lab”（咳，公司）一样，两头都做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你说得太直白了。现实是这些 GPU 又不会自己付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那回到问题：你们的商业模式是什么？几年后主要收入从哪来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得变化会非常快，这话听起来像废话，但就是事实。我一直把 AI 叫“超音速海啸”，我喜欢迭代。真正会发生的事是：当人形机器人进入规模化阶段，机器人会比任何人类公司更高效地生产产品、提供服务。所以，“放大人类公司的生产力”只是短期玩法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来的公司是纯AI、纯机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你预期会出现“纯数字公司”？而不是像 SpaceX 这种慢慢变成“半 AI 公司”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会有数字公司，但我得说一些听起来有点“末日论”的判断。不是为了搞笑，只是我认为会发生：纯 AI、纯机器人驱动的公司，会全面碾压“还需要人参与闭环”的公司，而且会发生得非常快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以拿“computer”这个职业做类比：以前真的有人叫“计算员”，整栋大楼、几十层楼的人只负责做计算。现在呢？一台笔记本加一个 spreadsheet，就能替代整座大楼，而且算得更多、快得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那你再想，如果 spreadsheet 里只有一部分格子是电脑算的，另一部分让人来算，会怎样？只会更慢、更差。同样道理，未来“人还在流程环里”的公司，会比“全 AI 闭环”的公司弱很多。纯 AI、纯机器人公司会变成默认形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你能不能给点建议，美国要怎么才能像中国那样，用低成本、规模化造出“人形机器人军团”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人形机器人真正难的，其实就三件事：第一，真实世界智能；第二，一双真正好用的手；第三，规模化制造。我还没看到哪家的 demo 能做出“人类手那种自由度”的手。Optimus 会有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那手怎么做到？瓶颈是扭矩？电机？硬件到底卡在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们必须自己做全套定制执行器，从电机、齿轮、功率电子、控制、传感器，全都得从物理第一性原理设计，因为现在根本没有现成供应链能满足需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：操控层面，除了手以外还有什么特别难？还是说只要手搞定了就基本稳了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：从机电角度看，手的难度比其它全部加起来还难，人类手真的很夸张。但除了手，你还需要真实世界智能。我们为车训练的智能，其实非常适用于机器人：主要是“视觉输入”。车用的视觉更多，同时也会听警笛，会融合 GPS、IMU 等其他信号，但核心还是视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后是输出控制指令。大概就是，Tesla 每秒吃进海量视频数据，最后吐出极小的控制输出。把高维感知压缩成低维控制，这就是本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：从“惊艳 demo”到“真能落地”，往往要很多年。十年前就有很强的自动驾驶 demo，但直到现在 Robotaxi、Waymo 这些才真正规模化。那家庭机器人会不会更慢？毕竟我们连“高级手”的 demo 都还没见到特别成熟的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们做人形机器人已经有一段时间了，而且车上做过的很多东西可以复用到机器人上：机器人会用同样的 Tesla AI 芯片，同样的基本原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实，机器人自由度比车多得多，但你把问题抽象成“信息流”的话，AI 本质上就是对输入流做压缩与相关，把它映射到控制输出。你必须学会忽略不重要的细节，保留关键细节，比如路边树叶纹理不重要，但路牌、红绿灯、行人很重要，甚至“对方车辆有没有注意到你”这种微妙线索也可能重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;车是“视觉输入 → 多级压缩 → 控制输出”，机器人也是一样。人类其实也是“光子进来，动作出去”，你的一生大部分时间就是视觉输入和运动输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但车和人形机器人差别很大，车的执行器就那么几个维度，转向、加速、刹车；机器人光手臂、手指就几十个自由度。而且 Tesla 在车上还有巨大优势，就是车在路上跑，天然收集了海量人类驾驶数据。机器人没法像车那样“先扔出去跑着收数据”，因为你不可能大规模部署一堆还不好用的 Optimus。自由度更高、数据更稀缺，这会怎么解决？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你指出了一个关键差异，就是车的训练飞轮很难复制到机器人上。我们确实会有千万量级的车在路上，这种数据规模机器人短期做不到。所以，我们要做的是造很多机器人，把它们放进一个类似 “Optimus Academy” 的环境，让它们在真实世界里做 self-play。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们会至少有上万台 Optimus，至少两三万台做自我探索、做不同任务的测试。同时我们也有很强的仿真系统，车上用过的物理精度仿真，会同样用于机器人。你可以让现实世界里几万台机器人干活，再在仿真里跑几百万台。用真实世界机器人来“闭合仿真与现实的差距”，把 sim-to-real gap 缩到足够小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 和 Optimus 的协同呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，你可以让 Grok 来编排 Optimus 的行为。比如你要建一座工厂，Grok 可以调度一群 Optimus，给它们分配任务，让它们把工厂搭起来，生产你想要的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那是不是意味着 xAI 和 Tesla 最终得合并？因为协同太深了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们刚才不是还在说“别聊公司结构”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那我换个问法，你还在等什么信号，才会下决心说“我们要造十万台 Optimus”？是硬件还要再成熟一点还是软件还要更强？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经在往量产推进了。量产爬坡非常难，但大方向是这样。我认为 Optimus 3 是适合推到“年产百万台”量级的版本；如果你要冲到“年产千万台”，可能需要 Optimus 4。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;制造业的产出爬坡基本都遵循 S 曲线：一开始慢得让人抓狂，然后进入指数上升，再进入线性，最后趋于平稳。但 Optimus 会是一条被拉长的 S 曲线，因为它太多东西是全新的，因此没有现成供应链。执行器、电子系统，几乎一切都是从第一性原理开始定制设计，不是从现成的里选。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：“定制”的水到底到多深？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们还没到“连电容都自研”的程度，至少现在还没有。但几乎没有什么东西能直接从目录里买来就完事，所以前期爬坡会更慢，但最终会到百万台量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：中国那些人形机器人价格能压到几千美元、上万美元。你们是希望把 Optimus 的 BOM 压到那个水平，正面打价格战？还是你觉得它们本质上不是同一个产品，所以才卖得那么低？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Optimus 的定位是足够高的智能、接近人类甚至超过人类的机电灵巧度，很多便宜的小机器人没有这个能力。而且 Optimus 体型也更大，是要长期搬重物、不发热不过载、在执行器功率范围内稳定工作的。它很高、很强、智能也高，所以必然比“小型、低智能”的机器人贵，但也不会贵很多。关键是，随着 Optimus 机器人开始“造 Optimus 机器人”，成本会非常快地往下掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那最开始这一百万台 Optimus 会去做什么？最“值钱”的使用场景是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一开始肯定先做那些你能确保它稳定做好的简单任务。而且早期最划算的方向，是所有需要持续运行的工作，也就是全天候的任务，因为机器人可以连续工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果放在 Giga Factory，Gen 3 大概能替代现在多少人类在做的工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定，可能一到两成，也许更多。但我们不会因此裁人。反过来，工厂的人数可能还会增加，只是总产出会涨得更快。换句话说，Tesla 的员工总数会增加，但机器人和汽车的产量增长会更夸张。最终效果是每个“人类”对应的汽车和机器人产出会大幅上升；同时人类员工数量也会上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中外有工作投入差距&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你觉得还应该加码更多出口限制吗？比如无人机产业这类？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你得先承认一个现实，在大多数制造领域，中国都非常先进，真正落后的只是极少数环节。中国的制造能力是“下一层级”的强，很多人低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿矿石冶炼和精炼来说，我粗略估计，中国的精炼能力大概是世界其他地区加起来的两倍。还有一些关键材料，比如镓的精炼，听说全球绝大部分产能都在中国。所以整体上，中国在制造业的大多数环节都非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果“谁拥有更多熟练制造劳动力”决定了谁能更快造出人形机器人，中国先把规模做起来，就会先进入你说的“自我扩张”未来，然后一路滚雪球。你之前还说“做到年产百万台 Optimus”需要强制造能力，但那恰恰又是 Optimus 未来要帮你补齐的能力，这听起来像个悖论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：递归闭环可以很快跑起来。先让少量机器人帮着造机器人，递归闭环就能闭合，然后你就能冲到年产数千万台。如果某个国家能做到年产上亿台，那它会成为压倒性的最强竞争者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不可能靠纯人力赢，而且美国跑得太久了，就像职业体育强队打久了会松懈、会产生“理所当然”的心态，最后就不再赢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我直观感受是，中国平均工作投入度比美国更高。所以不是只有人口差距，还有工作投入差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有没有一些东西是你过去很想做，但因为太费人、太贵，所以没做成。现在有了 Optimus，你觉得终于能回头把它做起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有。比如我们想在 Tesla 做更多精炼厂。我们在 Texas 的 Corpus Christi 刚建完锂精炼厂并开始投产；在 Austin 这边有镍精炼厂，主要做电池正极材料相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些项目在中国之外已经算是非常大的精炼能力了，甚至可以说在美国几乎“独一份”。但还可以做得更多：更多精炼厂能提升美国的精炼竞争力。而这类工作很多美国人并不想做，不是因为它“脏”，其实我们的精炼流程没有那种夸张的有毒排放问题，但现实是人就是不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：为什么不能用人做？只是没人愿意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不是“没人愿意”那么简单，是你根本凑不出足够的人。你让这些人去做精炼，他们就没法去做别的。所以怎么建出足够的精炼产能？你得靠 Optima。美国很少有人“向往”去做精炼这种长期密集的制造工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：比亚迪的产量或销量正在追上 Tesla。你觉得中国 EV 制造规模继续扩大后，全球市场会怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：中国在制造上极其有竞争力，所以我认为会出现一波巨大的中国车“洪水”，不只是汽车，还有大量其他制成品。我前面说过，基础层才是关键：能源、采矿、精炼。中国在这些基础层的规模大概是世界其他地区加起来的两倍。所以很多产品不可避免带有中国供应链的成分，然后他们会一路做到成品车。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国就是制造强国，我甚至认为，中国的发电量会远超美国。电力是实体经济的一个不错代理指标：你要跑工厂、跑产业链，就离不开电。如果中国的发电量达到美国的三倍，那它的工业产能粗略看也会是美国的三倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你还要把 AI 规模推到太空，你需要太空能力、需要人形机器人、需要真实世界 AI，你需要做到每年百万吨级别入轨运力。如果再进一步，把月球上的 mass driver 搞起来，那我觉得就算赢了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;破除大厂迷信，被挖人防不了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我们之前聊了很多你“怎么管人”的体系：你早期亲自面了 SpaceX 最开始那几千个员工。你当年在面试里到底在抓什么，是别人没法替代的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我本人不可能那样。一天就那么多小时，这从逻辑上就不成立。不过你问我当时在看什么，我觉得我在“评估技术人才”这件事上，“训练数据”比大多数人多得多，尤其是技术岗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我做过太多技术面试，也见过面试后真实的结果，所以我的“训练集”很大、覆盖面也很宽。我一般要的不是简历有多漂亮，而是“异常能力的证据”，最好用 bullet points 列出来：你做过哪些明显超出常人的事。这些证据不一定非得和岗位领域完全一致，离谱一点也行。只要对方能说出一两件让你听完觉得“这人确实不一般”的事——如果能说出三件，那就是非常强的信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但为什么一定要你来判？难道不能交给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：当然不可能都我来。我们所有公司加起来二十万人，我怎么可能亲自判？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那早期你当时为什么觉得必须亲自上？你在那些面试里抓的是什么而不能委派？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我得先建立自己的“训练集”。如果我只面几百人，我肯定会犯更多错。面得越多，我就越能回看：我以为某个人会做得很好，结果没做成，为什么？到底是哪种信号误导了我？我相当于在“对自己做 RL”：不断纠错，提高命中率。我的命中率不是百分百，但确实很高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那人选“没成”的原因里，有什么是你觉得意外的？是你曾经很看好最后却翻车的那种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我给自己的原则是，别太信简历，要信你和他面对面交流的感受。简历可能很华丽，但如果聊了二十分钟，你发现对话质量不行，那就相信对话，不要相信纸面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：外界以前有个梗，说 Tesla 高管像“旋转门”一样来来去去。但实际上，Tesla 这些年高管队伍挺稳定的，而且很多是内部成长起来的。SpaceX 也有很多长期跟着你的人，比如 Mark Juncosa、Steve Davis。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：还有 Gwynne Shotwell（你刚才说漏了她）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，感觉你能长期跑起来，一个重要原因就是你身边有一批很强的技术副手。这些人到底有什么共同点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Tesla 的核心团队现在平均在岗时间大概十来年，这很长。但也得承认，公司在不同阶段需要的人不一样：管五十人的团队、五百人、五千人、五万人，能力结构不可能完全同一拨人通吃。公司增长越快，管理岗位的变化也会越快，这是正相关的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有个额外挑战：当 Tesla 处在很成功的阶段，我们会被同行疯狂挖人。比如 Apple 当年做电动车项目的时候，简直是“地毯式轰炸”Tesla，招募电话打到工程师直接拔电话线的程度。他们甚至可以不面试，直接开出接近翻倍的薪酬把人挖走。那时候就出现一种“Tesla pixie dust”的迷信：好像只要挖一个 Tesla 高管过去，你家项目就会立刻起飞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也曾经被这种迷信影响过，觉得从 Google、Apple 挖来的人，马上就会神奇成功，但现实不是这样。人就是人，不存在什么魔法加成。再加上 Tesla 主要工程团队在加州，很多人跳槽都不用搬家，通勤差不多，成本很低，所以被挖得更凶。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你怎么防？怎么避免这种“大家都来挖你的人”的局面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得基本防不了。你同时在 Silicon Valley，又叠加“pixie dust”效应，别人就会非常积极地挖人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：搬到 Austin 会好一点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会好一点，但 Tesla 的工程师多数还是在加州，“让工程师搬家”这件事仍然很难，很多人还有家庭、配偶工作之类的牵制。Starbase 更难，因为你去了 Brownsville、Texas，能找到一个“不在 SpaceX”的同类型工作几乎不可能。那地方有点像“技术修道院”，很偏、基本都是男的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到本质，这些在 Tesla、SpaceX 技术上非常能打的人，除了技术很强之外，你觉得他们还有什么共同点？是组织能力？是能跟你配合？是足够灵活但又不漂？什么才算你的“好对手”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不需要什么“对手”。很简单，能把事情做成的人，我就喜欢；做不成，我就不喜欢。我也尽量不让“适配我的个人偏好”变成招聘标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更通用的标准是：才华、冲劲、可信赖，而且“善良”也很重要，我会给这一条一定权重：他是不是个好人？是不是值得信任？聪明、有能力、肯拼、可信，这些底层特质是改不了的，领域知识可以后补，但这些本质属性补不了。所以你会发现，Tesla 和 SpaceX 很多人一开始并不是来自汽车行业或航天行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你的管理风格在公司从小到大扩张过程中，变化最大是什么？你一直以“微观管理、钻细节”出名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一天的时间是固定的，公司越大、事情越多，我的时间就必然被稀释。所以，我不可能“持续微观管理”，那意味着我每天得有几千小时，这在逻辑上就不可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有些时候我会把自己“下钻”到一个具体问题里，因为它是公司进展的瓶颈。我往下“钻”不是为了显摆、也不是随便挑小事，而是因为它决定了胜负。如果我把时间花在无关紧要的小事上，公司必然失败，但也确实存在一些“很小但决定生死”的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：比如你当年把 Starship 的方案从复合材料改成钢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，最开始我们计划用复合材料，因为大家觉得碳纤维轻。问题在于，碳纤维即便规模化生产，材料成本仍然很高，尤其是那种能承受低温液氧环境、强度又很高的特种碳纤维，成本大概是钢的几十倍。室温条件下，像 F1 这种结构件，碳纤维确实很有优势，但我们要造的是一枚巨型火箭，用碳纤维推进得非常慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更麻烦的是工艺，高强度碳纤维需要Autoclave，本质是高压烘箱。你要做九米甚至更大直径的壳体，就得建一个史无前例巨大、极难制造的Autoclave；如果用常温固化，时间又太长、问题又多。总之，我们进展慢到受不了，所以我当时的判断是：必须换路子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么一定要你来拍板？团队里那么多工程师，为什么他们没自己得到这个结论？这关系到你在公司里真正的“比较优势”是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：因为我们在碳纤维上卡得太严重，只能换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Falcon 9 的主结构用的是 aluminum-lithium，这其实是很好的策略，某些性能上不比碳纤维差。但 aluminum-lithium 很难加工，要焊它通常要用 friction stir welding，一种让金属不进入液相、用搅拌摩擦把它“揉”在一起的焊接方式。这种工艺对规模化非常不友好，更糟的是你想后期改结构、加东西，很多时候你没法直接焊上去，只能靠机械连接再加密封。我不想让 Starship 的主结构走这条路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候我想到了钢。因为历史上 Atlas 火箭就用过“steel balloon tank”，所以不是没用过钢。更关键的是，你不能只看室温性能，要看低温下的材料属性，某些应变强化过的不锈钢，强度重量比其实可以接近碳纤维。Starship 的燃料和氧化剂都是低温的，液态甲烷、液态氧，结构长期处于低温环境。所以主结构基本是“低温工况”。在这个情况下，不锈钢的强度重量比并不吃亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它的优势太大了，原材料便宜得多，加工方便；你在户外就能焊，甚至开玩笑说“抽着雪茄也能焊”；结构改动、外挂部件非常容易。如果你要加东西，直接焊上去就行。再算上耐热性上，钢的熔点比铝高很多，大约是铝的两倍。Starship 再入时像“燃烧的流星”，耐温能力决定了隔热系统的重量。钢能让隔热层显著减重，迎风面热防护可以大幅减薄，背风面甚至几乎不需要那么多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果整体算下来，钢反而可能比碳纤维版本更轻。因为碳纤维里的树脂在高温下会软化、甚至融化；碳纤维和铝的耐温等级其实接近，而钢耐温空间大得多。这些都是非常粗的数量级解释，但逻辑大概是这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我去过 Starbase，我注意到一个现象，大家特别以“简单”为荣，总有人跟我说 Starship 就是个“大铁罐”，我们在招焊工，你只要会焊，来这儿就能焊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我知道。但 Starship 其实是个非常复杂的火箭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想表达的可能是：你不需要“在火箭行业干过”才能来做 Starship。只要人聪明、肯干、可靠，就能参与造火箭，不需要既往航天履历。但机器本身，Starship 是人类造过最复杂的机器，没有之一，差得非常远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：具体复杂在哪些方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：几乎所有方面。我能想到的任何项目，都比这容易。也正因为这样，历史上从来没有人做出“完全可重复使用”的轨道级火箭。没人成功过，很多非常聪明的人、带着巨量资源都失败了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在也还没彻底成功。Falcon 只能算部分可复用，上面级还不行。但 Starship 的 V3 设计，我认为是能做到全复用的，而全复用才是让我们成为多行星文明的关键。说实话，哪怕一个普通的液压阀门之类的小问题，都比把 Starship 彻底做成要容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在 Starship 的瓶颈是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：先让它别炸。真的，就这么朴素。那种大推力燃烧发动机，天生就“很想爆炸”。我们已经有两次 booster 在测试台上炸了，其中一次把整个测试设施都炸没了。一个小错误，就能造成巨大的损失。Starship 里装的能量太吓人了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：所以它比 Falcon 难，是因为能量更大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一部分是能量更大，更重要的是大量新技术，性能边界推得太极限。Raptor 是非常非常先进的发动机，毫无疑问是史上最强的火箭发动机，但它也“非常想炸”。我给你个直观对比：起飞那一瞬间，整枚火箭输出的功率超过一百吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：离谱。这个类比太震撼了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，而且还得“别炸”。有时候能做到，有时候做不到。爆炸的方式有上千种，不爆的方式只有一种。我们的目标其实不是“永远不炸”，而是做到“可靠飞行”，最好能形成很高的发射节奏，比如一天多次、甚至一小时一次。但如果经常炸，就很难维持高频节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你问“最大的单点难题是什么”，我觉得是把 heat shield 做到真正可复用。到目前为止，从来没有人做出“可复用的轨道级热防护系统”。它要在上升段扛住冲击，不掉一堆 tiles；再入时也不能掉一堆 tiles，不能把主结构烤坏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我挺好奇你怎么把那种“紧迫感、冲刺感”在组织里推起来。我看过一些你的传记，总觉得你特别能把“必须现在就干、必须把这件事做成”灌进团队。SpaceX 和 Tesla 现在都很大了，但你还能维持这种文化。别的公司为什么做不到？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我也说不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你今天不是开了一堆 SpaceX 会议吗？你到底在会议里做什么，能把这股劲维持住？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：紧迫感来自领导者。我的紧迫感非常强，强到有点“疯”，这股劲会传导到整个公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那是因为“后果”吗？比如 Elon 定了疯狂 deadline，如果我做不到就会出事？还是因为你能迅速识别瓶颈、清掉障碍，让大家跑起来？你怎么理解你们为什么能跑这么快？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一直在处理 limiting factor（限制因素）。至于 deadline，我通常会设一个我认为“有五成概率能做到”的目标。它不是不可能，但一定是我能想到的最激进版本，这就意味着它一半时间会延期，但没关系。排期这事也像“气体膨胀定律”，你给多少时间，事情就会膨胀到占满多少时间，你说“五年做完”，那它就会花五年。对我来说，五年几乎等于无限长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也有物理极限，比如制造业扩产的速度受限于“搬运原子”的速度。你不可能今天拍板，明天就年产百万，你得设计产线、爬 S 曲线。但总体来说：强烈的紧迫感很关键；再配合一个激进但仍有机会的计划，然后不断找出当下的瓶颈，帮团队把瓶颈打穿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：Starlink 其实酝酿了很多年。你们一开始在 Redmond 也有团队，但后来你认为这个团队不行。问题是它“慢”不是一天两天，你为什么不更早动手？你又为什么在那个时点动手？怎么判断“现在就是必须出手的时刻”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我每周都会做非常细的 engineering reviews（工程审查），细到一种很不常见的颗粒度。我几乎没见过有制造业公司 CEO 能下钻到我这个程度。所以我对真实进展其实掌握得很清楚，因为我们会把问题摊开讲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也很信跨级会议，不是只听直接汇报给我的人说，而是让他们下面一层、再下面一层的人都在技术评审里直接说，而且不让“提前排练”，不然你听到的就是一堆被打磨过的漂亮话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你怎么防止他们提前准备？随机点名？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不用，就绕着会议室一圈走，每个人都更新。信息量确实很大，但你每周甚至一周两次这么开，你就会有“这个人上次说了什么”的快照。你可以在脑子里把这些点画成曲线：我们到底是在逼近解，还是在原地打转？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一般只在我确认“如果不采取极端措施，就没有任何成功可能”的时候，才会下狠手。当我得出这个结论，就必须做果断出手了。当年就是这样判断的，然后出手，把问题扳过来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你公司这么多，每一家都做这种深度工程理解、瓶颈识别、技术评审。你怎么把这套扩到五六七家公司？甚至一家公司里又像套娃一样有很多“子公司”。这里的上限是什么？你能不能管到八十家公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：看情况。有些公司我不会定期开会，因为它们在“顺畅巡航”。如果一个东西进展很好，那我把时间花在那儿没有意义。我分配时间完全按问题来：哪里卡、哪里慢、哪里是瓶颈，我就去哪里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以现实是，事情干得顺的时候，他们很少见到我；事情卡住的时候，他们会经常见到我。也不一定是“干得很差”，更准确就是：它是 limiting factor。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那如果某个东西在 SpaceX 或 Tesla 成了瓶颈，你会怎么介入？是每天/每周跟负责的工程师聊吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：多数瓶颈是每周一次，有些是每周两次。比如 AI5 芯片评审就是每周两次，固定在周二和周六。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这种会议是开放式的？想开多久就开多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：理论上是。但一般就是两三个小时，有时更短。看阶段，看要过多少问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：很多公司 CEO 不做工程评审；而且会议被切得很碎，半小时、十五分钟一场。你这边更像“讨论到搞清楚为止”的长会，这个差异挺明显的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有时候会这样，但多数还是尽量按时结束。比如今天的 Starship 工程评审就开得久一点，因为话题更多，我们在讨论怎么把入轨运力扩到“每年百万吨级别以上”，这个很难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回头看你那段“下场搞政治”的经历，你怎么评价？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得这些事是必须做的，目的就是尽可能提高“未来是好的”的概率。但政治本身通常很琐碎，而且会让人失去客观性，人们很难看到对方阵营的优点，也很难承认自己阵营的问题。很多时候你根本没法跟人讲道理，一旦站队了，他们就会坚信“自己这边永远是对的，对面永远是错的”，几乎无法说服。但总体上，我认为那些行动，包括收购 Twitter，尽管会让很多人愤怒，还是对文明有益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BYXbuik3dgA&quot;&gt;https://www.youtube.com/watch?v=BYXbuik3dgA&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</link><guid isPermaLink="false">https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</guid><pubDate>Mon, 09 Feb 2026 02:30:39 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item></channel></rss>