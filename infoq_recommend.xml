<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Sat, 14 Feb 2026 00:05:35 GMT</lastBuildDate><ttl>5</ttl><item><title>优步将内部搜索索引迁移至OpenSearch的拉取式摄入架构</title><description>&lt;p&gt;优步&lt;a href=&quot;https://www.uber.com/en-AU/blog/how-uber-indexes-streaming-data-with-pull-based-ingestion-in-opensearch/&quot;&gt;将其内部搜索索引系统迁移到了OpenSearch&lt;/a&gt;&quot;，引入了面向大规模流式数据的拉取式（pull‑based）数据摄入框架。此次改造的目标是提升实时索引工作负载的可靠性、回压（backpressure）处理能力与故障恢复能力。此前，不断演进的产品需求暴露出很多问题，例如，维护自研搜索平台的成本与复杂度持续攀升，还面临着模式演进、相关性调优以及多区域一致性等方面的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步的搜索基础设施支撑行程发现、配送选择和基于位置的查询，以近实时方式处理持续的事件流。他们&lt;a href=&quot;https://www.uber.com/en-EG/blog/evolution-of-ubers-search-platform/&quot;&gt;自研的搜索平台&lt;/a&gt;&quot;原本基于推送式（push‑based）数据摄入，也就是上游服务直接向集群写入数据。这种方式在小规模场景下效果不错，但在流量突增与故障场景下表现乏力，会导致写入丢失、重试逻辑复杂等问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拉取式数据摄入将责任转移到&lt;a href=&quot;https://opensearch.org/&quot;&gt;OpenSearch集群&lt;/a&gt;&quot;本身。分片会从&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;&quot;、&lt;a href=&quot;https://aws.amazon.com/pm/kinesis&quot;&gt;Kinesis&lt;/a&gt;&quot;等持久化流中主动拉取数据，这些消息队列充当缓冲层，实现可控速率、内置回压与可重放恢复。优步的工程师表示，该方案在流量峰值期间显著减少了索引失败，并简化了运维恢复流程。此前会压垮分片队列的突发流量，现在可被&lt;a href=&quot;https://www.cs.cornell.edu/courses/cs4410/2010su/queue.pdf&quot;&gt;每个分片的有界队列&lt;/a&gt;&quot;平滑吸收，提升了吞吐量与稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6f/6f9577be91dbbcf59837fb3f239abfe1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;在流量突增期间，推送式和拉取式摄入表现对比（图片来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/how-uber-indexes-streaming-data-with-pull-based-ingestion-in-opensearch/&quot;&gt;优步的技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拉取式管道由多个交互组件构成。事件会被写入Kafka或Kinesis主题，每个分片映射到一个流分区以支持确定性重放；流消费者将消息拉取到阻塞队列，实现消费与处理解耦，并支持并行写入；消息由独立线程完成校验、转换与索引请求准备，再交给摄入引擎；引擎直接写入Lucene，绕过事务日志（translog），同时跟踪已处理的偏移量以支持确定性恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b91b82f5d64d1c8ab2ad488b1781142b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;拉取式摄入的流式索引架构（图片来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/how-uber-indexes-streaming-data-with-pull-based-ingestion-in-opensearch/&quot;&gt;优步的技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据优步工程师介绍，拉取式摄入还提供细粒度的运维控制能力。外部的版本机制确保乱序消息不会覆盖更新数据，至少一次投递能够保证一致性；运维人员可配置失败策略：丢弃策略下消息会直接丢弃，阻塞策略下则会无限重试；通过API可暂停、恢复或重置到指定偏移量，帮助团队在故障后快速处理积压。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步支持两种摄入模式。段复制模式仅在主分片上摄入数据，副本从主分片拉取完整段，这可以降低CPU开销，但存在轻微可见性延迟。全活模式会在所有分片副本上同时摄入，实现近乎实时可见，但计算成本更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;拉取式摄入是优步高可用、多区域搜索架构的核心。每个区域的OpenSearch集群从全局聚合的Kafka主题消费数据，构建完整、最新的索引。该设计保证了冗余性、全局一致性与无缝故障转移，让全球用户都能获得一致的搜索视图，同时保持高可用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba500dd04f39127230279cf53f15af94.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;拉取式的索引模型（图片来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/how-uber-indexes-streaming-data-with-pull-based-ingestion-in-opensearch/&quot;&gt;优步的技术博客&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步正逐步将所有搜索场景迁移到OpenSearch的拉取式摄入架构，向&lt;a href=&quot;https://github.com/opensearch-project/OpenSearch/issues/17957&quot;&gt;云原生&lt;/a&gt;&quot;、可扩展架构演进，并持续优化平台和回馈OpenSearch社区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/uber-pull-based-opensearch/&quot;&gt;Uber Moves In-House Search Indexing to Pull-Based Ingestion in OpenSearch&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ettgwgXcs5b5QTuIG7gi</link><guid isPermaLink="false">https://www.infoq.cn/article/ettgwgXcs5b5QTuIG7gi</guid><pubDate>Sat, 14 Feb 2026 00:00:00 GMT</pubDate><author>作者：Leela Kumili</author><category>架构</category></item><item><title>Cloudflare推出Moltworker，将自托管AI智能体带到边缘环境中</title><description>&lt;p&gt;Cloudflare推出了开源的&lt;a href=&quot;https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/&quot;&gt;Moltworker&lt;/a&gt;&quot;，支持在Cloudflare开发者平台上运行Moltbot（一款可自托管的个人AI智能体），从而不再需要专用的本地硬件。Moltbot最初名为Clawdbot，设计定位是通过聊天应用提供个人助手服务，可集成AI模型、浏览器与第三方工具，且全程由用户自主掌控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltworker将Moltbot适配到了Cloudflare Workers中，在架构上组合了入口Worker与隔离的沙箱（Sandbox）容器。Worker充当API路由与管理层，而Moltbot运行时及其各类集成则在沙箱内执行。包括对话记忆与会话数据在内的持久化状态会存储在Cloudflare R2中，解决了容器本身无状态、生命周期短的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该实现充分利用了Cloudflare Workers近期在Node.js兼容性上的增强。Cloudflare表示，对Node API更完善的原生支持减少了对变通方案的需求，让更多npm包可以不经修改直接运行。尽管Moltbot当前主要在容器中运行，但Cloudflare认为，这种更强的兼容性未来将让更多智能体逻辑可以更贴近边缘节点执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Moltworker集成了多项Cloudflare服务，复刻并扩展了本地版Moltbot的体验。AI请求会经由Cloudflare AI Gateway进行路由，支持多模型厂商、集中式可观测性与多种配置选项；浏览器自动化任务通过Cloudflare Browser Rendering来处理，使Moltbot可以控制无头Chromium实例完成页面导航、表单填写与内容抓取，而无需在容器内直接运行浏览器；API与管理后台的身份认证则通过Cloudflare Zero Trust Access实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c6/c6bc892e6ac5b2779fd47fa818eadd7b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该项目在早期用户中引发了褒贬不一的反响。有人认为，基于Cloudflare托管的方案显著降低了使用门槛。Peter Choi在&lt;a href=&quot;https://x.com/pitachoi/status/2016959987319545936?s=20&quot;&gt;评论&lt;/a&gt;&quot;&amp;nbsp;中指出，在Cloudflare上运行Moltbot有望大幅提升普及度，但同时质疑这种迁移是否会改变项目最初强调完全本地控制的核心吸引力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一些用户则强调其运维优势，有的用户这样&lt;a href=&quot;https://x.com/Voxyz_AI/status/2016894763929055507?s=20&quot;&gt;说到&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我一直在VPS上实现自托管，虽然能用，但管理服务器本身很麻烦。这个方案看起来是“部署后无需维护”的版本，很好奇状态在多次Worker调用之间是如何持久化的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare已经在GitHub上开源了Moltworker，并将其定位为概念验证（proof of concept），而非正式支持的产品。官方表示，该项目旨在展示其开发者平台（结合了Workers、Sandboxes、AI Gateway、Browser Rendering与存储服务）如何在边缘环境安全地、规模化地运行AI智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/cloudflare-moltworker/&quot;&gt;Cloudflare Demonstrates Moltworker, Bringing Self-Hosted AI Agents to the Edge&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PRrDv1gQQ2JhuOp1xI9b</link><guid isPermaLink="false">https://www.infoq.cn/article/PRrDv1gQQ2JhuOp1xI9b</guid><pubDate>Fri, 13 Feb 2026 10:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>云计算</category></item><item><title>一文读懂 Snowflake：7 个 AI 与语义层的关键策略要点 ｜ 技术实践</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层并非新兴概念。早在二十一世纪初我便开始使用该技术，并在多项分析解决方案（尤其是企业数据仓库）中主导其设计、配置与实施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我曾使用多种语义层工具，包括 IBM Cognos Framework Manager 及微软数据平台的各类语义层功能。&lt;/p&gt;&lt;p&gt;事实上，几乎所有“商业智能(BI)+自助服务”场景都涉及语义层的应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近年来，由于智能体 AI 发展的需求，语义层正迎来复兴浪潮。因此，本文将围绕 Snowflake 平台语境，提出 7 项利用语义层实现 AI 就绪的战略建议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;注：本文全篇手动撰写，内容基于我的行业经验与专业知识、相关研究，以及对客户需求与行业趋势的观察。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/037ec4ca778ecd680edb27942531645d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;利用语义层提升 Snowflake 平台上 Cortex AI Analyst 的 AI 提示准确性。如图所示，本例中的语义层集成位于顶部：VWS_CUSTOMER_DETAILS。图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点一：理解语义层的传统作用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先需要明确基本概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据《牛津英语词典》的定义：“语义学是语言学和逻辑学中研究意义的分支。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，维基百科将语义层定义为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“……将复杂数据映射为熟悉的业务术语，例如产品、客户或收入。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层能够为数据提供易于业务理解的、一致的解释视角，即一种抽象的“逻辑”视图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0b95a717332fb4a915145622f7f3134.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;数据抽象逻辑视图示例。图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图中展示了一个抽象逻辑视图的示例，该视图在清晰描述数据的同时保持了更贴近业务用户需求的设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层可用于定义和配置以下内容：&lt;/p&gt;&lt;p&gt;标准化定义与含义数据对象间的关系指标&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层过去乃至现在仍广泛应用于商业智能场景。这对数据治理具有重要价值，有助于提升业务逻辑的一致性、可复用性及可维护性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点二：了解人工智能为何推动语义层“复兴”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过语义层提升 Snowflake 平台上 Cortex AI Analyst 的 AI 提示词准确性。图中顶部的 VWS_CUSTOMER_DETAILS 即为此类语义层集成示例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点：语义层有助于提升Agentic AI文本转SQL的准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，当我们用AI增强商业智能分析时，若采用自然语言+智能体与数据进行交互，则必须将自然语言转换为底层数据仓库的查询语言（通常为SQL）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层可将大型语言模型处理（常称为“推理”）与基于规则的定义相结合。规则定义承载了业务理解与业务逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义层有助于管控不准确性风险（亦称“幻觉”），从而提升可信人工智能的可靠性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点三：数据建模是这一领域的关键技能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如前所述，语义层是数据的“逻辑”或抽象视图。因此，在使用语义层时，我们无需深入了解数据存储库的内部工作机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在设计语义层时，必须有人完成以下任务：&lt;/p&gt;&lt;p&gt;确定使用哪些源数据库表及其他对象；梳理这些对象之间的关联关系；对语义层对象进行命名，确保其符合业务通俗性；制定相应指标逻辑；确认业务定义；简而言之，有效建立从物理数据库到语义层的映射关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f5/f5da4291ab20727d2850d0af70653a8e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;物理数据模型示意图。图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实现上述目标的前提是理解数据。从实施层面来看，数据建模角色最适合承担此项工作，同时需要结合业务用户等领域专家的意见进行完善。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点四：AI 增强型语义层设计与人机协同机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/67/673fd31002c81eaf81a07bdd707aa84b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Human in the Loop：评估来自 Snowflake AI Cortex Analyst的AI生成语义层建议。 图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管人工智能能够提出建议并增强语义层设计，但从质量保证的角度来看，用户验证至关重要。若我们采用基于 AI 的语义层配置建议，则必须建立相应的“人机协同”审批流程，例如涉及：&lt;/p&gt;&lt;p&gt;确定概念同义词；业务友好的描述定义；正确的提示词与查询组合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Snowflake 云语义视图为例，其 Cortex AI 服务能够为语义层设计提供有益建议。这些建议可能包括描述、指标及查询语句。然而，必须由具备相应领域知识的系统用户对这些建议进行评估。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f7/f7d2310897308e3aa91dd268ff50d94c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Snowflake Cortex Analyst语义视图已验证查询。 图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个典型产出是已验证查询。在此场景中，系统会监测自然语言提示词及其对应查询，以识别潜在可复用的模式。一旦发现候选查询，系统将自动向用户推送建议。用户可选择接受、编辑或拒绝该条目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点五：技术栈兼容性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如同技术生态系统的任何组成部分，语义层的兼容性始终是需要考量的关键因素。因此，务必确保您的技术栈能够满足实际需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Snowflake 平台为例，其功能支持包括：&lt;/p&gt;&lt;p&gt;配置语义视图时提供多种接口与选项，例如 SQL、用户界面、REST API 及 YAML 格式，并集成了智能体AI功能；支持第三方商业智能工具，如 Sigma、Omni、Honeydew 和 Hex；兼容第三方设计与交付工具，例如采用 dbt 进行集成测试，且其路线图中已纳入 SqlDBM 的设计功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点六：语义层开放标准即将到来&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/47e33c727eac08e51c413fbbaef3edc1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;该标准源于开放语义交换规范（OSI）。图片来源：Snowflake&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开放标准不仅能降低供应商锁定风险，还有助于推动技术普及。当前，语义层开放标准——开放语义交换规范（OSI）正在制定中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该标准主要关注以下三个核心领域：&lt;/p&gt;&lt;p&gt;标准化：提供统一的定义语言；互操作性：促进数据交换；可扩展性：支持模型随数据需求演进而灵活适配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该项目已获得众多机构的共同推进，参与方涵盖从 Blackrock 到 AWS 再到 Salesforce 等各类组织。&lt;/p&gt;&lt;p&gt;从数据与人工智能战略视角来看，这一开放标准将带来显著价值，特别是在 AI/BI 工具及相关互操作性方面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;要点七：面向语义层与智能体人工智能的治理方法&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若无法将语义层开放给智能体人工智能（Agentic AI），则前述所有工作均无法实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再次以 Snowflake 平台为例，我们可以配置 AI 智能体，使其调用 Cortex Analyst 语义视图。这将为智能体提供我们在前文讨论的各个领域的明确指引，例如：&lt;/p&gt;&lt;p&gt;数据含义；指标定义；针对智能体提示的已验证预制查询语句。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义视图还可应用于 BI 工具、Notebook 环境或 AI/MCP 集成中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/87/874a7dcabd47308346d0af63594fd7bb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;利用 Snowflake Intelligence 功能，通过智能体人工智能+ Cortex Analyst 语义视图提交自然语言提示。图片来源：Dan Galavan&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，Snowflake Intelligence 功能可结合前述 AI 智能体与语义视图对数据进行查询。我们能够通过提交自然语言提示生成图表，并识别数据趋势。另一优势在于，Snowflake 的生成式人工智能能力与平台整体的数据治理功能紧密结合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我曾在生成式人工智能方兴未艾的 2023 年 Snowflake 全球峰会上就此话题发表过演讲。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;结论&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如我们所看到的，语义层在与商业智能（BI）工作负载相关的领域虽已存在多年，但当前正迎来一场智能体人工智能（Agentic AI）的复兴。这场复兴的驱动力，源于确保“与数据对话”时实现可信人工智能的迫切需求。换句话说，这正是我们在“以AI增强BI”的过程中所要应对的挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然而，在此过程中，务必牢记以下几点：&lt;/p&gt;&lt;p&gt;数据建模在此背景下为何扮演关键角色&lt;/p&gt;&lt;p&gt;AI增强如何助力语义层的设计Human in the Loop 的重要性即将发布的开放语义交换规范采用治理化方法运用智能体人工智能（本文以 Snowflake Intelligence 平台的语义视图为例进行说明）综合考虑以上要素，将有助于有效推进数据与人工智能战略的落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我还有许多相关的实践建议希望分享，并期待能尽快抽时间撰写成文。敬请持续关注！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此，预祝各位在语义层与智能体人工智能的探索之旅中一切顺利。如有任何疑问，欢迎随时&lt;a href=&quot;https://galavan.com/contact/&quot;&gt;联系&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://galavan.com/about/&quot;&gt;© Dan Galavan 2026&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dan Galavan 是一名独立的数据治理与数据管理顾问，拥有 27 年的客户咨询与数据解决方案交付领导经验。他自 2019 年起开始使用 Snowflake 数据平台，持有都柏林大学学院颁发的“商业高级人工智能”文凭，并已获得 Snowflake Snowpro 高级架构师认证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://medium.com/snowflake/snowflake-in-a-nutshell-7-strategic-ai-semantic-layer-tips-78422f7e0bdc&quot;&gt;https://medium.com/snowflake/snowflake-in-a-nutshell-7-strategic-ai-semantic-layer-tips-78422f7e0bdc&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/aveYPAdOFvSH1CdHe0Py</link><guid isPermaLink="false">https://www.infoq.cn/article/aveYPAdOFvSH1CdHe0Py</guid><pubDate>Fri, 13 Feb 2026 08:58:05 GMT</pubDate><author>Dan Galavan</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>昨夜，OpenAI 祭出首个实时编码模型，没用英伟达芯片！谷歌重磅更新 Deep Think，姚顺宇参与</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenAI 发布新模型，专为实时编码而生&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨晚，OpenAI 正式发布了 GPT-5.3-Codex-Spark 的研究预览版本。这是一款从 GPT-5.3-Codex 主模型中“裁剪”而来的精简版本，同时也是 OpenAI 首个专门围绕实时编码（real-time coding）场景设计的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/40/40e4db469feb0208f685bf341526fb35.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从定位上看，Codex-Spark 并不是为了替代现有的 Codex，而是补齐其在“即时交互”场景中的短板：在过去，Codex 更擅长长时间运行的复杂任务，而 Codex-Spark 的目标则非常明确——把人与模型之间的交互延迟压缩到接近“无感”的程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一发布同时也是 OpenAI 与 芯片初创企业 Cerebras 合作的重要阶段性成果。为了减少对英伟达芯片的依赖，上个月 OpenAI 签署了一项金额超过100亿美元的协议，使用Cerebras的硬件以提升其模型的响应速度，而 Codex-Spark 被视为这项合作落地的第一个技术里程碑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为实时而生：Codex-Spark 的核心是“速度”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在官方定义中，Codex-Spark 是一个“专为实时使用 Codex 而设计的模型”，它支持进行针对性编辑、重塑逻辑或优化界面，并能立即查看结果。这一表述背后，隐含的是对交互方式的重新假设。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在传统的 AI 编码流程中，开发者往往需要等待模型完成一次较完整的推理和生成，再基于结果进行下一轮调整。这种模式在复杂任务中是必要的，但在日常开发中——例如小范围代码修改、逻辑重构、界面样式调整——高延迟本身就成为效率瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex-Spark 针对的正是这一类高频、碎片化、对即时反馈极度敏感的使用场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 OpenAI 介绍，Codex-Spark在执行长时间运行的任务方面展现出卓越的优势，无需人工干预即可自主运行数小时、数天甚至数周。借助 Codex-Spark，Codex 现在既支持长时间运行的复杂任务，也支持即时完成工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex-Spark 在发布时拥有 128k 的上下文窗口，并且仅支持文本。在研究预览期间，Codex-Spark 将拥有独立的速率限制，其使用量不计入标准速率限制。但是，当需求量较高时，用户可能会遇到访问受限或临时排队的情况，因为需要平衡不同用户的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 还表示，Codex-Spark 针对交互式工作进行了优化，在这种工作环境中，延迟与智能同样重要。用户可以与模型实时协作，在模型运行过程中随时中断或重定向它，并快速迭代，获得近乎即时的响应。由于 Codex-Spark 注重速度，因此其默认工作方式非常轻量级：它只进行最少的、有针对性的编辑，并且除非用户主动要求，否则不会自动运行测试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提示词：制作一款贪食蛇游戏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;编码能力如何？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在评估层面，Codex-Spark 作为一个小型模型，仍然在多个软件工程基准测试中表现突出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex-Spark 特意针对快速推理进行了优化。在 SWE-Bench Pro 和 Terminal-Bench 2.0 这两个评估智能体软件工程能力的基准测试中，GPT-5.3-Codex-Spark 表现出色，且完成任务所需时间远低于 GPT-5.3-Codex。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cf/cf7b638e673dfe4a3eeb6fac03d0cbcd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持续时间估计为以下各项之和：（1）输出生成时间（输出token ÷ 采样速度），（2）预填充时间（预填充令牌÷预填充速度），（3）工具执行总时间，以及（4）网络总开销。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/67/675490fa54e46bc2d946ec330767ce7d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，这样的编程表现是如何实现的？在训练 Codex-Spark 的过程中，OpenAI意识到模型速度只是实现实时协作的一部分——还需要降低整个请求-响应流程的延迟。所以研发团队在框架中实现了端到端的延迟优化，这将使所有模型受益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Codex-Spark 的研发过程中，OpenAI 意识到一个关键问题：模型本身的速度只是实时体验的一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正影响用户感受的，是从客户端发出请求，到第一个可见 token 出现，再到持续生成的整个端到端路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，OpenAI 对 Codex 的底层架构进行了系统级优化，包括：简化客户端到服务器、以及服务器返回响应的流程、重写推理栈中的关键路径、改进会话初始化机制、引入持久化 WebSocket 连接以及对响应 API 进行针对性优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些改动带来的量化结果包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;客户端/服务器单次往返开销降低 80%每个 token 的处理开销降低 30%第一个 token 的出现时间缩短 50%&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex-Spark 默认启用 WebSocket 路径，而这一通信方式也将在未来逐步成为所有模型的默认配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这印证了 Codex-Spark 的核心定位：不是通过更复杂的推理链取胜，而是通过更快的反馈循环提升整体效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;开发者关注的不只是“更快”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 OpenAI 发布面向实时编码场景的 Codex-Spark 研究预览版后，在x上迅速展开讨论。与官方强调的“超低延迟”和“即时协作体验”相比，社区关注的焦点明显更加集中在一个问题上：在速度大幅提升的同时，模型是否还能维持足够的推理深度与代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从目前的讨论来看，围绕 Codex-Spark 的反馈并不单一，而是呈现出几种具有代表性的声音。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有x 用户表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“真正的问题不仅仅是速度。关键在于它能否在压力下保持质量。如果延迟降低而推理深度没有减少，这将改变日常工作流程。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0a/0a2229df4e158e0652a7cd695a57a761.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有用户指责OpenAI 过于关注编码性能，其他性能被忽视了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“你们把所有注意力都放在代码和那些影响用户体验的广告上，但这并非绝大多数日常用户真正关心的。你们无视 &lt;a href=&quot;https://x.com/hashtag/Keep4o?src=hashtag_click&quot;&gt;#Keep4o&lt;/a&gt;&quot; （保留4o模型）的声音，就像我们无视你们那些垃圾般的新产品一样。即便你们装作视而不见，我们也不会停止。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“速度更快”固然很好，但真正的问题是：它能否在速度的同时保持代码质量？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有用户指出，速度快但有缺陷的代码毫无用处。代码速度慢但正确才有用。期待看看 Spark 能否在这两方面都做到最好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6f/6fec5726110bfb3b94c243c81b12d7ee.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多位用户表达了类似的观点，认为只速度快有什么意义？它至少应该达到 GPT 5.3 编解码器的水平。“否则，你很快就会一无所获”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a0cdc5b424eeaa57bdfb7304e297f45.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谷歌更新 Gemini 3 Deep Think，能处理真实科研难题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 发新模型的同时，谷歌也没闲着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌昨晚同步更新了旗下最具研究取向的推理模型——Gemini 3 Deep Think。这次更新并非一次常规能力迭代，而是一次明确面向现代科学研究、工程建模与复杂推理问题的系统性升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得注意的是，去年 9 月加盟谷歌DeepMind 的清华物理系知名研究者姚顺宇（Shunyu Yao），同样是 Deep Think 新模型的核心参与者之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9f/9fb1b1bc47e3ae89abedc0886ac5acd4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从官方定位来看，Gemini 3 Deep Think 的目标并不是更流畅的对话体验，而是解决那些长期困扰科研人员和工程师的“硬问题”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些问题往往缺乏明确的解题路径，不存在唯一正确答案，数据本身也常常不完整、噪声较多，甚至彼此矛盾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，此次更新是在与大量科学家和研究人员的长期合作基础上完成的，模型的设计思路也明显偏向真实科研与工程实践，而不仅是抽象推理能力的展示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;全新 Deep Think 现已在 Gemini 应用中上线，供 Google AI Ultra 订阅用户使用。此外，我们首次通过 Gemini API 向部分研究人员、工程师和企业开放 Deep Think 的使用权限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Deep Think 访问地址：&lt;a href=&quot;https://forms.gle/eEF5natXTQimPhYH9&quot;&gt;https://forms.gle/eEF5natXTQimPhYH9&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是早期测试用户如何使用最新版 Deep Think 的演示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;罗格斯大学的数学家丽莎·卡博内致力于研究高能物理学界所需的数学结构，以弥合爱因斯坦引力理论和量子力学之间的鸿沟。由于该领域缺乏大量的训练数据，她利用Deep Think技术审阅了一篇高度专业的数学论文。Deep Think成功地识别出了一个细微的逻辑缺陷，而这个缺陷此前在人工同行评审中均未被发现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在杜克大学，王氏实验室利用Deep Think技术优化了复杂晶体生长的制备方法，以期发现新的半导体材料。DeepThink成功设计了一种能够生长厚度大于 100 微米薄膜的工艺，达到了以往方法难以企及的精确目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌平台与设备部门研发主管、前 Liftware 首席执行官 Anupam Pathak测试了新的 Deep Think，以加速物理组件的设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运用数学和算法的严谨性提升推理能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在以往的大模型评估体系中，推理能力往往通过标准化问题来衡量：问题定义清晰、目标明确、评价方式单一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而 Gemini 3 Deep Think 试图应对的，是另一类问题——研究型问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这类问题通常具备几个特征：&lt;/p&gt;&lt;p&gt;没有固定模板没有明确步骤数据来源复杂且不完备解题过程本身可能需要不断修正假设&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌在技术博客中强调，Deep Think 的更新重点，在于将深厚的科学知识与工程实践中的常识和方法论结合起来，让模型不再停留在理论层面，而是更贴近真实世界的研究流程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在推理能力的提升上，数学与算法仍然是 Gemini 3 Deep Think 的核心抓手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早在去年，谷歌就曾展示过专门定制的 Deep Think 版本，在多项高难度推理任务中取得突破，并在国际数学和编程类赛事中达到金牌水平。此次更新，在这一方向上继续向前推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据谷歌披露的数据，升级后的 Deep Think 在多项严苛学术基准测试中刷新了当前水平，包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Humanity’s Last Exam（“人类的最后考验”）中，在不借助任何外部工具的前提下，取得 48.4% 的成绩。这一基准被认为是专门用于测试前沿模型能力极限的高难度测试。在 ARC-AGI-2 测试中，Deep Think 取得 84.6% 的成绩，并已通过 ARC Prize Foundation 的官方验证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3ca3e7d5f8385b2ab125d99bd9fb33c8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在竞技编程平台 Codeforces 上，模型达到了 3455 Elo 的评分区间，这一水平在该平台上已属于极高段位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Gemini Deep Think&amp;nbsp;3455 的得分来看，其编码能力排名世界第八。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/43/4367a2e01d6a636faba01517b08e5e99.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 2025 年国际数学奥林匹克竞赛的评测中，整体表现达到了金牌水平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d57dbf5179e56cb52aebcf3cc6c7db7f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些结果表明，Deep Think 的提升并非集中在单一任务类型，而是在多种高约束推理环境下保持了稳定表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;不止于数学：向复杂科学领域扩展&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比以往更多集中在数学与代码推理上的展示，Gemini 3 Deep Think 此次更新明显扩大了能力覆盖范围。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，当前版本的 Deep Think 已经在化学、物理等多个科学领域中展现出显著提升，尤其是在需要跨学科知识和多层次建模的任务中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在官方披露的测试中：&lt;/p&gt;&lt;p&gt;在 2025 年国际物理奥林匹克竞赛和国际化学奥林匹克竞赛的笔试部分，Deep Think 均达到了金牌级别表现。在评估高等理论物理能力的 CMT-Benchmark 中，模型取得了 50.5% 的分数，显示出其在凝聚态物理等高度抽象领域中的推理潜力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/ae28df3e8d762562b051681903af9527.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些结果意味着，Deep Think 已不再局限于形式化推理问题，而开始具备处理真实科研难题的能力基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;面向真实工程场景，而非“榜单模型”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌在介绍中反复强调，Gemini 3 Deep Think 的设计目标，并不是单纯在榜单中取得高分。&lt;/p&gt;&lt;p&gt;在工程应用层面，Deep Think 被定位为一种辅助研究与工程决策的工具，可用于：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;帮助研究人员理解结构复杂、变量众多的数据协助工程师使用代码对物理系统进行建模与仿真在设计与验证阶段提供多路径推理支持&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其是在工程与科研交叉的场景中，Deep Think 被视为一种潜在的“认知放大器”，而不是自动化替代方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌表示，接下来将继续通过 Gemini API 等渠道，将这一能力逐步提供给真正需要它的研究人员和从业者，并在真实使用中持续优化模型行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从此次更新可以看出，Gemini 3 Deep Think 的发展方向，正在从单点能力展示，逐步走向更底层的科研与工程智能基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在大模型普遍追求通用性和产品化体验的背景下，谷歌选择继续在 Deep Think 上深耕高复杂度、低确定性的任务空间。这一策略，也使其在当前大模型格局中，形成了与偏重实时交互和工具化路径的模型体系的明显区隔。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着 Gemini API 的逐步开放，Gemini 3 Deep Think 是否能够真正嵌入科研与工程流程，并在真实环境中经受住复杂问题的考验，将成为外界关注的下一步关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用户：这是真正有用的工具&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和 OpenAI Codex Spark 一样，谷歌Deep Think 也一样逃不掉网友热议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在x上，有用户认为，Deep Think 的价值在于它能否经受住现实的考验：返回可运行的代码，显示假设/单位，并在数据缺失时发出明确的错误提示。如果它仍然只是“推理”工具，无法交付模拟程序或调试模型，那么它只不过是一个更高级的自动补全工具而已。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0d/0d4964d58bdb748b825bc4e1d375471c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有x用户认为这是一次重要的升级，他表示：“Gemini 将草图转化为 3D 打印模型的功能简直太棒了——这才是工程师们真正会使用的 AI 升级。如果这种趋势持续下去，原型制作速度将提升近 10 倍。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ee/ee2cfc99f879c3555381c15f1c2cffef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一位主页介绍为 Amazon 工程师的x用户表示：我们正在从聊天时代迈向推理时代。谷歌刚刚升级了 Gemini 3 Deep Think，以解决科学和工程领域最棘手的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“为什么这次更新是一次力量倍增器：它通过探索多个假设来解决没有单一‘正确’答案的问题。针对研究和高级工程中混乱、不完整的数据进行了优化。它使用‘思维签名’来保持长期、复杂项目的逻辑性。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a8/a8a23ee27ac7f6676c56cbd8ef9f9725.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有用户表示，此次更新的模型取得的基准测试结果令人印象深刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;但真正的变革将在以下情况下发生：人工智能可将工程时间缩短 50%；人工智能改进科学建模；人工智能降低研发成本；&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59c8ede8f55203019af72aebae17f97f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/&quot;&gt;https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/introducing-gpt-5-3-codex-spark/&quot;&gt;https://openai.com/index/introducing-gpt-5-3-codex-spark/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://codeforces.com/ratings&quot;&gt;https://codeforces.com/ratings&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/re3CXNqi9tyPH4lIOohq</link><guid isPermaLink="false">https://www.infoq.cn/article/re3CXNqi9tyPH4lIOohq</guid><pubDate>Fri, 13 Feb 2026 08:01:34 GMT</pubDate><author>李冬梅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>Next Moca 开源智能体定义语言 ADL</title><description>&lt;p&gt;Moca 已开源 &lt;a href=&quot;https://www.nextmoca.com/blogs/agent-definition-language-adl-the-open-source-standard-for-defining-ai-agents&quot;&gt;Agent Definition Language&lt;/a&gt;&quot;（ADL，智能体定义语言），一种与供应商无关的规范，旨在为 AI 智能体的定义、审查与治理提供标准化方案，使其可跨框架、跨平台使用。该项目采用了 Apache 2.0 许可协议，被定位为 AI 智能体“定义层”的关键缺失环节，作用堪比 OpenAPI 在 API 领域的地位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADL 提供了一种声明式格式用于定义 AI 智能体，包括身份、角色、语言模型设置、工具、权限、RAG 数据访问、依赖关系以及治理元数据（如所有权和版本历史），旨在提升智能体在不同平台与供应商之间的可移植性、可审计性和互操作性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次发布旨在解决智能体开发中日益严重的碎片化问题。目前，智能体相关逻辑往往分散在提示词、代码、特定框架的配置文件及无文档记录的预设逻辑中，导致团队难以厘清智能体的能力、边界与审批状态等基础信息，也进一步提升了安全审查、合规管控与复用的复杂度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADL 将智能体定义整合为结构化、机器可读的格式，从而提升其可检查性与治理能力。它与框架无关，专注于定义而非执行，不涉及智能体通信、运行时工具调用或消息传输。ADL 旨在对 A2A、MCP、OpenAPI 及工作流引擎等现有技术形成补充。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在宣布该项目开源发布时，Next Moca 创始人 Kiran Kashalkar 将 ADL &lt;a href=&quot;https://www.linkedin.com/posts/kirankashalkar_nextmoca-adl-adlc-share-7403917843865481216-_YPM?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAACX5yoEBhsg1xPtc5iaJXHCu_Rv298CmfZA&quot;&gt;形容&lt;/a&gt;&quot;为“智能体的 OpenAPI（Swagger）”，并补充说它提供了“一个单一的声明式规范，明确了智能体的定义、可调用的工具、可访问的数据及配置方式”。Kashalkar 强调，可移植性、可审计性与供应商中立性是其核心设计目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 Next Moca 介绍，ADL 面向的是构建生产级 AI 系统的团队。在这类系统中，智能体越来越多地作为自主组件运行，可以访问工具、数据和外部系统。Next Moca 认为，标准化的定义层能够实现更清晰的规划、在 CI 管道中完成一致验证、明确对比智能体能力，并通过版本控制与回滚实现类软件的生命周期管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该项目包含已发布的 JSON Schema、智能体定义示例、验证工具，以及涵盖治理与贡献流程的相关文档。开发者可一次性完成智能体定义并在本地验证，再将同一套定义共享给安全、平台及合规团队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Next Moca 将 ADL 定位为处于早期阶段的标准，并邀请社区提供反馈、参与贡献，以推动其发展。该公司表示，开源这一规范旨在推动其广泛应用与中立治理，并围绕这一通用格式构建包含编辑器、验证器、注册表和测试工具的生态体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ADL 的代码和文档可在 &lt;a href=&quot;https://github.com/nextmoca/adl&quot;&gt;GitHub&lt;/a&gt;&quot; 上获取，其中包含贡献指南和公开路线图，并概述了后续规划步骤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/agent-definition-language/&quot;&gt;https://www.infoq.com/news/2026/02/agent-definition-language/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/4KDHTtLauzMq0apWmDLr</link><guid isPermaLink="false">https://www.infoq.cn/article/4KDHTtLauzMq0apWmDLr</guid><pubDate>Fri, 13 Feb 2026 08:00:00 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>AI&amp;大模型</category></item><item><title>面向 AI Agents 的高性能数据基座：架构和工程实践</title><description>&lt;p&gt;随着大模型快速发展，以 AI Agents 驱动的新一代 AI 原生应用快速发展，取得巨大成功。AI 原生应用以大模型为基础，通过各类 Agents 和应用数据交互，智能地完成各类任务。AI Agents 驱动的应用开发迭代迅速，同时维护多种模态的数据，不同模态数据的访问模式和流量差别巨大，这些特点给底层数据平台提出了新的挑战。未来 AI Agents 驱动的原生应用需要怎样的数据基座？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 InfoQ 举办的 &lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/&quot;&gt;QCon 全球软件开发大会（北京站）&lt;/a&gt;&quot;上，晨章数据创始人、首席架构师陈亮带来了题为《&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6377&quot;&gt;面向 AI Agents 的高性能数据基座：架构和工程实践&lt;/a&gt;&quot;》的演讲，分享了关于 AI 时代数据基座架构的思考，如何通过该架构解决 AI 原生应用的数据挑战，以及在云计算、新硬件环境下实现高性能数据基座的工程实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：将于 4 月 16 - 18 召开的 QCon 北京站设计了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1909&quot;&gt;AI 原生基础设施&lt;/a&gt;&quot;」专题，本专题重点交流探讨如何构建 AI 原生基础设施，包括业界容器 / Serverless 等云原生基础设施如何朝 AI 演进，以及如何利用一些新兴分布式技术构建 AI 原生基础设施等等。如果你有相关技术案例，欢迎&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;加入这场技术共创&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI Agent 驱动的 AI 原生应用&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，AI Agent 正在引领整个软件范式的变革。在 AI 时代之前，我们讨论的是 SaaS，彼时软件作为工具实际上是构建了一个工作流程，在这个工作流程中帮助人来完成某些工作。而 SaaS 变成 AI 驱动后就会发生范式变化，软件变得更加智能，变成了智能体，可以执行非常复杂的任务，甚至可以有一定的自我演化和改进的能力。从这个角度来说，它不再是个帮助人的软件工具，它自己就变成一个智能体，可以直接提供一个服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 SaaS 时代，SaaS 软件会有工作流，用户会提供一个输入，工作流帮助用户完成某些任务。工作流中间会收集很多数据，很多状态，这些状态会记录在某个数据库里，很多时候它是结构化数据。这里有一个显著的特性，就是第一个数据是由这个软件生成出来的，或者我们认为说数据是软件的一个排放。所以在这样的一个架构下，大家对数据会有一些比较简化的期待。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一个数据的格式往往是软件开发人员定义的。因为这是我写的软件，所以我想定义这个数据有什么属性，大概以什么格式，到底是个表格还是个图，这个是我开发者来定义的。同时数据也是在软件的运行过程中不断收集的，意味着我的数据量是随着我的软件规模和用户互动慢慢在增长，总的来说是可控的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然随着软件越来越复杂，收集数据越来越多，最终数据的格式可能会变得更加复杂，我需要更加智能的分析，但这个过程是一个相对缓慢的过程，这个过程是随着我的软件越来越流行，用户量越来越大而发展的。可能很多软件并没有爆款，它对数据的需求也就并没有那么高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Agent 时代会有什么变化？首先在 Agent 场景中，它的工作流就不再是工作流，开发更多关注在 Agent 的编排上，我们可能会有很多个 Agent。当然核心是大模型，我们怎么用大模型去驱动不同的 Agent？这里有一个很不一样的点，就是说在今天我们刚开始开发应用时就需要有数据，数据可能来自知识库，可能来自外部的某些结构化数据，这个数据实际是作为 Agent 的燃料，就像汽车冷启动需要燃料一样。大模型更多像一个驱动引擎，因为大模型只能提供一些比较通识性的东西，我要去实现一些非常领域特定的任务实际是有困难的，所以我需要很多数据，而且是行业的数据。但这个数据是外部来的，所以数据的格式、数据的规模可能不是我所能完全掌控的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 与用户不断交互还会产生更多数据，同样它会生成底层的数据。我们接触了很多 Agent 开发的项目，我发现它们在第一天的时候就考虑数据的反哺，换句话说我不但要收集数据，还要用数据丰富我的知识库。最终我提供的就是一个服务，用户看到的是一个整体的服务，它不再是帮助人的工具，这时它自己可能就变成了一个智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;举一个具体的例子，这是一个金融的场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/43/43cf67c83d39a687d616c363389edca5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里有 4 个 Agent，有一个 Agent 主要负责市场分析，一个 Agent 要关注风控，等等。从数据的视角来看，在这个 App 里我们可能需要很多种不同的数据库。我可能需要有用户的信息，用户的信息一般是存在表格里的。我可能会有财报，往往是一种半结构化的数据，里面可能有一些结构化的东西可以抽出来。如果外部的知识库很大，包括很多日志，它可能也是要放入 Mongo 的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Pinecone 和 Elastic 大家比较熟悉，因为在大模型时代你的文本是很重要的事情。当我们提到文本搜索时，实际上向量和全文往往是要一起做的，同时可能还要搭配一个 Ranker。当然可能还会有知识库，有些知识库是通过图来表示的，因为用户有不断的反馈的时候，这个时候你的 Agent 往往需要一些对话的信息，而且延时要求很短，所以我需要一些基于内存的数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以在搭建 Agent 应用的第一天，可能就会涉及到很多种数据库了。而外部来的数据规模你是不可管控的，如果规模很大怎么办？这个时候你就得用一个有扩展性的选择。同时从性能要求来说，因为有些业务是和用户要交互的，所以延时要求天生比较严苛，所以我一定会有一个纯内存的东西。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/81/813d5fc184e8e8c14aaa18c123f367bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图展示了今天常见的一个 Agent 的工作流。它一般是一个网络服务，用户会登录进来，登录之后你会有用户的信息，这时你就要访问一些关系型数据库了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 里有一个很重要的，我们叫 Agent Loop 或者叫一个环。因为交互往往不是一轮的，需要很多人不断迭代。这个 Agent 它自己可能会调大模型，获取一些信息，还会有很多外部调用。外部调用可能来自网络搜索，可能外部有某个计算服务。当然还有一个很重要的方式就是 RAG，可能会有全文知识库的 RAG。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/42/425323a191eb183c8c315786ebd7101d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里还有短期和长期的记忆，有不同的延时需求。短期记忆可能要放在一个基于内存的东西，长期的规模比较大的就放在一个相对持久的数据库里。所以不同的数据都需要自己对应的数据库，而且在应用交互的过程中还会不断生成新的数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单总结一下，互联网时代和 Agent 时代从数据角度来看的第一个区别就是前者的数据是由应用生成的，意味着数据是可控的。但在 AI 时代就不一样了，因为可能会有很多外部的数据进来，这个东西不是你完全可控的，而且规模也可能会很大。另外 AI 时代还会有大量非结构化数据，所以今天几乎所有的数据库都要发展向量的能力，因为搜索变得越来越重要。最后一点就是 Agent 是会互相交互，会跟外部交互的，交互时它是要把内容记录下来，所以数据量很快就会积累起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI 原生应用面临的数据挑战&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从系统的角度来说，AI 时代的这些特点会给数据库管理带来很多挑战。第一个挑战就是我们希望数据库会有多模态。第二个是当我们有很多个数据库，数据的同步和数据一致性总是要考虑的。比如说我们在聊天里可能有短期记忆，最终总是要把它变成长期记忆的，同时应用输出的内容也总要反馈到原来的各种数据模型下，就会有一个数据的环。第三点，应用中不同数据库对性能、规模等属性的要求各不一样。最后就是多系统的运维和管理。今天的 AI 时代我们可以快速开发一个应用，可能 3 个人的团队 6 个月就可以快速搭建 1 个 App，但我要运维 App 反倒变成了一个很大的成本，因为数据要不断积累，这是你的核心价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总结来说，AI Agent 驱动的应用在早期就会面临传统大厂才会有的数据挑战，同时数据飞轮在 AI Agent 时代迭代更加迅速，加剧了数据库系统的压力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;多模态数据基座&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样的背景下，我们的思考就是我们应该做哪些事情，能不能有一个统一的数据架构来做这样的事情。最终的方向就是多模态的数据基座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/93/93aa2592936022e32a436501cab70ef3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的设计目标有三点，第一点就是支持多种数据模态。在 AI 时代，可能一个应用就会面临多模态支持的问题。我们想特别强调两个方面，第一就是我们希望它的 API 是原生兼容的。比如说我有个 Json 的 API，至少应该是跟 Mongo 兼容；我是个 SQL API，应该可以和 MySQL 兼容，这是一个很重要的点。因为开发人员希望我的系统是可扩展的，可迁移的，有的时候我可能想在云上部署，有可能我想在私有化部署，私有化部署甚至还可能有很多限制，所以你用标准的 API 变得非常关键。我当然可以自己定一个 API，但如果让大家来我这边建立 App，未来就会有很大的风险。第二个我想强调的是性能。性能和成本永远是长期的考量，我觉得这可能是最关键的，对系统开发人员来说可能是最重要的点。用户在选择的时候，如果你的系统性能比别人慢，你说我的价值来自多模态，这个论述就变得非常弱了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二点设计目标就是动态伸缩，自动管理。这是和今天云原生的趋势是很吻合的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三点目标是跨模态访问和一致性。模态之间是有数据的相互同步，同样有一致的访问。我并不希望比如我有 8 个数据库，大家各干各的。数据库之间的壁垒要消融掉，是多模态很重要的点。我并不需要一个中间件或者一个代理，把所有的数据库连接起来，然后统一提供接口，这个意义并没有很大，你没有降低它的管理成本。同时在有些应用里，从长期记忆到短期记忆，从我的结果到抽取回知识库里，很多时候都是跨模态的访问。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在讲多模态数据架构之前，我先简单回顾一下数据库架构的演化历史。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/12/125351919f5763581f38ca33efe4e748.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据库在很早以前实际上就一台机器，后来数据库演化就分叉了，分成了 OLAP 和 OLTP。到了云时代，之前 OLAP 数据库的无共享架构就不是特别理想，于是就有了新的做法叫存算分离。OLTP 一开始也有无共享架构，但后来它的演化就没那么简单。因为 TP 和 AP 之间有一个很大的不同，AP 里不太在意内存缓存。因为它要不断扫描大量数据，内存肯定放不下，最后总是要扫描磁盘的。可在线的 TP 就不一样了，因为需要毫秒级的延迟，所以内存缓存非常重要，是保证延时最重要的手段。而无共享架构下计算和缓存不在一块，每次访问都要走网络，那么在 Agent 时代这个延迟就很难保证。所以业界提出了 Aurora 的架构，把缓存提上去，计算和缓存在一块，然后下面有个存储，我们叫共享存储架构，这样它的延时可以保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/7a/7ac594623812e162efc035e088f2db9d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的思路是，在计算和存储之间会有一个数据的基层，在基层里我最强调的就是缓存，缓存是保证在线数据库延时里最重要的一个事。而且在线数据的所有模态，不管是做向量搜索、全文搜索还是做 graph，最重要的都是缓存，不在缓存里的东西延时是很难有保证的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2e/2e715e9d352694a597d6499fb5ced942.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的数据基座里上面会有计算，下面会有存储。计算引擎是可以换的，什么样的引擎都可以，我们可以和很多开源组件原生兼容。中间我们会有一个数据基层，它抽象出来了在线数据库里最核心的一些功能，其中最重要的就是缓存。同时我们希望能用统一的缓存格式来弥合或消除不同数据模态之间的壁垒，使得数据在统一访问，或者在跨模态访问时可以高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外在我们这里缓存和计算是逻辑解耦，物理耦合的。物理耦合指的是缓存尽量放在机器本地内存上，减少跨网络读取。逻辑解耦是想用它来消除不同模态之间的差别，通过缓存实现和原生系统一样的性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/89/89f9b35c93a9d2b21c5b419dcbbf5e30.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提到缓存，大家可能第一反应就是一个传统的哈希表。但因为我们的数据是想支持不同模态的，所以我要努力支持更加复杂的数据结构，这个结构是随着我的模态而变的。所以缓存的设计并不是一个简单的 key 和 value 的映射，我们希望有一个能支持更加复杂数据结构的设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ba/ba47bec80189aaba351d84ec744bf273.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总的来说我们会有一个逻辑上的分布式哈希表，它不是简单的 string，是一种复杂的数据结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/03/0335ca6defae7b417583e18632ce71d6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存和存储之间还会有互动，当数据做更改之后，缓存会用异步的方式把它存到存储上，实时操作只会更新到日志里，不会触发持久化存储。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/92/921f4038903900c7fa9d4b1d7dfdc3ee.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后我们还有一个容错和恢复的协议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们这样的融合数据库，希望能通过一个数据基层，在融合缓存上装配各种计算引擎。因为缓存和计算是物理耦合的，所以性能可以做到和原生一样好。同时我的协议也是完全兼容原生的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/17/1720476ee7806a2445ac53222d5aaf02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样我就可以支持多种模态，兼容现有标准，同时满足性能需求，并做到统一的运维管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/66/66c5559f57b629f6ea4c301ee554083f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面向未来的工程实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈到工程实践，我们先来看看 AI 时代基础设施环境是什么样子的。我们会有云计算、高速网络、高速存储设备，还有 GPU 这样的新型计算设备提供巨大的算力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去十年来，CPU 的性能增长了一倍半，而存储性能增长了 11 倍以上，网络性能有 20 倍的增长。按照这个趋势发展，未来我们的存储或数据基座设备会变成 CPU 瓶颈，因为 IOPS 在快速增长，CPU 的增长却很缓慢。所以如果今天我们什么都不做的话，未来 IOPS 这一块我们会看不到任何性能提升，因为 CPU 没有提升。这也就意味着传统数据库系统的性能无法随着 IO 设备的迭代而增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/07/07569af80591ad9669fc8ab5843f2974.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对这个问题我们做了很多事情，比如说我们所有的执行都是协程，比如说我们都是要异步编程，比如说我们每个核心只有一个线程，最后我们希望能采用一些缓存友好的数据结构。这些都是为了降低 CPU 的负载。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/a7/a71d23d54b52dd4cec3afc8127564f50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们公司还做了一个试验性的存储，它是一个 k-v 存储，是多读单写的。我们每个查询做一个协程，提交后切换协程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/f1/f121ae716bd6b56cad59f85d1fe552bd.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;然后我们做了简单的测试，就是我们有个 16 核的机器，故意把缓存设成了 4GB，就是为了看数据不是全部缓存在内存里是什么表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8f/8f240ac7328475471aa0c15888b88e8e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果发现在同样的 CPU 下，我们的设计可以做到接近理论上线，比 RocksDB 高出很多。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b8/b8acd358d4399170352d48267c55b8e6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也就证明传统的数据库架构会遇到 CPU 的瓶颈，出现这种瓶颈时单纯更换 IO 设备是很难有增长的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;总结和展望&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总的来说，我觉得 AI Agent 时代会带来软件范式的变革，软件范式变革必然会让数据管理产生巨大的变化。总结起来就是我们希望有多模态的支持，有原生 API 的支持，有高性能的支持，我们希望扩缩容更加方便，从管理来讲会更加易用。最后结合工程实践，我们在性能方面也有一些新的思考。希望这次的分享能给大家带来启发，谢谢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;陈亮，北京晨章数据科技有限公司创始人，首席架构师。前微软亚洲研究员首席研究员。数据库领域顶级专家。微软 SQL Server XML 索引发明人和架构师，微软 Cosmos DB 图数据库架构师。曾在 SIGMOD、VLDB、 ICDE 等国际顶级会议上发表多篇学术论文。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026，AI 正在以更工程化的方式深度融入软件生产，Agentic AI 的探索也将从局部试点迈向体系化工程建设！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026&lt;/a&gt;&quot; 已正式启动，本届大会以“Agentic AI 时代的软件工程重塑”为核心主线，推动技术探索从「AI For What」真正落地到可持续的「Value From AI」。从前沿技术雷达、架构设计与数据底座、效能与成本、产品与交互、可信落地、研发组织进化六大维度，系统性展开深度探索。QCon 北京 2026，邀你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ae/aea4cfa5bfc8aecce0a3a51c0c81849b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2B1adWKlxpisOtxhOSfj</link><guid isPermaLink="false">https://www.infoq.cn/article/2B1adWKlxpisOtxhOSfj</guid><pubDate>Fri, 13 Feb 2026 06:28:09 GMT</pubDate><author>作者：陈亮</author><category>架构</category><category>AI&amp;大模型</category></item><item><title>Vibe Coding 在代码生成与协作中的实践与思考</title><description>&lt;p&gt;AI 发展过程中诞生了许多优秀的 Coding 产品，但非专业开发者需要掌握一些简单的研发知识才能完成研发任务，而这些工具和研发知识的匮乏，都在不同程度上影响非专业开发者的热情。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文整理自阿里巴巴高级技术专家向邦宇在 &amp;nbsp;&lt;a href=&quot;https://qcon.infoq.cn/2025/shanghai/&quot;&gt;2025 QCon 全球软件开发大会（上海站）&lt;/a&gt;&quot;的分享 “&lt;a href=&quot;https://qcon.infoq.cn/2025/shanghai/presentation/6746&quot;&gt;Vibe Coding 在代码生成与协作中的实践与思考&lt;/a&gt;&quot;”。主要探讨如何构建下一代 Vibe Coding 工具，从阿里当前的挑战出发，提出以用户为中心、强化工具质量、深化场景适配、支持协作与包容不确定性的核心设计原则与实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：将于 4 月 16 - 18 召开的 QCon 北京站设计了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1918&quot;&gt;Coding Agent 驱动的研发新范式&lt;/a&gt;&quot;」专题，本专题聚焦 Coding Agent 驱动的研发新范式，探讨其在需求理解、代码生成、测试修复与协作流程中的工程实践，以及对研发工作流、工程效率与研发组织方式带来的变化。如果你有相关技术案例，欢迎&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;加入这场技术共创&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内容亮点&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vibe Coding 工具在建设过程中遇到的问题，以及解决的办法构建 Vibe Coding 工具所趟过的产品方面的坑构建 Vibe Coding 工具时的技术创新与落地实践&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多年来，我一直在阿里巴巴内部的技术研发设施平台上从事研发者工具的工作，其中包括内部的 AI 编程工具以及 Web IDE 工具等。从 2023 年开始，我参与了相关工作的转型，从之前的内部 Copilot 逐步转向如今的 Agent 方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我拿到这次演讲选题时，我在思考 Vibe Coding 这一主题。虽然 Vibe Coding 已经出现几个月了，但它似乎还不是一个非常确定性的概念，因为大家对它的理解以及所使用的相关工具都存在差异。而我由于接触了大量内部用户对这些工具的使用情况，包括他们在使用过程中遇到的问题，以及作为产品提供方，面对众多用户在使用工具时所遇到的问题，我需要思考如何解决这些问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我会简单介绍一下我们内部在哪些行业以及具体使用了哪些 Vibe Coding 工具。接着，我会讲述用户在使用 Web 编程工具过程中遇到的一些问题。然后，作为 Vibe Coding 工具的两位核心主导者之一，我会分享我是如何思考这些问题的。最后，我在之前的许多分享中已经介绍过我们如何使用国产模型以及在适配国产模型过程中遇到的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Vibe Coding 产品形态&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，Vibe Coding 工具大致可以分为四类。首先是 Native IDE，例如近年来较为流行的 Cursor、Trae，以及我们阿里巴巴的 Qoder 等，它们都以本地集成开发环境的形式存在。第二类是 IDE 插件，比如我们内部的 Aone Copilot 等工具，这些插件大多是基于现有的开发环境，如 VSCode 或 JetBrains 的插件形式存在。目前来看，内部用户使用这类插件仍是一种比较主流的习惯，尽管其灵活性可能不如 Native IDE 那么高。第三类是 Web Agent，它的入口在浏览器上，整个执行过程在一个异步容器中进行，可能是沙箱环境。它可以解决信任问题以及云端执行中的安全问题，并且对于协作更加友好，能够在 Web Agent 中实现多人同步协作和分享。这类主要是跨平台工具，具有广泛的适用性。最后一类是 CLI 命令行工具，这其实是一个比较意外的类别。我们之前并没有预料到像 Claude Code 这样的 CLI 工具会如此受欢迎。最初，我们认为这种工具不会受到主流研发人员的欢迎，但后来发现大家其实非常接受这种模式。现在我们认为，CLI 模式在被集成的方式中，比如 CI 或一些异步容器中执行垂直任务时，具有更高的可用性。这就是我对 Vibe Coding 工具大致分类的介绍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/54/544f605c75f1c6c3f50225a4874d96d0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Vibe Coding 在阿里内部的发展现状&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我主要介绍一下我主导的两个 Vibe Coding 工具的使用情况。首先是基于 IDE 的 Vibe Coding 工具。我们内部有一个名为 Aone Copilot 的工具，它已经存在多年，拥有众多用户，每周大约有数千的活跃用户。目前，用户在使用 IDE 的 Vibe Agent 工具时，主要场景包括新增代码、修复漏洞以及代码分析等。在后端场景中，这种工具的渗透率相对较高，而在前端场景中，大家可能更倾向于使用 Native IDE，如 Cursor 或 Qoder。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个我主导的项目是 Aone Agent。这是一个以外部容器发起的异步任务工具。它强调用户可以通过自然语言发起任务，我们在异步容器中启动一个 Agent，这个 Agent 会自行调用各种工具，无论是搜索工具、文件读取工具还是 Shell 工具。这种在容器内执行的异步 Agent 与前面提到的 IDE Agent 有本质区别。虽然用户主要是后端人员，但我们发现测试人员、前端人员、算法工程师、产品经理、运营人员、设计师以及运维人员等都在使用这种工具。它的用户群体更加多元，提交的任务类型也更加丰富多样，包括代码分析、代码修改、单元测试、代码生成以及文案方案调研等，用户通过这种工具进行各种探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/12/12b57d9e082440710b9a59ddaa18acbe.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Vibe Coding，尤其是 Agent 模式发展之后，我们看到了一些显著的变化。以 Aone Copilot 的 Agent 模式为例，从 4 月份开始，我们观察到用户提交代码行数的变化。蓝色的线表示高频用户，即那些经常使用该工具的用户。我们发现，在 Agent 模式下，这些高频用户的代码提交行数有了显著提升。虽然整体趋势都在上升，但高频用户的提升更为明显。从定量角度来看，9 月份高频用户每天提交的代码行数约为 560 行，而其他用户只有 400 多行。这至少证明了 Agent 模式在提高效率方面是有效的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们还发现，不同用户对这些工具的使用方式有所不同。前 10% 的用户提交的代码行数是其他用户的两倍。但我认为，Agent 对人的效率提升可能不止两倍，因为大量的工作可能涉及协作或会议等。我们还发现，TOP 10 用户的 Token 消耗占总消耗的 80%。在 Vibe Coding 工具的使用下，由 AI 生成的代码提交占比越来越高。随着 Vibe Coding 工具的发展，像 JDK 升级、NPM 包升级或 SDK 升级等任务已经可以由 AI 完成，尤其是 JDK 11 及以上版本的升级场景，我们内部几乎全部交由 Vibe Coding 工具来完成。此外，数据分析和数据整理工作也部分交给了 Agent。过去，一些必须由人工完成的任务，如大促过程中的截图或压力测试中的重复任务，现在都可以由 Agent 完成。还有一些在研发过程中成本过高而无法进行的事情，比如一次发布是否会引发其他相关系统的故障，现在也在探索使用 Agent 来解决。过去，由于无法审查每一行代码对其他系统的影响，这类问题很难处理，但如今 Agent 可以承担这项任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;用户在 Vibe Coding 过程中遇到的挑战&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在审视当前技术发展现状时，从用户的角度来看，技术和产品都面临着一些亟待解决的问题。首先，用户常常因为 AI 的表现不尽如人意而感到沮丧。从后台日志中，我们可以看到大量用户抱怨“电脑太笨了”等类似的不满情绪，这些反馈充满了挫败感。同时，用户频繁地删除和修改代码的现象也屡见不鲜。无论是公司内部还是在社区中，都存在许多用户因 Agent 能力不足而陷入困境的情况。此前，甚至有用户在 GitHub 上分享关于 AI 的“八荣八耻”提示词，其中不乏诸如“以不修改原始代码为荣”等观点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/d5/d5fc7523bfb7626ccef97107234b5eb0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合来看，Vibe Coding 工具给用户带来的问题主要体现在以下几个方面。首先是代码质量问题，生成的代码往往缺乏质量把控。其次是调试和维护困难，这给用户带来了额外的负担。第三是用户体验不佳，目前的 AI 编程工具尚未达到让用户满意的程度。最后是成本与效率问题，这些问题也在一定程度上影响了工具的使用效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/aa/aa3862c6377aa3643968a5102a4f97bb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为代码质量不足主要体现在几个方面。首先是代码一致性不足。在不同场景下，生成代码的质量和风格存在较大差异。例如，在存量代码仓库中编写代码时，AI 往往会按照自己的风格生成代码，这与现有代码风格不一致。其次，边界条件的处理不够完善。对于复杂业务逻辑的边界情况，AI 生成的代码往往处理得不够充分。此外，生成的代码还存在性能缺失的问题。最后，安全漏洞问题尤为突出，尤其是 SQL 注入类漏洞。斯坦福大学的一项研究指出，AI 生成的代码中存在注入类漏洞的比例约为 45%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在实际应用中，我们发现了一些典型案例。首先是安全漏洞，包括 SQL 注入和 XSS 攻击。其次是在边界逻辑处理方面，逻辑错误和边界条件处理不当的情况较为常见，例如空指针异常和数组越界等问题，这些都是我们在用户使用过程中观察到的现象。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/22/22b4774a93f01a59477f4a93acb91b4e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们发现 AI 在代码生成过程中存在自洽问题。过去，我们曾考虑让 AI 生成代码的同时，也生成对应的单元测试，以此来解决代码质量问题。然而，我们很快发现，如果让 AI 同时负责代码逻辑和单元测试的生成，它无法保证质量，因为 AI 会在逻辑上进行自洽。例如，下图展示的一段数组去重函数及其对应的测试代码，虽然测试通过率达到了 100%，但其逻辑实际上是存在问题的。这说明，如果完全依赖 AI 来完成代码和测试，很容易出现自我拟合的情况。因此，我们建议用户在使用 AI 生成代码时，至少有一项由人工进行 Review 或主导，以确保质量&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/bc/bc1528eb86a7621f02cb5a7cff151107.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在用户使用 Vibe Coding 工具的过程中，我们还发现调试时间增加了 30% 到 50%。这是因为 Vibe Coding 更倾向于生成黑盒代码逻辑，尽管最终会让人确认代码的差异（DIFF）后才能提交，但生成过程和代码本身通常不会被逐条仔细检查。因此，我们将其视为一种黑盒操作，AI 生成代码就像一种“黑魔法”，一旦出现问题，用户可能不知道从何处入手，技术债务也会不断累积。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个问题是上下文理解的局限性。对于存量任务，其业务逻辑往往是经过多年积累形成的，一些代码为何如此编写，是否可以删除等问题，对于 Agent 来说都是难题。我们认为，Vibe Coding 工具缺乏全局思维，生成的代码模块化程度不足，代码耦合度较高。为了解决这一问题，目前有一些方案，例如 Repo Wiki 或 Deep Wiki 等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，Vibe Coding 缺乏可追溯性，这限制了工具的使用。由于 Vibe Coding 一次性生成大量代码，我们很难确定是新的需求导致代码出错，还是最初生成时就存在错误。因此，如何引入版本管理的概念，以便在代码出错后能够回滚到正确状态，是一个亟待解决的问题。目前有一些方法，例如在每次修改并通过测试后提交一个 Commit，以便后续能够从该 Commit 回滚。也有一些工具，如 Cursor 或其他回滚工具，但总体而言，Vibe Coding 在可追溯性方面仍有不足。用户在生成大量代码或经过多次迭代后，往往无法进行有效的版本管理，只能选择回滚或重新开始。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前 Vibe Coding 工具还无法像人类开发者那样熟练运用常见的调试工具。在过去传统的编程模式中，开发者们常常会大量使用调试工具，例如在代码中设置断点，或者在浏览器中进行调试。然而，对于 Vibe Coding 工具来说，要利用这些调试工具来定位问题的堆栈信息，几乎是不可能完成的任务。那么，Vibe Coding 工具是如何应对这种情况的呢？它们通常会通过大量打印日志（如 console log）来解决问题。它们要求用户在执行代码后，将控制台中的报错信息或打印内容复制并粘贴给工具，以便进一步分析。这种模式不仅需要人工介入，而且效率低下。因此，我认为大型模型的调试手段相对单一，传统的调试方法很难被这些模型有效利用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/db/db9fac19f14491584a912bfbfdb88681.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/52/5218e8bd51553f8de0420f0f91159de9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从用户使用 Vibe Coding 工具的角度来看，除了编码层面的问题外，工具本身也存在诸多不足。首先，稳定性和成功率是最大的问题之一。Vibe Coding 工具的执行时间往往较长，用户可能需要等待 30 秒到 5 分钟才能得到结果，而且并非每次都能成功。失败的原因可能是模型返回错误、工具调用出错，或者 IDE 本身不稳定等。一些用户在初次使用后，发现结果不稳定，尤其是在时间紧迫、任务繁重的情况下，他们就不再愿意使用这类工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，交互界面设计也存在一些问题。这并非缺陷，而是因为许多 Vibe Coding 工具频繁改版，导致用户难以找到以前的功能，或者工具中不断增加新功能，使得用户感到困惑。以 Devin 为例，它在改版过程中，曾经引入了剧本、MCP 市场和知识库等功能，但后来又取消了。这种频繁的改版让用户难以适应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，沟通和交互存在障碍，主要表现为 AI 的理解能力不足。用户需要反复确认意图，尤其是在不同场景下，这种确认虽然有意义，但也增加了沟通成本。例如，在最近流行的 Spec Coding 中，用户先提出需求，生成设计稿，再让 Agent 执行。对于复杂的任务，这种模式可能是必要的，但对于其他任务，可能需要 Agent 自由探索。此外，长链路任务的执行能力也存在不足，无法维持长期的上下文对话。由于 Agent 大模型的 Token 有上限，当上下文过长时，其记忆和召回能力就会下降。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，工程工作流程的中断也是一个问题。目前有大量 Vibe Coding 工具，包括 IDE、CLI 和 Web Agent 等，每种工具都有其擅长的领域，但它们无法让用户在一个统一的流程或上下文中解决问题。例如，用户在 IDE 中完成一项任务后，如果切换到 CLI，就需要重新向新的 Agent 介绍需求。这种频繁切换不仅增加了用户的负担，也降低了工作效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b5/b597c5decce61459cca07e7a1a936fa7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b9/b993a4469b9446307a8fed42c9529bac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Vibe Coding 产品自身遇到的挑战&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 Agent 和模型能力的不断提升，产品功能也在不断演进。从最初的单代码补全场景，单个任务 4000 个 Token，到后来的 Chat 模式，单个任务 1000 个 Token，输出约为 4000 个 Token。再到 IDE 或 CLI &amp;nbsp;模式，Token 消耗量达到十万级别。如今，Web Agent 模式具备独立容器，能够广泛使用各种工具，实现多种任务类型的 Agent 模式，Token 消耗量更是达到百万级别。像 Cursor、Trae 等 Native IDE 工具正在探索 Sub-Agent 或 Multi-Agent 架构，单个任务的 Token 消耗量甚至可能达到上亿级别。这种演进模式虽然为用户提供了更强大的功能，但也给产品设计带来了挑战。一方面，我们需要让用户满意，另一方面，成本控制必须与用户规模相匹配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/5d/5d1b09d8325d711ac4ade34252d47f09.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品设计方面，Vibe Coding 工具，无论是 IDE Agent 还是 Web Agent，都处于摸索阶段。尽管模型能力的提升推动了产品功能的不断变化，但产品界面的区分度却不够。例如，Chat、Deep Research、Agent 等产品都采用对话框形式，用户难以区分不同产品的功能差异。此外，用户缺乏引导，面对 Vibe Coding 的对话框，用户往往不知道该输入什么内容。不同工具适用于不同场景，但用户常常一刀切地认为某个产品应该满足他们的需求，然而在实际使用中，他们发现产品无法达到预期目标。这不仅增加了用户的学习成本，也降低了产品的使用频次。我们观察到，像 Devin 这样的 Web Agent 工具，留存率非常低，这反映出用户在使用过程中遇到的诸多问题。另一个问题是缺乏一站式的功能闭环。用户面临的不仅仅是代码编写问题，还包括发布、部署、调试等多方面的问题。目前的 Vibe Coding 工具无法在一个产品中同时解决不同难度问题。比如，初学者可能需要更多指导和简化功能，而复杂问题则需要更强大的工具支持。这种功能上的割裂导致用户在使用过程中需要频繁切换工具，增加了使用成本和学习难度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/44/44d705f7a6b9ee75bba031265a7dcde1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vibe Coding 工具的安全性问题值得我们高度关注。可能大家有所耳闻，例如 Cursor 曾出现过删除用户本地代码的情况，虽然这类事件相对较少，但今年已经发生了好几次。另一个案例是 Anthropic 的 Claude Code 被劫持，攻击者利用 Vibe Coding 工具在用户网络中探测漏洞，并编写代码将敏感信息暴露出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在内网环境中，我们可能还无法完全信任 Vibe Coding 工具。当前，供应链攻击和开源代码的发展带来了新的挑战。许多人会在开源社区中潜入木马，一旦我们稍不留意，拉取的 SDK 或代码可能本身就存在漏洞。Vibe Coding 工具由于对代码或当前电脑具有一定的控制能力，能够进行自由探索，可能会发现系统中的漏洞并加以利用。因此，我们在使用 Vibe Coding 工具时，必须谨慎对待其安全性问题，确保在安全的环境中使用，并对工具的权限进行严格管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Agent 建设过程中的一些经验&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在参与 Agent 建设的过程中，我积累了一些经验，这些经验对我们后续的工作有着重要的启示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最初，我们采用了一种 All In One 架构，这种架构在建设 Vibe Agent 时带来了诸多问题。当时，Vibe Agent 的核心是一个输入框，围绕这个输入框的是 MCP 工具、知识库（Knowledge）以及各种剧本（Playbook）。这些外围工具构成了一个完整的场景图，涵盖了数据处理、后端开发、前端开发、代码审查、风险管理等多个方面。在这种架构下，所有工具和知识都需要放入上下文中，导致上下文内容异常庞大，成本难以压缩。例如，当时我们使用 Claude 模型执行一个任务，成本高达几百元，这显然是不可持续的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，这种 All In One 架构还导致任务成功率较低。当所有工具和知识集中在一起时，上下文过长，消耗大量 Token，不仅增加了成本，还降低了任务执行的效率。更重要的是，这种架构难以针对不同场景进行优化。例如，当我们对比其他类似产品时，我们的 Vibe Agent 在前端场景上的表现却不尽如人意。这说明，我们的架构缺乏灵活性，无法根据不同场景进行针对性的调整和优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/ba/ba5c1dfab159fbfa25dbd3b1fff1a3c3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在后续的 Agent 建设过程中，我们采取了一系列措施来优化工具的性能和用户体验。首先，我们对知识和数据进行了调整，特别是在代码数据建设方面，通过构建 Repo Wiki 和 Embedding 数据库，提升了对整体代码库的搜索理解和搜索能力。此外，我们还将研发行为数据纳入考量，包括构建 CI、CR、发布监控等行为。由于我们依托的是集团内部的发布平台和代码平台，因此能够将代码数据与需求数据相结合，形成一个综合的数据体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们意识到，传统的文档知识库难以直接被 Agent 使用，原因在于这些知识库可能存在信息过时、前后矛盾、图文混杂以及错误信息等问题。这些问题如果直接传递给 Agent，可能会导致误导。因此，我们没有采用传统的 RAG 技术，而是通过建立一个中间层来处理面向 Agent 的数据协议，从而解决文档知识库的引入问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Agent 的建设过程中，我们还发现很多知识并不在文档或代码中，而是存在于开发者的头脑中。因此，我们思考如何设计一个产品，帮助用户将这些知识沉淀下来。这并非通过自动生成实现，而是需要用户主动参与编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/75/752f671a24366ff402b1b84e41c55fc6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在上下文记忆方面，我们进行了大量处理工作，包括写入、提取、压缩和隔离等操作。我们的 Agent 工具旨在满足大多数用户的需求。为此，我们在容器中集成了大量工具，涵盖任务管理、基本交互、文件操作（读写、编辑、管理）、命令行执行监控等功能。由于 Agent 可以执行命令行，对于一些耗时较长的命令，我们需要监听其执行结果，并在超时后进行中断处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/93/93d15489fe0aa1f7e25cc3235b13c1e0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们还加入了浏览器自动化工具，例如使用 Playwright 等工具进行网页操作，帮助用户完成登录等交互任务。同时，我们还集成了多媒体开发工具，支持用户将代码部署到特定环境进行调试。在协作方面，我们设计了团队协作功能，用户可以将任务分享给他人，基于任务继续协作。我们还加入了高级功能，如并行执行优化和网络搜索等&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e0/e0084432e2890d4d726f24339dcd10a2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在面对模板和成本过高的问题时，我们采取了一系列措施来优化和解决。最初，我们发现单个任务的 Token 消耗量接近 400 万到 1000 万，这是一个极为严重的问题。为了降低 Token 成本，我们进行了一些操作和设计调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/dc/dc5ea7be38b023759861a0207764f27f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;积极适配和拥抱国产开源模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在探讨为何要解决成本问题时，我相信从事相关工作的人都能理解其重要性。实际上，解决成本问题的另一个重要方向是积极拥抱国产开源模型。然而，国产开源模型并非针对我们的具体场景进行训练，因此仍存在诸多问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用国外的 SOTA 闭源模型也存在诸多风险。首先，这类模型非常昂贵，尤其是处理复杂问题时，需要在长链路任务中运行，成本极高。其次，隐私问题不容忽视，闭源模型可能存在合规风险。第三，我们还发现了被限流和性能下降的问题，即使是同一模型、同一供应商，在不同时间的表现也可能不同，有时会出现格式错误或陷入死循环等问题。最后，国外模型在面向 C 端用户时，可能还存在备案等额外问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，国产模型在短链任务中表现良好，但在长链任务中仍存在一些问题。例如，死循环问题较为常见，因为 Agent 有多种选择和入口，可能在执行过程中陷入某种循环，无法跳出。另一个问题是格式遵循能力不足，例如 XML 标签格式不准确，前后无法匹配，导致无法正确解析，容易失败。此外，还存在指令遵循问题，在处理大量 Token 的上下文时，模型可能忘记某些指令，尤其是在未被充分训练的情况下。最后，我们还发现全局智能方面存在缺陷，模型容易陷入“走一步看一步”的情况，导致 Token 消耗大，步骤时间长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/eb/ebe374f6ad263a57ae8619372c39c8bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了应对这些问题，我们采取了一系列措施。首先，针对稳定性问题，我们设计了主备模型切换和重试机制。其次，为了解决速度慢或 Infra 稳定性问题，当模型输出被截断时，我们引入了流式输出和续写设计。此外，我们还进行了健康检查和死循环检测，在 Agent 中针对重复执行指令或相同错误点的无限循环问题进行了优化。当检测到明显错误逻辑时，我们能够及时干预。同时，我们还进行了格式检查和修复，针对模型生成的 XML 标签格式错误，通过堆栈或自动补齐方式解决格式缺失问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，我们已经将所有国外模型替换为国产模型。在运行过程中，我们会实时检测任务是否进入死循环，一旦发现，会采取干预措施，例如截断后续任务执行，或对任务进行总结和压缩，使其能够继续执行。这些措施都是我们在上下文管理方面的探索和实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/7a/7a711e8f68ddcc44ef4e4df2aae12ef8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在思考如何提升产品用户体验和降低使用成本时，我发现了一个核心问题：普通用户甚至小白用户在使用我们的产品时，往往不清楚产品能做什么。即便他们知道自己需要什么，也难以准确地提出需求，不知道如何在产品中选择合适的工具或知识。这导致产品的任务成功率很低，同时 Token 消耗量却很大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这些问题，我考虑是否可以将一些已经成功完成的垂直任务进行抽象和模板化。例如，如果某个任务经过多次探索后成功完成且用户非常满意，我们能否将其经验抽象出来，形成一套标准化的模板？通过这种方式，我们可以针对不同的垂直场景不断积累模板，从而提高任务的成功率，降低 Token 消耗。当用户面对对话框时，模板也能提供一定的引导性，帮助他们更好地使用产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在模板设计方面，这些模板可以被理解为工具组合和知识组合的集合。有了模板后，用户在使用对话框时可以先选择一个模板，这大大提高了任务的完成率。目前，大约有 50% 的用户任务都使用了模板，任务完成率提高到了 95% 以上。通过固化 Prompt、工具和知识，形成模板后，用户在下次生成或执行任务时可以先选择模板，再进行具体操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2b/2ba88582d002fa923d3743c1fe18a8a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Manus 1.5 提出了一个新概念：Agent 也是一种工具。这意味着我们可以将 Agent 视为一个工具，例如一个专门用于深度调研的工具，它可以独立完成网页搜索和内容总结。这样，主 Agent 只需要调用这个工具即可，从而将部分任务抽象化，形成一个工具。从最初的“函数即工具”，到“LLM 即工具”，再到现在的“Agent 即工具”，我们将所有任务都视为子任务，通过工具化的方式进行处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b9/b963c0210fbb37c24c8999c99fe68438.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以上内容是我关于产品和用户体验方面的分享。实际上，我们的工作不仅局限于内部，也已经向外部用户开放使用。未来，我们还将进一步把内部的技术成果开放给社区，以促进更广泛的交流与合作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/91/91505653f510411374c7db0b579f9433.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;向邦宇，阿里巴巴代码平台负责人，在代码管理、代码结构化数据处理、代码搜索、代码评审以及编辑器技术等领域拥有丰富的专业知识和实践经验。在阿里，负责了包括 CloudIDE、代码搜索、CodeReview 等多个关键产品的开发与管理，成功引领了代码智能平台的建设与发展。他主导实现的阿里内部多个 AI Coding 工具，包括 Aone Copilot 和 Aone Agent 等，在阿里内部被广泛使用。他还主导开发了 AI Development 产品“搭叩”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026，AI 正在以更工程化的方式深度融入软件生产，Agentic AI 的探索也将从局部试点迈向体系化工程建设！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;QCon 北京 2026 &lt;/a&gt;&quot;已正式启动，本届大会以“Agentic AI 时代的软件工程重塑”为核心主线，推动技术探索从「AI For What」真正落地到可持续的「Value From AI」。从前沿技术雷达、架构设计与数据底座、效能与成本、产品与交互、可信落地、研发组织进化六大维度，系统性展开深度探索。QCon 北京 2026，邀你一起，站在拐点之上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/63/63b4c3be4d6b02fdac07384dc295e338.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QtQVbAc62O1ib1V2WftO</link><guid isPermaLink="false">https://www.infoq.cn/article/QtQVbAc62O1ib1V2WftO</guid><pubDate>Fri, 13 Feb 2026 06:22:07 GMT</pubDate><author>作者：向邦宇</author><category>AI&amp;大模型</category><category>软件工程</category></item><item><title>Xcode 26.3 扩展支持编程智能体，可接入 Claude Agent、Codex 等</title><description>&lt;p&gt;Xcode 的最新版本 &lt;a href=&quot;https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/&quot;&gt;26.3 扩展了对编程代理的支持&lt;/a&gt;&quot;，可接入 Anthropic 的 Claude Agent 和 OpenAI 的 Codex，助力开发者处理复杂任务并提升生产力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;借助智能体编程，Xcode 能够以更高的自主性朝着开发者的目标推进工作——从任务分解、基于项目架构做出决策，到使用内置工具执行操作。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新版本在 Xcode 26 已引入的智能体编程能力基础上，进一步为编程智能体开放了更多 Xcode 功能的访问权限。苹果公司表示，智能体现在可以进行协作、搜索文档、浏览文件结构以及修改项目设置。此外，智能体还能通过捕获 Xcode 预览来验证代码，查看正在构建的界面效果，识别问题并在此基础上迭代优化。Anthropic 表示，这“&lt;a href=&quot;https://www.anthropic.com/news/apple-xcode-claude-agent-sdk&quot;&gt;在构建 SwiftUI 视图时尤为实用&lt;/a&gt;&quot;，因为视觉输出是重中之重”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anthropic 还强调，Xcode 26.3 集成了 &lt;a href=&quot;https://platform.claude.com/docs/en/agent-sdk/overview&quot;&gt;Claude Agent SDK&lt;/a&gt;&quot;，该 SDK 为 Claude Code 提供支持，让开发者能够使用“Claude Code 的全部功能，包括子智能体、后台任务和插件”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54cc9d20ff5a4f0861e4eace9c65f887.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Xcode 26.3 另一项重要新功能是支持模型上下文协议（Model Context Protocol，MCP），允许开发者将任意兼容 MCP 的智能体或工具与 Xcode 配合使用，也使得在 IDE 直接使用 Claude、Codex 之外的其他智能体成为可能。&lt;a href=&quot;https://developer.apple.com/documentation/xcode/giving-agentic-coding-tools-access-to-xcode&quot;&gt;MCP 集成可通过 xcrun mpcbridge 来启用&lt;/a&gt;&quot;，示例如下：&lt;/p&gt;&lt;p&gt;codex mcp add xcode -- xcrun mcpbridge&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;iOS 开发者 &lt;a href=&quot;https://www.linkedin.com/posts/akhlaq-dmg_xcode-263-unlocks-the-power-of-agentic-coding-activity-7424794057270251520-GCR1/&quot;&gt;Akhlaq Ahmad 在 LinkedIn 上指出&lt;/a&gt;&quot;，该公告似乎标志着从 AI 编程助手向更多 AI 协作伙伴变，因为智能体现在可以与 Xcode 交互，“分解目标、规划、实施、运行构建/测试，并持续优化直到代码编译成功并按预期运行”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然对 MCP 的支持有望让 Xcode 以以往无法实现的方式与外部工具交互，但 &lt;a href=&quot;https://www.reddit.com/r/iOSProgramming/comments/1quzw7n/comment/o3fgpb5/&quot;&gt;Reddit 用户 TrajansRow 提醒，其权限模型“会带来一些阻碍”&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你将 Xcode MCP 服务器添加到外部智能体系统，每次有新的智能体 PID 发出请求时，都必须手动关闭“允许‘agent’访问 Xcode？”的弹窗。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，Hacker News 的读者 drak0n1c4 反馈，&lt;a href=&quot;https://news.ycombinator.com/item?id=46887855&quot;&gt;Xcode 26.3 中的 MCP 支持“目前存在缺陷”&lt;/a&gt;&quot;，它返回的格式与其声明的 Schema 不一致，导致无法与 OpenCode 配合使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Xcode 26.3 虽可安装在旧版 macOS 上，但 AI 编程相关功能仅在运行代号为 Tahoe 的 macOS 26 时可用。Xcode 26.3 目前已通过苹果开发者网站向 Apple Developer Program 会员提供，不久后将通过 App Store 向所有开发者开放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/xcode-26-3-agentic-coding/&quot;&gt;https://www.infoq.com/news/2026/02/xcode-26-3-agentic-coding/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/39BuXj6WpGFqPERk4X9x</link><guid isPermaLink="false">https://www.infoq.cn/article/39BuXj6WpGFqPERk4X9x</guid><pubDate>Fri, 13 Feb 2026 04:00:00 GMT</pubDate><author>Sergio De Simone</author><category>后端</category></item><item><title>一线实践视角：在资源受限的环境中构建大语言模型</title><description>&lt;p&gt;在 AI 飞速发展的当下，行业讨论往往聚焦于追求规模更大、结构更复杂的大语言模型。然而，在那些基础配套设施与海量数据并非唾手可得的领域，正涌现出另一种截然不同的发展思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://qconlondon.com/presentation/apr2025/ecologies-and-economics-language-ai-practice&quot;&gt;这一叙事由 Lelapa AI 首席技术官兼联合创始人 Jade Abbott 等人提出&lt;/a&gt;&quot;，强调资源限制反而会成为自然语言模型研发中的创新催化剂。这些限制非但没有成为阻碍，在严苛条件下运行的必要性反而催生出一种务实的研发思路，这或许会重新定义我们在全球范围内构建与推广 AI 的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 开发的传统思路往往依赖庞大的计算资源、完善的云基础设施与海量数据集，而这些条件大多只存在于语言生态成熟的环境中。这种模式虽在特定场景下有效，却忽视了非洲大陆等地区所独有的挑战与机遇。在这些地区，电力供应不稳定、高速互联网未普及、数百种语言缺乏数字化语料，这些现实都要求我们对 AI 开发方式进行根本性的重新思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Abbott 的工作提供了一个极具参考价值的案例，展示了如何在不照搬西方模式的前提下应对这些复杂问题，开辟出全新路径，将效率、可及性与文化适配性放在优先位置，尤其是在基础配套设施和模型训练数据都十分匮乏的情况下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们“分而治之”的理念、创新的合成数据生成方法、战略性的模型选择，以及在资源受限环境中稳健评估与持续改进的核心思路，共同确保了模型能够贴合实际需求、解决真实问题，不受地域与资源条件的限制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该公司采用务实、以问题为中心的方法，与&lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_general_intelligence&quot;&gt;通用人工智能（AGI）&lt;/a&gt;&quot;抽象、通用特性形成鲜明对比，包括细致定义待解决的具体问题，再针对现有约束条件优化工程解决方案。这种“分而治之”的思路并非只是理论概念，而是已深入落地的技术实践，影响着其大语言模型开发的方方面面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下段落详细阐述了这种思维方式在实践中的体现：在电力与网络不稳定的环境中运行模型，在数据缺失时主动创造数据，并依据反馈持续迭代系统。这一实践充分证明，明确的约束、扎实的工程能力与局部洞察相结合，便能打造出在现实世界中真正行之有效的 AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;通过提升工程效率应对基础设施匮乏&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非洲大陆面临一系列独特的基础设施难题，亟需创新的工程解决方案。与电力、互联网普及且稳定的地区不同，这里许多区域存在供电不稳、网络受限的问题。这一现实直接影响了大型依赖云计算的大语言模型的部署与运行可行性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对务实的技术实践者而言，这意味着需要高度优化、低功耗的模型，使其能够在边缘设备上运行，或尽可能减少对持续云服务的依赖。研究问题也从“我们的模型能有多强大？”转变为“在现有能源与网络限制下，我们的模型如何创造价值？”这通常会涉及以下技术：&lt;/p&gt;&lt;p&gt;模型量化：降低模型中数值表示的精度（例如从 32 位浮点数转换为 8 位整数），可以显著减少内存占用与计算开销，使模型能够在性能较弱的硬件上运行。模型蒸馏：通过训练较小的“学生”模型，模仿更大、更复杂的“教师”模型的行为，可将高性能但资源消耗大的模型知识，迁移到更高效、更适合在资源受限环境中部署的模型中。边缘部署策略：设计可直接在移动设备或本地服务器上运行的大语言模型，尽量减少与远程数据中心的持续通信。这需要对模型架构、推理优化进行细致考量，同时针对文本转语音、基础翻译等特定任务实现离线运行能力。异步数据同步：对于确实需要一定网络连接的模型，采用可靠的异步数据同步机制，确保在网络可用时高效交换更新与新数据，而非依赖持续在线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些技术并非只是理论探讨，而是让 AI 能够在每瓦特功耗、每字节传输都至关重要的场景下真正落地部署的基础工程实践。其核心是在真实的运行条件下实现实用价值，而非不计成本地追求理论性能极限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;应对数据稀缺：合成数据生成的技术与应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为非洲语言开发大语言模型最主要的技术障碍之一是数字化语言数据极度稀缺。历史上，许多土著语言很少被文字记录，而殖民影响进一步压制了其书面形式的发展。这导致 AI 开发者缺乏支撑英语等主流大语言模型训练所需的大规模文本语料库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Abbott应对这一挑战的解决方案是有针对性地生成高质量合成数据。这并非随机生成文本，而是一套经过精心工程化的流程，所生成的数据既贴合特定应用场景，又能代表相应的人口统计特征。这种方法不仅适用于稀有语言，也适用于受隐私问题或法规保护的敏感数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/podcasts/build-effective-llms-infrastructure-data/&quot;&gt;这个看似“枯燥”却很实用的案例&lt;/a&gt;&quot;是为约翰内斯堡开发呼叫中心转录模型。传统方案需要采集并转录大量真实的呼叫中心音频，但受隐私法规限制，加之人工转录成本极高，往往难以落地。这种场景需要一套包含以下内容的解决方案：&lt;/p&gt;&lt;p&gt;问题定义：明确界定问题的范围——例如，转录特定语言或方言的呼叫中心对话，聚焦特定类型的咨询，并限定来电者的年龄区间。人机回环数据创建：该公司并非完全依赖算法生成数据，而是雇佣团队（通常为前呼叫中心坐席）来模拟呼叫中心的互动场景。这些工作人员会依据脚本和指导方针，分别扮演坐席与来电者，生成高度贴近真实对话的音频数据。这一方式能确保数据捕捉到自然的语音模式、口音及特定领域的专业术语。受控环境模拟：搭建可模拟呼叫中心环境的系统，实现音频数据的可控生成。该系统可调整背景噪音、通话质量与说话人特征，从而构建出稳健且多样化的数据集。迭代改进：在模型部署并收集反馈后开展错误分析。若模型在特定语言细节或嘈杂环境下表现不佳，就优化数据生成流程，补充更多能解决这些问题的样本。这种迭代反馈循环可确保合成数据在质量和相关性上持续提升。用于数据生成的特征提取：当真实客户数据因隐私问题受到严格保护时，可从中提取关键特征与属性，无需直接访问敏感内容。这些特征将为新合成数据的生成参数与指导规则提供依据，确保生成数据能够体现受保护真实数据的统计特征与语言模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种实操性的数据生成方法虽在人力资本上投入较大，但能生成高度针对性、且符合伦理规范的数据集，而这些数据原本难以获取。它体现了从数据收集到数据创建的根本性转变，这也是任何在数据稀缺环境下工作的技术人员所必备的关键能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;战略模型选择与持续改进&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基础模型的选择是一项关键决策，需要基于对现有约束条件的务实判断。一味追求选择体量最大、宣传最广的模型，往往会适得其反，尤其在数据与计算资源有限的情况下。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于技术专家来说，模型选择过程涉及：&lt;/p&gt;&lt;p&gt;定义操作约束：在评估模型之前，先明确定义运行环境：延迟要求是多少？可用硬件有哪些（CPU、GPU、内存）？功耗限制是多少？这些约束条件决定了模型可行的大小与复杂度范围。对较小模型进行基准测试：不要从最大的模型入手，而是先评估Hugging Face等平台上现有的更小、更高效的模型。这类模型通常能提供不错的基线效果，且微调所需的资源要少得多。性能与资源的权衡：要理解模型性能、大小与计算需求之间存在持续的权衡关系。在资源受限的环境中，一个精度稍低但更快、更小的模型，往往比精度略高却体积过大、难以部署的模型更具实用价值。特定领域预训练：这个案例研究表明，针对特定领域或特定语言进行预训练能够显著提升模型在上下文敏感型应用中的表现。以非洲语言（如斯瓦希里语）为核心预训练的小模型，在面向相关非洲语言任务微调后效果往往优于规模更大、但以英语为中心的模型。这一结果凸显了基础训练数据在语言与文化上保持对齐的重要性。迭代实验和错误分析：模型选择过程很少是一次性的决策。它涉及：&lt;/p&gt;&lt;p&gt;候选选择：筛选出若干个满足初始约束条件、具备应用潜力的模型。快速原型设计和微调：在生成的合成数据上对这些候选模型进行微调。定性错误分析：除了定量指标外，还应对模型错误进行定性分析：它主要出现哪类错误？这些错误能否通过补充数据、更换微调方法或调整模型架构来解决？战略杠杆：根据错误分析结果，确定需要采取哪些优化手段：生成更具针对性的数据，应用模型优化技术（量化、蒸馏），或放弃当前模型，改用其他架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种迭代式、数据驱动的方法，能够确保为当前问题找到最优的可用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“AI缺陷”的演变定义与持续集成&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 中“缺陷”的概念与传统软件工程有着本质区别。在传统软件中，缺陷通常是二元的：要么已修复，要么未修复。而在 AI 领域，性能是按梯度衡量的，“错误”往往只是特定场景下 1% 的准确率下降，而非彻底的系统故障。这种精细化的理解，对于将 AI 集成到持续改进流程中至关重要。因此，管理 AI “缺陷”所采用的方法主要包括：&lt;/p&gt;&lt;p&gt;将用户反馈封装为测试集：当用户报告问题（例如“模型在 X、Y 场景下表现不佳”）时，这类反馈不会被当作孤立事件处理。相反，它会被转化为用于定位该问题的小型代表性测试集，并成为评估套件中的永久组成部分。梯度式进度追踪：这些“缺陷”测试集并不采用二元的“已修复 / 未修复”状态，而是基于百分比进行评估。一个模型在某类缺陷上可能呈现出 70% 的改进效果，这表明即便问题尚未完全解决，也已取得明显进展。这为模型迭代提供了更贴合实际、更具可操作性的视角。构建&quot;缺陷数据库&quot;：随着时间的推移，这些小型测试集将逐步积累成一个庞大的数据库。该数据库可充当全面的安全保障，确保新模型在部署时，能够持续针对各类已知问题和边缘场景进行评估。集成到CI/CD中：每个候选模型在部署前都会基于这个完整的“缺陷数据库”进行测试。这为 AI 建立了持续集成机制，让开发团队乃至业务相关方都能清晰理解模型改动在各个问题领域所带来的影响。战略性资源分配：缺陷数据库的结果可为战略决策提供依据。如果某类缺陷反复出现或仅取得有限改进，可据此决定为该场景投入更多数据生成资源、探索不同模型架构，或采用更激进的优化技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种将软件缺陷理念适配到机器学习领域并将其融入持续反馈循环的做法是构建可靠、负责任的AI系统的关键一步。它跳出了抽象的性能指标，转向具体、与业务相关的评估，为管理AI开发中固有的不确定性提供了实用框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在多维度场景中衡量影响&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于任何科技公司，尤其是向消费者提供各类产品（包括开源产品与商业产品）的企业而言，衡量自身影响力至关重要。其有效性唯有通过多维度方法进行评估，超越简单指标，才能真正体现工作所带来的更广泛价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从实践角度来看，这涉及：&lt;/p&gt;&lt;p&gt;用户参与度指标：对于商业服务而言，追踪增值对话量、模型使用频率、用户留存率等指标，能够直观反映大语言模型的实用性与应用情况。开源采用情况：对于开源发布的模型与框架，GitHub、Hugging Face等平台上的下载量、分叉数和贡献数等指标，能够反映其社区参与度与更广泛的技术影响力。研究与出版物：通过学术论文与出版物传播知识，有助于推动学术交流、树立思想领导力。引用量、阅读量等指标可作为衡量这种学术影响力的依据。叙事转变与倡导：除直接技术成果外，该公司还积极致力于改变非洲人工智能领域的发展叙事。这包括公开演讲、政策参与，以及倡导更具包容性与伦理规范的人工智能实践。尽管这类“叙事影响力”较难量化，但它对培育健康的支持性生态系统至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种多维度的影响力评估方法能够体现整体工作如何与技术进步、社会效益和应用创新相互融合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;联邦学习：理想的前沿&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望未来，人们正积极探索将联邦学习作为一种持续优化模型的机制，尤其适用于部署在网络连接不稳定的移动设备上的模型。联邦学习可让多个持有本地数据样本的去中心化设备协同训练模型，且无需交换原始数据，仅将模型更新（如参数权重变化）上传至中央服务器，从而有效保护用户隐私。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管对于NLP领域的真实应用场景而言，这在很大程度上仍偏理想化，但其技术影响十分重大：&lt;/p&gt;&lt;p&gt;隐私保护更新：用户数据保留在设备本地，解决了关键的隐私问题，在数据保护法规不断完善的地区尤为重要。持续的设备端改进：模型可以直接从设备端的真实使用模式中学习并自适应，长期来看能够实现更个性化、更精准的效果。克服连接障碍：模型更新可进行批量处理，并在网络连接恢复时传输，让系统在间歇性网络环境下仍能稳定运行。去中心化智能：这种方法推动了更去中心化的AI生态系统，减少对集中式云基础设施的依赖，并为本地社区提供更贴合需求、响应更及时的AI工具。&lt;/p&gt;&lt;p&gt;将联邦学习成功应用于AI模型将是一次重大的技术飞跃，尤其在资源受限的环境中，可让模型持续迭代，适配多样的语言与语境差异，同时无需牺牲用户隐私，也不依赖持续的网络连接。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本案例研究概述了在现实约束条件下开发AI系统的实用框架，阐述了有限的基础设施、数据稀缺以及效率要求等挑战，如何推动形成更具针对性的设计方案与迭代式工程实践。综合来看，这些案例表明：AI的进步往往更少依赖规模，而更多取决于目标明确性、严谨的实验以及贴合场景的问题解决能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管非洲大陆严苛的技术条件看似与西方环境截然不同，但深入观察后可以发现，Lelapa AI所采用的方法，同样适用于发达经济体中监管严格的场景，尤其是已实施隐私相关立法的地区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们通过务实解决各类问题，在尽可能多的场景中为用户创造价值，证明了即便在传统资源稀缺的条件下，依然能够构建并推广具有影响力的 AI 系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本案例研究得出的经验教训并不局限于特定地理背景，而是适用于所有希望构建可靠、合规且有价值的AI解决方案的技术人员与组织。将约束视为创新的催化剂、精准定义问题、为效率而设计、通过严谨评估推动持续学习，我们就能跳出单纯追求规模的误区，打造出真正满足人类多元需求的人工智能。AI 的未来不在于打造更大的模型，而在于为所有人开发更智能、更具适应性、更易获取的智能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/building-llms-resource-constrained-environments/&quot;&gt;https://www.infoq.com/articles/building-llms-resource-constrained-environments/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zjLVIHKAhr7B9qTo39Dp</link><guid isPermaLink="false">https://www.infoq.cn/article/zjLVIHKAhr7B9qTo39Dp</guid><pubDate>Fri, 13 Feb 2026 02:00:00 GMT</pubDate><author>作者：Olimpiu Pop</author><category>AI&amp;大模型</category></item><item><title>Conductor Quantum 推出量子计算自然语言交互接口 Coda</title><description>&lt;p&gt;Conductor Quantum 发布 &lt;a href=&quot;https://conductorquantum.substack.com/p/coda-natural-language-quantum-computing&quot;&gt;Coda&lt;/a&gt;&quot;，一个用于在真实量子硬件上运行量子程序的自然语言接口。该系统作为一个软件层，可将用户的高层级意图转化为可执行的量子电路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助 Coda，用户只需用自然语言描述待解决的问题，平台便会将其转换为量子电路，并依据正确性要求和硬件约束进行验证，随后提交至可用的量子处理器执行。据 Conductor Quantum 介绍，该系统旨在降低设置、编排及底层编程的负担，同时保留对底层量子操作的可见性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;公告发布后，Abdul Moid Ahmed 在 X 上&lt;a href=&quot;https://x.com/indieshipx247/status/2014269896339955788?s=20&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;以自然语言作为交互接口颇具吸引力，但真正的挑战在于：随着问题复杂度提升，用户意图能否准确转化为正确的量子电路。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次发布正值量子硬件持续取得进展之际，量子比特数量不断增加、可靠性持续提升，云托管量子系统也日益普及。Conductor Quantum 指出，软件工具的发展长期滞后于硬件进步，许多现有平台仍要求用户深入掌握量子编程模型及设备特定的约束条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coda 被设计为在现有量子 SDK 和硬件供应商平台之上运行。它并非要取代这些工具，而是充当一个编排层，负责生成并执行量子程序，使用户无需直接管理完整的软硬件堆栈。针对希望深入了解的用户，Coda 还提供了学习模式，可解释电路的构建逻辑、特定操作的选择原因，以及如何解读系统输出的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布之初，Coda 即支持访问 Rigetti 的 84 量子比特量子系统。该平台还集成了 NVIDIA 的 cuQuantum 库和 CUDA-Q 平台，可对多达 34 个量子比特进行模拟，方便用户在正式提交量子硬件运行前，迭代优化工作负载并测试混合量子-经典工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coda 基于多年的量子控制软件开发经验打造。Conductor Quantum 此前已为硅量子芯片控制开发 API，与 SemiQon 合作在 64 个量子设备上部署软件，并向多家公司提供控制系统。这些技术积累为 Coda 的电路生成、执行验证及硬件限制处理奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Conductor Quantum 计划从三方面扩展 Coda：一是加强 GPU 经典计算与量子处理单元的集成，二是深化与设备级控制及调优软件的衔接，三是在更大规模量子系统问世时及时扩展支持。该公司表示，其目标是随量子硬件持续升级，不断拉近用户意图与实际执行之间的距离。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coda 目前可通过 &lt;a href=&quot;https://coda.conductorquantum.com/&quot;&gt;Conductor Quantum 的平台&lt;/a&gt;&quot;获取。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/conductor-quantum-coda/&quot;&gt;https://www.infoq.com/news/2026/02/conductor-quantum-coda/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zxyv3HhCW1qkEIi44JBP</link><guid isPermaLink="false">https://www.infoq.cn/article/zxyv3HhCW1qkEIi44JBP</guid><pubDate>Fri, 13 Feb 2026 00:00:00 GMT</pubDate><author>作者： Robert Krzaczyński</author><category>自然语言处理</category></item><item><title>2025年的Web开发：AI的React偏见 vs 原生Web</title><description>&lt;p&gt;随着越来越多的开发者寻求React生态系统之外的解决方案，像Astro和Svelte这样的前端框架越来越受欢迎，今年Web开发的复杂性进一步降低。与此同时，原生Web平台的特性证明了它们能够胜任构建复杂的Web应用程序的工作——尤其是CSS在2025年得到特别改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;话虽如此，也许今年最大的Web开发趋势是AI辅助编码的兴起——事实证明，它倾向于默认使用React和领先的React框架Next.js。因为React在前端领域占据主导地位，大语言模型（LLM）有很多React代码进行训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我们更详细地看看2025年的五大Web开发趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;1. 原生Web特性的崛起&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年，许多原生Web特性悄然赶上了JavaScript框架提供的功能。例如，&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API&quot;&gt;视图&lt;/a&gt;&quot;、转换&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API&quot;&gt;API&lt;/a&gt;&quot; ——它可以让你的网站在页面之间流畅地切换——成为了&lt;a href=&quot;https://webstatus.dev/?q=baseline_date%3A2025-01-01..2025-12-31&quot;&gt;Baseline 2025&lt;/a&gt;&quot;索引跨浏览器支持的一部分。因此，现在Web开发人员可以广泛使用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Baseline是一个由W3C的WebDX社区小组协调的项目，包括来自&lt;a href=&quot;https://cloud.google.com/?utm_content=inline+mention&quot;&gt;谷歌&lt;/a&gt;&quot;、Mozilla、微软和其他组织的代表。它从2023年才开始运行，&lt;a href=&quot;https://thenewstack.io/interop-unites-browser-makers-to-smooth-web-inconsistencies/&quot;&gt;但今年它真正成为了实践Web开发人员的有用资&lt;/a&gt;&quot;源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/6568cddec29d68c4020dc2f6710d7ac4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Baseline特性的稳定年度增长，&lt;a href=&quot;https://webstatus.dev/stats&quot;&gt;通过Web平台状态站点观测&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如The New Stack的Mary Branscombe&lt;a href=&quot;https://thenewstack.io/baseline-newly-available-stay-on-top-of-new-web-features/&quot;&gt;在6月份所报道&lt;/a&gt;&quot;的那样，有很多方法可以跟踪Baseline的变化：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“谷歌的Web.Dev有关于Baseline特性和新闻的&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/release-notes/march-2025/&quot;&gt;月度更新&lt;/a&gt;&quot;，WebDX特性浏览器允许你查看&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/widely-available/&quot;&gt;有限可用、新可用或广泛可用&lt;/a&gt;&quot;的特性；&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/release-notes/march-2025/&quot;&gt;月度发布&lt;/a&gt;&quot;说明涵盖了哪些特性达到了新的Baseline状态。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Web功能的角度来看，现在真的没有理由不使用原生Web功能。正如资深Web开发人员&lt;a href=&quot;https://adactio.com/journal/22235&quot;&gt;Jeremy Keith&lt;/a&gt;&quot;最近所说，框架“限制了你在web浏览器中所能做的事情的可能性空间”。在随后的一篇文章中，Keith敦促开发人员尤其不要在浏览器中使用React，因为文件大小对用户来说成本太高了。相反，他鼓励开发人员“研究在浏览器中可以使用纯JavaScript做些什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2. AI编码助手默认为React&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年，AI成为了Web开发工具链的标准组成部分（尽管并不总是得到开发者的认可，特别是那些在Mastodon或Bluesky而不是X或LinkedIn上社交的开发者）。无论你是不是应用程序开发中的AI粉丝，都有一个大问题：LLMs倾向于默认使用React和Next.js。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当OpenAI的&lt;a href=&quot;https://thenewstack.io/gpt-5-a-choose-your-own-adventure-for-frontend-developers/&quot;&gt;GPT-5&lt;/a&gt;&quot;在8月发布时，其所谓的优势之一是编码。GPT-5最初从开发者那里获得了褒贬不一的评价，所以在那个时候，我联系了OpenAI，向他们询问编码特性。OpenAI的研究员&lt;a href=&quot;https://www.linkedin.com/in/ishaan-singal/&quot;&gt;Ishaan Singal&lt;/a&gt;&quot;通过电子邮件回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我向Singal指出，在&lt;a href=&quot;https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide&quot;&gt;GPT-5&lt;/a&gt;&quot;提示指南中，有三个推荐的框架：Next.js（TypeScript）、React和HTML。我问是否有与Next.js和React项目团队合作，以优化GPT-5对这些框架的支持？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们选择这些框架是基于它们的受欢迎程度和通用性，但我们并没有直接与Next.js或React团队在GPT-5上合作，”他回答说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c5/c58dfbb6641b7ba07d9d13ffa16dddb1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI的GPT-5提示指南中的“组织GPT-5代码编辑规则”的示例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们知道，负责&lt;a href=&quot;https://thenewstack.io/vercels-frontend-and-the-rise-of-the-hybrid-developer/&quot;&gt;Next.js&lt;/a&gt;&quot;框架的公司Vercel是GPT-5的粉丝。在发布当天，它称GPT-5是“最好的前端AI模型”。所以这里发生了一个很好的交换条件——GPT-5之所以能够成为Next.js的专家，是因为它的受欢迎程度，这可能进一步增加了它的受欢迎程度。这对OpenAI和Vercel都有帮助。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“归根结底，这是开发者的选择，”Singal总结道，关于开发者想要使用哪些Web技术。“但成熟的代码库有更好的社区支持。这有助于开发者自助维护。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;3. AI智能体和聊天机器人中web应用的出现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年，我们看到了AI聊天机器人和智能体中小型Web应用的出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mcpui.dev/&quot;&gt;MCP-UI&lt;/a&gt;&quot;是Web将成为AI智能体关键部分的第一个迹象。顾名思义，&lt;a href=&quot;https://mcpui.dev/&quot;&gt;MCP-UI&lt;/a&gt;&quot;使用流行的模型上下文协议作为通信基础。&lt;a href=&quot;https://mcpui.dev/guide/introduction&quot;&gt;该项目&lt;/a&gt;&quot;“旨在标准化模型和工具如何在客户端应用程序中请求显示丰富的HTML界面。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/mcp-ui-creators-on-why-ai-agents-need-rich-user-interfaces/&quot;&gt;在8月的一次采访中&lt;/a&gt;&quot;，两位创始人（其中一位当时在Shopify工作）解释说，MCP-UI有两种类型的SDK：客户端SDK和连接到MCP服务器的服务器SDK。服务器SDK提供TypeScript、Ruby和Python版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/af16ee40af1d4efb52383ea105019ece.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个UI被插入到Claude 3.7 Sonnet聊天中的MCP-UI演示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP-UI听起来很有前途，但很快就被OpenAI 10月初发布的&lt;a href=&quot;https://developers.openai.com/apps-sdk&quot;&gt;Apps SDK&lt;/a&gt;&quot; 盖过了风头。&lt;a href=&quot;https://developers.openai.com/apps-sdk&quot;&gt;Apps SDK&lt;/a&gt;&quot; 允许第三方开发者构建基于Web的应用程序，这些应用程序作为ChatGPT对话中的交互式组件运行——这让我们想起了2008年苹果推出应用商店时的情景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Apps SDK的决定性特征是基于Web的UI模型（类似于MCP-UI）。ChatGPT应用组件是一个Web UI，运行在ChatGPT对话中的沙箱框架中。ChatGPT作为应用程序的主机。你可以将第三方ChatGPT应用程序视为直接嵌入ChatGPT界面的“迷你Web应用程序”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到10月底，像Vercel这样的行业巨头已经想出了如何使用他们的JavaScript框架来构建&lt;a href=&quot;https://thenewstack.io/next-js-in-chatgpt-vercel-brings-the-dynamic-web-to-ai-chat/&quot;&gt;ChatGPT&lt;/a&gt;&quot;应用程序。Vercel将Next.js与ChatGPT应用程序平台的快速集成表明，AI聊天机器人将不仅仅局限于轻度交互的小部件——复杂的Web应用程序也将在这些平台上存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;4. 浏览器中的Web AI和设备上的推理&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年的另一个并行发展是&lt;a href=&quot;https://thenewstack.io/how-google-is-shifting-ai-from-the-cloud-to-your-browser/&quot;&gt;在浏览器中运行客户端AI的兴起&lt;/a&gt;&quot;，这允许LLM推理在设备上进行。谷歌在这方面尤为突出；它对这种趋势的称呼是“Web AI”。&lt;a href=&quot;https://www.linkedin.com/in/webai/&quot;&gt;Jason Mayes&lt;/a&gt;&quot;，谷歌这些举措的负责人，将&lt;a href=&quot;https://www.linkedin.com/pulse/life-edge-web-ai-history-future-smarter-digital-agentic-jason-mayes-fbqbc/&quot;&gt;Web AI&lt;/a&gt;&quot;定义为“通过Web浏览器在用户设备上运行任何机器学习模型或服务的艺术。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;11月，谷歌举办了一场仅限受邀者参加的活动，名为谷歌&lt;a href=&quot;https://www.linkedin.com/pulse/life-edge-web-ai-history-future-smarter-digital-agentic-jason-mayes-fbqbc/&quot;&gt;Web AI&lt;/a&gt;&quot;峰会。之后，我采访了活动的组织者兼主持人Mayes，他解释说，一个关键技术是LiteRT.js，谷歌的Web AI运行时，目标是生产Web应用程序。它建立在&lt;a href=&quot;https://ai.google.dev/edge/litert&quot;&gt;LiteRT&lt;/a&gt;&quot;的基础上，后者旨在直接在设备（移动、嵌入式或边缘）上运行机器学习（ML）模型，而不是依赖于云推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Web AI峰会的主题演讲中，谷歌负责Chrome和Web生态系统的副总裁兼总经理&lt;a href=&quot;https://www.linkedin.com/in/parisatabriz/&quot;&gt;Parisa Tabriz&lt;/a&gt;&quot;强调了去年8月Chrome内置的&lt;a href=&quot;https://thenewstack.io/googles-web-ai-playbook-the-paved-road-vs-the-open-field/&quot;&gt;AI API&lt;/a&gt;&quot;，以及去年6月发布的作为Chrome内置功能的Gemini Nano——谷歌的主要设备上模型。这些和其他Web技术正在推动当前的Web AI趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7ae02d903acc9c4fe0e82d469631c54f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Parisa Tabriz在Web AI峰会上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌与微软一起参与的另一项创新是&lt;a href=&quot;https://thenewstack.io/how-webmcp-lets-developers-control-ai-agents-with-javascript/&quot;&gt;WebMCP&lt;/a&gt;&quot;的发布，它允许开发人员使用客户端JavaScript控制AI智能体如何与网站交互。在9月与微软Edge的Web平台产品经理&lt;a href=&quot;https://www.linkedin.com/in/kylepflug/&quot;&gt;Kyle Pflug&lt;/a&gt;&quot;的采访中，他解释道：“核心概念是允许Web开发者用JavaScript为他们的网站定义‘工具’，就像传统MCP服务器提供的工具一样。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Web AI不仅仅是由商业公司推广。万维网联盟（W3C）也在探索“&lt;a href=&quot;https://thenewstack.io/the-agentic-web-how-ai-agents-are-shaping-the-webs-future/&quot;&gt;代理式Web&lt;/a&gt;&quot;”的构建模块，其中包括使用MCP-UI、WebMCP和另一个新兴的称为&lt;a href=&quot;https://thenewstack.io/cloudflares-balancing-act-protect-content-while-pushing-ai/&quot;&gt;NLWeb&lt;/a&gt;&quot;的标准（由微软开发）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;5. JavaScript生态系统的“生命化”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这听起来像是AI主导了今年的web开发——事实上也确实如此。但前端工具也看到了它的创新份额。有一款产品特别引人注目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://vite.dev/&quot;&gt;Vite&lt;/a&gt;&quot;，由&lt;a href=&quot;https://evanyou.me/?utm=22b03&quot;&gt;Evan You&lt;/a&gt;&quot;创建，已经成为现代前端框架的首选构建工具，包括Vue、SvelteKit、Astro和React——也有来自Remix和Angular的实验性支持。在9月份接受&lt;a href=&quot;https://thenewstack.io/how-vite-became-the-backbone-of-modern-frontend-frameworks/&quot;&gt;The New Stack&lt;/a&gt;&quot;采访时，You告诉我，Vite成功的关键在于它早期使用了ES模块 (ESM)，这是一种标准化的JavaScript模块系统，允许你“将JavaScript代码分解成不同的片段，不同的模块，你可以加载。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2d/2d49491dee779acc4fc1a378385f4cad.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Even You在ViteConf上展示的Vite生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;You和他的公司VoidZero现在正在构建 Vite+，一个新的统一JavaScript工具链，旨在解决JavaScript碎片化问题。在今年的ViteConf 活动上，You正式推出了Vite+，将其定位为企业开发工具包。他说它包括“你喜欢Vite的一切——加上你一直在用胶带粘合在一起的一切。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Web开发的十字路口&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年底，感觉我们正处于前端开发的十字路口。一方面，有一种方法可以解决React的复杂性难题：使用原生Web特性和工具，如 Astro，减轻用户的负担。虽然这确实是今年的一个趋势，但它有可能在2026年被我们越来越依赖AI工具编码所掩盖——正如所指出的，这些工具倾向于依赖React。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实是，现在的大多数开发者——包括成千上万以前不属于开发者生态系统的“vibe程序员”——将继续由AI系统提供React代码。这使得Web开发社区在明年继续支持和倡导原生Web代码变得更加必要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/web-development-in-2025-ais-react-bias-vs-native-web/&quot;&gt;https://thenewstack.io/web-development-in-2025-ais-react-bias-vs-native-web/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SIQ9aJiSeqplKcOeAAmM</link><guid isPermaLink="false">https://www.infoq.cn/article/SIQ9aJiSeqplKcOeAAmM</guid><pubDate>Thu, 12 Feb 2026 10:21:41 GMT</pubDate><author>Richard MacManus</author><category>生成式 AI</category><category>架构/框架</category></item><item><title>“攻击迫在眉睫”，39%的云环境存在最高严重性的React漏洞</title><description>&lt;p&gt;广泛使用的JavaScript库React以及包括&lt;a href=&quot;http://next.js/&quot;&gt;Next.js&lt;/a&gt;&quot;在内的几个基于React的框架存在一个最高严重性的漏洞，允许未经身份验证的远程攻击者在易受攻击的实例上执行恶意代码。安全研究人员表示，这个漏洞很容易被滥用，大规模利用“迫在眉睫”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React团队在周三披露了React Server Components中的未经身份验证的远程代码执行（RCE）漏洞。它被跟踪为&lt;a href=&quot;https://www.cve.org/CVERecord?id=CVE-2025-55182&quot;&gt;CVE-2025-55182&lt;/a&gt;&quot;，并获得了最高的10.0的CVSS严重性评级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一个大问题，因为大部分互联网都是建立在React之上的——据估计，39%的云环境容易受到这个漏洞的影响。因此，这个问题应该在你的待办事项列表中占据一个突出的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞影响了19.0、19.1.0、19.1.1和19.2.0版本的：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-webpack&quot;&gt;react-server-dom-webpack&lt;/a&gt;&quot;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-parcel&quot;&gt;react-server-dom-parcel&lt;/a&gt;&quot;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-turbopack?activeTab=readme&quot;&gt;React-server-dom-turbopack&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它还影响了包括&lt;a href=&quot;https://www.npmjs.com/package/next&quot;&gt;next&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/react-router&quot;&gt;react-router&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/waku&quot;&gt;waku&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/@parcel/rsc&quot;&gt;@parcel/rsc&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/@vitejs/plugin-rsc&quot;&gt;@vitejs/plugin-rsc&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.npmjs.com/package/rwsdk&quot;&gt;rwsdk&lt;/a&gt;&quot;在内的几个React框架和打包器的默认配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;项目的维护者表示，升级到&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.0.1&quot;&gt;19.0.1&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.1.2&quot;&gt;19.1.2&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.2.1&quot;&gt;19.2.1&lt;/a&gt;&quot;版本可以修复这个漏洞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们建议立即升级，”React团队在周三的安全咨询中表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“CVE-2025-55182对世界上使用最广泛的Web应用程序框架之一的用户构成了重大风险，”风险管理工具供应商watchTowr的创始人兼首席执行官Benjamin Harris告诉The Register。“利用几乎不需要先决条件，[并且]毫无疑问，一旦攻击者开始分析现在公开的补丁，就会立即进行野外利用。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Next.js的创建者和主要维护者Vercel为该漏洞分配了自己的CVE（&lt;a href=&quot;https://github.com/vercel/next.js/security/advisories/GHSA-9qr9-h5gf-34mp&quot;&gt;CVE-2025-66478&lt;/a&gt;&quot;），并在周三发布了警报和补丁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然我们没有太多关于这个漏洞的细节，但我们知道它滥用了React解码发送到React Server Function端点的有效载荷的缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未经身份验证的攻击者可以制作一个恶意的HTTP请求到任何Server Function端点，当被React反序列化时，可以在服务器上实现远程代码执行，”安全警报警告说。“有关该漏洞的更多细节将在修复完成后提供。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周六，研究员Lachlan Davidson发现并报告了这个缺陷给Meta，Meta创建了这个开源项目。Meta与React团队合作，在四天后迅速推出了一个紧急补丁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React的应用非常广泛——Meta的Facebook和Instagram、Netflix、Airbnb、Shopify、Hello Fresh、Walmart和Asana都依赖于它，数百万开发者也依赖于它——许多框架都依赖于易受攻击的React包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，这个CVE将大部分互联网置于危险之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Wiz数据表明，39%的云环境包含易受CVE-2025-55182和/或CVE-2025-66478影响的Next.js或React的实例，”云安全商店的威胁猎人Gili Tikochinski、Merav Bar和Danielle Aminov在周三表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.theregister.com/2025/11/05/googles_32b_wiz_acquisition_its/&quot;&gt;这家即将被谷歌收购的公司&lt;/a&gt;&quot;对这个漏洞进行了实验和修复，并报告说“利用这个漏洞的保真度很高，成功率接近100%，可以利用它来执行完整的远程代码。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“由于其严重性和易利用性，需要立即打补丁，”三人补充说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在撰写本文时，The Register没有发现任何野外利用的报告。然而，可以肯定的是，犯罪分子已经在逆向工程补丁，并在互联网上扫描暴露的、易受攻击的实例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“由于React和Next.js等框架的广泛使用，这个漏洞预计将引起极大的关注，”Rapid7的高级首席研究员Stephen Fewer告诉The Register。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术细节和利用代码被公开的可能性很高，因此利用可能很快就会发生，”他说。“因此，立即修补这个漏洞至关重要。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare声称，如果他们的React应用程序流量是通过WAF代理的，那么他们的Web应用程序防火墙（WAF）可以保护他们免受该漏洞的侵害。®&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/?td=rt-3a&quot;&gt;https://www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/?td=rt-3a&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/qRHs8o3kirzwSImTe7iO</link><guid isPermaLink="false">https://www.infoq.cn/article/qRHs8o3kirzwSImTe7iO</guid><pubDate>Thu, 12 Feb 2026 10:15:19 GMT</pubDate><author>Jessica Lyons</author><category>架构/框架</category></item><item><title>RFC 规范中的 CNAME 顺序问题是如何导致 Cloudflare 1.1.1.1 宕机的</title><description>&lt;p&gt;在一篇题为&lt;a href=&quot;https://blog.cloudflare.com/cname-a-record-order-dns-standards/&quot;&gt;《先有 CNAME，还是先有 A 记录？》&lt;/a&gt;&quot;的最新文章中，Cloudflare 解释了一个 RFC 规范表述不清的问题，是如何导致其广受欢迎的 1.1.1.1 公共 DNS 服务发生故障的。在定位问题并发现旧版 DNS 标准中关于记录顺序的模糊之处后，Cloudflare 提出了一份澄清后的规范建议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日，一次看似例行的 DNS 服务更新改变了响应中 CNAME 记录出现的顺序，导致部分 DNS 客户端在解析域名时失败，因为它们假定别名记录必须先出现。尽管大多数现代软件认为 DNS 响应中记录的顺序并不重要，但 Cloudflare 团队发现，一些实现实际上依赖 CNAME 记录必须出现在其他记录类型之前。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当这一顺序发生变化后，DNS 解析开始失败，最终引发了 1.1.1.1 这一流行公共 DNS 服务的一次严重中断。Cloudflare 的系统工程师 &lt;a href=&quot;https://www.linkedin.com/in/sebastiaan-n/&quot;&gt;Sebastiaan Neuteboom&lt;/a&gt;&quot; 解释了该变更引入的原因和时间点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在对缓存实现进行一些降低内存占用的改进时，我们引入了一个关于 CNAME 记录顺序的细微变化。该变更于 2025 年 12 月 2 日引入，12 月 10 日发布到测试环境，并于 2026 年 1 月 7 日开始部署。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当 DNS 解析器查询一个包含 CNAME 的域名时，它可能会看到一系列别名记录，将原始名称一路链接到最终的地址，并且解析器会以不同的过期时间缓存链路中的每一步。Cloudflare 指出，如果这条链中的某一部分在缓存中已过期，解析器只会重新获取过期的那一段，并与仍然有效的部分组合，形成完整的响应。Neuteboom 补充道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;之前的代码会创建一个新的列表，先插入已有的 CNAME 链，然后再追加新获取的记录（……）。但为了减少内存分配和拷贝，代码被修改为直接把 CNAME 追加到现有的 answer 列表中。结果是，1.1.1.1 返回的响应中，CNAME 记录有时会出现在最底部，也就是最终解析结果之后。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;示例如下：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;;; QUESTION SECTION:
;; www.example.com.       IN    A


;; ANSWER SECTION:
cdn.example.com.    300    IN    A      198.51.100.1
www.example.com.    3600   IN    CNAME  cdn.example.com.&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然许多 DNS 客户端实现并不依赖记录顺序，例如 systemd-resolved，但也有一些实现（包括 glibc 中的 getaddrinfo 函数）在解析过程中会跟踪“期望的记录名称”，并按顺序遍历响应内容，假定在任何最终答案之前都能先遇到 CNAME 记录。Reddit 上有用户&lt;a href=&quot;https://www.reddit.com/r/technology/comments/1qhg8ww/what_came_first_the_cname_or_the_a_record/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一方面，我非常敬佩他们在事后分析中展现出的细节和极高的工程标准；但另一方面，我也忍不住觉得，他们似乎并没有建立起足够完善的测试体系（以及相应的工程文化），来真正理解他们的改动在全球范围内会产生怎样的影响。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 &lt;a href=&quot;https://news.ycombinator.com/item?id=46681611%E3%80%81&quot;&gt;Hacker News 上的一篇热门讨论&lt;/a&gt;&quot;中，许多用户围绕 RFC 是否真的存在歧义展开了争论，尤其是在 RRset 与 RR 在消息分区中的细微区别上，还是说 Cloudflare 的工程师误解了规范。Patrick May 则评论道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是一个典型的 Hyrum 定律案例：“当一个 API 拥有足够多的用户时，你在契约中承诺了什么并不重要，系统中所有可观察到的行为，都会被某些人所依赖。”再叠加上未能遵循 Postel 定律：“发送时要保守，接收时要宽容。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一份即将在 IETF 讨论的 &lt;a href=&quot;https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section/&quot;&gt;Internet-Draft&lt;/a&gt;&quot; 中，Cloudflare 提议制定一份 RFC，明确规定 DNS 响应中应如何正确处理 CNAME 记录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据公开的时间线，Cloudflare 于 1 月 7 日开始全球部署，并在 1 月 8 日 17:40（UTC）覆盖了 90% 的服务器。公司随后宣布发生事故，并于 1 月 8 日 18:27 开始回滚变更，最终在 19:55 完成回滚。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/cname-rfc-cloudflare-outage/&lt;/p&gt;</description><link>https://www.infoq.cn/article/h5huIV6bVgqR2BeSNuPC</link><guid isPermaLink="false">https://www.infoq.cn/article/h5huIV6bVgqR2BeSNuPC</guid><pubDate>Thu, 12 Feb 2026 10:00:00 GMT</pubDate><author>作者：Renato Losio</author><category>云计算</category></item><item><title>“代码 + 编译器”要消失了？马斯克在 xAI 全员会上放话：到今年年底，AI 或将直接生成二进制</title><description>&lt;p&gt;一家 AI 公司要是在很短时间里接连走了联合创始人和一批核心工程师，外界第一反应通常就一句话：完了，这肯定出事了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 过去一周的离职潮就是这种感觉。消息在 X 上越滚越大，最后干脆被玩成梗：有人明明从没在 xAI 上过班，也跑去发帖“我也离职了”，用跟风式的调侃把“集体出走”的说法跑偏成了大型玩梗现场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克没做任何解释，直接把一场 45 分钟的全员大会录像公开出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段视频等于一次对外说明：离职到底是大家自己走，还是公司在做组织调整？xAI 现在在忙什么、谁负责什么？Grok、编程模型、视频生成、Macrohard（多智能体软件公司）这四条线接下来要怎么推进？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更狠的是，马斯克在视频里还抛出一个判断：到 2026 年底，AI 甚至可能不写代码了，直接生成二进制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad582af38bae009849a41a7279367722.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;埃隆·马斯克预测，到 2026 年底，AI 将彻底绕过传统编程，不再写代码，而是直接生成二进制程序。他认为，AI 所生成的二进制文件，其效率可以超过任何编译器所能产出的结果。也就是说，未来你只需要告诉 AI：“为这个特定目标生成一个经过优化的二进制程序”，就可以直接跳过传统意义上的编程过程。&amp;nbsp;当前的软件开发流程是：代码 → 编译器 → 二进制 → 执行；而马斯克设想中的未来将变成：Prompt（指令）→ AI 生成的二进制 → 执行。他还表示，Grok Code 有望在 2 到 3 个月内达到业界最顶尖（state-of-the-art）水平。&amp;nbsp;在马斯克看来，软件开发正站在一次根本性变革的门槛上。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克：不是离职潮，是我在裁员&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一周之内，一家 AI 公司接连失去两位联合创始人和多名核心工程师，这很难再被当作正常的人才流动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 xAI，离职消息几乎是“扎堆”出现的，很快就不只是“谁走了”的八卦，而变成了另一个更直接的问题：这家公司还能不能稳住？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TechCrunch 统计显示，过去一周里，至少有 9 名工程师公开宣布离开 xAI，其中包括两位联合创始人 Jimmy Ba 和 Tony Wu。短短几天，创始团队就少了将近一半。对任何一家仍在高速扩张的公司来说，这种速度和幅度都足够让人警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da069f16948f6a8abc03e7451d081609.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8304fae2ddccdacf0b04d5699863775.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f7/f7288f1e4996a8b6b75b527506eeca5d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克显然意识到了这一点。在离职叙事迅速发酵、并开始脱离公司控制之前，他选择了一种极不寻常的应对方式：公开一场内部全员大会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一段长达 45 分钟的 all-hands meeting 视频，被直接放到了 X 上，对所有人开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在公开视频之前，马斯克已经在内部会议上给出了他的判断。周二晚间的全员大会上，他将这轮离职定性为“阶段适配问题”，而非绩效问题。“因为我们已经发展到一定规模，我们正在重组公司结构，以便在这个规模下更高效地运作，”他说，“而事实上，在这种情况下，有些人更适合公司的早期阶段，却不太适合后期阶段。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克随后也在 X 上明确表示，这是一轮因组织结构调整而产生的人员分离——本质上是裁员，而非单纯的个人选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4df6287ba0edb86de871d2f9c61a68ab.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“随着公司快速成长，组织结构必须进化。这不幸地意味着需要与一些人分道扬镳。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在外界看来，这一解释并未完全平息争议。xAI 在 两天之内失去两位联合创始人 的事实，很快引发了更多关于内部节奏与执行压力的讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;The information则给出了另一种解读。据知情人士透露，xAI 原计划在去年年底或今年 1 月初发布 Grok 4.2，但这一时间点最终被错过。“埃隆不喜欢延期，”他们写道，“当他对某个项目感到愤怒，或者高度聚焦时……就会有人出局。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3cdc69559c9b240ac746a0ec432e0d9.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一说法虽非官方表态，却在舆论场中迅速传播，也从侧面解释了为什么这场 all-hands meeting 不只是一次例行沟通，而更像是一场在压力之下，对内对外同时展开的解释与重组。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;xAI 离职时间表（公开信息汇总）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 6 日，Ayush Jaiswal（工程师）写道：“这是我在 xAI 的最后一周。接下来几个月我会陪伴家人，同时折腾一些 AI 相关的东西。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 7 日，Shayan Salehian（负责产品基础设施和模型后训练行为，曾就职于 X）写道：“我已离开 xAI，准备开启新的事业，也正式结束我在 Twitter、X 和 xAI 工作的 7 年多时光，满怀感激。”他还提到，与马斯克近距离共事让他学会了“对细节的偏执关注、近乎疯狂的紧迫感，以及从第一性原理思考问题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 9 日，Simon Zhai（技术人员）写道：“今天是我在 xAI 的最后一天，非常感激这次机会。这是一段令人惊叹的旅程。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 9 日，Yuhuai（Tony）Wu（联合创始人、推理负责人）写道：“我今天从 xAI 辞职了。是时候开启新篇章了。这是一个充满可能性的时代：一个配备 AI 的小团队，可以移山填海，重新定义可能性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Jimmy Ba（联合创始人、研究与安全负责人）写道：“今天是我在 xAI 的最后一天。借助合适的工具，我们正迈向 100 倍生产力的时代。递归式自我改进循环很可能在未来 12 个月内上线。是时候重新校准我在宏观层面的‘梯度’了。2026 年将会疯狂，并且很可能是关乎我们物种未来、最忙碌的一年。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Vahid Kazemi（机器学习博士）写道，他“几周前”就已离开 xAI，并表示：“在我看来，所有 AI 实验室都在做同样的事，这很无聊。我认为还有更大的创意空间，所以我要开始做点新的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Hang Gao（从事多模态项目，包括 Grok Imagine）写道：“我今天离开了 xAI。”他称这段经历“非常有价值”，并提到自己对 Grok Imagine 多次发布的贡献，同时称赞团队“谦逊的工匠精神和雄心勃勃的愿景”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Roland Gavrilescu（去年 11 月离职创办 Nuraline）发帖称：“我离开了 xAI，正与其他离开 xAI 的人一起打造新的东西。我们在招聘 :)”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Chace Lee（Macrohard 创始团队成员）写道：“短暂重置一下，然后重返前沿。”（Macrohard 是 xAI 旗下的纯 AI 软件项目，目标是利用 Grok 驱动的多 agent 系统，实现软件开发、编码和运维的全自动化；其名字带有对微软的调侃意味。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 现在员工还是一千多人，所以短期内不太可能因为这波离职就“运转不下去”。但人走得太集中、太快，网上很容易越传越夸张：一些 X 用户甚至干脆跟着玩梗，发帖“我也离开 xAI 了”——明明他们从来没在那儿上过班。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/78ddd553b93495da131900d3f6c2b380.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/62/621bc914bc79992e9118b59001fcc9c0.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是 xAI 随后选择 公开视频 all-hands meeting 的直接背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场全员大会上，马斯克反复强调了两个判断。第一，离职并非绩效问题，而是阶段适配问题。第二，在当前阶段，xAI 的唯一优先级只有一个——速度（velocity）和加速度（acceleration）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你在某个技术领域里跑得比所有人都快，那么你最终一定会成为领导者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;xAI 新架构：四大团队，各自干什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克在这次大会上，大概说了几件事儿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先他给xAI进行了一个定位：别把它当成熟公司看，毕竟这个公司才成立两年半。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他把 xAI 形容成“幼儿”，强调：“我们还小，但长得特别快”。对手很多都干了五年、十年甚至二十年，起步资源更好、人更多，但 xAI 硬是在短短几年里把不少关键方向做到了前排，甚至拿了“第一”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后他一口气列了几条“成绩单”：语音、图像、视频生成做到了行业领先；他还强调“预测能力”才是衡量智能的关键指标，并说 Grok 4.20 在预测任务上赢过别的模型。应用形态上，xAI 已经把 Grok 和 Imagine 这种能力整合进一个 App，还对 X 做了更激进的改造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们还有一个更野的目标：Grok-pedia 不只是“做个更好的维基百科”，而是要做成“银河百科全书”——把所有知识（包括图像、视频）都装进去，规模和准确性都要上一个数量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后谈到离职和重组，马斯克表示这不是“崩了”，而是“公司长大了”：xAI 已经达到了一个新的规模节点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他用了生命体成长的比喻：公司创业初期几十个人，大家可以随便聊，到几百人就必须有结构；再长大就得“分化出器官、长出四肢，甚至一度还会有尾巴——好在后来尾巴消失了”。所以重组是为了跑得更快。也因此会出现现实情况：有人适合早期冲锋，但不一定适合后期规模化运作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后他公布了新的组织架构，xAI 接下来就按四条线打：&lt;/p&gt;&lt;p&gt;第一，是 Grok Main 和语音，这是核心的 Grok 主模型；&lt;/p&gt;&lt;p&gt;第二，是专门面向编程的模型；&lt;/p&gt;&lt;p&gt;第三，是图像和视频模型，也就是 Imagine；&lt;/p&gt;&lt;p&gt;第四，是 MacroHard，它的目标是对整个公司级系统进行完整的数字化仿真。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97302dad27f36206b61cb0be96ceb600.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Grok 仍然是 xAI 对外最重要的产品入口；Coding 团队则被放在了一个更加核心的位置，不只是为了“写代码”，而是为了压缩整个软件生产链路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Grok 团队：一年内，Grok 装进了 200 万辆特斯拉&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Grok 团队是 xAI 当前最核心、也最直接面向用户的一条产品线，几乎承载了外界对 xAI 的全部直观认知：聊天、语音、车载、API，以及与 X 平台的深度整合。这条线的负责人是 Aman Madaan（2024 年加入 xAI）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd0ba2a787266ef16cd385ab81f85815.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果只用一句话概括 Grok 团队这一年的进展，那就是：从“什么都没有”，到成为 xAI 最快落地、规模化最成功的产品线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aman 用“从零到第一”的方式概括语音线的推进速度：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Grok Main 和语音团队将合并为一个团队。在语音上有一个很典型的例子。2024 年 9 月，OpenAI 已经推出了高级语音模式，而当时我们什么都没有，连模型都没有。但我们是在那之后才开始的，在短短六个月里，我们从零开始、完全自研，在团队里几乎没有音频背景的情况下，做出了一个在六个月内就已经超越 OpenAI 的语音产品。而现在，不到一年，Grok 已经部署在超过 200 万辆特斯拉汽车中，同时我们也推出了 Grok Voice Agent API。&amp;nbsp;一年时间里，我们从“什么都没有”变成了行业领导者。这种事情，只可能发生在像 xAI 这样的地方：小团队、极度投入、使命导向，再加上充足的算力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Grok 主模型上，xAI把重点从“问答”推向“Everything App”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在聊天模型这条线上也是同样的故事。从 Grok 1.5、Grok 2 到 Grok 3，我们始终站在推理能力的最前沿。&amp;nbsp;我们想要走向一个不只是“问答”的世界，而是打造一个真正的“Everything App”。你可以来这里咨询法律问题、制作幻灯片、解决复杂问题，真正把事情做完。&amp;nbsp;我们的目标，是打造一个入口，让你可以完成所有工作，真正放大每一个人的能力，让他们完成远超个人极限的事情，而且这一切都会通过一个极其简单、自然、无缝的使用体验来实现。&amp;nbsp;未来几个月，知识工作者能够完成的工作量，将出现数量级提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;编程（Coding）团队：到今年年底，你可能都不用写代码了&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说 Grok 是 xAI 面向用户的“对话入口”，那 Coding 团队就是整个公司真正的执行引擎。这个团队不仅负责 xAI 内部的编码系统，更承担着一个更激进的使命：让 AI 自行写代码，并最终替代“写代码”这件事本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Coding 团队负责人 Makro 的发言，是这场 all-hands 里最容易让工程师产生情绪波动的一段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按他的说法，这已经不再是“提效”，而是一条自我加速的链路：这一代 Grok Code 正在训练下一代 Grok Code。等“写代码”变成训练流程的一部分，讨论的重点就不再是工具顺不顺手，而是系统会不会沿着这条路一路跑下去。所以，编程被直接提到公司最高优先级之一。而且投入了等效“百万张 H100”的训练算力，目标是训练出世界上最强的编程模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克的判断更为激进。在他看来，“写代码”本身正在显露出一种过渡形态的特征，最终只需要一句“为这个特定目标生成一个经过优化的二进制程序”，就可以直接绕过传统意义上的编程过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Makro 在会上先从“质变”讲起：模型终于从“看起来能用”变成“真的能用”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;最近这段时间，编程这件事真的发生了很大的变化。&amp;nbsp;以前我一直在吐槽：大家老是劝我用编程模型，我也试过，但说实话，并没有被真正说服。但最近不一样了——这些模型已经能产出相当不错、可用的代码质量了。&amp;nbsp;当然，你还是需要去 review、去给反馈，但已经很容易看出来，它们能把人的效率拉高很多。这已经不只是“帮你写代码”了，而是它们对你的直觉理解得比以前好太多。现在我描述一个问题的时候，只需要像跟一个已经熟悉代码库的工程师同事解释一样去说就行；而以前，你基本得像牵着一个幼儿一样，一步一步教它该怎么改。&amp;nbsp;而且它们不只是写代码，还可以帮你 debug 代码。现在我们会让 Grok Code 连续跑上好几个小时，来确保对训练系统这种更复杂的改动，真的能在生产环境里稳定工作。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Makro 也把 Grok Code 的用途描述为“生产级验证 + 递归自我改进”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;所以对我们来说，这已经不只是“写代码更快一点”、让工程师 10x 更高效的问题了。我们已经清楚地看到：我们正走在一条递归式自我改进的路径上——这一代 Grok Code，正在训练下一代 Grok Code。而且这条路径已经进入指数级起飞阶段，并且会继续下去。&amp;nbsp;正因为如此，我们在公司里全面加码编程方向，把 coding 提升为公司最高优先级之一。&amp;nbsp;如果你对编程感到兴奋，不管你是非常擅长训练模型，还是一名对系统设计感兴趣的底层软件工程师——这里就是你该来的地方。我们现在拥有等效百万张 H100 的训练算力，目标就是训练出世界上最强的编程模型。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Guodong则表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;随着时间推移，我们越来越清楚地意识到：至少在编程这个维度上，我们正走向某种“奇点”。&amp;nbsp;真正的限制因素，可能已经不在算法或模型上了，而是在计算资源和能源：是否能运行起足够强的模型，去支持和赋能所有人。而现在，通过这次调整，我们已经是一个统一的团队；我们会在算力上取胜，我们正在赢下“太空算力”这条路。&amp;nbsp;所以，对每一位工程师来说——不管你现在是在写内核、写编译器，都可以想一想：这件事是否还值得你亲手去做？ 也许你应该加入我们，在 coding 方向上，多少“自动化掉你自己的一部分”，让自己跑得更快。&amp;nbsp;说实话，这是一个非常疯狂、也非常令人兴奋的年份。真的是“活在这个时代太夸张了”。我已经能清晰地感受到 AGI 的气息——至少在编程这件事上，已经非常接近了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c1bb842437b9166acc6930f3b96379f8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，马斯克在 Coding 段补了一句极具冲击力的判断，把“写代码”本身都当成中间态：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对，我觉得事情会走到一个阶段——可能甚至今年年底就会到——你都不会再费劲去“写代码”了，AI 会直接把二进制给你生成出来。&amp;nbsp;而且 AI 生成的二进制，效率会比任何编译器做到的都更高。所以你就直接说：“给我一个针对这个具体目标的最优化二进制。” 然后你甚至连传统意义上的编码都绕过了。写代码这一步，其实只是个中间步骤，很可能到……我觉得今年年底左右，就不需要了。&amp;nbsp;而且我们预计，Grok Code 会在两到三个月内达到最先进水平（state of the art）。这一切发生得非常、非常快。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bd/bd36e008f7a07eddcbc3fca86e63882d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9de7210559039975b7d7fe77c972144.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;&amp;nbsp;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Imagine 团队：每天 5000 万条视频，是所有竞争对手的总和&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Imagine 是 xAI 的图像与视频生成产品线，也是公司里算力消耗最大的方向之一。负责人是 Guodong，核心成员包括主攻视频方向的 Haotian，以及Chaitu。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Guodong 在会上把 Imagine 的进展描述为“从零到全面铺开”的速度战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Imagine 团队几乎是六个月前从零开始的。没有扩散模型代码，没有现成基础，但现在 Imagine 已经全面集成进我们所有产品，包括 X 应用。你现在就可以在 X 里长按图片，直接编辑，或者把图片变成视频。&amp;nbsp;Imagine 的增长速度极其惊人。用户现在每天生成接近 5000 万个视频，过去 30 天里生成了 60 亿张图片。作为对比，Google 最近表示他们的模型 30 天生成了 10 亿张图片，而我们是它的六倍。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Haotian 则把时间表拉到“今年年底”，强调“长视频一键生成 + 无干预”的路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;随着模型能力不断扩展，我们正在构建与现实难以区分的视觉世界。到今年年底，我们很可能会拥有可以一次性生成 10 分钟、20 分钟视频的模型，而且不需要任何中途干预。你只需要给出你的想象力，其余的一切都会由模型和智能体自动完成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，马斯克也补了一句方向性判断，把未来算力押注在“实时视频理解/生成”上：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我的预测是：未来大多数 AI 计算资源都会用在实时视频理解与实时视频生成上，而我们预计会成为这方面的领导者。这里值得再次强调：六个月前，我们在视频与图像生成、编辑方面几乎什么都没有，或者说非常弱；但在六个月内，我们就冲到了第一名。&amp;nbsp;而且我相信，大家会对即将发布的 Grok 4.2 模型印象非常深刻——它是一次显著提升。不过那只是我们新模型体系里的“小版本”。接下来还会有中等版本和大型版本，它们会更加智能。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;MacroHard：AI Agent 的终极实验场&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MacroHard 被定义为一个由 AI Agent 驱动、用来“模拟公司运转”的方向，目标远不止写代码：它要模拟人类使用计算机，自动运行软件和各类公司流程，甚至进一步做到对整家公司进行仿真。负责人是 Toby，核心成员包括负责执行层推进的 John M.。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Toby 在会上给 MacroHard 的一句话定义非常硬核：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MacroHard 正在构建一个完全能力齐备、数字化、实时的人类模拟器。它能够在计算机上完成任何一个人类能完成的事情，包括使用工程和医学等领域的高级工具。未来应该会出现由 AI 完整设计的火箭发动机。&amp;nbsp;从某种意义上说，这是 AI 目前仍然显著弱于人类的领域之一。也正因为如此，这是最令人兴奋、最值得投入、也最有可能真正改变整个领域的方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;John M. 则把 MacroHard 的路径拆成“CLI → GUI → 端到端编排”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们正在构建这些强推理模型，而它们将会控制我们的 CLI（命令行界面）。我们每天都在积极使用这些模型，它们给整个团队带来了巨大的生产力提升。我知道语音团队在这方面做得非常出色。&amp;nbsp;这也是为什么我们需要算力——我们需要大规模算力来运行这些模型，从而提升我们自己的生产力。但现实是：全球 80% 到 90%，甚至 95% 的软件世界，都有 GUI（图形界面）。这是一个非常重要的事实。要真正让人们生活更容易，我们必须开发能够在 GUI 上完成日常任务的模型。&amp;nbsp;所以 MacroHard 的目标，是模拟一家“输出是数字化成果”的公司。这是智能体的下一步：MacroHard 将实现跨桌面端的真正端到端编排，并将带来巨大的经济繁荣。&amp;nbsp;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，马斯克把 MacroHard 的意义抬到“人类仿真”的高度：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MacroHard 这个项目，随着时间推移，可能会成为我们最重要的项目。我们讨论的是：对整家人类公司进行仿真。&amp;nbsp;理论上完全有可能完整仿真任何一家“输出是数字化产物”的公司。这将开启一个繁荣时代，其程度可能是我们现在几乎无法想象的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;基础设施：xAI 真正的护城河&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 xAI，基础设施不是“后台部门”，而是以上所有激进判断能不能落地的前提。ML 基础设施团队负责搭建公司的训练、推理以及整套工具链系统。用他们自己的话说：站在软件工程师视角，这可能是你能做的最酷的一类系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最典型的例子发生在 Grok 3 的训练上。当时，xAI 已经拿到 10 万张 H100，硬件都交付到位，但软件并没有真正准备好。团队原本以为系统能跑，结果规模一拉到 3 万卡，现实给了一个很明确的反馈：系统跑不起来。问题不是某一个 bug，而是数据中心里的“意外”太多：交换机抖动、链路抖动、交换机宕机、GPU 频繁损坏、数值不稳定……这些都不可能提前枚举完。但目标只有一个：让 10 万张 H100 像一个整体一样工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一次训练 step 可能只有 5 秒：每 5 秒往前推一步，但这 5 秒里什么都可能发生。所以系统必须做到：意外不断出现也能自动恢复、持续推进，而不是一出问题就停下来等人来救。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种问题在别的地方也很难遇到。不是工程师不够聪明，而是很少有人同时拥有这种规模的算力、以及这种密度的人才。当时整个预训练团队大约 15 人，真正负责训练系统的可能只有 7 人，但他们刻意维持了这种“人才密度”，而不是靠堆人数去堆规模，最终靠这支小团队完成了Grok3的训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达 CEO 黄仁勋在多次采访中说过一句评价：在把 AI 算力上线这件事上，没有人比 xAI 更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，RL 与推理团队接力。这个团队负责在地球上、以及很快可能在太空中，大规模运行训练任务和生产推理系统，目标很直接：把系统从 10 万张芯片扩展到数百万张芯片，并且让它对已知和未知的硬件故障都具备韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前他们的成果都汇聚到了孟菲斯的数据中心里：xAI 已经建起了全球规模最大的 AI 训练集群之一，而且仍在扩张——第一阶段是 33 万张 GB300，接下来还将再增加 22 万张 GB300。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9f/9f6cac7ed43c5970050a1985c02069eb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;显然，想要最好的模型，必须有大规模训练算力，这一点是绝对基础。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在这场 all-hands 的最后，马斯克把这个逻辑推到了一个几乎只存在于科幻里的地方。如果地球已经装不下这些算力了，那下一步呢？答案是：月球。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克认为，要真正理解宇宙，最终必须离开地球去探索，而这正是 SpaceX 与 xAI 合并到一起的动机：加速人类理解宇宙的未来，把意识的光延伸到群星之间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9ec2a34f72072e1a538f73ef4f6b0b24.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从能源的角度看，他给了一个非常极端的对比：今天整个人类文明，使用的只是地球可用能量的 1% 左右。而如果人类哪怕只想用到太阳能量的百万分之一，那也是现在文明能耗的一百万倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题在于：你在地球上，根本不可能拿到那样的能量。地球在整个太阳系里，只是一粒“极小的尘埃”。太阳占了太阳系 99.8% 的质量，如果不走出地球，你几乎不可能对太阳能量的利用产生任何实质性的提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在他看来，下一步不是“更大的地球数据中心”，而是“离开地球的数据中心”。先把数据中心送上地球轨道；再往后，就把制造和发射搬到月球——在月球上建工厂生产 AI 卫星，再用质量驱动器（mass driver）把它们一颗接一颗“弹射”到深空，把算力扩展到地球根本承载不了的规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aOVnB88Cd1A&quot;&gt;https://www.youtube.com/watch?v=aOVnB88Cd1A&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/&quot;&gt;https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hsoVQfxDa9ofFiMZsDai</link><guid isPermaLink="false">https://www.infoq.cn/article/hsoVQfxDa9ofFiMZsDai</guid><pubDate>Thu, 12 Feb 2026 09:51:56 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>“每给 Claude Code 提一个请求，我就点上一根烟，放松下”</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenFGA核心维护者、Ona（前 Gitpod）软件工程师 Siddhant Khare最近写了一篇博客吐槽了自己在使用AI 编程中的“疲惫感”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他以自身经历指出AI 带来的职业疲惫真实存在且被行业集体回避：单任务变快≠工作变轻松，反而更累，期间工程师任务量膨胀、频繁切换引发深层耗竭；工作角色从创造者转为高消耗的 AI 产出评审者，加之 AI 输出的不确定性打破了工程师熟悉的确定性逻辑，持续带来焦虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，行业技术迭代过快形成 “FOMO 跑步机”，频繁追新工具造成时间浪费与知识衰减，还易陷入 “prompt 螺旋” 陷阱，长期依赖更会导致独立思考能力退化，社交媒体的高光展示则进一步加剧比较焦虑。他指出，AI 时代工程师的核心能力并非极致使用 AI，而是懂得设边界、及时停止，保护认知资源，追求可持续的长期产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他的博文引发了工程师们的共鸣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“对我来说，这种疲惫感有点不一样，它来自于不断在‘写一点代码 / 做一点工作 / 看一点 review’和“‘停下来等大模型生成结果’之间来回切换。等待的时间是不可预测的，你根本不知道是该继续等，还是该切去做别的事。于是你只能在机器“思考”的时候，随便干点事打发时间。&lt;/p&gt;&lt;p&gt;你永远进不了心流状态，只能时刻盯着后台任务什么时候跑完。这种持续的“警觉等待”会让人特别消耗精力。我并不觉得自己更高效了，反而感觉自己像个偷懒的保姆，只是勉强看着孩子别把自己弄伤而已。”开发者Parpfish跟帖道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我知道这建议听起来既不负责任又很幼稚，但我现在的做法是：每次给 Claude Code 提一个不知道要跑多久的请求，我就点上一根烟，放松一下。有时候我也会切去玩那种随时拿起来、随时放下都不影响的小游戏。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“对我个人来说，编程很多年前就没什么乐趣了，”Parpfish也表示，“但有了 Claude Code 之后，我又重新觉得好玩起来了。虽然感觉不一样，但在我现在这个人生阶段，这样反而更让我享受。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是Siddhant Khare的文章，我们进行了翻译，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 疲惫真实存在，但几乎没人谈&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你用 AI 是为了更高效，为什么反而比以前更累？这是每个工程师都得正视的悖论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上个季度，我交付的代码量超过职业生涯任何一个季度；同时，我也比职业生涯任何一个季度都更疲惫。这两件事并不矛盾，甚至高度相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的工作就是搭建 AI agent 的基础设施。我是 OpenFGA（CNCF Incubating）的核心维护者之一；做过用于 agent 授权的 agentic-authz；做过用于上下文去重的 Distill；上线过 MCP servers。我不是偶尔玩玩 AI 的那种人，我在这个领域深扎很久，我写的工具，正被其他工程师拿去把 AI agents 跑进生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但即便如此，我还是撞墙了。那种疲惫不是换一套工具、再优化一点流程就能解决的，而是一种更底层的耗竭感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你是每天都在用 AI 的工程师，做设计评审、生成代码、排查 bug、写文档、做架构决策，并且你发现自己在“AI 时代”反而比以前更累，那这篇文章就是写给你的。你没有在幻想，也不是你不够强。你感受到的是真实存在的东西，只是行业在集体回避它：大家拼命讲效率、讲产出，却不讲代价。一个全职做 agent 基建的人都能在 AI 上 burnout，这件事可能发生在任何人身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我想实话实说。不是那种“AI 太神了，这是我的工作流”的版本，而是真实版本：夜里 11 点，你盯着屏幕，周围堆着一大片 AI 生成的代码还得你去 review，你开始怀疑那个本该帮你省时间的工具，为什么反而吞掉了你整天的时间。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/88c29fe34534eb4c2e9e12e2e49155a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;没有人提醒过我们的悖论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有件事我曾经想了很久才想明白：AI 的确能让单个任务更快，这不是谎言。以前要 3 小时的事情，现在 45 分钟就能搞定，写设计文档、搭服务骨架、补测试用例、研究不熟的 API……都更快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我的工作日变得更难了，不是更轻松，而是更难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因一旦看清就很简单，只是我花了几个月才真正意识到：当每个任务变快，你不会做更少的任务，你只会做更多。你的“产能”看起来提升了，于是工作会膨胀来填满它，甚至还会超出。你的领导看到你交付变快了，预期会跟着调整；你看到自己交付变快了，对自己的预期也会跟着调整。基准线被整体抬升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 AI 之前，我可能会用整整一天只专注一个设计问题：在纸上画草图、洗澡时想、出去走走、回来突然清晰。节奏慢，但认知负担可控，一天只扛一个问题，深度专注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在呢？一天可能要摸六个问题。每个问题都“只要一小时，AI 帮你很快搞定”。但在六个问题之间来回切换，对人脑的代价极其昂贵。AI 不会在问题之间疲惫，我会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是悖论：AI 降低了“生产”的成本，却抬高了“协调、评审、决策”的成本，而这些成本几乎全部落在人的身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;你变成了 reviewer，而你从没签过这份合同&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前我的工作流程是：想清楚问题 → 写代码 → 测试 → 发布。我是创造者，是建造者，这也是很多人最初喜欢工程的原因：能亲手把东西做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 之后，我的工作越来越像：写 prompt → 等 → 读输出 → 评估输出 → 判断是否正确 → 判断是否安全 → 判断是否符合架构 → 修不对的部分 → 再 prompt → 再重复。我变成了审稿人、裁判、质检员，站在一条永不停歇的流水线旁边。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一种完全不同的劳动类型。创造会给人能量，评审会消耗能量。相关研究早就指出，“生成型任务”和“评估型任务”在心理体验上截然不同：生成更容易进入心流，评估更容易触发决策疲劳。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我第一次明确意识到这点，是在某一周用 AI 重度开发一个新 microservice 的时候。到周三，我连简单的决定都做不动了：这个 function 该叫什么？无所谓。配置放哪？也无所谓。我的大脑不是因为写代码累，而是因为“判断代码”累，每天一整个时间都在做无数个细小判断，会把你掏空。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更残酷的是：AI 生成的代码，往往比人写的更需要谨慎 review。人写的代码我大致知道对方的习惯、长处、盲点：可信的地方可以快扫，不放心的地方重点看。AI 不一样，每一行都值得怀疑。代码看起来很自信，能编译，甚至能过测试，但可能在极隐蔽的地方错得很深，直到线上、在高压负载下、凌晨三点才爆出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是你只能逐行读。读自己没写过、由一个不了解你代码库历史和团队约定的系统生成出来的代码，是一种非常消耗人的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是为什么我一直觉得 agent 安全和权限这么重要。我们不可能 review AI 产出的所有东西，规模一上来就做不到了，那就必须先在系统层面约束 agent 能做什么：最小权限原则、范围限制 tokens、审计轨迹。你越不需要担心“AI 会不会做出危险动作”，你越能把认知预算留给真正重要的工作。这不仅是安全问题，更是“人能否长期承受”的可持续问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;非确定性问题：AI 破坏了工程师最熟悉的契约&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工程师从业训练的底层假设是确定性：同样的输入，得到同样的输出。它是调试的基础，也是系统推理的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 把这份契约撕了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有个 prompt 周一跑得完美，生成了结构清晰、很干净的 API endpoint。周二我用同样的 prompt 做一个类似 endpoint，输出结构却明显不同，这次错误处理换了套路，还引入了我没要的依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么？没有原因。更准确地说，没有我能触达的原因。这里没有“模型今天换了想法”的stack trace，也没有日志告诉你“temperature sampling 走了 B 路径不是 A 路径”。它就是……不一样了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对一个职业生涯建立在“坏了我就能找到为什么”的人来说，这种体验会带来持续的、背景噪音式的焦虑。它不一定戏剧化，却足够磨人。你无法完全信任输出，也无法真正放松，每一次交互都必须保持警惕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我试过对抗，给 prompt 做版本控制，写复杂的 system message，做模板。一部分有用，但都无法解决根本矛盾。你在和一个概率系统协作，而你的大脑天生更擅长确定性系统，这种错位会长期产生低强度压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为这种挫败感，我后来做了 Distill：为 LLM 做确定性的上下文去重，不调用 LLM，不用 embeddings，也不靠概率启发式，而是用纯算法，在大约 12ms 内把 context 清理干净。我至少想让 AI pipeline 里有一段东西是可推理、可调试、可信的。模型输出再怎么不确定，输入至少要干净、可控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我发现适应得最好的一批工程师，通常已经“和不确定性和解”了。他们把 AI 当作一个聪明但不靠谱的实习生写的初稿，默认要重写其中 30%，并且提前把这部分重写时间算进计划。他们不会因为输出错了而愤怒，因为他们从来没期待它“正确”，只期待它“有用”。这两者差别很大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“FOMO 跑步机”：你永远追不上&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;深呼吸一下，试着只跟上最近几个月的变化：Claude Code 先发 sub-agents，再发 skills，再发 Agent SDK，再发 Claude Cowork；OpenAI 上线 Codex CLI，又上 GPT-5.3-Codex，一个甚至“参与了自我编写”的模型；新的 coding agents 宣布 background mode，可并发上百个 autonomous sessions；Google 推出 Gemini CLI；GitHub 增加 MCP Registry；并购几乎每周发生；Amazon Q Developer 得到 agentic 升级；CrewAI、AutoGen、LangGraph、MetaGPT，随便挑一个 agent framework，每周都冒出新版本；Google 发布 A2A（Agent-to-Agent protocol）对标 Anthropic 的 MCP；OpenAI 发布自己的 Swarm framework；Kimi K2.5 采用 agent swarm 架构，编排 100 个并行 agents；“Vibe coding”成了热词；OpenClaw 上线 skills marketplace，一周之内研究者在 ClawHub 发现 400+ 恶意 agent skills；与此同时 LinkedIn 还会冒出一句话：“2026 年不做 sub-agent orchestration，你就已经过时了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这还不是一年发生的事，是短短几个月，并且我还漏掉了很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也曾深陷其中：周末不断评测新工具，追每一条 changelog，看每一个 demo，拼命留在所谓“前沿”，因为我害怕落后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现实是什么？周六下午我搭起一个新的 AI coding tool，周日形成基本 workflow，而到了下周三，社交网络开始吹另一个“更强”的工具，我就会焦虑；下一个周末又去搭新的，旧的躺着吃灰。从一个 coding assistant 迁到下一个、再迁下一个，最后又回到第一个，每次迁移耗掉我一个周末，换来大概 5% 的提升，而且我还很难测出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这种循环乘以所有类别：coding assistants、聊天界面、agent frameworks、多 agent 编排平台、MCP servers、context 管理工具、prompt 库、swarm 架构、skills marketplace，你会变成一个永远在学习新工具、却从没真正把任何一个工具用深的人。Hacker News 首页就足够让人眩晕：今天是“Show HN：Autonomous Research Swarm”，明天是“Ask HN：AI swarms 怎么协作？”没人知道答案，但大家都在造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更糟糕的是知识衰减。我在 2025 年初花了两周搭出一套复杂的 prompt 工程流程：精雕 system prompts、few-shot examples、chain-of-thought 模板。它当时非常好用，三个月后模型更新，最佳实践迁移，一半模板反而不如一句简短指令效果好。那两周不是“投资”，而是“消耗”。我的 MCP server 也是：我写了五个自定义 servers（Dev.to 发布、Apple Notes 集成、Python/TypeScript 沙盒等），后来协议演进，GitHub 上线 MCP Registry，突然出现成千上万预制 servers，我的部分工作一夜之间变得可有可无。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent framework 的 churn 更夸张。我见过团队一年内从 LangChain → CrewAI → AutoGen → 自研编排连续迁移。每次迁移都意味着重写集成、重学 API、重建 workflow。那些选择“等等再说”的团队，很多时候反而比早早冲进去、被迫迁两次的人更占便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我换了策略,不再追每个新工具，而是深挖它们下面的基础设施层。工具会来会走，它们解决的问题不会。context 效率、agent authorization、audit trails、runtime security，这些是跨框架、跨周期的耐久问题，这也是我把 agentic-authz 建在 OpenFGA 上、而不是绑死某个 agent framework 的原因；也是 Distill 做 context 层、而不是 prompt 层的原因：要构建在不那么 churn 的层上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我仍然会密切关注生态，做基础设施的人必须如此。但我关注是为了理解方向，而不是把每个新东西都立刻搬进生产。信息充分和被动反应，是两回事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“再改一版 prompt 就好了”陷阱&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个陷阱非常阴险：你让 AI 生成一个很具体的东西，第一版 70% 是对的；于是你 refine prompt；第二版 75% 的对，但把第一版对的地方弄坏了；第三版 80% 对，但结构又变了；第四次你回过神来，已经 45 分钟过去了，而你自己从头写可能 20 分钟就写完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我叫它 prompt spiral，AI 时代的 yak shaving。你原本有明确目标，半小时后却在调 prompt，而不是写代码。你在优化“给模型的指令”，而不是解决真正的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更危险的是，prompt spiral 会让你产生“我在推进”的错觉。每一轮都有小进步，你会继续投入，但边际收益正在快速递减，你甚至忘了目标从来不是“让 AI 产出完美内容”，而是交付功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在有一条硬规则：三次。三次 prompt 内拿不到 70% 可用的结果，我就自己写，没有例外。这条规则省下的时间，超过我学过的任何 prompt 技巧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;完美主义遇上概率输出：最优秀的人往往最难受&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工程师倾向完美主义：喜欢干净代码、喜欢测试全绿、喜欢可预测系统。这不是缺点，是我们能做出可靠软件的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 AI 输出从来不是“完美”，永远是“还不错”，大约 70–80%：变量名不对味，错误处理不完整，边界条件没考虑，抽象不符合你的代码库。能跑，但“不对”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对完美主义者来说，这很折磨，因为“差一点对”比“完全错”更糟。完全错，你直接丢掉重来；差一点对，你会花一小时去修修补补。修 AI 输出尤其痛苦，因为你在修“别人做的设计决策”，而这个“别人”并不分享你的审美、你的上下文和你的标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不得不学会放下。不是放下质量，我仍然在乎质量，而是放下“AI 会产出质量”的期待。我现在把每次 AI 输出都当作毛坯、当作起点、当作原材料。它出现的那一刻，我脑子里就贴上“draft”的标签。仅仅是这个心智框架的变化，就让我的挫败感减少了一半。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多在 AI 上最痛苦的工程师，恰恰是最好的工程师：标准最高、细节最敏感、瑕疵一眼就能看出来。AI 奖励的反而是另一种能力：能快速从不完美的输出里榨取价值，而不把情绪绑定在“把它打磨到完美”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;思考能力在萎缩：这才是最让我害怕的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在一次设计评审会上发现了这个问题。有人让我在白板上推一个并发问题，没有电脑、没有 AI，只有我和一支笔，我居然卡住了。不是我不懂概念，我懂，而是我几个月没练这个“肌肉”了。我把“第一轮思考”外包给 AI 太久，导致从零推理的能力在退化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它像 GPS 和导航。没有 GPS 的年代，你会建立城市的心理地图，能自己推路线。用了多年 GPS，你离开它就不会走了，因为这项技能已经萎缩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 对工程思考也是一样：当你总是先问 AI，你就少了自己挣扎的过程。而学习就发生在挣扎里：困惑是理解成形的地方。跳过它，你会更快拿到输出，但理解会更浅。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在刻意让每天的第一个小时不碰 AI：在纸上思考、手绘架构、用慢的方法推问题。它确实低效，但它让我的思考保持锋利，而锋利的思考会在我之后使用 AI 时带来回报。因为你自己的推理被“热身”后，你对 AI 输出的评估会更准确。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;比较陷阱：社交媒体只展示高光，不展示代价&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交媒体上到处都是“看起来已经把 AI 玩明白的人”：晒 workflow、晒产出数据、晒“我两小时用 AI 做完一个 app”。你回头看自己的经历：prompt 失败、时间浪费、生成代码不得不重写，于是你开始怀疑自己是不是哪里不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你没有任何问题。那些帖子是高光剪辑，没人会发：“我花了三小时让 Claude 理解我的数据库 schema，最后放弃，迁移还是手写。”没人会发：“AI 生成的代码线上吞错导致事故。”没人会发：“我很累。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更麻烦的是，AI 技能很难衡量。传统工程里，你看看代码大致能判断水平；AI 输出却受模型、prompt、上下文、temperature、甚至玄学因素影响。别人一个惊艳 demo，很可能在你的机器、你的代码库上复现不出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我后来对 AI 内容变得更挑，我仍然关注这个领域，毕竟是工作，但我更少看“热闹”，更多看“真的在建和在交付的人”。信号和焦虑的比例很重要。如果一个信息流让你更焦虑而不是更清醒，那它就不在为你服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;真正有用的改变是什么&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我具体说说，哪些做法让我的 AI 使用方式从对抗变成可持续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;给 AI 使用设时间盒。 我不再开放式使用 AI。会设定计时器：这件事用 AI 30 分钟，时间到就交付现状或自己写。它同时拦住了 prompt spiral 和完美主义陷阱。把思考时间和执行时间分开。 早上用来思考，下午用来 AI 辅助执行。规则不绝对，但有默认结构，就能确保大脑既锻炼也得到助力。接受 AI 只做到 70%。 我不再追求完美输出，70% 可用就够了，剩下我自己补。这个接受，是我减少 AI 挫败感最有效的一件事。对 hype cycle 保持策略性。 我会跟踪生态，但不再每个新工具一上线就立刻迁移。我只用一个主力 coding assistant，并把它用深。评估新工具看“几个月后的验证”，不看“几天内的热度”。信息充分和被动反应，是两回事。记录 AI 什么时候帮忙、什么时候拖后腿。 我做过两周简单日志：任务、是否用 AI、耗时、满意度。数据非常清晰：AI 在样板代码、文档、测试生成上省了大量时间；在架构决策、复杂调试、需要深代码库上下文的工作上反而耗时。知道这一点后，我更清楚什么时候该用它，什么时候不该用。不再试图 review AI 产出的每一行。 这很难接受，但如果你用 AI 生成大量代码，你不可能以同样严苛的标准逐行审。我的 review 精力集中在最关键的部分：安全边界、数据处理、错误路径；其它交给自动化测试和静态分析。非关键代码有一点粗糙是可以接受的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;可持续性问题：AI 不是治好 burnout，而是在放大它&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;科技行业的 burnout 早在 AI 之前就存在。AI 正在让它更严重，不是因为 AI 很坏，而是因为 AI 移除了曾经保护我们的“自然限速器”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 AI 之前，你一天能产出多少有上限：打字速度、思考速度、查资料的时间。这些限制有时令人沮丧，但它们也是一种“调速器”。工作本身会限制你把自己榨干的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 拿掉了这个调速器。现在唯一的上限是你的认知耐力，而大多数人只有在把这条线冲破之后，才知道自己的极限在哪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 2025 年末 burnout 了。不是戏剧化那种：我没有辞职，也没有崩溃。我只是开始不在乎了。code review 变成走过场，设计决策变成“AI 怎么说就怎么做”。我在机械地产出更多，却感受更少。我花了一个月才意识到发生了什么，又花了一个月才恢复过来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;恢复并不是“少用 AI”，而是“换一种方式用 AI”：设边界、有意图，并且承认我不是机器，我不需要跟机器同速。Working at Ona 让我更清楚看到这一点：当你为企业客户做 AI agent 基础设施，你会看到不可持续的 AI 工作流在规模化之后的“人类成本”。这不是个人问题，而是系统问题，必须在工具层面解决，而不只是靠个人硬扛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;讽刺的是，我最好的几个项目反而诞生在 burnout 期间。当我停止追工具、开始思考到底哪里坏掉时，问题第一次变得清晰：context window 被垃圾填满，这催生了 Distill；agents 拿着全权限 API key，这催生了 agentic-authz；无法审计 agent 做了什么，这正在变成 AgentTrace。疲惫迫使我停止消费、开始建设，不是更快地堆功能，而是更有意识地去做正确的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 时代真正的技能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为 AI 时代最重要的技能不是 prompt engineering，不是知道该用哪个模型，也不是拥有“完美工作流”，而是知道什么时候该停。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知道 AI 输出什么时候“够用”；知道什么时候该自己写；知道什么时候该合上电脑；知道边际提升不值得继续消耗认知；知道你的大脑是一种有限资源，保护它不是偷懒，而是一种工程能力。&lt;/p&gt;&lt;p&gt;我们做系统会优化可持续性：加熔断、做 backpressure、设计优雅降级。我们也应该对自己做同样的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 是我用过最强的工具，同时也是最消耗人的工具，这两件事可以同时成立。能在这个时代长期活得好的工程师，不会是用 AI 用得最多的人，而会是用得最聪明的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你很累，不是因为你做错了，而是因为这件事本身就很难。工具很新，模式还在形成，行业却装作“更多产出=更多价值”。事实不是这样。可持续的产出才是价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我依旧每天都在这个领域里建agent authorization、context engineering、audit trails、runtime security，让 AI agents 真正在生产环境可运行的基础设施。我对 AI 的投入比以往更深，但我会按自己的节奏、用自己的边界，去做真正重要的事，而不是追逐短暂的趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;照顾好你的大脑。它只有一个，而任何 AI 都无法替代它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://siddhantkhare.com/writing/ai-fatigue-is-real&quot;&gt;https://siddhantkhare.com/writing/ai-fatigue-is-real&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PpYEQfkrB25gN5rpPuKr</link><guid isPermaLink="false">https://www.infoq.cn/article/PpYEQfkrB25gN5rpPuKr</guid><pubDate>Thu, 12 Feb 2026 04:00:00 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>宝马、Indeed 和 WHOOP 的降本增效实践：如何在 Lakehouse 上构建分析与 AI 能力 ｜ 技术实践</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据可用性空前提高，企业却发现规模化交付可靠的分析与人工智能解决方案变得前所未有的困难。随着数据湖逐渐成为业务关键型分析与决策的共享基础平台，可靠性、并发处理能力及成本可预测性等方面的挑战迅速显现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开放表格与数据格式部分缓解了这一难题。通过标准化数据存储与访问方式，Apache Iceberg 等格式使机构能够更有效地掌控数据，并为跨引擎的可互操作分析奠定基础。然而，仅靠开放性尚不足以解决复杂的分析困境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着数据分布于多云环境、各类数据目录及工具之中，众多团队依然难以交付符合业务预期的分析成果。性能调优、运维负担与碎片化的安全管理模型，常常横亘在原始数据与可靠洞察之间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今，越来越多的组织正以提升效率为目标重构其分析架构。将计算能力引向数据所在之处的核心理念，源于对开放存储中单一受治理数据副本的坚持——这使团队能专注于挖掘数据价值，而非反复迁移或复制数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里正孕育着一种全新的技术路径。它基于 Apache Iceberg 这类开放数据表格式构建，同时支持 Delta 等其他格式。Snowflake 将一套为企业关键工作负载设计的强大分析引擎，直接部署于数据原生位置。团队无需再将数据迁移至另一个独立系统，即可在其存储原址处理全量数据，同时确保性能、可靠性与成本可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然这一路径在理念上令人振奋，但其在实践中的应用更具价值。本期综述将重点展示三大品牌——BMW Group、Indeed 与 WHOOP——如何运用该方案，在其全域数据资产中驱动分析与人工智能应用，从而将开放数据架构转化为可量化的商业成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从愿景到实践&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Indeed 在扩大自助数据访问规模的同时降低成本 43%&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Indeed 运营着一个 52PB 规模的数据湖，为全公司的关键业务报表、分析与实验提供支持。随着自助式数据访问（即读写 Apache Iceberg™ 表的能力）需求的增长，数据工程团队亟需一种既能扩展分析能力、又避免形成瓶颈的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过将数据湖从 Hive-ORC 架构迁移至 Apache Iceberg，Indeed 采用了与其开放数据战略相契合的“一次写入，随处读取”模式。借助 Snowflake 平台，分析人员能够直接读写 Iceberg 表，同时通过 Horizon 目录保持安全与治理控制，包括列级安全策略和数据脱敏机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在内部测试中，Indeed 发现，相比在同一环境中评估的其他分析引擎，使用 Snowflake 查询 Iceberg 表的成本降低了 43%–74%。这种开源格式、受控访问与高性能分析的结合，使得 Indeed 能够在为规模化构建的湖仓平台上，加速实验探索、产品分析与洞察生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;凭借 Snowflake 对 Apache Iceberg 的原生支持，Indeed 将庞大的数据湖转变为受控的自助分析平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;WHOOP 在提供实时健康洞察的同时大幅削减计算时间&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;WHOOP 每天通过其可穿戴设备分析数十亿条生物特征信号，为会员洞察、产品创新及业务预测提供支持。随着公司业务规模扩大，其需要一种在统一各系统数据的同时、能对敏感健康信息保持严格治理的方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过将数据整合至 Snowflake 平台并采用 Apache Iceberg 技术，WHOOP 在借助 Horizon Catalog 保障数据安全的前提下，简化了数据访问与管理流程。公司发现其新一代 AI/ML 财务预测模型运行速度提升了 3 倍，且通过降低运维复杂性，团队每日可节省 20 小时的计算资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;依托 Snowflake，WHOOP 将数据分析与人工智能转化为竞争优势，实现了更快速的财务预测能力，并为会员提供了更具个性化的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;宝马集团利用全球数据洞察连接万名用户，同时提升效率 25%&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宝马集团通过其云数据枢纽（Cloud Data Hub）运营着一个大规模的全球数据环境，整合了集团内制造、服务、供应链及可持续发展等多类业务场景的数据。该平台涵盖15个业务领域、超过6,000个数据集，每月服务用户数超10,000名，在规模化运营中需兼顾架构灵活性与统一治理规范。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为支撑这一“最佳架构”体系，宝马采用 Apache Iceberg 并结合 AWS 原生工具来管理开放、分布式数据；同时，在需要高效可靠分析的场景中集成 Snowflake 平台。Snowflake 为宝马现有数据资产提供了高性能计算能力，可在不干扰既有系统或不必要复制数据的前提下，实现复杂的运营分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一策略已取得显著成效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;宝马集团报告称，在某些服务类数据工作负载上平均节省了25%的成本，并已在Snowflake平台上部署超过60个数据应用场景，帮助各团队更快获取业务洞察，同时确保跨地区、跨工作负载的数据治理一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从复杂走向清晰&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管宝马集团（BMW Group）、Indeed 与 WHOOP 面临的具体挑战各异，但其应对策略背后存在一个共同模式：它们均优先采用将工具引入数据的策略，以保障架构效率，维持统一、开放、受治理的数据基座。向 Apache Iceberg 等开放表格式的转型使这一模式成为可能，这些格式提供了管理大规模数据所需的结构化、一致性与互操作性。而 Snowflake 则在此基础上提供了关键补充：一个能够直接在上述开放数据上运行的可靠分析与AI引擎，其内置功能旨在帮助团队在规模扩展时管理并发与成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些企业并未选择拼接多个计算引擎与治理层，而是通过将 Snowflake 引入其数据环境，与 Snowflake 平台上的既有工作形成互补。它们基于存储在 Snowflake 中、无需移动的开放数据，直接部署了统一且强大的分析引擎，覆盖整个数据资产。这一转变使得它们能够加速创新、简化运营，并在无需重构数据平台的前提下，交付可信的分析与AI能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在上述案例中，三个核心架构原则始终贯穿其中：&lt;/p&gt;&lt;p&gt;就地访问数据：直接在各处数据存储位置进行处理——无论是Iceberg表、Delta表还是Parquet文件，无需迁移或复制数据；实现规模化高性能：在业务量增长时，以稳定可靠的性能支持高并发关键业务负载，确保性能表现可预测；统一分析与人工智能：通过统一的分析平台，赋能全组织各团队优化决策流程，打破数据孤岛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake 并未取代这些机构的开放架构体系，而是为其数据提供了所需的性能与可靠性，从而化解了开放性与运营稳定性之间的取舍难题。它帮助 WHOOP 达成服务等级协议（SLA）、助力 BMW 降低成本，并提升了 Indeed 数据团队的价值产出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下能力展示各团队如何在不改变数据存储位置的前提下，为开放数据体系引入生产级分析引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;更便捷地运行分析功能与人工智能，无论您的数据位于何处&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在组织探索架构效率的今天，一个共识正逐渐成形：与其在系统间迁移数据，不如将数据作为单一受治理副本留存，并将分析引擎与人工智能引擎部署至数据所在之处。Snowflake 提供统一的引擎与世界一流的平台，助力企业将这些数据转化为可信的分析洞察与人工智能应用。BMW、Indeed 和 WHOOP 等案例展示了不同行业如何借助这一架构，实现更快决策、更强治理控制与更高效运营。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您的数据已准备就绪。现在，是时候让它们发挥价值了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/bringing-ai-analytics-lakehouses/&quot;&gt;https://www.snowflake.com/en/blog/bringing-ai-analytics-lakehouses/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/kwoK6RQOoOvhxfoUHR7d</link><guid isPermaLink="false">https://www.infoq.cn/article/kwoK6RQOoOvhxfoUHR7d</guid><pubDate>Thu, 12 Feb 2026 03:46:32 GMT</pubDate><author>Amit Kapadia</author><category>Snowflake</category><category>数据湖仓</category><category>AI&amp;大模型</category></item><item><title>谷歌为 Gemini 3 Flash 推出 Agentic Vision 功能</title><description>&lt;p&gt;谷歌已为 Gemini 3 Flash &lt;a href=&quot;https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/&quot;&gt;添加智能体视觉（Agentic Vision）功能&lt;/a&gt;&quot;，将视觉推理与代码执行相结合，实现“基于视觉证据的精准回答”。据谷歌介绍，这不仅能提升准确性，更重要的是解锁了全新的 AI 驱动行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单地说，Gemini 3 Flash 不再是一次性分析图像，而是以类似智能体的方式进行视觉调查：规划步骤、操作图像，并在回答问题之前通过代码验证细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这形成了一个“思考—&amp;gt;行动—&amp;gt;观察”的循环：模型首先分析提示词和图像，制定多步骤方案；然后生成并执行 Python 代码来操作图像并提取额外信息，如裁剪、缩放、标注或计算；最后将转换后的图像添加到上下文中，再生成新的回答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌表示，这种方法在大多数视觉基准测试中将准确率提升了 5% 至 10%，主要归功于两大因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，代码执行允许通过放大图像中的较小视觉元素（如微小文字）进行细粒度检查，而非依赖猜测。Gemini 还能通过绘制边界框和标签来标注图像，从而加强视觉推理能力，例如正确计数物体。谷歌表示，借助此类标注，他们已经解决了手部数字计数这一众所周知的“难题”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，原本需要 AI 模型直接处理的视觉算术和数据可视化任务可以转移给 Python 和 Matplotlib 编写的代码来完成，从而减少基于图像的复杂数学运算可能产生的幻觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对谷歌的这次发布，X 用户 Kanika &lt;a href=&quot;https://x.com/KanikaBK/status/2016397455773073587?s=20&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;读完这个，再回头看早期的视觉工具，感觉都不完整了。过去存在那么多边缘案例，仅仅是因为模型无法进行视觉干预或验证。智能体视觉感觉像是所有人最终都会采用的方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2ay0d9/&quot;&gt;Reddit 用户 Izento 评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这带来的影响是巨大的。本质上，他们为 AI 在实际物理机器人中实现视觉推理带来了可能性。机器人将拥有更强的情境感知和智能体能力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其他 Reddit 用户指出，&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2az67v/&quot;&gt;ChatGPT 已经通过代码解释器（Code Interpreter）采用类似方法相当长一段时间了&lt;/a&gt;&quot;；尽管如此，它&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2cy696/&quot;&gt;似乎仍然无法可靠地数清手指头数目&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌的智能体视觉路线图涵盖更多隐式交互行为，如无需明确提示即可自动触发缩放、旋转等操作；新增网络搜索、反向图像搜索等工具，丰富模型可调用的参考依据；并将支持扩展到 Flash 之外的其他 Gemini 系列模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户可通过 Google AI Studio 和 Vertex AI 中的 Gemini API 使用智能体视觉，并已开始以“思考模式（Thinking mode）”在 Gemini 应用中逐步推出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/google-gemini-agentic-vision/&quot;&gt;https://www.infoq.com/news/2026/02/google-gemini-agentic-vision/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2WMVYZUZHPArVTCUZzc4</link><guid isPermaLink="false">https://www.infoq.cn/article/2WMVYZUZHPArVTCUZzc4</guid><pubDate>Thu, 12 Feb 2026 02:17:39 GMT</pubDate><author>Sergio De Simone</author><category>计算机视觉</category></item><item><title>LinkedIn利用GitHub Actions、CodeQL和Semgrep进行代码扫描</title><description>&lt;p&gt;LinkedIn&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;重新设计了其静态应用安全测试（static application security testing，SAST）流水线&lt;/a&gt;&quot;，以便在基于GitHub的多仓库开发环境中提供统一、可强制执行的代码扫描能力。该举措源于公司的安全左移（shift-left）战略，也就是通过在Pull Request中直接提供快速、可靠且可落地的安全反馈，增强LinkedIn代码与基础设施的安全性，帮助保护用户与客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从宏观层面来看，&lt;a href=&quot;https://en.wikipedia.org/wiki/Static_application_security_testing&quot;&gt;SAST&lt;/a&gt;&quot;指的就是通过分析源代码，在开发生命周期早期识别潜在的漏洞。在LinkedIn的规模下，传统方案依赖多个相互独立的扫描器与定制化集成，导致覆盖度不均、流水线健康状况可见性有限，并给开发者带来额外的负担。此次重新设计旨在标准化扫描能力、简化接入流程，并将安全更深度地嵌入开发者工作流，同时避免引入性能瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在设计初期，LinkedIn工程师确立了核心指导原则，那就是优先以开发者为中心的安全设计，最小化对工作流的干扰；具备可扩展性，允许其他团队添加规则或集成；具备高韧性，避免故障影响开发者；具备可观测性，能够在大规模场景下监控覆盖范围与性能表现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新架构基于&lt;a href=&quot;https://github.com/features/actions&quot;&gt;GitHub Actions&lt;/a&gt;&quot;进行编排，并整合了两款核心扫描引擎&lt;a href=&quot;https://codeql.github.com/&quot;&gt;CodeQL&lt;/a&gt;&quot;与&lt;a href=&quot;https://github.com/semgrep/semgrep&quot;&gt;Semgrep&lt;/a&gt;&quot;，选择它们是因为二者覆盖范围互补且易于扩展。LinkedIn工程师实现了自定义工作流，用于管理规则执行、编排扫描流程并处理扫描结果。所有漏洞发现结果均基于&lt;a href=&quot;https://docs.github.com/en/code-security/reference/code-scanning/sarif-support-for-code-scanning&quot;&gt;SARIF&lt;/a&gt;&quot;标准进行规范化，并补充元数据，为开发者与安全团队提供清晰的修复指引与可落地的上下文信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/25a5bc4a224b0afc10c8173d23fc95bc.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用CodeQL的GitHub Actions工作流的宏观概览&amp;nbsp;(图片来源：&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;LinkedIn博客文章)&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LinkedIn工程师最初希望使用GitHub Required Workflows来强制执行安全流水线，并在数万个仓库中实现定时扫描，但该功能不支持定时任务与自动部署。因此，工作流文件必须被推送到每个仓库才能可靠地传播变更，这在大规模场景下会带来一定的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决该问题，LinkedIn在每个仓库中部署了轻量级的桩工作流（stub workflow），将实际执行委托给集中维护的中心工作流。这种设计使得扫描逻辑、强制策略与可观测性埋点的更新能够即时生效，无需修改单个仓库。同时，还有一套漂移管理系统（Drift Management System） 持续校验桩工作流的存在与否与配置情况，新仓库也会自动预置该文件。这套组合方案确保了LinkedIn多仓库环境下的统一覆盖与强制执行，在大规模场景下保持可靠性与开发者工作流效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;强制机制通过&lt;a href=&quot;https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/managing-rulesets/about-rulesets&quot;&gt;GitHub仓库规则集（repository rulesets）&lt;/a&gt;&quot;来实现，也就是阻塞Pull Request合并，直到静态分析完成且漏洞处于可接受的风险阈值内。为避免扫描器故障或基础设施异常中断开发者流程，LinkedIn构建了多重安全机制，包括紧急停止开关（kill switches）与自动降级策略。在故障场景下，系统会注入空SARIF报告以解除阻塞的合并请求，同时仍会采集遥测数据用于事后分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/13849a0b375ed0e4b0a2684223112856.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阻塞模型的流程（图片来源：&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;LinkedIn博客文章)&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如LinkedIn的工程师所强调的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们从碎片化的生态系统，转向了一套统一、原生基于GitHub的安全流水线，在不拖慢开发者速度的前提下，提供一致的覆盖度与可落地的安全反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该SAST流水线会收集详细的执行指标、故障报告与接入统计数据，使安全团队能够监控覆盖范围、可靠性与组织级影响。LinkedIn指出，SAST只是整体应用安全战略的一部分，该战略还包括依赖项扫描与密钥检测，共同构成一套覆盖代码与基础设施的一体化安全防护体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/linkedin-redesigns-sast-pipeline/&quot;&gt;LinkedIn Leverages GitHub Actions, CodeQL, and Semgrep for Code Scanning&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/8YqVuKOhz6RzKmmxEFje</link><guid isPermaLink="false">https://www.infoq.cn/article/8YqVuKOhz6RzKmmxEFje</guid><pubDate>Thu, 12 Feb 2026 02:15:34 GMT</pubDate><author>作者：Leela Kumili</author><category>后端</category></item><item><title>在遗留规则误拦合法流量后，GitHub重构了分层防御体系</title><description>&lt;p&gt;GitHub的工程师近期排查发现，用户反馈意外出现了“请求过多（Too Many Requests）”错误，其根源在于部分滥用防护规则在触发其生效的安全事件结束后，仍被意外长期启用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.blog/engineering/infrastructure/when-protections-outlive-their-purpose-a-lesson-on-managing-defense-systems-at-scale/&quot;&gt;据GitHub说明&lt;/a&gt;&quot;，受影响的用户并非产生了高流量请求，只是“发起了少量常规请求”，却依然触发了防护机制。经调查，这些早期为应对安全事件制定的规则，其依据的流量特征在当时与滥用行为高度相关，但后续却开始匹配部分未登录用户的合法请求。GitHub表示，这类检测机制结合了行业标准的指纹识别技术与平台专属的业务逻辑，而“多维度信号组合偶尔会产生误判”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GitHub还量化了这套多层级检测信号的实际运行表现。在匹配到可疑指纹的请求中，仅有一小部分会被拦截，只有同时触发业务逻辑规则的请求才会被阻断，这类请求约占指纹匹配请求的0.5%~0.9%；而误判请求在总请求量中的占比则极低，约为每10万次请求仅出现数次。尽管如此，GitHub在博文里强调，该问题对用户造成的影响仍不可接受，并以此次事件揭示了一个更普遍的运维问题：应急防护规则在安全事件发生时通常能正常发挥作用，但随着威胁模式的演变以及合法工具和使用场景的变化，这些规则会逐渐 “失效脱节”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GitHub此次复盘的核心结论之一就是，多层级防护体系会增加定位故障根本原因的难度。工程师需要跨多层基础设施追踪请求链路，才能确定拦截行为发生在哪个环节，而实际排查的难点在于，每一层基础设施都具备合理的限流、拦截权限，要定位具体是哪一层做出的拦截决策，就必须对多个采用不同数据格式的系统日志进行关联分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7af781e97dcf3edf14fbb5872cbc0fed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：GitHub&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决当下的问题，GitHub对所有防护规则开展了全面复核，对比每条规则当前的拦截范围与最初设计的防护目标，移除了已经没有实际防护意义的规则，同时保留了针对现存威胁的防护措施。从长期来看，GitHub表示正投入资源完善防护规则的生命周期管理体系，打造更完善的跨层级可观测的能力，实现限流与拦截行为的源头追踪；将应急防护规则默认设为临时生效；新增事件后复盘机制，推动应急防护规则向可持续、精准化的解决方案演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d7fdd6a0edcff06aea8221423531ffe9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管GitHub的博文聚焦于规则生命周期管理与跨层级可观测性，但这种采用“纵深防御”架构的请求处理流水线，在其他处理互联网流量的大型平台中也十分常见。例如，&lt;a href=&quot;https://vercel.com/blog/life-of-a-vercel-request-what-happens-when-a-user-presses-enter&quot;&gt;Vercel&lt;/a&gt;&quot;公开的请求处理生命周期中提到，请求会经过其防火墙的多个防护阶段，覆盖网络层（L3）、传输层（L4）和应用层（L7），后续还会针对项目级策略增加Web应用防火墙（WAF）防护环节。Vercel还指出，各防护层级间存在反馈机制，若某条WAF规则触发了持续性拦截动作，上游防护层会提前拦截后续的同类请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种分层防护的设计并非仅存在于边缘流量管理领域：&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers&quot;&gt;Kubernetes的API服务器安全模型&lt;/a&gt;&quot;也采用了明确的阶段式设计，准入控制器会在身份认证、授权校验完成后，数据持久化之前拦截请求，形成一套结构化的校验链路，后续可在此基础上不断叠加新的策略与安全检查规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些案例共同揭示了大型系统中一种普遍的设计权衡：多层级防护机制能提升系统的抗风险能力与灵活性，但也会增加防护规则“脱离其设计背景，依然失效存在”的风险。GitHub的此次经历也印证了，纵深防御体系的长期有效性，不仅取决于防护规则的部署层级，更在于随着系统与使用模式的演变，能否清晰把控每条规则的设计初衷、实际影响与生效周期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/github-layered-def/&quot;&gt;GitHub Reworks Layered Defenses After Legacy Protections Block Legitimate Traffic&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fpVPsSlLV5XCmB9Hl7ag</link><guid isPermaLink="false">https://www.infoq.cn/article/fpVPsSlLV5XCmB9Hl7ag</guid><pubDate>Thu, 12 Feb 2026 02:12:28 GMT</pubDate><author>作者： Matt Foster</author><category>软件工程</category></item><item><title>从告警疲劳到代理辅助的智能可观测性</title><description>&lt;p&gt;如果你曾经值过班，那么你肯定知道这么一种情况。凌晨两点，电话打来，你猛然惊醒，抓起笔记本电脑开始排查。你先是检查服务仪表板，接着是分析依赖关系图，然后是查看日志，最后再查看来自三个不同监控工具的指标。半小时后你发现，这不过是虚惊一场：阈值设置过严，金丝雀部署触发了可以自动恢复的告警，或是网络瞬态波动导致了短暂峰值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你不能就这样回去睡觉。你得等等，观察一下。你必须确保告警窗口干净利落地关闭了，而且没有任何其他告警被触发。等你确信问题已经真正解决时，你已经失去了一小时的睡眠时间，更大的问题是几乎睡意全无。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种场景在各地运维团队中屡见不鲜。我们不断地调整告警阈值，试图找到一个完美的平衡点：过于敏感会被误报淹没，过于宽松则会错失真实事件。这种动态变化会导致告警疲劳——工程师被大量无需处理的告警淹没。随着时间推移，这会削弱人们对告警的信任度，并降低对真实问题的响应速度。&lt;a href=&quot;https://www.atlassian.com/incident-management/on-call/alert-fatigue&quot;&gt;关于告警疲劳的研究表明，这种响应迟滞现象普遍存在&lt;/a&gt;&quot;：安全监控领域的调查发现，超过半数的告警属于误报，而IT运维领域也呈现出类似的模式。这并非配置问题，而是监控复杂分布式系统时面临的一个根本性挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了优化告警规则，团队经常要花费无数个小时，他们应该这样做。但根本问题仍然存在：我们需要监控的范围超出了我们手动维护和解释的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;我们不愿谈论的监控悖论&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于现代系统，一个现实是它们会一直不停地增长。每增加一个新功能都会有更多的日志需要解析，更多的指标需要跟踪，更多的仪表板需要维护。最初简洁明了的架构与直观的监控，逐渐演变成了一个需要持续投入精力的庞大的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着系统的增长，维护负担也在增加。仅仅是保持可观测性基础设施的更新就需要花费团队大量的时间。新服务需要配置监控工具，仪表盘需要持续更新，流量模式变化时需要调整告警阈值。依赖关系不断变化，监控方案也必须随之调整。虽然这些工作都是例行公事，却也不可或缺，它们消耗了本可用于开发功能或提升可靠性的宝贵时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个典型的微服务架构会产生巨量的遥测数据：来自数十个服务的日志，来自数百个容器的指标，跨多个系统的跟踪信息。当事件发生时，工程师会面临一个相关性问题：哪些信号至关重要？它们如何关联？近期发生的哪些变化可能解释这种行为？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI队友加入&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初次接触可观测性AI代理的概念时，我是持怀疑态度的。这听起来像是供应商炒作与流行词汇的结合体。但随着技术的日趋成熟，早期应用方案的陆续出现，其潜力正变得越来越清晰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键转变在于将这些系统视为队友而非替代者。具体而言，这些队友擅长处理事件响应中人类厌烦的那些环节：在海量数据集中进行模式匹配，记住以往的每一个事件，并在周二凌晨两点保持警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能可观测性意味着你的监控系统不仅仅是收集指标和触发告警，还要能理解它所看到的信息。它能够：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;留意不符合常规模式的现象：不仅仅是超出阈值，还有行为中那些微妙的变化，在情况变得严重之前发现事情不对劲。串联技术栈中的各个点，将数据库延迟峰值与那些认证错误和六小时前的部署关联起来。生成有实际用处的错误信息摘要。想象一下，提供这样的错误信息，“在下午2:15部署后，认证服务延迟增加了200%；与新的Redis连接池配置相关”，而不仅仅是“错误率超过阈值”。记住制度性知识。每一个事件都能教会可观测性代理一些东西。关于缓存的那个奇怪问题？代理会记住你是如何修复的，并在下次出现类似的问题时提供建议。在规定的范围内采取行动。在适当的监督下，智能可观测性可以执行你基于定义好的策略预先批准的安全补救步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统监控与这种方法的区别在于：一个只发出告警，而另一个会分析告警的含义。传统监控告诉你有东西越过了阈值。代理辅助的可观测性会帮助解释发生了什么变化，可能与什么相关，以及接下来应该查看什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;生产环境中究竟发生了什么&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向智能可观测性的转变改变了工程工作开展的方式。在每次处理事件时，工程师不用首先花二十分钟手动在仪表板上关联日志和指标，而是可以审查AI生成的事件摘要，其中链接了部署时间、错误模式和基础设施变化。事件工单自动填充了上下文信息。根因分析现在从一个清晰的假设开始，而不像以前那样需要先进行广泛的调查。工程师仍然需要做出决定，但他们是基于分析数据而不是原始信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这节省了时间，减少了认知负荷，让你最好的工程师们可以把更多的时间投入到功能建设上，而不是用在救火上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;实际路径&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你正在考虑采用智能可观测性，下面是一个分阶段采用的步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第一阶段：只读学习&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，将现有的遥测数据（日志、追踪信息、指标等）输入到一个处于观察模式的智能代理中，它会分析实时和历史数据，学习模式并标记异常，但不触发告警或执行操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个阶段的目标是建立信任。团队看到智能代理提供了合理的建议。你捕捉到了本来可能会错过的异常。工程师们开始在深入分析日志之前查看智能代理生成的摘要信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第二阶段：启用上下文感知分析&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个阶段关乎让智能代理理解你特有的环境，并利用这些知识进行智能调查。它有两个协同运作的关键组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;添加操作上下文&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向智能代理提供专有知识：运行手册、服务所有权文档、架构图、依赖关系图和过往事件报告。这些信息将智能代理从一个通用的模式匹配器转变为一个理解特定系统的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，当它检测到异常时，它有一个上下文。它不再简单地显示“检测到高错误率”，而是能具体说明：“（通信团队负责的）通知服务出现高错误率。该服务依赖于邮件网关和消息队列。最近部署记录：v1.8.2版本于3小时前部署”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;启用智能关联&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了这个上下文，智能代理现在可以关联日志、指标和追踪信息中的相关信号。它将模式与过去的事件做匹配，并根据系统的实际拓扑和历史信息提出调查路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面这个例子是一个成熟的智能代理生成的分析结果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e5c6223c5639870e3df05401485d03e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能代理不做决定。相反，它正在做工程师手工操作通常需要20分钟才能完成的仪表板跳转、日志搜索及其他相关工作。它提供了一个连贯的叙事和可操作的调查步骤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第三阶段：根据运营经验定义自动化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在观察和咨询模式下运行智能代理几周后，你会发现某些规律。有些事件会重复发生。特定的诊断步骤会反复出现。有些补救措施简单直接且风险低。这时，你就可以定义哪些工作流程在什么条件下可以自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键是从实际的操作经验出发，而不是理论。查看事件历史并问这样的问题：我们反复采取了哪些行动？哪些是安全且可预测的？哪些可以在低风险时段自动运行？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;常见的自动化选项包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;重启不健康的Pod或容器，它们未能通过健康检查运行标准诊断脚本，收集分析数据流量高峰时段，在预设的边界内扩展资源在发生异常时触发日志收集或性能分析&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但自动化需要有所限制。在启用任何自动化操作之前都应定义清晰的策略：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何时可以自动运行？也许只在非高峰时段，或者最初只针对非关键服务，或者从不在部署窗口或主版本发布期间运行。什么需要升级？高严重性事件、面向客户的服务或代理的置信度低于某个阈值时，必须始终由人工介入处理。什么需要审计？每个自动化操作都应该记录其背后的逻辑依据、触发操作的上下文和操作结果。这有助于建立责任机制，并随着时间推移不断完善自动化规则。谁可以控制或暂停自动化？需要有一个简单的方法，让工程师可以在需要时（如维护、测试或敏感时期）禁用自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从一两个低风险的自动化开始，观察它们在一周或两周内的表现。随着信心的建立和规则的完善，逐渐增加更多的自动化操作。我们的目标不是无人操作，而是要消除重复的劳动，使团队可以专注于需要人类判断的复杂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;集成的真实情形&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可能不需要替换任何东西。&lt;a href=&quot;https://landscape.cncf.io/&quot;&gt;大多数智能可观测性平台&lt;/a&gt;&quot;都集成了现有的监控和可观测性工具。无论是使用开源解决方案还是商业平台，智能代理通常都可以与你当前的技术栈协同工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以将其视为是在现有基础设施之上增加一个智能层，而不是推倒重来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;当它正常工作时&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在管理平台可靠性和观察团队如何应对监控挑战时，你会发现某些特定的模式。随着组织尝试使用智能可观测性系统，往往会出现类似下面这样的改进：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事件解决速度更快（例如，“我们的事件解决时间从平均45分钟减少到18分钟，而这仅用了三个月”）。提升值班生活质量（例如，“我现在可以一觉睡到天亮。智能代理处理日常事务，只有在遇到需要人类判断的事情时才会叫醒我”）。提升学习便利性（例如，“每个事件都会增加制度性知识。团队的新成员可以要求智能代理：‘告诉我过去的五次数据库事件是什么以及如何解决的’”）。提高主动捕获能力（例如，“在问题变成事件之前就发现并修复了它们。这种转变可能令人感到陌生，因为团队正从被动地响应事件转向主动地预防。”）。工程时间从调试转移到分析（例如，“工程师花在浏览日志上的时间减少了，花在分析模式和验证修复上的时间增加了。这切实提升了运维效率。团队从救火模式转变为真正地改善系统”）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;缺点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是实践中通常会遇到的几个挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI不会在第一天就神奇地理解了你的整个系统。智能代理需要时间来学习正常的模式应该是什么样子，所以早期的建议可能会偏离目标。它可能会做出不相关的关联，或提供明显没用的建议。经过数周的学习之后，洞察力才会真正变得有价值。设置上下文比你想象的更耗时。听起来，向智能代理提供运行手册、架构文档和专有知识很简单，但从中可以看出有多少关键信息只存在于人们的头脑中或过时的文档里。预计要实实在在花些时间来组织和上传这些上下文。真实存在的学习曲线。团队需要了解如何配置、信任和验证智能代理的行为，并为此预留时间。文化阻碍。会有一些工程师不信任AI，也会有一些人担心工作保障。直面这个问题，明确说明增强团队能力与人员替代之间的区别。调试调试器比调试系统本身更难。当智能代理判断错误时，问题可能出在信号、上下文和学习模式的组合方式上，而不是任何单一的指标或日志文件中。这会降低透明度，因此可解释性很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一种用于评估智能可观测性的简单检查方法&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果不确定智能可观测性是否适合你，那么可以问下你的团队下面这些问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在事件处理期间，我们是否反复运行相同的诊断命令？我们是否花费了大量的时间来关联多个工具之间的信号？误报是否导致我们错过了真正的问题？如果我们的初级工程师能够即时访问高级工程师积累的事件知识，他们是否能更快地响应事件，降低风险并减少混乱？我们是否花费了更多的时间灭火而不是预防？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果有两个或更多问题的答案是“是”，你将从中获益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;未来展望&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;系统变得越来越复杂，数据量在不断增加，停机成本变得越来越高，而人脑并没有变得更大或更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能可观测性的目标不是要取代工程师，而是为他们赋能：大规模地识别模式，保留以往事件的知识，并在毫秒（而非分钟）内对信息作出反应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从小处开始，逐步建立信任。让系统自己证明自己。可靠性的未来不是人类也不是AI，而是拥有AI的人类，使他们在工作中的表现更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也许，只是也许，我们都可以多睡一会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;免责声明：本文所表达的观点和意见仅代表作者个人立场，不代表其雇主机构的观点、政策或实践。所有示例和建议均基于行业普遍做法及个人经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/agent-assisted-intelligent-observability/&quot;&gt;https://www.infoq.com/articles/agent-assisted-intelligent-observability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6UIpbWJ5TWjcIyJmMAPE</link><guid isPermaLink="false">https://www.infoq.cn/article/6UIpbWJ5TWjcIyJmMAPE</guid><pubDate>Thu, 12 Feb 2026 02:09:59 GMT</pubDate><author>作者：Rohit Dhawan</author><category>可观测</category></item><item><title>如何使用Durable Objects处理响应和进行中的请求</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存是工程师优化分布式系统时首先采用的工具之一。我们会缓存已完成的响应（如数据库查询结果或HTTP响应体），以避免重复执行昂贵的任务。然而，传统缓存未能解决一个经常被忽视的低效源头，即重复的进行中请求（duplicate in-flight request）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当多个客户端几乎同时请求同一资源时，缓存未命中会触发相同的计算会并行开始执行。在单进程的JavaScript应用中，通常会通过在内存中存储进行中请求的Promise来缓解该问题，使后续调用者可以等待同一个结果。在其他语言和运行时中，可通过不同的并发原语实现类似效果，但底层假设是相同的，也就是共享内存和单一执行上下文。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分布式、无服务器或边缘环境中，这一假设就难以成立了。每个实例都有自己的内存，任何形式的进行中请求去重都仅限于单个进程的生命周期和范围之内。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;工程师通常会在缓存之外引入第二种机制，比如，锁、标记（marker）或协调记录（coordination record），以跟踪正在进行的工作。这些方法难以进行推理，并且经常退化为轮询或粗粒度的同步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文提出了一个不同的模型，那就是将已完成的响应和进行中的请求视为同一缓存条目的两种状态。借助Cloudflare Workers和Durable Objects，我们可以为每个缓存键分配一个单一、权威的所有者。该所有者可以安全地持有正在进行工作的内存表示，允许并发调用者等待它，然后在工作完成后将条目转换为已缓存的响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该模式并没有引入单独的协调层，而是将缓存和进行中请求去重统一在单一抽象之后。尽管它依赖于并非普遍可用的运行时特性，但在支持按键单例执行的环境中提供了一种简洁且实用的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深入分析该问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从宏观层面来看，问题不在于缓存本身，而是在于缓存条目存在之前发生了什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑一项代价高昂的操作：数据库查询、外部API调用或CPU密集型计算。在分布式边缘环境中，多个客户端可能在非常短的时间窗口内请求同一资源。如果缓存尚未包含该键的值，每个请求都会独立触发相同的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为边缘运行时会有意地进行水平扩展，这些请求通常由不同的执行上下文处理。每个上下文都观察到相同的缓存未命中，并且会继续执行，就好像它是第一个请求者一样。结果就是冗余的工作激增，而缓存本应防止这种情况发生，但由于缓存只有在第一个请求完成后才能发挥作用，因此这种现象难以避免。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为应对措施，很多系统引入了额外的机制来跟踪进行中的工作。一个缓存用于已完成的结果，而另一个结构（有时是内存映射，有时是分布式存储）用于标记请求为“进行中（in-flight）”。这种分离迅速增加了复杂性。请求的生命周期现在必须在两个独立系统之间协调，必须仔细处理竞争条件、失败和超时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按照进程进行内存去重部分缓解了这个问题，但是这种方法仅限于单个运行时实例的范围内。在无服务器和边缘环境中，实例的生命周期很短，并且在设计上是隔离的。两个同时请求不同节点的请求即使它们在逻辑上是相同的，也无法共享彼此的进行中状态。随着流量增长或地理分布更广泛，这种优化的效果迅速降低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它造成的结果就是，在单体或长期存活服务中表现良好的模式，但在水平扩展最激烈的环境中却崩溃了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存结果的缺失和第一次计算完成之间的这个时间差恰好是传统缓存策略无能为力的地方，也是进行中请求去重在分布式运行时中变得既必要又特别困难的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为何Durable Objects是合适的方案？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上一节所述的困难是现代无服务器和边缘平台设计的直接结果。隔离的执行上下文、短暂的进程和水平扩展都是它们的特性，而非缺陷。因此，任何针对进行中请求去重的解决方案都必须在这些约束下工作，而不是试图绕过它们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare &lt;a href=&quot;https://developers.cloudflare.com/durable-objects/&quot;&gt;Durable Objects&lt;/a&gt;&quot;提供了一套小但关键的保障措施，使得这一点成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，Durable Object 实例是一个按键存在的单例（per-key singleton）。对于给定的对象标识符，所有请求都路由到同一个逻辑实例，无论它们来自哪里。这立即消除了所有权的模糊性：对于给定的缓存键，只有一个地方可以存放进行中的状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，Durable Objects提供了跨请求共享的可变内存。与传统Worker 不同，Durable Object可以在请求之间保留内存状态。这允许它持有正在进行的工作的表述，例如一个正在进行的计算，而无需外部协调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，对Durable Object的请求是按顺序处理的。这种串行执行模型消除了在检查或更新进行中状态时需要显式锁定的需求。检查是否已经有计算正在进行中，如果没有的话，则创建它，并且它能够附加额外的等待者，这些都可以在单一执行上下文中确定性地进行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合这些特性，Durable Object能够充当进行中和已完成缓存条目的权威所有者。调用者不再需要询问“这个请求是否已经在其他地方开始了？”，而是简单地将请求转发给负责该键的对象并等待结果即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;重要的是，这种能力不是支持最终一致性的键值存储可以模拟的。KV系统非常适合持久化已完成的结果，但它们无法在避免使用轮询或外部信号的情况下，表示执行中的过程或允许多个调用者等待同一块内存中的操作。相比之下，Durable Objects对进行中工作的支持成为了首要的关注点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不意味着Durable Objects在所有情况下都适用。本文描述的模式依赖于它们的单例和内存保证，因此只适用于提供类似语义的运行时。在这些保证能够达成的环境中，Durable Objects提供了一个干净且最小的基础，可以统一缓存和进行中请求去重，而无需引入额外的协调层。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;突破Cloudflare的适用性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管本文中的示例使用了&lt;a href=&quot;https://developers.cloudflare.com/workers/&quot;&gt;Cloudflare Workers&lt;/a&gt;&quot;和Durable Objects，但底层模式并不是特定于Cloudflare的。重要的不是平台本身，而是上述的运行时保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运行时至少要提供：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按键单例执行：给定键的所有请求都路由到同一逻辑实例。该实例在请求间能够共享内存状态。串行请求处理，或等效的无需显式锁定的保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare Durable Objects明确满足这些要求，使它们成为一个方便且定义明确的示例。其他环境中也可以找到类似语义的功能，尽管通常以不同的名称或具有不同的权衡：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于Actor的系统，比如基于&lt;a href=&quot;https://akka.io/&quot;&gt;Akka&lt;/a&gt;&quot;或&lt;a href=&quot;https://dotnet.github.io/orleans/&quot;&gt;Orleans&lt;/a&gt;&quot;构建的系统，通过Actor身份和消息串行提供可类比的保证。在这些系统中，Actor可以自然地拥有给定键的进行中工作和已缓存的结果。有状态无服务器平台和“持久执行”模型也开始出现，不过其API和功能保证差异显著。它们的共同点是，并非所有无服务器计算都必须是无状态的，有限且范围明确的状态可简化某些协调问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，那些仅提供无状态函数并结合最终一致性键值存储的平台无法整洁地实现这一模式。没有单一权威所有者和共享内存执行上下文，进行中去重不可避免地退化为轮询或分布式锁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，本文所述模式应理解为依赖于运行时。它并不是传统缓存的通用替代方案，而是在执行模型支持的情况下才可行的针对性技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一个最小化的实现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在运行时保证确立后，实现本身就特别简单了。我们的目标不是构建一个通用的缓存，而是展示一个单一抽象如何同时处理进行中请求的去重和响应缓存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下示例展示了负责单个缓存键 Durable Object。该键的所有请求都路由到同一对象实例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;export class CacheObject {
  private inflight?: Promise&lt;response&gt;;
  private cached?: Response;


  async fetch(request: Request): Promise&lt;response&gt; {
    // Fast path: return cached response if it exists
    if (this.cached) {
      return this.cached.clone();
    }


    // If no computation is in-flight, start one
    if (!this.inflight) {
      this.inflight = this.compute().then((response) =&amp;gt; {
        // Store completed response
        this.cached = response.clone();
        // Clear in-flight state
        this.inflight = undefined;
        return response;
      });
    }


    // Await the same in-flight computation
    return (await this.inflight).clone();
  }


  private async compute(): Promise&lt;response&gt; {
    // Placeholder for an expensive operation
    // e.g. database query or external API call
    const data = await fetch(&quot;https://example.com/expensive&quot;).then(r =&amp;gt; r.text());
    return new Response(data, { status: 200 });
  }
}
&lt;/response&gt;&lt;/response&gt;&lt;/response&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该对象维护两种状态：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;inflight，表示正在进行的计算。cached，存储可用的已完成响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当请求到达时，对象首先检查是否存在缓存的响应。如果没有，它会检查是否已经有计算正在进行中。如果有的话，调用者只需等待同一个Promise。如果没有，对象就会启动计算，并将结果Promise存储在内存中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于Durable Objects按顺序处理请求，无需显式锁或原子操作。检查和创建进行中Promise的逻辑在单一执行上下文中能够确定性地执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从调用者的角度来看，这就像一个普通的缓存一样。不同之处在于，即使缓存最初是空的，并发调用者也不会触发重复的工作。一旦计算完成，所有等待的调用者都会收到相同的结果，后续请求直接从缓存响应中提供服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此示例有意省略了持久化、过期和错误处理。这些问题可以在不改变核心思想的情况下分层进行处理。比如，可选择将已完成的响应存储在键值存储中以实现持久性，但关键是，进行中的状态永远不会离开内存，从而保持了该模式的简洁性和正确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;该方法的优势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该模式的主要优势是将两个相关的关注点合并为单一抽象。它不将进行中请求去重和响应缓存视为独立的问题，而是将其建模为同一缓存条目的不同状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这会带来多项实际的优势：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，它消除了缓存无法发挥作用的情况下的重复工作。通过允许多个并发调用者等待同一个正在进行的计算，系统避免了在缓存未命中期间出现的冗余请求激增，这正是传统缓存最无能为力的场景。其次，该方法简化了系统设计。没有必要引入第二个协调层、分布式锁、与缓存数据分开存储的“进行中”标记。所有与请求合并、执行和结果重用相关的逻辑都集中在一个地方，由单一的运行时实体拥有。第三，它与JavaScript应用的编写方式自然对齐。等待共享Promise是惯用且易于理解的模式，Durable Objects使此模型可扩展到单个进程之外，而不必改变思维模型。调用者与缓存的交互，就像在本地进行一样，尽管它的执行是分布式的。第四，该模式可水平扩展而不丧失正确性。随着流量增长或地理分布更广泛，请求仍路由到每个键的同一权威所有者。行为不会随着更多边缘节点的添加而退化，这与按进程优化的常见情况不同。最后，该模型可增量扩展。过期策略、已完成响应的持久化、指标和重试均可添加，而不改变核心控制流。基本思想就是，一个所有者、一个进行中请求的计算、一个缓存结果的思路保持不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些特性使该模式适用于重复工作成本高且请求并发不可预测的工作负载，如边缘API、聚合端点或昂贵的上游集成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;权衡与限制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管优雅，此模式并非普遍适用。其有效性在很大程度上取决于底层运行时的执行模型，并引入了需仔细考虑的权衡项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最显著的限制是运行时依赖。进行中请求的去重需要具有共享内存状态的单一权威所有者。如果没有按键单例执行，就无法整洁地实现该模式。如果试图使用最终一致性的键值存储来模拟它，必然会导致轮询、分布式锁或其他形式的协调，从而破坏原有的简洁性。实现本身可能是平平无奇的。虽然最小化示例很小，但生产就绪版本必须考虑错误传播、重试、超时、驱逐和内存限制。必须小心确保失败的计算不会使系统处于永久的“进行中”状态，并且缓存的响应能够正确失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个重要的考虑因素是相关性。在许多架构良好的系统中，重复的进行中请求已经很少见了。幂等的上游API、自然地请求分散或粗粒度的缓存可能使进行中请求的去重变得无关紧要。在这些情况下，引入该模式可能会增加复杂性，而不会带来有价值的好处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个就是扩展方面的权衡。将给定键的所有请求路由到单一所有者引入了一个自然的序列化点。对于单个键非常热门的工作负载，这可能会成为瓶颈。在这些情况下，分片策略或替代缓存方法可能更合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，该模式并不替代传统的缓存策略。它是对它们的补充。已完成的响应可能仍需要持久化在键值存储或HTTP缓存中，以在进程驱逐或冷启动时生存下来。然而，关键在于，持久化应该只适用于已完成的结果，将进行中的状态移入外部存储会抵消该方法的好处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，该模式应被视为一种针对性的优化，而非默认的架构选择。如果运行时支持它并且工作负载证明它是合理的，那么统一响应缓存和进行中请求去重可以显著减少冗余工作。当这些条件不满足时，更简单的设计通常会更明智。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文概述了一个在分布式JavaScript运行时中统一响应缓存和进行中请求去重的模式：通过依赖于按键单例执行和共享内存状态，可以将正在进行的计算及其最终结果视为同一缓存条目的两种状态，从而消除重复性的工作，而无需引入轮询或外部协调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要强调的是，这种模式主要是一种设计提案，而不是经过实战验证的方案。虽然底层原语（Durable Objects、Promise和串行执行）是众所周知的，但这里描述的组合尚未在生产系统中得到广泛验证。关于运维行为、可观测性和长期性能特征的问题仍然存在，并且需要进一步探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管如此，该模式的价值可能在于它清晰地揭示了缓存与执行之间的关系。它表明，进行中请求去重的困难并非分布式系统所固有的，而是我们通常使用的执行模型所致。当运行时能够为每个键提供单一的权威所有者时，问题就大大简化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着无服务器和边缘平台的不断发展，有状态执行模型变得越来越普遍。这样的模式表明，重新审视长期以来的假设，比如，缓存和协调之间的严格分离，可能会产生更简单、更具表现力的设计。无论这种特定方法是否证明了广泛的实用性，还是仅仅作为一种小众优化存在，它都凸显了未来运行时和应用架构的重要方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/durable-objects-handle-inflight-requests/&lt;/p&gt;</description><link>https://www.infoq.cn/article/espgcPmH8wYJiLBkvI41</link><guid isPermaLink="false">https://www.infoq.cn/article/espgcPmH8wYJiLBkvI41</guid><pubDate>Thu, 12 Feb 2026 02:07:10 GMT</pubDate><author>作者：Gabor Koos</author><category>软件工程</category></item><item><title>3000亿美元因Agent一夜蒸发！纳德拉、MongoDB CEO等宣告：传统SaaS已走到拐点</title><description>&lt;p&gt;上周，SaaS、数据和软件类投资公司的市值蒸发了约3000亿美元。这并非因为盈利不及预期或宏观经济冲击，而是因为一款人工智能产品发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场危机已经持续数月。等到市场做出反应时，IGV软件指数已较9月下旬的峰值下跌了约30%。上周发生变化的不是方向，而是速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几家根基深厚的企业软件公司股价在一天之内大幅下跌。Salesforce、ServiceNow、Adobe 和 Workday 的股价均下跌约 7%。Intuit 的股价更是暴跌近 11%。与此同时，整个行业的估值倍数也急剧下降。软件公司的平均预期市盈率在短短几个月内从约 39 倍暴跌至约 21 倍。做空者已通过押注传统 SaaS 业务在 2026 年获利超过 200 亿美元，并且还在加倍下注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除非核心假设被打破，否则市场不会抹去如此巨大的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚打破的假设是传统 SaaS 增长模式的可持续性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;纳德拉判断 SaaS 已死&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去二十年的大部分时间里，企业软件受益于异常稳定的经济形势。软件开发成本高昂，转换成本也很高，数据都存储在专有系统中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦某个平台成为记录系统，它便会一直占据这一位置。这种信念支撑着从公开市场估值倍数到私募股权收购再到私募信贷承销等方方面面。经常性收入被视为可预测性的指标。合同被认为具有粘性。现金流被认为具有韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能现在正在同时测试该逻辑的每个部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周令投资者感到恐慌的并非人工智能能够生成更优质的功能。软件公司多年来一直在功能竞争中生存下来。真正的变化在于，现代人工智能系统能够直接取代大部分人类工作流程。研究、分析、起草、核对和协调不再需要局限于单一应用程序，它们可以跨系统自主执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Social Capital创始人、知名风险投资家 &amp;amp; 企业家 Chamath Palihapitiya&amp;nbsp;在 X 上发帖，直言不讳地描述了引发此次抛售的情绪：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“SaaS 大崩溃已经开始，而且没有回头路了……一种全新的以人工智能为导向的工作流程即将到来……SaaS 大崩溃已经开始。”&amp;nbsp;简而言之，那种依赖高增长、却长期低盈利甚至无盈利的SaaS发展路径，正在失去市场信任。&amp;nbsp;核心矛盾集中在两方面：短期来看，增长是否真正可持续？长期来看，在人工智能浪潮冲击下，盈利可能性是否正变得渺茫？&amp;nbsp;过去，几乎每家SaaS公司都向投资者与员工描绘过同样的蓝图：先以速度抢占市场，未来再兑现丰厚利润。然而，随着AI技术快速发展，这一逻辑基础可能已被颠覆。眼下最关键的命题是：许多SaaS企业的增长，是否会迅速被成本更低、以AI为核心的新解决方案取代？&amp;nbsp;如果你是一家依赖风险投资、产品仍基于传统“启发式算法+API+增删改查”模式的SaaS初创公司，那么你需要警惕——一套以AI为导向的新工作流程，可能正在瞄准你的市场。&amp;nbsp;对此，私募市场投资者已率先做出反应。他们意识到，继续为短期增长注入资金很可能得不到相应回报。公开市场的投资者同样转变了预期，不再相信长期盈利的故事，转而寻找更具韧性的领域。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/eaf013f045c16afdbd442d5dffde6ca6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在创立 Social Capital 之前，Chamath 是 Facebook 高级管理团队的早期成员，领导开发和推出推动公司全球增长的新平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Chamath还强调，以上种种都标志着一场深刻的变化：过去15年中盛行的那套风险投资与估值逻辑，正在被重新校准。下图所呈现的趋势，正是这一转变的直观体现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/09d3c7a62045409e61679fee4a8f06e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Chamath 的这一判断早在一年前，微软 CEO 纳德拉已经给出同样的判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一年前，微软纳德拉发出“SaaS 已死”的言论后在网上引发轩然大波。随后他做客了一档访谈栏目，详细解释了他为什么会这样说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在访谈中，纳德拉表示，在他看来，每一次真正的平台迁移，都会带来核心应用架构的根本性变革。回顾历史，从关系型数据库诞生开始，人们首次清晰地实现了数据层与应用程序的逻辑分离。在此之前，应用与数据库往往紧耦合，而关系型数据库的出现，通过引入关系代数和SQL，将数据独立为通用层，从而允许在其之上灵活构建业务逻辑。他继续说道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“随着网络等平台的出现，人们不断探索新的应用架构和业务逻辑组织方式。当前，我们正面临一场规模相当、甚至更大的变革，这次变革的核心在于应用逻辑本身。其关键在于，未来的智能体将不再受限于任何单一的SaaS应用及其私有数据。我们将进入一个以‘智能体为中心’的视角，由任务和意图驱动。智能体将能够跨越多个SaaS应用，对业务逻辑进行协调与编排。它们通过调用各类API工具，实现跨系统操作。更具体地说，我们可以在智能体层对模型进行训练，使其理解并驾驭多个SaaS应用。这是未来明确的发展方向。”&amp;nbsp;因此，当前的SaaS应用，其本质可被视为一个集成了大量定制化业务逻辑的‘增删改查’数据库。未来的变革在于，这个承载核心数据的‘数据库’层，其调用与编排将从原有SaaS应用的封闭业务逻辑中解放出来，成为一个更独立、更通用的可编排层。&amp;nbsp;以我个人的工作流为例：我只需向Copilot提出‘销售情况’的指令，它便能自动查询动态CRM系统获取客户信息，同时从Office 365中提取相关数据，整合后生成报告并与团队成员共享。我无需登录任何一个独立系统。过去，尽管每个企业都部署了CRM，但实际使用率很低，因为访问流程繁琐。现在，由于智能体的存在，查询CRM数据变得异常简便，因为它与其他所有工作流智能体无缝协同。这正是变革所在。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早在 Agent 还未大规模走到企业应用的2025年，纳德拉已经预测到，未来企业在招聘时，雇佣的将不仅仅是个人，更是其所携带的、由智能体构成的“工作流生态系统”。这可以理解为一组相互协作的智能体集群。一个恰当的类比是：今天招聘数据分析师，实际上是雇佣了“分析师及其所构建的电子表格库”；未来，员工将携带其“个人智能体工具篮”加入工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，当时这种趋势已现端倪。例如，针对存储在SharePoint中的大量领导团队会议文档与核心数据，我已经可以训练一个专属的SharePoint智能体，随时进行自然语言查询，无需跳转至独立界面。这极大地提升了效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;两周前，纳德拉再次做客一一档访谈栏目，聊到了AI时代的的商业革命：SaaS、OpenAI与微软将走向何方？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;纳德拉强调，下一代SaaS企业必须主动拥抱智能体。它们需要将智能体深度集成，甚至作为一等公民开放给Copilot等平台，并据此革新自身的商业模式。这不仅是一个巨大的市场机会，更是对任何现有SaaS公司（无论其宣称的“护城河”有多宽）的强大竞争向量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SaaS“崩溃”背后，实为市场重心迁移&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高盛近期的一项研究预测，到本十年末，人工智能代理将显著扩大整个软件市场，并攫取不成比例的利润份额。在他们的框架下，代理不仅仅是增强应用程序的功能，它们本身就成为了工作界面。到2030年，超过60%的软件经济效益可能会通过 Agent 系统而非传统的 SaaS 服务实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/db7ec0a0cae6ba01635679155aa9f496.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件行业的利润池预计将转向人工智能代理。来源：高盛、Gartner&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是关键区别。市场正在增长，而不是萎缩。但随着智能、内存和执行能力从静态应用程序转移到跨工具运行的自适应系统中，传统软件的经济效益正在被削弱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，企业并非在软件本身上花费更少，而是在许可证费用上花费更少，在最终成果上花费更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变既解释了抛售潮，也解释了其中的机遇。当利润池的流动速度超过收入的减少速度时，公开市场会立即做出反应，而私募市场则会随后跟进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些影响在私募股权和私募信贷领域尤为显著。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去十年，大量资金涌入软件行业，其背后基于一系列共同的假设：可预测的收入、低客户流失率和高回收价值。这些假设使得高杠杆和契约结构成为合理选择，并将软件行业的现金流视为经济中最安全的现金流之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能不会在一夜之间摧毁这些投资组合。它会造成滞后效应。支出压缩先于客户流失出现。利润率下降先于违约显现。经济现实与报告的指标存在差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;和纳德拉、Chamath 等大佬对 SaaS 有着同样观点的还包括MongoDB CEO&amp;nbsp;CJ Desai。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MongoDB CEO：产品终将被替代，平台才能长青&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，在《No Priors》播客首次现场录制中，主持人Sarah Guo与这位软件开发者出身的掌舵者展开深度对话，共同剖析了为何全球纯软件业务营收超百亿美元的公司屈指可数。对此，CJ Desai给出的答案是：产品终将被替代，而平台方能长青。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：自2022年以来，软件的未来被打上了问号。这不仅来自投资者群体，也来自客户。这是软件栈的一个非常关键的转折点。当你审视软件栈时，你会问：什么东西是必然会存在的？&lt;/p&gt;&lt;p&gt;今天有多少纯粹的软件公司营收能超过一百亿美元？个位数而已。为什么？软件行业历史悠久，由许许多多像你这样聪明的人创立。为何营收超过百亿的公司寥寥无几？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：因为真正的平台是稀有的。速度至关重要。当技术转型发生时，你是否在以最快的速度构建？你是否在那次技术转变中不断学习？无论是互联网时代、AI时代还是移动时代，你是否在快速转向？你必须保持领先。一旦落后，投资者或客户总会问你那个问题：贵公司的未来在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：CJ Desai，你曾在那些平台型企业和基础设施公司工作，最近成为了MongoDB的CEO。我觉得我们刚才谈到的一个问题，每个投资者都会问你，科技生态圈里的每个人也都在思考：当你可以生成一堆软件时，软件的价值究竟何在？我很想听听你的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：这是个很犀利的问题开场，我喜欢，能确保大家都清醒着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当我们思考技术转型，无论是互联网时代、大型机时代，还是现在的AI时代，你必须真正想清楚这里的本质是什么。无论创建什么应用，比如SaaS应用诞生于90年代末（我记得Salesforce最近刚庆祝了25周年），所以从转型角度看，SaaS至少存在了25年。现在有了AI，问题就变成了：软件的未来是什么？技术栈是什么？一家公司是否真的拥有护城河？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些人会说，他们的护城河是良好的客户关系或出色的渠道，并以此为基础进行内部颠覆。但从我的角度来看，速度至关重要。当技术转型发生时，你是否在以最快的速度构建并从中学习？无论是互联网时代、AI时代，还是2010年代初Meta向移动端的转型，你是否在快速转向？如果你能快速转向以利用技术，无论平台如何变迁，我认为都没问题。关键是你必须保持领先。一旦落后，投资者或客户总会问你那个问题：贵公司的未来在哪里？你必须走在最前沿。并非每次押注都会成功。但在我看来，那种认为某些软件的终端价值为零的极端观点，是夸大其词的。我们会一起找到答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你职业生涯的一部分是在ServiceNow领导产品，它曾被认为是最具韧性的企业软件公司之一——至少在不久前大家还这么认为，现在这个问题有待讨论了。对于许多具有工程思维、考虑购买开发者工具或使用开发者基础设施的人来说，“客户粘性”或“分销渠道作为护城河”这类词感觉很抽象。你能以ServiceNow为例，谈谈它对其客户为何如此重要，以及你的看法吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：有一点是：平台具有粘性，产品则没有。无论你今天在AI时代，还是过去创建的任何软件公司，产品都是可以被替代的。我在ServiceNow时的招聘经理Frank Slootman常说“工具是给傻瓜用的”——如果你的软件只是个“工具”，那可不是好兆头。所以，第一，产品可以被替代，因为软件市场是颠覆性的。因此，你必须确保你拥有的是一个呈现给客户的“平台”，无论客户是旧金山正在为一个新用例创建全新公司的开发者，还是一家向大型银行销售的大型企业。当你将自己定位为一个平台时，或许会拉长你的销售周期，但这意味着客户做出了深思熟虑的决定，平台因此具有粘性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，这可能与初创公司和风投圈的普遍建议不同。很多人谈论需要一个“楔子”（切入点）。对ServiceNow来说，服务台可能就是那个楔子。这是对历史的错误描述吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：很多人谈论需要一个“楔子”（切入点）。对ServiceNow来说，服务台可能就是那个楔子。这是对历史的错误描述吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：你需要一个初始用例，而且必须是杀手级用例。因为当你面对一家大银行、医疗保健公司或制造企业时，你需要展示一种颠覆性的方式来处理法律、财务或服务台用例。这是你的切入点。但问题是，如果你能轻易进入，退出也同样容易，因为他们还没有围绕你构建生态系统。这就是关键所在。今天它可能帮你从0做到1亿、10亿，但从100亿到500亿，再到100亿以上，会越来越难。今天有多少纯粹的软件公司营收超过一百亿美元？个位数而已。为什么？软件行业历史悠久，由许多聪明人创立。为何只有个位数公司营收超过百亿？因为平台是稀有的。平台是稀有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，一家软件公司的梦想或抱负应该是成为一个平台。一旦成为平台，就意味着你至少有两个或以上的产品被客户使用，并且这些产品能协同工作，从技术角度看真正具有粘性。更进一步，我认为客户需要将你的平台与他们现有系统做的所有集成也非常关键。记住，如果你面对一家大银行，有些银行已有100多年历史；大保险公司、医疗保健公司也是如此。财富10强、100强、500强公司才是市场所在（TAM）。如果你只提供一个产品，最终会遇到天花板，然后不得不增加更多东西。如果是平台，你的产品彼此协作，并与客户的所有系统集成，这就非常粘性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个具体例子，我曾代表MongoDB与一家银行交谈。他们在MongoDB上运行商业银行应用，构建了许多集成，完成了所有安全检查、治理等等。我问：“你们在MongoDB上构建了多少应用？”他们说非常重要。我问：“多少？”最终伦敦的CTO告诉我：“300个。”我说：“哇，太好了。300个应用构建在MongoDB上。”他说：“CJ，别担心。谢谢你来到伦敦。我们不会换掉的。”我问：“我能问问分母是多少吗？我知道分子是300。”他说：“9000。”我说9000，这对MongoDB是个巨大的机会。他笑着说他不打算换。所以，他们用得越多，我们就越粘性，我们就越融入他们基础设施的肌理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;氛围编程与按需应用的威胁&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：现在有些构建者、购买者和投资者持有一种观点：随着氛围编程和代码生成的兴起，那9000个应用中的一部分，将能按需或为每个公司以特定方式创建。你对此有何看法？如果这样，像那家银行或任何客户，都能得到他们真正想要的东西，而不再需要水平化、更标准化，甚至垂直化的应用了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：但银行在技术上有巨大的预算，对吧？你用了A、B、C等氛围编程平台，创建了一个应用，这很好，所以你的应用开发速度提高了。但问题来了，你仍然需要市场进入渠道，你如何接触银行？你真正具有颠覆性的方式是什么？然后银行会问你：我们和监管机构打交道的次数比和客户、供应商多得多。这能行吗？能通过我们的监管测试吗？我们需要高可用性。什么？你只建在AWS上，GCP用不了？我需要多云高可用性。我甚至需要为这个银行应用在内部部署，或者用个更专业的词，一个“气隙网络”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些才是企业级应用需要满足的要求，而大市场（TAM）就在那里。所以，这可能会限制你。这真是一个能去医疗公司或公共部门联邦客户那里销售的企业级应用吗？因此，氛围编程能让你快速创建应用，你有一个好用例，有颠覆性想法，这很棒。但从市场进入角度看，你需要做很多事才能突破，通过他们所有的检查、治理、安全审计等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;剖析SaaS看空论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：投资者现在将焦点放在模型层而非应用层，或者他们对SaaS应用感到焦虑。他们对数据基础设施也感到焦虑，因为构建应用的方式仍在快速演变。他们还对AI原生公司感到焦虑，担心所有价值最终都集中在模型里。我感觉目前整体上是一个焦虑的投资者环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：这就是看空论点，对吧？这是目前的主流观点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那么，改变我对这些假设的看法吧。你对未来应用仍将保持价值的方式，哪些方面更有信心？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：自2022年，特别是ChatGPT在秋季推出以来，我认为那是一个非常重要的转折点。现在三年多过去了，我从未见过这种情况，因为之前相当长一段时间都很平静。软件的未来确实存疑，这来自投资者群体，也来自客户，他们都在问该用X还是Y。这绝对是软件栈的一个关键转折点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后你审视软件栈，会问：什么东西是必然会存在的？大型语言模型（LLM）在可预见的未来将会存在，当你真正构建依赖这个栈的AI应用时。甚至你已经看到很多创新，比如XAI，它不知从何而来，但表现非常出色。但那个栈，在智能体软件框架中，是恒定的。数据层也必须存在，因为你需要把数据存在某处。这是第二个恒量。其他一切，都会演变。你最好展示出真正的价值，无论你使用平台类比还是什么，无论是技术栈的顶层，你真正理解保险行业的用例，并正在为保险行业构建一个AI原生公司。保险行业有大量用例。你可以说，请从旧的SaaS X 迁移到新的Y，这个新Y实现价值的速度会非常快，而且我们将始终保持领先，因为有了AI，过去用旧SAS无法实现的事情现在成为可能。所以，除了LLM层和数据层之外，顶层的、聚焦用例的部分，将始终至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：MongoDB的客户从初创公司、个人开发者一直到财富10强。你从这些最大公司的购买者和构建者那里，听到他们关于AI价值的真实看法是什么？他们现在对什么兴奋，对什么怀疑？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：我感觉如果一周没和至少10个客户交流，那这周就完全失败了。这需要很多准备和跟进，但通常至少有10个。所以我不断获取这些数据点，尝试进行模式匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我想说的第一点是，当你想到财富500强、全球2000强公司时，他们中的一些进展仍然不快。很多人尝试了办公生产力类型的copilot，但不清楚他们从中获得了多少价值。反馈不太好。关于编码辅助的反馈，在2024年有了显著突破，非常积极。2024年和2025年初始于GitHub Copilot，然后是其他一些工具，一直到Anthropic等等。所以从我的角度看，2025年是编码辅助的突破年，而且仍在继续。我从客户那里得到非常积极的反馈。然后人们还在摸索客服支持领域。他们想，如果是一家大型电信公司或医疗保健公司，客服能完全由AI原生公司处理吗？还没到那一步。他们正在切入初始用例，这很好。我指的是端到端的客户体验。而这些客户问我的问题是关于SaaS的：我应该把这个AI原生客服公司看作“补充”还是“替代”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5nCbHsCG334&quot;&gt;https://www.youtube.com/watch?v=5nCbHsCG334&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qQAhK2EWw4I&quot;&gt;https://www.youtube.com/watch?v=qQAhK2EWw4I&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/chamath/status/2014044948660887981&quot;&gt;https://x.com/chamath/status/2014044948660887981&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.forbes.com/sites/donmuir/2026/02/04/300-billion-evaporated-the-saaspocalypse-has-begun/&quot;&gt;https://www.forbes.com/sites/donmuir/2026/02/04/300-billion-evaporated-the-saaspocalypse-has-begun/&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GuqAUv4UKXo&quot;&gt;https://www.youtube.com/watch?v=GuqAUv4UKXo&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6ROkojXXxzqvZvBq2VHB</link><guid isPermaLink="false">https://www.infoq.cn/article/6ROkojXXxzqvZvBq2VHB</guid><pubDate>Thu, 12 Feb 2026 00:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>ChatGPT的第一块广告位，被谁买走了？OpenAI：别骂，我们这次所有底线都招了</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ChatGPT 无广告体验的日子要结束了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;经过数周的预热，刚刚，OpenAI宣布，将正式开始在其AI平台测试广告，ChatGPT用户可能很快就会在对话中看到广告。这些广告会以标注“赞助”的链接形式出现在ChatGPT回答底部，但OpenAI表示，广告不会影响ChatGPT给出的回答内容，在视觉上也会区分开来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，广告仅对免费版ChatGPT用户以及每月8美元的低价订阅服务Go套餐用户展示，Plus、Pro、商业版、企业版和教育版用户不会看到任何广告。也就是说，想要避开广告的用户至少需要每月支付20美元订阅Plus套餐。OpenAI提到，免费版用户要想退出广告，但代价是每日免费对话次数减少；Go套餐用户无法选择退出广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://uploader.shimo.im/f/2bKQWp6d3KB8gasG.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3NzA4MDQzMzUsImZpbGVHVUlEIjoiS2xrS3ZteFl6cENnbVhxZCIsImlhdCI6MTc3MDgwNDAzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwicGFhIjoiYWxsOmFsbDoiLCJ1c2VySWQiOjUxNzIzNzI1fQ.p13l2Z-_MU334JoSfqO9fDApNfEl33BiVbOB7URaies&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位接近OpenAI的消息人士表示，OpenAI预计从长远来看，广告收入占比将低于其总收入的一半。目前，该公司还通过其聊天机器人集成的购物功能，从用户购买的商品中抽取分成。另据外媒报道，OpenAI首席执行官Sam Altman告诉员工，ChatGPT“月增长率已恢复到10%以上”，将于本周部署“更新后的聊天模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;ChatGPT 广告规则全曝光&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次广告功能推出前，Anthropic在一则广告中暗讽了ChatGPT的广告模式。例如在其中一则广告里，一名年轻男子向人工智能求助练出六块腹肌，化身私人教练的 AI 先是为他提供指导，随后却开始推销一款虚构的增高鞋垫。之后，Anthropic修改了广告标语，改为：“广告自有其时间与场合，但你和AI的对话不该是其中之一。”而这很快引发了 Altman的不满，他称其“明显不诚实”，是在用“欺骗性广告去批评那些并不存在、理论上的欺骗性广告”，与他们实际的广告模式不符。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在OpenAI最新一期的播客里，OpenAI 的广告负责人之一 Asad Awan 详细介绍了其AI产品中的广告制定决策。“一方面走‘清高路线’，不做广告，但同时限制使用额度、用能力较弱的模型；另一方面，拥抱广告模式。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场对话中，不少外界关注的核心问题都摆到了台面上，且给出了清晰的回答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从用户角度看，为什么要做广告？为什么是现在？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“这回到了我们的使命，让 AGI 惠及全人类。”Awan解释道，当一款消费级产品有超过 8 亿用户时，如何把最好的产品带给每个人？广告是经过验证的成熟模式，对于一家希望把最好的 AI 带给全人类的公司来说，这是很自然的选择。提供最好的模型、更高的使用限额，让广告对用户和企业都真正有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示，大家担心广告可能带来负面影响，所以OpenAI 更看重广告的原则：为平台设立极高的标准，让广告真正有用。其确立了几条核心原则：&lt;/p&gt;&lt;p&gt;第一，模型回答与广告完全独立，无论视觉上还是模型训练、系统逻辑上，确保回答始终可信，整个产品都建立在信任之上。第二，对话是隐私的。敏感对话绝不会出现广告，对话内容绝不会共享给广告主。我们会在内部匹配合适的广告，但广告主看不到用户对话。第三，透明与可控。用户能清楚理解数据如何使用，并且可以自主控制。第四，激励机制以用户价值为中心。我们不追求用户在平台上的停留时长，一个真正有用的广告就足够了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“简单来说就是，用广告普及 AI，同时严防各种负面问题，从一开始就明确原则，持续测试、改进。”Awan说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;广告的出现频率是怎样的？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“最高原则是：有没有有用的广告可以展示。”Awan称，核心原则是：是否有用、有帮助、对用户的操作有补充，能不能展示优质商品，内容质量要高、广告质量要高、相关性要高。“如果没有，我们宁可一条都不展示。实际上在测试阶段，你会看到广告非常少，因为我们态度很保守，也在学习该在什么位置插入。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，模型看不到广告，也有防护措施。“狂轰滥炸广告对用户、对商家都没好处。我们不想让广告主乱花钱买曝光，也不想让用户看一堆广告，我们只想展示那条正确的广告。作为顶尖 AI 公司，这正是我们能做好的事。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;当公司有专门的广告收入部门时，还会把模型和广告之间这堵墙砌死吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“只要目标和激励机制是成为最值得信任的 AI，我们就不会走偏。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Awan强调，“我们的核心业务是信任。对 C 端用户，是提供可信的优质回答；对企业客户，信任更是一切，你把最重要的数据交给我们，我们必须守住。如果我们真的想成为你最贴身的智能助手，就必须让你放心分享最重要的信息，并且知道它会被妥善对待。我们的商业模式就是 信任，这和很多只做一次性查询、内容推荐的产品完全不同。对我们而言，信任不是可选项，而是必需品。我们希望被用户记住的，就是‘可信’。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;如何回应拿广告这件事开玩笑的竞争对手？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“不同公司使命不同。”Awan表示，OpenAI的业务场景更多元：企业业务、订阅业务、海量免费用户业务。企业版没有广告，订阅版没有广告，广告是为了支撑免费用户业务。“如果你的使命不是普惠 AI，那不做广告很合理；但我们的使命就是在各类场景里落地，让所有人用上最好的 AI。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他强调，如果只服务付费用户，当然可以说 “我们不用做广告”。但OpenAI的愿景不是抽象的，而是非常实在：AI 如何真正帮助普通人。“如果走精英路线，只有付得起钱的人才能用 AI，那从价值观上我们就不认同。我们的立场非常明确：每个人都应该用上最好的 AI。而且我们不是一家纯广告公司，纯广告公司的激励机制和我们完全不同。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;十年后的广告会变成什么样？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据Awan透露，下一步会是更真实的对话式广告，能真正了解产品是什么。再往后，AI 可以在后台自动聚合最优折扣、最划算的商品、最合适的版本。一边是用户主动搜索，一边是商家希望被合适的人发现，两边匹配起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来会更加智能体化。我们先从现有形式开始，把体验做好，让它更相关、可控、可理解、可信。随着主产品和系统进化，广告形态也会一起进化。”Awan表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;第一批投放广告的公司露面了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ChatGPT 推出广告之际，正值人工智能行业竞争压力不断加剧，且外界对大型 AI 平台的可持续盈利模式抱有更高期待。OpenAI表示，“ChatGPT 被数亿人用于学习、工作和日常决策。为了确保免费版和 Go 版的快速稳定运行，我们需要投入大量基础设施和持续资金。广告收入有助于资助这些工作，从而通过更高质量的免费和低成本选项，让更多人能够使用人工智能，并使我们能够不断提升所提供的智能和功能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，该公司称，广告商只会获得汇总的广告浏览量和点击量数据，不会获取用户的ChatGPT对话个性化数据或内容。免费版和Go版用户都可以对广告反馈、关闭广告个性化设置、关闭基于历史对话的广告推荐，从而限制赞助内容的推送方式并删除广告相关数据。此外，OpenAI现在并非对所有用户和对话都会投放广告，比如18 岁以下用户以及涉及健康、心理健康、政治等特定敏感话题的对话场景。即便免费版与 Go 版的成年用户，也未必会立即看到广告，因为该功能仍处于测试阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://uploader.shimo.im/f/vAnwSrozg2s8yFXP.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3NzA4MDQzMzUsImZpbGVHVUlEIjoiS2xrS3ZteFl6cENnbVhxZCIsImlhdCI6MTc3MDgwNDAzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwicGFhIjoiYWxsOmFsbDoiLCJ1c2VySWQiOjUxNzIzNzI1fQ.p13l2Z-_MU334JoSfqO9fDApNfEl33BiVbOB7URaies&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管此举在ChatGPT用户和行业观察人士中引发了褒贬不一的反应，但 OpenAI 坚称，投放广告是为了补贴免费及低价服务的使用成本。该公司强调，“此次测试的重点是学习。我们会密切关注反馈，以确保广告在推广之前能够实用且自然地融入ChatGPT体验。”早期用户的反馈将有助于改进广告，并可能在未来扩大广告投放范围。OpenAI 表示，将利用此次试点项目的洞察，更好地平衡盈利与用户体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，已有多家公司也透露了他们将如何投放 ChatGPT 广告。Adobe表示，将率先投放 Acrobat Studio 和 Firefly 相关广告作为初期试点。已经与 ChatGPT 完成集成的Target，则会在用户提出诸如“有哪些台面式厨电能让日常做饭更方便？”这类问题时展示广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而随着测试的推进，OpenAI 的做法很可能会影响其他人工智能公司对盈利模式的思考，以及广告在对话式 AI 工具中的角色。不过，开发 Claude AI 助手的 Anthropic 已 “承诺” 永远不会加入广告，甚至还在投放了的一系列超级碗广告里宣传这一决定。据报道， OpenAI 的竞争对手谷歌也曾暗示，其 Gemini AI 平台可能会在 2026 年投放广告，不过谷歌 DeepMind 首席执行官 Demis Hassabis 在 1 月底表示，Gemini “没有计划”投放广告。目前，谷歌已在搜索结果旁边的 AI 概览中投放广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在未来某一天，或许所有免费的AI应用和服务都会出现广告，但至少目前，ChatGPT是唯一一家率先推出广告的大型应用和服务商。微软Copilot之前的版本（当时名为Bing Chat）也曾出现过广告，但目前的版本似乎已经取消了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/testing-ads-in-chatgpt/&quot;&gt;https://openai.com/index/testing-ads-in-chatgpt/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/2026/02/09/sam-altman-touts-chatgpt-growth-as-openai-nears-100-billion-funding.html&quot;&gt;https://www.cnbc.com/2026/02/09/sam-altman-touts-chatgpt-growth-as-openai-nears-100-billion-funding.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2agJo3Jf_O4&quot;&gt;https://www.youtube.com/watch?v=2agJo3Jf_O4&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Y6SK6Mu5t4gmCk5mhxvk</link><guid isPermaLink="false">https://www.infoq.cn/article/Y6SK6Mu5t4gmCk5mhxvk</guid><pubDate>Wed, 11 Feb 2026 10:01:56 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>传字节今年要造10万颗推理芯片，1600 亿预算砸向AI！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，据两位知情人士透露，字节跳动正研发AI芯片，并与三星电子洽谈代工生产事宜。知情人士称，字节跳动目标是在3 月底前获得芯片样片。其中一位消息源及另一位相关人士表示，该芯片专为AI 推理任务设计，公司计划今年至少生产10 万颗，并有望逐步将产能提升至35 万颗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位消息源指出，与三星的谈判还包括获取存储芯片供应。在全球 AI 基础设施建设热潮下，存储芯片供应极度紧缺，这也让这笔合作更具吸引力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;字节跳动发言人在一份声明中表示，有关其自研芯片项目的信息不准确，但未做进一步说明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;若推进顺利，此举将成为字节跳动的一个里程碑。该公司长期以来一直希望研发芯片以支撑自身 AI 业务，其芯片相关布局最早可追溯至 2022 年，当时便已开始大规模招聘芯片领域人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该芯片项目代号为 SeedChip，是字节跳动全面加码 AI 研发的一部分。 从芯片到大语言模型，公司押注这项技术将彻底改造其涵盖短视频、电商、企业云服务的业务版图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;字节跳动于 2023 年成立 Seed 部门，专注研发 AI 大模型并推动其落地应用。据一位消息人士透露，字节跳动今年计划在 AI 相关采购上投入 超过 1600 亿元人民币（约 220 亿美元），其中超过一半用于采购英伟达芯片（包括 H200）以及推进自研芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据第四位知情会议内容的人士称，字节跳动高管赵祺在 1 月的全员大会上向员工表示，公司的 AI 投入将惠及所有业务部门。赵祺目前负责字节跳动的豆包聊天机器人及其海外版本 Dola。他坦言，公司的 AI 大模型仍落后于 OpenAI 等全球领先者，但承诺今年将继续大力支持 AI 研发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/world/asia-pacific/bytedance-developing-ai-chip-manufacturing-talks-with-samsung-sources-say-2026-02-11/&quot;&gt;https://www.reuters.com/world/asia-pacific/bytedance-developing-ai-chip-manufacturing-talks-with-samsung-sources-say-2026-02-11/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AradpbWZZoiWVmehvBLB</link><guid isPermaLink="false">https://www.infoq.cn/article/AradpbWZZoiWVmehvBLB</guid><pubDate>Wed, 11 Feb 2026 09:40:36 GMT</pubDate><author>华卫</author><category>芯片&amp;算力</category></item><item><title>从多模态走向全模态！蚂蚁开源 Ming-Flash-Omni 2.0，对标Gemini 2.5 Pro</title><description>&lt;p&gt;2 月 11 日，蚂蚁集团开源发布全模态大模型 Ming-Flash-Omni 2.0。在多项公开基准测试中，该模型在视觉语言理解、语音可控生成、图像生成与编辑等关键能力表现突出，部分指标超越Gemini 2.5 Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型地址：https://github.com/inclusionAI/Ming&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Ming-Flash-Omni 2.0 是业界首个全场景音频统一生成模型，可在同一条音轨中同时生成语音、环境音效与音乐。用户只需用自然语言下指令，即可对音色、语速、语调、音量、情绪与方言等进行精细控制。模型在推理阶段实现了&amp;nbsp;3.1Hz 的极低推理帧率，实现了分钟级长音频的实时高保真生成，在推理效率与成本控制上走在前列。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c6/c6ced55efdf5e6d69a184c6e8ce4b68a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：Ming-Flash-Omni-2.0&amp;nbsp;在视觉语言理解、语音可控生成、图像生成与编辑等核心领域实测表现均已达到开源领先水准）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;全模态和多模态有啥不一样？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型快速演进的背景下，“多模态”和“全模态”这两个概念正在频繁出现在技术发布和行业讨论中。表面看，它们都指向“模型能处理多种类型的数据”，但在底层实现路径和能力边界上，两者存在本质差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从能力表象来看，多模态与全模态高度相似。无论是多模态大模型还是全模态大模型，用户侧的直观体验都是：模型可以同时接收文本、图片、视频，甚至音频等不同模态的输入，并给出统一的输出结果。这也是许多非技术用户容易将两者混为一谈的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正的分水岭，出现在模型内部的实现方式。目前主流的“多模态大模型”，本质上是一种“模型拼装”思路：针对不同模态的数据，系统会分别调用对应的专用模型进行处理——例如，文本由语言模型理解，图像由视觉模型解析，音频交给语音模型识别。随后，再通过一个融合模块，将来自不同模型的结果进行整合，生成最终输出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构的优势在于工程实现相对成熟、可控，也便于在已有单模态模型基础上快速扩展能力。但其局限同样明显：不同模态之间更多是“后验融合”，信息在中间环节已经被压缩或结构化，跨模态的深层语义关联难以充分建模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，“全模态大模型”走的是一条更底层的路线。所谓全模态，并不是简单地“支持更多输入类型”，而是指模型在设计之初，就将多种模态视为统一的信息空间，在同一个模型参数体系中进行联合建模。文本、图像、音频、视频不再对应独立的子模型，而是通过统一的表示方式进入同一套网络中学习、推理和生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，全模态大模型具备原生的跨模态理解与生成能力：不同模态之间的关联不是在输出阶段“拼接”出来的，而是在模型内部的表示层和推理过程中自然形成的。从理论上看，这种架构更接近人类对世界的感知方式，也更有潜力支持复杂的跨模态推理任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当下，大多数落地应用仍然基于多模态架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内普遍认为，多模态大模型最终会走向更统一的架构，让不同模态与任务实现更深层协同。但现实是，“全模态”模型往往很难同时做到通用与专精：在特定单项能力上，开源模型往往不及专用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁集团在全模态方向已持续投入多年，Ming-Omni系列正是在这一背景下持续演进：早期版本构建统一多模态能力底座，中期版本验证规模增长带来的能力提升，而最新2.0版本通过更大规模数据与系统性训练优化，将全模态理解与生成能力推至开源领先水平，并在部分领域超越顶级专用模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次将Ming-Flash-Omni 2.0 开源，意味着其核心能力以“可复用底座”的形式对外释放，为端到端多模态应用开发提供统一能力入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ming-Flash-Omni 2.0 基于 Ling-2.0 架构（MoE，100B-A6B）训练，围绕“看得更准、听得更细、生成更稳”三大目标全面优化。视觉方面，融合亿级细粒度数据与难例训练策略，显著提升对近缘动植物、工艺细节和稀有文物等复杂对象的识别能力；音频方面，实现语音、音效、音乐同轨生成，支持自然语言精细控制音色、语速、情绪等参数，并具备零样本音色克隆与定制能力；图像方面，增强复杂编辑的稳定性，支持光影调整、场景替换、人物姿态优化及一键修图等功能，在动态场景中仍保持画面连贯与细节真实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百灵模型负责人周俊表示，全模态技术的关键在于通过统一架构实现多模态能力的深度融合与高效调用。开源后，开发者可基于同一套框架复用视觉、语音与生成能力，显著降低多模型串联的复杂度与成本。未来，团队将持续优化视频时序理解、复杂图像编辑与长音频生成实时性，完善工具链与评测体系，推动全模态技术在实际业务中规模化落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/d9TEFiU7kq8EKCIodTmI</link><guid isPermaLink="false">https://www.infoq.cn/article/d9TEFiU7kq8EKCIodTmI</guid><pubDate>Wed, 11 Feb 2026 09:31:43 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Snowflake 语义视图自动驾驶：分钟级 AI 驱动的语义建模 ｜ 技术趋势</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具备治理性、可信语义的数据层已成为AI就绪数据的基础能力。近日，Snowflake正式宣布语义视图自动驾驶（Semantic View Autopilot，SVA） 全面上市。该系统能够基于现有查询与商业智能资产，自动生成语义视图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;问题在于定义缺失，而非大语言模型能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年，开发 AI 智能体的团队发现，即便最先进的模型也难以应对不一致的业务逻辑。真正的障碍并非 AI 能力，而是数据定义的缺失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VTS 工程高级副总裁 Prashanth Sanagavarapu 指出：“构建并维护统一的语义层需要大量人工投入，以避免数据指标冲突。” 为此，我们推出语义视图自动驾驶方案，旨在自动化构建这一具备治理能力且可信的语义层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“语义视图自动驾驶技术为我们的 AI 系统提供了统一且受控的业务指标理解框架……使我们能够提供可靠的个性化服务及AI驱动的交互体验，赢得客户的长期信任。”Simon AI 首席技术官 Matt Walker 表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Snowflake 自动化语义视图创建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义视图不仅提供数据结构信息，更能阐释数据的业务含义与设计意图。它能够指导大语言模型（LLM）将原始数据转化为业务概念，但传统的创建过程往往耗时费力且高度依赖人工操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对数据团队而言，确保业务逻辑的一致性至关重要。然而，人工创建语义视图负担沉重，例如，产品团队定义“月度经常性收入”时，可能未意识到财务部门会排除一次性设置费用。这类隐藏规则通常仅在部署后、数据对不齐时才会暴露。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake语义视图自动化功能（SVA）通过自动创建和管理语义视图来弥合这一鸿沟。该功能无需工程师从零开始编写定义，而是基于查询历史与可信商业智能资产，主动推荐从中学到的候选指标与筛选条件，使团队能够在数分钟内完成审核、认证与部署，而非耗费数周时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运作机制：基于共识模式的学习&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 的核心原理在于：您的语义定义已蕴藏于查询历史、数据使用情况及仪表板之中。这使语义建模从编码工作转变为治理优化——团队只需专注于审阅 SVA 自动发现的业务逻辑。这些经治理的定义将为 Snowflake Cortex Analyst、Cortex Agents及Snowflake Intelligence 提供支持，以获取更精准可信的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SVA 通过分析以下三类关键信号实现这一过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模式识别与共识驱动提取&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 采用聚类算法分析查询模式与自然语言问题，以识别共识性业务逻辑。当存在冲突定义时（例如不同的“活跃用户”筛选条件），SVA 会将最高频出现的模式作为推荐方案提出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如：若 200 余次查询均将“活跃用户”计算规则定义为“用户参与度分数&amp;gt; 50 且 最后登录天数&amp;lt; 30”，则即使用户近期运行了不同条件的查询，SVA 仍会优先推荐此共识逻辑作为标准提案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;多源高置信信号学习&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最高置信源通常来自现有商业智能（BI）仪表板，其中沉淀了多年的业务逻辑。Tableau 是 SVA 支持的首个 BI 工具，未来将通过我们 20 余家 OSI 合作伙伴接入更多平台。SVA 可在数分钟内将静态仪表板转化为对话式人工智能应用（详见实践操作指南）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队亦可直接上传可信 SQL 查询语句。SVA 将自动提取数据关系与业务指标，并将其存储为经过验证的查询模型供后续使用。得益于全程在 Snowflake 内部运行，SVA 能够直接分析真实业务数据。通过列基数分析可推断关系类型，进而智能生成优化建议，例如为提升精度推荐部署 Cortex Search 服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;基于持续演化的使用模式进行迭代更新&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 通过监控用户查询模式，持续优化语义视图的时效性。例如当企业新增“专业版”订阅层级时，SVA 将自动识别包含 subscription_tier = &#39;pro&#39; 的新查询模式，并主动建议将其纳入语义视图体系，确保业务规则演进过程中智能应答始终保持一致性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从传统商业智能向 AI 智能体的转型，需要建立在数据使用实践而非 LLM 预设的语义基础之上。Semantic View Autopilot 通过真实使用场景驱动语义建模，为您提供当前最快捷的、具备治理能力的上下文感知AI实现路径，现已全面覆盖 Snowflake 所有支持 Cortex Analyst 服务的区域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即刻开启智能语义视图之旅。欢迎在您的 Snowflake 账户中体验 &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/views-semantic/autopilot&quot;&gt;SVA&lt;/a&gt;&quot;，并获取为 Cortex Analyst 构建语义视图的&lt;a href=&quot;https://www.snowflake.com/en/developers/guides/best-practices-semantic-views-cortex-analyst/&quot;&gt;最佳实践&lt;/a&gt;&quot;指南。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/semantic-view-autopilot/&quot;&gt;https://www.snowflake.com/en/blog/semantic-view-autopilot/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/4gjxPMiGtEfB946j2U27</link><guid isPermaLink="false">https://www.infoq.cn/article/4gjxPMiGtEfB946j2U27</guid><pubDate>Wed, 11 Feb 2026 09:25:22 GMT</pubDate><author>Abhinav Vadrevu</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>OpenAI 与 Anthropic双雄打擂台！专家：2026 年 Agent 将在产业里遍地开花</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;人工智能正处于阶梯式发展的平台期，当前研究路径的收益正在收敛，下一次跃迁需要全新的范式突破。与此同时，产业应用正在加速成熟，2026年有望成为Agent大规模落地的关键之年。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周，OpenAI 与 Anthropic 几乎在同一时间抛出了各自最新的模型更新——OpenAI Codex 5.3 与 Claude 4.6。没有发布会轰鸣，也没有颠覆式叙事，但在开发者社区和产业侧，这两次更新仍被迅速解读为一个清晰信号：大模型能力正在逼近一个阶段性的上限，而行业正在集体寻找新的突破口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用一个词来形容2025年的人工智能行业，那就是“临界”。一方面，大模型的通用能力已达到较高水平，在语言理解、推理、代码生成等维度上正在逼近甚至超过人类专家水准；另一方面，沿着既有路径继续堆叠规模与算力，边际收益正在迅速收敛。技术并未停滞，但“下一次质变从何而来”，正在成为整个行业共同面对的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一代范式突破的方向是什么？中美竞争的真正差距在哪里？Agent如何从概念走向真正的产业落地？这些追问贯穿整个行业，而在2026年，它们变得无法回避。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近期，带着这些问题我们与中关村人工智能研究院副院长&amp;amp; 北京中关村学院副教授（以下简称“中关村两院”）郑书新进行了一次深度访谈。郑书新认为，人工智能正处于阶梯式跃迁的平台期，下一次跃迁需要全新的范式突破。他同时指出，当前中美竞争的核心差距不在技术路线，而在高质量数据和算力资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产业侧，郑书新认为技术突破与产业普及之间始终存在时间差，这是历史常态而非失败。就像蒸汽机的发明并不会立刻带来工业革命的大规模落地，AI能力要转化为大规模应用，同样依赖配套系统与产品形态的逐步成熟。在他看来，2026年将是Agent在真实场景中集中落地的一年，而Coding Agent等新范式也正在重塑传统软件开发的基本逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为访谈实录，经由InfoQ编辑及整理：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开场：个人介绍与研究背景&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您在AI领域深耕多年，能否和我们分享一下您的研究历程和主要工作？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：我从十多年前开始接触人工智能，一直深耕大模型领域。早期专注于大规模分布式优化，搭建了当时微软最大的异步分布式训练系统。此后转向大语言模型研究，提出了Pre-LN等训练优化与架构改进方法，将模型训练效率提升了约一个数量级。这些成果后来被主流大模型广泛采用（如OpenAI开源模型gpt-oss 等）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在通用模型与方法研究阶段，我提出的Graphormer架构，现在是图（Graph）学习领域的主流基座模型之一。近期，我致力于将大模型与生成式AI技术引入科学发现领域，提出的分子平衡分布预测框架突破了传统生物分子模拟的瓶颈，将分子动力学模拟效率提升数十万倍，相关成果发表于《Science》封面及《Nature Machine Intelligence》等顶级期刊。&lt;/p&gt;&lt;p&gt;2024年底，我加入中关村两院，现任学院副教授、研究院副院长，在AI基础学部负责大模型方向的研究与战略布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到目前在中关村两院AI基础学部负责大模型方向的研究。中关村两院肩负着北京乃至国家AI创新生态建设的使命，能否介绍一下两院的核心定位？AI基础学部在其中扮演怎样的角色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：北京中关村学院与中关村人工智能研究院是一体两面，融合发展，是教育科技人才一体化的新尝试，是新型研发机构的二次方。北京中关村学院肩负着培养人工智能领军人才的重要使命，是国家教育、科技、人才一体化改革的&quot;试验田&quot;。中关村人工智能研究院与中关村学院共同开展面向未来、具有产业价值、颠覆性的人工智能技术研发及成果产业化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI基础学部在这个框架下承担具体的技术攻关和方向布局，我们的战略目标是补全AGI下半场的关键拼图，在产业上输出能真正重塑行业逻辑的核心变量，在人才上培养兼具工程能力与科学直觉的领军人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI整体发展Overview&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：站在2026年初这个时间节点，您认为当前中国 AI发展最需要解决的关键问题是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;AI发展正处在阶梯式跃迁的平台期，沿着现有技术路径的边际收益在递减，需要找到下一代突破方向。同时， AI本身也有两个特征：它是根植于产业的技术；并且，这场博弈有明确的时间窗口，很有可能在3-5年内见分晓。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些判断，我认为当前有两个核心问题需要关注。第一是战略层面：这场范式竞争的背后是中美科技博弈，我们如何争取先手、发展自主生态。第二是应用层面：AI如何真正拉动GDP，实现高质量发展。现在AI的行业渗透率已经很高，但对GDP的实际贡献还很有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI技术发展现状&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到技术上的关键问题是中美技术博弈中争取先手。能否展开谈谈，您如何看待当前AI技术的发展阶段？下一代技术突破的方向会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;人工智能的发展遵循&quot;阶梯式跃迁&quot;的规律。最近一次重大跃迁是GPT带来的规模定律。但现在，智能性提升进入平台期，沿着现有技术路径的收益在递减，近期已经有多个迹象有所印证。其一，预训练范式遇到瓶颈。规模定律的红利趋近耗竭，可用于模型训练的互联网高质量数据见顶，继续扩大模型规模的边际收益显著下降。其二，后训练范式同样存在局限。当前业界普遍转向精细化的奖励函数设计，奖励函数的设计复杂度已经堪比当年的特征工程，本质上是在既定框架内反复调优。Meta近期发布的研究也表明，后训练的增量空间可能比预期更有限。如果“Less Structure, More Intelligence”成立，那么现有策略能否一路带领我们通向AGI，坦率说是存疑的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，下一代突破的方向是什么？可能是针对本代AI范式的缺点进行改进、寻找突破口，例如突破记忆与持续学习的瓶颈、打通经验学习（Learning from Experience）和自我博弈（Self-Play）的路径、提高长上下文支持能力、探索动态数据的新训练方法等。但也有可能需要探索全新的技术范式，例如受神经科学启发的软硬件结合架构、新的数据来源、离散Diffusion等新的建模方式、以及新的智能性理论与奖励函数设计等。然而，下一代探索是高风险、长周期的，对商业公司而言往往优先级较低，毕竟它们需要兼顾短期业绩和股东回报；而多数高校虽有学术自由度，但在算力和工程资源上存在现实约束。正因如此，中关村两院希望在这个时点带发挥独特作用，做难而正确的事情，沿现有路线突破和全新范式探索两个方向布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：2025年Agent很火，有人把Agent理解为大模型的应用层封装，有人把它理解为落地的应用形式。您如何看待当前AI Agent的发展现状？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;大家普遍把Agent理解为技术上的研究领域，或是一种落地的应用形式。但在我看来，Agent 就是基座模型，是当前业界押注智能性提升的主要技术路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么这么说？预训练Scaling Law边际效益递减的根本原因在于互联网高质量数据已接近上限。现在的核心解法之一就是找新的数据来源——合成数据，它的本质是搜索，在超高维的语言空间中使用预训练好的大模型去发现新的有价值数据，依托这些合成数据来进一步提升模型的性能。以o1为代表的推理模型，就是通过搜索和强化学习在语言空间中生成高质量的思维链数据；而Agent进一步扩展了搜索空间的边界，与环境交互并调用工具，发现全新的高价值数据，可能存在新的Scaling Law。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在2026年，您认为AI Agent领域最值得期待的技术突破点是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：类似整个AI 领域的进展方向，我期待的一是改进现有范式的短板，二是新的训练范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在现有范式的改进上，有几个方向值得关注。首先是运行时学习（Runtime Learning），让智能体能够在运行过程中持续学习和改进，而不只是依赖预训练阶段的能力。其次是记忆机制，Agent需要在长周期任务中保持上下文连贯，有效地存储和调用历史信息。此外，幻觉与可靠性、下一代评测方法、智能体系统的整体可用性与智能性等也是关键课题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在新范式的探索上，自我迭代的训练方式，以及内在动机（Intrinsic Motivation）驱动的奖励机制，都可能为Agent带来阶跃式的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些也是中关村两院大模型领域的重点布局方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：相比国外，您认为国内在AI研究方面最大的优势和短板分别是什么？在全球AI竞争中，我们最需要补上的“关键一课”是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;中国拥有庞大的人才基数和深厚的数理传统，大量工程师具备扎实的数学功底和出色的工程落地能力。与此同时，中国的产业门类齐全、应用场景丰富、市场规模庞大，这种独特的生态为AI落地提供了天然的试验田，也孕育了极强的产品化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说短板，目前核心有两点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一点是数据。目前中美技术路线上已经趋于透明，国内头部大厂和美国最大的差距就是数据，这是大模型智能性提升的主要来源。美国正在系统性地采集长程、复杂、高难度的专业级数据，这类数据的特点是推理链条长、多轮交互、涉及多种工具调用，单条价值可达上千美金。这也是OpenAI等公司研发的重点，目前已经有专门的公司在帮大厂收集编程、金融、法律、咨询等领域的专家级知识和数据，可以预见2026年在这些专业领域会有显著突破。我们在这方面还比较欠缺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二点是算力。我认为算力是智能性提升的第一性原理——科学的进步依赖多样性的探索，而多样性的探索依赖充足的算力。但目前我们在这方面面临不少挑战：一是芯片本身的性能受限，二是大规模组网能力有待提升。据传美国xAI已经有80万张H100级别的集群，而国内头部的&quot;六小龙&quot;基本还在5万张上下。在这种情况下，对我们的要求就更高了——需要特别巧妙精细的设计，省着用，才能做出东西；但美国目前可以进行大规模、多方向的并行探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI产业现状&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您之前提到，产业上目前的问题是行业渗透率高，但对GDP的实际拉动效益还很有限。从整个AI领域来看，您认为产业真正的爆发拐点会在什么时候到来？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;现在确实面临技术跑在前面的情况，即模型能力已经在很多领域达到“博士级别”智能，但在产业端体感还比较弱，对GDP拉动有限。不过这是正常的，因为技术研发和产业落地之间存在时间差。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;打个比方，蒸汽机的出现是一场动力革命——它重新定义了制造业、交通、能源等几乎所有行业。但从瓦特改良蒸汽机到工业革命全面铺开，中间隔了几十年，因为需要铁路、工厂、煤炭供应链等一整套配套系统逐步成型。AI也正处在类似的阶段：核心的&quot;动力源&quot;已经出现，但要真正重塑产业，还需要数据基础设施、工程化工具链、行业know-how的深度融合。不同的是，这一轮的节奏会快得多，可能几年而不是几十年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，这个进程已经在加速。2025年Agent的突破是一个缩影——更广泛地看，AI已经在各行各业开始渗透，很多场景不需要&quot;博士级&quot;智能，关键是被打磨成真正可用的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我的判断是，2026年会是AI产业落地的关键一年。一方面，Agent、Coding Agent 等产品形态会让更多用户在工作和生活中真正用上AI；另一方面，垂直行业的AI应用也在快速成熟，一级市场已经有大量公司在做得不错的公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤其值得关注的是白领和知识工作者群体。当前模型在多学科领域已经接近博士级智能，法律、金融、咨询、研究等领域有望率先释放生产力红利，AI对GDP的拉动很可能从这里开始。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：Coding Agent是当前讨论的热门方向，您怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;Coding Agent正在颠覆传统软件开发的范式。过去的逻辑是一个团队精心打磨3个产品，最后可能有1个成功；现在借助Coding Agent，个体就能快速开发100个产品，成功的概率和路径都被彻底改变了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我自己每天都在用Codex这些工具，经常多个任务并行。此刻我的电脑上就同时跑着4个Codex Agent，帮我完成各种任务。很多以前停留在想法阶段的项目，现在都能快速变成可运行的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更让我兴奋的是，这种能力可以快速复制给零基础的人。我在北京中关村学院开了门AI Agent编程课程，宗旨都是“零帧起手手写代码”。大约半个月前，斯坦福也开出一门类似课程，理念是“全程不写一行代码”，和我不谋而合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;课程只有四个半天，学生来自物理、材料、金融等各专业，很多人零编程基础。但结课时，所有小组都拿出了可运行的Demo：有人把Deep Research做成了“带事实核查的Deep Research”；有人把语音对话GPT改造成&quot;带快慢双系统的版本&quot;——快系统负责即时回应，慢系统在后台深度推理，最后融合呈现。零基础、跨背景，四个半天就能独立做出产品，这在以前是不可想象的，也是Coding Agent带来的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在您看来，有哪些公司或产品在Agent领域做得比较出色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;现在这个领域非常活跃，Agent的发展正在从“对话”向“办事”演进。如果说去年大家还在讨论概念，今年我们已经看到了很多能真正提高生产力的落地案例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如当下非常热门的几个产品，它们的共性在于：深度接管系统与文件，自主规划、异步执行、完成任务。如开源的Clawdbot被称为“AI Jarvis”；Anthropic的Claude Cowork实现了从“对话助手”到“数字同事”的跨越。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coding Agent是目前落地最快的方向之一。海外的Cursor、Claude Code已成为开发者标配；国内方面，Kimi K2.5作为Agentic模型表现亮眼，基座模型中GLM-4.7领先，DeepSeek-V3.2、Qwen3、MiniMax-M2.1也都不错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到了一些Agent应用产品，也提到了一些基座模型厂商。这其实涉及到行业里一个持续讨论的话题：通用大模型是否只是大厂之间的游戏？之前有嘉宾认为，通用大模型需要耗费大量人力物力财力，应该留给大厂去做，其他厂商可以在垂域模型中寻找生存空间。对此您怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;如果讨论的是大语言模型，我倾向于认为所谓的“生存空间”其实更多是“讲故事的空间”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用大模型的发展已经非常成熟，以最近发布的模型为例，像Gemini 3和GPT-5.2 Deep Think版本都非常强大。目前来看，很难找到能在某个领域超越这两个模型的垂域模型。以法律和教育问题为例，我更倾向于直接使用GPT-5.2或Gemini 3，而不是专门的法律或教育模型。虽然这些通用模型的成本较高，但其性能已经非常出色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果我要针对某个垂域开发应用，我会直接基于GPT-5.2进行开发，做好用户界面、数据库和基本范式，而不是自己去研发垂域模型。这种观点可能比较极端，但这是基于目前技术现状的判断——垂域模型的生存空间很有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：但垂域模型厂商会说他们的成本更低，这是否是一个优势？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我觉得这种说法有些本末倒置。首先，模型需要能够真正解决问题，才能谈成本优化。现在很多具身智能公司还在纠结成本问题，但它们可能都还没有找准真正能产生价值的应用场景。这种&quot;成本倒置&quot;的思路是不合理的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;科研方向与人才培养&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：刚才我们聊了很多技术方向和产业趋势，您也提到了不少前沿探索的可能性。能否具体谈谈您目前的科研方向与布局？您最看好哪个方向，为什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：我在学院负责大模型方向的研究，团队并行推进的方向很多，最近的一项工作是让智能体“预测未来”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各行各业本质上都绕不开同一道关：通过预测未来辅助科学决策。这听起来宏大，不同领域、不同机构，都在用各自的方式探索这个方向。比如政府出台政策前需要预判市场与社会反馈；企业制定战略前需要预估行业走势；金融机构甚至用系统去预测美国大选结果、下一场球赛谁输谁赢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这项工作的核心在于将“信息采集—逻辑推演—仿真模拟”三个环节形成闭环：首先通过智能体全自动打捞全网多模态开源情报，消除信息差；然后借助大模型的复杂推理能力进行因果建模和趋势判断；最后在虚拟环境中让成千上万个智能体反复演练，输出不同时间尺度下的演化曲线与风险概率。我们已参加多项国际预测评测，最好成绩全球第二，最新模型正在冲刺第一。把这三个环节打通，预测未来就不再是玄学，而成为可工程化的科学决策平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您之前介绍中关村两院和AI基础学部时，特别强调了人才培养这个维度。在AI攻坚克难的过程中，我们需要大量技术人才。您如何判断一个年轻人是否具备成为优秀科学家的潜力？在您看来，中国未来的AI人才应该具备哪三类核心能力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我去判断一个人是否有潜力时，会看重三个特质：首先是问题意识，他能不能自己发现问题、定义问题，而不只是等别人给题目；其次是挫折反应，科研99%的时间是失败，关键看如何应对失败；最后是跨界好奇心，他会不会主动去了解自己领域之外的东西，很多突破来自领域交叉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优秀人才还应该具备三类核心能力：一是数学和物理的第一性原理思维，这是AI时代下更重要的底层能力；二是系统工程能力，能把一个想法从论文变成可运行的系统；三是科学品味，知道什么问题值得做，这个最难教，但也最重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：随着AI的普及，我们观察到一个现象：无论是企事业单位、高校还是中小学，大家都在学习AI和编程，但也越来越依赖现成工具——从调用API、套模板，到直接使用AutoML、Copilot等——而对数学基础、算法原理的关注反而不足。微软CEO萨提亚·纳德拉也曾提到，AI很重要，但要避免过度依赖。您如何看待这种&quot;工具熟练度高，但科学基础薄弱&quot;的趋势？会担心未来的研究者变成&quot;只会调包、不会创新&quot;吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我的观点可能稍有不同，我想用一段技术演进的历史来解释这个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最早的程序员需要用“0和1”直接跟计算机对话，甚至在纸带上打孔输入程序。后来有了汇编语言，可以用简单的英文指令代替那些0和1。再后来出现了Python，写代码几乎像写英语句子。你会发现，每一次演进都在做同一件事：把繁琐的底层操作打包藏起来，让人不用操心&quot;怎么做&quot;，而是专注于“做什么”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个过程中，每一次进步都伴随着类似您提到的担忧：新一代程序员不懂底层原理了怎么办？但事实是，正是因为不用再纠结底层细节，程序员们才能腾出精力去解决更复杂、更有价值的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天的AI工具也是一样。它让研究者可以跳过很多繁琐的技术步骤，把精力放在真正重要的问题上——比如提出新假设、设计新实验、发现新规律。这些才是创新的本质，而不是亲手写每一行代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我的建议反而是：大胆拥抱最先进的工具，但要清楚自己真正想解决的问题是什么。工具是手段，问题才是目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：如果让您预测2030年最具影响力的AI科学突破，您会押注在哪三件事上？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我会押注在这三个方向上：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，AI智能性超过人类，ASI（超级人工智能）实现。&lt;/p&gt;&lt;p&gt;第二，AI在科学研究中能够自主完成发现和突破，比如找到治愈癌症的路径，或者解决数学领域悬而未决的开放问题。&lt;/p&gt;&lt;p&gt;第三，AI走进物理世界，对实体产业形成实质性推动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;采访嘉宾：&lt;/p&gt;&lt;p&gt;郑书新，中关村人工智能研究院副院长&amp;amp; 北京中关村学院副教授&lt;/p&gt;</description><link>https://www.infoq.cn/article/iHkvlLuTCWNJv27eJ1XY</link><guid isPermaLink="false">https://www.infoq.cn/article/iHkvlLuTCWNJv27eJ1XY</guid><pubDate>Wed, 11 Feb 2026 09:00:00 GMT</pubDate><author>李冬梅</author><category>AI 工程化</category></item><item><title>如何利用 Snowflake 将 AI 创新转化为可靠、生产就绪的应用 ｜ 技术趋势</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;人工智能创新正在持续重塑各行业与企业级的应用场景与用户体验。各公司日益聚焦于为终端用户创造可量化的实际价值。要实现这些价值，就需要可扩展、安全可靠且与企业数据深度整合的人工智能技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Snowflake，我们致力于帮助客户将人工智能与机器学习的宏伟蓝图转化为现实影响。这意味着我们将开发工具置于核心位置，使开发者能够更轻松地构建可靠的智能体，加速人工智能/机器学习工作流的上线部署，并在规模化扩展时从容管控相关负载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们最新的产品创新赋予客户基于 Snowflake 平台构建可靠、企业级应用的能力。这将带来更高效的执行、更简化的运维流程，以及企业可放心投入生产环境的人工智能工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Snowflake Intelligence 作为即开即用的企业级智能体&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 整合了一系列功能模块，旨在帮助企业用户快速、安全、自主地实现人工智能价值。本次更新聚焦三大核心需求：&lt;/p&gt;&lt;p&gt;&amp;nbsp;支持用户将有价值的对话输出保存为成果资产，并可将这些资产共享给其他利益相关方以支持商业决策（即将推出）；通过安全的原生移动端访问，满足业务人员随时随地使用需求（即将推出）；客户现可将业务人员纳入Snowflake Intelligence使用范畴，同时限制其对SQL及数据工具的访问权限。所有现有安全策略持续生效，管理员仅需通过单一用户属性即可启用该功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 致力于在工作发生的任何场景下提供可信洞察。其自然语言交互界面支持每位员工在Snowflake安全可控的平台内直接提出问题、挖掘数据表象背后的成因，并及时采取数据驱动的行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些功能共同构筑了 Snowflake Intelligence 作为可信赖企业智能体的核心能力，在用户需要的时空节点交付关键洞察，为全组织范围内的时效性数据驱动决策提供支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Artifacts：将对话转化为商业成果&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Artifacts（即将开启公开预览）代表着 Snowflake Intelligence 在赋能商业用户方式上的根本性转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Artifacts 可将 Snowflake Intelligence 中的对话转化为可保存、可共享的输出成果，例如图表与表格，并完整保留可视化呈现、底层 SQL 及上下文元数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Artifacts 是 Snowflake Intelligence 中实现企业知识捕获、共享与执行的核心单元。用户可通过保存 Artifacts 避免重复分析工作，安全地向团队成员共享实时引用，并在上下文中探索后续问题。Artifacts 支持用户回溯已构建的内容，与他人共享，并基于可信的企业数据直接展开协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更广泛而言，Artifacts 是 Snowflake Intelligence 向终端用户交付商业洞察能力的基础架构。通过Artifacts，Snowflake Intelligence 不再仅限于临时查询或后续追问，而是成为驱动业务发展的起点。借助Artifacts，我们正将 Snowflake Intelligence 打造为全组织统一、可靠决策的核心枢纽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Snowflake Intelligence 即将登陆移动端&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 将以 iOS 移动应用程序的形式（即将进入公开预览阶段）推出，提供更优的原生移动体验。移动端访问确保企业领导者和业务用户能够全天候连接企业知识库，无论是查看核心指标、追踪趋势变化，还是在决策过程中实时跟进关键问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为提供安全易用的体验，Snowflake Intelligence 移动应用将支持基于 FaceID 的会话续期功能（即将进入公开预览阶段）。用户可通过 FaceID 进行身份验证，令牌将在后台自动刷新。刷新令牌始终保持受保护状态，绑定设备并定期轮换，在实现企业级安全管控的同时，提供流畅的消费级移动体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;扩展访问权限：支持受限登录与 Snowflake Intelligence 专属用户&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 现支持用户直接登录，使业务用户无需了解 Snowflake 或操作 Snowsight 即可登录平台并开始提出问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于需要更严格管控的企业，Snowflake Intelligence 专属用户功能允许业务用户仅访问 Snowflake Intelligence，无法使用 Snowsight、SQL 接口或其他数据工具。这一设计让业务用户专注于专为其打造的交互界面，同时帮助企业统一管控使用范围、控制成本，并自动实施所有现有安全策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 还支持身份提供程序重定向功能。通过配置的身份提供程序（如 Okta 或 Entra ID）进行认证的用户，可获得简化的 Snowflake Intelligence 登录体验。这些功能相结合，使得在保障集中化治理控制的同时，能够轻松扩展企业内部的访问范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;轻松构建、部署与迭代智能体体验&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体现已成为企业工作流的核心。企业需要一个可靠、可信的体系架构，以在受管控且能跨团队、跨应用扩展的环境中，提供稳定精准的智能体验。我们很高兴宣布 Snowflake 平台上的重要创新，这些创新将帮助客户自信地构建并扩展生产级智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现已全面推出的 Cortex Code，通过赋能各类构建者——从资深工程师到非技术团队——利用自然语言交互构建并优化智能体，全面支持这一进程。它帮助团队轻松生成合成数据，创建与调试语义视图，并快速构建和调试智能体行为，从而加速在 Snowflake AI 数据云上的生产部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即将全面推出的语义视图自动巡航功能，可助力团队自动化创建并部署生产就绪的语义视图。通过学习查询历史，语义视图自动巡航简化了建模工作流，帮助组织更快接入新用例，同时在跨团队间提供一致的洞察分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为推进智能体在组织内的广泛应用，Cortex Agent Sharing（即将正式发布）可帮助用户轻松发现、复用并规模化部署由内部团队或合作伙伴构建的智能体。该功能使企业能够统一智能体能力标准，避免重复开发，并将经过验证的智能体快速拓展至各团队，无需为不同用例重复构建。团队可通过 Snowflake Marketplace 获取各类方案，并利用合作伙伴构建的智能体加速实现业务价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Agent Evaluations（即将正式发布），客户能够深入洞察智能体的推理过程、工具选择与响应生成机制，从而优化智能体行为，并在其演进过程中持续提升准确性。这种透明度有助于团队通过便捷的准确性验证与逻辑一致性检查，建立对智能体质量的信心，确保其满足生产环境要求。通过完整呈现智能体的“思考过程”，Agent Evaluations 减少了调试过程中的猜测性工作，使团队能够快速定位并修复错误或性能瓶颈。最终，通过对答案、逻辑及工具使用进行验证，企业可放心地将智能体从早期实验阶段推进为团队信赖的、可用于生产环境的成熟系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;面向企业数据访问的模型上下文协议（Model Context Protocol）支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 现已支持模型上下文协议（MCP），以简化与第三方工具及服务的集成。我们于 2025 年 10 月推出了由 Snowflake 托管的&lt;a href=&quot;https://www.snowflake.com/en/blog/managed-mcp-servers-secure-data-agents/&quot;&gt; MCP &lt;/a&gt;&quot;server，并在此基础上进一步推出 Snowflake MCP 客户端（即将全面上市），帮助客户以更便捷、可靠的方式连接外部数据源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Snowflake MCP 客户端，账户管理员可以注册预置或自定义的 MCP 服务器（例如 Atlassian、Salesforce 或 Workday），并将其直接集成至 Cortex 智能体中。开发人员可在智能体编排过程中使用 MCP 服务器，实现无缝的工具发现与调用。Snowflake 统一管理包括令牌处理在内的认证流程，并提供可观测性支持，确保集成过程安全可控。在本次发布中，Snowflake 支持在智能体调用期间完整的 MCP 工具发现功能，同时提供监控与令牌管理能力，使客户能够安全地跨系统访问并处理企业数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;面向企业级智能体的高性能与低延迟&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生产环境中，一致性及准确性对用户体验与应用推广至关重要。Snowflake 持续投入智能体技术栈的全面优化，致力于提供响应更迅速、结果更精准且具备规模化可预测性的AI驱动体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 即将推出持续学习型智能体记忆库（公共预览版），这是企业级智能体在质量层面的重大升级。该功能使智能体能够持续从跨用户的高质量历史响应中学习，从而提升回答一致性并增强可信度。同时，智能体可长期记忆个体用户的偏好与事实信息，为用户提供更加个性化的 Snowflake Intelligence 体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将文本转 SQL 功能深度集成至智能体编排流程，Snowflake 进一步提升了分析工作流的准确性与响应速度。用户得以更高效地访问数据，在查看 SQL 执行过程的同时透视 LLM 决策逻辑，并针对多样化工作负载灵活优化智能体行为模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;支持智能体版本管理与成本追踪的治理机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着人工智能应用不断发展，企业需要具备相应的治理能力以实现规模化扩展。Snowflake 通过智能体版本管理与集成化运行监控功能，为企业提供此类治理支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能体版本管理功能（即将开放公开预览）为 Snowflake Cortex 智能体提供 CI/CD 支持，使客户能够安全地构建、部署和迭代智能体工作负载。开发人员可创建版本快照，通过 Git 管理变更，并安全地推进或回滚部署。此外，客户即将通过使用量视图（即将正式发布）追踪 Snowflake Intelligence 与智能体的使用情况，从而获得更完善的运行状态洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除可视化监控外，Snowflake 还支持团队主动管控 AI 成本。已正式发布的 AI_COUNT_TOKENS 函数可在执行前预估使用量，而即将发布的 AI 函数增量计量视图（即将正式发布）将为运行中的查询提供使用量与成本数据，帮助团队在执行期间实施限额管控并触发相应操作。这些功能使企业能够在维持可预测开支与运行管控的同时，实现生产环境中 AI 应用的规模化扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过版本管理与成本追踪相结合，团队能够在保持清晰洞察的前提下快速发展，以负责任的方式构建高性能规模化应用程序。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;通过智能体工作流加速多模态机器学习模型的在线部署&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在人工智能领域，传统机器学习仍然占据重要地位。我们欣然宣布，Snowflake ML 在智能体、多模态及实时工作流方面推出了全新功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们持续投入现代化开发体验，致力于提升生产效率。新一代 Snowflake Notebooks（现已正式发布）现已成为 Snowflake Workspaces 的核心组成部分，运行于基于 Snowflake 容器运行时构建的 Jupyter 环境中。Snowflake Notebooks 使开发者能够将已有的基于 Jupyter 的笔记本、脚本及模型训练流程无缝引入 Snowflake 统一平台，实现先进的模型开发工作流。通过与 Snowsight 中的 Cortex Code 功能（即将正式发布）深度集成，Snowflake Notebooks 进一步提升了开发与迭代的效能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据科学家在开发和调试机器学习工作流时，常常面临周期冗长的问题，导致运维瓶颈以及实际投产的模型数量有限。如今，Snowflake 将 Cortex Code 集成至 Snowflake Notebooks 的机器学习工作流中，引入智能体人工智能，使其能够基于简单的自然语言提示自主迭代、优化并生成完整可执行的机器学习流水线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对实时机器学习模型，Snowflake ML 现已正式发布&lt;a href=&quot;https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/create-and-serve-online-features-python&quot;&gt;在线特征存储&lt;/a&gt;&quot;与&lt;a href=&quot;https://docs.snowflake.com/en/developer-guide/snowflake-ml/inference/real-time-inference-rest-api&quot;&gt;在线模型服务&lt;/a&gt;&quot;功能，使模型部署更加便捷。开发者现可将特征服务延迟控制在 30 毫秒内，模型服务延迟控制在 100 毫秒内，有力支持个性化推荐、欺诈检测等低延迟在线场景，且无需额外基础设施或复杂配置。此外，基于 Hugging Face 等主流多模态模型中心进行大规模推理的功能，目前已进入公开预览阶段。结合图像、视频等非结构化数据进行推理，可在 Snowflake 平台上直接实现物体检测、视觉问答和自动语音识别等多种人工智能应用，无需构建复杂流程或迁移数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 发展的未来&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今日发布的多项成果，共同奠定了 Cortex Agents 作为企业级AI统一基础的地位。Semantic View Autopilot 助力开发者提升 Cortex Agents 的准确性，并加速推进高级用例的落地。最新的 &lt;a href=&quot;https://www.snowflake.com/en/blog/production-ml-workflows/&quot;&gt;Snowflake ML&lt;/a&gt;&quot; 升级，使开发者能够构建可供 Cortex Agents 直接调用的模型，从而为用户提供基于机器学习的预测与建议。在生产环境中，我们推出的 Evaluations for Cortex Agents 确保智能体输出结果既可信赖，又便于监控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助 Snowflake 平台，企业能够将 AI 智能体与应用从实验阶段推进至生产部署，并获得团队信赖、由运维人员统一管理，最终直接赋能业务成效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;行动倡议：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;立即开始在 &lt;a href=&quot;https://www.snowflake.com/en/product/snowflake-intelligence/&quot;&gt;Snowflake Intelligence&lt;/a&gt;&quot; 中创建、保存并共享各类资产，以促进协同并推动业务行动。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;探索与 &lt;a href=&quot;https://www.snowflake.com/en/news/press-releases/snowflake-unveils-cortex-code-an-ai-coding-agent-that-drastically-increases-productivity-by-understanding-your-enterprise-data-context/&quot;&gt;Cortex Code&lt;/a&gt;&quot; 相关的发布内容。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;通过此篇&lt;a href=&quot;https://www.snowflake.com/en/blog/production-ml-workflows/&quot;&gt;博客&lt;/a&gt;&quot;，进一步了解机器学习领域的最新动态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/building-reliable-applications/&quot;&gt;https://www.snowflake.com/en/blog/building-reliable-applications/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/HQCgKB6UVJvDTePdIdoU</link><guid isPermaLink="false">https://www.infoq.cn/article/HQCgKB6UVJvDTePdIdoU</guid><pubDate>Wed, 11 Feb 2026 08:32:13 GMT</pubDate><author>Arun Agarwal</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>离开半年，48 岁前 GitHub CEO 携开源 AI 开发者平台和老东家打擂</title><description>&lt;p&gt;在很长一段时间里，Thomas Dohmke 都被视为“最不像 CEO 的 CEO”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他会在深夜亲自回复 GitHub Issues，会在发布会上公开演示自己写代码的过程，也会在 Copilot 最早的内测阶段，反复强调一句话：“如果这个东西不能改变开发者每天的工作方式，那它就不值得存在。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但去年 8 月，这位 GitHub 首席执行官正式离职。外界一度猜测，他是否会加入另一家大厂，或转向 AI 创业投资。但几个月后，他给出的答案更直接——重新创业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年48岁的 Thomas Dohmke 创办了一家名为 Entire 的新公司，这是一个面向“智能编码时代”的开源开发者平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他在x上宣布了这一消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/256cac6b650826d68c54daf800c89f1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Grab 首席产品官 Philipp Kandal在x上发帖表示祝贺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/08fdab18156a75f989f5442f40a04ba1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Entire 是谁？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这个 Entire 到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Dohmke 介绍，Entire 是一个平台，但它未必会与 GitHub 展开竞争。Dohmke 表示，其理念是在技术栈的更高层构建一个平台，让开发者能够管理智能体的推理过程并与之协作。代码仓库仍将是其中的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d4/d42e49833f9dd976315947b71e172543.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Entire 正在构建的是一个三层平台，其基础是一个从零开始构建的全新 Git 兼容数据库，中间是一个语义推理层，最上面是一个用户界面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队认为，由于这些新存储库中存储的信息有所不同，因此需要一个新的数据库层——具体来说，智能体在使用这些工具时能够提供比人类更多的上下文信息。这个新数据库将允许人类和智能体不仅可以查询代码，还可以查询代码背后的逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于代理使用此数据库及其 API 端点的频率可能远远高于人类使用 Git 存储库的频率，因此团队还需要考虑性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 还表示，与传统的集中式Git仓库不同，这种新型数据库可以构建成一个全球分布式的节点网络。对于需要（或希望）确保数据主权的用户来说，这是一个重要的卖点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然用户界面仍在开发中，但 Entire 已经构建了部分功能，用于可视化存储在Git中的检查点。不过，目前团队主要专注于命令行体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于为什么要开发这样一个平台，Dohmke给出了他的解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke强调，当前开发者/客服工作流程中存在的一个问题是我们现在经常听到的：代码交付的瓶颈不在于编写代码，而在于审查客服编写的代码。这已经导致开发者精疲力竭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如，爱尔兰软件工程师、谷歌Gemini 开发者（同时也在参与Chrome的开发） Addy Osmani 就曾公开表达了对 Vibe Coding 不信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们在谷歌也使用Vibe 编码——我发现它非常适合原型设计和最小可行产品（MVP），对学习也很有帮助……”Osmani 在11月初的一个播客节目中说道。“但总的来说，Vibe 编码更注重速度和探索，而不是正确性和可维护性。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3cb679cd4e55564b299da15829e314e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dohmke 认为，未来将会出现更多的Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“如果你在整个软件生命周期中都遵循这个流程，那么编写代码之后的下一步就是代码审查——无论是你自己的代码，还是通过拉取请求审查团队成员的代码，”Dohmke说道。“但是拉取请求也存在同样的问题（在理解代码方面）。它会显示一些我从未编写过的文件的更改。而像Copilot这样的代码审查工具会给我提供关于其代码的反馈，这在我对代码还有一些基本理解的情况下非常有用，但如果我不真正理解这些代码的作用，那么这些反馈就变得毫无意义或多余了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当代码较多而上下文较少时，解决方案可能是使用代理和确定性工具来测试代码，并确保其合规性和安全性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他解释说：“这正逐渐成为瓶颈，所以你必须从流程中剔除这一步骤。我认为这是业内最大的挑战之一，因为在我们应对日益增多的网络攻击的同时，许多组织已经引入了零信任流程，这意味着任何部署都必须经过人工审核。因此，我认为，在我们看来，许多创新将在这一领域涌现，而我们希望成为其中的一份子。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;首发新品&amp;nbsp;Checkpoints，并开源&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Checkpoints 是 Entire 公司发布的首款产品。这款全新开源工具集成了 Claude Code 和 Google 的 Gemini CLI（即将支持 Open Codex），能够自动提取并记录智能体的推理、意图和结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在当前的 Agent 开发流程中，一个长期被忽视的问题正在变得愈发突出：会话是短暂的，而决策是不可逆的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大多数情况下，智能体的提示词停留在终端里，推理过程被塞进上下文窗口。一旦会话结束，这些信息便随之消失。代码最终被提交进 Git，但 Git 只记录了“改了什么”，却无法回答“为什么这么改”。当智能体在一次会话中生成成百上千行代码时，这种上下文的缺失会迅速放大：早期的约束条件、设计取舍、被否定的方案，都无法被追溯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果是，智能体之间几乎无法真正协作。它们会在不同会话中重复推理、重复试错，重新消耗token，甚至推翻数小时、数天前已经做出的决定。随着代码库规模扩大，这种“失忆式开发”正在成为效率和一致性的隐性成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这一问题，一种名为 Checkpoints 的新机制被提出。它试图把原本易逝的智能体上下文，变成可持久、可追溯的工程资产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Checkpoints 的核心思路是：将智能体的完整会话上下文，作为 Git 中的一等版本数据保存下来。当由智能体生成的代码被提交时，系统不仅记录代码本身，还会同步捕获这次会话中的关键信息，包括提示词、日志、访问过的文件、工具调用情况以及令牌消耗等。这些信息与代码提交一一绑定，构成一条“为什么这样写”的语义轨迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从使用方式上看，Checkpoints 以一个“Git 感知”的命令行工具运行。每一次由智能体触发的提交，都会生成一个结构化的检查点对象，并与对应的提交 SHA 关联。代码仓库的内容本身并不发生改变，新增的是一层上下文元数据。当开发者将代码推送到远程仓库时，这些检查点会被同步写入一个独立的、只追加的分支，形成完整的审计日志。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，开发者不再只能回溯代码差异本身，还可以追溯到产生这些差异的推理过程和决策背景。在多人、多智能体协作的场景下，代码库的演进第一次具备了“记忆能力”，而不再只是结果的堆叠。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“中间层的作用在于向人类和Agent提供所有促成软件产品诞生的信息，” Dohmke 解释说。“而如今，在GitHub代码库中，包含了所有代码，有时还有文档和依赖项，但基本上缺少了所有关于如何实现这些代码的信息。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是因为这些系统是为人类开发者设计的，虽然开发者在完成代码编写后可能会编写测试用例和文档，但记录他们具体的推理步骤却从未被纳入流程。而在传统的、非智能体的工作流程中，大量的机构知识从未被记录下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这是我们更大愿景的第一步，即在软件项目的生命周期中提供语义推理层，这样你就可以在未来的任何时间点，以人类或智能体的身份，追踪决策的制定原因，”Dohmke 解释道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过保存所有这些数据，Checkpoints 将允许开发人员查看代理是如何生成代码的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有x用户在x上询问数据会如何处理？是否会被用于其他地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 表示：“Entire 会将上下文和Checkpoints存储在用户的 GitHub 代码库中。只要用户登陆上来，Entire 会将其同步到 Supabase 数据库，仅用于显示目的。从长远来看，我们希望构建一个语义层，以便人类开发者和智能体能够并行地进行推理、协作和构建。我们不会将您的数据用于除向您和您的团队提供平台功能之外的任何其他用途。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8f/8fe72ef1e0359147e9f550de7bc5ad27.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一位“工程师型 CEO”的来时路&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Thomas Dohmke 并不是传统意义上“职业经理人”出身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在加入 GitHub 之前，他是一名工程师、创业者，也是 GitHub 的长期重度用户。2018 年 GitHub 被微软收购后，他进入微软体系，随后在 2021 年接任 CEO，成为 GitHub 历史上第一位真正意义上的“工程师型掌舵人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他任内，GitHub 完成了一次关键转向：从“代码托管平台”，变成“以 Copilot 为核心的 AI 开发平台”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Copilot 的推出，并非单点功能升级，而是一次底层范式变化。GitHub 不再只服务“人如何协作写代码”，而是开始服务“人如何与 AI 一起写代码”。这一判断，后来被证明是整个行业的分水岭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，当 Dohmke 在 2025 年宣布离职时，他特意强调：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是一次完全友好的离开，不是对 GitHub 或微软路线的否定。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据他在接受 The New Stack 采访时回忆，自己在 6 月与微软 CEO 萨提亚·纳德拉进行过一次长谈。他向纳德拉坦陈了自己的想法：想回到“从零开始造东西”的状态。纳德拉的回应是，希望他“把 CEO 的工作好好做完”，同时也欢迎他继续留在微软生态中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也解释了一个细节：微软风投部门 M12，成为 Entire 的投资方之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/38/389d436b05484d1818c46c4cfffc844e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了GitHub技术领导者这样的职业经历做背书，Dohmke 在资本市场备受青睐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Entire 的首轮融资规模达 6000 万美元，由 Felicis 领投，Madrona、Basis Set 以及微软 M12 共同参与，公司估值达 3 亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，6000 万美元的融资在开发者工具领域并不常见。更重要的是，这是一个产品仍处于早期阶段的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在投资人看来，这并不是一次“押产品”，而是一次押人 + 押判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 给出的核心判断是：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;GitHub 所代表的那一代开发者平台，诞生于“人写代码”的时代，而不是“Agent写代码”的时代。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e9d6afa029f9291b02c2e7e84ed650df.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Felicis 创始人Aydin Senkut则押注的是Dohmke 丰富的行业经验，他在x上发文称：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我第一次见到 &lt;a href=&quot;https://x.com/ashtom&quot;&gt;@ashtom&lt;/a&gt;&quot; 他的远见卓识令我叹服。他对现代开发流程有着深刻的理解。作为 GitHub 的 CEO，他带领公司完成了人工智能的转型，并将平台规模扩展到全球超过 1.5 亿开发者。但他同时也清晰地预见到，如今整个行业都需要彻底革新。他的信念和洞察力令我深受鼓舞。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f8/f8d7d538bc3952e4d699c4c6b9c37b66.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着本轮融资的完成，Entire计划将其员工人数从目前的15人增加到约30人，并尽快搭建其平台。但正如Dohmke强调的那样，如今重要的不仅仅是员工。Entire团队甚至在其新闻稿中也提到，他们计划将团队规模扩大到“数百名客服人员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我认为到了2026年，任何领导者都需要重新审视员工人数，不再仅仅关注薪资、福利、差旅和开支，还要关注代币价值。我和一些工程师交流过，包括我自己团队的工程师，以及湾区的工程师，他们都在谈论每月价值数千美元的代币，”多姆克说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于商业模式，Dohmke 告诉我们，团队计划遵循成熟的开源模式，即以宽松的许可协议提供平台的大部分功能，然后提供具有附加功能的托管服务来实现盈利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：能用，但没必要硬吹&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Thomas Dohmke 宣布创办 Entire、并完成 6000 万美元种子轮融资之后，Hacker News 很快成为这条消息的“情绪放大器”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有Hacker News用户称，这并不意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位长期定义开发者工作流的关键人物，带着“为智能体时代重构软件工程”的宏大叙事重新创业，再加上一笔在开发者工具领域堪称夸张的种子轮融资，本身就足以触发 Hacker News 最典型的那种讨论：技术是否真的新？价值是否被高估？以及——这到底是在投产品，还是在投人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从整体来看，HN 上的讨论并未形成简单的“看好 / 唱衰”对立，而是呈现出几条高度一致、反复交织的主线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一条主线，大家更多的是在讨论“Entire到底是不是一个新原语？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;支持者与反对者的分歧，首先集中在 Entire 的首个产品 Checkpoints 本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1e/1e2f474191775ab0427f59e7f92160c9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在支持一方看来，Checkpoints 并不是一个“方便功能”，而是一种新的软件工程原语。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有开发者指出，Checkpoints 的关键不在于“保存 AI 生成的代码”，而在于它将代理的完整上下文——包括会话记录、提示、访问过的文件、工具调用和 token 使用情况——作为一级版本化数据，与代码提交一并捕获并长期保存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一视角下，Checkpoints 并不是在解决“怎么写代码”，而是在解决一个更根本的问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当代码主要由代理生成时，软件工程应该如何记录“思考过程”？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种观点认为，如果开发者无法回溯代理为什么在某个时间点做出某种选择，那么代码审查、安全验证乃至长期维护都会变得越来越脆弱。从这个意义上说，把推理过程纳入版本控制体系，本身就是一次范式转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但反对者几乎立刻给出了另一种解读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他们看来，这种能力并不新，甚至实现成本极低：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“你完全可以把 AI 生成的上下文当成文本，用 git add 提交。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这类评论中，Checkpoints 被描述为一个“被概念包装过的简单想法”：不是不能用，而是远不足以支撑一个被资本高度追捧的平台叙事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外一个比较集中被讨论的话题是，6000 万美元，究竟买的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说产品价值仍有争论，那么融资规模几乎是 Hacker News 上争议最集中的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位用户直言，他们并不否认“记录开发决策”这件事的意义，但完全无法理解：为什么这值得 6000 万美元的种子轮？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人直接点破，这笔融资隐含的估值，很可能已经超过 6 亿美元。在他们看来，这并不是“新的经济学”，而是风险投资在为自身账面价值服务——通过对早期项目给出极高定价，抬升整个基金组合的名义估值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7b/7b5f1d6900b3a69822bf1764b77eceda.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更激进的评论甚至认为，这种行为本身就应该被监管。但也有另一种更冷静、也更现实的声音指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;钱并不是在寻找“完美产品”，而是在寻找“可能的落点”。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/02/10/former-github-ceo-raises-record-60m-dev-tool-seed-round-at-300m-valuation/?utm_source=dlvr.it&amp;amp;utm_medium=twitter&quot;&gt;https://techcrunch.com/2026/02/10/former-github-ceo-raises-record-60m-dev-tool-seed-round-at-300m-valuation/?utm_source=dlvr.it&amp;amp;utm_medium=twitter&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/thomas-dohmke-interview-entire/&quot;&gt;https://thenewstack.io/thomas-dohmke-interview-entire/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=NrQkdDVupQE&quot;&gt;https://www.youtube.com/watch?v=NrQkdDVupQE&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fcjA0034GUQVp20cjHZU</link><guid isPermaLink="false">https://www.infoq.cn/article/fcjA0034GUQVp20cjHZU</guid><pubDate>Wed, 11 Feb 2026 08:16:45 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>网易有道发布中国版“OpenClaw”，推出全场景个人助理Agent“LobsterAI”</title><description>&lt;p&gt;随着AI从“能聊天”迈向“能办事”的Agent阶段，个人AI Agent已成为科技圈公认的下一波浪潮。2月11日，网易有道正式推出桌面级Agent“LobsterAI”（中文名：有道龙虾）。这是一个定位为“7×24 小时帮你干活的全场景个人助理Agent”，目前已在官网开放内测申请。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bfbe9878668dc1d15c000b8fcd3c8da.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图源：LobsterAI 官网&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，LobsterAI在产品形态上展现了独特的融合创新思路：它不仅具备海外爆火的“OpenClaw”那样自主跨应用执行复杂任务的能力，更融合了类似“Claude Cowork”的GUI（图形化交互）界面，旨在打造一款让用户能轻松驾驭、更安全、更易配置的中国版自主智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e2/e2b11ad15dc6a935aff9dd607be834c7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LobsterAI 界面图，图源：网易有道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前，海外开源项目“OpenClaw”因展示出惊人的“自主操控能力”而引爆技术圈，证明了Agent产品的能力边界；而另一款产品“Claude Cowork”，则利用具备强编程能力的模型，通过编程来完成各类任务，不仅取得了很好的效果，也为行业打开了更广阔的想象空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“LobsterAI也希望打造一款拥有高自由度，且具有长时记忆、定时任务等功能的产品，来拓宽和探索Agent在工作与学习场景下的应用潜能。”LobsterAI相关负责人介绍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统对话式AI不同，LobsterAI具备真正的“干活能力”——它摒弃了复杂的命令行操作，采用了类似Claude Cowork的直观GUI界面；无论是资讯获取、日程管理、还是深度数据分析，用户只需与其对话，LobsterAI便能在获得授权后，自动在本地计算机中通过程序化方式执行复杂流程，并交付结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从官方释出的信息来看，目前在设备支持上，LobsterAI已打通移动端与PC端的连接，用户可通过手机端在钉钉、飞书等软件中进行远程交互；哪怕你不在电脑前，也能指挥家里的LobsterAI帮你处理紧急工作，真正实现“数字分身”随时待命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6d/6df8b8804fc336c898105a9805d06fa5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LobsterAI 支持手机端钉钉远程交互，图源：网易有道&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用过程中，LobsterAI还支持定时任务机制，例如设定每天清晨自动搜集行业新闻、整理邮件摘要，当你开始工作时，所需资料已准备就绪。同时，LobsterAI还具备长上下文记忆能力，能够在多次协作中逐步理解用户偏好，形成更高效、连贯的个性化体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而针对用户关心的Agent自动操作风险，LobsterAI也采用了严格的“本地优先”策略。系统默认在沙盒环境（Sandbox）下的指定文件夹内运行，防止误操作破坏系统文件；同时支持数据本地化处理，杜绝云端泄露风险。在模型支持上，LobsterAI既预置了主流大模型API，也支持通过Ollama等框架调用DeepSeek&amp;nbsp;等本地开源模型，用户可根据任务对隐私和性能的需求灵活切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内观察人士指出，LobsterAI的上线，是有道多年积累的AI底层能力与应用洞察的一次集中爆发。它巧妙地结合了OpenClaw的技术深度与消费级产品的易用性，为国内Agent赛道提供了可落地的范本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品内测申请入口：&lt;a href=&quot;https://lobsterai.youdao.com/&quot;&gt;lobsterai.youdao.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fuib2PTStVR1lfgjZUMc</link><guid isPermaLink="false">https://www.infoq.cn/article/fuib2PTStVR1lfgjZUMc</guid><pubDate>Wed, 11 Feb 2026 08:15:10 GMT</pubDate><author>网易有道技术团队</author><category>企业动态</category><category>AI&amp;大模型</category></item></channel></rss>