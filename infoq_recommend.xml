<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Thu, 12 Feb 2026 17:05:59 GMT</lastBuildDate><ttl>5</ttl><item><title>2025年的Web开发：AI的React偏见 vs 原生Web</title><description>&lt;p&gt;随着越来越多的开发者寻求React生态系统之外的解决方案，像Astro和Svelte这样的前端框架越来越受欢迎，今年Web开发的复杂性进一步降低。与此同时，原生Web平台的特性证明了它们能够胜任构建复杂的Web应用程序的工作——尤其是CSS在2025年得到特别改进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;话虽如此，也许今年最大的Web开发趋势是AI辅助编码的兴起——事实证明，它倾向于默认使用React和领先的React框架Next.js。因为React在前端领域占据主导地位，大语言模型（LLM）有很多React代码进行训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我们更详细地看看2025年的五大Web开发趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;1. 原生Web特性的崛起&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年，许多原生Web特性悄然赶上了JavaScript框架提供的功能。例如，&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API&quot;&gt;视图&lt;/a&gt;&quot;、转换&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API&quot;&gt;API&lt;/a&gt;&quot; ——它可以让你的网站在页面之间流畅地切换——成为了&lt;a href=&quot;https://webstatus.dev/?q=baseline_date%3A2025-01-01..2025-12-31&quot;&gt;Baseline 2025&lt;/a&gt;&quot;索引跨浏览器支持的一部分。因此，现在Web开发人员可以广泛使用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Baseline是一个由W3C的WebDX社区小组协调的项目，包括来自&lt;a href=&quot;https://cloud.google.com/?utm_content=inline+mention&quot;&gt;谷歌&lt;/a&gt;&quot;、Mozilla、微软和其他组织的代表。它从2023年才开始运行，&lt;a href=&quot;https://thenewstack.io/interop-unites-browser-makers-to-smooth-web-inconsistencies/&quot;&gt;但今年它真正成为了实践Web开发人员的有用资&lt;/a&gt;&quot;源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/6568cddec29d68c4020dc2f6710d7ac4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Baseline特性的稳定年度增长，&lt;a href=&quot;https://webstatus.dev/stats&quot;&gt;通过Web平台状态站点观测&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如The New Stack的Mary Branscombe&lt;a href=&quot;https://thenewstack.io/baseline-newly-available-stay-on-top-of-new-web-features/&quot;&gt;在6月份所报道&lt;/a&gt;&quot;的那样，有很多方法可以跟踪Baseline的变化：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“谷歌的Web.Dev有关于Baseline特性和新闻的&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/release-notes/march-2025/&quot;&gt;月度更新&lt;/a&gt;&quot;，WebDX特性浏览器允许你查看&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/widely-available/&quot;&gt;有限可用、新可用或广泛可用&lt;/a&gt;&quot;的特性；&lt;a href=&quot;https://web-platform-dx.github.io/web-features-explorer/release-notes/march-2025/&quot;&gt;月度发布&lt;/a&gt;&quot;说明涵盖了哪些特性达到了新的Baseline状态。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Web功能的角度来看，现在真的没有理由不使用原生Web功能。正如资深Web开发人员&lt;a href=&quot;https://adactio.com/journal/22235&quot;&gt;Jeremy Keith&lt;/a&gt;&quot;最近所说，框架“限制了你在web浏览器中所能做的事情的可能性空间”。在随后的一篇文章中，Keith敦促开发人员尤其不要在浏览器中使用React，因为文件大小对用户来说成本太高了。相反，他鼓励开发人员“研究在浏览器中可以使用纯JavaScript做些什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2. AI编码助手默认为React&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年，AI成为了Web开发工具链的标准组成部分（尽管并不总是得到开发者的认可，特别是那些在Mastodon或Bluesky而不是X或LinkedIn上社交的开发者）。无论你是不是应用程序开发中的AI粉丝，都有一个大问题：LLMs倾向于默认使用React和Next.js。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当OpenAI的&lt;a href=&quot;https://thenewstack.io/gpt-5-a-choose-your-own-adventure-for-frontend-developers/&quot;&gt;GPT-5&lt;/a&gt;&quot;在8月发布时，其所谓的优势之一是编码。GPT-5最初从开发者那里获得了褒贬不一的评价，所以在那个时候，我联系了OpenAI，向他们询问编码特性。OpenAI的研究员&lt;a href=&quot;https://www.linkedin.com/in/ishaan-singal/&quot;&gt;Ishaan Singal&lt;/a&gt;&quot;通过电子邮件回复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我向Singal指出，在&lt;a href=&quot;https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide&quot;&gt;GPT-5&lt;/a&gt;&quot;提示指南中，有三个推荐的框架：Next.js（TypeScript）、React和HTML。我问是否有与Next.js和React项目团队合作，以优化GPT-5对这些框架的支持？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们选择这些框架是基于它们的受欢迎程度和通用性，但我们并没有直接与Next.js或React团队在GPT-5上合作，”他回答说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c5/c58dfbb6641b7ba07d9d13ffa16dddb1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI的GPT-5提示指南中的“组织GPT-5代码编辑规则”的示例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们知道，负责&lt;a href=&quot;https://thenewstack.io/vercels-frontend-and-the-rise-of-the-hybrid-developer/&quot;&gt;Next.js&lt;/a&gt;&quot;框架的公司Vercel是GPT-5的粉丝。在发布当天，它称GPT-5是“最好的前端AI模型”。所以这里发生了一个很好的交换条件——GPT-5之所以能够成为Next.js的专家，是因为它的受欢迎程度，这可能进一步增加了它的受欢迎程度。这对OpenAI和Vercel都有帮助。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“归根结底，这是开发者的选择，”Singal总结道，关于开发者想要使用哪些Web技术。“但成熟的代码库有更好的社区支持。这有助于开发者自助维护。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;3. AI智能体和聊天机器人中web应用的出现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年，我们看到了AI聊天机器人和智能体中小型Web应用的出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://mcpui.dev/&quot;&gt;MCP-UI&lt;/a&gt;&quot;是Web将成为AI智能体关键部分的第一个迹象。顾名思义，&lt;a href=&quot;https://mcpui.dev/&quot;&gt;MCP-UI&lt;/a&gt;&quot;使用流行的模型上下文协议作为通信基础。&lt;a href=&quot;https://mcpui.dev/guide/introduction&quot;&gt;该项目&lt;/a&gt;&quot;“旨在标准化模型和工具如何在客户端应用程序中请求显示丰富的HTML界面。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/mcp-ui-creators-on-why-ai-agents-need-rich-user-interfaces/&quot;&gt;在8月的一次采访中&lt;/a&gt;&quot;，两位创始人（其中一位当时在Shopify工作）解释说，MCP-UI有两种类型的SDK：客户端SDK和连接到MCP服务器的服务器SDK。服务器SDK提供TypeScript、Ruby和Python版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/af/af16ee40af1d4efb52383ea105019ece.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个UI被插入到Claude 3.7 Sonnet聊天中的MCP-UI演示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MCP-UI听起来很有前途，但很快就被OpenAI 10月初发布的&lt;a href=&quot;https://developers.openai.com/apps-sdk&quot;&gt;Apps SDK&lt;/a&gt;&quot; 盖过了风头。&lt;a href=&quot;https://developers.openai.com/apps-sdk&quot;&gt;Apps SDK&lt;/a&gt;&quot; 允许第三方开发者构建基于Web的应用程序，这些应用程序作为ChatGPT对话中的交互式组件运行——这让我们想起了2008年苹果推出应用商店时的情景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Apps SDK的决定性特征是基于Web的UI模型（类似于MCP-UI）。ChatGPT应用组件是一个Web UI，运行在ChatGPT对话中的沙箱框架中。ChatGPT作为应用程序的主机。你可以将第三方ChatGPT应用程序视为直接嵌入ChatGPT界面的“迷你Web应用程序”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到10月底，像Vercel这样的行业巨头已经想出了如何使用他们的JavaScript框架来构建&lt;a href=&quot;https://thenewstack.io/next-js-in-chatgpt-vercel-brings-the-dynamic-web-to-ai-chat/&quot;&gt;ChatGPT&lt;/a&gt;&quot;应用程序。Vercel将Next.js与ChatGPT应用程序平台的快速集成表明，AI聊天机器人将不仅仅局限于轻度交互的小部件——复杂的Web应用程序也将在这些平台上存在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;4. 浏览器中的Web AI和设备上的推理&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年的另一个并行发展是&lt;a href=&quot;https://thenewstack.io/how-google-is-shifting-ai-from-the-cloud-to-your-browser/&quot;&gt;在浏览器中运行客户端AI的兴起&lt;/a&gt;&quot;，这允许LLM推理在设备上进行。谷歌在这方面尤为突出；它对这种趋势的称呼是“Web AI”。&lt;a href=&quot;https://www.linkedin.com/in/webai/&quot;&gt;Jason Mayes&lt;/a&gt;&quot;，谷歌这些举措的负责人，将&lt;a href=&quot;https://www.linkedin.com/pulse/life-edge-web-ai-history-future-smarter-digital-agentic-jason-mayes-fbqbc/&quot;&gt;Web AI&lt;/a&gt;&quot;定义为“通过Web浏览器在用户设备上运行任何机器学习模型或服务的艺术。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;11月，谷歌举办了一场仅限受邀者参加的活动，名为谷歌&lt;a href=&quot;https://www.linkedin.com/pulse/life-edge-web-ai-history-future-smarter-digital-agentic-jason-mayes-fbqbc/&quot;&gt;Web AI&lt;/a&gt;&quot;峰会。之后，我采访了活动的组织者兼主持人Mayes，他解释说，一个关键技术是LiteRT.js，谷歌的Web AI运行时，目标是生产Web应用程序。它建立在&lt;a href=&quot;https://ai.google.dev/edge/litert&quot;&gt;LiteRT&lt;/a&gt;&quot;的基础上，后者旨在直接在设备（移动、嵌入式或边缘）上运行机器学习（ML）模型，而不是依赖于云推理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Web AI峰会的主题演讲中，谷歌负责Chrome和Web生态系统的副总裁兼总经理&lt;a href=&quot;https://www.linkedin.com/in/parisatabriz/&quot;&gt;Parisa Tabriz&lt;/a&gt;&quot;强调了去年8月Chrome内置的&lt;a href=&quot;https://thenewstack.io/googles-web-ai-playbook-the-paved-road-vs-the-open-field/&quot;&gt;AI API&lt;/a&gt;&quot;，以及去年6月发布的作为Chrome内置功能的Gemini Nano——谷歌的主要设备上模型。这些和其他Web技术正在推动当前的Web AI趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7ae02d903acc9c4fe0e82d469631c54f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Parisa Tabriz在Web AI峰会上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌与微软一起参与的另一项创新是&lt;a href=&quot;https://thenewstack.io/how-webmcp-lets-developers-control-ai-agents-with-javascript/&quot;&gt;WebMCP&lt;/a&gt;&quot;的发布，它允许开发人员使用客户端JavaScript控制AI智能体如何与网站交互。在9月与微软Edge的Web平台产品经理&lt;a href=&quot;https://www.linkedin.com/in/kylepflug/&quot;&gt;Kyle Pflug&lt;/a&gt;&quot;的采访中，他解释道：“核心概念是允许Web开发者用JavaScript为他们的网站定义‘工具’，就像传统MCP服务器提供的工具一样。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Web AI不仅仅是由商业公司推广。万维网联盟（W3C）也在探索“&lt;a href=&quot;https://thenewstack.io/the-agentic-web-how-ai-agents-are-shaping-the-webs-future/&quot;&gt;代理式Web&lt;/a&gt;&quot;”的构建模块，其中包括使用MCP-UI、WebMCP和另一个新兴的称为&lt;a href=&quot;https://thenewstack.io/cloudflares-balancing-act-protect-content-while-pushing-ai/&quot;&gt;NLWeb&lt;/a&gt;&quot;的标准（由微软开发）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;5. JavaScript生态系统的“生命化”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这听起来像是AI主导了今年的web开发——事实上也确实如此。但前端工具也看到了它的创新份额。有一款产品特别引人注目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://vite.dev/&quot;&gt;Vite&lt;/a&gt;&quot;，由&lt;a href=&quot;https://evanyou.me/?utm=22b03&quot;&gt;Evan You&lt;/a&gt;&quot;创建，已经成为现代前端框架的首选构建工具，包括Vue、SvelteKit、Astro和React——也有来自Remix和Angular的实验性支持。在9月份接受&lt;a href=&quot;https://thenewstack.io/how-vite-became-the-backbone-of-modern-frontend-frameworks/&quot;&gt;The New Stack&lt;/a&gt;&quot;采访时，You告诉我，Vite成功的关键在于它早期使用了ES模块 (ESM)，这是一种标准化的JavaScript模块系统，允许你“将JavaScript代码分解成不同的片段，不同的模块，你可以加载。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2d/2d49491dee779acc4fc1a378385f4cad.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Even You在ViteConf上展示的Vite生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;You和他的公司VoidZero现在正在构建 Vite+，一个新的统一JavaScript工具链，旨在解决JavaScript碎片化问题。在今年的ViteConf 活动上，You正式推出了Vite+，将其定位为企业开发工具包。他说它包括“你喜欢Vite的一切——加上你一直在用胶带粘合在一起的一切。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Web开发的十字路口&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2025年底，感觉我们正处于前端开发的十字路口。一方面，有一种方法可以解决React的复杂性难题：使用原生Web特性和工具，如 Astro，减轻用户的负担。虽然这确实是今年的一个趋势，但它有可能在2026年被我们越来越依赖AI工具编码所掩盖——正如所指出的，这些工具倾向于依赖React。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实是，现在的大多数开发者——包括成千上万以前不属于开发者生态系统的“vibe程序员”——将继续由AI系统提供React代码。这使得Web开发社区在明年继续支持和倡导原生Web代码变得更加必要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/web-development-in-2025-ais-react-bias-vs-native-web/&quot;&gt;https://thenewstack.io/web-development-in-2025-ais-react-bias-vs-native-web/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/SIQ9aJiSeqplKcOeAAmM</link><guid isPermaLink="false">https://www.infoq.cn/article/SIQ9aJiSeqplKcOeAAmM</guid><pubDate>Thu, 12 Feb 2026 10:21:41 GMT</pubDate><author>Richard MacManus</author><category>生成式 AI</category><category>架构/框架</category></item><item><title>“攻击迫在眉睫”，39%的云环境存在最高严重性的React漏洞</title><description>&lt;p&gt;广泛使用的JavaScript库React以及包括&lt;a href=&quot;http://next.js/&quot;&gt;Next.js&lt;/a&gt;&quot;在内的几个基于React的框架存在一个最高严重性的漏洞，允许未经身份验证的远程攻击者在易受攻击的实例上执行恶意代码。安全研究人员表示，这个漏洞很容易被滥用，大规模利用“迫在眉睫”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React团队在周三披露了React Server Components中的未经身份验证的远程代码执行（RCE）漏洞。它被跟踪为&lt;a href=&quot;https://www.cve.org/CVERecord?id=CVE-2025-55182&quot;&gt;CVE-2025-55182&lt;/a&gt;&quot;，并获得了最高的10.0的CVSS严重性评级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一个大问题，因为大部分互联网都是建立在React之上的——据估计，39%的云环境容易受到这个漏洞的影响。因此，这个问题应该在你的待办事项列表中占据一个突出的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个漏洞影响了19.0、19.1.0、19.1.1和19.2.0版本的：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-webpack&quot;&gt;react-server-dom-webpack&lt;/a&gt;&quot;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-parcel&quot;&gt;react-server-dom-parcel&lt;/a&gt;&quot;&lt;a href=&quot;https://www.npmjs.com/package/react-server-dom-turbopack?activeTab=readme&quot;&gt;React-server-dom-turbopack&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它还影响了包括&lt;a href=&quot;https://www.npmjs.com/package/next&quot;&gt;next&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/react-router&quot;&gt;react-router&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/waku&quot;&gt;waku&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/@parcel/rsc&quot;&gt;@parcel/rsc&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.npmjs.com/package/@vitejs/plugin-rsc&quot;&gt;@vitejs/plugin-rsc&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.npmjs.com/package/rwsdk&quot;&gt;rwsdk&lt;/a&gt;&quot;在内的几个React框架和打包器的默认配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;项目的维护者表示，升级到&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.0.1&quot;&gt;19.0.1&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.1.2&quot;&gt;19.1.2&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/facebook/react/releases/tag/v19.2.1&quot;&gt;19.2.1&lt;/a&gt;&quot;版本可以修复这个漏洞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们建议立即升级，”React团队在周三的安全咨询中表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“CVE-2025-55182对世界上使用最广泛的Web应用程序框架之一的用户构成了重大风险，”风险管理工具供应商watchTowr的创始人兼首席执行官Benjamin Harris告诉The Register。“利用几乎不需要先决条件，[并且]毫无疑问，一旦攻击者开始分析现在公开的补丁，就会立即进行野外利用。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Next.js的创建者和主要维护者Vercel为该漏洞分配了自己的CVE（&lt;a href=&quot;https://github.com/vercel/next.js/security/advisories/GHSA-9qr9-h5gf-34mp&quot;&gt;CVE-2025-66478&lt;/a&gt;&quot;），并在周三发布了警报和补丁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然我们没有太多关于这个漏洞的细节，但我们知道它滥用了React解码发送到React Server Function端点的有效载荷的缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未经身份验证的攻击者可以制作一个恶意的HTTP请求到任何Server Function端点，当被React反序列化时，可以在服务器上实现远程代码执行，”安全警报警告说。“有关该漏洞的更多细节将在修复完成后提供。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周六，研究员Lachlan Davidson发现并报告了这个缺陷给Meta，Meta创建了这个开源项目。Meta与React团队合作，在四天后迅速推出了一个紧急补丁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;React的应用非常广泛——Meta的Facebook和Instagram、Netflix、Airbnb、Shopify、Hello Fresh、Walmart和Asana都依赖于它，数百万开发者也依赖于它——许多框架都依赖于易受攻击的React包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，这个CVE将大部分互联网置于危险之中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Wiz数据表明，39%的云环境包含易受CVE-2025-55182和/或CVE-2025-66478影响的Next.js或React的实例，”云安全商店的威胁猎人Gili Tikochinski、Merav Bar和Danielle Aminov在周三表示。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.theregister.com/2025/11/05/googles_32b_wiz_acquisition_its/&quot;&gt;这家即将被谷歌收购的公司&lt;/a&gt;&quot;对这个漏洞进行了实验和修复，并报告说“利用这个漏洞的保真度很高，成功率接近100%，可以利用它来执行完整的远程代码。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“由于其严重性和易利用性，需要立即打补丁，”三人补充说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在撰写本文时，The Register没有发现任何野外利用的报告。然而，可以肯定的是，犯罪分子已经在逆向工程补丁，并在互联网上扫描暴露的、易受攻击的实例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“由于React和Next.js等框架的广泛使用，这个漏洞预计将引起极大的关注，”Rapid7的高级首席研究员Stephen Fewer告诉The Register。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术细节和利用代码被公开的可能性很高，因此利用可能很快就会发生，”他说。“因此，立即修补这个漏洞至关重要。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare声称，如果他们的React应用程序流量是通过WAF代理的，那么他们的Web应用程序防火墙（WAF）可以保护他们免受该漏洞的侵害。®&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/?td=rt-3a&quot;&gt;https://www.theregister.com/2025/12/03/exploitation_is_imminent_react_vulnerability/?td=rt-3a&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/qRHs8o3kirzwSImTe7iO</link><guid isPermaLink="false">https://www.infoq.cn/article/qRHs8o3kirzwSImTe7iO</guid><pubDate>Thu, 12 Feb 2026 10:15:19 GMT</pubDate><author>Jessica Lyons</author><category>架构/框架</category></item><item><title>RFC 规范中的 CNAME 顺序问题是如何导致 Cloudflare 1.1.1.1 宕机的</title><description>&lt;p&gt;在一篇题为&lt;a href=&quot;https://blog.cloudflare.com/cname-a-record-order-dns-standards/&quot;&gt;《先有 CNAME，还是先有 A 记录？》&lt;/a&gt;&quot;的最新文章中，Cloudflare 解释了一个 RFC 规范表述不清的问题，是如何导致其广受欢迎的 1.1.1.1 公共 DNS 服务发生故障的。在定位问题并发现旧版 DNS 标准中关于记录顺序的模糊之处后，Cloudflare 提出了一份澄清后的规范建议。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 8 日，一次看似例行的 DNS 服务更新改变了响应中 CNAME 记录出现的顺序，导致部分 DNS 客户端在解析域名时失败，因为它们假定别名记录必须先出现。尽管大多数现代软件认为 DNS 响应中记录的顺序并不重要，但 Cloudflare 团队发现，一些实现实际上依赖 CNAME 记录必须出现在其他记录类型之前。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当这一顺序发生变化后，DNS 解析开始失败，最终引发了 1.1.1.1 这一流行公共 DNS 服务的一次严重中断。Cloudflare 的系统工程师 &lt;a href=&quot;https://www.linkedin.com/in/sebastiaan-n/&quot;&gt;Sebastiaan Neuteboom&lt;/a&gt;&quot; 解释了该变更引入的原因和时间点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在对缓存实现进行一些降低内存占用的改进时，我们引入了一个关于 CNAME 记录顺序的细微变化。该变更于 2025 年 12 月 2 日引入，12 月 10 日发布到测试环境，并于 2026 年 1 月 7 日开始部署。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当 DNS 解析器查询一个包含 CNAME 的域名时，它可能会看到一系列别名记录，将原始名称一路链接到最终的地址，并且解析器会以不同的过期时间缓存链路中的每一步。Cloudflare 指出，如果这条链中的某一部分在缓存中已过期，解析器只会重新获取过期的那一段，并与仍然有效的部分组合，形成完整的响应。Neuteboom 补充道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;之前的代码会创建一个新的列表，先插入已有的 CNAME 链，然后再追加新获取的记录（……）。但为了减少内存分配和拷贝，代码被修改为直接把 CNAME 追加到现有的 answer 列表中。结果是，1.1.1.1 返回的响应中，CNAME 记录有时会出现在最底部，也就是最终解析结果之后。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;示例如下：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;;; QUESTION SECTION:
;; www.example.com.       IN    A


;; ANSWER SECTION:
cdn.example.com.    300    IN    A      198.51.100.1
www.example.com.    3600   IN    CNAME  cdn.example.com.&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然许多 DNS 客户端实现并不依赖记录顺序，例如 systemd-resolved，但也有一些实现（包括 glibc 中的 getaddrinfo 函数）在解析过程中会跟踪“期望的记录名称”，并按顺序遍历响应内容，假定在任何最终答案之前都能先遇到 CNAME 记录。Reddit 上有用户&lt;a href=&quot;https://www.reddit.com/r/technology/comments/1qhg8ww/what_came_first_the_cname_or_the_a_record/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一方面，我非常敬佩他们在事后分析中展现出的细节和极高的工程标准；但另一方面，我也忍不住觉得，他们似乎并没有建立起足够完善的测试体系（以及相应的工程文化），来真正理解他们的改动在全球范围内会产生怎样的影响。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 &lt;a href=&quot;https://news.ycombinator.com/item?id=46681611%E3%80%81&quot;&gt;Hacker News 上的一篇热门讨论&lt;/a&gt;&quot;中，许多用户围绕 RFC 是否真的存在歧义展开了争论，尤其是在 RRset 与 RR 在消息分区中的细微区别上，还是说 Cloudflare 的工程师误解了规范。Patrick May 则评论道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是一个典型的 Hyrum 定律案例：“当一个 API 拥有足够多的用户时，你在契约中承诺了什么并不重要，系统中所有可观察到的行为，都会被某些人所依赖。”再叠加上未能遵循 Postel 定律：“发送时要保守，接收时要宽容。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一份即将在 IETF 讨论的 &lt;a href=&quot;https://datatracker.ietf.org/doc/draft-jabley-dnsop-ordered-answer-section/&quot;&gt;Internet-Draft&lt;/a&gt;&quot; 中，Cloudflare 提议制定一份 RFC，明确规定 DNS 响应中应如何正确处理 CNAME 记录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据公开的时间线，Cloudflare 于 1 月 7 日开始全球部署，并在 1 月 8 日 17:40（UTC）覆盖了 90% 的服务器。公司随后宣布发生事故，并于 1 月 8 日 18:27 开始回滚变更，最终在 19:55 完成回滚。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/cname-rfc-cloudflare-outage/&lt;/p&gt;</description><link>https://www.infoq.cn/article/h5huIV6bVgqR2BeSNuPC</link><guid isPermaLink="false">https://www.infoq.cn/article/h5huIV6bVgqR2BeSNuPC</guid><pubDate>Thu, 12 Feb 2026 10:00:00 GMT</pubDate><author>作者：Renato Losio</author><category>云计算</category></item><item><title>“代码 + 编译器”要消失了？马斯克在 xAI 全员会上放话：到今年年底，AI 或将直接生成二进制</title><description>&lt;p&gt;一家 AI 公司要是在很短时间里接连走了联合创始人和一批核心工程师，外界第一反应通常就一句话：完了，这肯定出事了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 过去一周的离职潮就是这种感觉。消息在 X 上越滚越大，最后干脆被玩成梗：有人明明从没在 xAI 上过班，也跑去发帖“我也离职了”，用跟风式的调侃把“集体出走”的说法跑偏成了大型玩梗现场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克没做任何解释，直接把一场 45 分钟的全员大会录像公开出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段视频等于一次对外说明：离职到底是大家自己走，还是公司在做组织调整？xAI 现在在忙什么、谁负责什么？Grok、编程模型、视频生成、Macrohard（多智能体软件公司）这四条线接下来要怎么推进？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更狠的是，马斯克在视频里还抛出一个判断：到 2026 年底，AI 甚至可能不写代码了，直接生成二进制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad582af38bae009849a41a7279367722.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;埃隆·马斯克预测，到 2026 年底，AI 将彻底绕过传统编程，不再写代码，而是直接生成二进制程序。他认为，AI 所生成的二进制文件，其效率可以超过任何编译器所能产出的结果。也就是说，未来你只需要告诉 AI：“为这个特定目标生成一个经过优化的二进制程序”，就可以直接跳过传统意义上的编程过程。&amp;nbsp;当前的软件开发流程是：代码 → 编译器 → 二进制 → 执行；而马斯克设想中的未来将变成：Prompt（指令）→ AI 生成的二进制 → 执行。他还表示，Grok Code 有望在 2 到 3 个月内达到业界最顶尖（state-of-the-art）水平。&amp;nbsp;在马斯克看来，软件开发正站在一次根本性变革的门槛上。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克：不是离职潮，是我在裁员&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一周之内，一家 AI 公司接连失去两位联合创始人和多名核心工程师，这很难再被当作正常的人才流动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 xAI，离职消息几乎是“扎堆”出现的，很快就不只是“谁走了”的八卦，而变成了另一个更直接的问题：这家公司还能不能稳住？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;TechCrunch 统计显示，过去一周里，至少有 9 名工程师公开宣布离开 xAI，其中包括两位联合创始人 Jimmy Ba 和 Tony Wu。短短几天，创始团队就少了将近一半。对任何一家仍在高速扩张的公司来说，这种速度和幅度都足够让人警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da069f16948f6a8abc03e7451d081609.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d8304fae2ddccdacf0b04d5699863775.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f7/f7288f1e4996a8b6b75b527506eeca5d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克显然意识到了这一点。在离职叙事迅速发酵、并开始脱离公司控制之前，他选择了一种极不寻常的应对方式：公开一场内部全员大会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一段长达 45 分钟的 all-hands meeting 视频，被直接放到了 X 上，对所有人开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在公开视频之前，马斯克已经在内部会议上给出了他的判断。周二晚间的全员大会上，他将这轮离职定性为“阶段适配问题”，而非绩效问题。“因为我们已经发展到一定规模，我们正在重组公司结构，以便在这个规模下更高效地运作，”他说，“而事实上，在这种情况下，有些人更适合公司的早期阶段，却不太适合后期阶段。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克随后也在 X 上明确表示，这是一轮因组织结构调整而产生的人员分离——本质上是裁员，而非单纯的个人选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4d/4df6287ba0edb86de871d2f9c61a68ab.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“随着公司快速成长，组织结构必须进化。这不幸地意味着需要与一些人分道扬镳。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在外界看来，这一解释并未完全平息争议。xAI 在 两天之内失去两位联合创始人 的事实，很快引发了更多关于内部节奏与执行压力的讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;The information则给出了另一种解读。据知情人士透露，xAI 原计划在去年年底或今年 1 月初发布 Grok 4.2，但这一时间点最终被错过。“埃隆不喜欢延期，”他们写道，“当他对某个项目感到愤怒，或者高度聚焦时……就会有人出局。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3cdc69559c9b240ac746a0ec432e0d9.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一说法虽非官方表态，却在舆论场中迅速传播，也从侧面解释了为什么这场 all-hands meeting 不只是一次例行沟通，而更像是一场在压力之下，对内对外同时展开的解释与重组。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;xAI 离职时间表（公开信息汇总）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 6 日，Ayush Jaiswal（工程师）写道：“这是我在 xAI 的最后一周。接下来几个月我会陪伴家人，同时折腾一些 AI 相关的东西。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 7 日，Shayan Salehian（负责产品基础设施和模型后训练行为，曾就职于 X）写道：“我已离开 xAI，准备开启新的事业，也正式结束我在 Twitter、X 和 xAI 工作的 7 年多时光，满怀感激。”他还提到，与马斯克近距离共事让他学会了“对细节的偏执关注、近乎疯狂的紧迫感，以及从第一性原理思考问题”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 9 日，Simon Zhai（技术人员）写道：“今天是我在 xAI 的最后一天，非常感激这次机会。这是一段令人惊叹的旅程。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 9 日，Yuhuai（Tony）Wu（联合创始人、推理负责人）写道：“我今天从 xAI 辞职了。是时候开启新篇章了。这是一个充满可能性的时代：一个配备 AI 的小团队，可以移山填海，重新定义可能性。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Jimmy Ba（联合创始人、研究与安全负责人）写道：“今天是我在 xAI 的最后一天。借助合适的工具，我们正迈向 100 倍生产力的时代。递归式自我改进循环很可能在未来 12 个月内上线。是时候重新校准我在宏观层面的‘梯度’了。2026 年将会疯狂，并且很可能是关乎我们物种未来、最忙碌的一年。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Vahid Kazemi（机器学习博士）写道，他“几周前”就已离开 xAI，并表示：“在我看来，所有 AI 实验室都在做同样的事，这很无聊。我认为还有更大的创意空间，所以我要开始做点新的。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Hang Gao（从事多模态项目，包括 Grok Imagine）写道：“我今天离开了 xAI。”他称这段经历“非常有价值”，并提到自己对 Grok Imagine 多次发布的贡献，同时称赞团队“谦逊的工匠精神和雄心勃勃的愿景”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Roland Gavrilescu（去年 11 月离职创办 Nuraline）发帖称：“我离开了 xAI，正与其他离开 xAI 的人一起打造新的东西。我们在招聘 :)”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 10 日，Chace Lee（Macrohard 创始团队成员）写道：“短暂重置一下，然后重返前沿。”（Macrohard 是 xAI 旗下的纯 AI 软件项目，目标是利用 Grok 驱动的多 agent 系统，实现软件开发、编码和运维的全自动化；其名字带有对微软的调侃意味。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;xAI 现在员工还是一千多人，所以短期内不太可能因为这波离职就“运转不下去”。但人走得太集中、太快，网上很容易越传越夸张：一些 X 用户甚至干脆跟着玩梗，发帖“我也离开 xAI 了”——明明他们从来没在那儿上过班。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/78/78ddd553b93495da131900d3f6c2b380.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/62/621bc914bc79992e9118b59001fcc9c0.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是 xAI 随后选择 公开视频 all-hands meeting 的直接背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场全员大会上，马斯克反复强调了两个判断。第一，离职并非绩效问题，而是阶段适配问题。第二，在当前阶段，xAI 的唯一优先级只有一个——速度（velocity）和加速度（acceleration）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你在某个技术领域里跑得比所有人都快，那么你最终一定会成为领导者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;xAI 新架构：四大团队，各自干什么？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克在这次大会上，大概说了几件事儿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先他给xAI进行了一个定位：别把它当成熟公司看，毕竟这个公司才成立两年半。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他把 xAI 形容成“幼儿”，强调：“我们还小，但长得特别快”。对手很多都干了五年、十年甚至二十年，起步资源更好、人更多，但 xAI 硬是在短短几年里把不少关键方向做到了前排，甚至拿了“第一”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后他一口气列了几条“成绩单”：语音、图像、视频生成做到了行业领先；他还强调“预测能力”才是衡量智能的关键指标，并说 Grok 4.20 在预测任务上赢过别的模型。应用形态上，xAI 已经把 Grok 和 Imagine 这种能力整合进一个 App，还对 X 做了更激进的改造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们还有一个更野的目标：Grok-pedia 不只是“做个更好的维基百科”，而是要做成“银河百科全书”——把所有知识（包括图像、视频）都装进去，规模和准确性都要上一个数量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后谈到离职和重组，马斯克表示这不是“崩了”，而是“公司长大了”：xAI 已经达到了一个新的规模节点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他用了生命体成长的比喻：公司创业初期几十个人，大家可以随便聊，到几百人就必须有结构；再长大就得“分化出器官、长出四肢，甚至一度还会有尾巴——好在后来尾巴消失了”。所以重组是为了跑得更快。也因此会出现现实情况：有人适合早期冲锋，但不一定适合后期规模化运作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后他公布了新的组织架构，xAI 接下来就按四条线打：&lt;/p&gt;&lt;p&gt;第一，是 Grok Main 和语音，这是核心的 Grok 主模型；&lt;/p&gt;&lt;p&gt;第二，是专门面向编程的模型；&lt;/p&gt;&lt;p&gt;第三，是图像和视频模型，也就是 Imagine；&lt;/p&gt;&lt;p&gt;第四，是 MacroHard，它的目标是对整个公司级系统进行完整的数字化仿真。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/97302dad27f36206b61cb0be96ceb600.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Grok 仍然是 xAI 对外最重要的产品入口；Coding 团队则被放在了一个更加核心的位置，不只是为了“写代码”，而是为了压缩整个软件生产链路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Grok 团队：一年内，Grok 装进了 200 万辆特斯拉&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Grok 团队是 xAI 当前最核心、也最直接面向用户的一条产品线，几乎承载了外界对 xAI 的全部直观认知：聊天、语音、车载、API，以及与 X 平台的深度整合。这条线的负责人是 Aman Madaan（2024 年加入 xAI）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dd/dd0ba2a787266ef16cd385ab81f85815.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果只用一句话概括 Grok 团队这一年的进展，那就是：从“什么都没有”，到成为 xAI 最快落地、规模化最成功的产品线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Aman 用“从零到第一”的方式概括语音线的推进速度：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Grok Main 和语音团队将合并为一个团队。在语音上有一个很典型的例子。2024 年 9 月，OpenAI 已经推出了高级语音模式，而当时我们什么都没有，连模型都没有。但我们是在那之后才开始的，在短短六个月里，我们从零开始、完全自研，在团队里几乎没有音频背景的情况下，做出了一个在六个月内就已经超越 OpenAI 的语音产品。而现在，不到一年，Grok 已经部署在超过 200 万辆特斯拉汽车中，同时我们也推出了 Grok Voice Agent API。&amp;nbsp;一年时间里，我们从“什么都没有”变成了行业领导者。这种事情，只可能发生在像 xAI 这样的地方：小团队、极度投入、使命导向，再加上充足的算力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Grok 主模型上，xAI把重点从“问答”推向“Everything App”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;在聊天模型这条线上也是同样的故事。从 Grok 1.5、Grok 2 到 Grok 3，我们始终站在推理能力的最前沿。&amp;nbsp;我们想要走向一个不只是“问答”的世界，而是打造一个真正的“Everything App”。你可以来这里咨询法律问题、制作幻灯片、解决复杂问题，真正把事情做完。&amp;nbsp;我们的目标，是打造一个入口，让你可以完成所有工作，真正放大每一个人的能力，让他们完成远超个人极限的事情，而且这一切都会通过一个极其简单、自然、无缝的使用体验来实现。&amp;nbsp;未来几个月，知识工作者能够完成的工作量，将出现数量级提升。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;编程（Coding）团队：到今年年底，你可能都不用写代码了&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说 Grok 是 xAI 面向用户的“对话入口”，那 Coding 团队就是整个公司真正的执行引擎。这个团队不仅负责 xAI 内部的编码系统，更承担着一个更激进的使命：让 AI 自行写代码，并最终替代“写代码”这件事本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Coding 团队负责人 Makro 的发言，是这场 all-hands 里最容易让工程师产生情绪波动的一段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;按他的说法，这已经不再是“提效”，而是一条自我加速的链路：这一代 Grok Code 正在训练下一代 Grok Code。等“写代码”变成训练流程的一部分，讨论的重点就不再是工具顺不顺手，而是系统会不会沿着这条路一路跑下去。所以，编程被直接提到公司最高优先级之一。而且投入了等效“百万张 H100”的训练算力，目标是训练出世界上最强的编程模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克的判断更为激进。在他看来，“写代码”本身正在显露出一种过渡形态的特征，最终只需要一句“为这个特定目标生成一个经过优化的二进制程序”，就可以直接绕过传统意义上的编程过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Makro 在会上先从“质变”讲起：模型终于从“看起来能用”变成“真的能用”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;最近这段时间，编程这件事真的发生了很大的变化。&amp;nbsp;以前我一直在吐槽：大家老是劝我用编程模型，我也试过，但说实话，并没有被真正说服。但最近不一样了——这些模型已经能产出相当不错、可用的代码质量了。&amp;nbsp;当然，你还是需要去 review、去给反馈，但已经很容易看出来，它们能把人的效率拉高很多。这已经不只是“帮你写代码”了，而是它们对你的直觉理解得比以前好太多。现在我描述一个问题的时候，只需要像跟一个已经熟悉代码库的工程师同事解释一样去说就行；而以前，你基本得像牵着一个幼儿一样，一步一步教它该怎么改。&amp;nbsp;而且它们不只是写代码，还可以帮你 debug 代码。现在我们会让 Grok Code 连续跑上好几个小时，来确保对训练系统这种更复杂的改动，真的能在生产环境里稳定工作。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Makro 也把 Grok Code 的用途描述为“生产级验证 + 递归自我改进”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;所以对我们来说，这已经不只是“写代码更快一点”、让工程师 10x 更高效的问题了。我们已经清楚地看到：我们正走在一条递归式自我改进的路径上——这一代 Grok Code，正在训练下一代 Grok Code。而且这条路径已经进入指数级起飞阶段，并且会继续下去。&amp;nbsp;正因为如此，我们在公司里全面加码编程方向，把 coding 提升为公司最高优先级之一。&amp;nbsp;如果你对编程感到兴奋，不管你是非常擅长训练模型，还是一名对系统设计感兴趣的底层软件工程师——这里就是你该来的地方。我们现在拥有等效百万张 H100 的训练算力，目标就是训练出世界上最强的编程模型。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Guodong则表示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;随着时间推移，我们越来越清楚地意识到：至少在编程这个维度上，我们正走向某种“奇点”。&amp;nbsp;真正的限制因素，可能已经不在算法或模型上了，而是在计算资源和能源：是否能运行起足够强的模型，去支持和赋能所有人。而现在，通过这次调整，我们已经是一个统一的团队；我们会在算力上取胜，我们正在赢下“太空算力”这条路。&amp;nbsp;所以，对每一位工程师来说——不管你现在是在写内核、写编译器，都可以想一想：这件事是否还值得你亲手去做？ 也许你应该加入我们，在 coding 方向上，多少“自动化掉你自己的一部分”，让自己跑得更快。&amp;nbsp;说实话，这是一个非常疯狂、也非常令人兴奋的年份。真的是“活在这个时代太夸张了”。我已经能清晰地感受到 AGI 的气息——至少在编程这件事上，已经非常接近了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c1bb842437b9166acc6930f3b96379f8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，马斯克在 Coding 段补了一句极具冲击力的判断，把“写代码”本身都当成中间态：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;对，我觉得事情会走到一个阶段——可能甚至今年年底就会到——你都不会再费劲去“写代码”了，AI 会直接把二进制给你生成出来。&amp;nbsp;而且 AI 生成的二进制，效率会比任何编译器做到的都更高。所以你就直接说：“给我一个针对这个具体目标的最优化二进制。” 然后你甚至连传统意义上的编码都绕过了。写代码这一步，其实只是个中间步骤，很可能到……我觉得今年年底左右，就不需要了。&amp;nbsp;而且我们预计，Grok Code 会在两到三个月内达到最先进水平（state of the art）。这一切发生得非常、非常快。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/bd/bd36e008f7a07eddcbc3fca86e63882d.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b9/b9de7210559039975b7d7fe77c972144.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;&amp;nbsp;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Imagine 团队：每天 5000 万条视频，是所有竞争对手的总和&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Imagine 是 xAI 的图像与视频生成产品线，也是公司里算力消耗最大的方向之一。负责人是 Guodong，核心成员包括主攻视频方向的 Haotian，以及Chaitu。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Guodong 在会上把 Imagine 的进展描述为“从零到全面铺开”的速度战：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Imagine 团队几乎是六个月前从零开始的。没有扩散模型代码，没有现成基础，但现在 Imagine 已经全面集成进我们所有产品，包括 X 应用。你现在就可以在 X 里长按图片，直接编辑，或者把图片变成视频。&amp;nbsp;Imagine 的增长速度极其惊人。用户现在每天生成接近 5000 万个视频，过去 30 天里生成了 60 亿张图片。作为对比，Google 最近表示他们的模型 30 天生成了 10 亿张图片，而我们是它的六倍。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Haotian 则把时间表拉到“今年年底”，强调“长视频一键生成 + 无干预”的路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;随着模型能力不断扩展，我们正在构建与现实难以区分的视觉世界。到今年年底，我们很可能会拥有可以一次性生成 10 分钟、20 分钟视频的模型，而且不需要任何中途干预。你只需要给出你的想象力，其余的一切都会由模型和智能体自动完成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，马斯克也补了一句方向性判断，把未来算力押注在“实时视频理解/生成”上：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我的预测是：未来大多数 AI 计算资源都会用在实时视频理解与实时视频生成上，而我们预计会成为这方面的领导者。这里值得再次强调：六个月前，我们在视频与图像生成、编辑方面几乎什么都没有，或者说非常弱；但在六个月内，我们就冲到了第一名。&amp;nbsp;而且我相信，大家会对即将发布的 Grok 4.2 模型印象非常深刻——它是一次显著提升。不过那只是我们新模型体系里的“小版本”。接下来还会有中等版本和大型版本，它们会更加智能。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;MacroHard：AI Agent 的终极实验场&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;MacroHard 被定义为一个由 AI Agent 驱动、用来“模拟公司运转”的方向，目标远不止写代码：它要模拟人类使用计算机，自动运行软件和各类公司流程，甚至进一步做到对整家公司进行仿真。负责人是 Toby，核心成员包括负责执行层推进的 John M.。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Toby 在会上给 MacroHard 的一句话定义非常硬核：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MacroHard 正在构建一个完全能力齐备、数字化、实时的人类模拟器。它能够在计算机上完成任何一个人类能完成的事情，包括使用工程和医学等领域的高级工具。未来应该会出现由 AI 完整设计的火箭发动机。&amp;nbsp;从某种意义上说，这是 AI 目前仍然显著弱于人类的领域之一。也正因为如此，这是最令人兴奋、最值得投入、也最有可能真正改变整个领域的方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;John M. 则把 MacroHard 的路径拆成“CLI → GUI → 端到端编排”：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们正在构建这些强推理模型，而它们将会控制我们的 CLI（命令行界面）。我们每天都在积极使用这些模型，它们给整个团队带来了巨大的生产力提升。我知道语音团队在这方面做得非常出色。&amp;nbsp;这也是为什么我们需要算力——我们需要大规模算力来运行这些模型，从而提升我们自己的生产力。但现实是：全球 80% 到 90%，甚至 95% 的软件世界，都有 GUI（图形界面）。这是一个非常重要的事实。要真正让人们生活更容易，我们必须开发能够在 GUI 上完成日常任务的模型。&amp;nbsp;所以 MacroHard 的目标，是模拟一家“输出是数字化成果”的公司。这是智能体的下一步：MacroHard 将实现跨桌面端的真正端到端编排，并将带来巨大的经济繁荣。&amp;nbsp;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，马斯克把 MacroHard 的意义抬到“人类仿真”的高度：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;MacroHard 这个项目，随着时间推移，可能会成为我们最重要的项目。我们讨论的是：对整家人类公司进行仿真。&amp;nbsp;理论上完全有可能完整仿真任何一家“输出是数字化产物”的公司。这将开启一个繁荣时代，其程度可能是我们现在几乎无法想象的。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;基础设施：xAI 真正的护城河&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 xAI，基础设施不是“后台部门”，而是以上所有激进判断能不能落地的前提。ML 基础设施团队负责搭建公司的训练、推理以及整套工具链系统。用他们自己的话说：站在软件工程师视角，这可能是你能做的最酷的一类系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最典型的例子发生在 Grok 3 的训练上。当时，xAI 已经拿到 10 万张 H100，硬件都交付到位，但软件并没有真正准备好。团队原本以为系统能跑，结果规模一拉到 3 万卡，现实给了一个很明确的反馈：系统跑不起来。问题不是某一个 bug，而是数据中心里的“意外”太多：交换机抖动、链路抖动、交换机宕机、GPU 频繁损坏、数值不稳定……这些都不可能提前枚举完。但目标只有一个：让 10 万张 H100 像一个整体一样工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一次训练 step 可能只有 5 秒：每 5 秒往前推一步，但这 5 秒里什么都可能发生。所以系统必须做到：意外不断出现也能自动恢复、持续推进，而不是一出问题就停下来等人来救。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种问题在别的地方也很难遇到。不是工程师不够聪明，而是很少有人同时拥有这种规模的算力、以及这种密度的人才。当时整个预训练团队大约 15 人，真正负责训练系统的可能只有 7 人，但他们刻意维持了这种“人才密度”，而不是靠堆人数去堆规模，最终靠这支小团队完成了Grok3的训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;英伟达 CEO 黄仁勋在多次采访中说过一句评价：在把 AI 算力上线这件事上，没有人比 xAI 更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随后，RL 与推理团队接力。这个团队负责在地球上、以及很快可能在太空中，大规模运行训练任务和生产推理系统，目标很直接：把系统从 10 万张芯片扩展到数百万张芯片，并且让它对已知和未知的硬件故障都具备韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前他们的成果都汇聚到了孟菲斯的数据中心里：xAI 已经建起了全球规模最大的 AI 训练集群之一，而且仍在扩张——第一阶段是 33 万张 GB300，接下来还将再增加 22 万张 GB300。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9f/9f6cac7ed43c5970050a1985c02069eb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;显然，想要最好的模型，必须有大规模训练算力，这一点是绝对基础。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在这场 all-hands 的最后，马斯克把这个逻辑推到了一个几乎只存在于科幻里的地方。如果地球已经装不下这些算力了，那下一步呢？答案是：月球。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克认为，要真正理解宇宙，最终必须离开地球去探索，而这正是 SpaceX 与 xAI 合并到一起的动机：加速人类理解宇宙的未来，把意识的光延伸到群星之间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9ec2a34f72072e1a538f73ef4f6b0b24.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从能源的角度看，他给了一个非常极端的对比：今天整个人类文明，使用的只是地球可用能量的 1% 左右。而如果人类哪怕只想用到太阳能量的百万分之一，那也是现在文明能耗的一百万倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;问题在于：你在地球上，根本不可能拿到那样的能量。地球在整个太阳系里，只是一粒“极小的尘埃”。太阳占了太阳系 99.8% 的质量，如果不走出地球，你几乎不可能对太阳能量的利用产生任何实质性的提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在他看来，下一步不是“更大的地球数据中心”，而是“离开地球的数据中心”。先把数据中心送上地球轨道；再往后，就把制造和发射搬到月球——在月球上建工厂生产 AI 卫星，再用质量驱动器（mass driver）把它们一颗接一颗“弹射”到深空，把算力扩展到地球根本承载不了的规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aOVnB88Cd1A&quot;&gt;https://www.youtube.com/watch?v=aOVnB88Cd1A&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/&quot;&gt;https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hsoVQfxDa9ofFiMZsDai</link><guid isPermaLink="false">https://www.infoq.cn/article/hsoVQfxDa9ofFiMZsDai</guid><pubDate>Thu, 12 Feb 2026 09:51:56 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>“每给 Claude Code 提一个请求，我就点上一根烟，放松下”</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenFGA核心维护者、Ona（前 Gitpod）软件工程师 Siddhant Khare最近写了一篇博客吐槽了自己在使用AI 编程中的“疲惫感”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他以自身经历指出AI 带来的职业疲惫真实存在且被行业集体回避：单任务变快≠工作变轻松，反而更累，期间工程师任务量膨胀、频繁切换引发深层耗竭；工作角色从创造者转为高消耗的 AI 产出评审者，加之 AI 输出的不确定性打破了工程师熟悉的确定性逻辑，持续带来焦虑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，行业技术迭代过快形成 “FOMO 跑步机”，频繁追新工具造成时间浪费与知识衰减，还易陷入 “prompt 螺旋” 陷阱，长期依赖更会导致独立思考能力退化，社交媒体的高光展示则进一步加剧比较焦虑。他指出，AI 时代工程师的核心能力并非极致使用 AI，而是懂得设边界、及时停止，保护认知资源，追求可持续的长期产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他的博文引发了工程师们的共鸣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“对我来说，这种疲惫感有点不一样，它来自于不断在‘写一点代码 / 做一点工作 / 看一点 review’和“‘停下来等大模型生成结果’之间来回切换。等待的时间是不可预测的，你根本不知道是该继续等，还是该切去做别的事。于是你只能在机器“思考”的时候，随便干点事打发时间。&lt;/p&gt;&lt;p&gt;你永远进不了心流状态，只能时刻盯着后台任务什么时候跑完。这种持续的“警觉等待”会让人特别消耗精力。我并不觉得自己更高效了，反而感觉自己像个偷懒的保姆，只是勉强看着孩子别把自己弄伤而已。”开发者Parpfish跟帖道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我知道这建议听起来既不负责任又很幼稚，但我现在的做法是：每次给 Claude Code 提一个不知道要跑多久的请求，我就点上一根烟，放松一下。有时候我也会切去玩那种随时拿起来、随时放下都不影响的小游戏。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“对我个人来说，编程很多年前就没什么乐趣了，”Parpfish也表示，“但有了 Claude Code 之后，我又重新觉得好玩起来了。虽然感觉不一样，但在我现在这个人生阶段，这样反而更让我享受。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是Siddhant Khare的文章，我们进行了翻译，以飨读者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 疲惫真实存在，但几乎没人谈&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你用 AI 是为了更高效，为什么反而比以前更累？这是每个工程师都得正视的悖论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上个季度，我交付的代码量超过职业生涯任何一个季度；同时，我也比职业生涯任何一个季度都更疲惫。这两件事并不矛盾，甚至高度相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的工作就是搭建 AI agent 的基础设施。我是 OpenFGA（CNCF Incubating）的核心维护者之一；做过用于 agent 授权的 agentic-authz；做过用于上下文去重的 Distill；上线过 MCP servers。我不是偶尔玩玩 AI 的那种人，我在这个领域深扎很久，我写的工具，正被其他工程师拿去把 AI agents 跑进生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但即便如此，我还是撞墙了。那种疲惫不是换一套工具、再优化一点流程就能解决的，而是一种更底层的耗竭感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你是每天都在用 AI 的工程师，做设计评审、生成代码、排查 bug、写文档、做架构决策，并且你发现自己在“AI 时代”反而比以前更累，那这篇文章就是写给你的。你没有在幻想，也不是你不够强。你感受到的是真实存在的东西，只是行业在集体回避它：大家拼命讲效率、讲产出，却不讲代价。一个全职做 agent 基建的人都能在 AI 上 burnout，这件事可能发生在任何人身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我想实话实说。不是那种“AI 太神了，这是我的工作流”的版本，而是真实版本：夜里 11 点，你盯着屏幕，周围堆着一大片 AI 生成的代码还得你去 review，你开始怀疑那个本该帮你省时间的工具，为什么反而吞掉了你整天的时间。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/88c29fe34534eb4c2e9e12e2e49155a5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;没有人提醒过我们的悖论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有件事我曾经想了很久才想明白：AI 的确能让单个任务更快，这不是谎言。以前要 3 小时的事情，现在 45 分钟就能搞定，写设计文档、搭服务骨架、补测试用例、研究不熟的 API……都更快了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我的工作日变得更难了，不是更轻松，而是更难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因一旦看清就很简单，只是我花了几个月才真正意识到：当每个任务变快，你不会做更少的任务，你只会做更多。你的“产能”看起来提升了，于是工作会膨胀来填满它，甚至还会超出。你的领导看到你交付变快了，预期会跟着调整；你看到自己交付变快了，对自己的预期也会跟着调整。基准线被整体抬升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 AI 之前，我可能会用整整一天只专注一个设计问题：在纸上画草图、洗澡时想、出去走走、回来突然清晰。节奏慢，但认知负担可控，一天只扛一个问题，深度专注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在呢？一天可能要摸六个问题。每个问题都“只要一小时，AI 帮你很快搞定”。但在六个问题之间来回切换，对人脑的代价极其昂贵。AI 不会在问题之间疲惫，我会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是悖论：AI 降低了“生产”的成本，却抬高了“协调、评审、决策”的成本，而这些成本几乎全部落在人的身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;你变成了 reviewer，而你从没签过这份合同&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前我的工作流程是：想清楚问题 → 写代码 → 测试 → 发布。我是创造者，是建造者，这也是很多人最初喜欢工程的原因：能亲手把东西做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 之后，我的工作越来越像：写 prompt → 等 → 读输出 → 评估输出 → 判断是否正确 → 判断是否安全 → 判断是否符合架构 → 修不对的部分 → 再 prompt → 再重复。我变成了审稿人、裁判、质检员，站在一条永不停歇的流水线旁边。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一种完全不同的劳动类型。创造会给人能量，评审会消耗能量。相关研究早就指出，“生成型任务”和“评估型任务”在心理体验上截然不同：生成更容易进入心流，评估更容易触发决策疲劳。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我第一次明确意识到这点，是在某一周用 AI 重度开发一个新 microservice 的时候。到周三，我连简单的决定都做不动了：这个 function 该叫什么？无所谓。配置放哪？也无所谓。我的大脑不是因为写代码累，而是因为“判断代码”累，每天一整个时间都在做无数个细小判断，会把你掏空。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更残酷的是：AI 生成的代码，往往比人写的更需要谨慎 review。人写的代码我大致知道对方的习惯、长处、盲点：可信的地方可以快扫，不放心的地方重点看。AI 不一样，每一行都值得怀疑。代码看起来很自信，能编译，甚至能过测试，但可能在极隐蔽的地方错得很深，直到线上、在高压负载下、凌晨三点才爆出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是你只能逐行读。读自己没写过、由一个不了解你代码库历史和团队约定的系统生成出来的代码，是一种非常消耗人的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是为什么我一直觉得 agent 安全和权限这么重要。我们不可能 review AI 产出的所有东西，规模一上来就做不到了，那就必须先在系统层面约束 agent 能做什么：最小权限原则、范围限制 tokens、审计轨迹。你越不需要担心“AI 会不会做出危险动作”，你越能把认知预算留给真正重要的工作。这不仅是安全问题，更是“人能否长期承受”的可持续问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;非确定性问题：AI 破坏了工程师最熟悉的契约&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工程师从业训练的底层假设是确定性：同样的输入，得到同样的输出。它是调试的基础，也是系统推理的基础。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 把这份契约撕了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有个 prompt 周一跑得完美，生成了结构清晰、很干净的 API endpoint。周二我用同样的 prompt 做一个类似 endpoint，输出结构却明显不同，这次错误处理换了套路，还引入了我没要的依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为什么？没有原因。更准确地说，没有我能触达的原因。这里没有“模型今天换了想法”的stack trace，也没有日志告诉你“temperature sampling 走了 B 路径不是 A 路径”。它就是……不一样了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对一个职业生涯建立在“坏了我就能找到为什么”的人来说，这种体验会带来持续的、背景噪音式的焦虑。它不一定戏剧化，却足够磨人。你无法完全信任输出，也无法真正放松，每一次交互都必须保持警惕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我试过对抗，给 prompt 做版本控制，写复杂的 system message，做模板。一部分有用，但都无法解决根本矛盾。你在和一个概率系统协作，而你的大脑天生更擅长确定性系统，这种错位会长期产生低强度压力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为这种挫败感，我后来做了 Distill：为 LLM 做确定性的上下文去重，不调用 LLM，不用 embeddings，也不靠概率启发式，而是用纯算法，在大约 12ms 内把 context 清理干净。我至少想让 AI pipeline 里有一段东西是可推理、可调试、可信的。模型输出再怎么不确定，输入至少要干净、可控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我发现适应得最好的一批工程师，通常已经“和不确定性和解”了。他们把 AI 当作一个聪明但不靠谱的实习生写的初稿，默认要重写其中 30%，并且提前把这部分重写时间算进计划。他们不会因为输出错了而愤怒，因为他们从来没期待它“正确”，只期待它“有用”。这两者差别很大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“FOMO 跑步机”：你永远追不上&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;深呼吸一下，试着只跟上最近几个月的变化：Claude Code 先发 sub-agents，再发 skills，再发 Agent SDK，再发 Claude Cowork；OpenAI 上线 Codex CLI，又上 GPT-5.3-Codex，一个甚至“参与了自我编写”的模型；新的 coding agents 宣布 background mode，可并发上百个 autonomous sessions；Google 推出 Gemini CLI；GitHub 增加 MCP Registry；并购几乎每周发生；Amazon Q Developer 得到 agentic 升级；CrewAI、AutoGen、LangGraph、MetaGPT，随便挑一个 agent framework，每周都冒出新版本；Google 发布 A2A（Agent-to-Agent protocol）对标 Anthropic 的 MCP；OpenAI 发布自己的 Swarm framework；Kimi K2.5 采用 agent swarm 架构，编排 100 个并行 agents；“Vibe coding”成了热词；OpenClaw 上线 skills marketplace，一周之内研究者在 ClawHub 发现 400+ 恶意 agent skills；与此同时 LinkedIn 还会冒出一句话：“2026 年不做 sub-agent orchestration，你就已经过时了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这还不是一年发生的事，是短短几个月，并且我还漏掉了很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也曾深陷其中：周末不断评测新工具，追每一条 changelog，看每一个 demo，拼命留在所谓“前沿”，因为我害怕落后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现实是什么？周六下午我搭起一个新的 AI coding tool，周日形成基本 workflow，而到了下周三，社交网络开始吹另一个“更强”的工具，我就会焦虑；下一个周末又去搭新的，旧的躺着吃灰。从一个 coding assistant 迁到下一个、再迁下一个，最后又回到第一个，每次迁移耗掉我一个周末，换来大概 5% 的提升，而且我还很难测出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这种循环乘以所有类别：coding assistants、聊天界面、agent frameworks、多 agent 编排平台、MCP servers、context 管理工具、prompt 库、swarm 架构、skills marketplace，你会变成一个永远在学习新工具、却从没真正把任何一个工具用深的人。Hacker News 首页就足够让人眩晕：今天是“Show HN：Autonomous Research Swarm”，明天是“Ask HN：AI swarms 怎么协作？”没人知道答案，但大家都在造。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更糟糕的是知识衰减。我在 2025 年初花了两周搭出一套复杂的 prompt 工程流程：精雕 system prompts、few-shot examples、chain-of-thought 模板。它当时非常好用，三个月后模型更新，最佳实践迁移，一半模板反而不如一句简短指令效果好。那两周不是“投资”，而是“消耗”。我的 MCP server 也是：我写了五个自定义 servers（Dev.to 发布、Apple Notes 集成、Python/TypeScript 沙盒等），后来协议演进，GitHub 上线 MCP Registry，突然出现成千上万预制 servers，我的部分工作一夜之间变得可有可无。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent framework 的 churn 更夸张。我见过团队一年内从 LangChain → CrewAI → AutoGen → 自研编排连续迁移。每次迁移都意味着重写集成、重学 API、重建 workflow。那些选择“等等再说”的团队，很多时候反而比早早冲进去、被迫迁两次的人更占便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来我换了策略,不再追每个新工具，而是深挖它们下面的基础设施层。工具会来会走，它们解决的问题不会。context 效率、agent authorization、audit trails、runtime security，这些是跨框架、跨周期的耐久问题，这也是我把 agentic-authz 建在 OpenFGA 上、而不是绑死某个 agent framework 的原因；也是 Distill 做 context 层、而不是 prompt 层的原因：要构建在不那么 churn 的层上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我仍然会密切关注生态，做基础设施的人必须如此。但我关注是为了理解方向，而不是把每个新东西都立刻搬进生产。信息充分和被动反应，是两回事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“再改一版 prompt 就好了”陷阱&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个陷阱非常阴险：你让 AI 生成一个很具体的东西，第一版 70% 是对的；于是你 refine prompt；第二版 75% 的对，但把第一版对的地方弄坏了；第三版 80% 对，但结构又变了；第四次你回过神来，已经 45 分钟过去了，而你自己从头写可能 20 分钟就写完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我叫它 prompt spiral，AI 时代的 yak shaving。你原本有明确目标，半小时后却在调 prompt，而不是写代码。你在优化“给模型的指令”，而不是解决真正的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更危险的是，prompt spiral 会让你产生“我在推进”的错觉。每一轮都有小进步，你会继续投入，但边际收益正在快速递减，你甚至忘了目标从来不是“让 AI 产出完美内容”，而是交付功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在有一条硬规则：三次。三次 prompt 内拿不到 70% 可用的结果，我就自己写，没有例外。这条规则省下的时间，超过我学过的任何 prompt 技巧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;完美主义遇上概率输出：最优秀的人往往最难受&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;工程师倾向完美主义：喜欢干净代码、喜欢测试全绿、喜欢可预测系统。这不是缺点，是我们能做出可靠软件的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 AI 输出从来不是“完美”，永远是“还不错”，大约 70–80%：变量名不对味，错误处理不完整，边界条件没考虑，抽象不符合你的代码库。能跑，但“不对”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对完美主义者来说，这很折磨，因为“差一点对”比“完全错”更糟。完全错，你直接丢掉重来；差一点对，你会花一小时去修修补补。修 AI 输出尤其痛苦，因为你在修“别人做的设计决策”，而这个“别人”并不分享你的审美、你的上下文和你的标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不得不学会放下。不是放下质量，我仍然在乎质量，而是放下“AI 会产出质量”的期待。我现在把每次 AI 输出都当作毛坯、当作起点、当作原材料。它出现的那一刻，我脑子里就贴上“draft”的标签。仅仅是这个心智框架的变化，就让我的挫败感减少了一半。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多在 AI 上最痛苦的工程师，恰恰是最好的工程师：标准最高、细节最敏感、瑕疵一眼就能看出来。AI 奖励的反而是另一种能力：能快速从不完美的输出里榨取价值，而不把情绪绑定在“把它打磨到完美”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;思考能力在萎缩：这才是最让我害怕的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在一次设计评审会上发现了这个问题。有人让我在白板上推一个并发问题，没有电脑、没有 AI，只有我和一支笔，我居然卡住了。不是我不懂概念，我懂，而是我几个月没练这个“肌肉”了。我把“第一轮思考”外包给 AI 太久，导致从零推理的能力在退化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它像 GPS 和导航。没有 GPS 的年代，你会建立城市的心理地图，能自己推路线。用了多年 GPS，你离开它就不会走了，因为这项技能已经萎缩。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 对工程思考也是一样：当你总是先问 AI，你就少了自己挣扎的过程。而学习就发生在挣扎里：困惑是理解成形的地方。跳过它，你会更快拿到输出，但理解会更浅。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我现在刻意让每天的第一个小时不碰 AI：在纸上思考、手绘架构、用慢的方法推问题。它确实低效，但它让我的思考保持锋利，而锋利的思考会在我之后使用 AI 时带来回报。因为你自己的推理被“热身”后，你对 AI 输出的评估会更准确。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;比较陷阱：社交媒体只展示高光，不展示代价&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交媒体上到处都是“看起来已经把 AI 玩明白的人”：晒 workflow、晒产出数据、晒“我两小时用 AI 做完一个 app”。你回头看自己的经历：prompt 失败、时间浪费、生成代码不得不重写，于是你开始怀疑自己是不是哪里不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你没有任何问题。那些帖子是高光剪辑，没人会发：“我花了三小时让 Claude 理解我的数据库 schema，最后放弃，迁移还是手写。”没人会发：“AI 生成的代码线上吞错导致事故。”没人会发：“我很累。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更麻烦的是，AI 技能很难衡量。传统工程里，你看看代码大致能判断水平；AI 输出却受模型、prompt、上下文、temperature、甚至玄学因素影响。别人一个惊艳 demo，很可能在你的机器、你的代码库上复现不出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我后来对 AI 内容变得更挑，我仍然关注这个领域，毕竟是工作，但我更少看“热闹”，更多看“真的在建和在交付的人”。信号和焦虑的比例很重要。如果一个信息流让你更焦虑而不是更清醒，那它就不在为你服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;真正有用的改变是什么&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我具体说说，哪些做法让我的 AI 使用方式从对抗变成可持续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;给 AI 使用设时间盒。 我不再开放式使用 AI。会设定计时器：这件事用 AI 30 分钟，时间到就交付现状或自己写。它同时拦住了 prompt spiral 和完美主义陷阱。把思考时间和执行时间分开。 早上用来思考，下午用来 AI 辅助执行。规则不绝对，但有默认结构，就能确保大脑既锻炼也得到助力。接受 AI 只做到 70%。 我不再追求完美输出，70% 可用就够了，剩下我自己补。这个接受，是我减少 AI 挫败感最有效的一件事。对 hype cycle 保持策略性。 我会跟踪生态，但不再每个新工具一上线就立刻迁移。我只用一个主力 coding assistant，并把它用深。评估新工具看“几个月后的验证”，不看“几天内的热度”。信息充分和被动反应，是两回事。记录 AI 什么时候帮忙、什么时候拖后腿。 我做过两周简单日志：任务、是否用 AI、耗时、满意度。数据非常清晰：AI 在样板代码、文档、测试生成上省了大量时间；在架构决策、复杂调试、需要深代码库上下文的工作上反而耗时。知道这一点后，我更清楚什么时候该用它，什么时候不该用。不再试图 review AI 产出的每一行。 这很难接受，但如果你用 AI 生成大量代码，你不可能以同样严苛的标准逐行审。我的 review 精力集中在最关键的部分：安全边界、数据处理、错误路径；其它交给自动化测试和静态分析。非关键代码有一点粗糙是可以接受的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;可持续性问题：AI 不是治好 burnout，而是在放大它&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;科技行业的 burnout 早在 AI 之前就存在。AI 正在让它更严重，不是因为 AI 很坏，而是因为 AI 移除了曾经保护我们的“自然限速器”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 AI 之前，你一天能产出多少有上限：打字速度、思考速度、查资料的时间。这些限制有时令人沮丧，但它们也是一种“调速器”。工作本身会限制你把自己榨干的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 拿掉了这个调速器。现在唯一的上限是你的认知耐力，而大多数人只有在把这条线冲破之后，才知道自己的极限在哪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 2025 年末 burnout 了。不是戏剧化那种：我没有辞职，也没有崩溃。我只是开始不在乎了。code review 变成走过场，设计决策变成“AI 怎么说就怎么做”。我在机械地产出更多，却感受更少。我花了一个月才意识到发生了什么，又花了一个月才恢复过来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;恢复并不是“少用 AI”，而是“换一种方式用 AI”：设边界、有意图，并且承认我不是机器，我不需要跟机器同速。Working at Ona 让我更清楚看到这一点：当你为企业客户做 AI agent 基础设施，你会看到不可持续的 AI 工作流在规模化之后的“人类成本”。这不是个人问题，而是系统问题，必须在工具层面解决，而不只是靠个人硬扛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;讽刺的是，我最好的几个项目反而诞生在 burnout 期间。当我停止追工具、开始思考到底哪里坏掉时，问题第一次变得清晰：context window 被垃圾填满，这催生了 Distill；agents 拿着全权限 API key，这催生了 agentic-authz；无法审计 agent 做了什么，这正在变成 AgentTrace。疲惫迫使我停止消费、开始建设，不是更快地堆功能，而是更有意识地去做正确的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 时代真正的技能&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我认为 AI 时代最重要的技能不是 prompt engineering，不是知道该用哪个模型，也不是拥有“完美工作流”，而是知道什么时候该停。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;知道 AI 输出什么时候“够用”；知道什么时候该自己写；知道什么时候该合上电脑；知道边际提升不值得继续消耗认知；知道你的大脑是一种有限资源，保护它不是偷懒，而是一种工程能力。&lt;/p&gt;&lt;p&gt;我们做系统会优化可持续性：加熔断、做 backpressure、设计优雅降级。我们也应该对自己做同样的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 是我用过最强的工具，同时也是最消耗人的工具，这两件事可以同时成立。能在这个时代长期活得好的工程师，不会是用 AI 用得最多的人，而会是用得最聪明的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你很累，不是因为你做错了，而是因为这件事本身就很难。工具很新，模式还在形成，行业却装作“更多产出=更多价值”。事实不是这样。可持续的产出才是价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我依旧每天都在这个领域里建agent authorization、context engineering、audit trails、runtime security，让 AI agents 真正在生产环境可运行的基础设施。我对 AI 的投入比以往更深，但我会按自己的节奏、用自己的边界，去做真正重要的事，而不是追逐短暂的趋势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;照顾好你的大脑。它只有一个，而任何 AI 都无法替代它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://siddhantkhare.com/writing/ai-fatigue-is-real&quot;&gt;https://siddhantkhare.com/writing/ai-fatigue-is-real&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PpYEQfkrB25gN5rpPuKr</link><guid isPermaLink="false">https://www.infoq.cn/article/PpYEQfkrB25gN5rpPuKr</guid><pubDate>Thu, 12 Feb 2026 04:00:00 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>宝马、Indeed 和 WHOOP 的降本增效实践：如何在 Lakehouse 上构建分析与 AI 能力 ｜ 技术实践</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据可用性空前提高，企业却发现规模化交付可靠的分析与人工智能解决方案变得前所未有的困难。随着数据湖逐渐成为业务关键型分析与决策的共享基础平台，可靠性、并发处理能力及成本可预测性等方面的挑战迅速显现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开放表格与数据格式部分缓解了这一难题。通过标准化数据存储与访问方式，Apache Iceberg 等格式使机构能够更有效地掌控数据，并为跨引擎的可互操作分析奠定基础。然而，仅靠开放性尚不足以解决复杂的分析困境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着数据分布于多云环境、各类数据目录及工具之中，众多团队依然难以交付符合业务预期的分析成果。性能调优、运维负担与碎片化的安全管理模型，常常横亘在原始数据与可靠洞察之间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如今，越来越多的组织正以提升效率为目标重构其分析架构。将计算能力引向数据所在之处的核心理念，源于对开放存储中单一受治理数据副本的坚持——这使团队能专注于挖掘数据价值，而非反复迁移或复制数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里正孕育着一种全新的技术路径。它基于 Apache Iceberg 这类开放数据表格式构建，同时支持 Delta 等其他格式。Snowflake 将一套为企业关键工作负载设计的强大分析引擎，直接部署于数据原生位置。团队无需再将数据迁移至另一个独立系统，即可在其存储原址处理全量数据，同时确保性能、可靠性与成本可控性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然这一路径在理念上令人振奋，但其在实践中的应用更具价值。本期综述将重点展示三大品牌——BMW Group、Indeed 与 WHOOP——如何运用该方案，在其全域数据资产中驱动分析与人工智能应用，从而将开放数据架构转化为可量化的商业成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从愿景到实践&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Indeed 在扩大自助数据访问规模的同时降低成本 43%&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Indeed 运营着一个 52PB 规模的数据湖，为全公司的关键业务报表、分析与实验提供支持。随着自助式数据访问（即读写 Apache Iceberg™ 表的能力）需求的增长，数据工程团队亟需一种既能扩展分析能力、又避免形成瓶颈的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过将数据湖从 Hive-ORC 架构迁移至 Apache Iceberg，Indeed 采用了与其开放数据战略相契合的“一次写入，随处读取”模式。借助 Snowflake 平台，分析人员能够直接读写 Iceberg 表，同时通过 Horizon 目录保持安全与治理控制，包括列级安全策略和数据脱敏机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在内部测试中，Indeed 发现，相比在同一环境中评估的其他分析引擎，使用 Snowflake 查询 Iceberg 表的成本降低了 43%–74%。这种开源格式、受控访问与高性能分析的结合，使得 Indeed 能够在为规模化构建的湖仓平台上，加速实验探索、产品分析与洞察生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;凭借 Snowflake 对 Apache Iceberg 的原生支持，Indeed 将庞大的数据湖转变为受控的自助分析平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;WHOOP 在提供实时健康洞察的同时大幅削减计算时间&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;WHOOP 每天通过其可穿戴设备分析数十亿条生物特征信号，为会员洞察、产品创新及业务预测提供支持。随着公司业务规模扩大，其需要一种在统一各系统数据的同时、能对敏感健康信息保持严格治理的方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过将数据整合至 Snowflake 平台并采用 Apache Iceberg 技术，WHOOP 在借助 Horizon Catalog 保障数据安全的前提下，简化了数据访问与管理流程。公司发现其新一代 AI/ML 财务预测模型运行速度提升了 3 倍，且通过降低运维复杂性，团队每日可节省 20 小时的计算资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;依托 Snowflake，WHOOP 将数据分析与人工智能转化为竞争优势，实现了更快速的财务预测能力，并为会员提供了更具个性化的体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;宝马集团利用全球数据洞察连接万名用户，同时提升效率 25%&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;宝马集团通过其云数据枢纽（Cloud Data Hub）运营着一个大规模的全球数据环境，整合了集团内制造、服务、供应链及可持续发展等多类业务场景的数据。该平台涵盖15个业务领域、超过6,000个数据集，每月服务用户数超10,000名，在规模化运营中需兼顾架构灵活性与统一治理规范。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为支撑这一“最佳架构”体系，宝马采用 Apache Iceberg 并结合 AWS 原生工具来管理开放、分布式数据；同时，在需要高效可靠分析的场景中集成 Snowflake 平台。Snowflake 为宝马现有数据资产提供了高性能计算能力，可在不干扰既有系统或不必要复制数据的前提下，实现复杂的运营分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一策略已取得显著成效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;宝马集团报告称，在某些服务类数据工作负载上平均节省了25%的成本，并已在Snowflake平台上部署超过60个数据应用场景，帮助各团队更快获取业务洞察，同时确保跨地区、跨工作负载的数据治理一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从复杂走向清晰&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管宝马集团（BMW Group）、Indeed 与 WHOOP 面临的具体挑战各异，但其应对策略背后存在一个共同模式：它们均优先采用将工具引入数据的策略，以保障架构效率，维持统一、开放、受治理的数据基座。向 Apache Iceberg 等开放表格式的转型使这一模式成为可能，这些格式提供了管理大规模数据所需的结构化、一致性与互操作性。而 Snowflake 则在此基础上提供了关键补充：一个能够直接在上述开放数据上运行的可靠分析与AI引擎，其内置功能旨在帮助团队在规模扩展时管理并发与成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些企业并未选择拼接多个计算引擎与治理层，而是通过将 Snowflake 引入其数据环境，与 Snowflake 平台上的既有工作形成互补。它们基于存储在 Snowflake 中、无需移动的开放数据，直接部署了统一且强大的分析引擎，覆盖整个数据资产。这一转变使得它们能够加速创新、简化运营，并在无需重构数据平台的前提下，交付可信的分析与AI能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在上述案例中，三个核心架构原则始终贯穿其中：&lt;/p&gt;&lt;p&gt;就地访问数据：直接在各处数据存储位置进行处理——无论是Iceberg表、Delta表还是Parquet文件，无需迁移或复制数据；实现规模化高性能：在业务量增长时，以稳定可靠的性能支持高并发关键业务负载，确保性能表现可预测；统一分析与人工智能：通过统一的分析平台，赋能全组织各团队优化决策流程，打破数据孤岛。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake 并未取代这些机构的开放架构体系，而是为其数据提供了所需的性能与可靠性，从而化解了开放性与运营稳定性之间的取舍难题。它帮助 WHOOP 达成服务等级协议（SLA）、助力 BMW 降低成本，并提升了 Indeed 数据团队的价值产出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下能力展示各团队如何在不改变数据存储位置的前提下，为开放数据体系引入生产级分析引擎。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;更便捷地运行分析功能与人工智能，无论您的数据位于何处&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在组织探索架构效率的今天，一个共识正逐渐成形：与其在系统间迁移数据，不如将数据作为单一受治理副本留存，并将分析引擎与人工智能引擎部署至数据所在之处。Snowflake 提供统一的引擎与世界一流的平台，助力企业将这些数据转化为可信的分析洞察与人工智能应用。BMW、Indeed 和 WHOOP 等案例展示了不同行业如何借助这一架构，实现更快决策、更强治理控制与更高效运营。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您的数据已准备就绪。现在，是时候让它们发挥价值了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/bringing-ai-analytics-lakehouses/&quot;&gt;https://www.snowflake.com/en/blog/bringing-ai-analytics-lakehouses/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/kwoK6RQOoOvhxfoUHR7d</link><guid isPermaLink="false">https://www.infoq.cn/article/kwoK6RQOoOvhxfoUHR7d</guid><pubDate>Thu, 12 Feb 2026 03:46:32 GMT</pubDate><author>Amit Kapadia</author><category>Snowflake</category><category>数据湖仓</category><category>AI&amp;大模型</category></item><item><title>谷歌为 Gemini 3 Flash 推出 Agentic Vision 功能</title><description>&lt;p&gt;谷歌已为 Gemini 3 Flash &lt;a href=&quot;https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/&quot;&gt;添加智能体视觉（Agentic Vision）功能&lt;/a&gt;&quot;，将视觉推理与代码执行相结合，实现“基于视觉证据的精准回答”。据谷歌介绍，这不仅能提升准确性，更重要的是解锁了全新的 AI 驱动行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单地说，Gemini 3 Flash 不再是一次性分析图像，而是以类似智能体的方式进行视觉调查：规划步骤、操作图像，并在回答问题之前通过代码验证细节。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这形成了一个“思考—&amp;gt;行动—&amp;gt;观察”的循环：模型首先分析提示词和图像，制定多步骤方案；然后生成并执行 Python 代码来操作图像并提取额外信息，如裁剪、缩放、标注或计算；最后将转换后的图像添加到上下文中，再生成新的回答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌表示，这种方法在大多数视觉基准测试中将准确率提升了 5% 至 10%，主要归功于两大因素。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，代码执行允许通过放大图像中的较小视觉元素（如微小文字）进行细粒度检查，而非依赖猜测。Gemini 还能通过绘制边界框和标签来标注图像，从而加强视觉推理能力，例如正确计数物体。谷歌表示，借助此类标注，他们已经解决了手部数字计数这一众所周知的“难题”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，原本需要 AI 模型直接处理的视觉算术和数据可视化任务可以转移给 Python 和 Matplotlib 编写的代码来完成，从而减少基于图像的复杂数学运算可能产生的幻觉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对谷歌的这次发布，X 用户 Kanika &lt;a href=&quot;https://x.com/KanikaBK/status/2016397455773073587?s=20&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;读完这个，再回头看早期的视觉工具，感觉都不完整了。过去存在那么多边缘案例，仅仅是因为模型无法进行视觉干预或验证。智能体视觉感觉像是所有人最终都会采用的方向。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2ay0d9/&quot;&gt;Reddit 用户 Izento 评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这带来的影响是巨大的。本质上，他们为 AI 在实际物理机器人中实现视觉推理带来了可能性。机器人将拥有更强的情境感知和智能体能力。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其他 Reddit 用户指出，&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2az67v/&quot;&gt;ChatGPT 已经通过代码解释器（Code Interpreter）采用类似方法相当长一段时间了&lt;/a&gt;&quot;；尽管如此，它&lt;a href=&quot;https://www.reddit.com/r/singularity/comments/1qpn3bo/comment/o2cy696/&quot;&gt;似乎仍然无法可靠地数清手指头数目&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谷歌的智能体视觉路线图涵盖更多隐式交互行为，如无需明确提示即可自动触发缩放、旋转等操作；新增网络搜索、反向图像搜索等工具，丰富模型可调用的参考依据；并将支持扩展到 Flash 之外的其他 Gemini 系列模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户可通过 Google AI Studio 和 Vertex AI 中的 Gemini API 使用智能体视觉，并已开始以“思考模式（Thinking mode）”在 Gemini 应用中逐步推出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/google-gemini-agentic-vision/&quot;&gt;https://www.infoq.com/news/2026/02/google-gemini-agentic-vision/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/2WMVYZUZHPArVTCUZzc4</link><guid isPermaLink="false">https://www.infoq.cn/article/2WMVYZUZHPArVTCUZzc4</guid><pubDate>Thu, 12 Feb 2026 02:17:39 GMT</pubDate><author>Sergio De Simone</author><category>计算机视觉</category></item><item><title>LinkedIn利用GitHub Actions、CodeQL和Semgrep进行代码扫描</title><description>&lt;p&gt;LinkedIn&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;重新设计了其静态应用安全测试（static application security testing，SAST）流水线&lt;/a&gt;&quot;，以便在基于GitHub的多仓库开发环境中提供统一、可强制执行的代码扫描能力。该举措源于公司的安全左移（shift-left）战略，也就是通过在Pull Request中直接提供快速、可靠且可落地的安全反馈，增强LinkedIn代码与基础设施的安全性，帮助保护用户与客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从宏观层面来看，&lt;a href=&quot;https://en.wikipedia.org/wiki/Static_application_security_testing&quot;&gt;SAST&lt;/a&gt;&quot;指的就是通过分析源代码，在开发生命周期早期识别潜在的漏洞。在LinkedIn的规模下，传统方案依赖多个相互独立的扫描器与定制化集成，导致覆盖度不均、流水线健康状况可见性有限，并给开发者带来额外的负担。此次重新设计旨在标准化扫描能力、简化接入流程，并将安全更深度地嵌入开发者工作流，同时避免引入性能瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在设计初期，LinkedIn工程师确立了核心指导原则，那就是优先以开发者为中心的安全设计，最小化对工作流的干扰；具备可扩展性，允许其他团队添加规则或集成；具备高韧性，避免故障影响开发者；具备可观测性，能够在大规模场景下监控覆盖范围与性能表现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新架构基于&lt;a href=&quot;https://github.com/features/actions&quot;&gt;GitHub Actions&lt;/a&gt;&quot;进行编排，并整合了两款核心扫描引擎&lt;a href=&quot;https://codeql.github.com/&quot;&gt;CodeQL&lt;/a&gt;&quot;与&lt;a href=&quot;https://github.com/semgrep/semgrep&quot;&gt;Semgrep&lt;/a&gt;&quot;，选择它们是因为二者覆盖范围互补且易于扩展。LinkedIn工程师实现了自定义工作流，用于管理规则执行、编排扫描流程并处理扫描结果。所有漏洞发现结果均基于&lt;a href=&quot;https://docs.github.com/en/code-security/reference/code-scanning/sarif-support-for-code-scanning&quot;&gt;SARIF&lt;/a&gt;&quot;标准进行规范化，并补充元数据，为开发者与安全团队提供清晰的修复指引与可落地的上下文信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/25a5bc4a224b0afc10c8173d23fc95bc.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用CodeQL的GitHub Actions工作流的宏观概览&amp;nbsp;(图片来源：&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;LinkedIn博客文章)&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LinkedIn工程师最初希望使用GitHub Required Workflows来强制执行安全流水线，并在数万个仓库中实现定时扫描，但该功能不支持定时任务与自动部署。因此，工作流文件必须被推送到每个仓库才能可靠地传播变更，这在大规模场景下会带来一定的挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决该问题，LinkedIn在每个仓库中部署了轻量级的桩工作流（stub workflow），将实际执行委托给集中维护的中心工作流。这种设计使得扫描逻辑、强制策略与可观测性埋点的更新能够即时生效，无需修改单个仓库。同时，还有一套漂移管理系统（Drift Management System） 持续校验桩工作流的存在与否与配置情况，新仓库也会自动预置该文件。这套组合方案确保了LinkedIn多仓库环境下的统一覆盖与强制执行，在大规模场景下保持可靠性与开发者工作流效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;强制机制通过&lt;a href=&quot;https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/managing-rulesets/about-rulesets&quot;&gt;GitHub仓库规则集（repository rulesets）&lt;/a&gt;&quot;来实现，也就是阻塞Pull Request合并，直到静态分析完成且漏洞处于可接受的风险阈值内。为避免扫描器故障或基础设施异常中断开发者流程，LinkedIn构建了多重安全机制，包括紧急停止开关（kill switches）与自动降级策略。在故障场景下，系统会注入空SARIF报告以解除阻塞的合并请求，同时仍会采集遥测数据用于事后分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/13/13849a0b375ed0e4b0a2684223112856.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阻塞模型的流程（图片来源：&lt;a href=&quot;https://www.linkedin.com/blog/engineering/security/modernizing-linkedins-static-application-security-testing-capabilities&quot;&gt;LinkedIn博客文章)&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如LinkedIn的工程师所强调的：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们从碎片化的生态系统，转向了一套统一、原生基于GitHub的安全流水线，在不拖慢开发者速度的前提下，提供一致的覆盖度与可落地的安全反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该SAST流水线会收集详细的执行指标、故障报告与接入统计数据，使安全团队能够监控覆盖范围、可靠性与组织级影响。LinkedIn指出，SAST只是整体应用安全战略的一部分，该战略还包括依赖项扫描与密钥检测，共同构成一套覆盖代码与基础设施的一体化安全防护体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/linkedin-redesigns-sast-pipeline/&quot;&gt;LinkedIn Leverages GitHub Actions, CodeQL, and Semgrep for Code Scanning&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/8YqVuKOhz6RzKmmxEFje</link><guid isPermaLink="false">https://www.infoq.cn/article/8YqVuKOhz6RzKmmxEFje</guid><pubDate>Thu, 12 Feb 2026 02:15:34 GMT</pubDate><author>作者：Leela Kumili</author><category>后端</category></item><item><title>在遗留规则误拦合法流量后，GitHub重构了分层防御体系</title><description>&lt;p&gt;GitHub的工程师近期排查发现，用户反馈意外出现了“请求过多（Too Many Requests）”错误，其根源在于部分滥用防护规则在触发其生效的安全事件结束后，仍被意外长期启用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.blog/engineering/infrastructure/when-protections-outlive-their-purpose-a-lesson-on-managing-defense-systems-at-scale/&quot;&gt;据GitHub说明&lt;/a&gt;&quot;，受影响的用户并非产生了高流量请求，只是“发起了少量常规请求”，却依然触发了防护机制。经调查，这些早期为应对安全事件制定的规则，其依据的流量特征在当时与滥用行为高度相关，但后续却开始匹配部分未登录用户的合法请求。GitHub表示，这类检测机制结合了行业标准的指纹识别技术与平台专属的业务逻辑，而“多维度信号组合偶尔会产生误判”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GitHub还量化了这套多层级检测信号的实际运行表现。在匹配到可疑指纹的请求中，仅有一小部分会被拦截，只有同时触发业务逻辑规则的请求才会被阻断，这类请求约占指纹匹配请求的0.5%~0.9%；而误判请求在总请求量中的占比则极低，约为每10万次请求仅出现数次。尽管如此，GitHub在博文里强调，该问题对用户造成的影响仍不可接受，并以此次事件揭示了一个更普遍的运维问题：应急防护规则在安全事件发生时通常能正常发挥作用，但随着威胁模式的演变以及合法工具和使用场景的变化，这些规则会逐渐 “失效脱节”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;GitHub此次复盘的核心结论之一就是，多层级防护体系会增加定位故障根本原因的难度。工程师需要跨多层基础设施追踪请求链路，才能确定拦截行为发生在哪个环节，而实际排查的难点在于，每一层基础设施都具备合理的限流、拦截权限，要定位具体是哪一层做出的拦截决策，就必须对多个采用不同数据格式的系统日志进行关联分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7a/7af781e97dcf3edf14fbb5872cbc0fed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：GitHub&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决当下的问题，GitHub对所有防护规则开展了全面复核，对比每条规则当前的拦截范围与最初设计的防护目标，移除了已经没有实际防护意义的规则，同时保留了针对现存威胁的防护措施。从长期来看，GitHub表示正投入资源完善防护规则的生命周期管理体系，打造更完善的跨层级可观测的能力，实现限流与拦截行为的源头追踪；将应急防护规则默认设为临时生效；新增事件后复盘机制，推动应急防护规则向可持续、精准化的解决方案演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d7fdd6a0edcff06aea8221423531ffe9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管GitHub的博文聚焦于规则生命周期管理与跨层级可观测性，但这种采用“纵深防御”架构的请求处理流水线，在其他处理互联网流量的大型平台中也十分常见。例如，&lt;a href=&quot;https://vercel.com/blog/life-of-a-vercel-request-what-happens-when-a-user-presses-enter&quot;&gt;Vercel&lt;/a&gt;&quot;公开的请求处理生命周期中提到，请求会经过其防火墙的多个防护阶段，覆盖网络层（L3）、传输层（L4）和应用层（L7），后续还会针对项目级策略增加Web应用防火墙（WAF）防护环节。Vercel还指出，各防护层级间存在反馈机制，若某条WAF规则触发了持续性拦截动作，上游防护层会提前拦截后续的同类请求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种分层防护的设计并非仅存在于边缘流量管理领域：&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers&quot;&gt;Kubernetes的API服务器安全模型&lt;/a&gt;&quot;也采用了明确的阶段式设计，准入控制器会在身份认证、授权校验完成后，数据持久化之前拦截请求，形成一套结构化的校验链路，后续可在此基础上不断叠加新的策略与安全检查规则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些案例共同揭示了大型系统中一种普遍的设计权衡：多层级防护机制能提升系统的抗风险能力与灵活性，但也会增加防护规则“脱离其设计背景，依然失效存在”的风险。GitHub的此次经历也印证了，纵深防御体系的长期有效性，不仅取决于防护规则的部署层级，更在于随着系统与使用模式的演变，能否清晰把控每条规则的设计初衷、实际影响与生效周期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/github-layered-def/&quot;&gt;GitHub Reworks Layered Defenses After Legacy Protections Block Legitimate Traffic&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fpVPsSlLV5XCmB9Hl7ag</link><guid isPermaLink="false">https://www.infoq.cn/article/fpVPsSlLV5XCmB9Hl7ag</guid><pubDate>Thu, 12 Feb 2026 02:12:28 GMT</pubDate><author>作者： Matt Foster</author><category>软件工程</category></item><item><title>从告警疲劳到代理辅助的智能可观测性</title><description>&lt;p&gt;如果你曾经值过班，那么你肯定知道这么一种情况。凌晨两点，电话打来，你猛然惊醒，抓起笔记本电脑开始排查。你先是检查服务仪表板，接着是分析依赖关系图，然后是查看日志，最后再查看来自三个不同监控工具的指标。半小时后你发现，这不过是虚惊一场：阈值设置过严，金丝雀部署触发了可以自动恢复的告警，或是网络瞬态波动导致了短暂峰值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你不能就这样回去睡觉。你得等等，观察一下。你必须确保告警窗口干净利落地关闭了，而且没有任何其他告警被触发。等你确信问题已经真正解决时，你已经失去了一小时的睡眠时间，更大的问题是几乎睡意全无。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种场景在各地运维团队中屡见不鲜。我们不断地调整告警阈值，试图找到一个完美的平衡点：过于敏感会被误报淹没，过于宽松则会错失真实事件。这种动态变化会导致告警疲劳——工程师被大量无需处理的告警淹没。随着时间推移，这会削弱人们对告警的信任度，并降低对真实问题的响应速度。&lt;a href=&quot;https://www.atlassian.com/incident-management/on-call/alert-fatigue&quot;&gt;关于告警疲劳的研究表明，这种响应迟滞现象普遍存在&lt;/a&gt;&quot;：安全监控领域的调查发现，超过半数的告警属于误报，而IT运维领域也呈现出类似的模式。这并非配置问题，而是监控复杂分布式系统时面临的一个根本性挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了优化告警规则，团队经常要花费无数个小时，他们应该这样做。但根本问题仍然存在：我们需要监控的范围超出了我们手动维护和解释的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;我们不愿谈论的监控悖论&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于现代系统，一个现实是它们会一直不停地增长。每增加一个新功能都会有更多的日志需要解析，更多的指标需要跟踪，更多的仪表板需要维护。最初简洁明了的架构与直观的监控，逐渐演变成了一个需要持续投入精力的庞大的生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着系统的增长，维护负担也在增加。仅仅是保持可观测性基础设施的更新就需要花费团队大量的时间。新服务需要配置监控工具，仪表盘需要持续更新，流量模式变化时需要调整告警阈值。依赖关系不断变化，监控方案也必须随之调整。虽然这些工作都是例行公事，却也不可或缺，它们消耗了本可用于开发功能或提升可靠性的宝贵时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个典型的微服务架构会产生巨量的遥测数据：来自数十个服务的日志，来自数百个容器的指标，跨多个系统的跟踪信息。当事件发生时，工程师会面临一个相关性问题：哪些信号至关重要？它们如何关联？近期发生的哪些变化可能解释这种行为？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI队友加入&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;初次接触可观测性AI代理的概念时，我是持怀疑态度的。这听起来像是供应商炒作与流行词汇的结合体。但随着技术的日趋成熟，早期应用方案的陆续出现，其潜力正变得越来越清晰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键转变在于将这些系统视为队友而非替代者。具体而言，这些队友擅长处理事件响应中人类厌烦的那些环节：在海量数据集中进行模式匹配，记住以往的每一个事件，并在周二凌晨两点保持警觉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能可观测性意味着你的监控系统不仅仅是收集指标和触发告警，还要能理解它所看到的信息。它能够：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;留意不符合常规模式的现象：不仅仅是超出阈值，还有行为中那些微妙的变化，在情况变得严重之前发现事情不对劲。串联技术栈中的各个点，将数据库延迟峰值与那些认证错误和六小时前的部署关联起来。生成有实际用处的错误信息摘要。想象一下，提供这样的错误信息，“在下午2:15部署后，认证服务延迟增加了200%；与新的Redis连接池配置相关”，而不仅仅是“错误率超过阈值”。记住制度性知识。每一个事件都能教会可观测性代理一些东西。关于缓存的那个奇怪问题？代理会记住你是如何修复的，并在下次出现类似的问题时提供建议。在规定的范围内采取行动。在适当的监督下，智能可观测性可以执行你基于定义好的策略预先批准的安全补救步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;传统监控与这种方法的区别在于：一个只发出告警，而另一个会分析告警的含义。传统监控告诉你有东西越过了阈值。代理辅助的可观测性会帮助解释发生了什么变化，可能与什么相关，以及接下来应该查看什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;生产环境中究竟发生了什么&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向智能可观测性的转变改变了工程工作开展的方式。在每次处理事件时，工程师不用首先花二十分钟手动在仪表板上关联日志和指标，而是可以审查AI生成的事件摘要，其中链接了部署时间、错误模式和基础设施变化。事件工单自动填充了上下文信息。根因分析现在从一个清晰的假设开始，而不像以前那样需要先进行广泛的调查。工程师仍然需要做出决定，但他们是基于分析数据而不是原始信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这节省了时间，减少了认知负荷，让你最好的工程师们可以把更多的时间投入到功能建设上，而不是用在救火上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;实际路径&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你正在考虑采用智能可观测性，下面是一个分阶段采用的步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第一阶段：只读学习&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，将现有的遥测数据（日志、追踪信息、指标等）输入到一个处于观察模式的智能代理中，它会分析实时和历史数据，学习模式并标记异常，但不触发告警或执行操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个阶段的目标是建立信任。团队看到智能代理提供了合理的建议。你捕捉到了本来可能会错过的异常。工程师们开始在深入分析日志之前查看智能代理生成的摘要信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第二阶段：启用上下文感知分析&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个阶段关乎让智能代理理解你特有的环境，并利用这些知识进行智能调查。它有两个协同运作的关键组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;添加操作上下文&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;向智能代理提供专有知识：运行手册、服务所有权文档、架构图、依赖关系图和过往事件报告。这些信息将智能代理从一个通用的模式匹配器转变为一个理解特定系统的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，当它检测到异常时，它有一个上下文。它不再简单地显示“检测到高错误率”，而是能具体说明：“（通信团队负责的）通知服务出现高错误率。该服务依赖于邮件网关和消息队列。最近部署记录：v1.8.2版本于3小时前部署”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;启用智能关联&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有了这个上下文，智能代理现在可以关联日志、指标和追踪信息中的相关信号。它将模式与过去的事件做匹配，并根据系统的实际拓扑和历史信息提出调查路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面这个例子是一个成熟的智能代理生成的分析结果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0e/0e5c6223c5639870e3df05401485d03e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能代理不做决定。相反，它正在做工程师手工操作通常需要20分钟才能完成的仪表板跳转、日志搜索及其他相关工作。它提供了一个连贯的叙事和可操作的调查步骤。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;第三阶段：根据运营经验定义自动化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在观察和咨询模式下运行智能代理几周后，你会发现某些规律。有些事件会重复发生。特定的诊断步骤会反复出现。有些补救措施简单直接且风险低。这时，你就可以定义哪些工作流程在什么条件下可以自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;关键是从实际的操作经验出发，而不是理论。查看事件历史并问这样的问题：我们反复采取了哪些行动？哪些是安全且可预测的？哪些可以在低风险时段自动运行？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;常见的自动化选项包括：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;重启不健康的Pod或容器，它们未能通过健康检查运行标准诊断脚本，收集分析数据流量高峰时段，在预设的边界内扩展资源在发生异常时触发日志收集或性能分析&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但自动化需要有所限制。在启用任何自动化操作之前都应定义清晰的策略：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;何时可以自动运行？也许只在非高峰时段，或者最初只针对非关键服务，或者从不在部署窗口或主版本发布期间运行。什么需要升级？高严重性事件、面向客户的服务或代理的置信度低于某个阈值时，必须始终由人工介入处理。什么需要审计？每个自动化操作都应该记录其背后的逻辑依据、触发操作的上下文和操作结果。这有助于建立责任机制，并随着时间推移不断完善自动化规则。谁可以控制或暂停自动化？需要有一个简单的方法，让工程师可以在需要时（如维护、测试或敏感时期）禁用自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从一两个低风险的自动化开始，观察它们在一周或两周内的表现。随着信心的建立和规则的完善，逐渐增加更多的自动化操作。我们的目标不是无人操作，而是要消除重复的劳动，使团队可以专注于需要人类判断的复杂问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;集成的真实情形&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可能不需要替换任何东西。&lt;a href=&quot;https://landscape.cncf.io/&quot;&gt;大多数智能可观测性平台&lt;/a&gt;&quot;都集成了现有的监控和可观测性工具。无论是使用开源解决方案还是商业平台，智能代理通常都可以与你当前的技术栈协同工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可以将其视为是在现有基础设施之上增加一个智能层，而不是推倒重来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;当它正常工作时&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在管理平台可靠性和观察团队如何应对监控挑战时，你会发现某些特定的模式。随着组织尝试使用智能可观测性系统，往往会出现类似下面这样的改进：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事件解决速度更快（例如，“我们的事件解决时间从平均45分钟减少到18分钟，而这仅用了三个月”）。提升值班生活质量（例如，“我现在可以一觉睡到天亮。智能代理处理日常事务，只有在遇到需要人类判断的事情时才会叫醒我”）。提升学习便利性（例如，“每个事件都会增加制度性知识。团队的新成员可以要求智能代理：‘告诉我过去的五次数据库事件是什么以及如何解决的’”）。提高主动捕获能力（例如，“在问题变成事件之前就发现并修复了它们。这种转变可能令人感到陌生，因为团队正从被动地响应事件转向主动地预防。”）。工程时间从调试转移到分析（例如，“工程师花在浏览日志上的时间减少了，花在分析模式和验证修复上的时间增加了。这切实提升了运维效率。团队从救火模式转变为真正地改善系统”）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;缺点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是实践中通常会遇到的几个挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI不会在第一天就神奇地理解了你的整个系统。智能代理需要时间来学习正常的模式应该是什么样子，所以早期的建议可能会偏离目标。它可能会做出不相关的关联，或提供明显没用的建议。经过数周的学习之后，洞察力才会真正变得有价值。设置上下文比你想象的更耗时。听起来，向智能代理提供运行手册、架构文档和专有知识很简单，但从中可以看出有多少关键信息只存在于人们的头脑中或过时的文档里。预计要实实在在花些时间来组织和上传这些上下文。真实存在的学习曲线。团队需要了解如何配置、信任和验证智能代理的行为，并为此预留时间。文化阻碍。会有一些工程师不信任AI，也会有一些人担心工作保障。直面这个问题，明确说明增强团队能力与人员替代之间的区别。调试调试器比调试系统本身更难。当智能代理判断错误时，问题可能出在信号、上下文和学习模式的组合方式上，而不是任何单一的指标或日志文件中。这会降低透明度，因此可解释性很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一种用于评估智能可观测性的简单检查方法&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果不确定智能可观测性是否适合你，那么可以问下你的团队下面这些问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在事件处理期间，我们是否反复运行相同的诊断命令？我们是否花费了大量的时间来关联多个工具之间的信号？误报是否导致我们错过了真正的问题？如果我们的初级工程师能够即时访问高级工程师积累的事件知识，他们是否能更快地响应事件，降低风险并减少混乱？我们是否花费了更多的时间灭火而不是预防？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果有两个或更多问题的答案是“是”，你将从中获益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;未来展望&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;系统变得越来越复杂，数据量在不断增加，停机成本变得越来越高，而人脑并没有变得更大或更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能可观测性的目标不是要取代工程师，而是为他们赋能：大规模地识别模式，保留以往事件的知识，并在毫秒（而非分钟）内对信息作出反应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从小处开始，逐步建立信任。让系统自己证明自己。可靠性的未来不是人类也不是AI，而是拥有AI的人类，使他们在工作中的表现更好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也许，只是也许，我们都可以多睡一会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;免责声明：本文所表达的观点和意见仅代表作者个人立场，不代表其雇主机构的观点、政策或实践。所有示例和建议均基于行业普遍做法及个人经验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/agent-assisted-intelligent-observability/&quot;&gt;https://www.infoq.com/articles/agent-assisted-intelligent-observability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6UIpbWJ5TWjcIyJmMAPE</link><guid isPermaLink="false">https://www.infoq.cn/article/6UIpbWJ5TWjcIyJmMAPE</guid><pubDate>Thu, 12 Feb 2026 02:09:59 GMT</pubDate><author>作者：Rohit Dhawan</author><category>可观测</category></item><item><title>如何使用Durable Objects处理响应和进行中的请求</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存是工程师优化分布式系统时首先采用的工具之一。我们会缓存已完成的响应（如数据库查询结果或HTTP响应体），以避免重复执行昂贵的任务。然而，传统缓存未能解决一个经常被忽视的低效源头，即重复的进行中请求（duplicate in-flight request）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当多个客户端几乎同时请求同一资源时，缓存未命中会触发相同的计算会并行开始执行。在单进程的JavaScript应用中，通常会通过在内存中存储进行中请求的Promise来缓解该问题，使后续调用者可以等待同一个结果。在其他语言和运行时中，可通过不同的并发原语实现类似效果，但底层假设是相同的，也就是共享内存和单一执行上下文。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在分布式、无服务器或边缘环境中，这一假设就难以成立了。每个实例都有自己的内存，任何形式的进行中请求去重都仅限于单个进程的生命周期和范围之内。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;工程师通常会在缓存之外引入第二种机制，比如，锁、标记（marker）或协调记录（coordination record），以跟踪正在进行的工作。这些方法难以进行推理，并且经常退化为轮询或粗粒度的同步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文提出了一个不同的模型，那就是将已完成的响应和进行中的请求视为同一缓存条目的两种状态。借助Cloudflare Workers和Durable Objects，我们可以为每个缓存键分配一个单一、权威的所有者。该所有者可以安全地持有正在进行工作的内存表示，允许并发调用者等待它，然后在工作完成后将条目转换为已缓存的响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该模式并没有引入单独的协调层，而是将缓存和进行中请求去重统一在单一抽象之后。尽管它依赖于并非普遍可用的运行时特性，但在支持按键单例执行的环境中提供了一种简洁且实用的方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深入分析该问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从宏观层面来看，问题不在于缓存本身，而是在于缓存条目存在之前发生了什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;考虑一项代价高昂的操作：数据库查询、外部API调用或CPU密集型计算。在分布式边缘环境中，多个客户端可能在非常短的时间窗口内请求同一资源。如果缓存尚未包含该键的值，每个请求都会独立触发相同的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因为边缘运行时会有意地进行水平扩展，这些请求通常由不同的执行上下文处理。每个上下文都观察到相同的缓存未命中，并且会继续执行，就好像它是第一个请求者一样。结果就是冗余的工作激增，而缓存本应防止这种情况发生，但由于缓存只有在第一个请求完成后才能发挥作用，因此这种现象难以避免。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为应对措施，很多系统引入了额外的机制来跟踪进行中的工作。一个缓存用于已完成的结果，而另一个结构（有时是内存映射，有时是分布式存储）用于标记请求为“进行中（in-flight）”。这种分离迅速增加了复杂性。请求的生命周期现在必须在两个独立系统之间协调，必须仔细处理竞争条件、失败和超时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按照进程进行内存去重部分缓解了这个问题，但是这种方法仅限于单个运行时实例的范围内。在无服务器和边缘环境中，实例的生命周期很短，并且在设计上是隔离的。两个同时请求不同节点的请求即使它们在逻辑上是相同的，也无法共享彼此的进行中状态。随着流量增长或地理分布更广泛，这种优化的效果迅速降低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它造成的结果就是，在单体或长期存活服务中表现良好的模式，但在水平扩展最激烈的环境中却崩溃了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缓存结果的缺失和第一次计算完成之间的这个时间差恰好是传统缓存策略无能为力的地方，也是进行中请求去重在分布式运行时中变得既必要又特别困难的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为何Durable Objects是合适的方案？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上一节所述的困难是现代无服务器和边缘平台设计的直接结果。隔离的执行上下文、短暂的进程和水平扩展都是它们的特性，而非缺陷。因此，任何针对进行中请求去重的解决方案都必须在这些约束下工作，而不是试图绕过它们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare &lt;a href=&quot;https://developers.cloudflare.com/durable-objects/&quot;&gt;Durable Objects&lt;/a&gt;&quot;提供了一套小但关键的保障措施，使得这一点成为可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，Durable Object 实例是一个按键存在的单例（per-key singleton）。对于给定的对象标识符，所有请求都路由到同一个逻辑实例，无论它们来自哪里。这立即消除了所有权的模糊性：对于给定的缓存键，只有一个地方可以存放进行中的状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，Durable Objects提供了跨请求共享的可变内存。与传统Worker 不同，Durable Object可以在请求之间保留内存状态。这允许它持有正在进行的工作的表述，例如一个正在进行的计算，而无需外部协调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三，对Durable Object的请求是按顺序处理的。这种串行执行模型消除了在检查或更新进行中状态时需要显式锁定的需求。检查是否已经有计算正在进行中，如果没有的话，则创建它，并且它能够附加额外的等待者，这些都可以在单一执行上下文中确定性地进行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合这些特性，Durable Object能够充当进行中和已完成缓存条目的权威所有者。调用者不再需要询问“这个请求是否已经在其他地方开始了？”，而是简单地将请求转发给负责该键的对象并等待结果即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;重要的是，这种能力不是支持最终一致性的键值存储可以模拟的。KV系统非常适合持久化已完成的结果，但它们无法在避免使用轮询或外部信号的情况下，表示执行中的过程或允许多个调用者等待同一块内存中的操作。相比之下，Durable Objects对进行中工作的支持成为了首要的关注点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不意味着Durable Objects在所有情况下都适用。本文描述的模式依赖于它们的单例和内存保证，因此只适用于提供类似语义的运行时。在这些保证能够达成的环境中，Durable Objects提供了一个干净且最小的基础，可以统一缓存和进行中请求去重，而无需引入额外的协调层。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;突破Cloudflare的适用性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管本文中的示例使用了&lt;a href=&quot;https://developers.cloudflare.com/workers/&quot;&gt;Cloudflare Workers&lt;/a&gt;&quot;和Durable Objects，但底层模式并不是特定于Cloudflare的。重要的不是平台本身，而是上述的运行时保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运行时至少要提供：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按键单例执行：给定键的所有请求都路由到同一逻辑实例。该实例在请求间能够共享内存状态。串行请求处理，或等效的无需显式锁定的保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudflare Durable Objects明确满足这些要求，使它们成为一个方便且定义明确的示例。其他环境中也可以找到类似语义的功能，尽管通常以不同的名称或具有不同的权衡：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于Actor的系统，比如基于&lt;a href=&quot;https://akka.io/&quot;&gt;Akka&lt;/a&gt;&quot;或&lt;a href=&quot;https://dotnet.github.io/orleans/&quot;&gt;Orleans&lt;/a&gt;&quot;构建的系统，通过Actor身份和消息串行提供可类比的保证。在这些系统中，Actor可以自然地拥有给定键的进行中工作和已缓存的结果。有状态无服务器平台和“持久执行”模型也开始出现，不过其API和功能保证差异显著。它们的共同点是，并非所有无服务器计算都必须是无状态的，有限且范围明确的状态可简化某些协调问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，那些仅提供无状态函数并结合最终一致性键值存储的平台无法整洁地实现这一模式。没有单一权威所有者和共享内存执行上下文，进行中去重不可避免地退化为轮询或分布式锁。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，本文所述模式应理解为依赖于运行时。它并不是传统缓存的通用替代方案，而是在执行模型支持的情况下才可行的针对性技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一个最小化的实现&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在运行时保证确立后，实现本身就特别简单了。我们的目标不是构建一个通用的缓存，而是展示一个单一抽象如何同时处理进行中请求的去重和响应缓存。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下示例展示了负责单个缓存键 Durable Object。该键的所有请求都路由到同一对象实例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;export class CacheObject {
  private inflight?: Promise&lt;response&gt;;
  private cached?: Response;


  async fetch(request: Request): Promise&lt;response&gt; {
    // Fast path: return cached response if it exists
    if (this.cached) {
      return this.cached.clone();
    }


    // If no computation is in-flight, start one
    if (!this.inflight) {
      this.inflight = this.compute().then((response) =&amp;gt; {
        // Store completed response
        this.cached = response.clone();
        // Clear in-flight state
        this.inflight = undefined;
        return response;
      });
    }


    // Await the same in-flight computation
    return (await this.inflight).clone();
  }


  private async compute(): Promise&lt;response&gt; {
    // Placeholder for an expensive operation
    // e.g. database query or external API call
    const data = await fetch(&quot;https://example.com/expensive&quot;).then(r =&amp;gt; r.text());
    return new Response(data, { status: 200 });
  }
}
&lt;/response&gt;&lt;/response&gt;&lt;/response&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该对象维护两种状态：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;inflight，表示正在进行的计算。cached，存储可用的已完成响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当请求到达时，对象首先检查是否存在缓存的响应。如果没有，它会检查是否已经有计算正在进行中。如果有的话，调用者只需等待同一个Promise。如果没有，对象就会启动计算，并将结果Promise存储在内存中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于Durable Objects按顺序处理请求，无需显式锁或原子操作。检查和创建进行中Promise的逻辑在单一执行上下文中能够确定性地执行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从调用者的角度来看，这就像一个普通的缓存一样。不同之处在于，即使缓存最初是空的，并发调用者也不会触发重复的工作。一旦计算完成，所有等待的调用者都会收到相同的结果，后续请求直接从缓存响应中提供服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此示例有意省略了持久化、过期和错误处理。这些问题可以在不改变核心思想的情况下分层进行处理。比如，可选择将已完成的响应存储在键值存储中以实现持久性，但关键是，进行中的状态永远不会离开内存，从而保持了该模式的简洁性和正确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;该方法的优势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该模式的主要优势是将两个相关的关注点合并为单一抽象。它不将进行中请求去重和响应缓存视为独立的问题，而是将其建模为同一缓存条目的不同状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这会带来多项实际的优势：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，它消除了缓存无法发挥作用的情况下的重复工作。通过允许多个并发调用者等待同一个正在进行的计算，系统避免了在缓存未命中期间出现的冗余请求激增，这正是传统缓存最无能为力的场景。其次，该方法简化了系统设计。没有必要引入第二个协调层、分布式锁、与缓存数据分开存储的“进行中”标记。所有与请求合并、执行和结果重用相关的逻辑都集中在一个地方，由单一的运行时实体拥有。第三，它与JavaScript应用的编写方式自然对齐。等待共享Promise是惯用且易于理解的模式，Durable Objects使此模型可扩展到单个进程之外，而不必改变思维模型。调用者与缓存的交互，就像在本地进行一样，尽管它的执行是分布式的。第四，该模式可水平扩展而不丧失正确性。随着流量增长或地理分布更广泛，请求仍路由到每个键的同一权威所有者。行为不会随着更多边缘节点的添加而退化，这与按进程优化的常见情况不同。最后，该模型可增量扩展。过期策略、已完成响应的持久化、指标和重试均可添加，而不改变核心控制流。基本思想就是，一个所有者、一个进行中请求的计算、一个缓存结果的思路保持不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些特性使该模式适用于重复工作成本高且请求并发不可预测的工作负载，如边缘API、聚合端点或昂贵的上游集成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;权衡与限制&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管优雅，此模式并非普遍适用。其有效性在很大程度上取决于底层运行时的执行模型，并引入了需仔细考虑的权衡项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最显著的限制是运行时依赖。进行中请求的去重需要具有共享内存状态的单一权威所有者。如果没有按键单例执行，就无法整洁地实现该模式。如果试图使用最终一致性的键值存储来模拟它，必然会导致轮询、分布式锁或其他形式的协调，从而破坏原有的简洁性。实现本身可能是平平无奇的。虽然最小化示例很小，但生产就绪版本必须考虑错误传播、重试、超时、驱逐和内存限制。必须小心确保失败的计算不会使系统处于永久的“进行中”状态，并且缓存的响应能够正确失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个重要的考虑因素是相关性。在许多架构良好的系统中，重复的进行中请求已经很少见了。幂等的上游API、自然地请求分散或粗粒度的缓存可能使进行中请求的去重变得无关紧要。在这些情况下，引入该模式可能会增加复杂性，而不会带来有价值的好处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还有一个就是扩展方面的权衡。将给定键的所有请求路由到单一所有者引入了一个自然的序列化点。对于单个键非常热门的工作负载，这可能会成为瓶颈。在这些情况下，分片策略或替代缓存方法可能更合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，该模式并不替代传统的缓存策略。它是对它们的补充。已完成的响应可能仍需要持久化在键值存储或HTTP缓存中，以在进程驱逐或冷启动时生存下来。然而，关键在于，持久化应该只适用于已完成的结果，将进行中的状态移入外部存储会抵消该方法的好处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，该模式应被视为一种针对性的优化，而非默认的架构选择。如果运行时支持它并且工作负载证明它是合理的，那么统一响应缓存和进行中请求去重可以显著减少冗余工作。当这些条件不满足时，更简单的设计通常会更明智。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文概述了一个在分布式JavaScript运行时中统一响应缓存和进行中请求去重的模式：通过依赖于按键单例执行和共享内存状态，可以将正在进行的计算及其最终结果视为同一缓存条目的两种状态，从而消除重复性的工作，而无需引入轮询或外部协调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;需要强调的是，这种模式主要是一种设计提案，而不是经过实战验证的方案。虽然底层原语（Durable Objects、Promise和串行执行）是众所周知的，但这里描述的组合尚未在生产系统中得到广泛验证。关于运维行为、可观测性和长期性能特征的问题仍然存在，并且需要进一步探索。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尽管如此，该模式的价值可能在于它清晰地揭示了缓存与执行之间的关系。它表明，进行中请求去重的困难并非分布式系统所固有的，而是我们通常使用的执行模型所致。当运行时能够为每个键提供单一的权威所有者时，问题就大大简化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着无服务器和边缘平台的不断发展，有状态执行模型变得越来越普遍。这样的模式表明，重新审视长期以来的假设，比如，缓存和协调之间的严格分离，可能会产生更简单、更具表现力的设计。无论这种特定方法是否证明了广泛的实用性，还是仅仅作为一种小众优化存在，它都凸显了未来运行时和应用架构的重要方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/durable-objects-handle-inflight-requests/&lt;/p&gt;</description><link>https://www.infoq.cn/article/espgcPmH8wYJiLBkvI41</link><guid isPermaLink="false">https://www.infoq.cn/article/espgcPmH8wYJiLBkvI41</guid><pubDate>Thu, 12 Feb 2026 02:07:10 GMT</pubDate><author>作者：Gabor Koos</author><category>软件工程</category></item><item><title>3000亿美元因Agent一夜蒸发！纳德拉、MongoDB CEO等宣告：传统SaaS已走到拐点</title><description>&lt;p&gt;上周，SaaS、数据和软件类投资公司的市值蒸发了约3000亿美元。这并非因为盈利不及预期或宏观经济冲击，而是因为一款人工智能产品发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场危机已经持续数月。等到市场做出反应时，IGV软件指数已较9月下旬的峰值下跌了约30%。上周发生变化的不是方向，而是速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几家根基深厚的企业软件公司股价在一天之内大幅下跌。Salesforce、ServiceNow、Adobe 和 Workday 的股价均下跌约 7%。Intuit 的股价更是暴跌近 11%。与此同时，整个行业的估值倍数也急剧下降。软件公司的平均预期市盈率在短短几个月内从约 39 倍暴跌至约 21 倍。做空者已通过押注传统 SaaS 业务在 2026 年获利超过 200 亿美元，并且还在加倍下注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除非核心假设被打破，否则市场不会抹去如此巨大的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刚刚打破的假设是传统 SaaS 增长模式的可持续性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;纳德拉判断 SaaS 已死&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去二十年的大部分时间里，企业软件受益于异常稳定的经济形势。软件开发成本高昂，转换成本也很高，数据都存储在专有系统中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一旦某个平台成为记录系统，它便会一直占据这一位置。这种信念支撑着从公开市场估值倍数到私募股权收购再到私募信贷承销等方方面面。经常性收入被视为可预测性的指标。合同被认为具有粘性。现金流被认为具有韧性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能现在正在同时测试该逻辑的每个部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周令投资者感到恐慌的并非人工智能能够生成更优质的功能。软件公司多年来一直在功能竞争中生存下来。真正的变化在于，现代人工智能系统能够直接取代大部分人类工作流程。研究、分析、起草、核对和协调不再需要局限于单一应用程序，它们可以跨系统自主执行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Social Capital创始人、知名风险投资家 &amp;amp; 企业家 Chamath Palihapitiya&amp;nbsp;在 X 上发帖，直言不讳地描述了引发此次抛售的情绪：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“SaaS 大崩溃已经开始，而且没有回头路了……一种全新的以人工智能为导向的工作流程即将到来……SaaS 大崩溃已经开始。”&amp;nbsp;简而言之，那种依赖高增长、却长期低盈利甚至无盈利的SaaS发展路径，正在失去市场信任。&amp;nbsp;核心矛盾集中在两方面：短期来看，增长是否真正可持续？长期来看，在人工智能浪潮冲击下，盈利可能性是否正变得渺茫？&amp;nbsp;过去，几乎每家SaaS公司都向投资者与员工描绘过同样的蓝图：先以速度抢占市场，未来再兑现丰厚利润。然而，随着AI技术快速发展，这一逻辑基础可能已被颠覆。眼下最关键的命题是：许多SaaS企业的增长，是否会迅速被成本更低、以AI为核心的新解决方案取代？&amp;nbsp;如果你是一家依赖风险投资、产品仍基于传统“启发式算法+API+增删改查”模式的SaaS初创公司，那么你需要警惕——一套以AI为导向的新工作流程，可能正在瞄准你的市场。&amp;nbsp;对此，私募市场投资者已率先做出反应。他们意识到，继续为短期增长注入资金很可能得不到相应回报。公开市场的投资者同样转变了预期，不再相信长期盈利的故事，转而寻找更具韧性的领域。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ea/eaf013f045c16afdbd442d5dffde6ca6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在创立 Social Capital 之前，Chamath 是 Facebook 高级管理团队的早期成员，领导开发和推出推动公司全球增长的新平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Chamath还强调，以上种种都标志着一场深刻的变化：过去15年中盛行的那套风险投资与估值逻辑，正在被重新校准。下图所呈现的趋势，正是这一转变的直观体现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/09/09d3c7a62045409e61679fee4a8f06e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Chamath 的这一判断早在一年前，微软 CEO 纳德拉已经给出同样的判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一年前，微软纳德拉发出“SaaS 已死”的言论后在网上引发轩然大波。随后他做客了一档访谈栏目，详细解释了他为什么会这样说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在访谈中，纳德拉表示，在他看来，每一次真正的平台迁移，都会带来核心应用架构的根本性变革。回顾历史，从关系型数据库诞生开始，人们首次清晰地实现了数据层与应用程序的逻辑分离。在此之前，应用与数据库往往紧耦合，而关系型数据库的出现，通过引入关系代数和SQL，将数据独立为通用层，从而允许在其之上灵活构建业务逻辑。他继续说道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“随着网络等平台的出现，人们不断探索新的应用架构和业务逻辑组织方式。当前，我们正面临一场规模相当、甚至更大的变革，这次变革的核心在于应用逻辑本身。其关键在于，未来的智能体将不再受限于任何单一的SaaS应用及其私有数据。我们将进入一个以‘智能体为中心’的视角，由任务和意图驱动。智能体将能够跨越多个SaaS应用，对业务逻辑进行协调与编排。它们通过调用各类API工具，实现跨系统操作。更具体地说，我们可以在智能体层对模型进行训练，使其理解并驾驭多个SaaS应用。这是未来明确的发展方向。”&amp;nbsp;因此，当前的SaaS应用，其本质可被视为一个集成了大量定制化业务逻辑的‘增删改查’数据库。未来的变革在于，这个承载核心数据的‘数据库’层，其调用与编排将从原有SaaS应用的封闭业务逻辑中解放出来，成为一个更独立、更通用的可编排层。&amp;nbsp;以我个人的工作流为例：我只需向Copilot提出‘销售情况’的指令，它便能自动查询动态CRM系统获取客户信息，同时从Office 365中提取相关数据，整合后生成报告并与团队成员共享。我无需登录任何一个独立系统。过去，尽管每个企业都部署了CRM，但实际使用率很低，因为访问流程繁琐。现在，由于智能体的存在，查询CRM数据变得异常简便，因为它与其他所有工作流智能体无缝协同。这正是变革所在。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早在 Agent 还未大规模走到企业应用的2025年，纳德拉已经预测到，未来企业在招聘时，雇佣的将不仅仅是个人，更是其所携带的、由智能体构成的“工作流生态系统”。这可以理解为一组相互协作的智能体集群。一个恰当的类比是：今天招聘数据分析师，实际上是雇佣了“分析师及其所构建的电子表格库”；未来，员工将携带其“个人智能体工具篮”加入工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，当时这种趋势已现端倪。例如，针对存储在SharePoint中的大量领导团队会议文档与核心数据，我已经可以训练一个专属的SharePoint智能体，随时进行自然语言查询，无需跳转至独立界面。这极大地提升了效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;两周前，纳德拉再次做客一一档访谈栏目，聊到了AI时代的的商业革命：SaaS、OpenAI与微软将走向何方？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;纳德拉强调，下一代SaaS企业必须主动拥抱智能体。它们需要将智能体深度集成，甚至作为一等公民开放给Copilot等平台，并据此革新自身的商业模式。这不仅是一个巨大的市场机会，更是对任何现有SaaS公司（无论其宣称的“护城河”有多宽）的强大竞争向量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SaaS“崩溃”背后，实为市场重心迁移&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高盛近期的一项研究预测，到本十年末，人工智能代理将显著扩大整个软件市场，并攫取不成比例的利润份额。在他们的框架下，代理不仅仅是增强应用程序的功能，它们本身就成为了工作界面。到2030年，超过60%的软件经济效益可能会通过 Agent 系统而非传统的 SaaS 服务实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/db/db7ec0a0cae6ba01635679155aa9f496.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件行业的利润池预计将转向人工智能代理。来源：高盛、Gartner&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是关键区别。市场正在增长，而不是萎缩。但随着智能、内存和执行能力从静态应用程序转移到跨工具运行的自适应系统中，传统软件的经济效益正在被削弱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;换句话说，企业并非在软件本身上花费更少，而是在许可证费用上花费更少，在最终成果上花费更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变既解释了抛售潮，也解释了其中的机遇。当利润池的流动速度超过收入的减少速度时，公开市场会立即做出反应，而私募市场则会随后跟进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些影响在私募股权和私募信贷领域尤为显著。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去十年，大量资金涌入软件行业，其背后基于一系列共同的假设：可预测的收入、低客户流失率和高回收价值。这些假设使得高杠杆和契约结构成为合理选择，并将软件行业的现金流视为经济中最安全的现金流之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人工智能不会在一夜之间摧毁这些投资组合。它会造成滞后效应。支出压缩先于客户流失出现。利润率下降先于违约显现。经济现实与报告的指标存在差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;和纳德拉、Chamath 等大佬对 SaaS 有着同样观点的还包括MongoDB CEO&amp;nbsp;CJ Desai。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;MongoDB CEO：产品终将被替代，平台才能长青&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，在《No Priors》播客首次现场录制中，主持人Sarah Guo与这位软件开发者出身的掌舵者展开深度对话，共同剖析了为何全球纯软件业务营收超百亿美元的公司屈指可数。对此，CJ Desai给出的答案是：产品终将被替代，而平台方能长青。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：自2022年以来，软件的未来被打上了问号。这不仅来自投资者群体，也来自客户。这是软件栈的一个非常关键的转折点。当你审视软件栈时，你会问：什么东西是必然会存在的？&lt;/p&gt;&lt;p&gt;今天有多少纯粹的软件公司营收能超过一百亿美元？个位数而已。为什么？软件行业历史悠久，由许许多多像你这样聪明的人创立。为何营收超过百亿的公司寥寥无几？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：因为真正的平台是稀有的。速度至关重要。当技术转型发生时，你是否在以最快的速度构建？你是否在那次技术转变中不断学习？无论是互联网时代、AI时代还是移动时代，你是否在快速转向？你必须保持领先。一旦落后，投资者或客户总会问你那个问题：贵公司的未来在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：CJ Desai，你曾在那些平台型企业和基础设施公司工作，最近成为了MongoDB的CEO。我觉得我们刚才谈到的一个问题，每个投资者都会问你，科技生态圈里的每个人也都在思考：当你可以生成一堆软件时，软件的价值究竟何在？我很想听听你的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：这是个很犀利的问题开场，我喜欢，能确保大家都清醒着。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当我们思考技术转型，无论是互联网时代、大型机时代，还是现在的AI时代，你必须真正想清楚这里的本质是什么。无论创建什么应用，比如SaaS应用诞生于90年代末（我记得Salesforce最近刚庆祝了25周年），所以从转型角度看，SaaS至少存在了25年。现在有了AI，问题就变成了：软件的未来是什么？技术栈是什么？一家公司是否真的拥有护城河？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些人会说，他们的护城河是良好的客户关系或出色的渠道，并以此为基础进行内部颠覆。但从我的角度来看，速度至关重要。当技术转型发生时，你是否在以最快的速度构建并从中学习？无论是互联网时代、AI时代，还是2010年代初Meta向移动端的转型，你是否在快速转向？如果你能快速转向以利用技术，无论平台如何变迁，我认为都没问题。关键是你必须保持领先。一旦落后，投资者或客户总会问你那个问题：贵公司的未来在哪里？你必须走在最前沿。并非每次押注都会成功。但在我看来，那种认为某些软件的终端价值为零的极端观点，是夸大其词的。我们会一起找到答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你职业生涯的一部分是在ServiceNow领导产品，它曾被认为是最具韧性的企业软件公司之一——至少在不久前大家还这么认为，现在这个问题有待讨论了。对于许多具有工程思维、考虑购买开发者工具或使用开发者基础设施的人来说，“客户粘性”或“分销渠道作为护城河”这类词感觉很抽象。你能以ServiceNow为例，谈谈它对其客户为何如此重要，以及你的看法吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：有一点是：平台具有粘性，产品则没有。无论你今天在AI时代，还是过去创建的任何软件公司，产品都是可以被替代的。我在ServiceNow时的招聘经理Frank Slootman常说“工具是给傻瓜用的”——如果你的软件只是个“工具”，那可不是好兆头。所以，第一，产品可以被替代，因为软件市场是颠覆性的。因此，你必须确保你拥有的是一个呈现给客户的“平台”，无论客户是旧金山正在为一个新用例创建全新公司的开发者，还是一家向大型银行销售的大型企业。当你将自己定位为一个平台时，或许会拉长你的销售周期，但这意味着客户做出了深思熟虑的决定，平台因此具有粘性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二点，这可能与初创公司和风投圈的普遍建议不同。很多人谈论需要一个“楔子”（切入点）。对ServiceNow来说，服务台可能就是那个楔子。这是对历史的错误描述吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：很多人谈论需要一个“楔子”（切入点）。对ServiceNow来说，服务台可能就是那个楔子。这是对历史的错误描述吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：你需要一个初始用例，而且必须是杀手级用例。因为当你面对一家大银行、医疗保健公司或制造企业时，你需要展示一种颠覆性的方式来处理法律、财务或服务台用例。这是你的切入点。但问题是，如果你能轻易进入，退出也同样容易，因为他们还没有围绕你构建生态系统。这就是关键所在。今天它可能帮你从0做到1亿、10亿，但从100亿到500亿，再到100亿以上，会越来越难。今天有多少纯粹的软件公司营收超过一百亿美元？个位数而已。为什么？软件行业历史悠久，由许多聪明人创立。为何只有个位数公司营收超过百亿？因为平台是稀有的。平台是稀有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，一家软件公司的梦想或抱负应该是成为一个平台。一旦成为平台，就意味着你至少有两个或以上的产品被客户使用，并且这些产品能协同工作，从技术角度看真正具有粘性。更进一步，我认为客户需要将你的平台与他们现有系统做的所有集成也非常关键。记住，如果你面对一家大银行，有些银行已有100多年历史；大保险公司、医疗保健公司也是如此。财富10强、100强、500强公司才是市场所在（TAM）。如果你只提供一个产品，最终会遇到天花板，然后不得不增加更多东西。如果是平台，你的产品彼此协作，并与客户的所有系统集成，这就非常粘性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个具体例子，我曾代表MongoDB与一家银行交谈。他们在MongoDB上运行商业银行应用，构建了许多集成，完成了所有安全检查、治理等等。我问：“你们在MongoDB上构建了多少应用？”他们说非常重要。我问：“多少？”最终伦敦的CTO告诉我：“300个。”我说：“哇，太好了。300个应用构建在MongoDB上。”他说：“CJ，别担心。谢谢你来到伦敦。我们不会换掉的。”我问：“我能问问分母是多少吗？我知道分子是300。”他说：“9000。”我说9000，这对MongoDB是个巨大的机会。他笑着说他不打算换。所以，他们用得越多，我们就越粘性，我们就越融入他们基础设施的肌理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;氛围编程与按需应用的威胁&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：现在有些构建者、购买者和投资者持有一种观点：随着氛围编程和代码生成的兴起，那9000个应用中的一部分，将能按需或为每个公司以特定方式创建。你对此有何看法？如果这样，像那家银行或任何客户，都能得到他们真正想要的东西，而不再需要水平化、更标准化，甚至垂直化的应用了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：但银行在技术上有巨大的预算，对吧？你用了A、B、C等氛围编程平台，创建了一个应用，这很好，所以你的应用开发速度提高了。但问题来了，你仍然需要市场进入渠道，你如何接触银行？你真正具有颠覆性的方式是什么？然后银行会问你：我们和监管机构打交道的次数比和客户、供应商多得多。这能行吗？能通过我们的监管测试吗？我们需要高可用性。什么？你只建在AWS上，GCP用不了？我需要多云高可用性。我甚至需要为这个银行应用在内部部署，或者用个更专业的词，一个“气隙网络”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些才是企业级应用需要满足的要求，而大市场（TAM）就在那里。所以，这可能会限制你。这真是一个能去医疗公司或公共部门联邦客户那里销售的企业级应用吗？因此，氛围编程能让你快速创建应用，你有一个好用例，有颠覆性想法，这很棒。但从市场进入角度看，你需要做很多事才能突破，通过他们所有的检查、治理、安全审计等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;剖析SaaS看空论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：投资者现在将焦点放在模型层而非应用层，或者他们对SaaS应用感到焦虑。他们对数据基础设施也感到焦虑，因为构建应用的方式仍在快速演变。他们还对AI原生公司感到焦虑，担心所有价值最终都集中在模型里。我感觉目前整体上是一个焦虑的投资者环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：这就是看空论点，对吧？这是目前的主流观点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那么，改变我对这些假设的看法吧。你对未来应用仍将保持价值的方式，哪些方面更有信心？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：自2022年，特别是ChatGPT在秋季推出以来，我认为那是一个非常重要的转折点。现在三年多过去了，我从未见过这种情况，因为之前相当长一段时间都很平静。软件的未来确实存疑，这来自投资者群体，也来自客户，他们都在问该用X还是Y。这绝对是软件栈的一个关键转折点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后你审视软件栈，会问：什么东西是必然会存在的？大型语言模型（LLM）在可预见的未来将会存在，当你真正构建依赖这个栈的AI应用时。甚至你已经看到很多创新，比如XAI，它不知从何而来，但表现非常出色。但那个栈，在智能体软件框架中，是恒定的。数据层也必须存在，因为你需要把数据存在某处。这是第二个恒量。其他一切，都会演变。你最好展示出真正的价值，无论你使用平台类比还是什么，无论是技术栈的顶层，你真正理解保险行业的用例，并正在为保险行业构建一个AI原生公司。保险行业有大量用例。你可以说，请从旧的SaaS X 迁移到新的Y，这个新Y实现价值的速度会非常快，而且我们将始终保持领先，因为有了AI，过去用旧SAS无法实现的事情现在成为可能。所以，除了LLM层和数据层之外，顶层的、聚焦用例的部分，将始终至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：MongoDB的客户从初创公司、个人开发者一直到财富10强。你从这些最大公司的购买者和构建者那里，听到他们关于AI价值的真实看法是什么？他们现在对什么兴奋，对什么怀疑？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CJ Desai：我感觉如果一周没和至少10个客户交流，那这周就完全失败了。这需要很多准备和跟进，但通常至少有10个。所以我不断获取这些数据点，尝试进行模式匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我想说的第一点是，当你想到财富500强、全球2000强公司时，他们中的一些进展仍然不快。很多人尝试了办公生产力类型的copilot，但不清楚他们从中获得了多少价值。反馈不太好。关于编码辅助的反馈，在2024年有了显著突破，非常积极。2024年和2025年初始于GitHub Copilot，然后是其他一些工具，一直到Anthropic等等。所以从我的角度看，2025年是编码辅助的突破年，而且仍在继续。我从客户那里得到非常积极的反馈。然后人们还在摸索客服支持领域。他们想，如果是一家大型电信公司或医疗保健公司，客服能完全由AI原生公司处理吗？还没到那一步。他们正在切入初始用例，这很好。我指的是端到端的客户体验。而这些客户问我的问题是关于SaaS的：我应该把这个AI原生客服公司看作“补充”还是“替代”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5nCbHsCG334&quot;&gt;https://www.youtube.com/watch?v=5nCbHsCG334&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qQAhK2EWw4I&quot;&gt;https://www.youtube.com/watch?v=qQAhK2EWw4I&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://x.com/chamath/status/2014044948660887981&quot;&gt;https://x.com/chamath/status/2014044948660887981&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.forbes.com/sites/donmuir/2026/02/04/300-billion-evaporated-the-saaspocalypse-has-begun/&quot;&gt;https://www.forbes.com/sites/donmuir/2026/02/04/300-billion-evaporated-the-saaspocalypse-has-begun/&lt;/a&gt;&quot;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GuqAUv4UKXo&quot;&gt;https://www.youtube.com/watch?v=GuqAUv4UKXo&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/6ROkojXXxzqvZvBq2VHB</link><guid isPermaLink="false">https://www.infoq.cn/article/6ROkojXXxzqvZvBq2VHB</guid><pubDate>Thu, 12 Feb 2026 00:00:00 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>ChatGPT的第一块广告位，被谁买走了？OpenAI：别骂，我们这次所有底线都招了</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ChatGPT 无广告体验的日子要结束了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;经过数周的预热，刚刚，OpenAI宣布，将正式开始在其AI平台测试广告，ChatGPT用户可能很快就会在对话中看到广告。这些广告会以标注“赞助”的链接形式出现在ChatGPT回答底部，但OpenAI表示，广告不会影响ChatGPT给出的回答内容，在视觉上也会区分开来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，广告仅对免费版ChatGPT用户以及每月8美元的低价订阅服务Go套餐用户展示，Plus、Pro、商业版、企业版和教育版用户不会看到任何广告。也就是说，想要避开广告的用户至少需要每月支付20美元订阅Plus套餐。OpenAI提到，免费版用户要想退出广告，但代价是每日免费对话次数减少；Go套餐用户无法选择退出广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://uploader.shimo.im/f/2bKQWp6d3KB8gasG.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3NzA4MDQzMzUsImZpbGVHVUlEIjoiS2xrS3ZteFl6cENnbVhxZCIsImlhdCI6MTc3MDgwNDAzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwicGFhIjoiYWxsOmFsbDoiLCJ1c2VySWQiOjUxNzIzNzI1fQ.p13l2Z-_MU334JoSfqO9fDApNfEl33BiVbOB7URaies&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位接近OpenAI的消息人士表示，OpenAI预计从长远来看，广告收入占比将低于其总收入的一半。目前，该公司还通过其聊天机器人集成的购物功能，从用户购买的商品中抽取分成。另据外媒报道，OpenAI首席执行官Sam Altman告诉员工，ChatGPT“月增长率已恢复到10%以上”，将于本周部署“更新后的聊天模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;ChatGPT 广告规则全曝光&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次广告功能推出前，Anthropic在一则广告中暗讽了ChatGPT的广告模式。例如在其中一则广告里，一名年轻男子向人工智能求助练出六块腹肌，化身私人教练的 AI 先是为他提供指导，随后却开始推销一款虚构的增高鞋垫。之后，Anthropic修改了广告标语，改为：“广告自有其时间与场合，但你和AI的对话不该是其中之一。”而这很快引发了 Altman的不满，他称其“明显不诚实”，是在用“欺骗性广告去批评那些并不存在、理论上的欺骗性广告”，与他们实际的广告模式不符。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就在OpenAI最新一期的播客里，OpenAI 的广告负责人之一 Asad Awan 详细介绍了其AI产品中的广告制定决策。“一方面走‘清高路线’，不做广告，但同时限制使用额度、用能力较弱的模型；另一方面，拥抱广告模式。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场对话中，不少外界关注的核心问题都摆到了台面上，且给出了清晰的回答。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从用户角度看，为什么要做广告？为什么是现在？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“这回到了我们的使命，让 AGI 惠及全人类。”Awan解释道，当一款消费级产品有超过 8 亿用户时，如何把最好的产品带给每个人？广告是经过验证的成熟模式，对于一家希望把最好的 AI 带给全人类的公司来说，这是很自然的选择。提供最好的模型、更高的使用限额，让广告对用户和企业都真正有用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表示，大家担心广告可能带来负面影响，所以OpenAI 更看重广告的原则：为平台设立极高的标准，让广告真正有用。其确立了几条核心原则：&lt;/p&gt;&lt;p&gt;第一，模型回答与广告完全独立，无论视觉上还是模型训练、系统逻辑上，确保回答始终可信，整个产品都建立在信任之上。第二，对话是隐私的。敏感对话绝不会出现广告，对话内容绝不会共享给广告主。我们会在内部匹配合适的广告，但广告主看不到用户对话。第三，透明与可控。用户能清楚理解数据如何使用，并且可以自主控制。第四，激励机制以用户价值为中心。我们不追求用户在平台上的停留时长，一个真正有用的广告就足够了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“简单来说就是，用广告普及 AI，同时严防各种负面问题，从一开始就明确原则，持续测试、改进。”Awan说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;广告的出现频率是怎样的？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“最高原则是：有没有有用的广告可以展示。”Awan称，核心原则是：是否有用、有帮助、对用户的操作有补充，能不能展示优质商品，内容质量要高、广告质量要高、相关性要高。“如果没有，我们宁可一条都不展示。实际上在测试阶段，你会看到广告非常少，因为我们态度很保守，也在学习该在什么位置插入。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，模型看不到广告，也有防护措施。“狂轰滥炸广告对用户、对商家都没好处。我们不想让广告主乱花钱买曝光，也不想让用户看一堆广告，我们只想展示那条正确的广告。作为顶尖 AI 公司，这正是我们能做好的事。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;当公司有专门的广告收入部门时，还会把模型和广告之间这堵墙砌死吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“只要目标和激励机制是成为最值得信任的 AI，我们就不会走偏。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Awan强调，“我们的核心业务是信任。对 C 端用户，是提供可信的优质回答；对企业客户，信任更是一切，你把最重要的数据交给我们，我们必须守住。如果我们真的想成为你最贴身的智能助手，就必须让你放心分享最重要的信息，并且知道它会被妥善对待。我们的商业模式就是 信任，这和很多只做一次性查询、内容推荐的产品完全不同。对我们而言，信任不是可选项，而是必需品。我们希望被用户记住的，就是‘可信’。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;如何回应拿广告这件事开玩笑的竞争对手？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“不同公司使命不同。”Awan表示，OpenAI的业务场景更多元：企业业务、订阅业务、海量免费用户业务。企业版没有广告，订阅版没有广告，广告是为了支撑免费用户业务。“如果你的使命不是普惠 AI，那不做广告很合理；但我们的使命就是在各类场景里落地，让所有人用上最好的 AI。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他强调，如果只服务付费用户，当然可以说 “我们不用做广告”。但OpenAI的愿景不是抽象的，而是非常实在：AI 如何真正帮助普通人。“如果走精英路线，只有付得起钱的人才能用 AI，那从价值观上我们就不认同。我们的立场非常明确：每个人都应该用上最好的 AI。而且我们不是一家纯广告公司，纯广告公司的激励机制和我们完全不同。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;十年后的广告会变成什么样？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据Awan透露，下一步会是更真实的对话式广告，能真正了解产品是什么。再往后，AI 可以在后台自动聚合最优折扣、最划算的商品、最合适的版本。一边是用户主动搜索，一边是商家希望被合适的人发现，两边匹配起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来会更加智能体化。我们先从现有形式开始，把体验做好，让它更相关、可控、可理解、可信。随着主产品和系统进化，广告形态也会一起进化。”Awan表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;第一批投放广告的公司露面了&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ChatGPT 推出广告之际，正值人工智能行业竞争压力不断加剧，且外界对大型 AI 平台的可持续盈利模式抱有更高期待。OpenAI表示，“ChatGPT 被数亿人用于学习、工作和日常决策。为了确保免费版和 Go 版的快速稳定运行，我们需要投入大量基础设施和持续资金。广告收入有助于资助这些工作，从而通过更高质量的免费和低成本选项，让更多人能够使用人工智能，并使我们能够不断提升所提供的智能和功能。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，该公司称，广告商只会获得汇总的广告浏览量和点击量数据，不会获取用户的ChatGPT对话个性化数据或内容。免费版和Go版用户都可以对广告反馈、关闭广告个性化设置、关闭基于历史对话的广告推荐，从而限制赞助内容的推送方式并删除广告相关数据。此外，OpenAI现在并非对所有用户和对话都会投放广告，比如18 岁以下用户以及涉及健康、心理健康、政治等特定敏感话题的对话场景。即便免费版与 Go 版的成年用户，也未必会立即看到广告，因为该功能仍处于测试阶段。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://uploader.shimo.im/f/vAnwSrozg2s8yFXP.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJleHAiOjE3NzA4MDQzMzUsImZpbGVHVUlEIjoiS2xrS3ZteFl6cENnbVhxZCIsImlhdCI6MTc3MDgwNDAzNSwiaXNzIjoidXBsb2FkZXJfYWNjZXNzX3Jlc291cmNlIiwicGFhIjoiYWxsOmFsbDoiLCJ1c2VySWQiOjUxNzIzNzI1fQ.p13l2Z-_MU334JoSfqO9fDApNfEl33BiVbOB7URaies&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管此举在ChatGPT用户和行业观察人士中引发了褒贬不一的反应，但 OpenAI 坚称，投放广告是为了补贴免费及低价服务的使用成本。该公司强调，“此次测试的重点是学习。我们会密切关注反馈，以确保广告在推广之前能够实用且自然地融入ChatGPT体验。”早期用户的反馈将有助于改进广告，并可能在未来扩大广告投放范围。OpenAI 表示，将利用此次试点项目的洞察，更好地平衡盈利与用户体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前，已有多家公司也透露了他们将如何投放 ChatGPT 广告。Adobe表示，将率先投放 Acrobat Studio 和 Firefly 相关广告作为初期试点。已经与 ChatGPT 完成集成的Target，则会在用户提出诸如“有哪些台面式厨电能让日常做饭更方便？”这类问题时展示广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而随着测试的推进，OpenAI 的做法很可能会影响其他人工智能公司对盈利模式的思考，以及广告在对话式 AI 工具中的角色。不过，开发 Claude AI 助手的 Anthropic 已 “承诺” 永远不会加入广告，甚至还在投放了的一系列超级碗广告里宣传这一决定。据报道， OpenAI 的竞争对手谷歌也曾暗示，其 Gemini AI 平台可能会在 2026 年投放广告，不过谷歌 DeepMind 首席执行官 Demis Hassabis 在 1 月底表示，Gemini “没有计划”投放广告。目前，谷歌已在搜索结果旁边的 AI 概览中投放广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在未来某一天，或许所有免费的AI应用和服务都会出现广告，但至少目前，ChatGPT是唯一一家率先推出广告的大型应用和服务商。微软Copilot之前的版本（当时名为Bing Chat）也曾出现过广告，但目前的版本似乎已经取消了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/testing-ads-in-chatgpt/&quot;&gt;https://openai.com/index/testing-ads-in-chatgpt/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/2026/02/09/sam-altman-touts-chatgpt-growth-as-openai-nears-100-billion-funding.html&quot;&gt;https://www.cnbc.com/2026/02/09/sam-altman-touts-chatgpt-growth-as-openai-nears-100-billion-funding.html&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2agJo3Jf_O4&quot;&gt;https://www.youtube.com/watch?v=2agJo3Jf_O4&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Y6SK6Mu5t4gmCk5mhxvk</link><guid isPermaLink="false">https://www.infoq.cn/article/Y6SK6Mu5t4gmCk5mhxvk</guid><pubDate>Wed, 11 Feb 2026 10:01:56 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>传字节今年要造10万颗推理芯片，1600 亿预算砸向AI！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，据两位知情人士透露，字节跳动正研发AI芯片，并与三星电子洽谈代工生产事宜。知情人士称，字节跳动目标是在3 月底前获得芯片样片。其中一位消息源及另一位相关人士表示，该芯片专为AI 推理任务设计，公司计划今年至少生产10 万颗，并有望逐步将产能提升至35 万颗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位消息源指出，与三星的谈判还包括获取存储芯片供应。在全球 AI 基础设施建设热潮下，存储芯片供应极度紧缺，这也让这笔合作更具吸引力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;字节跳动发言人在一份声明中表示，有关其自研芯片项目的信息不准确，但未做进一步说明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;若推进顺利，此举将成为字节跳动的一个里程碑。该公司长期以来一直希望研发芯片以支撑自身 AI 业务，其芯片相关布局最早可追溯至 2022 年，当时便已开始大规模招聘芯片领域人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该芯片项目代号为 SeedChip，是字节跳动全面加码 AI 研发的一部分。 从芯片到大语言模型，公司押注这项技术将彻底改造其涵盖短视频、电商、企业云服务的业务版图。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;字节跳动于 2023 年成立 Seed 部门，专注研发 AI 大模型并推动其落地应用。据一位消息人士透露，字节跳动今年计划在 AI 相关采购上投入 超过 1600 亿元人民币（约 220 亿美元），其中超过一半用于采购英伟达芯片（包括 H200）以及推进自研芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据第四位知情会议内容的人士称，字节跳动高管赵祺在 1 月的全员大会上向员工表示，公司的 AI 投入将惠及所有业务部门。赵祺目前负责字节跳动的豆包聊天机器人及其海外版本 Dola。他坦言，公司的 AI 大模型仍落后于 OpenAI 等全球领先者，但承诺今年将继续大力支持 AI 研发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.reuters.com/world/asia-pacific/bytedance-developing-ai-chip-manufacturing-talks-with-samsung-sources-say-2026-02-11/&quot;&gt;https://www.reuters.com/world/asia-pacific/bytedance-developing-ai-chip-manufacturing-talks-with-samsung-sources-say-2026-02-11/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AradpbWZZoiWVmehvBLB</link><guid isPermaLink="false">https://www.infoq.cn/article/AradpbWZZoiWVmehvBLB</guid><pubDate>Wed, 11 Feb 2026 09:40:36 GMT</pubDate><author>华卫</author><category>芯片&amp;算力</category></item><item><title>从多模态走向全模态！蚂蚁开源 Ming-Flash-Omni 2.0，对标Gemini 2.5 Pro</title><description>&lt;p&gt;2 月 11 日，蚂蚁集团开源发布全模态大模型 Ming-Flash-Omni 2.0。在多项公开基准测试中，该模型在视觉语言理解、语音可控生成、图像生成与编辑等关键能力表现突出，部分指标超越Gemini 2.5 Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型地址：https://github.com/inclusionAI/Ming&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Ming-Flash-Omni 2.0 是业界首个全场景音频统一生成模型，可在同一条音轨中同时生成语音、环境音效与音乐。用户只需用自然语言下指令，即可对音色、语速、语调、音量、情绪与方言等进行精细控制。模型在推理阶段实现了&amp;nbsp;3.1Hz 的极低推理帧率，实现了分钟级长音频的实时高保真生成，在推理效率与成本控制上走在前列。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c6/c6ced55efdf5e6d69a184c6e8ce4b68a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（图说：Ming-Flash-Omni-2.0&amp;nbsp;在视觉语言理解、语音可控生成、图像生成与编辑等核心领域实测表现均已达到开源领先水准）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;全模态和多模态有啥不一样？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型快速演进的背景下，“多模态”和“全模态”这两个概念正在频繁出现在技术发布和行业讨论中。表面看，它们都指向“模型能处理多种类型的数据”，但在底层实现路径和能力边界上，两者存在本质差异。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从能力表象来看，多模态与全模态高度相似。无论是多模态大模型还是全模态大模型，用户侧的直观体验都是：模型可以同时接收文本、图片、视频，甚至音频等不同模态的输入，并给出统一的输出结果。这也是许多非技术用户容易将两者混为一谈的原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正的分水岭，出现在模型内部的实现方式。目前主流的“多模态大模型”，本质上是一种“模型拼装”思路：针对不同模态的数据，系统会分别调用对应的专用模型进行处理——例如，文本由语言模型理解，图像由视觉模型解析，音频交给语音模型识别。随后，再通过一个融合模块，将来自不同模型的结果进行整合，生成最终输出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构的优势在于工程实现相对成熟、可控，也便于在已有单模态模型基础上快速扩展能力。但其局限同样明显：不同模态之间更多是“后验融合”，信息在中间环节已经被压缩或结构化，跨模态的深层语义关联难以充分建模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比之下，“全模态大模型”走的是一条更底层的路线。所谓全模态，并不是简单地“支持更多输入类型”，而是指模型在设计之初，就将多种模态视为统一的信息空间，在同一个模型参数体系中进行联合建模。文本、图像、音频、视频不再对应独立的子模型，而是通过统一的表示方式进入同一套网络中学习、推理和生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着，全模态大模型具备原生的跨模态理解与生成能力：不同模态之间的关联不是在输出阶段“拼接”出来的，而是在模型内部的表示层和推理过程中自然形成的。从理论上看，这种架构更接近人类对世界的感知方式，也更有潜力支持复杂的跨模态推理任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当下，大多数落地应用仍然基于多模态架构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内普遍认为，多模态大模型最终会走向更统一的架构，让不同模态与任务实现更深层协同。但现实是，“全模态”模型往往很难同时做到通用与专精：在特定单项能力上，开源模型往往不及专用模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蚂蚁集团在全模态方向已持续投入多年，Ming-Omni系列正是在这一背景下持续演进：早期版本构建统一多模态能力底座，中期版本验证规模增长带来的能力提升，而最新2.0版本通过更大规模数据与系统性训练优化，将全模态理解与生成能力推至开源领先水平，并在部分领域超越顶级专用模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次将Ming-Flash-Omni 2.0 开源，意味着其核心能力以“可复用底座”的形式对外释放，为端到端多模态应用开发提供统一能力入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ming-Flash-Omni 2.0 基于 Ling-2.0 架构（MoE，100B-A6B）训练，围绕“看得更准、听得更细、生成更稳”三大目标全面优化。视觉方面，融合亿级细粒度数据与难例训练策略，显著提升对近缘动植物、工艺细节和稀有文物等复杂对象的识别能力；音频方面，实现语音、音效、音乐同轨生成，支持自然语言精细控制音色、语速、情绪等参数，并具备零样本音色克隆与定制能力；图像方面，增强复杂编辑的稳定性，支持光影调整、场景替换、人物姿态优化及一键修图等功能，在动态场景中仍保持画面连贯与细节真实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;百灵模型负责人周俊表示，全模态技术的关键在于通过统一架构实现多模态能力的深度融合与高效调用。开源后，开发者可基于同一套框架复用视觉、语音与生成能力，显著降低多模型串联的复杂度与成本。未来，团队将持续优化视频时序理解、复杂图像编辑与长音频生成实时性，完善工具链与评测体系，推动全模态技术在实际业务中规模化落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/d9TEFiU7kq8EKCIodTmI</link><guid isPermaLink="false">https://www.infoq.cn/article/d9TEFiU7kq8EKCIodTmI</guid><pubDate>Wed, 11 Feb 2026 09:31:43 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Snowflake 语义视图自动驾驶：分钟级 AI 驱动的语义建模 ｜ 技术趋势</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具备治理性、可信语义的数据层已成为AI就绪数据的基础能力。近日，Snowflake正式宣布语义视图自动驾驶（Semantic View Autopilot，SVA） 全面上市。该系统能够基于现有查询与商业智能资产，自动生成语义视图。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;问题在于定义缺失，而非大语言模型能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年，开发 AI 智能体的团队发现，即便最先进的模型也难以应对不一致的业务逻辑。真正的障碍并非 AI 能力，而是数据定义的缺失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VTS 工程高级副总裁 Prashanth Sanagavarapu 指出：“构建并维护统一的语义层需要大量人工投入，以避免数据指标冲突。” 为此，我们推出语义视图自动驾驶方案，旨在自动化构建这一具备治理能力且可信的语义层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“语义视图自动驾驶技术为我们的 AI 系统提供了统一且受控的业务指标理解框架……使我们能够提供可靠的个性化服务及AI驱动的交互体验，赢得客户的长期信任。”Simon AI 首席技术官 Matt Walker 表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Snowflake 自动化语义视图创建&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;语义视图不仅提供数据结构信息，更能阐释数据的业务含义与设计意图。它能够指导大语言模型（LLM）将原始数据转化为业务概念，但传统的创建过程往往耗时费力且高度依赖人工操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对数据团队而言，确保业务逻辑的一致性至关重要。然而，人工创建语义视图负担沉重，例如，产品团队定义“月度经常性收入”时，可能未意识到财务部门会排除一次性设置费用。这类隐藏规则通常仅在部署后、数据对不齐时才会暴露。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake语义视图自动化功能（SVA）通过自动创建和管理语义视图来弥合这一鸿沟。该功能无需工程师从零开始编写定义，而是基于查询历史与可信商业智能资产，主动推荐从中学到的候选指标与筛选条件，使团队能够在数分钟内完成审核、认证与部署，而非耗费数周时间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运作机制：基于共识模式的学习&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 的核心原理在于：您的语义定义已蕴藏于查询历史、数据使用情况及仪表板之中。这使语义建模从编码工作转变为治理优化——团队只需专注于审阅 SVA 自动发现的业务逻辑。这些经治理的定义将为 Snowflake Cortex Analyst、Cortex Agents及Snowflake Intelligence 提供支持，以获取更精准可信的结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;SVA 通过分析以下三类关键信号实现这一过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模式识别与共识驱动提取&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 采用聚类算法分析查询模式与自然语言问题，以识别共识性业务逻辑。当存在冲突定义时（例如不同的“活跃用户”筛选条件），SVA 会将最高频出现的模式作为推荐方案提出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例如：若 200 余次查询均将“活跃用户”计算规则定义为“用户参与度分数&amp;gt; 50 且 最后登录天数&amp;lt; 30”，则即使用户近期运行了不同条件的查询，SVA 仍会优先推荐此共识逻辑作为标准提案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;多源高置信信号学习&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最高置信源通常来自现有商业智能（BI）仪表板，其中沉淀了多年的业务逻辑。Tableau 是 SVA 支持的首个 BI 工具，未来将通过我们 20 余家 OSI 合作伙伴接入更多平台。SVA 可在数分钟内将静态仪表板转化为对话式人工智能应用（详见实践操作指南）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队亦可直接上传可信 SQL 查询语句。SVA 将自动提取数据关系与业务指标，并将其存储为经过验证的查询模型供后续使用。得益于全程在 Snowflake 内部运行，SVA 能够直接分析真实业务数据。通过列基数分析可推断关系类型，进而智能生成优化建议，例如为提升精度推荐部署 Cortex Search 服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;基于持续演化的使用模式进行迭代更新&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;SVA 通过监控用户查询模式，持续优化语义视图的时效性。例如当企业新增“专业版”订阅层级时，SVA 将自动识别包含 subscription_tier = &#39;pro&#39; 的新查询模式，并主动建议将其纳入语义视图体系，确保业务规则演进过程中智能应答始终保持一致性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从传统商业智能向 AI 智能体的转型，需要建立在数据使用实践而非 LLM 预设的语义基础之上。Semantic View Autopilot 通过真实使用场景驱动语义建模，为您提供当前最快捷的、具备治理能力的上下文感知AI实现路径，现已全面覆盖 Snowflake 所有支持 Cortex Analyst 服务的区域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即刻开启智能语义视图之旅。欢迎在您的 Snowflake 账户中体验 &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/views-semantic/autopilot&quot;&gt;SVA&lt;/a&gt;&quot;，并获取为 Cortex Analyst 构建语义视图的&lt;a href=&quot;https://www.snowflake.com/en/developers/guides/best-practices-semantic-views-cortex-analyst/&quot;&gt;最佳实践&lt;/a&gt;&quot;指南。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/semantic-view-autopilot/&quot;&gt;https://www.snowflake.com/en/blog/semantic-view-autopilot/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/4gjxPMiGtEfB946j2U27</link><guid isPermaLink="false">https://www.infoq.cn/article/4gjxPMiGtEfB946j2U27</guid><pubDate>Wed, 11 Feb 2026 09:25:22 GMT</pubDate><author>Abhinav Vadrevu</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>OpenAI 与 Anthropic双雄打擂台！专家：2026 年 Agent 将在产业里遍地开花</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;人工智能正处于阶梯式发展的平台期，当前研究路径的收益正在收敛，下一次跃迁需要全新的范式突破。与此同时，产业应用正在加速成熟，2026年有望成为Agent大规模落地的关键之年。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;上周，OpenAI 与 Anthropic 几乎在同一时间抛出了各自最新的模型更新——OpenAI Codex 5.3 与 Claude 4.6。没有发布会轰鸣，也没有颠覆式叙事，但在开发者社区和产业侧，这两次更新仍被迅速解读为一个清晰信号：大模型能力正在逼近一个阶段性的上限，而行业正在集体寻找新的突破口。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果用一个词来形容2025年的人工智能行业，那就是“临界”。一方面，大模型的通用能力已达到较高水平，在语言理解、推理、代码生成等维度上正在逼近甚至超过人类专家水准；另一方面，沿着既有路径继续堆叠规模与算力，边际收益正在迅速收敛。技术并未停滞，但“下一次质变从何而来”，正在成为整个行业共同面对的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下一代范式突破的方向是什么？中美竞争的真正差距在哪里？Agent如何从概念走向真正的产业落地？这些追问贯穿整个行业，而在2026年，它们变得无法回避。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近期，带着这些问题我们与中关村人工智能研究院副院长&amp;amp; 北京中关村学院副教授（以下简称“中关村两院”）郑书新进行了一次深度访谈。郑书新认为，人工智能正处于阶梯式跃迁的平台期，下一次跃迁需要全新的范式突破。他同时指出，当前中美竞争的核心差距不在技术路线，而在高质量数据和算力资源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产业侧，郑书新认为技术突破与产业普及之间始终存在时间差，这是历史常态而非失败。就像蒸汽机的发明并不会立刻带来工业革命的大规模落地，AI能力要转化为大规模应用，同样依赖配套系统与产品形态的逐步成熟。在他看来，2026年将是Agent在真实场景中集中落地的一年，而Coding Agent等新范式也正在重塑传统软件开发的基本逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下为访谈实录，经由InfoQ编辑及整理：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开场：个人介绍与研究背景&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您在AI领域深耕多年，能否和我们分享一下您的研究历程和主要工作？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：我从十多年前开始接触人工智能，一直深耕大模型领域。早期专注于大规模分布式优化，搭建了当时微软最大的异步分布式训练系统。此后转向大语言模型研究，提出了Pre-LN等训练优化与架构改进方法，将模型训练效率提升了约一个数量级。这些成果后来被主流大模型广泛采用（如OpenAI开源模型gpt-oss 等）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在通用模型与方法研究阶段，我提出的Graphormer架构，现在是图（Graph）学习领域的主流基座模型之一。近期，我致力于将大模型与生成式AI技术引入科学发现领域，提出的分子平衡分布预测框架突破了传统生物分子模拟的瓶颈，将分子动力学模拟效率提升数十万倍，相关成果发表于《Science》封面及《Nature Machine Intelligence》等顶级期刊。&lt;/p&gt;&lt;p&gt;2024年底，我加入中关村两院，现任学院副教授、研究院副院长，在AI基础学部负责大模型方向的研究与战略布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到目前在中关村两院AI基础学部负责大模型方向的研究。中关村两院肩负着北京乃至国家AI创新生态建设的使命，能否介绍一下两院的核心定位？AI基础学部在其中扮演怎样的角色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：北京中关村学院与中关村人工智能研究院是一体两面，融合发展，是教育科技人才一体化的新尝试，是新型研发机构的二次方。北京中关村学院肩负着培养人工智能领军人才的重要使命，是国家教育、科技、人才一体化改革的&quot;试验田&quot;。中关村人工智能研究院与中关村学院共同开展面向未来、具有产业价值、颠覆性的人工智能技术研发及成果产业化落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI基础学部在这个框架下承担具体的技术攻关和方向布局，我们的战略目标是补全AGI下半场的关键拼图，在产业上输出能真正重塑行业逻辑的核心变量，在人才上培养兼具工程能力与科学直觉的领军人才。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI整体发展Overview&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：站在2026年初这个时间节点，您认为当前中国 AI发展最需要解决的关键问题是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;AI发展正处在阶梯式跃迁的平台期，沿着现有技术路径的边际收益在递减，需要找到下一代突破方向。同时， AI本身也有两个特征：它是根植于产业的技术；并且，这场博弈有明确的时间窗口，很有可能在3-5年内见分晓。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些判断，我认为当前有两个核心问题需要关注。第一是战略层面：这场范式竞争的背后是中美科技博弈，我们如何争取先手、发展自主生态。第二是应用层面：AI如何真正拉动GDP，实现高质量发展。现在AI的行业渗透率已经很高，但对GDP的实际贡献还很有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI技术发展现状&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到技术上的关键问题是中美技术博弈中争取先手。能否展开谈谈，您如何看待当前AI技术的发展阶段？下一代技术突破的方向会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;人工智能的发展遵循&quot;阶梯式跃迁&quot;的规律。最近一次重大跃迁是GPT带来的规模定律。但现在，智能性提升进入平台期，沿着现有技术路径的收益在递减，近期已经有多个迹象有所印证。其一，预训练范式遇到瓶颈。规模定律的红利趋近耗竭，可用于模型训练的互联网高质量数据见顶，继续扩大模型规模的边际收益显著下降。其二，后训练范式同样存在局限。当前业界普遍转向精细化的奖励函数设计，奖励函数的设计复杂度已经堪比当年的特征工程，本质上是在既定框架内反复调优。Meta近期发布的研究也表明，后训练的增量空间可能比预期更有限。如果“Less Structure, More Intelligence”成立，那么现有策略能否一路带领我们通向AGI，坦率说是存疑的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;那么，下一代突破的方向是什么？可能是针对本代AI范式的缺点进行改进、寻找突破口，例如突破记忆与持续学习的瓶颈、打通经验学习（Learning from Experience）和自我博弈（Self-Play）的路径、提高长上下文支持能力、探索动态数据的新训练方法等。但也有可能需要探索全新的技术范式，例如受神经科学启发的软硬件结合架构、新的数据来源、离散Diffusion等新的建模方式、以及新的智能性理论与奖励函数设计等。然而，下一代探索是高风险、长周期的，对商业公司而言往往优先级较低，毕竟它们需要兼顾短期业绩和股东回报；而多数高校虽有学术自由度，但在算力和工程资源上存在现实约束。正因如此，中关村两院希望在这个时点带发挥独特作用，做难而正确的事情，沿现有路线突破和全新范式探索两个方向布局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：2025年Agent很火，有人把Agent理解为大模型的应用层封装，有人把它理解为落地的应用形式。您如何看待当前AI Agent的发展现状？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;大家普遍把Agent理解为技术上的研究领域，或是一种落地的应用形式。但在我看来，Agent 就是基座模型，是当前业界押注智能性提升的主要技术路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么这么说？预训练Scaling Law边际效益递减的根本原因在于互联网高质量数据已接近上限。现在的核心解法之一就是找新的数据来源——合成数据，它的本质是搜索，在超高维的语言空间中使用预训练好的大模型去发现新的有价值数据，依托这些合成数据来进一步提升模型的性能。以o1为代表的推理模型，就是通过搜索和强化学习在语言空间中生成高质量的思维链数据；而Agent进一步扩展了搜索空间的边界，与环境交互并调用工具，发现全新的高价值数据，可能存在新的Scaling Law。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在2026年，您认为AI Agent领域最值得期待的技术突破点是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：类似整个AI 领域的进展方向，我期待的一是改进现有范式的短板，二是新的训练范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在现有范式的改进上，有几个方向值得关注。首先是运行时学习（Runtime Learning），让智能体能够在运行过程中持续学习和改进，而不只是依赖预训练阶段的能力。其次是记忆机制，Agent需要在长周期任务中保持上下文连贯，有效地存储和调用历史信息。此外，幻觉与可靠性、下一代评测方法、智能体系统的整体可用性与智能性等也是关键课题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在新范式的探索上，自我迭代的训练方式，以及内在动机（Intrinsic Motivation）驱动的奖励机制，都可能为Agent带来阶跃式的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些也是中关村两院大模型领域的重点布局方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：相比国外，您认为国内在AI研究方面最大的优势和短板分别是什么？在全球AI竞争中，我们最需要补上的“关键一课”是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;中国拥有庞大的人才基数和深厚的数理传统，大量工程师具备扎实的数学功底和出色的工程落地能力。与此同时，中国的产业门类齐全、应用场景丰富、市场规模庞大，这种独特的生态为AI落地提供了天然的试验田，也孕育了极强的产品化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说短板，目前核心有两点：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一点是数据。目前中美技术路线上已经趋于透明，国内头部大厂和美国最大的差距就是数据，这是大模型智能性提升的主要来源。美国正在系统性地采集长程、复杂、高难度的专业级数据，这类数据的特点是推理链条长、多轮交互、涉及多种工具调用，单条价值可达上千美金。这也是OpenAI等公司研发的重点，目前已经有专门的公司在帮大厂收集编程、金融、法律、咨询等领域的专家级知识和数据，可以预见2026年在这些专业领域会有显著突破。我们在这方面还比较欠缺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二点是算力。我认为算力是智能性提升的第一性原理——科学的进步依赖多样性的探索，而多样性的探索依赖充足的算力。但目前我们在这方面面临不少挑战：一是芯片本身的性能受限，二是大规模组网能力有待提升。据传美国xAI已经有80万张H100级别的集群，而国内头部的&quot;六小龙&quot;基本还在5万张上下。在这种情况下，对我们的要求就更高了——需要特别巧妙精细的设计，省着用，才能做出东西；但美国目前可以进行大规模、多方向的并行探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI产业现状&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：您之前提到，产业上目前的问题是行业渗透率高，但对GDP的实际拉动效益还很有限。从整个AI领域来看，您认为产业真正的爆发拐点会在什么时候到来？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;现在确实面临技术跑在前面的情况，即模型能力已经在很多领域达到“博士级别”智能，但在产业端体感还比较弱，对GDP拉动有限。不过这是正常的，因为技术研发和产业落地之间存在时间差。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;打个比方，蒸汽机的出现是一场动力革命——它重新定义了制造业、交通、能源等几乎所有行业。但从瓦特改良蒸汽机到工业革命全面铺开，中间隔了几十年，因为需要铁路、工厂、煤炭供应链等一整套配套系统逐步成型。AI也正处在类似的阶段：核心的&quot;动力源&quot;已经出现，但要真正重塑产业，还需要数据基础设施、工程化工具链、行业know-how的深度融合。不同的是，这一轮的节奏会快得多，可能几年而不是几十年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，这个进程已经在加速。2025年Agent的突破是一个缩影——更广泛地看，AI已经在各行各业开始渗透，很多场景不需要&quot;博士级&quot;智能，关键是被打磨成真正可用的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我的判断是，2026年会是AI产业落地的关键一年。一方面，Agent、Coding Agent 等产品形态会让更多用户在工作和生活中真正用上AI；另一方面，垂直行业的AI应用也在快速成熟，一级市场已经有大量公司在做得不错的公司。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤其值得关注的是白领和知识工作者群体。当前模型在多学科领域已经接近博士级智能，法律、金融、咨询、研究等领域有望率先释放生产力红利，AI对GDP的拉动很可能从这里开始。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：Coding Agent是当前讨论的热门方向，您怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;Coding Agent正在颠覆传统软件开发的范式。过去的逻辑是一个团队精心打磨3个产品，最后可能有1个成功；现在借助Coding Agent，个体就能快速开发100个产品，成功的概率和路径都被彻底改变了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我自己每天都在用Codex这些工具，经常多个任务并行。此刻我的电脑上就同时跑着4个Codex Agent，帮我完成各种任务。很多以前停留在想法阶段的项目，现在都能快速变成可运行的产品。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更让我兴奋的是，这种能力可以快速复制给零基础的人。我在北京中关村学院开了门AI Agent编程课程，宗旨都是“零帧起手手写代码”。大约半个月前，斯坦福也开出一门类似课程，理念是“全程不写一行代码”，和我不谋而合。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;课程只有四个半天，学生来自物理、材料、金融等各专业，很多人零编程基础。但结课时，所有小组都拿出了可运行的Demo：有人把Deep Research做成了“带事实核查的Deep Research”；有人把语音对话GPT改造成&quot;带快慢双系统的版本&quot;——快系统负责即时回应，慢系统在后台深度推理，最后融合呈现。零基础、跨背景，四个半天就能独立做出产品，这在以前是不可想象的，也是Coding Agent带来的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：在您看来，有哪些公司或产品在Agent领域做得比较出色？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;现在这个领域非常活跃，Agent的发展正在从“对话”向“办事”演进。如果说去年大家还在讨论概念，今年我们已经看到了很多能真正提高生产力的落地案例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如当下非常热门的几个产品，它们的共性在于：深度接管系统与文件，自主规划、异步执行、完成任务。如开源的Clawdbot被称为“AI Jarvis”；Anthropic的Claude Cowork实现了从“对话助手”到“数字同事”的跨越。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Coding Agent是目前落地最快的方向之一。海外的Cursor、Claude Code已成为开发者标配；国内方面，Kimi K2.5作为Agentic模型表现亮眼，基座模型中GLM-4.7领先，DeepSeek-V3.2、Qwen3、MiniMax-M2.1也都不错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您刚才提到了一些Agent应用产品，也提到了一些基座模型厂商。这其实涉及到行业里一个持续讨论的话题：通用大模型是否只是大厂之间的游戏？之前有嘉宾认为，通用大模型需要耗费大量人力物力财力，应该留给大厂去做，其他厂商可以在垂域模型中寻找生存空间。对此您怎么看？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;如果讨论的是大语言模型，我倾向于认为所谓的“生存空间”其实更多是“讲故事的空间”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通用大模型的发展已经非常成熟，以最近发布的模型为例，像Gemini 3和GPT-5.2 Deep Think版本都非常强大。目前来看，很难找到能在某个领域超越这两个模型的垂域模型。以法律和教育问题为例，我更倾向于直接使用GPT-5.2或Gemini 3，而不是专门的法律或教育模型。虽然这些通用模型的成本较高，但其性能已经非常出色。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果我要针对某个垂域开发应用，我会直接基于GPT-5.2进行开发，做好用户界面、数据库和基本范式，而不是自己去研发垂域模型。这种观点可能比较极端，但这是基于目前技术现状的判断——垂域模型的生存空间很有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：但垂域模型厂商会说他们的成本更低，这是否是一个优势？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我觉得这种说法有些本末倒置。首先，模型需要能够真正解决问题，才能谈成本优化。现在很多具身智能公司还在纠结成本问题，但它们可能都还没有找准真正能产生价值的应用场景。这种&quot;成本倒置&quot;的思路是不合理的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;科研方向与人才培养&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：刚才我们聊了很多技术方向和产业趋势，您也提到了不少前沿探索的可能性。能否具体谈谈您目前的科研方向与布局？您最看好哪个方向，为什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：我在学院负责大模型方向的研究，团队并行推进的方向很多，最近的一项工作是让智能体“预测未来”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各行各业本质上都绕不开同一道关：通过预测未来辅助科学决策。这听起来宏大，不同领域、不同机构，都在用各自的方式探索这个方向。比如政府出台政策前需要预判市场与社会反馈；企业制定战略前需要预估行业走势；金融机构甚至用系统去预测美国大选结果、下一场球赛谁输谁赢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这项工作的核心在于将“信息采集—逻辑推演—仿真模拟”三个环节形成闭环：首先通过智能体全自动打捞全网多模态开源情报，消除信息差；然后借助大模型的复杂推理能力进行因果建模和趋势判断；最后在虚拟环境中让成千上万个智能体反复演练，输出不同时间尺度下的演化曲线与风险概率。我们已参加多项国际预测评测，最好成绩全球第二，最新模型正在冲刺第一。把这三个环节打通，预测未来就不再是玄学，而成为可工程化的科学决策平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：您之前介绍中关村两院和AI基础学部时，特别强调了人才培养这个维度。在AI攻坚克难的过程中，我们需要大量技术人才。您如何判断一个年轻人是否具备成为优秀科学家的潜力？在您看来，中国未来的AI人才应该具备哪三类核心能力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我去判断一个人是否有潜力时，会看重三个特质：首先是问题意识，他能不能自己发现问题、定义问题，而不只是等别人给题目；其次是挫折反应，科研99%的时间是失败，关键看如何应对失败；最后是跨界好奇心，他会不会主动去了解自己领域之外的东西，很多突破来自领域交叉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优秀人才还应该具备三类核心能力：一是数学和物理的第一性原理思维，这是AI时代下更重要的底层能力；二是系统工程能力，能把一个想法从论文变成可运行的系统；三是科学品味，知道什么问题值得做，这个最难教，但也最重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：随着AI的普及，我们观察到一个现象：无论是企事业单位、高校还是中小学，大家都在学习AI和编程，但也越来越依赖现成工具——从调用API、套模板，到直接使用AutoML、Copilot等——而对数学基础、算法原理的关注反而不足。微软CEO萨提亚·纳德拉也曾提到，AI很重要，但要避免过度依赖。您如何看待这种&quot;工具熟练度高，但科学基础薄弱&quot;的趋势？会担心未来的研究者变成&quot;只会调包、不会创新&quot;吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我的观点可能稍有不同，我想用一段技术演进的历史来解释这个问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最早的程序员需要用“0和1”直接跟计算机对话，甚至在纸带上打孔输入程序。后来有了汇编语言，可以用简单的英文指令代替那些0和1。再后来出现了Python，写代码几乎像写英语句子。你会发现，每一次演进都在做同一件事：把繁琐的底层操作打包藏起来，让人不用操心&quot;怎么做&quot;，而是专注于“做什么”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个过程中，每一次进步都伴随着类似您提到的担忧：新一代程序员不懂底层原理了怎么办？但事实是，正是因为不用再纠结底层细节，程序员们才能腾出精力去解决更复杂、更有价值的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天的AI工具也是一样。它让研究者可以跳过很多繁琐的技术步骤，把精力放在真正重要的问题上——比如提出新假设、设计新实验、发现新规律。这些才是创新的本质，而不是亲手写每一行代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我的建议反而是：大胆拥抱最先进的工具，但要清楚自己真正想解决的问题是什么。工具是手段，问题才是目的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;InfoQ：如果让您预测2030年最具影响力的AI科学突破，您会押注在哪三件事上？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;郑书新：&amp;nbsp;我会押注在这三个方向上：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一，AI智能性超过人类，ASI（超级人工智能）实现。&lt;/p&gt;&lt;p&gt;第二，AI在科学研究中能够自主完成发现和突破，比如找到治愈癌症的路径，或者解决数学领域悬而未决的开放问题。&lt;/p&gt;&lt;p&gt;第三，AI走进物理世界，对实体产业形成实质性推动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;采访嘉宾：&lt;/p&gt;&lt;p&gt;郑书新，中关村人工智能研究院副院长&amp;amp; 北京中关村学院副教授&lt;/p&gt;</description><link>https://www.infoq.cn/article/iHkvlLuTCWNJv27eJ1XY</link><guid isPermaLink="false">https://www.infoq.cn/article/iHkvlLuTCWNJv27eJ1XY</guid><pubDate>Wed, 11 Feb 2026 09:00:00 GMT</pubDate><author>李冬梅</author><category>AI 工程化</category></item><item><title>如何利用 Snowflake 将 AI 创新转化为可靠、生产就绪的应用 ｜ 技术趋势</title><description>&lt;p&gt;2026 年，智能体将在企业级应用中取得哪些实质性突破？&lt;a href=&quot;https://www.infoq.cn/minibook/keTZm4fpOmFEzmx77Zpq&quot;&gt;点击下载&lt;/a&gt;&quot;《2026 年 AI 与数据发展预测》白皮书，获悉专家一手前瞻，抢先拥抱新的工作方式！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;人工智能创新正在持续重塑各行业与企业级的应用场景与用户体验。各公司日益聚焦于为终端用户创造可量化的实际价值。要实现这些价值，就需要可扩展、安全可靠且与企业数据深度整合的人工智能技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Snowflake，我们致力于帮助客户将人工智能与机器学习的宏伟蓝图转化为现实影响。这意味着我们将开发工具置于核心位置，使开发者能够更轻松地构建可靠的智能体，加速人工智能/机器学习工作流的上线部署，并在规模化扩展时从容管控相关负载。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们最新的产品创新赋予客户基于 Snowflake 平台构建可靠、企业级应用的能力。这将带来更高效的执行、更简化的运维流程，以及企业可放心投入生产环境的人工智能工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Snowflake Intelligence 作为即开即用的企业级智能体&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 整合了一系列功能模块，旨在帮助企业用户快速、安全、自主地实现人工智能价值。本次更新聚焦三大核心需求：&lt;/p&gt;&lt;p&gt;&amp;nbsp;支持用户将有价值的对话输出保存为成果资产，并可将这些资产共享给其他利益相关方以支持商业决策（即将推出）；通过安全的原生移动端访问，满足业务人员随时随地使用需求（即将推出）；客户现可将业务人员纳入Snowflake Intelligence使用范畴，同时限制其对SQL及数据工具的访问权限。所有现有安全策略持续生效，管理员仅需通过单一用户属性即可启用该功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 致力于在工作发生的任何场景下提供可信洞察。其自然语言交互界面支持每位员工在Snowflake安全可控的平台内直接提出问题、挖掘数据表象背后的成因，并及时采取数据驱动的行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些功能共同构筑了 Snowflake Intelligence 作为可信赖企业智能体的核心能力，在用户需要的时空节点交付关键洞察，为全组织范围内的时效性数据驱动决策提供支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Artifacts：将对话转化为商业成果&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Artifacts（即将开启公开预览）代表着 Snowflake Intelligence 在赋能商业用户方式上的根本性转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Artifacts 可将 Snowflake Intelligence 中的对话转化为可保存、可共享的输出成果，例如图表与表格，并完整保留可视化呈现、底层 SQL 及上下文元数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Artifacts 是 Snowflake Intelligence 中实现企业知识捕获、共享与执行的核心单元。用户可通过保存 Artifacts 避免重复分析工作，安全地向团队成员共享实时引用，并在上下文中探索后续问题。Artifacts 支持用户回溯已构建的内容，与他人共享，并基于可信的企业数据直接展开协作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更广泛而言，Artifacts 是 Snowflake Intelligence 向终端用户交付商业洞察能力的基础架构。通过Artifacts，Snowflake Intelligence 不再仅限于临时查询或后续追问，而是成为驱动业务发展的起点。借助Artifacts，我们正将 Snowflake Intelligence 打造为全组织统一、可靠决策的核心枢纽。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Snowflake Intelligence 即将登陆移动端&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 将以 iOS 移动应用程序的形式（即将进入公开预览阶段）推出，提供更优的原生移动体验。移动端访问确保企业领导者和业务用户能够全天候连接企业知识库，无论是查看核心指标、追踪趋势变化，还是在决策过程中实时跟进关键问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为提供安全易用的体验，Snowflake Intelligence 移动应用将支持基于 FaceID 的会话续期功能（即将进入公开预览阶段）。用户可通过 FaceID 进行身份验证，令牌将在后台自动刷新。刷新令牌始终保持受保护状态，绑定设备并定期轮换，在实现企业级安全管控的同时，提供流畅的消费级移动体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;扩展访问权限：支持受限登录与 Snowflake Intelligence 专属用户&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 现支持用户直接登录，使业务用户无需了解 Snowflake 或操作 Snowsight 即可登录平台并开始提出问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于需要更严格管控的企业，Snowflake Intelligence 专属用户功能允许业务用户仅访问 Snowflake Intelligence，无法使用 Snowsight、SQL 接口或其他数据工具。这一设计让业务用户专注于专为其打造的交互界面，同时帮助企业统一管控使用范围、控制成本，并自动实施所有现有安全策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 还支持身份提供程序重定向功能。通过配置的身份提供程序（如 Okta 或 Entra ID）进行认证的用户，可获得简化的 Snowflake Intelligence 登录体验。这些功能相结合，使得在保障集中化治理控制的同时，能够轻松扩展企业内部的访问范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;轻松构建、部署与迭代智能体体验&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体现已成为企业工作流的核心。企业需要一个可靠、可信的体系架构，以在受管控且能跨团队、跨应用扩展的环境中，提供稳定精准的智能体验。我们很高兴宣布 Snowflake 平台上的重要创新，这些创新将帮助客户自信地构建并扩展生产级智能体。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现已全面推出的 Cortex Code，通过赋能各类构建者——从资深工程师到非技术团队——利用自然语言交互构建并优化智能体，全面支持这一进程。它帮助团队轻松生成合成数据，创建与调试语义视图，并快速构建和调试智能体行为，从而加速在 Snowflake AI 数据云上的生产部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即将全面推出的语义视图自动巡航功能，可助力团队自动化创建并部署生产就绪的语义视图。通过学习查询历史，语义视图自动巡航简化了建模工作流，帮助组织更快接入新用例，同时在跨团队间提供一致的洞察分析。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为推进智能体在组织内的广泛应用，Cortex Agent Sharing（即将正式发布）可帮助用户轻松发现、复用并规模化部署由内部团队或合作伙伴构建的智能体。该功能使企业能够统一智能体能力标准，避免重复开发，并将经过验证的智能体快速拓展至各团队，无需为不同用例重复构建。团队可通过 Snowflake Marketplace 获取各类方案，并利用合作伙伴构建的智能体加速实现业务价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Agent Evaluations（即将正式发布），客户能够深入洞察智能体的推理过程、工具选择与响应生成机制，从而优化智能体行为，并在其演进过程中持续提升准确性。这种透明度有助于团队通过便捷的准确性验证与逻辑一致性检查，建立对智能体质量的信心，确保其满足生产环境要求。通过完整呈现智能体的“思考过程”，Agent Evaluations 减少了调试过程中的猜测性工作，使团队能够快速定位并修复错误或性能瓶颈。最终，通过对答案、逻辑及工具使用进行验证，企业可放心地将智能体从早期实验阶段推进为团队信赖的、可用于生产环境的成熟系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;面向企业数据访问的模型上下文协议（Model Context Protocol）支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake Intelligence 现已支持模型上下文协议（MCP），以简化与第三方工具及服务的集成。我们于 2025 年 10 月推出了由 Snowflake 托管的&lt;a href=&quot;https://www.snowflake.com/en/blog/managed-mcp-servers-secure-data-agents/&quot;&gt; MCP &lt;/a&gt;&quot;server，并在此基础上进一步推出 Snowflake MCP 客户端（即将全面上市），帮助客户以更便捷、可靠的方式连接外部数据源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Snowflake MCP 客户端，账户管理员可以注册预置或自定义的 MCP 服务器（例如 Atlassian、Salesforce 或 Workday），并将其直接集成至 Cortex 智能体中。开发人员可在智能体编排过程中使用 MCP 服务器，实现无缝的工具发现与调用。Snowflake 统一管理包括令牌处理在内的认证流程，并提供可观测性支持，确保集成过程安全可控。在本次发布中，Snowflake 支持在智能体调用期间完整的 MCP 工具发现功能，同时提供监控与令牌管理能力，使客户能够安全地跨系统访问并处理企业数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;面向企业级智能体的高性能与低延迟&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在生产环境中，一致性及准确性对用户体验与应用推广至关重要。Snowflake 持续投入智能体技术栈的全面优化，致力于提供响应更迅速、结果更精准且具备规模化可预测性的AI驱动体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 即将推出持续学习型智能体记忆库（公共预览版），这是企业级智能体在质量层面的重大升级。该功能使智能体能够持续从跨用户的高质量历史响应中学习，从而提升回答一致性并增强可信度。同时，智能体可长期记忆个体用户的偏好与事实信息，为用户提供更加个性化的 Snowflake Intelligence 体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过将文本转 SQL 功能深度集成至智能体编排流程，Snowflake 进一步提升了分析工作流的准确性与响应速度。用户得以更高效地访问数据，在查看 SQL 执行过程的同时透视 LLM 决策逻辑，并针对多样化工作负载灵活优化智能体行为模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;支持智能体版本管理与成本追踪的治理机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着人工智能应用不断发展，企业需要具备相应的治理能力以实现规模化扩展。Snowflake 通过智能体版本管理与集成化运行监控功能，为企业提供此类治理支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能体版本管理功能（即将开放公开预览）为 Snowflake Cortex 智能体提供 CI/CD 支持，使客户能够安全地构建、部署和迭代智能体工作负载。开发人员可创建版本快照，通过 Git 管理变更，并安全地推进或回滚部署。此外，客户即将通过使用量视图（即将正式发布）追踪 Snowflake Intelligence 与智能体的使用情况，从而获得更完善的运行状态洞察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除可视化监控外，Snowflake 还支持团队主动管控 AI 成本。已正式发布的 AI_COUNT_TOKENS 函数可在执行前预估使用量，而即将发布的 AI 函数增量计量视图（即将正式发布）将为运行中的查询提供使用量与成本数据，帮助团队在执行期间实施限额管控并触发相应操作。这些功能使企业能够在维持可预测开支与运行管控的同时，实现生产环境中 AI 应用的规模化扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过版本管理与成本追踪相结合，团队能够在保持清晰洞察的前提下快速发展，以负责任的方式构建高性能规模化应用程序。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;通过智能体工作流加速多模态机器学习模型的在线部署&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在人工智能领域，传统机器学习仍然占据重要地位。我们欣然宣布，Snowflake ML 在智能体、多模态及实时工作流方面推出了全新功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们持续投入现代化开发体验，致力于提升生产效率。新一代 Snowflake Notebooks（现已正式发布）现已成为 Snowflake Workspaces 的核心组成部分，运行于基于 Snowflake 容器运行时构建的 Jupyter 环境中。Snowflake Notebooks 使开发者能够将已有的基于 Jupyter 的笔记本、脚本及模型训练流程无缝引入 Snowflake 统一平台，实现先进的模型开发工作流。通过与 Snowsight 中的 Cortex Code 功能（即将正式发布）深度集成，Snowflake Notebooks 进一步提升了开发与迭代的效能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据科学家在开发和调试机器学习工作流时，常常面临周期冗长的问题，导致运维瓶颈以及实际投产的模型数量有限。如今，Snowflake 将 Cortex Code 集成至 Snowflake Notebooks 的机器学习工作流中，引入智能体人工智能，使其能够基于简单的自然语言提示自主迭代、优化并生成完整可执行的机器学习流水线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对实时机器学习模型，Snowflake ML 现已正式发布&lt;a href=&quot;https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/create-and-serve-online-features-python&quot;&gt;在线特征存储&lt;/a&gt;&quot;与&lt;a href=&quot;https://docs.snowflake.com/en/developer-guide/snowflake-ml/inference/real-time-inference-rest-api&quot;&gt;在线模型服务&lt;/a&gt;&quot;功能，使模型部署更加便捷。开发者现可将特征服务延迟控制在 30 毫秒内，模型服务延迟控制在 100 毫秒内，有力支持个性化推荐、欺诈检测等低延迟在线场景，且无需额外基础设施或复杂配置。此外，基于 Hugging Face 等主流多模态模型中心进行大规模推理的功能，目前已进入公开预览阶段。结合图像、视频等非结构化数据进行推理，可在 Snowflake 平台上直接实现物体检测、视觉问答和自动语音识别等多种人工智能应用，无需构建复杂流程或迁移数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 发展的未来&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今日发布的多项成果，共同奠定了 Cortex Agents 作为企业级AI统一基础的地位。Semantic View Autopilot 助力开发者提升 Cortex Agents 的准确性，并加速推进高级用例的落地。最新的 &lt;a href=&quot;https://www.snowflake.com/en/blog/production-ml-workflows/&quot;&gt;Snowflake ML&lt;/a&gt;&quot; 升级，使开发者能够构建可供 Cortex Agents 直接调用的模型，从而为用户提供基于机器学习的预测与建议。在生产环境中，我们推出的 Evaluations for Cortex Agents 确保智能体输出结果既可信赖，又便于监控。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;借助 Snowflake 平台，企业能够将 AI 智能体与应用从实验阶段推进至生产部署，并获得团队信赖、由运维人员统一管理，最终直接赋能业务成效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;行动倡议：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;立即开始在 &lt;a href=&quot;https://www.snowflake.com/en/product/snowflake-intelligence/&quot;&gt;Snowflake Intelligence&lt;/a&gt;&quot; 中创建、保存并共享各类资产，以促进协同并推动业务行动。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;探索与 &lt;a href=&quot;https://www.snowflake.com/en/news/press-releases/snowflake-unveils-cortex-code-an-ai-coding-agent-that-drastically-increases-productivity-by-understanding-your-enterprise-data-context/&quot;&gt;Cortex Code&lt;/a&gt;&quot; 相关的发布内容。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;通过此篇&lt;a href=&quot;https://www.snowflake.com/en/blog/production-ml-workflows/&quot;&gt;博客&lt;/a&gt;&quot;，进一步了解机器学习领域的最新动态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.snowflake.com/en/blog/building-reliable-applications/&quot;&gt;https://www.snowflake.com/en/blog/building-reliable-applications/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3625913187f520bdbc21798ff22d17aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;点击链接立即报名注册：&lt;a href=&quot;https://www.snowflake.com/events/ascent-snowflake-platform-training-china-cn/&quot;&gt;Ascent - Snowflake Platform Training - China&lt;/a&gt;&quot;，更多 Snowflake 精彩活动请关注&lt;a href=&quot;https://www.infoq.cn/space/snowflake&quot;&gt;专区&lt;/a&gt;&quot;。&lt;/p&gt;</description><link>https://www.infoq.cn/article/HQCgKB6UVJvDTePdIdoU</link><guid isPermaLink="false">https://www.infoq.cn/article/HQCgKB6UVJvDTePdIdoU</guid><pubDate>Wed, 11 Feb 2026 08:32:13 GMT</pubDate><author>Arun Agarwal</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>离开半年，48 岁前 GitHub CEO 携开源 AI 开发者平台和老东家打擂</title><description>&lt;p&gt;在很长一段时间里，Thomas Dohmke 都被视为“最不像 CEO 的 CEO”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他会在深夜亲自回复 GitHub Issues，会在发布会上公开演示自己写代码的过程，也会在 Copilot 最早的内测阶段，反复强调一句话：“如果这个东西不能改变开发者每天的工作方式，那它就不值得存在。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但去年 8 月，这位 GitHub 首席执行官正式离职。外界一度猜测，他是否会加入另一家大厂，或转向 AI 创业投资。但几个月后，他给出的答案更直接——重新创业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今年48岁的 Thomas Dohmke 创办了一家名为 Entire 的新公司，这是一个面向“智能编码时代”的开源开发者平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他在x上宣布了这一消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/25/256cac6b650826d68c54daf800c89f1d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Grab 首席产品官 Philipp Kandal在x上发帖表示祝贺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/08fdab18156a75f989f5442f40a04ba1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Entire 是谁？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，这个 Entire 到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据 Dohmke 介绍，Entire 是一个平台，但它未必会与 GitHub 展开竞争。Dohmke 表示，其理念是在技术栈的更高层构建一个平台，让开发者能够管理智能体的推理过程并与之协作。代码仓库仍将是其中的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d4/d42e49833f9dd976315947b71e172543.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Entire 正在构建的是一个三层平台，其基础是一个从零开始构建的全新 Git 兼容数据库，中间是一个语义推理层，最上面是一个用户界面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;团队认为，由于这些新存储库中存储的信息有所不同，因此需要一个新的数据库层——具体来说，智能体在使用这些工具时能够提供比人类更多的上下文信息。这个新数据库将允许人类和智能体不仅可以查询代码，还可以查询代码背后的逻辑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于代理使用此数据库及其 API 端点的频率可能远远高于人类使用 Git 存储库的频率，因此团队还需要考虑性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 还表示，与传统的集中式Git仓库不同，这种新型数据库可以构建成一个全球分布式的节点网络。对于需要（或希望）确保数据主权的用户来说，这是一个重要的卖点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然用户界面仍在开发中，但 Entire 已经构建了部分功能，用于可视化存储在Git中的检查点。不过，目前团队主要专注于命令行体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于为什么要开发这样一个平台，Dohmke给出了他的解释。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke强调，当前开发者/客服工作流程中存在的一个问题是我们现在经常听到的：代码交付的瓶颈不在于编写代码，而在于审查客服编写的代码。这已经导致开发者精疲力竭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如，爱尔兰软件工程师、谷歌Gemini 开发者（同时也在参与Chrome的开发） Addy Osmani 就曾公开表达了对 Vibe Coding 不信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我们在谷歌也使用Vibe 编码——我发现它非常适合原型设计和最小可行产品（MVP），对学习也很有帮助……”Osmani 在11月初的一个播客节目中说道。“但总的来说，Vibe 编码更注重速度和探索，而不是正确性和可维护性。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3c/3cb679cd4e55564b299da15829e314e1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Dohmke 认为，未来将会出现更多的Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“如果你在整个软件生命周期中都遵循这个流程，那么编写代码之后的下一步就是代码审查——无论是你自己的代码，还是通过拉取请求审查团队成员的代码，”Dohmke说道。“但是拉取请求也存在同样的问题（在理解代码方面）。它会显示一些我从未编写过的文件的更改。而像Copilot这样的代码审查工具会给我提供关于其代码的反馈，这在我对代码还有一些基本理解的情况下非常有用，但如果我不真正理解这些代码的作用，那么这些反馈就变得毫无意义或多余了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当代码较多而上下文较少时，解决方案可能是使用代理和确定性工具来测试代码，并确保其合规性和安全性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他解释说：“这正逐渐成为瓶颈，所以你必须从流程中剔除这一步骤。我认为这是业内最大的挑战之一，因为在我们应对日益增多的网络攻击的同时，许多组织已经引入了零信任流程，这意味着任何部署都必须经过人工审核。因此，我认为，在我们看来，许多创新将在这一领域涌现，而我们希望成为其中的一份子。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;首发新品&amp;nbsp;Checkpoints，并开源&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Checkpoints 是 Entire 公司发布的首款产品。这款全新开源工具集成了 Claude Code 和 Google 的 Gemini CLI（即将支持 Open Codex），能够自动提取并记录智能体的推理、意图和结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在当前的 Agent 开发流程中，一个长期被忽视的问题正在变得愈发突出：会话是短暂的，而决策是不可逆的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大多数情况下，智能体的提示词停留在终端里，推理过程被塞进上下文窗口。一旦会话结束，这些信息便随之消失。代码最终被提交进 Git，但 Git 只记录了“改了什么”，却无法回答“为什么这么改”。当智能体在一次会话中生成成百上千行代码时，这种上下文的缺失会迅速放大：早期的约束条件、设计取舍、被否定的方案，都无法被追溯。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果是，智能体之间几乎无法真正协作。它们会在不同会话中重复推理、重复试错，重新消耗token，甚至推翻数小时、数天前已经做出的决定。随着代码库规模扩大，这种“失忆式开发”正在成为效率和一致性的隐性成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这一问题，一种名为 Checkpoints 的新机制被提出。它试图把原本易逝的智能体上下文，变成可持久、可追溯的工程资产。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Checkpoints 的核心思路是：将智能体的完整会话上下文，作为 Git 中的一等版本数据保存下来。当由智能体生成的代码被提交时，系统不仅记录代码本身，还会同步捕获这次会话中的关键信息，包括提示词、日志、访问过的文件、工具调用情况以及令牌消耗等。这些信息与代码提交一一绑定，构成一条“为什么这样写”的语义轨迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从使用方式上看，Checkpoints 以一个“Git 感知”的命令行工具运行。每一次由智能体触发的提交，都会生成一个结构化的检查点对象，并与对应的提交 SHA 关联。代码仓库的内容本身并不发生改变，新增的是一层上下文元数据。当开发者将代码推送到远程仓库时，这些检查点会被同步写入一个独立的、只追加的分支，形成完整的审计日志。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，开发者不再只能回溯代码差异本身，还可以追溯到产生这些差异的推理过程和决策背景。在多人、多智能体协作的场景下，代码库的演进第一次具备了“记忆能力”，而不再只是结果的堆叠。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“中间层的作用在于向人类和Agent提供所有促成软件产品诞生的信息，” Dohmke 解释说。“而如今，在GitHub代码库中，包含了所有代码，有时还有文档和依赖项，但基本上缺少了所有关于如何实现这些代码的信息。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是因为这些系统是为人类开发者设计的，虽然开发者在完成代码编写后可能会编写测试用例和文档，但记录他们具体的推理步骤却从未被纳入流程。而在传统的、非智能体的工作流程中，大量的机构知识从未被记录下来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这是我们更大愿景的第一步，即在软件项目的生命周期中提供语义推理层，这样你就可以在未来的任何时间点，以人类或智能体的身份，追踪决策的制定原因，”Dohmke 解释道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过保存所有这些数据，Checkpoints 将允许开发人员查看代理是如何生成代码的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有x用户在x上询问数据会如何处理？是否会被用于其他地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 表示：“Entire 会将上下文和Checkpoints存储在用户的 GitHub 代码库中。只要用户登陆上来，Entire 会将其同步到 Supabase 数据库，仅用于显示目的。从长远来看，我们希望构建一个语义层，以便人类开发者和智能体能够并行地进行推理、协作和构建。我们不会将您的数据用于除向您和您的团队提供平台功能之外的任何其他用途。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8f/8fe72ef1e0359147e9f550de7bc5ad27.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;一位“工程师型 CEO”的来时路&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Thomas Dohmke 并不是传统意义上“职业经理人”出身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在加入 GitHub 之前，他是一名工程师、创业者，也是 GitHub 的长期重度用户。2018 年 GitHub 被微软收购后，他进入微软体系，随后在 2021 年接任 CEO，成为 GitHub 历史上第一位真正意义上的“工程师型掌舵人”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他任内，GitHub 完成了一次关键转向：从“代码托管平台”，变成“以 Copilot 为核心的 AI 开发平台”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Copilot 的推出，并非单点功能升级，而是一次底层范式变化。GitHub 不再只服务“人如何协作写代码”，而是开始服务“人如何与 AI 一起写代码”。这一判断，后来被证明是整个行业的分水岭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，当 Dohmke 在 2025 年宣布离职时，他特意强调：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这是一次完全友好的离开，不是对 GitHub 或微软路线的否定。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据他在接受 The New Stack 采访时回忆，自己在 6 月与微软 CEO 萨提亚·纳德拉进行过一次长谈。他向纳德拉坦陈了自己的想法：想回到“从零开始造东西”的状态。纳德拉的回应是，希望他“把 CEO 的工作好好做完”，同时也欢迎他继续留在微软生态中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也解释了一个细节：微软风投部门 M12，成为 Entire 的投资方之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/38/389d436b05484d1818c46c4cfffc844e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了GitHub技术领导者这样的职业经历做背书，Dohmke 在资本市场备受青睐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Entire 的首轮融资规模达 6000 万美元，由 Felicis 领投，Madrona、Basis Set 以及微软 M12 共同参与，公司估值达 3 亿美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事实上，6000 万美元的融资在开发者工具领域并不常见。更重要的是，这是一个产品仍处于早期阶段的平台。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在投资人看来，这并不是一次“押产品”，而是一次押人 + 押判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Dohmke 给出的核心判断是：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;GitHub 所代表的那一代开发者平台，诞生于“人写代码”的时代，而不是“Agent写代码”的时代。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e9d6afa029f9291b02c2e7e84ed650df.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Felicis 创始人Aydin Senkut则押注的是Dohmke 丰富的行业经验，他在x上发文称：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我第一次见到 &lt;a href=&quot;https://x.com/ashtom&quot;&gt;@ashtom&lt;/a&gt;&quot; 他的远见卓识令我叹服。他对现代开发流程有着深刻的理解。作为 GitHub 的 CEO，他带领公司完成了人工智能的转型，并将平台规模扩展到全球超过 1.5 亿开发者。但他同时也清晰地预见到，如今整个行业都需要彻底革新。他的信念和洞察力令我深受鼓舞。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f8/f8d7d538bc3952e4d699c4c6b9c37b66.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着本轮融资的完成，Entire计划将其员工人数从目前的15人增加到约30人，并尽快搭建其平台。但正如Dohmke强调的那样，如今重要的不仅仅是员工。Entire团队甚至在其新闻稿中也提到，他们计划将团队规模扩大到“数百名客服人员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“我认为到了2026年，任何领导者都需要重新审视员工人数，不再仅仅关注薪资、福利、差旅和开支，还要关注代币价值。我和一些工程师交流过，包括我自己团队的工程师，以及湾区的工程师，他们都在谈论每月价值数千美元的代币，”多姆克说道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于商业模式，Dohmke 告诉我们，团队计划遵循成熟的开源模式，即以宽松的许可协议提供平台的大部分功能，然后提供具有附加功能的托管服务来实现盈利。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;网友：能用，但没必要硬吹&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Thomas Dohmke 宣布创办 Entire、并完成 6000 万美元种子轮融资之后，Hacker News 很快成为这条消息的“情绪放大器”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有Hacker News用户称，这并不意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位长期定义开发者工作流的关键人物，带着“为智能体时代重构软件工程”的宏大叙事重新创业，再加上一笔在开发者工具领域堪称夸张的种子轮融资，本身就足以触发 Hacker News 最典型的那种讨论：技术是否真的新？价值是否被高估？以及——这到底是在投产品，还是在投人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从整体来看，HN 上的讨论并未形成简单的“看好 / 唱衰”对立，而是呈现出几条高度一致、反复交织的主线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一条主线，大家更多的是在讨论“Entire到底是不是一个新原语？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;支持者与反对者的分歧，首先集中在 Entire 的首个产品 Checkpoints 本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1e/1e2f474191775ab0427f59e7f92160c9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在支持一方看来，Checkpoints 并不是一个“方便功能”，而是一种新的软件工程原语。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有开发者指出，Checkpoints 的关键不在于“保存 AI 生成的代码”，而在于它将代理的完整上下文——包括会话记录、提示、访问过的文件、工具调用和 token 使用情况——作为一级版本化数据，与代码提交一并捕获并长期保存。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一视角下，Checkpoints 并不是在解决“怎么写代码”，而是在解决一个更根本的问题：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;当代码主要由代理生成时，软件工程应该如何记录“思考过程”？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种观点认为，如果开发者无法回溯代理为什么在某个时间点做出某种选择，那么代码审查、安全验证乃至长期维护都会变得越来越脆弱。从这个意义上说，把推理过程纳入版本控制体系，本身就是一次范式转移。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但反对者几乎立刻给出了另一种解读。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他们看来，这种能力并不新，甚至实现成本极低：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“你完全可以把 AI 生成的上下文当成文本，用 git add 提交。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这类评论中，Checkpoints 被描述为一个“被概念包装过的简单想法”：不是不能用，而是远不足以支撑一个被资本高度追捧的平台叙事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外一个比较集中被讨论的话题是，6000 万美元，究竟买的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果说产品价值仍有争论，那么融资规模几乎是 Hacker News 上争议最集中的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多位用户直言，他们并不否认“记录开发决策”这件事的意义，但完全无法理解：为什么这值得 6000 万美元的种子轮？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有人直接点破，这笔融资隐含的估值，很可能已经超过 6 亿美元。在他们看来，这并不是“新的经济学”，而是风险投资在为自身账面价值服务——通过对早期项目给出极高定价，抬升整个基金组合的名义估值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7b/7b5f1d6900b3a69822bf1764b77eceda.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更激进的评论甚至认为，这种行为本身就应该被监管。但也有另一种更冷静、也更现实的声音指出：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;钱并不是在寻找“完美产品”，而是在寻找“可能的落点”。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/2026/02/10/former-github-ceo-raises-record-60m-dev-tool-seed-round-at-300m-valuation/?utm_source=dlvr.it&amp;amp;utm_medium=twitter&quot;&gt;https://techcrunch.com/2026/02/10/former-github-ceo-raises-record-60m-dev-tool-seed-round-at-300m-valuation/?utm_source=dlvr.it&amp;amp;utm_medium=twitter&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://thenewstack.io/thomas-dohmke-interview-entire/&quot;&gt;https://thenewstack.io/thomas-dohmke-interview-entire/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=NrQkdDVupQE&quot;&gt;https://www.youtube.com/watch?v=NrQkdDVupQE&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fcjA0034GUQVp20cjHZU</link><guid isPermaLink="false">https://www.infoq.cn/article/fcjA0034GUQVp20cjHZU</guid><pubDate>Wed, 11 Feb 2026 08:16:45 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>网易有道发布中国版“OpenClaw”，推出全场景个人助理Agent“LobsterAI”</title><description>&lt;p&gt;随着AI从“能聊天”迈向“能办事”的Agent阶段，个人AI Agent已成为科技圈公认的下一波浪潮。2月11日，网易有道正式推出桌面级Agent“LobsterAI”（中文名：有道龙虾）。这是一个定位为“7×24 小时帮你干活的全场景个人助理Agent”，目前已在官网开放内测申请。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2b/2bfbe9878668dc1d15c000b8fcd3c8da.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图源：LobsterAI 官网&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，LobsterAI在产品形态上展现了独特的融合创新思路：它不仅具备海外爆火的“OpenClaw”那样自主跨应用执行复杂任务的能力，更融合了类似“Claude Cowork”的GUI（图形化交互）界面，旨在打造一款让用户能轻松驾驭、更安全、更易配置的中国版自主智能体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e2/e2b11ad15dc6a935aff9dd607be834c7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LobsterAI 界面图，图源：网易有道&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此前，海外开源项目“OpenClaw”因展示出惊人的“自主操控能力”而引爆技术圈，证明了Agent产品的能力边界；而另一款产品“Claude Cowork”，则利用具备强编程能力的模型，通过编程来完成各类任务，不仅取得了很好的效果，也为行业打开了更广阔的想象空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“LobsterAI也希望打造一款拥有高自由度，且具有长时记忆、定时任务等功能的产品，来拓宽和探索Agent在工作与学习场景下的应用潜能。”LobsterAI相关负责人介绍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与传统对话式AI不同，LobsterAI具备真正的“干活能力”——它摒弃了复杂的命令行操作，采用了类似Claude Cowork的直观GUI界面；无论是资讯获取、日程管理、还是深度数据分析，用户只需与其对话，LobsterAI便能在获得授权后，自动在本地计算机中通过程序化方式执行复杂流程，并交付结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从官方释出的信息来看，目前在设备支持上，LobsterAI已打通移动端与PC端的连接，用户可通过手机端在钉钉、飞书等软件中进行远程交互；哪怕你不在电脑前，也能指挥家里的LobsterAI帮你处理紧急工作，真正实现“数字分身”随时待命。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/6d/6df8b8804fc336c898105a9805d06fa5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;LobsterAI 支持手机端钉钉远程交互，图源：网易有道&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用过程中，LobsterAI还支持定时任务机制，例如设定每天清晨自动搜集行业新闻、整理邮件摘要，当你开始工作时，所需资料已准备就绪。同时，LobsterAI还具备长上下文记忆能力，能够在多次协作中逐步理解用户偏好，形成更高效、连贯的个性化体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而针对用户关心的Agent自动操作风险，LobsterAI也采用了严格的“本地优先”策略。系统默认在沙盒环境（Sandbox）下的指定文件夹内运行，防止误操作破坏系统文件；同时支持数据本地化处理，杜绝云端泄露风险。在模型支持上，LobsterAI既预置了主流大模型API，也支持通过Ollama等框架调用DeepSeek&amp;nbsp;等本地开源模型，用户可根据任务对隐私和性能的需求灵活切换。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业内观察人士指出，LobsterAI的上线，是有道多年积累的AI底层能力与应用洞察的一次集中爆发。它巧妙地结合了OpenClaw的技术深度与消费级产品的易用性，为国内Agent赛道提供了可落地的范本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;产品内测申请入口：&lt;a href=&quot;https://lobsterai.youdao.com/&quot;&gt;lobsterai.youdao.com&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/fuib2PTStVR1lfgjZUMc</link><guid isPermaLink="false">https://www.infoq.cn/article/fuib2PTStVR1lfgjZUMc</guid><pubDate>Wed, 11 Feb 2026 08:15:10 GMT</pubDate><author>网易有道技术团队</author><category>企业动态</category><category>AI&amp;大模型</category></item><item><title>谷歌推动模型上下文协议支持gRPC</title><description>&lt;p&gt;谷歌云&lt;a href=&quot;https://cloud.google.com/blog/products/networking/grpc-as-a-native-transport-for-mcp&quot;&gt;宣布&lt;/a&gt;&quot;将为模型上下文协议（Model Context Protocol，MCP）贡献一个gRPC传输包，填补那些在微服务中全面标准化使用gRPC的企业所面临的关键空白。MCP是&lt;a href=&quot;https://www.anthropic.com/news/model-context-protocol&quot;&gt;Anthropic推出的协议&lt;/a&gt;&quot;，用于实现AI智能体与外部工具和数据的集成，目前在企业环境中获得了广泛关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，MCP默认使用&lt;a href=&quot;https://www.jsonrpc.org/historical/json-rpc-over-http.html&quot;&gt;基于HTTP的JSON-RPC&lt;/a&gt;&quot;作为传输层。这在处理自然语言负载时表现良好，但对于已全面采用gRPC的开发者而言，却带来了极大的不便。其他可选方案包括，重写服务以适配MCP的JSON传输、搭建转码代理，或并行维护两套独立实现，但是这些方案均不理想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Spotify已经亲身体验过这种痛苦。该公司的高级员工工程师兼开发者体验技术负责人Stefan Särne在谷歌的&lt;a href=&quot;https://cloud.google.com/blog/products/networking/grpc-as-a-native-transport-for-mcp&quot;&gt;博客文章&lt;/a&gt;&quot;中表示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;由于gRPC是我们后端的标准协议，我们已在内部为基于gRPC的MCP提供了实验性支持，并且我们已经看到了其优势：对开发者而言非常易用且熟悉，同时通过利用结构化和静态类型的API，减少了构建MCP服务器所需的工作量。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一举措也得到了社区的支持。至少从2025年4月起，开发者就开始呼吁，在&lt;a href=&quot;https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/1144&quot;&gt;GitHub的一次讨论（#1144）&lt;/a&gt;&quot;中，从业者们主张MCP从一开始就应该围绕gRPC构建，部分开发者在此期间已推出了自己基于gRPC的MCP服务器。2025年7月的一个&lt;a href=&quot;https://github.com/modelcontextprotocol/modelcontextprotocol/issues/966&quot;&gt;GitHub 问题（#966）&lt;/a&gt;&quot;获得了43个赞，开发者们指出，基于HTTP的JSON传输存在JSON序列化带来的高开销、资源监听时低效的长轮询，以及API契约缺乏类型安全性等问题。MCP维护者此后已经同意在SDK中支持可插拔得传输层，而谷歌计划自行贡献并分发gRPC传输包。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过在底层使用&lt;a href=&quot;https://protobuf.dev/&quot;&gt;Protocol Buffers&lt;/a&gt;&quot;替换JSON，可以显著降低网络带宽和CPU开销。对于已部署gRPC基础设施的企业而言，这意味着AI智能体可以直接与现有服务通信，无需额外添加转换层。Protocol Buffers的结构化、类型化契约也与大多数后端服务的定义方式更为契合。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但是，该提案并未完全解决一个现实的矛盾。在&lt;a href=&quot;https://www.aifire.co/p/mcp-vs-grpc-the-future-of-ai-native-agent-connectivity&quot;&gt;Medium上&lt;/a&gt;&quot;，有一篇对比MCP与gRPC的分析文章指出：“gRPC的服务反射提供了结构信息（方法名、参数），但缺乏LLM所需的语义化、自然语言描述（也就是‘何时’和‘为何’）。”MCP 从设计之初就是为了向AI智能体提供这类上下文，即工具描述、资源说明、提示词指导，而gRPC本身并不具备这一能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，更大的架构问题依然存在：MCP是应该适配gRPC这类现有的RPC系统，还是这些系统需要学习MCP的语言？从业者们对此意见不一。一些人认为，强制将运行良好的gRPC服务重写为JSON-RPC是完全不必要的麻烦。另一些人则认为，不能简单地将gRPC强加于一个以AI为中心的协议之上，而不添加LLM实际运行所需的语义层。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于将AI智能体投入生产环境的开发者而言，实际优势显而易见。那些已深度使用gRPC的企业（包括谷歌自身），它们“在全球范围内依赖gRPC来启用服务和提供API”，现在均可以直接采用MCP，而无需破坏现有的服务契约了。谷歌还为其自有服务推出了具有全球一致性端点的全托管远程MCP服务器，结合gRPC支持，使谷歌云能够直接面向那些已投资gRPC、希望添加AI智能体能力的企业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;gRPC传输层仍在开发中。谷歌正通过Python SDK中一个关于可插拔传输接口的活跃&lt;a href=&quot;https://github.com/modelcontextprotocol/python-sdk/pull/1591&quot;&gt;pull request&lt;/a&gt;&quot;，与MCP社区合作推进。如果开发者关注该领域的话，MCP的GitHub仓库和&lt;a href=&quot;https://modelcontextprotocol.io/community/communication&quot;&gt;贡献者频道&lt;/a&gt;&quot;是了解最新进展的主要渠道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/google-grpc-mcp-transport/&quot;&gt;&amp;nbsp;Google Pushes for gRPC Support in Model Context Protocol&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/IvpIeymHIWETqu4X7qo7</link><guid isPermaLink="false">https://www.infoq.cn/article/IvpIeymHIWETqu4X7qo7</guid><pubDate>Wed, 11 Feb 2026 08:00:00 GMT</pubDate><author>作者：Steef-Jan Wiggers</author><category>Google</category><category>AI&amp;大模型</category></item><item><title>Datadog 在其 LLM 可观测性工具中集成 Google ADK</title><description>&lt;p&gt;Datadog 近期&lt;a href=&quot;https://cloud.google.com/blog/products/management-tools/datadog-integrates-agent-development-kit-or-adk/&quot;&gt;宣布&lt;/a&gt;&quot;，其 LLM 可观测性平台已为使用 &lt;a href=&quot;https://google.github.io/adk-docs/&quot;&gt;Google Agent Development Kit (ADK)&lt;/a&gt;&quot; 构建的应用程序提供自动埋点功能，帮助用户更深入地洞察 AI 驱动型智能体系统的行为、性能、成本及安全性。该集成在 &lt;a href=&quot;https://cloud.google.com/blog/products/management-tools/datadog-integrates-agent-development-kit-or-adk/&quot;&gt;Google Cloud 博客&lt;/a&gt;&quot;上进行了重点介绍，旨在让开发者和 SRE 团队无需繁琐的手动配置或自定义埋点即可轻松监控和排查复杂的多步骤 AI 智能体工作流。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着企业越来越多地采用 ADK 等框架构建自主 AI 智能体，这些系统的非确定性特质使得预测输出、诊断故障和控制成本变得困难。Datadog 的新集成将 ADK 应用的信号接入其可观测性系统，使团队能够可视化智能体决策路径、追踪工具调用、测量令牌使用量和延迟，并标记出可能导致性能下降或 API 成本激增的意外循环和错误路由步骤。Datadog 通过将这些遥测数据与其他系统指标关联，帮助团队提升智能体的可靠性和运营信心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该集成还填补了智能体部署中的一个空白：虽然 ADK 为跨场景构建 AI 智能体提供了灵活的框架，但其本身缺乏针对生产环境的监控和治理工具。Datadog 的埋点功能通过自动追踪每个智能体的操作并将其呈现在统一的时间线上，填补了这一空白，使团队能够轻松定位工具选择错误或低效重试循环等问题，从而避免因这些问题导致延迟增加或令牌开销上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.datadoghq.com/product/llm-observability/&quot;&gt;Datadog 的 LLM 可观测性&lt;/a&gt;&quot;平台现在支持查看每个工具和工作流分支的令牌消耗及延迟情况，帮助识别智能体的异常行为和成本超支风险。这在企业环境中尤为重要，因为复杂的智能体编排往往涉及多模型、多工作流及外部系统集成，而传统应用性能监控难以应对以 AI 为核心的业务逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这一集成，Datadog 将其可观测性平台（已覆盖基础设施、安全和分布式系统）拓展至新兴的智能体 AI 应用领域，弥合了 AI 实验与稳定生产部署之间的鸿沟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其他可观测性厂商也在开发类似的集成功能，帮助企业更好地理解和使用 LLM：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://newrelic.com/&quot;&gt;New Relic&lt;/a&gt;&quot; 提供全栈可观测性和 APM，具备强大的分布式追踪和性能洞察能力，正通过扩展遥测关联和 AI 感知监控功能向 AI 可观测性演进。虽然它尚未拥有与 Datadog ADK 集成相同水平的专用 LLM 工具，但它为应用和基础设施提供了坚实的端到端可见性，帮助团队理解 AI 和智能体工作负载如何与系统的其他部分交互。New Relic 采用基于数据摄取量而非主机数量的定价模式，对关注成本的团队而言更具可预测性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Splunk 的可观测性产品（包括 &lt;a href=&quot;https://www.splunk.com/en_us/products/observability-cloud.html&quot;&gt;Splunk Observability Cloud&lt;/a&gt;&quot;）擅长高容量日志摄取和查询，在跨各类数据集的详细取证分析方面表现突出。然而，与 Datadog 深度集成的智能体可观测性特性相比，开箱即用地关联 AI 特定信号（如令牌消耗或模型决策路径）可能需要更多配置工作。Splunk 在处理大规模非结构化遥测和以安全为中心的监控方面表现依然强劲，但在没有自定义埋点或插件的情况下，其内置的 AI/智能体工作流功能可能相对滞后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕 AI 和智能体可观测性的新兴需求正推动各厂商持续升级其工具，聚焦运行时追踪、序列与路径可视化，以及 AI 工作负载的成本和延迟洞察，但各厂商均基于自身核心优势采取了差异化策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/datadog-google-llm-observability/&quot;&gt;https://www.infoq.com/news/2026/02/datadog-google-llm-observability/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ybR4DQTz6udDxBmKZpOi</link><guid isPermaLink="false">https://www.infoq.cn/article/ybR4DQTz6udDxBmKZpOi</guid><pubDate>Wed, 11 Feb 2026 07:00:00 GMT</pubDate><author>作者：Craig Risi</author><category>AI&amp;大模型</category></item><item><title>微软发布OData .NET（ODL）9.0.0预览版3：安全性、现代化API及规范遵从性</title><description>&lt;p&gt;微软&lt;a href=&quot;https://devblogs.microsoft.com/odata/announcing-odata-net-odl-9-preview-3-release/&quot;&gt;发布&lt;/a&gt;&quot;了OData .NET（ODL）9.0.0预览版3（这是OData .NET客户端和核心库的最新预览版本），延续了该库的现代化进程。这个预览版聚焦于更安全的默认行为、运行时API清理以及OData规范遵从性提升。OData .NET团队正朝着9.x的稳定版本努力推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OData .NET核心库（如Microsoft.OData.Core）当前的稳定版本仍然是NuGet上的&lt;a href=&quot;https://github.com/OData/odata.net/releases/tag/8.4.3&quot;&gt;8.4.x系列版本&lt;/a&gt;&quot;，其中，8.4.3是该系列的最新稳定版本。该稳定分支支持OData v4/v4.01，并且广泛应用于生产环境，而&lt;a href=&quot;https://github.com/OData/odata.net/releases/tag/9.0.0-preview.3&quot;&gt;9.x版本仍在预览当中&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版3延续了9.x早期预览版的约定，但根据开发者的反馈以及OData规范进行了以下几个方面的增强：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;动作查询执行现在使用SingleOrDefault()语义处理可空引用，在保留对非空值的严格检查的同时，减少了由常见的空响应所引发的意料之外的异常。移除了与ISerializable相关的旧序列化构造函数，消除了现代SDK上的构建警告。放弃了旧的CsdlTarget概念，并弃用了过时的返回类型访问器，转而支持更新的EDM接口。与IEdmOperation接口返回类型属性（ReturnType）相关的过时API也已被新的IEdmOperationReturn抽象完全替换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些变化反映了这样一种发展方向：与.NET 8/9/10运行时保持兼容、内存占用更低的分配模式（如添加ReadOnlySpan&lt;char&gt;查找重载）以及对平台内置API的依赖。&lt;/char&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;预览版3的一个关键行为变化是强制对非类型化值进行结构化类型反序列化（不再有ReadUntypedAsString切换），使运行时行为更接近&lt;a href=&quot;https://www.odata.org/documentation/&quot;&gt;官方的OData JSON格式&lt;/a&gt;&quot;。此外，未指定类型的数值现在默认推断为特定的CLR数值类型，并提供兼容性标志以支持旧版结果（即解析为decimal的数值）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从稳定的8.x系列版本升级到9.x预览版的NuGet包应被视为破坏性变更：开发者需要检查可空返回值处理、预期的非类型化JSON shapes以及对已移除的旧API的依赖。由于9.x版本仍处于预览阶段，不建议在没有仔细测试的情况下用于生产环境。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OData生态系统继续向前发展。举例来说，ASP.NET Core OData包独自进入了自己的9.x+系列（包括像&lt;a href=&quot;https://www.nuget.org/packages/Microsoft.AspNetCore.OData/9.4.1&quot;&gt;Microsoft.AspNetCore.OData 9.4.x&lt;/a&gt;&quot;这样的稳定版本），这表明服务端和客户端OData技术栈的相关工作正在并行推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有兴趣提供反馈或跟踪稳定化计划的开发者，可以关注&lt;a href=&quot;https://www.infoq.com/news/2026/01/odata-net-preview-9/odata.net&quot;&gt;OData/odata.net GitHub存储库&lt;/a&gt;&quot;和&lt;a href=&quot;https://devblogs.microsoft.com/odata/&quot;&gt;OData官方博客&lt;/a&gt;&quot;，获取预览公告、迁移指南和9.0最终稳定版的路线图动态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/odata-net-preview-9/&quot;&gt;https://www.infoq.com/news/2026/01/odata-net-preview-9/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/f0ev3d8fZtw2DaIdp85S</link><guid isPermaLink="false">https://www.infoq.cn/article/f0ev3d8fZtw2DaIdp85S</guid><pubDate>Wed, 11 Feb 2026 02:36:54 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>后端</category></item><item><title>OpenEverest：开源数据库自动化平台</title><description>&lt;p&gt;近日，Percona宣布推出&lt;a href=&quot;https://openeverest.io/&quot;&gt;OpenEverest&lt;/a&gt;&quot;，这是一个支持多种数据库技术的开源平台，用于自动化数据库配置和管理。该平台最初发布时名为Percona Everest，可以托管在任何Kubernetes基础设施上，既可以是云端也可以是本地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目的主要目标是避免供应商锁定，同时提供自动化的私有DBaaS。它基于Kubernetes operator构建，旨在避免依赖单一云供应商技术的复杂部署。OpenEverest是模块化的，允许开发人员和数据库管理员组合不同的数据库、存储系统和部署方法以满足特定的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aac5d2f8395d99b4d773040c671752c7.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;作为一个插件系统，其核心功能支持GKE Autopilot和Pod调度策略等特性。OpenEverest维护者、Solarica创始人&lt;a href=&quot;https://www.linkedin.com/in/sergeypronin/&quot;&gt;Sergey Pronin&lt;/a&gt;&quot;&lt;a href=&quot;https://openeverest.io/blog/welcome-to-everest/&quot;&gt;解释&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;目前，我们专注于数据库管理，但我们真正的愿景远不止于此。我们正在构建一个模块化的基础架构，让你可以无缝地集成更多的数据引擎，连接整个运维体系，从而应对更广泛的数据基础设施挑战。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目通过其Web UI和&lt;a href=&quot;https://openeverest.io/docs/api/1.10.0/&quot;&gt;REST API&lt;/a&gt;&quot;简化了软件更新、监控、存储扩展和外部访问配置等运维任务。自定义资源DatabaseCluster、DatabaseClusterBackup和DatabaseClusterRestore定义了OpenEverest如何在Kubernetes中声明式地配置数据库集群以及管理它们的备份和恢复，使这些操作可以作为版本化的原生Kubernetes对象进行处理，并隐藏了特定于数据库运营商的大部分差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当Percona推出该项目的测试版时，社区反响褒贬不一。在Hacker News上，这引发了一场关于在Kubernetes上运行数据库集群是否是个好主意的&lt;a href=&quot;https://news.ycombinator.com/item?id=41411122&quot;&gt;辩论&lt;/a&gt;&quot;：一些人对使用Kubernetes运行数据工作负载持怀疑态度，其他人则强调托管备份、集群、扩展、升级、优化的好处，其中有位用户指出，“Kubernetes不适合运行数据库”是一个非常过时的看法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，该项目支持通过各数据库引擎专属的&lt;a href=&quot;https://www.percona.com/software/percona-operators&quot;&gt;Percona operator&lt;/a&gt;&quot;部署和管理MySQL、PostgreSQL及MongoDB数据库集群。其功能涵盖数据库配置与扩展、备份及灾难恢复、基于角色的访问控制，以及在Kubernetes环境中灵活地分配资源。最新版本&lt;a href=&quot;https://newreleases.io/project/github/openeverest/openeverest/release/v1.11.0&quot;&gt;OpenEverest v1.11.0&lt;/a&gt;&quot;新增对PostgreSQL 18.1的支持，并通过NodePort支持实现了更灵活的网络配置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/openeverest/roadmap&quot;&gt;正在进行当中的工作&lt;/a&gt;&quot;包括支持ClickHouse、Vitess、DocumentDB、Valkey等工具以及集成Prometheus和其他可观察性平台。根据&lt;a href=&quot;https://vision.openeverest.io/&quot;&gt;项目愿景页面的介绍&lt;/a&gt;&quot;，其长期目标是为构建和运营数据平台提供一个灵活的开源选项，并充分利用Kubernetes的普及性：“根据Kubernetes的调查数据，已经有50%的组织在生产环境的Kubernetes上运行数据工作负载。”Pronin&lt;a href=&quot;https://openeverest.io/blog/welcome-to-everest/&quot;&gt;阐述&lt;/a&gt;&quot;了从单供应商解决方案向开源转型的过程：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;该项目正在转变为OpenEverest——一个采用开放治理模式、拥有蓬勃发展的多供应商社区的独立开源项目。（……）OpenEverest将通过社区驱动的开放治理模式运作，摆脱单一供应商的控制。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该团队计划将该项目捐赠给CNCF，以保证其长期的独立性，并继续指导其孵化过程。OpenEverest并非在Kubernetes上管理数据库集群的唯一选择。KubeBlocks是一款开源operator（遵循AGPL-3.0许可），设计用于通过统一的API管理多种数据库类型，它目前支持35种数据库引擎，远超OpenEverest；而作为数据库管理平台，KubeDB虽然支持多种数据库，但已不再完全开源。此外，StackGres等特定于数据库的operator则专注于为单一主流开源数据库引擎提供深度功能集。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenEverest遵循Apache License 2.0许可。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/openeverest-kubernetes-databases/&quot;&gt;https://www.infoq.com/news/2026/01/openeverest-kubernetes-databases/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QIisU8EsIh2itQlUSFj9</link><guid isPermaLink="false">https://www.infoq.cn/article/QIisU8EsIh2itQlUSFj9</guid><pubDate>Wed, 11 Feb 2026 02:34:03 GMT</pubDate><author>作者：Renato Losio</author><category>大数据</category><category>开源</category></item><item><title>预防数据泄露：在GCP上实施VPC服务控制的实践</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;云环境中数据窃取方面的挑战&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云计算革命彻底改变了应用程序的开发与部署方式。然而，传统的网络安全模式（即“&lt;a href=&quot;https://www.cloudflare.com/learning/access-management/castle-and-moat-network-security/&quot;&gt;城堡与护城河&lt;/a&gt;&quot;”方式）在云原生架构中就显得力不从心了。在云环境中，资源是分布式的、短暂的，并且可以从任何地方进行访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于迁移到公有云的企业来说，通过内部威胁、凭据盗取和服务配置错误所导致的数据窃取已成为一个重要的问题。行业报告显示，涉及云配置错误的数据泄露事件，每次给组织造成的平均损失为445万美元。在金融服务、医疗保健和受监管的行业，客户数据保护不仅仅是安全问题，更是合规性和法律的强制要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然本文主要关注Google Cloud Platform的VPC Service Controls (VPC-SC)，但其原则、挑战和最佳实践广泛适用于各大云服务商。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AWS通过VPC Endpoints和Service Control Policies提供了类似的功能，而Azure则提供了Service Endpoints和Private Link。虽然实现细节不同，但防止数据窃取的战略方法（全面发现、分阶段推出、组织协调和分层安全）是超越任何特定平台而共通的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Google Cloud Platform中，&lt;a href=&quot;https://docs.cloud.google.com/vpc-service-controls/docs/overview&quot;&gt;VPC-SC&lt;/a&gt;&quot;在敏感云资源周围创建安全边界，防止未经授权的数据窃取，同时保持云的敏捷性和可扩展性。然而，在企业规模上实施VPC-SC（跨越数百个项目、多个区域和多样化的应用程序）需要战略级的规划、组织协调，并且要对安全需求和操作限制有着深刻的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文分享了在大型金融科技组织的Google Cloud Platform (GCP)环境中实施VPC-SC的经验教训，以保护支付处理工作负载、客户数据分析和多区域部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文不会提供分步骤的配置指南，而是分享战略决策、组织挑战和来之不易的经验教训，这些因素决定了安全实施是否能够成功。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;深入理解VPC Service Controls：超越基础的周边安全性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC Service Controls会在GCP服务周围创建安全边界，强制执行基于资源位置、身份和网络来源的上下文感知访问策略。与在网络层操作的VPC防火墙不同，VPC-SC在服务API层进行操作，无论网络路径如何，都能控制对Google Cloud Storage、BigQuery、Vertex AI和Compute Engine的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它的三大核心构建块包括：&lt;/p&gt;&lt;p&gt;服务边界 (Service Perimeters)：在受保护的GCP项目和资源周围定义逻辑边界。边界内的资源可以自由通信，来自外部的访问则需要通过访问级别或入站/出站策略进行明确授权。访问级别 (Access Levels)：基于IP地址、设备状态、用户身份或地理位置定义访问边界内资源的条件，从而能够超越简单的允许/拒绝规则，实现上下文感知的安全性。入站和出站策略 (Ingress and Egress Policies)：指定哪些身份可以访问边界内的资源，以及边界内可以访问哪些外部资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一个常见的误解，那就是VPC-SC并不会取代网络安全，而是它的补充。实际上，如果VPC-SC配置得当，即使攻击者攻陷了VPC网络内的虚拟机，也无法将数据窃取到外部云存储桶中，无论网络连接情况如何，API调用都会被阻止。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个关键区别是，VPC-SC保护的是受支持的GCP服务，而不是任意的网络流量。Google Cloud Storage、BigQuery、Compute Engine、Vertex AI、BigTable、GKE等都能得到保护，但VPC-SC并不控制虚拟机的出站互联网流量，也不检查应用程序协议。与虚拟防火墙和Cloud Armor的集成对于全面安全性仍然至关重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构图&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e9fa02aacd83fc80372700b8dac5d7ea.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;设计VPC-SC架构：重要的战略决策&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;规划期间所做的关键设计决策决定了实施的成功或失败。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从数据分类开始&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并不是所有的数据都需要相同的保护。根据敏感性对数据进行分类，例如需要符合PCI-DSS的支付卡数据、受GDPR（General Data Protection Regulation）或CCPA（California Consumer Privacy Act）约束的个人身份信息 (personally identifiable information，PII)、机密业务数据和非敏感运营数据。这种分类会驱动边界的规划。高敏感的数据需要严格控制边界，允许的例外情况最少，而较低敏感的数据则允许更宽松的策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;建议分为三个边界层级：高安全性，用于支付处理，无出站策略；中安全性，用于客户分析，对出站访问受控的服务进行限制； 低安全性，用于开发/测试，策略较为宽松。这种方法在安全严谨性和运营灵活性之间取得了平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;彻底映射依赖关系&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;依赖关系映射不完整是VPC-SC实施失败的头号原因。现代云应用程序依赖于共享服务、跨项目通信、CI/CD流水线、监控工具和第三方集成。在执行之前，必须记录每一个依赖关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们建议使用Cloud Asset Inventory进行资源发现，并分析Cloud Logging以获取服务到服务之间的通信模式。同时，采访应用程序团队以了解日志中看不到的外部依赖关系。对于大型组织，建议为发现阶段预留四到六周的时间，过于匆忙容易引发生产事故。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;服务账户是隐藏的依赖噩梦：在多个项目中共享的某个服务账户会引发意想不到的跨边界依赖关系。在为我所在的组织实施VPC-SC期间，我发现了散布在遗留系统、批处理作业和第三方集成中的数十个未记录的服务账户。其中许多可以追溯到多年以前，而维护它们的团队早已离职。每次发现都需要进行仔细评估，以确定该账号的使用是代表合法的业务需求，还是需要补救的安全漏洞。这一经验强化了服务账户发现必须与资源发现需要一样严格的观点，忽略某几个身份标识可能会破坏你的整个边界策略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;边界拓扑：一个大边界还是多个小边界？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应该创建一个大的边界，还是按应用程序、业务部门或数据分类组织的多个较小的边界呢？答案取决于具体的组织结构和安全要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多个边界会提供更强的隔离性，某个边界的违规不会危及其他边界。然而，它们增加了复杂性，因为跨边界通信需要显式策略或边界桥接。我发现混合方法效果最好：按安全层级（高/中/低）组织的广泛边界，以及用于共享服务（如集中日志记录、监控和CI/CD）或多个边界之间入站/出站策略的边界桥接。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于多区域部署，要避免为每个区域创建单独的边界。区域边界会带来不必要的复杂性，而不会增加额外的安全价值。VPC-SC策略是全局应用的。建议将所有区域资源包含在单个逻辑边界内，并在需要时使用IAM策略或访问级别进行特定于区域的访问控制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;实施：从设计到生产的三个阶段&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第一阶段：发现与基线（四到六周）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发现工作不仅仅涉及技术资产盘点，还需要理解团队的工作方式、应用程序的通信模式以及安全漏洞的位置。为此，我组建了一个跨职能的工作组，成员包括安全工程师、基础设施团队、应用程序开发人员和业务利益相关者，我们每周开会，以审查发现结果、解决依赖性问题，并就边界范围达成共识。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术发现会利用多个来源：Google的Cloud Asset Inventory用于资源发现，Cloud Logging用于API模式，并且要采用团队访谈的方式来获取GCP日志无法捕获的上下文。我创建了可视化工具来映射服务的依赖关系，使得识别应该在边界内分组的资源集群变得更容易。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第二阶段：演练模式实施（至少六到八周）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC-SC的演练模式（Dry-run Mode）使安全实施成为可能。在演练模式下，会评估边界策略，违规行为会被记录，但API调用不会被阻止。这种方法允许我们在生产环境中测试边界配置，而不会产生服务中断的风险。我的建议是，将初始的边界配置以演练模式部署至少30天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每日分析违规日志，按服务、方法和主体对违规行为进行分组，以识别模式。特定服务账户产生的大量违规可能表明存在未被发现的合法用例，或者需要限制权限过高的凭证。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;区分违规的性质：并非所有违规都是同等重要的。例如，被VPC-SC阻止的BigQuery读取操作可能会破坏关键的分析仪表板；而服务向外部存储桶写入数据的行为，可能正是我们要防止的数据窃取。建立违规分类（比如，需要调整策略的合法用例、可接受的已记录风险、需要补救的安全漏洞），并通过这个框架处理每一个违规行为。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;建立自动化仪表盘，显示违规趋势。违规数量的下降表明针对合法用例的政策调整取得了成功，而稳定或增加的违规行为则表明持续有依赖关系被发现，或者团队正在寻找绕过（尚未强制执行）控制措施的解决方法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第三阶段：强制执行与运维管理&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;强制执行的决策应该以数据为驱动：违规行为应该被分类和批准，测试并记录回滚程序，并获得受影响团队的利益相关者签字。强制执行是逐步进行的：首先是开发环境，然后是预发布环境，最后是生产环境。每个环境强制执行两周，然后再进入下一个环境，以确保意外问题会首先在风险较低的环境中出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;强制执行后，应该建立明确的例外请求流程：开发人员会遇到被边界策略阻止的合法场景。例外流程必须在安全性（不能授予破坏控制措施的全面例外权限）与敏捷（避免创建官僚式的冗长申请机制）之间取得平衡。在我的项目中，我创建了分层的例外机制：临时（72小时，由安全团队批准）、永久（需要安全架构审查）和书面申请。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;现实世界中的挑战与解决方案&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;BigQuery分析中断事件&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在强制执行数据分析边界三周后，我们的商业智能仪表板停止了更新。调查发现，边界外的服务账户正在访问BigQuery数据集。这个依赖关系在演练测试中被遗漏了，因为相关的批处理作业每月才运行一次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们的紧急修复是创建一个临时的出站策略，允许特定的BigQuery进行操作，而长期解决方案则涉及重构批处理作业以使用边界内的服务账户，并更新依赖关系的文档。这一样例再次向我们重申，演练周期必须跨越完整的业务周期，并且自动化的依赖关系发现应该补充而不是替代人类的知识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;平衡安全性与开发人员的生产力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最大的挑战并非技术层面，而是在组织层面。起初，开发人员将VPC-SC视为阻碍工作且无明显收益的障碍。一些人甚至试图通过在服务前使用&lt;a href=&quot;https://docs.cloud.google.com/vpc/docs/private-service-connect&quot;&gt;Private Service Connect (PSC)&amp;nbsp;&lt;/a&gt;&quot;来绕过边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方案需要改变我对安全性的沟通方式。我不再将VPC-SC呈现为一种限制，而是将其框架化为一种工具，保护他们的应用程序免受数据泄露，保护公司免受监管处罚，并保护他们的团队免受与安全事件相关联的风险。我的团队在自助工具上投入了大量资源，建立了一个网络门户，开发人员可以在其中检查服务账户访问权限、请求具有明确承诺的例外，并查看政策违规的解释和建议的补救措施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当开发人员看到安全团队迅速响应合法需求，同时在不必要的例外方面保持坚定的边界时，他们的情绪也从抵制转变为接受。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;传统应用重构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对本地环境设计的遗留应用程序通常会假定对存储和数据库都能够无限制地进行访问。一个从本地迁移过来的支付处理应用程序，几乎没有做任何更改，就试图将日志写入另一个GCP组织中的云存储桶，这在本地环境中是合理的模式，但在云安全边界中却是违规的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们并没有为这个应用程序创建一个例外来允许跨组织的数据传输，而是与应用程序团队合作重构了日志记录。现在，日志写入边界内的一个桶中，并且一个授权的导出过程将经过清理的日志移动到另一个组织中的桶中进行合规归档。这种重构花了六周时间，但提高了安全态势并减少了运营复杂性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当VPC-SC阻止操作时，首先问一下自己，“这应该是被允许吗？”而不是问，“我们该如何允许它？”有时候被阻止的操作代表了需要补救的技术债务，而不是需要适应的现状。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;将VPC-SC融入更广泛的云安全架构中&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;采用VPC Service Controls是全面云安全架构的一部分，而非独立的解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;分层的防御模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将云安全视为一个同心层，需要在不同的层次提供保护。VPC-SC在服务API层运行，控制对GCP服务的访问。虚拟防火墙（例如Palo Alto Networks VM-Series）在网络层运行，控制IP流量并检查应用程序协议。Cloud Armor在应用程序层提供分布式拒绝服务（DDoS）保护和Web应用程序防火墙（WAF）功能。身份和访问管理（IAM）在资源级别控制基于身份的访问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个层次都会捕捉不同的威胁向量。攻击者攻陷虚拟机后，即使他们已经绕过了网络级防火墙规则，可能也会被VPC-SC阻止通过云存储API进行数据窃取。DDoS攻击可能会在压垮虚拟防火墙之前被Cloud Armor缓解。即使攻击者已经进入了VPC网络和边界内，被盗凭据可能会被IAM的上下文感知访问策略检测到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC-SC为敏感资源定义了广泛的安全边界，虚拟防火墙提供了细粒度的网络流量控制和协议检查，Cloud Armor保护面向互联网的应用程序，而IAM在边界内强制执行最小权限访问。单一的控制措施是不够的，安全性来自它们的相互作用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;监控与事件响应&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC-SC会为每次策略评估生成审计日志，从而为安全监控创建了丰富的数据。这些日志可以流式传输到Splunk等安全信息和事件管理（Security Information and Event Management，SIEM）平台进行分析和警报，或者使用原生GCP日志进行分析。关键的监控场景包括：&lt;/p&gt;&lt;p&gt;策略违规异常激增表明可能存在攻击或配置错误。同一主体重复违规可能表明存在合法的访问问题或侦察行为。出站违规访问敏感数据或外部存储桶可能表明正在尝试进行数据窃取。VPC-SC配置的变更应该仅通过批准的基础设施即代码流程进行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;针对VPC-SC检测到的数据窃取尝试的事件响应剧本包括：立即调查源主体和目的地，如果确认攻击则暂时收紧边界政策，进行Cloud Logging的取证分析以确定访问的数据，进行事后审查以确定事件是由安全漏洞还是成功攻击引起的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;衡量是否成功：指标与KPI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;量化VPC-SC的影响需要定义业务、安全和运营指标。我们项目中的测量指标包括：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;安全指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在前六个月内阻止了847次尝试进行数据窃取。通过逻辑隔离，PCI-DSS审计范围减少了40%。数据访问异常检测时间减少了45%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;运维指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在稳定后，部署时间增加不到5%。标准例外的平均处理时间为四小时。通过自动化，策略管理时间减少了60%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务指标&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预计每年因为避免数据泄露减少了450万美元的损失每年合规成本节约20万美元实施成本为80万美元（12名工程师耗时6个月）在18个月内实现了正投资回报率（ROI）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最佳实践与经验教训&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;服务发现永无止境，即便采用穷举式的发现方法，也要预期会有意料之外的依赖关系。将服务发现过程视为持续进行的行为，维护依赖关系待办事项列表，安排季度审查，并要求对所有新部署进行依赖关系文档编制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;演练时长至关重要，30天的演练测试是最低要求，而不是目标。对于具有每月批处理作业、季度报告周期或季节性流量模式的应用程序，要延长演练时间以捕获完整的业务周期。一周的强制执行延迟与生产中断的成本相比是微不足道的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;例外流程决定了成败，你的例外流程决定了VPC-SC是增强还是阻碍。明确的时间承诺、透明的批准标准和自助式请求提交，能够使开发人员将安全视为合作伙伴而非障碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;全面自动化，手工化的策略管理难以为继。对于所有的VPC-SC配置均应使用基础设施即代码（例如Terraform），或者构建一个自助工具，可以使用Cloud Functions添加策略。在生产部署之前实施自动化验证测试策略更改。自动化的预部署验证可以在生产之前捕获策略冲突。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;沟通能够消除阻力，技术卓越并不意味着能够被采用。我在沟通和利益相关者管理方面花的时间和技术实施一样多。制定定期的办公时间解释为什么VPC-SC能够保护每个人，带有示例和故障排除指南的清晰文档，对被VPC-SC阻止的开发人员积极进行响应，将组织文化从抵制转变为支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来改进的方向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;VPC Service Controls代表了当前GCP数据窃取预防的最佳实践，但威胁和技术仍在不断发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;将VPC-SC与零信任原则对齐，明确验证的合规性，使用最小权限进行访问，并假定存在漏洞。未来的演进应该加强基于实时风险评分的动态访问级别，与身份威胁检测进行集成，在检测到可疑行为时及时撤销访问，以及基于威胁情报自动进行边界策略调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当前的IaC方式将安全策略视为静态配置。下一代的方法会将策略视为可测试、版本化的代码，并进行自动验证和部署。我们正在朝着策略测试、针对模拟攻击场景验证边界有效性、策略漂移检测（当部署的配置与批准的基线发生偏离时发出警报）以及策略影响分析（在部署之前预测对开发人员生产力的影响）的方向演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在企业级规模上实施VPC Service Controls表明，成功的安全不仅仅是技术，更关乎人员、流程和组织文化。VPC-SC在技术方面有着良好的口碑，并且相对简单。困难之处在于理解组织的独特需求，驾驭复杂的依赖关系，获得利益相关者的支持，并在实现业务敏捷性的同时保持安全严谨性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;核心原则超越了任何特定技术。安全的作用应该是促进而不是阻碍；严重损害生产力的控制措施需要规避。你应该设计避免不必要摩擦的安全防护，自动化可以进行大规模扩展，而手工过程难以做到这一点，因此要投资工具，使安全实践成为最简单的路径。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;指标很重要。无法衡量就难以改进：跟踪安全和运营的影响。完美是完成的敌人，请现在就部署有效的安全控制，而不是等待永远不会实现的完美控制。采用持续改进的方式，而非试图毕其功于一役。安全不是目的地，而是一种不断适应和完善的持续实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;VPC Service Controls为GCP环境提供了强大的数据窃取预防机制，但其有效性取决于详尽的设计、分阶段实施、组织协调以及与更广泛安全架构的集成。愿意投资于全面规划、接受迭代改进，并在安全与可用性之间取得平衡的组织，将发现VPC-SC是云安全战略中非常有价值组成部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;威胁环境将继续演变，防御措施必须相应地发展。最重要的不是任何单一的技术，而是建立组织能力来评估风险、实施适当的控制措施、衡量效果，并持续改进能力，以服务于采用任意云平台或安全技术的组织。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/preventing-data-exfiltration-google-cloud/&quot;&gt;Preventing Data Exfiltration: A Practical Implementation of VPC Service Controls at Enterprise Scale in Google Cloud Platform&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QaR8toToJhEANT6gdQ6s</link><guid isPermaLink="false">https://www.infoq.cn/article/QaR8toToJhEANT6gdQ6s</guid><pubDate>Wed, 11 Feb 2026 02:31:35 GMT</pubDate><author>作者：Shijin Nair</author><category>大数据</category></item><item><title>千问发布最新图像模型Qwen-Image-2.0，支持1K token超长文字输入和2K高分辨率</title><description>&lt;p&gt;2月10日，阿里巴巴正式发布新一代图像生成及编辑模型Qwen-Image-2.0。据介绍，Qwen-Image-2.0集生图和编辑于一体，在AI Arena文生图评测中斩获1029分，超过Seedream4.5、Flux2-Max等模型，仅次于谷歌Nano Banana Pro和GPT Image1.5。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/83/83cfb6dd0c5fbd8506d351ef9bb95cfc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;AI Arena文生图评测中，Qwen-Image-2.0位居第三&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0支持1K token的超长文字输入和2K高分辨率，可准确渲染复杂指令，生成专业的PPT及信息图；同时，千问新模型拥有极强中文汉字渲染能力，数百字的古文全文几乎都能完全渲染在图片中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0在Qwen-Image和Qwen-Image-Edit两大模型基础上全新升级，首次将图像生成和编辑统一到一个模型中去，以更轻量的模型架构，实现了生图和改图性能的大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0 生图质感进一步提升，生成的人物、自然、建筑等常用图片更加逼真。在权威评测AI Arena中，千问新模型在图像生成中得分1029，位列第三；在图片编辑中得分1034，仅次于Nano Banana Pro。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/170cafb63ad908bde8359a00384f463f.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0生图，以瘦金体写诗配图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在中文汉字渲染方面，官方表示Qwen-Image-2.0 不但可以以多种字体准确渲染汉字，而且写得又多又准，效果比 Nano Banana Pro更优。千问新模型将输入提示词扩展到1K token，可详尽描述任务，实现更专业的文字渲染，在专业PPT、高级海报、多格漫画等复杂图片方面有不错表现，比如以小楷字体几近完全渲染《兰亭集序》数百字的全文配图，以自然语言生成论文格式配图的复杂PPT等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/da/da0795cfaf18b0dea35ee0ded31a16e6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0生图，多文字复杂PPT一键生成&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，基于Qwen-Image-2.0模型，用户可与AI协同创作出更丰富、更实用的图片，比如一句话生成宫保鸡丁的做法流程图，杭州两日旅游攻略图，4x6的多格漫画组图，儿童绘本图，写实风格的电影海报，极为逼真的绿色丛林等等；同时，用户也可上传数张图片进行编辑，生成诸如九宫格多手势自拍，真人配字表情包，双人逼真AI合影，诗词配图等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a6a92a9fd10000108fdab5f3917ea14.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Qwen-Image-2.0编辑图片&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据了解，阿里云百炼上已开通API邀测，开发者也可通过Qwen Chat免费体验新模型。&lt;/p&gt;</description><link>https://www.infoq.cn/article/b8LJJIs08XT0h9dRflsH</link><guid isPermaLink="false">https://www.infoq.cn/article/b8LJJIs08XT0h9dRflsH</guid><pubDate>Wed, 11 Feb 2026 02:09:10 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>模力工场 032 周 AI 应用榜：桌面 Agent 强势来袭，阶跃登顶本周榜首</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;新鲜事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/?utm_source=20260210infoQ&quot;&gt;模力工场&lt;/a&gt;&quot;邀你用 AI 一键生成新年财运红包封面！2月5日至25日，设计松鼠 × 模力创意红包，即可赢金币参与多轮现金抽奖。扫码进群，马上开启你的开年好运！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/cafd85344e3eb0dfc0714172e79b1126.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;032 周上榜应用精选（附用户热评）&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场 32 周 AI 应用周榜来啦～本周共有 25 款应用上架新榜，所有排名均来自用户真实使用、测评与社区讨论热度。本期用户讨论最高的是：桌面 Agent 形态的出现。AI 正在从“对话框里的助手”，走向“接管桌面的执行者”。AI 开始在真实桌面环境中，操作网页、处理本地文件、生成办公文档，甚至能把多个应用里的任务一口气跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们从中精选出十款最具声量的应用，聚焦五大垂直领域，为你更详细地解读榜单背后的 AI 行业风向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;一、桌面 Agent 类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：接管桌面｜跨应用操作｜真实执行&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/stepfun-desktop?utm_source=20260210infoQ&quot;&gt;阶跃AI桌面伙伴 📍上海&lt;/a&gt;&quot;：一个更懂中文办公的国产桌面 AI 伙伴，无需复杂设置，全平台支持，深度整合钉钉、飞书等本土工具，用截图提问、智能整理、定时任务等贴心功能，为你打造真正懂中文、懂场景、懂流程的下一代智能工作台。&lt;/p&gt;&lt;p&gt;    &lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/36/3660388f8cfbad8960e33fd0215d44f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/workany?utm_source=20260210infoQ&quot;&gt;WorkAny 📍广州&lt;/a&gt;&quot;：艾逗比开发的开源跨平台桌面智能体，可以通过安全沙盒执行各类脚本，无缝处理文件整理、文档生成、网页制作等办公任务，更支持自定义模型与并行处理，用本地订阅打造你的专属 AI 生产力中心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/72fd4ee7cf65a17a99ff9138019d141c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;二、学习 / 知识管理类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：结构化学习｜知识转写｜理解与记忆&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/chatglm?utm_source=20260210infoQ&quot;&gt;智谱清言 AI 学习搭子&lt;/a&gt;&quot;：植入在智谱清言生态中的学习辅助模块，擅长把教材、文档和概念转化为知识地图、卡片和讲解内容，并配套随堂测试，更偏“陪伴式学习”和知识消化。&lt;a href=&quot;https://agicamp.com/products/thetawaveai?utm_source=20260210infoQ&quot;&gt;Thetawave AI&lt;/a&gt;&quot;：偏重输入端的学习整理工具，支持录音、视频、文档、网页等多源内容转写，并生成结构化笔记、思维导图和测验，适合学生和知识工作者做系统性复盘。&lt;a href=&quot;https://agicamp.com/products/notebooklmgoogle?utm_source=20260210infoQ&quot;&gt;Notebook LM&lt;/a&gt;&quot;：Google 推出的研究型笔记工具，更偏“资料理解与问答”。围绕用户上传的 PDF、网页、视频等材料进行摘要、提问和交互式研究整理，适合研究与长期项目。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三、内容与视频创作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：内容工业化｜全流程生成｜效率提升&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/daoying?utm_source=20260210infoQ&quot;&gt;道影 AI 📍杭州&lt;/a&gt;&quot;：AI 视频全链路生产平台，面向短剧、漫剧等专业内容创作者。从剧本到成片一体化设计，强调流程贯通与规模化生产，而非单点创意工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;四、开发 / 编程协作类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：Vibe Coding｜一体化开发｜任务式编程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/zaizhi?utm_source=20260210infoQ&quot;&gt;OpenCode&lt;/a&gt;&quot;：为 Vibe Coding 场景设计的 AI 编程工具。把聊天、代码编辑、文件树和终端放在同一界面，支持 skill 封装与多模型切换，对编程新手非常友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【用户热评】：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/0398aaa68bffa221cef5510894d6bccb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;五、专业与底层能力类&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：专业生成｜算力平台｜企业与垂直场景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://agicamp.com/products/mureka?utm_source=20260210infoQ&quot;&gt;Mureka V8&lt;/a&gt;&quot;：昆仑万维推出的 AI 音乐生成平台，从自然语言或歌词直接生成结构完整、编曲成熟、人声自然的音乐作品，面向专业音乐创作场景。&lt;a href=&quot;https://agicamp.com/products/Prism?utm_source=20260210infoQ&quot;&gt;Prism&lt;/a&gt;&quot;：OpenAI 的 Prism 是一个不错的学术写作结构梳理与格式排版工具。它尤其适合在开题与文献综述阶段，帮你将思路系统化、可视化，并接手繁琐的 LaTeX 排版与参考文献管理。&lt;a href=&quot;https://agicamp.com/products/lanyun?utm_source=20260210infoQ&quot;&gt;蓝耘元生代 📍北京&lt;/a&gt;&quot;：以自研 MetaGen 智能算力操作系统为核心，面向企业提供集算力调度、模型服务与数据生成于一体的智算云平台，支撑 AI 应用落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;榜单之外但有趣的应用&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【应用名称】：&lt;a href=&quot;https://agicamp.com/products/Flora?utm_source=20260210infoQ&quot;&gt;Flora&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【关键词】：节点式创作｜无限画布｜创意工作流&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;【模力小A推荐】：Flora 是一款节点式创意 AI 平台，通过“无限画布”把文本、图像和视频生成串成可复用的工作流，适合品牌视觉、广告概念等跨媒介创作场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本周上榜应用趋势解读&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从本期榜单可以清晰看到一个信号：AI 的主战场正在从“会不会回答问题”，转向“能不能把事做完”。桌面 Agent 的集中出现，是这一变化最直观的体现。相比以往停留在对话框里的助手，本周讨论热度最高的产品，已经开始直接接管桌面环境，真实操作网页、处理本地文件、生成办公文档，甚至跨多个应用连续执行任务。用户关注的核心不再是模型能力，而是执行稳定性、流程完整度和对真实工作场景的适配程度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，学习、内容创作和编程类应用的演进路径也在发生变化：它们不再强调“单次生成”，而是围绕结构化理解、完整流程和长期使用进行设计。无论是学习工具对多源资料的系统整理，还是内容平台对从创意到成片的全链路打通，本质上都在向“可持续使用的生产力工具”靠拢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体来看，本期周榜反映出的并非某一个爆款应用，而是一种明确趋势：AI 正在从能力展示，进入到执行与交付阶段。谁能真正嵌入用户的工作流，承担连续、可验证的任务，谁才更有可能成为下一阶段被长期留下来的 AI 应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后再介绍一下模力工场的上榜机制和加入榜单的参与方式，欢迎大家继续积极参与提交 AI 应用～&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模力工场AI 应用榜并非依靠“点赞刷榜”，而是参考以下权重维度：&lt;/p&gt;&lt;p&gt;评论数（核心指标，代表社区真实反馈）&lt;/p&gt;&lt;p&gt;收藏与点赞（次级指标）&lt;/p&gt;&lt;p&gt;推荐人贡献（注册推荐人可直接为好应用打 Call）&lt;/p&gt;&lt;p&gt;加入榜单的参与方式：&lt;/p&gt;&lt;p&gt;如果你是开发者：上传你的 AI 应用，描述使用场景与核心亮点；&lt;/p&gt;&lt;p&gt;如果你是推荐人：发现好工具，发布推荐理由；&lt;/p&gt;&lt;p&gt;如果你是用户：关注榜单，评论互动，影响榜单权重，贡献真实声音。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One More Thing，对于所有在模力工场上发布的 AI 应用，极客邦科技会借助旗下各品牌资源进行传播，短时间内触达千万级技术决策者与开发者、AI 用户：&lt;/p&gt;&lt;p&gt;InfoQ 全媒体矩阵&lt;/p&gt;&lt;p&gt;AI 前线全媒体矩阵&lt;/p&gt;&lt;p&gt;极客时间全媒体矩阵&lt;/p&gt;&lt;p&gt;TGO 鲲鹏会全媒体矩阵&lt;/p&gt;&lt;p&gt;霍太稳视频号&lt;/p&gt;</description><link>https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</link><guid isPermaLink="false">https://www.infoq.cn/article/5MpkYtE3SNEXSvkAYM03</guid><pubDate>Tue, 10 Feb 2026 12:00:00 GMT</pubDate><author>霍太稳@极客邦科技</author><category>AI&amp;大模型</category><category>AGICamp</category></item><item><title>为 ChatGPT 和 Claude 提供“地基”的那家公司，在担心什么</title><description>&lt;p&gt;过去一年，关于 AI 的讨论出现了一种明显的反差：一边是模型能力不断刷新上限，另一边却是越来越多企业开始质疑——为什么真正落地依然这么难？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从概念验证到生产系统，从 60% 的“看起来可用”到 99.99% 的“不得不可靠”，企业级 AI 面对的从来不是算力或参数规模的问题，而是数据、决策责任、合规流程以及现实系统复杂性。而这些恰恰是大多数新闻叙事里最容易被忽略的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样的背景下，这期播客给出了一种罕见的“现实视角”。对话并没有继续渲染模型能力的指数级增长，而是把焦点放在一个更基础、也更棘手的问题上：AI 要真正进入企业和关键业务流程，还缺什么？答案指向了一个长期被低估的环节——数据，以及数据背后的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本期节目的对话者，正是站在这一环节核心位置的人。他所领导的公司，长期为几乎所有一线大模型实验室提供训练所需的基础数据；而在加入这家公司之前，他曾把一个看似边缘的想法，在极短时间内推演成一家年收入 200 亿美元的业务，也亲身经历过科技创业中最极端的法律与商业风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这场对谈中，他系统性地拆解了几个被反复误解的问题：&lt;/p&gt;&lt;p&gt;为什么大模型至今仍然离不开人类专家？&lt;/p&gt;&lt;p&gt;为什么绝大多数企业数据对 AI 来说毫无价值？&lt;/p&gt;&lt;p&gt;以及，当模型开始转向“智能体”和决策能力时，真正的瓶颈到底在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下是播客整理翻译：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;精华摘要&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您在数据、标记、训练等领域都是当之无愧的技术先驱，您能不能展望一下AI领域在过去一年半以来的发展轨迹？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：一年半之前，大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;收购之后，Scale 还是 Scale 吗？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：欢迎今天到场的嘉宾Jason Droge，请先简单向大家介绍一下您的背景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是Scale AI的新任CEO，这也是我在接替Alex Wang接受Meta收购之后参与的首次采访。Alex现在领导Meta旗下的超级智能团队。在加入Scale AI之前，我与Travis Calendar曾共同创立一家公司，再向前追溯还在Uber等几家初创公司工作。我最知名的成果应该是创立并领导了Uber Eats，跟团队的同事一道把这个点子培养成了如今市值数十亿美元的企业。在COVID期间，Uber Eats几乎是以一己之力支撑起了因社交隔离而陷入瘫痪的Uber业务体系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：这次采访的主题是AI模型如何拥有真正的智能。您觉得Scale AI在其中扮演了什么角色？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们在ChatGPT和Claude身上看到了诸多改进。目前各个前沿领域都存在领军级别的模型，各家实验室则聘请专家填补这些大模型的知识空白，校正其对于事物运作方式的理解。而Scale是这一领域的先驱，也可以说是创造了这种业务形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：我们都很关心Scale的近况以及被Meta收购之后的变化。Scale目前情况如何？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：Scale仍然是一家完全独立的公司。在此次交易中，Meta投入140多亿美元以换取Scale公司49%的无投票权股份，且未获得新的董事会席位。Scale的董事会保持不变，治理结构几乎未肥影响，Meta对于Scale的任何资源也都不具备优先访问权。我们跟Meta一直在数据业务方面保持着长期合作关系，随着双方关系更进一步，各方面合作也有望持续扩大。但我们与其他各方的合作不会受到影响，Meta无法访问任何之前不对其开放的信息，例如隐私和数据安全政策等。事实上，此次交易只涉及约15位员工的变动，而Scale共拥有约1100名员工。现在我们旗下拥有两大业务部门，其营收都达到了数亿美元规模。公司内部相当于两家独角兽，支撑着每月的业务增长。总之，我们很高兴能够继续构建并交付数据，维持之前的工作模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数据不是苦力活：标注为何变成专家工作&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到 Scale主要面向AI数据市场，那能不能解释一下数据标注工作是怎么从当初的低成本劳动力转向如今的专家处理形式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：没问题。首先，我认为其他竞争对手目前的定位是错误的，所以我先从这个角度切入，再逐渐延伸其他方面。这里我先花点时间介绍一下Scale的发展史，还有自2016年以来的发展脉络。Alex很早就意识到，数据对于模型来说至关重要。那时候他只有19、20岁，但他已经在考虑要如何围绕这个基本前提建立业务。他最初选择的方向是为自动驾驶做标注。标注数据的质量越高，汽车的行驶表现也会更好。之后这股浪潮演变成了计算机视觉，我们开始跟国防部门建立合作关系，为他们提供标注服务，到这里时间已经来到2020年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来大模型的性能越来越强、愈发完善，需要的数据类型也更为丰富。所以我们一直在不断调整以提供所需的数据类型。在此期间，行业本身也在不断变化。记得两、三年前这些大语言模型刚出现的时候，经常会闹出幻觉问题，比如给出特别浅显的错误答案等等。但情况变化很快，我们也一直在随之改变。Scale一直走在前沿，开始为更复杂的任务提供专家级的数据标注服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要聊过去一年半的情况我其实不太够格，因为我才加入公司13个月。刚加入时，我一直在做模型性能的测试。那时候大模型的能力极限就是生成一本短篇小说，不同模型生成的小说之间有优劣之分。而现在的大模型已经能帮哪怕最顶尖的Web开发者直接生成完整网站了，或者是就癌症诊疗问题给出相当全面且中肯的建议，而这些都需要专业人士耗费几个小时为其提供准确的统计数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的专家中，有80%的同事拥有学士学位证书，这跟其他竞争对手的定位完全不同。其中约有15%的员工对相关行业有深入了解。这些高知人群通过为模型添加标签、贡献专业知识来赚取丰厚的收入。我很喜欢我们这种以专家级别进行数据标注的业务定位，这能帮助公司与研究人员保持联系、了解他们的需求。我们内部也很早意识到大模型在高度专业的领域上表现欠佳，所以我们会主动联系开发基座模型的大厂，表示我们注意到了这个问题，而且有专家团队可以搞定。我喜欢这种与众不同的定位，这跟竞争对手的想法完全不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那Scale是怎么接洽并挽留这些专家贡献群体的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：确实不太容易接触得到，所以得制定相应的策略。具体的方式肯定不止一种，最主要的就是让专家们相互内推，而且他们很喜欢这种用自己的专业知识为AI做贡献的感觉，很酷。比如一位特定领域的博士在面对具体主题时，发现大模型的表现根本无法令人满意。那这时候他就可以通过有偿的方式提供专业建议，并借此赚取数百甚至数千美元。当然，我们也会推动校招，直接跟学校里的教授和学生们交流，询问有没有人愿意参与进来。当然，LinkedIn等传统渠道也是开放的，但效果最好的还是线下接洽和内推网络。这样能够为参与者提供良好的体验，因为他们的贡献一方面是为了赚钱，但更多是出于为AI模型做贡献的成就感。而且这个过程也是在替他们自己解决问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;智能体要学会做事：没有捷径，只有环境&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：有报道认为整个AI经济生态都将转向强化学习，您对此有何看法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：强化学习当然非常重要，我觉得这种趋势性判断也很有道理。强化学习环境就相当于AI智能体的沙箱，它们可以在沙箱中学会如何达成目标。我们在这方面也尝试了一年有余。比如在Salesforce实例当中，AI智能体要如何实现导航？襳中包含哪些需要识别的数据？这要求智能体执行一套可靠性极高的业务流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，智能体还得知道如果无法完成预期任务，或者判断正确完成任务的可能性较低，那要怎么向人类反馈以获取指引。所有这些都需要训练，而且不存在什么神奇的捷径。唯一的办法就是把AI智能体放进能代表人类正确操作的环境当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以想象，现实世界中此类环境的数据和其中的不同目标可以说是无穷无尽，所以我们花了一年多跟模型开发商保持良好的合作关系，共同观察在不同任务/环境下的通用性表现。很明显，这类环境、软件系统、配置、数据类型、规模和用户数量各有不同，复杂性也差异巨大。这&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就要求我们制定一种策略，让模型开发商能够收集到足够的通用数据以支撑广泛用例，而不必直接面对上万亿种任务和环境指标的排列组合。有些工作和数据之间具有更强的通用性，可以用一种简单的方式完成任务——比如在日历上找到要参与的访谈，让智能体浏览日历内容并弹出相应提示。接下来要做的，就是把智能体推广到一切日历搜索和日程管理操作。总之数据通用性越强，价值也就越大。我们的工作就是为模型开发商提供最有价值的数据，从而确保智能体尽可能为最终用户提供良好服务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;企业AI不是演示：从 95% 到‘五个9’，差着一个世界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：那您能举例聊聊具体向模型实验室提供哪些数据吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以。比如说我们的业务分为两个方面：其一是向模型开发商提供数据，也就是出售数据。其二则是向医疗保健系统、保险系统之类客户出售应用程序和服务解决方案。比方说我们跟一家医疗保健系统开发商合作，这套系统目前存在很多问题，部分专家需要定期处理少数罕见病例。因为专家人手不够，所以罕见病例会大量积压。这家医疗保健机构希望接诊更多病人，提供更好的护理体验并减少复诊次数，也就是在第一时间给出准确的诊断并制定治疗方案。如果没有AI的帮助，医生得耗费大量时间阅读长达两、三百页的病历文件。而我们开发的工具能帮助他们阅读这些文件，并指出其中最值得注意的五到十条内容。举例来说，某些过敏症状看似不起眼，但却可能跟医生开具的治疗药物发生冲突。AI工具可以快速提取这种人脑难以容纳的关联性要素，表现出相当完善的诊疗能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这方面，现成的模型肯定会有一定局限性，迫使医疗保健系统内部的人员亲自进行数据标注。不少企业乃至政府部门也会这么做，但仅靠现成模型加上一些零散数据没办法实现特别好的效果。毕竟很多银行或者医院一年的数据量就多达上百PB，他们自己根本没法判断哪些数据对模型有用。大多数数据都没价值，可怎么从中挑选出少数有价值的？而作为专业服务商，我们在数据的评判、挑选和专业知识储备方面非常出色，能够帮助客户顺利攻克这些瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：要保证AI变得越来越聪明，咱们人类到底还要参与多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：首先，数据标注本身就是一段不断创新的历程，就跟自动驾驶汽车一样。现在我们需要的数据标注量已经远远少于过去了，什么时候我们不再需要外部数据、模型的训练不再需要人工数据时，那就已经发展到新的阶段了。换言之，这意味着那时人类提供的一切技能和知识都已经不重要，没办法推动模型的进一步提升了。但对于Scale这样的企业，我们一直在研究如何刹那起能够发现新需求，并与贡献者网络合作的运营体系。我们会邀请专家贡献者来挖掘这些数据和信息。另外，他们的很多才能并不会第一时间就表现出价值。比如一年前很多知识对模型没用，但现在却突然有了大用。这是个不断进步的过程，需要将越来越多的数据输入到模型当中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;出于经济动机，我们相信人类永远会在其中占据一席之地。这不只是种商业判断，更是我的个人信念，就像AI系统永远要为人类服务一样。我甚至觉得随着脑力岗位的逐渐消失，这就是未来知识工作者的主要转型方向。而且从我对部分客户身上观察到的情况，这种转变很可能在未来一到两年内发生。我当然希望这种颠覆别来得太快，但目前来讲确实是一切皆有可能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至于从长远来看，新技术总会替代旧方案，比如收音机淘汰了现场播讲之类。人类还是很善于适应这种变化的，技术的发展史就是人类适应新模式的过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您经常会提到“评估”这个词。那评估工作在专家们的日常工作中占比多少？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：占比还是相当高的。在企业和政府客户中，大部分业务内容就是评估，因为需要有人来设定“好”的基准。以之前提到的医疗保健来讲，医生在工作中就会评估病历报告和记录内容，然后对“好”做出明确的定义。这样慢慢累积起来的“好”和“正确”，就会让模型变得越来越可用。当然，AI的能力仍然有局限。对于那些人工流程的准确率很低的场景来说，AI就特别重要，因为能够切实帮上大忙。如果AI在其中能够达到50%、60%甚至70%、80%的准确率，那大家就乐疯了。但对于剩余的情况，比如人工流程的准确率能够达到98%，希望AI能解决余下的2%，那就很困难了。正因为如此，我们才需要明确定义“好”，让自己构建的系统能够代表使用者做出判断。在这样的设计思路下，AI系统就能像人类一样根据当前的信息尽可能给出最佳建议或者行动方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：很多人觉得AI是基于海量历史数据训练出来的，那AI在智能水平上怎么超越人类呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这种理解对，但也不对。首先，跟以往的任何一次重大技术革命一样，新闻媒体总是过度乐观，而实际操作却没那么简单。互联网时代，宽带的铺设过程需要覆盖全国的每一条道路，还要跨大洲之间建立海底光缆，这些都需要时间。而且这些铺设工作总得有人去做。负责任地讲，作为从业者，我对于如今大模型在一致性和准确性方面的提升仍然感觉喜出望外。现在大家可能已经习惯了大模型越来越先进也越来越靠谱，但短短三年前这个问题还相当复杂，需要综合考虑多种因素。总之，大模型的发展是算力、模型本体和数据改进的共同产物，而这三条确实在同时进步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：关于未来两到三年的AI模型发展趋势，你觉得当前大家的普遍认知还有哪方面缺失？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：大家对这个问题讨论得倒是不少，新闻报道也是层出不穷。具体怎么理解，要看我们选择怎样的视角。目前的总体趋势是从模型到模型功能的过渡。接下来最关键的问题是，大模型能为我们做什么，智能体如何替我们做出决策。一旦到了这个阶段，我们之前提到的应用环境就非常重要了。比如怎么让智能体在医疗保健系统内正确导航、如何在手机上的天气应用中导航，又怎么替我们做出决策。目前这一切才刚刚起步，我也期待看到后续的更多变化。而这也是大家相对不太了解的层面，对于改进的方式也是莫衷一是。如果选择最乐观的判断，那这一切就是经济体系下的又一波正常变动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换言之，AI普及不再是技术问题，而是人力和政策层面的问题。虽然目前还没到这个程度，但我确实相信未来两到三年之内，AI技术会发展到中心让治理层和政策制定者认真对待的阶段。现在已经离那个状态不远了，也就是两到三年的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：最近有不少反对的声音，认为AI并没能满足人们对技术的全部想象，特别是在企业应用领域。您怎么看这个问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：主要还是炒作得太狠了。我们的工作是打造出真正能够为客户创造价值的产品，并真正在解决复杂性上有所突破。还是以医疗保健系统为例，我们就在为医保公司提供理赔流程管理。这样的财务决策其实就是个可以自动化的过程，但在具体落地上学问就大了。很多人觉得概念验证能达到60%、70%的成功率就行，但这跟规模化应用还差得远。以数据中心为例，正常运行时间、可靠性和备份稳定性方面每增加一个“9”对应的都是又一个数量级的投入。比如四个“9”基本就是大学生自架服务器的水平，而五个“9”看起来只高了一点点，但其实完全是另一个世界。比如说很多人认为95%是个挺高的标准，但一旦用这个标准处理采购订单，那必然会面对无穷无尽的故障和投诉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总之，企业在应用AI时需要一步步完成法律批准、政策批准、监管批准和变更管理等各个环节，确保精度能让所有人满意。因此，AI方案往往需要半年到一年才能真正发展壮大，实现关键业务流程的自动化。所以大家一定要分得清新闻炒作跟实践落地。就像我自己的教育背景，博士这个头衔说起来轻，但背后需要付出的努力远超大多数人的想象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;所有好生意，都是在不确定中被验证出来的&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您参与建立了Uber Eats，还创办过其他几家初创企业。关于获客户这个问题，您有没有什么独家心得可以分享？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是那种愿意尝试一切新鲜事物的人，而且我总觉得创业是个非常清晰而且可把握的过程。我自己的思路是这样：在实际行动之前，先质疑自己听到的每一句话。我不会从字面上理解客户的表达，而是从产品管理的角度来审视。这个大家已经讨论得足够多了，比如说别按他们说的做、而要按他们预期的效果来做，这才是真正值得关注的问题。总之我会关注客户的潜在动机，而这种动机并不总是经济性质的，也往往跟自尊心和职业发展相关。比如说如果我们要向某人推销企业软件，那就得让对方相信你的软件能帮他们做好工作、建立起信任让对方接受你参与到大的项目中来。这个过程中重要的不只是产品，更要思考他们想得到怎样的建议、需要我们提供什么、需要怎么做才能找到正确的产品实现方向等等。我知道这话听起来有点陈词滥调，但只要让我准确把握住对方的真实动机，我就能拿出正确的结果。我再举个例子，当初在发布Uber Eats之前，我有认真考察业务。在获客方面，我们其实还给不出餐厅导览的功能，对餐饮行业也一无所知。但在Uber，我们最想解决的是接下来该拓展哪些其他业务。在考察了大量企业之后，我们觉得外卖业务最值得尝试，结果也证明这是正确的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了搜集数据，我们找了一家餐厅供应商并拿到一份基础目录，比如说一份典型的餐食要用多少火腿、多少奶酪、多少面包和多少片生菜，再据此推断食材成本有多少、人工成本是多少，进而建立起基准数据。把这些因素综合起来，我们就能把餐食品类建立起清晰的认知。我们发现食材在每份餐食中的成本大约占20%到30%，人工又占20%到30%，10%是房租和其他开销。总之这就是一种链条，而结合核算出的附加价值之后，我们决定收取账单总价的30%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;餐厅刚开始会觉得这个数字太高了，本能性地想要拒绝。我们解释了自己的核算方式，说服对方做起来试试。事实证明餐厅的判断是正确的，这个比例确实太高了，最终确定下来的抽成是25%——跟我们的判断也相差不远。在这样的基础之上，我们再分析餐厅的主要价值实现形式是什么。对于游客型餐厅来讲，增加需求就是最关键的。在固定的餐厅店租、人力支出和食材成本都不变的前提下，需求增加三倍并不会增加人力支出，单纯的食材用量增加可以让产品的毛利率提升至70%到80%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这样的洞察，我们有信心在抽取一定费用之后店家和消费者都能接受。这就是市场经济的基本逻辑——不会只满足单独一方的所有需求，各方牺牲一点利益来保障自己余下的利益。Uber Eats就是这样的典型案例。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以善于独立思考闻名，这种能力为什么如此重要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：从创始人的角度来论述，在Uber我确实享受到了很多优势。我其实不算真正的创始人，只是参与了创业的流程。创业是涉及很多要素的，比如在97年创办第一家公司时，体验其实也就那样。但现在的创业可不一样了，每个人都在做自己的探索。但问题是，我们的研究方向多少都要受到周围言论的影响，那就没办法拥有独立的洞察力。所以最重要的独立思考，坚定去践行自己的判断。因此从创业的角度讲，我认为独特性非常重要。所以核心考验的就是人的洞察力，至于为什么我会幸运地拥有这种洞察力。这个问题的实质，是大家对于未来机遇的洞察，包括这种洞察来自哪里。为什么你能够在数百万头脑聪慧、乐于尝试的创业者当中脱颖而出，掌握其他人所没有洞察力。谁能做到这一点，谁就可以领先一步。也许是因为我有点“遗世独立”，也可能是因为我是那种擅长逆向思维的人，总会在寻求其他人不相信的真理，有时候这也挺有效的。而且最难的部分是，我们愿不愿意在自己的判断上押上五到十年时间？人们总会犯错，只能尽量跟客户沟通，试着解决困扰他们的问题。创业就是这样，我们必须有这种强烈的自我表达意愿，不断摧毁自己曾经坚信的东西。更难的一点在于，我们又要有能力超越自己的观点，不能自大到总认为自己就是全世界最了不起的独立思考者。这个事很辩证，但最终就是要靠结果来证明和支撑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您向来以严苛的业务衡量标准著名。那从创业的角度来看，您的核心理念是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有时候最靠谱的创业方向，并不在于所谓最好的思路和机会。当然，对于我这种职业生涯已经超过25年的从业者，那选择空间和容错面会大得多。我认为一种业务的成功可以有两种途径：其一，也可以说是最重要的一种，就是创始人长期保持一股自我迭代和更新的力量。但这种年复一年的坚持其实很艰难。第二点是，可以直接去照搬其他人的经验，比如什么是好的商业模式、什么是差的商业模式、什么是好的市场定位、什么是差的市场定位。哪怕是拥有再强大的知识储备，哪怕理论上要进入的是一种比较差的市场定位，那只需要全身心投入，那随着时间推移也会逐渐显现回报。当然，我个人不会选择这种方式，我认为还是要根据市场需求走。纵观顶尖风投企业投资的项目，就会发现他们的投资组合是有规律的，至少是在对应价值数百亿美元的商业模式方面是有共性的，而且是具有网络效应的。规模大的业务就是比规模小的业务更有价值。比如我在Uber做过的新业务筛选，淘汰不好的想法其实挺快的，费不了多少时间。至于在筛选剩下的业务中，那就可以根据自己的直觉和热情去推动了。总而言之，我觉得大多数人对于哪些业务有机会增长到千亿美元规模缺少基本的理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您推动了Uber Eats的上线。那在确定选择外卖赛道之前，你还探索过哪些其他想法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我肯定不是那种绝顶聪明的人，所以我会尽可能长期保持自己开放的态度，直到把各种有价值的元素都融合起来。有些想法刚开始看也许觉得比较差，但只需要不断挖掘，最终的对错判断很可能会反转。有一天，我在旧金山到处乱逛，看到了711之类的便利店。我就会想，大家要买到自己想要的东西需要转多少个拐角？难道不能直接把想要货品直接放进购物车吗？比如按下购物车上的按钮，它就直接把想要的货品送过来。毕竟叫“便利”店嘛，就得足够便利。所以我们在华盛顿特区推出了这项服务，在路上投放了大约十辆这样的卡车，里面装了大约250个吐货口。刚开始的情况很糟糕，根本就没多少人来买。于是我们意识到自己在下意识地找痛点，并不清楚便利店的核心吸引力是什么。我们的卡车不卖烟、不卖啤酒、不卖豌豆泥，我们不清楚大家最想买哪些商品。但说实话，工会的力量太强了，所以我天然地认为别用人工是最安全、成本最低的。我们很出色地解决了这个非经济学层面的问题，实现了便捷交付。但结果呢？我们做了Uber Spot还是什么，但跟点对点配送的Uber Direct一样，刚起步就表现不好。也就是说，消费者并没这方面需求，企业才有这种需求。2014年我们刚做尝试时，就没找到市场需求。后来我们持续更新了15个版本，最终发现外卖业务的表现才更出色，也拥有可靠的经济回报。这是个很酷的问题，我们可以用这些工具来支持夫妻店，让他们具备跟大企业竞争的资格。我们可以把房地产因素排除在外，让店面选址不再决定一切。只要你的餐食好吃，就完全可以吸引更多顾客。所以我觉得这是个很有趣的问题，真正促进当地经济发展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：Uber Eats最终在Uber的危机时刻拉了母品牌一把，现在业务规模发展到多大了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们是2015年12月在多伦多推出了这项服务，大约两小时内销售额就达到了2万美元，简直是疯狂。我们很快意识到这个路子是对的，而且经济效益很好。我在Uber待了六年左右，用了一年半左右才把这个项目真正做起来，并在四年半之后把销售额做到了200亿美元。必须承认，Uber非常擅长扩大业务规模，但竞争激烈的市场上其他友商也做得不错。我们击败了很多对手，也有一些对手确实压我们一头。目前业务规模正在向着800亿美元迈进，时间才过去了短短四年半。我想COVID在其中也发挥了很大的推动作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：众所周知，您曾经反对麦当劳加入Uber Eats。能分享一下这个故事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：这事说起来就有意思了。或许有时候不想太多反而会让人意外找到正确的原因。我们发布的Uber Eats在全球范围内获得了广泛成功，而其中的基本愿景很简单：让小餐馆也能跟大规模连锁餐厅正面竞争。以巴黎来说，大家去巴黎旅行肯定不想吃大牌连锁餐厅，而更想发掘本地特色小店。这是现实需求，我们也决定参与其中。但后来麦当劳联系了我们，表示想跟我们一起做外卖业务。我们拒绝了，哪怕对方强调他们的日均消费者高达8000万。在拖了四、五个月之后，我们团队觉得我肯定是疯了，他们想促成这件事、而且愿意为之倾力投入。最终，我们还是跟麦当劳建立了独家合作关系，获得了大量连锁店客户。那时候大家都担心每单收益还能不能保证，毕竟订单规模到了单日几千万级别，这肯定是笔大钱。面对现实问题，Uber的企业文化就是缩小配送半径、在必要时提高某些食物的价格，反正总有办法解决的。三个月之后又有新的问题出现，业务再次陷入困境……总之很多同事觉得我在跟麦当劳合作方面太固执了，但我觉得最终至少还是达成了一笔很棒的交易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：你一直很关注毛利率，能不能具体说说？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我是这样的，当然这只是我的评判标准之一。当然，也有不少企业本身很好，但毛利率也不高，比如说好市多、沃尔玛之类。亚马逊也有类似的情况。但总的来说，高毛利率加上相对较低的客户流失曲线，对企业来说肯定是个很健康的运营信号。毕竟生意的本质就是增加价值，这就像一块客观的试金石。我们在开展新业务的时候肯定也受到过毛利率问题的困扰，比如刚开始先试试毛利率40%的方案，发现可行再试着提升到60%——这时候商家就觉得不能接受了，大家再坐下来交流。至于离岸外包公司，他们的毛利率是多少？查了一下，大概在20%，而且已经运营了很长时间。那按这个规律来讲，我们的毛利率最终也将不可避免地从40%下降到20%，除非真能找到差异化的突破，否则必然要陷入这个巨坑。所以我认为毛利率只是个很粗糙的指标，远不能算是完美的工具。但至少它可以是种快速高效的过滤器，可以考查突然跳出来的想法能不能通过初步评估。比如说核算之后发现毛利率很低，就只能通过后续销量来弥补，那这事恐怕就不大行得通。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您提到“不输”是获得成功的先决条件，能不能给我们具体解释一下这个理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我们的科技文化是由投资者来建立投资组合，很多叙事是由投资者掌控。坦率地讲，创始人肯定也会参与其中。这种情况当然是比较理想的，只是我们没法确定自己会不会有这份运气。如果真说自己人生中只有一次尝试的机会，那我肯定不会轻易去行动，必须得三思而后行。虽然没有这方面数据做支撑，但我发现自己朋友圈里的企业家和最出色的创业者，会审视自己做决策时的风险状况，并在整个过程中都持续做出均衡和积极的决策调整。很多时候我们会忘记决策背后是对应着风险的。这里还有很多可以讨论的部分，因为我觉得用高风险决策最终取得成功是种特别不可取的文化现象，连培训当中都会认可这种思路。毕竟高风险决策必然带来巨大的波动，这对创始人最重要的特质——也就是坚持下去的能力是种直接挑战。大多数人在寻求最佳时机、与客户建立良好关系和将合适产品推向市场之前，就已经放弃退出了。而科技行业瞬息万变，我们确实可能在短时间内从平凡之人变成技术英雄，但大概率这会是个漫长的过程，得先活下来才能谈成功。而当前我们正身处炒作周期，每个人都想尝试、全力投入，但却没意识到客户会一直在，希望自己的问题能得到解决。总之，生存是一切的前提，我们要尽量别把业务发展置于危及的境地。当然，我也不是说完全拒绝任何冒险，这是个需要认真权衡利弊的大问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快问快答&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您肯定也经历过失败，能不能分享一条从痛苦经历中汲取到的教训？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：我还是认为多花点时间提前思考，可以避免后续的很多麻烦。我之前确实做过一些尝试，但结果一般就不细聊了。在2001年互联网泡沫破裂之后，我曾打算筹资创办一家公司、而且是能赚钱的公司，成果就是Scour。坦白讲，当时我没在科技行业发现什么好机会，所以我开始在网上卖二手高尔夫球杆，还真赚了不少钱。那会我才22岁，考虑得并不周全，而且我的预期也不高，因为我觉得这生意什么人都能做。但我确实赚了很多钱，甚至想过把全美国所有的二手高尔夫球杆都买下来，直接操控整个市场的交易价格。我太年轻、太自负了，根本没认真思考过这件事的可行性。总之我就这样进入了这个行业，还靠这个赚了几百万美元。但整个过程都让我很痛苦，因为&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您对招聘人才和组建团队很有见解，能不能聊聊自己的理念？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：最近我对这个问题又有了更具体的理解。比如某些职位，就必须要有对当前市场丰富的观察和理解。毕竟市场发展太快，没时间慢慢去培训，所以要找的就是那些哪怕其他条件弱些、但真正理解市场和能够跟客户建立良好关系的人。有了这个前提，其余的部分才能跟公司共同成长，并建立起理想的职业发展轨迹。当然，这类职位只能在公司中只占5%，但它们对于产品的快速上市非常重要。比如在面试当中，我就只考查三点：对于解决问题是否抱有足够的好奇心，是不是擅长把自己的想法准确表达出来，还有能不能很好地跟其他人合作、特别是扮演好领导者。我相对不那么看重专业知识，毕竟我自己肯定有能力边界，不可能在所有专业知识上都做出准确判断。但只要能成功做好这几点，对方的成功几率就相当高。面对世界的持续变化，我们需要的就是具备极强适应能力的人。以Uber Eats为例，当初组建项目管理团队时，我总会通过招人把团队设计成一个优势互补扔 机体，同时尽量减少团队中除运营以外的大部分冲突。而且从一无所有到高达200亿美元的估值，我的这个理念始终没有动摇。我一直坚信团队成员间了解彼此的优势和劣势，而且能够相互弥补，这比传统上的各种考核标准都要靠谱。换句话说，我必须得学会相信员工，因为我不可能亲自掌控一切。当然，人事系统是非常复杂的，不可能基于我简单的几句话就生搬硬套。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：在日常生活和工作当中，你发现了哪些AI应用方式？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：说实话，我在加入Scale之前是在消费电子领域工作，也参与过政府层面的一些应用项目。AI这个领域发展太快了，每当有新概念出现我都会认真学习，也会向公司里的其他同事请教技术细节，比如说数据和产品的技术特征。但他们的时间也有限，更多新概念还得靠自己主动学习。大家可能不相信，我的主要工作并不是处理跟AI相关的工程问题，而是管理这个组织。为了避免频繁打扰同事，很多时候我也会直接跟AI学习，在上下班的路上跟它聊天。这已经变成我生活中的一种习惯，也是我从业以来见证过的最不可思议的奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如之前采访Perplexity创始人的时候，对方就介绍他们要求同事在提出任何问题之前，必须先请教AI。那时候这还是种很疯狂的工作方式，但现在看来他们的领先恰恰体现在这里。至于在工作当中，我会上传一份内部文件，然后边亲自阅读边比较它的提炼结果。让人震惊的是，AI的表现真的非常出色，而且帮我节约了大量时间。在大规模组织中，我们经常会遇到这样的难题：我不知道你想让我说什么，我也不知道自己需要了解什么，这就导致大家各有议程、自说自话。那面对这样的传播挑战，AI确实能帮上大忙，太神奇了。我现在会用它来处理法律文件，比如快速了解对手打算怎么对付我、我又该怎么应对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您还有什么想跟听众们分享吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：有啊。我想向大家强调Scale团队的卓越贡献。我们一直非常努力，持续为客户提供巨大的价值。任何语言在这份努力面前都显得苍白无力，更无法体现客户在此基础之上解决的无数问题。我认为这份付出配得上一切尊重和回报。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最近发现什么自己特别喜欢的产品了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：应该说是Veo 3吧，虽然不算全新产品。高中时我曾梦想当个编剧，还认真写过剧本。所以这次我找出第一页、拍下照片，再上传到Veo 3。结果真的让我震惊，居然一张剧本照片也能生成相应的视频画面。现在我在考虑怎么把这些工具用来生成家庭录像，再利用其他工具让内容更加生动。虽然还有进一步迭代的空间，但这种体验真的很有趣。这类工具真的会改变人们的情感生活，比如让祖父母、亲戚或其他很久没见的人在照片中动起来，这会产生很大的情感冲击。训练出这套模型的技术人员很厉害，而帮助他们做专家级数据标注的服务者也很厉害。这种直接把文字转化成光景的能力很棒，也很酷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：您最喜欢的人生格言是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：终点永远不是终点。这是我最喜欢、也深深牢记心中的格言。这跟之前关于生存才是第一要务的观点差不多，只有先活下来才有机会获得成功。纵观自己的创业历程，我觉得这条的指导意义最大。每个人都会经历艰难的旅程，但只要能在这段旅程中坚持五年，那大家的精神承受力绝对会比99.9%的人强。更具体地讲，我们在努力工作时会深切感受到这句话的意义。有时候我们觉得自己太累了，想要停下来，但事实上只要继续前进，似乎就又可以坚持下去了。我牢记这句话，提醒自己任何一个节点都不是真正的终点，仍然有更远的标的有待探寻。所以无论当下的解决方案到底完不完美，我们都可以先勇敢接受，然后抖擞前行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Lenny：如果想要与您交流或者了解更多关于Scale的信息，应该怎么安排？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason Droege：当然可以，大家可以关注我的邮箱，随时了解最新动态。如果是招聘方面的诉求，可以直接访问scale.com、进入我们的招聘页面，目前公司开放了250个空缺职位。我们的业务仍在扩展，包括应用程序业务、数据业务和服务业务都在疯狂增长。我们需要更多人手来帮助我们推进这段旅程。我们还刚刚跟政府签下了巨额合同，金额是21亿美元——而且不是一份，而一个月内签了两份。我们的政府业务做得很好、企业业务做得很好、国际政府业务同样做得很好。公司市场需求很大，也让不少销售人员拿到了丰厚的佣金。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&quot;&gt;https://www.lennysnewsletter.com/p/first-interview-with-scale-ais-ceo-jason-droege&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</link><guid isPermaLink="false">https://www.infoq.cn/article/LtKGv3oRvYQHSzUir3O6</guid><pubDate>Tue, 10 Feb 2026 11:43:36 GMT</pubDate><author>核子可乐,Tina</author><category>生成式 AI</category></item><item><title>Java探索载体类以扩展面向数据编程</title><description>&lt;p&gt;OpenJDK的&lt;a href=&quot;https://openjdk.org/projects/amber/&quot;&gt;Amber项目&lt;/a&gt;&quot;发布了一份全新的设计说明，名为“&lt;a href=&quot;https://openjdk.org/projects/amber/design-notes/beyond-records&quot;&gt;Java面向数据编程：超越记录类（Record）&lt;/a&gt;&quot;”，阐述了一种探索性的方案，以便将类似记录类的特性拓展至更灵活的类设计中。该文档引入了载体类（carrier class）与载体接口（carrier interface）的概念，目标是提炼记录类的核心优势并进行通用化适配，同时不再强加严格的表述规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Java 16引入了记录类，为不可变数据载体的建模提供了简洁的方式。如下这种记录类的声明：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;record Point(int x, int y) { }
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译器会自动为其生成规范的构造器、访问器方法，以及equals、hashCode和toString方法的实现。记录类还支持解构（deconstruction）模式，可以配合instanceof和switch关键字使用。结合密封类与模式匹配特性，记录类能实现Java中代数数据类型的建模。例如，HTTP客户端或网关可以按如下方式定义不同的响应类型：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;public sealed interface HttpResponse permits HttpResponse.Success, HttpResponse.NotFound, HttpResponse.ServerError {
    record Success(int status, String body) implements HttpResponse {}
    record NotFound(String message) implements HttpResponse {}
    record ServerError(int status, String error) implements HttpResponse {}
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这样的响应类型层级，我们可以通过穷举式模式匹配进行统一处理：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;static String handle(HttpResponse response) {
   return switch (response) {
       case Success(var code, var body) -&amp;gt; &quot;OK (&quot; + code + &quot;): &quot; + body;
       case NotFound(var msg) -&amp;gt; &quot;404: &quot; + msg;
       case ServerError(var code, var err) -&amp;gt; &quot;Error (&quot; + code + &quot;): &quot; + err;
   };
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此示例中，编译器会强制检查是否覆盖了所有允许的响应类型。若新增一种响应类型，必须同步更新该switch表达式，从而降低不完整错误处理的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在近期的一次讨论中，甲骨文公司的Java语言架构师&lt;a href=&quot;https://www.linkedin.com/in/briangoetz/&quot;&gt;Brian Goetz&lt;/a&gt;&quot;指出，这些特性的组合虽然能实现强大的数据建模能力，但实际落地却经常会受到长期形成的面向对象设计习惯制约。他发现，即便现代语言特性已能大幅减少间接代码，开发人员仍会习惯性地设计用于中介数据访问的API。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明重点聚焦于记录类无法适用的场景。实际开发中，许多数据类型需要派生值或缓存值、可选的内部表示形式、可变性或继承特性。在这种情况下，开发人员只能退而求其次，使用传统类，并重写大量的样板代码。文档将这种转变形容为“断崖式回落”，对记录类的基准模型做微小调整，就会导致代码量的大幅增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了缓解这一问题，文档提出了载体类的设计思路。载体类以类似记录类头信息的状态描述作为开头，其余行为则与普通类完全一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，状态描述用于定义类的逻辑组件，编译器可基于这些组件自动生成访问器、对象方法及解构模式。与记录类不同，载体类不要求将所有状态仅存储在这些组件中，这也是其核心灵活性所在。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种灵活性能实现记录类难以表达的设计模式，例如缓存派生值的场景：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;class Point(int x, int y) {
    private final component int x;
    private final component int y;
    private final double norm;

    Point { norm = Math.hypot(x, y); }
    double norm() { return norm; }
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此例中，派生值norm在构造阶段计算完成，并且未纳入状态描述，但该类仍能借助编译器为其组件自动生成的方法，减少样板代码编写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;载体类同样设计为可与模式匹配深度集成，用法与记录类一致：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;if (obj instanceof Point(var x, var y)) {
    // use x and y
}
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设计说明中还进一步探讨了载体类与未来重构特性的兼容性，例如，针对记录类的&lt;a href=&quot;https://openjdk.org/jeps/468&quot;&gt;JEP 468&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除载体类外，该提案还引入了载体接口的概念，接口可声明自身的状态描述，并且所有实现类都能参与统一的模式匹配：&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;java&quot;&gt;interface Pair&lt;t, u=&quot;&quot;&gt;(T first, U second) { }

switch (pair) {
    case Pair(var a, var b) -&amp;gt; ...
}
&lt;/t,&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种设计能简化日常开发中常见的元组式抽象，同时保留Java强类型的优势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这份设计说明将载体类置于Java向面向数据编程整体转型的背景下，通过结合记录类、密闭类型、模式匹配，再加上潜在的载体类，Java正逐步引导开发者直接建模数据结构，而非依赖层级繁杂的API。Goetz认为，当前的核心挑战在于，帮助开发者意识到，在将“数据”作为首要抽象时，大量的支撑性代码都可被省略。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，“超越记录类”还属于探索性的文档，官方尚未公布具体的语法定义、JEP提案及版本发布时间表。但这份文档释放了明确的信号，那就是Amber项目将持续推进相关研发，进一步减少Java的样板代码，并将现代语言特性拓展至更复杂的类设计中，而这些探索，也许会将影响未来版本中Java开发者构建以数据为核心的 API的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/java-beyond-records/&quot;&gt;Java Explores Carrier Classes to Extend Data-Oriented Programming Beyond Records&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</link><guid isPermaLink="false">https://www.infoq.cn/article/PoMHdgGXx5dywm8pYDme</guid><pubDate>Tue, 10 Feb 2026 11:26:42 GMT</pubDate><author>作者：A N M Bazlur Rahman</author><category>编程语言</category></item><item><title>谷歌推出托管AlloyDB连接池</title><description>&lt;p&gt;谷歌云&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/release-notes#December_18_2025&quot;&gt;正式发布&lt;/a&gt;&quot;AlloyDB for PostgreSQL通用托管连接池，将类似PgBouncer的功能直接集成到数据库服务中。按照谷歌的说法，与直接连接相比，这一特性能够提供3倍多的客户端连接和高达5倍的事务吞吐量，帮助开发者解决了运行高并发工作负载时面临的扩展挑战。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;连接池并不是什么新鲜事。多年来，为了重用数据库连接而不是为每个请求创建新的连接，开发者们将&lt;a href=&quot;https://www.pgbouncer.org/&quot;&gt;PgBouncer&lt;/a&gt;&quot;或&lt;a href=&quot;https://www.pgpool.net/docs/latest/en/html/&quot;&gt;pgpool&lt;/a&gt;&quot;作为单独的基础设施进行了部署。现在，AlloyDB可以自动完成这些工作了。开发者可以通过控制台复选框或API调用来启用它，连接池使用6432端口，而常规连接使用5432端口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;托管连接池会缓存预先建好的连接，将它们分配给传入请求，并在使用完成后将它们返回给连接池，而不是关闭它们。谷歌表示，这可以消除“运维负担”，作为AlloyDB实例的一部分，连接池会自动升级和扩展。连接池和数据库之间的通信在谷歌云的网络内运行，可能比外部连接池设置的延迟小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于Cloud Run或Cloud Functions上的无服务器部署，其优势更为显著。这些平台会启动多个实例，每个实例都会打开数据库连接，在流量高峰时往往会超出PostgreSQL的连接限制。对于这种情况，连接池是一个很好的缓冲，它利用现有的连接处理请求，而非强制数据库同时处理数百个新的连接尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UKG高级首席架构师Jeff Bogenschneider在早期测试期间描述了其影响：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;AlloyDB的架构使我们能够在单个集群中部署的数据库数量远超其他Postgres托管服务。此前我们曾担心连接限制问题，而托管连接池可以帮助我们确保全球的客户都能获得最佳的性能，让我们得以自由地扩展业务，而不用担心在高峰使用时段遇到连接限制问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;运行微服务的开发者应该考虑将应用端连接池与AlloyDB的托管连接池配对。在&lt;a href=&quot;https://medium.com/google-cloud/elastic-microservices-rigid-databases-connection-exhaustion-8cdc558f212a&quot;&gt;Medium&lt;/a&gt;&quot;上，Adarsha Kuthuru和Kumar Ramamurthy详细描述了这种“双池”模式：像HikariCP这样的应用连接池为每个实例维持5-10个到AlloyDB连接池的连接，后者通过多路复用将这些连接连接到数量更少的后端数据库连接。这个方案可以避免为50个微服务实例各建立20个连接时，1000个并发连接冲击数据库的场景。作者建议为每个vCPU配置15-20个连接器连接，并协调各层的超时设置，避免连接重置错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能提供两种连接池模式。事务模式（默认）通过为每个事务分配独立的连接来最大化可扩展性；会话模式完全兼容PostgreSQL的功能。开发者可以通过AlloyDB API中的标准PgBouncer参数调整连接池规模、超时设置及空闲阈值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该功能存在一些限制。托管连接池不适用于AlloyDB Auth Proxy或语言连接器——开发者需要直接连接。这妨碍了依赖身份验证代理进行凭据轮换或简化TLS配置的部署模式。在2024年11月前部署的实例上启用连接池功能时，由于要更新VPC设置，会引发短暂的网络中断（持续时间少于15秒）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于已经单独运行PgBouncer的开发者而言，迁移至托管连接池主要在于整合基础设施——减少一个需要打补丁的组件。对于新增部署，尤其是无服务器或高并发工作负载，启用该功能所需的投入极少，却能防患于未然，在扩展问题爆发前将其及时化解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;谷歌提供了&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling&quot;&gt;配置托管连接池&lt;/a&gt;&quot;的文档和在现有实例上&lt;a href=&quot;https://docs.cloud.google.com/alloydb/docs/configure-managed-connection-pooling#enable-managed-connection-pooling&quot;&gt;启用该特性&lt;/a&gt;&quot;的最佳实践。对于双池模式，发表在Medium上的博文提供了一份部署指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&quot;&gt;https://www.infoq.com/news/2026/01/alloydb-managed-connection-pool/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</link><guid isPermaLink="false">https://www.infoq.cn/article/zjNtBfmQQa580Aoc9Rud</guid><pubDate>Tue, 10 Feb 2026 11:21:10 GMT</pubDate><author>Steef-Jan Wiggers</author><category>Google</category><category>大数据</category></item></channel></rss>